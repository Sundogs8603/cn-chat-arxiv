<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65292;&#24182;&#39564;&#35777;&#20102;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#20943;&#36731;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#23545;&#31574;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02067</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65306;&#26397;&#30528;&#26356;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability. (arXiv:2310.02067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65292;&#24182;&#39564;&#35777;&#20102;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#20943;&#36731;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#23545;&#31574;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#22270;&#20687;&#21462;&#35777;&#30340;&#32972;&#26223;&#19979;&#65292;&#24456;&#38590;&#30830;&#23450;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20165;&#20165;&#21033;&#29992;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#36890;&#24120;&#65292;&#26102;&#38388;&#30456;&#36817;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#23646;&#20110;&#21516;&#19968;&#24180;&#40836;&#31867;&#21035;&#30340;&#65289;&#20855;&#26377;&#19968;&#20123;&#20849;&#21516;&#30340;&#20869;&#23481;&#23646;&#24615;&#12290;&#36825;&#31181;&#20869;&#23481;&#20559;&#24046;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;&#20869;&#23481;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#24102;&#26377;&#23884;&#20837;&#24335;&#24180;&#40836;&#20449;&#21495;&#30340;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#39564;&#35777;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#34920;&#26126;&#65292;&#22312;&#24180;&#40836;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#35757;&#32451;&#30340;&#8220;&#26631;&#20934;&#8221;&#31070;&#32463;&#32593;&#32476;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#20316;&#20026;&#28508;&#22312;&#30340;&#23545;&#31574;&#65292;&#26412;&#25991;&#24212;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#20943;&#36731;&#35757;&#32451;&#36807;&#31243;&#20013;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of temporal image forensics, it is not evident that a neural network, trained on images from different time-slots (classes), exploit solely age related features. Usually, images taken in close temporal proximity (e.g., belonging to the same age class) share some common content properties. Such content bias can be exploited by a neural network. In this work, a novel approach that evaluates the influence of image content is proposed. This approach is verified using synthetic images (where content bias can be ruled out) with an age signal embedded. Based on the proposed approach, it is shown that a `standard' neural network trained in the context of age classification is strongly dependent on image content. As a potential countermeasure, two different techniques are applied to mitigate the influence of the image content during training, and they are also evaluated by the proposed method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#26469;&#35299;&#20915;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#30446;&#26631;&#23646;&#24615;&#12290;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#36890;&#36807;&#32852;&#21512;Transformer&#29983;&#25104;&#30340;&#26032;&#39062;&#20998;&#23376;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02066</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;Transformer&#36827;&#34892;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#26469;&#35299;&#20915;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#30446;&#26631;&#23646;&#24615;&#12290;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#36890;&#36807;&#32852;&#21512;Transformer&#29983;&#25104;&#30340;&#26032;&#39062;&#20998;&#23376;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#38656;&#35201;&#21516;&#26102;&#29983;&#25104;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#20854;&#30446;&#26631;&#23646;&#24615;&#65292;&#36825;&#23545;&#29983;&#25104;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#39033;&#33392;&#24040;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#65292;&#23427;&#23558;Transformer&#30340;&#35299;&#30721;&#22120;&#12289;&#32534;&#30721;&#22120;&#21644;&#39044;&#27979;&#22120;&#32467;&#21512;&#20026;&#19968;&#20010;&#20855;&#26377;&#20849;&#20139;&#26435;&#37325;&#30340;&#32852;&#21512;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#26469;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32852;&#21512;Transformer&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#30446;&#26631;&#23646;&#24615;&#30340;&#26032;&#39062;&#20998;&#23376;&#65292;&#30456;&#27604;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
De novo drug design requires simultaneously generating novel molecules outside of training data and predicting their target properties, making it a hard task for generative models. To address this, we propose Joint Transformer that combines a Transformer decoder, a Transformer encoder, and a predictor in a joint generative model with shared weights. We show that training the model with a penalized log-likelihood objective results in state-of-the-art performance in molecule generation, while decreasing the prediction error on newly sampled molecules, as compared to a fine-tuned decoder-only Transformer, by 42%. Finally, we propose a probabilistic black-box optimization algorithm that employs Joint Transformer to generate novel molecules with improved target properties, as compared to the training data, outperforming other SMILES-based optimization methods in de novo drug design.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AlignDiff&#65292;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#37327;&#21270;&#20154;&#31867;&#20559;&#22909;&#24182;&#25351;&#23548;&#38646;-shot&#34892;&#20026;&#23450;&#21046;&#30340;&#25193;&#25955;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#23558;&#20195;&#29702;&#34892;&#20026;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02054</link><description>&lt;p&gt;
AlignDiff: &#36890;&#36807;&#21487;&#23450;&#21046;&#34892;&#20026;&#25193;&#25955;&#27169;&#22411;&#23545;&#40784;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model. (arXiv:2310.02054v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AlignDiff&#65292;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#37327;&#21270;&#20154;&#31867;&#20559;&#22909;&#24182;&#25351;&#23548;&#38646;-shot&#34892;&#20026;&#23450;&#21046;&#30340;&#25193;&#25955;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#23558;&#20195;&#29702;&#34892;&#20026;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20195;&#29702;&#34892;&#20026;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#20173;&#28982;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#25277;&#35937;&#24615;&#21644;&#21487;&#21464;&#24615;&#30340;&#20869;&#22312;&#29305;&#24615;&#25152;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AlignDiff&#65292;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#37327;&#21270;&#20154;&#31867;&#20559;&#22909;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#25277;&#35937;&#24615;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20559;&#22909;&#26469;&#24341;&#23548;&#38646;-shot&#34892;&#20026;&#23450;&#21046;&#30340;&#25193;&#25955;&#35268;&#21010;&#65292;&#28085;&#30422;&#20102;&#21487;&#21464;&#24615;&#12290;AlignDiff&#33021;&#22815;&#20934;&#30830;&#21305;&#37197;&#29992;&#25143;&#23450;&#21046;&#30340;&#34892;&#20026;&#24182;&#39640;&#25928;&#22320;&#22312;&#19981;&#21516;&#34892;&#20026;&#20043;&#38388;&#20999;&#25442;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#22810;&#35282;&#24230;&#30340;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#23545;&#19981;&#21516;&#34892;&#20026;&#23646;&#24615;&#36827;&#34892;&#27604;&#36739;&#30340;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#23646;&#24615;&#24378;&#24230;&#27169;&#22411;&#26469;&#39044;&#27979;&#37327;&#21270;&#30340;&#30456;&#23545;&#24378;&#24230;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#24378;&#24230;&#37325;&#26032;&#26631;&#35760;&#34892;&#20026;&#25968;&#25454;&#38598;&#20043;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#35757;&#32451;&#20102;&#19968;&#20010;&#23646;&#24615;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#35268;&#21010;&#22120;&#65292;&#23646;&#24615;&#24378;&#24230;&#27169;&#22411;&#20316;&#20026;&#23548;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RL from Human Feedback (RLHF) to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. AlignDiff can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director fo
&lt;/p&gt;</description></item><item><title>&#35780;&#22996;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#19981;&#21516;&#31995;&#32479;&#21644;&#25351;&#26631;&#30340;&#35780;&#20272;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#24230;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.02040</link><description>&lt;p&gt;
&#35780;&#22996;&#65306;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Jury: A Comprehensive Evaluation Toolkit. (arXiv:2310.02040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02040
&lt;/p&gt;
&lt;p&gt;
&#35780;&#22996;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#19981;&#21516;&#31995;&#32479;&#21644;&#25351;&#26631;&#30340;&#35780;&#20272;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#24230;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#20316;&#20026;&#20219;&#20309;&#22522;&#20110;&#39044;&#27979;&#30340;&#31995;&#32479;&#30340;&#22522;&#26412;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#21644;&#21508;&#31181;&#25351;&#26631;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#20351;&#29992;&#19981;&#21516;&#25351;&#26631;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35780;&#22996;&#30340;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#26631;&#20934;&#21270;&#32467;&#26500;&#65292;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#35780;&#22996;&#30340;&#30446;&#26631;&#26159;&#26631;&#20934;&#21270;&#21644;&#25913;&#36827;&#25152;&#26377;&#31995;&#32479;&#30340;&#24230;&#37327;&#35780;&#20272;&#65292;&#24182;&#24110;&#21161;&#31038;&#21306;&#20811;&#26381;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#12290;&#33258;&#35780;&#22996;&#30340;&#24320;&#28304;&#21457;&#24067;&#20197;&#26469;&#65292;&#24050;&#32463;&#21560;&#24341;&#20102;&#24191;&#22823;&#29992;&#25143;&#65292;&#21487;&#22312;https://github.com/obss/jury &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation plays a critical role in deep learning as a fundamental block of any prediction-based system. However, the vast number of Natural Language Processing (NLP) tasks and the development of various metrics have led to challenges in evaluating different systems with different metrics. To address these challenges, we introduce jury, a toolkit that provides a unified evaluation framework with standardized structures for performing evaluation across different tasks and metrics. The objective of jury is to standardize and improve metric evaluation for all systems and aid the community in overcoming the challenges in evaluation. Since its open-source release, jury has reached a wide audience and is available at https://github.com/obss/jury.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#29305;&#24449;&#25552;&#21462;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;CLIP-ViT-B&#21644;ViT-H-14&#31561;&#27169;&#22411;&#20855;&#26377;&#26368;&#20339;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;CLIP-ResNet50&#27169;&#22411;&#20855;&#26377;&#31867;&#20284;&#24615;&#33021;&#20294;&#21464;&#21160;&#36739;&#23567;&#12290;&#36825;&#20026;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36873;&#25321;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.02037</link><description>&lt;p&gt;
&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#29305;&#24449;&#25552;&#21462;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An evaluation of pre-trained models for feature extraction in image classification. (arXiv:2310.02037v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#29305;&#24449;&#25552;&#21462;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;CLIP-ViT-B&#21644;ViT-H-14&#31561;&#27169;&#22411;&#20855;&#26377;&#26368;&#20339;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;CLIP-ResNet50&#27169;&#22411;&#20855;&#26377;&#31867;&#20284;&#24615;&#33021;&#20294;&#21464;&#21160;&#36739;&#23567;&#12290;&#36825;&#20026;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36873;&#25321;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#23567;&#25968;&#25454;&#38598;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#25104;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#19981;&#21516;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#29305;&#24449;&#25552;&#21462;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;16&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#25972;&#20307;&#24615;&#33021;&#30340;&#26159;CLIP-ViT-B&#21644;ViT-H-14&#65292;&#32780;CLIP-ResNet50&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20043;&#31867;&#20284;&#20294;&#21464;&#21160;&#36739;&#23567;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36873;&#25321;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#25552;&#20379;&#20102;&#35777;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, we have witnessed a considerable increase in performance in image classification tasks. This performance improvement is mainly due to the adoption of deep learning techniques. Generally, deep learning techniques demand a large set of annotated data, making it a challenge when applying it to small datasets. In this scenario, transfer learning strategies have become a promising alternative to overcome these issues. This work aims to compare the performance of different pre-trained neural networks for feature extraction in image classification tasks. We evaluated 16 different pre-trained models in four image datasets. Our results demonstrate that the best general performance along the datasets was achieved by CLIP-ViT-B and ViT-H-14, where the CLIP-ResNet50 model had similar performance but with less variability. Therefore, our study provides evidence supporting the choice of models for feature extraction in image classification tasks.
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861; Natural-XAI&#65292;&#35813;&#26041;&#27861;&#27880;&#37325;&#21487;&#25805;&#20316;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#20860;&#39038;&#19981;&#21464;&#24615;&#21644;&#20262;&#29702;&#20851;&#20999;&#12290;&#25105;&#20204;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#30830;&#23450;&#20102;&#20154;&#31867;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#20013;&#30340;&#20027;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#29305;&#24449;&#21487;&#34892;&#24615;&#20998;&#31867;&#23398;&#65292;&#20197;&#31616;&#21270;&#35299;&#37322;&#34920;&#36798;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.02019</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#23398;&#24341;&#23548;&#30340;&#27169;&#26495;&#21270;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Feasible Counterfactual Explanations: A Taxonomy Guided Template-based NLG Method. (arXiv:2310.02019v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861; Natural-XAI&#65292;&#35813;&#26041;&#27861;&#27880;&#37325;&#21487;&#25805;&#20316;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#20860;&#39038;&#19981;&#21464;&#24615;&#21644;&#20262;&#29702;&#20851;&#20999;&#12290;&#25105;&#20204;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#30830;&#23450;&#20102;&#20154;&#31867;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#20013;&#30340;&#20027;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#29305;&#24449;&#21487;&#34892;&#24615;&#20998;&#31867;&#23398;&#65292;&#20197;&#31616;&#21270;&#35299;&#37322;&#34920;&#36798;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322; (cf-XAI) &#25551;&#36848;&#20102;&#23558;&#32467;&#26524;&#20174;&#19968;&#20010;&#31867;&#21035;&#21464;&#20026;&#21478;&#19968;&#20010;&#31867;&#21035;&#25152;&#38656;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; cf-XAI &#26041;&#27861;&#24573;&#35270;&#20102;&#36825;&#20123;&#21464;&#21270;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861; (Natural-XAI)&#65292;&#22312;&#35299;&#37322;&#36807;&#31243;&#20013;&#27880;&#37325;&#21487;&#25805;&#20316;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#20860;&#39038;&#19981;&#21464;&#24615;&#21644;&#20262;&#29702;&#20851;&#20999;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20154;&#31867;&#25152;&#32452;&#25104;&#30340; cf-XAI &#20013;&#23384;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#20027;&#39064;&#65306;&#19982;&#20869;&#23481;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#20851;&#27880;&#22914;&#20309;&#20174;&#21453;&#20107;&#23454;&#21644;&#26597;&#35810;&#30340;&#35282;&#24230;&#21253;&#25324;&#29305;&#24449;&#21450;&#20854;&#20540;&#65307;&#19982;&#32467;&#26500;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#20851;&#27880;&#29992;&#20110;&#25551;&#36848;&#25152;&#38656;&#20540;&#21464;&#21270;&#30340;&#32467;&#26500;&#21644;&#26415;&#35821;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22235;&#20010;&#28165;&#26224;&#23450;&#20041;&#30340;&#29305;&#24449;&#21487;&#34892;&#24615;&#20998;&#31867;&#23398;&#65292;&#20197;&#31616;&#21270;&#35299;&#37322;&#34920;&#36798;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#20154;&#31867;&#29983;&#25104;&#30340; cf-XAI &#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#29305;&#24449;&#30340;&#21487;&#34892;&#24615;&#21010;&#20998;&#20026;&#22235;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#21487;&#30452;&#25509;&#25805;&#20316;&#12289;&#21487;&#38388;&#25509;&#25805;&#20316;&#12289;&#19981;&#21487;&#25805;&#20316;&#21644;&#26410;&#23450;&#20041;&#12290;&#36825;&#20010;&#20998;&#31867;&#23398;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915; cf-XAI &#26041;&#27861;&#20013;&#21487;&#34892;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (cf-XAI) describe the smallest changes in feature values necessary to change an outcome from one class to another. However, many cf-XAI methods neglect the feasibility of those changes. In this paper, we introduce a novel approach for presenting cf-XAI in natural language (Natural-XAI), giving careful consideration to actionable and comprehensible aspects while remaining cognizant of immutability and ethical concerns. We present three contributions to this endeavor. Firstly, through a user study, we identify two types of themes present in cf-XAI composed by humans: content-related, focusing on how features and their values are included from both the counterfactual and the query perspectives; and structure-related, focusing on the structure and terminology used for describing necessary value changes. Secondly, we introduce a feature actionability taxonomy with four clearly defined categories, to streamline the explanation presentation process. Using insights 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#28145;&#24230;&#38480;&#21046;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36991;&#20813;&#26799;&#24230;&#29190;&#28856;&#12290;&#30740;&#31350;&#32473;&#20986;&#20102;&#19968;&#31181;&#20855;&#20307;&#26500;&#36896;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#22312;&#20219;&#20309;&#28145;&#24230;&#37117;&#33021;&#20445;&#25345;&#20248;&#21270;&#30340;&#20449;&#21495;&#20256;&#25773;&#29305;&#24615;&#65292;&#24182;&#20855;&#26377;&#26377;&#30028;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.02012</link><description>&lt;p&gt;
&#36229;&#36234;&#28145;&#24230;&#38480;&#21046;&#30340;&#35757;&#32451;&#65306;&#25209;&#24402;&#19968;&#21270;&#36991;&#20813;&#26799;&#24230;&#29190;&#28856;
&lt;/p&gt;
&lt;p&gt;
Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion. (arXiv:2310.02012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#28145;&#24230;&#38480;&#21046;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36991;&#20813;&#26799;&#24230;&#29190;&#28856;&#12290;&#30740;&#31350;&#32473;&#20986;&#20102;&#19968;&#31181;&#20855;&#20307;&#26500;&#36896;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#22312;&#20219;&#20309;&#28145;&#24230;&#37117;&#33021;&#20445;&#25345;&#20248;&#21270;&#30340;&#20449;&#21495;&#20256;&#25773;&#29305;&#24615;&#65292;&#24182;&#20855;&#26377;&#26377;&#30028;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#23618;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#12290;&#19968;&#20123;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#25209;&#24402;&#19968;&#21270;&#25913;&#21892;&#20102;&#20449;&#21495;&#20256;&#25773;&#65292;&#36890;&#36807;&#36991;&#20813;&#34920;&#31034;&#22312;&#23618;&#20043;&#38388;&#21464;&#24471;&#20849;&#32447;&#12290;&#28982;&#32780;&#65292;&#25209;&#24402;&#19968;&#21270;&#30340;&#22343;&#22330;&#29702;&#35770;&#20063;&#24471;&#20986;&#32467;&#35770;&#65292;&#36825;&#31181;&#22909;&#22788;&#26159;&#20197;&#28145;&#24230;&#26799;&#24230;&#29190;&#28856;&#30340;&#20195;&#20215;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#25209;&#24402;&#19968;&#21270;&#30340;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#8220;&#25209;&#24402;&#19968;&#21270;&#32593;&#32476;&#33021;&#21542;&#20445;&#25345;&#26368;&#20248;&#30340;&#20449;&#21495;&#20256;&#25773;&#29305;&#24615;&#65292;&#20294;&#36991;&#20813;&#26799;&#24230;&#29190;&#28856;&#65311;&#8221;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#32473;&#20986;&#19968;&#31181;&#20855;&#20307;&#30340;&#26500;&#36896;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#32447;&#24615;&#28608;&#27963;&#21644;&#25209;&#24402;&#19968;&#21270;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#20219;&#20309;&#28145;&#24230;&#37117;&#20855;&#26377;&#26377;&#30028;&#26799;&#24230;&#12290;&#22522;&#20110;Weingarten&#24494;&#31215;&#20998;&#65292;&#25105;&#20204;&#20026;&#35813;&#26500;&#36896;&#30340;MLP&#24320;&#21457;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#38750;&#28176;&#36817;&#29702;&#35770;&#65292;&#32473;&#20986;&#20102;&#21069;&#21521;&#20449;&#21495;&#20256;&#25773;&#30340;&#31934;&#30830;&#29305;&#24449;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalization layers are one of the key building blocks for deep neural networks. Several theoretical studies have shown that batch normalization improves the signal propagation, by avoiding the representations from becoming collinear across the layers. However, results on mean-field theory of batch normalization also conclude that this benefit comes at the expense of exploding gradients in depth. Motivated by these two aspects of batch normalization, in this study we pose the following question: "Can a batch-normalized network keep the optimal signal propagation properties, but avoid exploding gradients?" We answer this question in the affirmative by giving a particular construction of an Multi-Layer Perceptron (MLP) with linear activations and batch-normalization that provably has bounded gradients at any depth. Based on Weingarten calculus, we develop a rigorous and non-asymptotic theory for this constructed MLP that gives a precise characterization of forward signal propagation, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Tsetlin&#26426;&#22120;&#22312;&#24191;&#20041;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#23398;&#20064;&#65288;PCL&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;TM&#32467;&#26500;&#21644;&#24341;&#20837;&#19987;&#29992;&#30340;&#21453;&#39304;&#26426;&#21046;&#21644;&#21253;&#21547;/&#25490;&#38500;&#27010;&#29575;&#22788;&#29702;&#21512;&#21462;&#24335;&#20013;&#30340;&#23383;&#38754;&#37327;&#65292;&#35777;&#26126;&#20102;PCL&#25910;&#25947;&#21040;&#19968;&#20010;&#21512;&#21462;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.02005</link><description>&lt;p&gt;
Tsetlin&#26426;&#22120;&#30340;&#24191;&#20041;&#25910;&#25947;&#20998;&#26512;&#65306;&#27010;&#29575;&#26041;&#27861;&#22312;&#27010;&#24565;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generalized Convergence Analysis of Tsetlin Machines: A Probabilistic Approach to Concept Learning. (arXiv:2310.02005v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Tsetlin&#26426;&#22120;&#22312;&#24191;&#20041;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#23398;&#20064;&#65288;PCL&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;TM&#32467;&#26500;&#21644;&#24341;&#20837;&#19987;&#29992;&#30340;&#21453;&#39304;&#26426;&#21046;&#21644;&#21253;&#21547;/&#25490;&#38500;&#27010;&#29575;&#22788;&#29702;&#21512;&#21462;&#24335;&#20013;&#30340;&#23383;&#38754;&#37327;&#65292;&#35777;&#26126;&#20102;PCL&#25910;&#25947;&#21040;&#19968;&#20010;&#21512;&#21462;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#22240;&#20854;&#36890;&#36807;&#21629;&#39064;&#20844;&#24335;&#23398;&#20064;&#27010;&#24565;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#34987;&#35777;&#26126;&#30340;&#39640;&#25928;&#24615;&#32780;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#29305;&#21035;&#26159;&#22312;&#24191;&#20041;&#24773;&#20917;&#65288;&#36755;&#20837;&#22823;&#20110;&#20004;&#20301;&#65289;&#19979;&#65292;&#23545;&#20110;Tsetlin&#26426;&#22120;&#30340;&#25910;&#25947;&#35777;&#26126;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;AND&#36816;&#31639;&#31526;&#65288;&#25991;&#29486;reasonable literals&#30340;&#24615;&#36136;&#65289;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;Tsetlin&#33258;&#21160;&#26426;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20840;&#38754;&#25910;&#25947;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#27010;&#29575;&#27010;&#24565;&#23398;&#20064;&#65288;PCL&#65289;&#65292;&#35813;&#26694;&#26550;&#31616;&#21270;&#20102;TM&#30340;&#32467;&#26500;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#19987;&#29992;&#30340;&#21453;&#39304;&#26426;&#21046;&#21644;&#19987;&#29992;&#30340;&#21253;&#21547;/&#25490;&#38500;&#27010;&#29575;&#26469;&#22788;&#29702;&#21512;&#21462;&#24335;&#20013;&#30340;&#23383;&#38754;&#37327;&#12290;&#32473;&#23450;n&#20010;&#29305;&#24449;&#65292;PCL&#26088;&#22312;&#23398;&#20064;&#19968;&#32452;&#19982;&#27599;&#20010;&#19981;&#21516;&#21253;&#21547;&#27010;&#29575;pi&#30456;&#20851;&#32852;&#30340;&#21512;&#21462;&#26465;&#27454;Ci&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#35777;&#26126;&#65292;&#35777;&#23454;&#20102;&#23545;&#20110;&#20219;&#20309;&#23376;&#21477;Ck&#65292;PCL&#37117;&#25910;&#25947;&#21040;&#19968;&#20010;&#21512;&#21462;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tsetlin Machines (TMs) have garnered increasing interest for their ability to learn concepts via propositional formulas and their proven efficiency across various application domains. Despite this, the convergence proof for the TMs, particularly for the AND operator (\emph{conjunction} of literals), in the generalized case (inputs greater than two bits) remains an open problem. This paper aims to fill this gap by presenting a comprehensive convergence analysis of Tsetlin automaton-based Machine Learning algorithms. We introduce a novel framework, referred to as Probabilistic Concept Learning (PCL), which simplifies the TM structure while incorporating dedicated feedback mechanisms and dedicated inclusion/exclusion probabilities for literals. Given $n$ features, PCL aims to learn a set of conjunction clauses $C_i$ each associated with a distinct inclusion probability $p_i$. Most importantly, we establish a theoretical proof confirming that, for any clause $C_k$, PCL converges to a conju
&lt;/p&gt;</description></item><item><title>L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.02003</link><description>&lt;p&gt;
L2MAC&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35745;&#31639;&#26426;&#29992;&#20110;&#26080;&#38480;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02003
&lt;/p&gt;
&lt;p&gt;
L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#24213;&#23618;Transformer&#26550;&#26500;&#22266;&#23450;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#22686;&#24378;&#35760;&#24518;&#30340;LLM&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#35835;&#21462;&#20869;&#23384;&#24182;&#23558;&#20854;&#28436;&#21464;&#20026;&#26032;&#20869;&#23384;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#24120;&#19987;&#38376;&#30340;&#20869;&#23384;&#65292;&#26080;&#27861;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;L2MAC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38271;&#19988;&#19968;&#33268;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#29992;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#12290;&#23427;&#30340;&#20869;&#23384;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#20196;&#27880;&#20876;&#34920;&#65292;&#20854;&#20013;&#22635;&#20805;&#20102;&#19968;&#20010;&#35299;&#20915;&#29992;&#25143;&#32473;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#31243;&#24207;&#65292;&#20197;&#21450;&#25991;&#20214;&#23384;&#20648;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#32456;&#21644;&#20013;&#38388;&#36755;&#20986;&#12290;&#27599;&#20010;&#25351;&#20196;&#30001;&#21333;&#29420;&#30340;LLM&#23454;&#20363;&#25191;&#34892;&#65292;&#20854;&#19978;&#19979;&#25991;&#30001;&#25511;&#21046;&#21333;&#20803;&#31649;&#29702;&#65292;&#33021;&#22815;&#31934;&#30830;&#35835;&#21462;&#21644;&#20889;&#20837;&#20869;&#23384;&#65292;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01991</link><description>&lt;p&gt;
&#22635;&#31354;&#39064;&#65306;&#25506;&#32034;&#24182;&#22686;&#24378;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems. (arXiv:2310.01991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#26399;&#30340;&#25991;&#29486;&#20013;&#24191;&#27867;&#25506;&#35752;&#20102;&#27491;&#21521;&#25512;&#29702;&#65288;&#21363;&#32473;&#23450;&#38382;&#39064;&#25214;&#31572;&#26696;&#65289;&#65292;&#20294;&#36870;&#21521;&#25512;&#29702;&#30456;&#23545;&#36739;&#23569;&#34987;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#32473;&#23450;&#19968;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#20854;&#31572;&#26696;&#65292;&#22312;&#38382;&#39064;&#20013;&#26377;&#20123;&#32454;&#33410;&#34987;&#30465;&#30053;&#20102;&#65292;LLM&#33021;&#21542;&#26377;&#25928;&#22320;&#36824;&#21407;&#20986;&#32570;&#22833;&#30340;&#20449;&#24687;&#65311;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#20462;&#25913;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#36825;&#19968;&#20219;&#21153;&#65306;GSM8k&#12289;SVAMP&#21644;MultiArith&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#27491;&#21521;&#25512;&#29702;&#30456;&#27604;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#27169;&#22411;&#65288;GPT4&#12289;GPT3.5&#12289;PaLM-2&#21644;LLaMa-2&#65289;&#22312;&#36870;&#21521;&#25512;&#29702;&#19978;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#21033;&#29992;&#35813;&#20219;&#21153;&#30340;&#29305;&#23450;&#26684;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#65306;Rephrase&#23558;&#32473;&#23450;&#30340;&#38382;&#39064;&#37325;&#36848;&#20026;&#19968;&#20010;&#27491;&#21521;&#25512;&#29702;&#38382;&#39064;&#65292;PAL-Tools&#32467;&#21512;&#20102;&#31243;&#24207;&#36741;&#21161;&#30340;LLM&#24605;&#24819;&#65292;&#29983;&#25104;&#19968;&#32452;&#26041;&#31243;&#24335;&#21487;&#20197;&#35299;&#20915;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information?  In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that ca
&lt;/p&gt;</description></item><item><title>Soda&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#21151;&#33021;&#24615;&#32534;&#31243;&#35821;&#35328;&#65292;&#29992;&#20110;&#25551;&#36848;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#22788;&#29702;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#23450;&#20041;&#27169;&#22411;&#21270;&#22797;&#26434;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.01961</link><description>&lt;p&gt;
Soda:&#19968;&#31181;&#29992;&#20110;&#25551;&#36848;&#20197;&#20154;&#20026;&#20013;&#24515;&#38382;&#39064;&#30340;&#38754;&#21521;&#23545;&#35937;&#30340;&#21151;&#33021;&#24615;&#32534;&#31243;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Soda: An Object-Oriented Functional Language for Specifying Human-Centered Problems. (arXiv:2310.01961v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01961
&lt;/p&gt;
&lt;p&gt;
Soda&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#21151;&#33021;&#24615;&#32534;&#31243;&#35821;&#35328;&#65292;&#29992;&#20110;&#25551;&#36848;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#22788;&#29702;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#23450;&#20041;&#27169;&#22411;&#21270;&#22797;&#26434;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Soda&#65288;Symbolic Objective Descriptive Analysis&#65289;&#65292;&#19968;&#31181;&#35821;&#35328;&#65292;&#26377;&#21161;&#20110;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#22788;&#29702;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#24182;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#26816;&#26597;&#23427;&#20204;&#27491;&#30830;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#35821;&#35328;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#26159;&#30001;&#23545;&#35745;&#31639;&#26426;&#31995;&#32479;&#22797;&#26434;&#35201;&#27714;&#36827;&#34892;&#25551;&#36848;&#30340;&#35774;&#35745;&#25152;&#28608;&#21457;&#30340;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#23450;&#20041;&#26469;&#24314;&#27169;&#36825;&#20123;&#35201;&#27714;&#26102;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#23646;&#24615;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#23427;&#26377;&#21161;&#20110;&#20197;&#26356;&#36879;&#26126;&#21644;&#26356;&#23569;&#20986;&#38169;&#30340;&#26041;&#24335;&#25551;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Soda (Symbolic Objective Descriptive Analysis), a language that helps to treat qualities and quantities in a natural way and greatly simplifies the task of checking their correctness. We present key properties for the language motivated by the design of a descriptive language to encode complex requirements on computer systems, and we explain how these key properties must be addressed to model these requirements with simple definitions. We give an overview of a tool that helps to describe problems in an easy way that we consider more transparent and less error-prone.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#20219;&#21153;&#20013;&#20351;&#29992;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26816;&#32034;&#23384;&#20648;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01960</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Language Models as Knowledge Bases for Visual Word Sense Disambiguation. (arXiv:2310.01960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#20219;&#21153;&#20013;&#20351;&#29992;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26816;&#32034;&#23384;&#20648;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#65288;VWSD&#65289;&#26159;&#19968;&#39033;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20301;&#20110;&#35821;&#20041;&#28040;&#27495;&#21644;&#32454;&#31890;&#24230;&#22810;&#27169;&#24335;&#26816;&#32034;&#20043;&#38388;&#12290;&#26368;&#36817;&#21457;&#23637;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VL transformers&#65289;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#19968;&#20123;&#29616;&#25104;&#30340;&#23454;&#29616;&#20855;&#26377;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#22686;&#24378;&#30693;&#35782;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#30693;&#35782;&#24211;&#65292;&#26469;&#25552;&#39640;VL transformers&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#26816;&#32034;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#20505;&#36873;&#31572;&#26696;&#65292;&#23558;VWSD&#36716;&#21270;&#20026;&#32431;&#25991;&#26412;&#38382;&#31572;&#65288;QA&#65289;&#38382;&#39064;&#12290;&#21033;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#26469;&#25506;&#32034;&#36825;&#31181;&#36716;&#25442;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#20511;&#21161;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies between linguistic sense disambiguation and fine-grained multimodal retrieval. The recent advancements in the development of visiolinguistic (VL) transformers suggest some off-the-self implementations with encouraging results, which however we argue that can be further improved. To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prom
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20013;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01957</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36827;&#34892;&#39550;&#39542;&#65306;&#34701;&#21512;&#23545;&#35937;&#32423;&#21521;&#37327;&#27169;&#24577;&#20197;&#35299;&#37322;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving. (arXiv:2310.01957v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20013;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#65292;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#39550;&#39542;&#24773;&#22659;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;10k&#20010;&#39550;&#39542;&#24773;&#22659;&#30340;160k&#20010;&#38382;&#31572;&#23545;&#65292;&#36825;&#20123;&#38382;&#31572;&#23545;&#19982;&#30001;RL&#20195;&#29702;&#25910;&#38598;&#30340;&#39640;&#36136;&#37327;&#25511;&#21046;&#21629;&#20196;&#21644;&#30001;&#25945;&#24072;LLM&#65288;GPT-3.5&#65289;&#29983;&#25104;&#30340;&#38382;&#39064;&#31572;&#26696;&#23545;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;&#21521;&#37327;&#23383;&#24149;&#35821;&#35328;&#25968;&#25454;&#26469;&#23545;&#40784;&#25968;&#23383;&#21521;&#37327;&#27169;&#24577;&#21644;&#38745;&#24577;LLM&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39550;&#39542;&#38382;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#22522;&#20110;LLM&#30340;&#39550;&#39542;&#34892;&#20026;&#29983;&#25104;&#19982;&#20256;&#32479;&#34892;&#20026;&#20811;&#38534;&#30456;&#27604;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#21040;&#36798;-&#36991;&#20813;&#27010;&#29575;&#21644;&#21512;&#25104;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21033;&#29992;&#21306;&#38388;&#20256;&#25773;&#21644;&#21521;&#21518;&#36882;&#24402;&#25216;&#26415;&#65292;&#35745;&#31639;&#20986;&#20102;&#27010;&#29575;&#30340;&#19979;&#30028;&#20316;&#20026;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.01951</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#24615;&#21040;&#36798;-&#36991;&#20813;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Reach-Avoid for Bayesian Neural Networks. (arXiv:2310.01951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#21040;&#36798;-&#36991;&#20813;&#27010;&#29575;&#21644;&#21512;&#25104;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21033;&#29992;&#21306;&#38388;&#20256;&#25773;&#21644;&#21521;&#21518;&#36882;&#24402;&#25216;&#26415;&#65292;&#35745;&#31639;&#20986;&#20102;&#27010;&#29575;&#30340;&#19979;&#30028;&#20316;&#20026;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#21516;&#26102;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#24182;&#32508;&#21512;&#20986;&#36866;&#29992;&#20110;&#20854;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#30830;&#20445;&#36890;&#36807;&#31574;&#30053;&#20316;&#20986;&#30340;&#24207;&#21015;&#20915;&#31574;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#26159;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#38382;&#39064;&#65306;&#31532;&#19968;&#65292;&#23545;&#30001;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#25551;&#36848;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#39044;&#27979;&#30340;&#21040;&#36798;-&#36991;&#20813;&#27010;&#29575;&#30340;&#35745;&#31639;&#65307;&#31532;&#20108;&#65292;&#21512;&#25104;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20197;&#28385;&#36275;&#32473;&#23450;&#30340;&#21040;&#36798;-&#36991;&#20813;&#35268;&#33539;&#65288;&#36798;&#21040;&#8220;&#30446;&#26631;&#8221;&#29366;&#24577;&#65292;&#21516;&#26102;&#36991;&#20813;&#19968;&#32452;&#8220;&#19981;&#23433;&#20840;&#8221;&#29366;&#24577;&#65289;&#21644;&#23398;&#20064;&#30340;BNN&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#21306;&#38388;&#20256;&#25773;&#21644;&#21521;&#21518;&#36882;&#24402;&#25216;&#26415;&#26469;&#35745;&#31639;&#31574;&#30053;&#21160;&#20316;&#24207;&#21015;&#28385;&#36275;&#21040;&#36798;-&#36991;&#20813;&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#19979;&#30028;&#12290;&#36825;&#26679;&#35745;&#31639;&#20986;&#30340;&#19979;&#30028;&#25552;&#20379;&#20102;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning seeks to simultaneously learn the dynamics of an unknown stochastic environment and synthesise an optimal policy for acting in it. Ensuring the safety and robustness of sequential decisions made through a policy in such an environment is a key challenge for policies intended for safety-critical scenarios. In this work, we investigate two complementary problems: first, computing reach-avoid probabilities for iterative predictions made with dynamical models, with dynamics described by Bayesian neural network (BNN); second, synthesising control policies that are optimal with respect to a given reach-avoid specification (reaching a "target" state, while avoiding a set of "unsafe" states) and a learned BNN model. Our solution leverages interval propagation and backward recursion techniques to compute lower bounds for the probability that a policy's sequence of actions leads to satisfying the reach-avoid specification. Such computed lower bounds provide saf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#29305;&#23450;&#24615;&#30340;&#20132;&#20114;&#31574;&#30053;&#30340;&#20998;&#24067;&#24335;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#36125;&#21494;&#26031;&#27010;&#24565;&#21644;&#24320;&#28304;&#23454;&#29616;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01943</link><description>&lt;p&gt;
Ravestate: &#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#29305;&#23450;&#24615;&#30340;&#20132;&#20114;&#31574;&#30053;&#30340;&#20998;&#24067;&#24335;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Ravestate: Distributed Composition of a Causal-Specificity-Guided Interaction Policy. (arXiv:2310.01943v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#29305;&#23450;&#24615;&#30340;&#20132;&#20114;&#31574;&#30053;&#30340;&#20998;&#24067;&#24335;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#36125;&#21494;&#26031;&#27010;&#24565;&#21644;&#24320;&#28304;&#23454;&#29616;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#20132;&#20114;&#31574;&#30053;&#35774;&#35745;&#20013;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#12289;&#34920;&#36798;&#21147;&#24378;&#21644;&#30452;&#35266;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Signal-Rule-Slot&#26694;&#26550;&#65292;&#23545;&#22522;&#20110;&#35268;&#21017;&#30340;&#31526;&#21495;&#31995;&#32479;&#35774;&#35745;&#30340;&#20808;&#21069;&#24037;&#20316;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#27010;&#24565;&#8212;&#8212;&#22240;&#26524;&#36335;&#24452;&#33258;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#35880;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#19968;&#20010;&#20016;&#23500;&#30340;&#24320;&#28304;&#21442;&#32771;&#23454;&#29616;Ravestate&#65292;&#36890;&#36807;&#22312;&#22522;&#20110;&#25991;&#26412;&#12289;&#35821;&#38899;&#21644;&#35270;&#35273;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27010;&#29575;&#35268;&#21017;&#31995;&#32479;&#30340;&#24378;&#22823;&#19978;&#19979;&#25991;&#34892;&#20026;&#12290;&#36825;&#20026;&#26356;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In human-robot interaction policy design, a rule-based method is efficient, explainable, expressive and intuitive. In this paper, we present the Signal-Rule-Slot framework, which refines prior work on rule-based symbol system design and introduces a new, Bayesian notion of interaction rule utility called Causal Pathway Self-information. We offer a rigorous theoretical foundation as well as a rich open-source reference implementation Ravestate, with which we conduct user studies in text-, speech-, and vision-based scenarios. The experiments show robust contextual behaviour of our probabilistically informed rule-based system, paving the way for more effective human-machine interaction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01929</link><description>&lt;p&gt;
&#31359;&#36234;&#25991;&#21270;&#40511;&#27807;&#65306;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E&#21644;StableDiffusion&#65292;&#22312;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#38646;&#23556;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#20316;&#20026;&#25991;&#21270;&#30340;&#23186;&#20171;&#65292;&#35821;&#35328;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32780;&#22609;&#36896;&#20102;&#23427;&#20204;&#30340;&#25991;&#21270;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25551;&#36848;&#25991;&#21270;&#32500;&#24230;&#65292;&#25991;&#21270;&#39046;&#22495;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#19977;&#20010;&#23618;&#27425;&#26469;&#25506;&#32034;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25216;&#26415;&#65292;&#21253;&#25324;&#20351;&#29992;CLIP&#31354;&#38388;&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36827;&#34892;&#22806;&#22312;&#35780;&#20272;&#20197;&#21450;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;TTI&#25991;&#21270;&#24863;&#30693;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CulText2I&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30340;TTI&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;
&lt;/p&gt;
&lt;p&gt;
Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
&lt;/p&gt;</description></item><item><title>DARTH&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#22806;&#35266;&#34920;&#31034;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22495;&#28418;&#31227;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.01926</link><description>&lt;p&gt;
DARTH: &#20840;&#38754;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DARTH: Holistic Test-time Adaptation for Multiple Object Tracking. (arXiv:2310.01926v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01926
&lt;/p&gt;
&lt;p&gt;
DARTH&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#22806;&#35266;&#34920;&#31034;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22495;&#28418;&#31227;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36319;&#36394;(MOT)&#26159;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#31995;&#32479;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#23545;&#26410;&#30693;&#26465;&#20214;&#30340;&#31283;&#20581;&#24615;&#26159;&#36991;&#20813;&#29983;&#21629;&#21361;&#38505;&#25925;&#38556;&#30340;&#35201;&#27714;&#12290;&#23613;&#31649;&#22312;&#39550;&#39542;&#31995;&#32479;&#20013;&#23433;&#20840;&#24615;&#36843;&#20999;&#65292;&#20294;&#20174;&#26410;&#25552;&#20986;&#36807;&#35299;&#20915;&#22810;&#30446;&#26631;&#36319;&#36394;&#36866;&#24212;&#38382;&#39064;&#21040;&#27979;&#35797;&#26102;&#38388;&#26465;&#20214;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;MOT&#31995;&#32479;&#30340;&#26412;&#36136;&#26159;&#22810;&#26679;&#30340;&#65292;&#38656;&#35201;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#20851;&#32852;&#65292;&#36866;&#24212;&#20854;&#25152;&#26377;&#32452;&#20214;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22806;&#35266;&#36319;&#36394;&#22120;&#22312;&#22495;&#28418;&#31227;&#19978;&#30340;&#24433;&#21709;&#65292;&#24182;&#20171;&#32461;&#20102;DARTH&#65292;&#19968;&#31181;&#20840;&#38754;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#19968;&#33268;&#24615;&#20844;&#24335;&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#21516;&#26102;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#22411;&#34917;&#19969;&#23545;&#27604;&#25439;&#22833;&#26469;&#36866;&#24212;&#23454;&#20363;&#22806;&#35266;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22495;&#28418;&#31227;&#19979;&#36827;&#34892;&#20102;&#26041;&#27861;&#35780;&#20272;&#65292;&#21253;&#25324;&#27169;&#25311;&#21040;&#30495;&#23454;&#65292;&#25143;&#22806;&#21040;&#23460;&#20869;&#65292;&#23460;&#20869;&#21040;&#25143;&#22806;&#65292;&#24182;&#22823;&#24133;&#25913;&#36827;&#20102;&#21407;&#22987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple object tracking (MOT) is a fundamental component of perception systems for autonomous driving, and its robustness to unseen conditions is a requirement to avoid life-critical failures. Despite the urge of safety in driving systems, no solution to the MOT adaptation problem to domain shift in test-time conditions has ever been proposed. However, the nature of a MOT system is manifold - requiring object detection and instance association - and adapting all its components is non-trivial. In this paper, we analyze the effect of domain shift on appearance-based trackers, and introduce DARTH, a holistic test-time adaptation framework for MOT. We propose a detection consistency formulation to adapt object detection in a self-supervised fashion, while adapting the instance appearance representations via our novel patch contrastive loss. We evaluate our method on a variety of domain shifts including sim-to-real, outdoor-to-indoor, indoor-to-outdoor - and substantially improve the sou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#35299;&#20915;&#39640;&#32500;&#34920;&#31034;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01892</link><description>&lt;p&gt;
FiGURe: &#20351;&#29992;&#36807;&#28388;&#22120;&#22686;&#24378;&#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations. (arXiv:2310.01892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#35299;&#20915;&#39640;&#32500;&#34920;&#31034;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#27169;&#25311;&#20302;&#36890;&#28388;&#27874;&#22120;&#30340;&#22686;&#24378;&#65292;&#38480;&#21046;&#20102;&#22312;&#38656;&#35201;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#30340;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#36807;&#28388;&#22120;&#30340;&#22686;&#24378;&#26041;&#27861;&#26469;&#25429;&#25417;&#29305;&#24449;&#39057;&#35889;&#30340;&#19981;&#21516;&#37096;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#20043;&#38388;&#20849;&#20139;&#30456;&#21516;&#26435;&#37325;&#26159;&#21487;&#33021;&#30340;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#38656;&#35201;&#39640;&#32500;&#34920;&#31034;&#12290;&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#25968;&#25454;&#26102;&#65292;&#29305;&#21035;&#26159;&#24403;&#28041;&#21450;&#22810;&#20010;&#22686;&#24378;&#26041;&#27861;&#26102;&#65292;&#22686;&#21152;&#20102;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#24182;&#24674;&#22797;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861; FiGURe &#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Unsupervised node representations learnt using contrastive learning-based methods have shown good performance on downstream tasks. However, these methods rely on augmentations that mimic low-pass filters, limiting their performance on tasks requiring different eigen-spectrum parts. This paper presents a simple filter-based augmentation method to capture different parts of the eigen-spectrum. We show significant improvements using these augmentations. Further, we show that sharing the same weights across these different filter augmentations is possible, reducing the computational load. In addition, previous works have shown that good performance on downstream tasks requires high dimensional representations. Working with high dimensions increases the computations, especially when multiple augmentations are involved. We mitigate this problem and recover good performance through lower dimensional embeddings using simple random Fourier feature projections. Our method, FiGURe achieves an ave
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;VMD&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#22534;&#21472;Informer&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#65292;&#23545;&#20110;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#39044;&#27979;&#24314;&#27169;&#26377;&#28508;&#22312;&#30340;&#20248;&#21270;&#26041;&#21521;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.01884</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#30340;&#25913;&#36827;VMD&#21644;&#22534;&#21472;Informer&#22312;&#22686;&#24378;&#32929;&#24066;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hybrid Model for Enhanced Stock Market Predictions Using Improved VMD and Stacked Informer. (arXiv:2310.01884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;VMD&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#22534;&#21472;Informer&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#65292;&#23545;&#20110;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#39044;&#27979;&#24314;&#27169;&#26377;&#28508;&#22312;&#30340;&#20248;&#21270;&#26041;&#21521;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#21464;&#20998;&#27169;&#24577;&#20998;&#35299;&#65288;VMD&#65289;&#12289;&#29305;&#24449;&#24037;&#31243;&#65288;FE&#65289;&#21644;&#22534;&#21472;Informer&#65292;&#24182;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;VMGCformer&#65288;Adam+GC+enhanced informer&#65289;&#65292;&#22312;&#22788;&#29702;&#32929;&#24066;&#25968;&#25454;&#30340;&#22797;&#26434;&#21160;&#24577;&#21644;&#27874;&#21160;&#24615;&#26041;&#38754;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24230;&#12290;&#22522;&#20110;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#24471;&#20986;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20986;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30456;&#23545;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#20248;&#21270;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#24182;&#20171;&#32461;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#27979;&#24314;&#27169;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an innovative adaptive hybrid model for stock market predictions, leveraging the capabilities of an enhanced Variational Mode Decomposition (VMD), Feature Engineering (FE), and stacked Informer integrated with an adaptive loss function. Through rigorous experimentation, the proposed model, termed Adam+GC+enhanced informer (We name it VMGCformer), demonstrates significant proficiency in addressing the intricate dynamics and volatile nature of stock market data. Experimental results, derived from multiple benchmark datasets, underscore the model's superiority in terms of prediction accuracy, responsiveness, and generalization capabilities over traditional and other hybrid models. The research further highlights potential avenues for optimization and introduces future directions to enhance predictive modeling, especially for small enterprises and feature engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01875</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#28418;&#31227;&#35843;&#25972;&#23454;&#29616;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Backdoor Purification through Feature Shift Tuning. (arXiv:2310.01875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#19968;&#23567;&#32452;&#35757;&#32451;&#26679;&#26412;&#26469;&#24694;&#24847;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38450;&#24481;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#23041;&#32961;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#22797;&#26434;&#20462;&#25913;&#65292;&#35201;&#20040;&#20005;&#37325;&#20381;&#36182;&#29305;&#23450;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24494;&#35843;&#24320;&#22987;&#65292;&#36890;&#36807;&#23545;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#30340;&#20840;&#38754;&#35780;&#20272;&#26469;&#25506;&#32034;&#26368;&#24120;&#35265;&#21644;&#26131;&#20110;&#37096;&#32626;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#36807;&#21021;&#27493;&#23454;&#39564;&#35266;&#23519;&#21457;&#29616;&#65292;&#19982;&#39640;&#27745;&#26579;&#29575;&#30340;&#26377;&#24076;&#26395;&#30340;&#38450;&#24481;&#32467;&#26524;&#30456;&#27604;&#65292;&#26222;&#36890;&#30340;&#35843;&#25972;&#26041;&#27861;&#22312;&#20302;&#27745;&#26579;&#29575;&#22330;&#26223;&#19979;&#23436;&#20840;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#30772;&#22351;&#20102;&#22522;&#20110;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#21644;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#30001;&#26410;&#35266;&#27979;&#28508;&#21464;&#37327;&#24341;&#36215;&#30340;&#28151;&#28102;&#20559;&#24046;&#65292;&#24182;&#22312;&#26080;&#38656;&#32447;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#24179;&#34913;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.01865</link><description>&lt;p&gt;
&#24102;&#26377;&#34920;&#31034;&#23398;&#20064;&#30340;&#26465;&#20214;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Conditional Instrumental Variable Regression with Representation Learning for Causal Inference. (arXiv:2310.01865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#21644;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#30001;&#26410;&#35266;&#27979;&#28508;&#21464;&#37327;&#24341;&#36215;&#30340;&#28151;&#28102;&#20559;&#24046;&#65292;&#24182;&#22312;&#26080;&#38656;&#32447;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#24179;&#34913;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#23384;&#22312;&#26410;&#35266;&#27979;&#28508;&#21464;&#37327;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#20004;&#38454;&#27573;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;TSLS&#65289;&#26041;&#27861;&#21450;&#20854;&#20855;&#26377;&#26631;&#20934;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#30340;&#21464;&#31181;&#24120;&#29992;&#20110;&#28040;&#38500;&#28151;&#28102;&#20559;&#24046;&#65292;&#21253;&#25324;&#30001;&#26410;&#35266;&#27979;&#28508;&#21464;&#37327;&#24341;&#36215;&#30340;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#32447;&#24615;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26631;&#20934;IV&#26041;&#27861;&#25552;&#20986;&#30340;&#26080;&#28151;&#28102;&#24037;&#20855;&#30340;&#20005;&#26684;&#26465;&#20214;&#23545;&#20110;&#23454;&#36341;&#26469;&#35828;&#22826;&#24378;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#20934;IV&#26041;&#27861;&#65288;&#32447;&#24615;&#20551;&#35774;&#21644;&#20005;&#26684;&#26465;&#20214;&#65289;&#30340;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23454;&#38469;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;IV&#65288;CIV&#65289;&#25918;&#26494;&#26631;&#20934;IV&#30340;&#26080;&#28151;&#28102;&#24037;&#20855;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;CIV&#22238;&#24402;&#19982;&#28151;&#28102;&#22343;&#34913;&#34920;&#31034;&#23398;&#20064;&#65288;CBRL.CIV&#65289;&#65292;&#29992;&#20110;&#21516;&#26102;&#28040;&#38500;&#30001;&#26410;&#35266;&#27979;&#28508;&#21464;&#37327;&#24341;&#36215;&#30340;&#28151;&#28102;&#20559;&#24046;&#21644;&#24179;&#34913;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the challenging problem of estimating causal effects from observational data, in the presence of unobserved confounders. The two-stage least square (TSLS) method and its variants with a standard instrumental variable (IV) are commonly used to eliminate confounding bias, including the bias caused by unobserved confounders, but they rely on the linearity assumption. Besides, the strict condition of unconfounded instruments posed on a standard IV is too strong to be practical. To address these challenging and practical problems of the standard IV method (linearity assumption and the strict condition), in this paper, we use a conditional IV (CIV) to relax the unconfounded instrument condition of standard IV and propose a non-linear CIV regression with Confounding Balancing Representation Learning, CBRL.CIV, for jointly eliminating the confounding bias from unobserved confounders and balancing the observed confounders, without the linearity assumption. We theoretically de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#31934;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#34920;&#31034;&#22312;&#31070;&#32463;&#35299;&#30721;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;10&#20010;NLU&#20219;&#21153;&#20013;&#65292;&#31934;&#35843;&#34920;&#31034;&#19981;&#33021;&#26356;&#22909;&#22320;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.01854</link><description>&lt;p&gt;
&#31934;&#35843;&#19982;&#25552;&#31034;&#35843;&#25972;&#30340;&#30417;&#30563;&#34920;&#31034;&#65306;&#21738;&#31181;&#26356;&#33021;&#35299;&#37322;&#22823;&#33041;&#20013;&#30340;&#35821;&#35328;&#34920;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?. (arXiv:2310.01854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#31934;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#34920;&#31034;&#22312;&#31070;&#32463;&#35299;&#30721;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;10&#20010;NLU&#20219;&#21153;&#20013;&#65292;&#31934;&#35843;&#34920;&#31034;&#19981;&#33021;&#26356;&#22909;&#22320;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#35835;&#20154;&#33041;&#35821;&#35328;&#34920;&#31034;&#32972;&#21518;&#30340;&#31639;&#27861;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#22312;NLU&#20219;&#21153;&#19978;&#36827;&#34892;&#36807;&#31934;&#35843;&#30340;&#39044;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#26469;&#25506;&#27979;&#22823;&#33041;&#23545;&#35821;&#35328;&#36755;&#20837;&#30340;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#23436;&#20840;&#30340;&#31934;&#35843;&#20250;&#26356;&#26032;&#25972;&#20010;&#21442;&#25968;&#31354;&#38388;&#24182;&#25197;&#26354;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#65292;&#19982;&#22823;&#33041;&#30340;&#24378;&#20581;&#22810;&#20219;&#21153;&#23398;&#20064;&#33021;&#21147;&#19981;&#19968;&#33268;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25552;&#31034;&#35843;&#25972;&#20445;&#25252;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#23884;&#20837;&#20197;&#36866;&#24212;&#20219;&#21153;&#12290;&#25552;&#31034;&#35843;&#25972;&#26159;&#21542;&#33021;&#29983;&#25104;&#26356;&#33021;&#35299;&#37322;&#22823;&#33041;&#35821;&#35328;&#34920;&#31034;&#30340;&#34920;&#31034;&#65311;&#22914;&#26524;&#26159;&#65292;&#21738;&#31181;NLU&#20219;&#21153;&#33021;&#35753;&#39044;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#20449;&#24687;&#34920;&#31034;&#65311;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#25552;&#31034;&#35843;&#25972;&#21644;&#31934;&#35843;&#30340;&#34920;&#31034;&#22312;&#31070;&#32463;&#35299;&#30721;&#20013;&#36827;&#34892;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#21363;&#20174;&#21050;&#28608;&#24341;&#21457;&#30340;&#22823;&#33041;&#27963;&#21160;&#20013;&#39044;&#27979;&#35821;&#35328;&#21050;&#28608;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;10&#20010;NLU&#20219;&#21153;&#20013;&#65292;&#20840;&#29699;&#31934;&#35843;&#34920;&#31034;&#37117;&#19981;&#33021;&#26356;&#22909;&#22320;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
To decipher the algorithm underlying the human brain's language representation, previous work probed brain responses to language input with pre-trained artificial neural network (ANN) models fine-tuned on NLU tasks. However, full fine-tuning generally updates the entire parametric space and distorts pre-trained features, cognitively inconsistent with the brain's robust multi-task learning ability. Prompt-tuning, in contrast, protects pre-trained weights and learns task-specific embeddings to fit a task. Could prompt-tuning generate representations that better account for the brain's language representations than fine-tuning? If so, what kind of NLU task leads a pre-trained model to better decode the information represented in the human brain? We investigate these questions by comparing prompt-tuned and fine-tuned representations in neural decoding, that is predicting the linguistic stimulus from the brain activities evoked by the stimulus. We find that on none of the 10 NLU tasks, full
&lt;/p&gt;</description></item><item><title>LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01852</link><description>&lt;p&gt;
LanguageBind:&#36890;&#36807;&#22522;&#20110;&#35821;&#20041;&#23545;&#40784;&#30340;&#35821;&#35328;&#23558;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#25193;&#23637;&#21040;N&#27169;&#24577;&#65288;arXiv:2310.01852v1[cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01852
&lt;/p&gt;
&lt;p&gt;
LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;-&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VL&#39044;&#35757;&#32451;&#26694;&#26550;&#38590;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#38500;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#22806;&#30340;&#22810;&#27169;&#24577;&#65288;N&#27169;&#24577;&#65292;N&gt;=3&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LanguageBind&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#32445;&#24102;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#24577;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#32034;&#65292;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;VL&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#12290;&#32467;&#26524;&#26159;&#65292;&#25152;&#26377;&#27169;&#24577;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#34429;&#28982;LanguageBind&#21487;&#20197;&#25193;&#23637;VL&#27169;&#24577;&#21040;N&#27169;&#24577;&#65292;&#20294;&#25105;&#20204;&#36824;&#38656;&#35201;&#19968;&#20010;&#24102;&#26377;&#20197;&#35821;&#35328;&#20026;&#20013;&#24515;&#30340;&#23545;&#40784;&#25968;&#25454;&#23545;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VIDAL-10M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35270;&#39057;&#12289;&#32418;&#22806;&#12289;&#28145;&#24230;&#12289;&#38899;&#39057;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#35328;&#25968;&#25454;&#65292;&#21629;&#21517;&#20026;VIDAL-10M&#12290;
&lt;/p&gt;
&lt;p&gt;
The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N&gt;=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01845</link><description>&lt;p&gt;
&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#22312;&#24120;&#35268;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#36965;&#24863;&#22270;&#20687;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#31934;&#30830;&#30340;&#24314;&#31569;&#29289;&#23454;&#20363;&#20998;&#21106;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#21463;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#24050;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#20247;&#22810;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;Segment Anything Model&#65288;SAM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#20854;&#25797;&#38271;&#26080;&#31867;&#21035;&#22270;&#20687;&#20998;&#21106;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;SAM&#30340;&#23616;&#38480;&#24615;&#65292;&#25581;&#31034;&#20102;&#23427;&#22312;&#24212;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;SAM&#19981;&#20855;&#22791;&#35782;&#21035;&#33021;&#21147;&#65292;&#22240;&#27492;&#26080;&#27861;&#23545;&#23450;&#20301;&#30340;&#23545;&#35937;&#36827;&#34892;&#20998;&#31867;&#21644;&#26631;&#35760;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different promp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2310.01837</link><description>&lt;p&gt;
&#25193;&#23637;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#26080;&#27861;&#23545;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#12289;&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;/&#25512;&#29702;&#25805;&#20316;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21482;&#33021;&#34987;&#35270;&#20026;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#19987;&#23478;&#38656;&#35201;&#24110;&#21161;&#29702;&#35299;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#34892;&#20026;&#21644;&#22522;&#30784;&#20915;&#31574;&#36807;&#31243;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#30830;&#20445;AI&#27169;&#22411;&#31283;&#20581;&#12289;&#23454;&#29992;&#21644;&#21487;&#20449;&#36182;&#37096;&#32626;&#30340;&#25163;&#27573;&#12290;&#24050;&#26377;&#19968;&#20123;XAI&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#35299;&#37322;&#21017;&#22522;&#26412;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;&#26368;&#26032;&#30340;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#20351;&#20854;&#36866;&#29992;&#20110;&#22810;&#31867;&#22270;&#20687;&#20998;&#21106;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20854;&#20013;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#24314;&#31569;&#29289;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current AI-based methods do not provide comprehensible physical interpretations of the utilized data, extracted features, and predictions/inference operations. As a result, deep learning models trained using high-resolution satellite imagery lack transparency and explainability and can be merely seen as a black box, which limits their wide-level adoption. Experts need help understanding the complex behavior of AI models and the underlying decision-making process. The explainable artificial intelligence (XAI) field is an emerging field providing means for robust, practical, and trustworthy deployment of AI models. Several XAI techniques have been proposed for image classification tasks, whereas the interpretation of image segmentation remains largely unexplored. This paper offers to bridge this gap by adapting the recent XAI classification algorithms and making them usable for muti-class image segmentation, where we mainly focus on buildings' segmentation from high-resolution satellite 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38750;&#27491;&#24335;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#21487;&#26816;&#26597;&#30340;&#31243;&#24207;&#35268;&#33539;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#25552;&#39640;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#35843;&#35797;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.01831</link><description>&lt;p&gt;
&#23558;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#24418;&#24335;&#21270;&#20026;&#31243;&#24207;&#35268;&#33539;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Formalizing Natural Language Intent into Program Specifications via Large Language Models. (arXiv:2310.01831v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38750;&#27491;&#24335;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#21487;&#26816;&#26597;&#30340;&#31243;&#24207;&#35268;&#33539;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#25552;&#39640;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#35843;&#35797;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#20195;&#30721;&#21151;&#33021;&#30340;&#38750;&#27491;&#24335;&#33258;&#28982;&#35821;&#35328;&#65292;&#20363;&#22914;&#20195;&#30721;&#27880;&#37322;&#25110;&#20989;&#25968;&#25991;&#26723;&#65292;&#21487;&#33021;&#21253;&#21547;&#20851;&#20110;&#31243;&#24207;&#24847;&#22270;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#31243;&#24207;&#30340;&#23454;&#29616;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26723;&#36890;&#24120;&#26080;&#27861;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#20449;&#24687;&#26377;&#28508;&#21147;&#25552;&#39640;&#25925;&#38556;&#23450;&#20301;&#12289;&#35843;&#35797;&#21644;&#20195;&#30721;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22266;&#26377;&#27495;&#20041;&#24615;&#65292;&#36825;&#20123;&#20449;&#24687;&#36890;&#24120;&#34987;&#20302;&#25928;&#21033;&#29992;&#65292;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#38590;&#20197;&#22312;&#31243;&#24207;&#20013;&#36827;&#34892;&#31243;&#24207;&#21270;&#26816;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#26032;&#20852;&#33021;&#21147;&#8221;&#26377;&#21487;&#33021;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#31243;&#24207;&#21487;&#26816;&#26597;&#30340;&#26029;&#35328;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;LLM&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#22320;&#23558;&#38750;&#27491;&#24335;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#36716;&#21270;&#20026;&#19982;&#31243;&#24207;&#21592;&#24847;&#22270;&#30456;&#21305;&#37197;&#30340;&#27491;&#24335;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a programs intent. However, there is typically no guarantee that a programs implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language which makes natural language intent challenging to check programmatically. The "emergent abilities" of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is uncl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2310.01828</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65306;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20851;&#38190;&#20219;&#21153;&#24212;&#29992;&#26102;&#30340;&#24517;&#22791;&#35201;&#27714;&#65292;&#30830;&#20445;&#25152;&#20351;&#29992;&#30340;&#40657;&#30418;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;XAI&#30340;&#37325;&#35201;&#24615;&#28085;&#30422;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#24448;&#24448;&#26159;&#40657;&#30418;&#23376;&#65292;&#22240;&#27492;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#23427;&#20204;&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#26512;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#36965;&#24863;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;XAI&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#65292;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;XAI&#31639;&#27861;&#26469;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) has emerged as an essential requirement when dealing with mission-critical applications, ensuring transparency and interpretability of the employed black box AI models. The significance of XAI spans various domains, from healthcare to finance, where understanding the decision-making process of deep learning algorithms is essential. Most AI-based computer vision models are often black boxes; hence, providing explainability of deep neural networks in image processing is crucial for their wide adoption and deployment in medical image analysis, autonomous driving, and remote sensing applications. Recently, several XAI methods for image classification tasks have been introduced. On the contrary, image segmentation has received comparatively less attention in the context of explainability, although it is a fundamental task in computer vision applications, especially in remote sensing. Only some research proposes gradient-based XAI algorithms for imag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20043;&#21069;&#23398;&#20064;&#30340;&#21407;&#22987;&#34892;&#20026;&#26469;&#24341;&#23548;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22238;&#39038;&#24335;&#32463;&#39564;&#37325;&#29616;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.01827</link><description>&lt;p&gt;
&#23398;&#20064;&#21644;&#37325;&#22797;&#20351;&#29992;&#21407;&#22987;&#34892;&#20026;&#20197;&#25552;&#39640;&#22238;&#39038;&#24335;&#32463;&#39564;&#37325;&#29616;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Learning and reusing primitive behaviours to improve Hindsight Experience Replay sample efficiency. (arXiv:2310.01827v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20043;&#21069;&#23398;&#20064;&#30340;&#21407;&#22987;&#34892;&#20026;&#26469;&#24341;&#23548;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22238;&#39038;&#24335;&#32463;&#39564;&#37325;&#29616;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#39038;&#24335;&#32463;&#39564;&#37325;&#29616;&#65288;HER&#65289;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#35757;&#32451;&#22522;&#20110;&#31163;&#32447;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20197;&#35299;&#20915;&#22522;&#20110;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20855;&#26377;&#38750;&#24120;&#39640;&#25928;&#30340;&#25928;&#26524;&#65292;&#20294;&#23613;&#31649;HER&#36890;&#36807;&#23398;&#20064;&#20197;&#24448;&#32463;&#39564;&#20013;&#30340;&#38169;&#35823;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23427;&#22312;&#25506;&#32034;&#29615;&#22659;&#26102;&#24182;&#19981;&#25552;&#20379;&#20219;&#20309;&#25351;&#23548;&#65292;&#36825;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38750;&#24120;&#38271;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20043;&#21069;&#23398;&#20064;&#30340;&#35299;&#20915;&#31616;&#21333;&#20219;&#21153;&#30340;&#21407;&#22987;&#34892;&#20026;&#65292;&#26469;&#24341;&#23548;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#26397;&#30528;&#26356;&#26377;&#22238;&#25253;&#30340;&#21160;&#20316;&#26041;&#21521;&#23398;&#20064;&#20854;&#20182;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#36825;&#31181;&#24341;&#23548;&#19981;&#26159;&#36890;&#36807;&#25163;&#21160;&#35774;&#35745;&#30340;&#35838;&#31243;&#26469;&#25191;&#34892;&#65292;&#32780;&#26159;&#20351;&#29992;&#35780;&#35770;&#23478;&#32593;&#32476;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#20197;&#21069;&#23398;&#20064;&#30340;&#21407;&#22987;&#31574;&#30053;&#25552;&#20379;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hindsight Experience Replay (HER) is a technique used in reinforcement learning (RL) that has proven to be very efficient for training off-policy RL-based agents to solve goal-based robotic manipulation tasks using sparse rewards. Even though HER improves the sample efficiency of RL-based agents by learning from mistakes made in past experiences, it does not provide any guidance while exploring the environment. This leads to very large training times due to the volume of experience required to train an agent using this replay strategy. In this paper, we propose a method that uses primitive behaviours that have been previously learned to solve simple tasks in order to guide the agent toward more rewarding actions during exploration while learning other more complex tasks. This guidance, however, is not executed by a manually designed curriculum, but rather using a critic network to decide at each timestep whether or not to use the actions proposed by the previously-learned primitive pol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.01825</link><description>&lt;p&gt;
&#20908;&#23567;&#40614;&#20998;&#21106;&#30340;PEFT&#25216;&#26415;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#26368;&#36817;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#38656;&#27714;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#21644;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#28508;&#22312;&#30340;PEFT&#24212;&#29992;&#12290;&#19981;&#21516;&#22320;&#21306;&#30340;&#27668;&#20505;&#22810;&#26679;&#24615;&#21644;&#23545;&#20840;&#38754;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#32473;&#31934;&#30830;&#35782;&#21035;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#31181;&#26893;&#23395;&#33410;&#30340;&#20316;&#29289;&#31867;&#22411;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20840;&#38754;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20351;&#29992;&#22269;&#20869;&#39046;&#20808;&#30340;&#20908;&#23567;&#40614;&#20316;&#29289;&#30417;&#27979;&#27169;&#22411;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;PEFT&#26041;&#27861;&#22312;&#20316;&#29289;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;PEFT&#26041;&#27861;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced significant growth and have been extensively employed to adapt large vision and language models to various domains, enabling satisfactory model performance with minimal computational needs. Despite these advances, more research has yet to delve into potential PEFT applications in real-life scenarios, particularly in the critical domains of remote sensing and crop monitoring. The diversity of climates across different regions and the need for comprehensive large-scale datasets have posed significant obstacles to accurately identify crop types across varying geographic locations and changing growing seasons. This study seeks to bridge this gap by comprehensively exploring the feasibility of cross-area and cross-year out-of-distribution generalization using the State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to explore PEFT approaches for crop monitoring. Specifically, we focus on adap
&lt;/p&gt;</description></item><item><title>Mini-BEHAVIOR&#26159;&#19968;&#20010;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;&#26234;&#33021;&#20307;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#24182;&#36890;&#36807;&#36807;&#31243;&#29983;&#25104;&#23454;&#29616;&#20102;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.01824</link><description>&lt;p&gt;
Mini-BEHAVIOR&#65306;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#38271;&#26399;&#20915;&#31574;&#21046;&#23450;&#30340;&#36807;&#31243;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI. (arXiv:2310.01824v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01824
&lt;/p&gt;
&lt;p&gt;
Mini-BEHAVIOR&#26159;&#19968;&#20010;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;&#26234;&#33021;&#20307;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#24182;&#36890;&#36807;&#36807;&#31243;&#29983;&#25104;&#23454;&#29616;&#20102;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Mini-BEHAVIOR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#25361;&#25112;&#26234;&#33021;&#20307;&#21033;&#29992;&#25512;&#29702;&#21644;&#20915;&#31574;&#25216;&#33021;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#20154;&#31867;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#12290;Mini-BEHAVIOR&#29615;&#22659;&#26159;&#19968;&#20010;&#24555;&#36895;&#65292;&#29616;&#23454;&#30340;Gridworld&#29615;&#22659;&#65292;&#26082;&#20855;&#26377;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#21644;&#26131;&#29992;&#24615;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20063;&#20445;&#30041;&#20102;&#22797;&#26434;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#22522;&#20934;&#20013;&#31526;&#21495;&#32423;&#30340;&#29289;&#29702;&#29616;&#23454;&#24863;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#38190;&#29305;&#24615;&#65292;&#22914;&#36807;&#31243;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;Mini-BEHAVIOR&#25552;&#20379;&#20102;&#21407;&#22987;BEHAVIOR&#22522;&#20934;&#20013;&#21508;&#31181;&#23478;&#21153;&#20219;&#21153;&#30340;&#23454;&#29616;&#65292;&#20197;&#21450;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#35757;&#32451;&#30340;&#20837;&#38376;&#20195;&#30721;&#12290;&#24635;&#20043;&#65292;Mini-BEHAVIOR&#20026;&#35780;&#20272;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#19968;&#20010;&#24555;&#36895;&#12289;&#24320;&#25918;&#24335;&#30340;&#22522;&#20934;&#12290;&#23427;&#20316;&#20026;&#30740;&#31350;&#30340;&#29992;&#25143;&#21451;&#22909;&#30340;&#20837;&#21475;&#28857;&#65292;&#20419;&#36827;&#20102;&#35780;&#20272;&#21644;&#21457;&#23637;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges agents to use reasoning and decision-making skills to solve complex activities that resemble everyday human challenges. The Mini-BEHAVIOR environment is a fast, realistic Gridworld environment that offers the benefits of rapid prototyping and ease of use while preserving a symbolic level of physical realism and complexity found in complex embodied AI benchmarks. We introduce key features such as procedural generation, to enable the creation of countless task variations and support open-ended learning. Mini-BEHAVIOR provides implementations of various household tasks from the original BEHAVIOR benchmark, along with starter code for data collection and reinforcement learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended benchmark for evaluating decision-making and planning solutions in embodied AI. It serves as a user-friendly entry point for research and facilitates the evaluation and devel
&lt;/p&gt;</description></item><item><title>MIMO-NeRF&#26159;&#19968;&#31181;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31070;&#32463;&#36752;&#23556;&#22330;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;MIMO MLP&#24182;&#36827;&#34892;&#20998;&#32452;&#26144;&#23556;&#65292;&#25552;&#39640;&#20102;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20943;&#36731;&#20102;&#37096;&#20998;&#27169;&#31946;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01821</link><description>&lt;p&gt;
MIMO-NeRF: &#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#24555;&#36895;&#31070;&#32463;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural Radiance Fields. (arXiv:2310.01821v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01821
&lt;/p&gt;
&lt;p&gt;
MIMO-NeRF&#26159;&#19968;&#31181;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31070;&#32463;&#36752;&#23556;&#22330;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;MIMO MLP&#24182;&#36827;&#34892;&#20998;&#32452;&#26144;&#23556;&#65292;&#25552;&#39640;&#20102;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20943;&#36731;&#20102;&#37096;&#20998;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#21333;&#36755;&#20837;&#21333;&#36755;&#20986;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;SISO MLP&#65289;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#23558;3D&#22352;&#26631;&#21644;&#35270;&#35282;&#26144;&#23556;&#21040;&#39068;&#33394;&#21644;&#20307;&#31215;&#23494;&#24230;&#65292;&#36825;&#20250;&#23548;&#33268;&#28210;&#26579;&#36895;&#24230;&#21464;&#24930;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;MIMO-NeRF&#65289;&#65292;&#36890;&#36807;&#23558;SISO MLP&#26367;&#25442;&#20026;MIMO MLP&#24182;&#36827;&#34892;&#20998;&#32452;&#26144;&#23556;&#26469;&#20943;&#23569;&#36816;&#34892;&#30340;MLP&#25968;&#37327;&#12290;&#20854;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#26159;&#65292;&#27599;&#20010;&#28857;&#30340;&#39068;&#33394;&#21644;&#20307;&#31215;&#23494;&#24230;&#21487;&#20197;&#26681;&#25454;&#32452;&#20869;&#36755;&#20837;&#22352;&#26631;&#30340;&#36873;&#25321;&#26377;&#25152;&#19981;&#21516;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19968;&#20123;&#26126;&#26174;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24555;&#36895;&#37325;&#26500;&#30340;MLP&#23545;MIMO MLP&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#20943;&#36731;&#27492;&#27169;&#31946;&#24615;&#32780;&#26080;&#38656;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#21253;&#25324;&#27604;&#36739;&#21644;&#28040;&#34701;&#30740;&#31350;&#22312;&#20869;&#30340;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural radiance fields (NeRFs) have shown impressive results for novel view synthesis. However, they depend on the repetitive use of a single-input single-output multilayer perceptron (SISO MLP) that maps 3D coordinates and view direction to the color and volume density in a sample-wise manner, which slows the rendering. We propose a multi-input multi-output NeRF (MIMO-NeRF) that reduces the number of MLPs running by replacing the SISO MLP with a MIMO MLP and conducting mappings in a group-wise manner. One notable challenge with this approach is that the color and volume density of each point can differ according to a choice of input coordinates in a group, which can lead to some notable ambiguity. We also propose a self-supervised learning method that regularizes the MIMO MLP with multiple fast reformulated MLPs to alleviate this ambiguity without using pretrained models. The results of a comprehensive experimental evaluation including comparative and ablation studies are presented to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#25311;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#26469;&#26356;&#21152;&#31070;&#32463;&#21487;&#34892;&#22320;&#23454;&#29616;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#23558;&#36830;&#32493;&#30340;&#34920;&#31034;&#31354;&#38388;&#21010;&#20998;&#20026;&#23545;&#24212;&#20110;&#31526;&#21495;&#24207;&#21015;&#30340;&#20998;&#21306;&#12290;&#36890;&#36807;&#24341;&#20837;&#31526;&#21495;&#31354;&#38388;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20016;&#23500;&#30340;&#24863;&#30693;&#36755;&#20837;&#30340;&#21560;&#24341;&#23376;&#25903;&#25345;&#34920;&#31034;&#31354;&#38388;&#20013;&#23454;&#29616;&#32452;&#21512;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01807</link><description>&lt;p&gt;
&#36890;&#36807;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#23454;&#29616;&#31163;&#25955;&#12289;&#32452;&#21512;&#21644;&#31526;&#21495;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Discrete, compositional, and symbolic representations through attractor dynamics. (arXiv:2310.01807v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#25311;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#26469;&#26356;&#21152;&#31070;&#32463;&#21487;&#34892;&#22320;&#23454;&#29616;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#23558;&#36830;&#32493;&#30340;&#34920;&#31034;&#31354;&#38388;&#21010;&#20998;&#20026;&#23545;&#24212;&#20110;&#31526;&#21495;&#24207;&#21015;&#30340;&#20998;&#21306;&#12290;&#36890;&#36807;&#24341;&#20837;&#31526;&#21495;&#31354;&#38388;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20016;&#23500;&#30340;&#24863;&#30693;&#36755;&#20837;&#30340;&#21560;&#24341;&#23376;&#25903;&#25345;&#34920;&#31034;&#31354;&#38388;&#20013;&#23454;&#29616;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#24615;&#26159;&#31163;&#25955;&#31526;&#21495;&#31995;&#32479;&#65288;&#22914;&#35821;&#35328;&#21644;&#31243;&#24207;&#65289;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#23427;&#20351;&#24471;&#36825;&#20123;&#31995;&#32479;&#23613;&#31649;&#20351;&#29992;&#26377;&#38480;&#30340;&#31526;&#21495;&#38598;&#21512;&#65292;&#20294;&#20173;&#20855;&#26377;&#26080;&#38480;&#30340;&#23481;&#37327;&#12290;&#23427;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#25512;&#29702;&#20013;&#37117;&#20855;&#26377;&#24456;&#22909;&#30340;&#25277;&#35937;&#24615;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#21644;&#31526;&#21495;&#22788;&#29702;&#20043;&#38388;&#30340;&#30028;&#38754;&#36890;&#24120;&#26159;&#36890;&#36807;&#31639;&#27861;&#32423;&#21035;&#19978;&#30340;&#37327;&#21270;&#25110;softmax&#37319;&#26679;&#27493;&#39588;&#26469;&#23454;&#29616;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#23558;&#31163;&#25955;&#21270;&#23454;&#29616;&#24471;&#26356;&#21152;&#31070;&#32463;&#21487;&#34892;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#36830;&#32493;&#30340;&#34920;&#31034;&#31354;&#38388;&#21010;&#20998;&#20026;&#23545;&#24212;&#20110;&#31526;&#21495;&#24207;&#21015;&#30340;&#20998;&#21306;&#12290;&#22312;&#21560;&#24341;&#23376;&#32593;&#32476;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20016;&#23500;&#30340;&#24863;&#30693;&#36755;&#20837;&#30340;&#21560;&#24341;&#23376;&#25903;&#25345;&#34920;&#31034;&#31354;&#38388;&#20013;&#24341;&#20837;&#31526;&#21495;&#31354;&#38388;&#32467;&#26500;&#21487;&#20197;&#20135;&#29983;&#32452;&#21512;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#31181;&#20449;&#24687;&#22686;&#38271;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositionality is an important feature of discrete symbolic systems, such as language and programs, as it enables them to have infinite capacity despite a finite symbol set. It serves as a useful abstraction for reasoning in both cognitive science and in AI, yet the interface between continuous and symbolic processing is often imposed by fiat at the algorithmic level, such as by means of quantization or a softmax sampling step. In this work, we explore how discretization could be implemented in a more neurally plausible manner through the modeling of attractor dynamics that partition the continuous representation space into basins that correspond to sequences of symbols. Building on established work in attractor networks and introducing novel training methods, we show that imposing structure in the symbolic space can produce compositionality in the attractor-supported representation space of rich sensory inputs. Lastly, we argue that our model exhibits the process of an information b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;YOLOv5s&#27169;&#22411;&#22312;&#23567;&#30446;&#26631;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;GhostNet&#30340;&#21367;&#31215;&#27169;&#22359;&#12289;&#22522;&#20110;RepGFPN&#30340;Neck&#27169;&#22359;&#20248;&#21270;&#12289;CA&#21644;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#21450;&#20351;&#29992;NWD&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#22797;&#26434;&#32972;&#26223;&#21644;&#24494;&#23567;&#30446;&#26631;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.01806</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#22359;&#20248;&#21270;&#30340;YOLOv5&#23567;&#30446;&#26631;&#35782;&#21035;&#30340;&#25913;&#36827;&#19982;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Improvement and Enhancement of YOLOv5 Small Target Recognition Based on Multi-module Optimization. (arXiv:2310.01806v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;YOLOv5s&#27169;&#22411;&#22312;&#23567;&#30446;&#26631;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;GhostNet&#30340;&#21367;&#31215;&#27169;&#22359;&#12289;&#22522;&#20110;RepGFPN&#30340;Neck&#27169;&#22359;&#20248;&#21270;&#12289;CA&#21644;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#21450;&#20351;&#29992;NWD&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#22797;&#26434;&#32972;&#26223;&#21644;&#24494;&#23567;&#30446;&#26631;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#24182;&#25913;&#36827;&#20102;YOLOv5s&#27169;&#22411;&#22312;&#23567;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;GhostNet&#30340;&#21367;&#31215;&#27169;&#22359;&#65292;&#22522;&#20110;RepGFPN&#30340;Neck&#27169;&#22359;&#20248;&#21270;&#65292;CA&#21644;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#20351;&#29992;NWD&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#20123;&#25913;&#36827;&#31574;&#30053;&#23545;&#27169;&#22411;&#30340;&#31934;&#30830;&#24230;&#65292;&#21484;&#22238;&#29575;&#21644;mAP&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#27979;&#35797;&#20013;&#65292;&#25913;&#36827;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#32972;&#26223;&#21644;&#24494;&#23567;&#30446;&#26631;&#26041;&#38754;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#20026;YOLOv5s&#27169;&#22411;&#22312;&#23567;&#30446;&#26631;&#26816;&#27979;&#19978;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#20026;&#26410;&#26469;&#30456;&#20851;&#30740;&#31350;&#21644;&#24212;&#29992;&#22880;&#23450;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, the limitations of YOLOv5s model on small target detection task are deeply studied and improved. The performance of the model is successfully enhanced by introducing GhostNet-based convolutional module, RepGFPN-based Neck module optimization, CA and Transformer's attention mechanism, and loss function improvement using NWD. The experimental results validate the positive impact of these improvement strategies on model precision, recall and mAP. In particular, the improved model shows significant superiority in dealing with complex backgrounds and tiny targets in real-world application tests. This study provides an effective optimization strategy for the YOLOv5s model on small target detection, and lays a solid foundation for future related research and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#24494;&#30005;&#32593;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#35843;&#24230;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#21508;&#31181;&#31639;&#27861;&#22312;&#32463;&#27982;&#35843;&#24230;&#21644;&#29615;&#22659;&#35843;&#24230;&#19979;&#25552;&#20379;&#20102;&#19981;&#21516;&#30340;&#35843;&#24230;&#32467;&#26524;&#65292;&#25581;&#31034;&#20102;&#26612;&#27833;&#21457;&#30005;&#26426;&#21644;&#24494;&#29123;&#27668;&#36718;&#26426;&#22312;&#24494;&#30005;&#32593;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.01805</link><description>&lt;p&gt;
&#24494;&#30005;&#32593;&#22810;&#30446;&#26631;&#20248;&#21270;&#35843;&#24230;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative study of microgrid optimal scheduling under multi-optimization algorithm fusion. (arXiv:2310.01805v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#24494;&#30005;&#32593;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#35843;&#24230;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#21508;&#31181;&#31639;&#27861;&#22312;&#32463;&#27982;&#35843;&#24230;&#21644;&#29615;&#22659;&#35843;&#24230;&#19979;&#25552;&#20379;&#20102;&#19981;&#21516;&#30340;&#35843;&#24230;&#32467;&#26524;&#65292;&#25581;&#31034;&#20102;&#26612;&#27833;&#21457;&#30005;&#26426;&#21644;&#24494;&#29123;&#27668;&#36718;&#26426;&#22312;&#24494;&#30005;&#32593;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#23545;&#21487;&#20877;&#29983;&#21644;&#28165;&#27905;&#33021;&#28304;&#30340;&#20851;&#27880;&#26085;&#30410;&#22686;&#38271;&#65292;&#24494;&#30005;&#32593;&#30340;&#30740;&#31350;&#21644;&#23454;&#26045;&#21464;&#24471;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#30446;&#26631;&#20248;&#21270;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#24494;&#30005;&#32593;&#30340;&#36816;&#33829;&#25104;&#26412;&#21644;&#29615;&#22659;&#25104;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#38598;&#25104;&#36951;&#20256;&#31639;&#27861;&#12289;&#27169;&#25311;&#36864;&#28779;&#31639;&#27861;&#12289;&#34433;&#32676;&#20248;&#21270;&#31639;&#27861;&#21644;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#31561;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#26469;&#36827;&#34892;&#24494;&#30005;&#32593;&#20248;&#21270;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#32463;&#27982;&#35843;&#24230;&#21644;&#29615;&#22659;&#35843;&#24230;&#19979;&#25552;&#20379;&#19981;&#21516;&#30340;&#35843;&#24230;&#32467;&#26524;&#65292;&#25581;&#31034;&#20102;&#26612;&#27833;&#21457;&#30005;&#26426;&#21644;&#24494;&#29123;&#27668;&#36718;&#26426;&#22312;&#24494;&#30005;&#32593;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#30740;&#31350;&#20026;&#24494;&#30005;&#32593;&#30340;&#35774;&#35745;&#21644;&#36816;&#33829;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#35265;&#35299;&#21644;&#23454;&#36341;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
As global attention on renewable and clean energy grows, the research and implementation of microgrids become paramount. This paper delves into the methodology of exploring the relationship between the operational and environmental costs of microgrids through multi-objective optimization models. By integrating various optimization algorithms like Genetic Algorithm, Simulated Annealing, Ant Colony Optimization, and Particle Swarm Optimization, we propose an integrated approach for microgrid optimization. Simulation results depict that these algorithms provide different dispatch results under economic and environmental dispatch, revealing distinct roles of diesel generators and micro gas turbines in microgrids. Overall, this study offers in-depth insights and practical guidance for microgrid design and operation.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.01798</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#19981;&#33021;&#33258;&#25105;&#32416;&#27491;&#25512;&#29702;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Cannot Self-Correct Reasoning Yet. (arXiv:2310.01798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01798
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20973;&#20511;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26080;&#21487;&#27604;&#25311;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#32780;&#25104;&#20026;&#31361;&#30772;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20854;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24403;&#24615;&#20173;&#23384;&#22312;&#30097;&#34385;&#12290;&#33258;&#25105;&#32416;&#27491;&#26041;&#27861;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;LLMs&#20869;&#37096;&#30340;&#33258;&#25105;&#32416;&#27491;&#30340;&#20316;&#29992;&#21644;&#25928;&#26524;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#32771;&#23519;&#65292;&#25581;&#31034;&#20102;&#20854;&#30495;&#27491;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20869;&#22312;&#33258;&#25105;&#32416;&#27491;&#30340;&#27010;&#24565;&#65292;&#21363;LLMs&#23581;&#35797;&#20165;&#20165;&#20381;&#38752;&#20854;&#22266;&#26377;&#33021;&#21147;&#26469;&#32416;&#27491;&#20854;&#21021;&#22987;&#21709;&#24212;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#21453;&#39304;&#30340;&#25903;&#25345;&#12290;&#22312;&#25512;&#29702;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;LLMs&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#33258;&#25105;&#32416;&#27491;&#20854;&#21709;&#24212;&#65292;&#29978;&#33267;&#26377;&#26102;&#20505;&#20854;&#34920;&#29616;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future resear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.01791</link><description>&lt;p&gt;
&#20855;&#26377;&#20219;&#24847;&#30830;&#23450;&#24615;&#20445;&#35777;&#30340;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Online POMDP Planning with Anytime Deterministic Guarantees. (arXiv:2310.01791v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#32463;&#24120;&#36935;&#21040;&#19981;&#30830;&#23450;&#24615;&#24182;&#22522;&#20110;&#19981;&#23436;&#25972;&#20449;&#24687;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#35268;&#21010;&#21487;&#20197;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;POMDP&#30340;&#26368;&#20248;&#35268;&#21010;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#21482;&#26377;&#22312;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#21487;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#36817;&#20284;&#31639;&#27861;&#65288;&#22914;&#26641;&#25628;&#32034;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36739;&#22823;&#38382;&#39064;&#30340;&#20808;&#36827;POMDP&#27714;&#35299;&#22120;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20165;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#37319;&#26679;&#30340;&#32536;&#25925;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#36873;&#25321;&#19968;&#32452;&#35266;&#27979;&#20197;&#22312;&#35745;&#31639;&#27599;&#20010;&#21518;&#39564;&#33410;&#28857;&#26102;&#20998;&#25903;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents operating in real-world scenarios frequently encounter uncertainty and make decisions based on incomplete information. Planning under uncertainty can be mathematically formalized using partially observable Markov decision processes (POMDPs). However, finding an optimal plan for POMDPs can be computationally expensive and is feasible only for small tasks. In recent years, approximate algorithms, such as tree search and sample-based methodologies, have emerged as state-of-the-art POMDP solvers for larger problems. Despite their effectiveness, these algorithms offer only probabilistic and often asymptotic guarantees toward the optimal solution due to their dependence on sampling. To address these limitations, we derive a deterministic relationship between a simplified solution that is easier to obtain and the theoretically optimal one. First, we derive bounds for selecting a subset of the observations to branch from while computing a complete belief at each posterior nod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#23545;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#19982;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#31185;&#23398;&#21453;&#39304;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01783</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#23545;&#30740;&#31350;&#35770;&#25991;&#26377;&#29992;&#30340;&#21453;&#39304;&#21527;&#65311;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models provide useful feedback on research papers? A large-scale empirical analysis. (arXiv:2310.01783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#23545;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#19982;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#31185;&#23398;&#21453;&#39304;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#30340;&#21453;&#39304;&#26159;&#20005;&#35880;&#30740;&#31350;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#23398;&#26415;&#20135;&#20986;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#22797;&#26434;&#30340;&#19987;&#19994;&#30693;&#35782;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#31185;&#23398;&#21453;&#39304;&#26426;&#21046;&#12290;&#36234;&#26469;&#36234;&#38590;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#21516;&#34892;&#35780;&#23457;&#24847;&#35265;&#12290;&#21021;&#32423;&#30740;&#31350;&#20154;&#21592;&#25110;&#26469;&#33258;&#36164;&#28304;&#21294;&#20047;&#30340;&#29615;&#22659;&#23588;&#20854;&#38590;&#20197;&#21450;&#26102;&#33719;&#24471;&#21453;&#39304;&#12290;&#38543;&#30528;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#21453;&#39304;&#24341;&#36215;&#20102;&#24191;&#27867;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#23436;&#25972;PDF&#25552;&#20379;&#35780;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#21453;&#39304;&#30340;&#36136;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;15&#26412;Nature&#31867;&#26399;&#21002;&#65288;&#24635;&#20849;3096&#31687;&#35770;&#25991;&#65289;&#21644;ICLR m &#19978;&#23450;&#37327;&#27604;&#36739;&#20102;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#19982;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR m
&lt;/p&gt;</description></item><item><title>STAMP&#26159;&#19968;&#31181;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.01775</link><description>&lt;p&gt;
STAMP&#65306;&#36890;&#36807;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#21487;&#24494;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent. (arXiv:2310.01775v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01775
&lt;/p&gt;
&lt;p&gt;
STAMP&#26159;&#19968;&#31181;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25805;&#20316;&#20219;&#21153;&#65292;&#22914;&#20351;&#29992;&#24037;&#20855;&#25110;&#35013;&#37197;&#38646;&#20214;&#65292;&#24448;&#24448;&#38656;&#35201;&#31526;&#21495;&#21644;&#20960;&#20309;&#25512;&#29702;&#12290;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#39640;&#32423;&#20219;&#21153;&#24207;&#21015;&#36827;&#34892;&#26641;&#25628;&#32034;&#24182;&#26816;&#26597;&#36816;&#21160;&#23398;&#21644;&#21160;&#21147;&#23398;&#21487;&#34892;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#34429;&#28982;&#24615;&#33021;&#33391;&#22909;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#65292;&#22240;&#20026;&#20854;&#26102;&#38388;&#22797;&#26434;&#24615;&#38543;&#21487;&#33021;&#21160;&#20316;&#21644;&#29289;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21482;&#33021;&#25214;&#21040;&#21333;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#21487;&#34892;&#30340;&#35745;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stein&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;STAMP&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#26469;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#35745;&#21010;&#12290;STAMP&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;TAMP&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#20197;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#35299;&#20915;&#30340;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65292;&#19968;&#31181;&#27010;&#29575;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning for many manipulation tasks, such as using tools or assembling parts, often requires both symbolic and geometric reasoning. Task and Motion Planning (TAMP) algorithms typically solve these problems by conducting a tree search over high-level task sequences while checking for kinematic and dynamic feasibility. While performant, most existing algorithms are highly inefficient as their time complexity grows exponentially with the number of possible actions and objects. Additionally, they only find a single solution to problems in which many feasible plans may exist. To address these limitations, we propose a novel algorithm called Stein Task and Motion Planning (STAMP) that leverages parallelization and differentiable simulation to efficiently search for multiple diverse plans. STAMP relaxes discrete-and-continuous TAMP problems into continuous optimization problems that can be solved using variational inference. Our algorithm builds upon Stein Variational Gradient Descent, a gra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.01770</link><description>&lt;p&gt;
&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#21387;&#32553;&#34920;&#31034;&#30340;&#31616;&#21333;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#31181;&#65292;&#21253;&#25324;&#33267;&#23569;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26223;&#35266;&#30340;&#24418;&#29366;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#31034;&#27969;&#24418;&#30340;&#32467;&#26500;&#65288;&#21363;&#21333;&#20301;&#27963;&#21160;&#30340;&#31354;&#38388;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20851;&#20294;&#24456;&#23569;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#21644;&#26126;&#30830;&#20851;&#32852;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#31070;&#32463;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#21442;&#25968;&#20248;&#21270;&#25152;&#25506;&#32034;&#30340;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#25439;&#22833;&#24179;&#22374;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#30001;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#26469;&#39044;&#27979;&#65306;&#25439;&#22833;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;\citet{ma_linear_2021}&#30340;&#20808;&#21069;&#30740;&#31350;&#23494;&#20999;&#30456;&#20851;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#24179;&#22374;&#24615;&#65288;&#21363;&#23567;&#29305;&#24449;&#20540;&#65289;&#19982;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24046;&#20998;&#32534;&#30721;&#35266;&#27979;&#31354;&#38388;&#26469;&#38477;&#20302;&#24863;&#30693;&#22686;&#24378;&#23398;&#20064;&#65288;DRL&#65289;&#31995;&#32479;&#35757;&#32451;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.01767</link><description>&lt;p&gt;
&#24046;&#20998;&#32534;&#30721;&#35266;&#27979;&#31354;&#38388;&#22312;&#24863;&#30693;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentially Encoded Observation Spaces for Perceptive Reinforcement Learning. (arXiv:2310.01767v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01767
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#32534;&#30721;&#35266;&#27979;&#31354;&#38388;&#26469;&#38477;&#20302;&#24863;&#30693;&#22686;&#24378;&#23398;&#20064;&#65288;DRL&#65289;&#31995;&#32479;&#35757;&#32451;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#22686;&#24378;&#23398;&#20064;&#65288;Perceptive deep reinforcement learning&#65292;DRL&#65289;&#22312;&#21033;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#22797;&#26434;AI&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#35768;&#22810;&#31361;&#30772;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#24212;&#29992;&#33539;&#22260;&#20174;&#36229;&#20154;&#32423;&#21035;&#30340;&#35270;&#39057;&#28216;&#25103;&#20195;&#29702;&#21040;&#28789;&#24039;&#12289;&#20855;&#26377;&#29289;&#29702;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#24863;&#30693;DRL&#31995;&#32479;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#65292;&#36890;&#24120;&#38656;&#35201;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22823;&#23481;&#37327;&#30340;&#32463;&#39564;&#22238;&#25918;&#32531;&#20914;&#21306;&#12290;&#36825;&#23545;&#26410;&#26469;&#19968;&#20195;&#30340;&#29616;&#22330;&#26426;&#22120;&#20154;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#20182;&#20204;&#38656;&#35201;&#33021;&#22815;&#22312;&#36793;&#32536;&#23398;&#20064;&#65292;&#20197;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24046;&#20998;&#32534;&#30721;&#35266;&#27979;&#31354;&#38388;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#23384;&#20648;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#37325;&#35299;&#37322;&#20026;&#35270;&#39057;&#65292;&#25105;&#20204;&#21033;&#29992;&#26080;&#25439;&#24046;&#20998;&#35270;&#39057;&#32534;&#30721;&#26041;&#26696;&#26469;&#21387;&#32553;&#32463;&#39564;&#22238;&#25918;&#32531;&#20914;&#21306;&#65292;&#32780;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;DRL&#31639;&#27861;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#24046;&#20998;&#22270;&#20687;&#32534;&#30721;&#21487;&#20197;&#38477;&#20302;&#35757;&#32451;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptive deep reinforcement learning (DRL) has lead to many recent breakthroughs for complex AI systems leveraging image-based input data. Applications of these results range from super-human level video game agents to dexterous, physically intelligent robots. However, training these perceptive DRL-enabled systems remains incredibly compute and memory intensive, often requiring huge training datasets and large experience replay buffers. This poses a challenge for the next generation of field robots that will need to be able to learn on the edge in order to adapt to their environments. In this paper, we begin to address this issue through differentially encoded observation spaces. By reinterpreting stored image-based observations as a video, we leverage lossless differential video encoding schemes to compress the replay buffer without impacting training performance. We evaluate our approach with three state-of-the-art DRL algorithms and find that differential image encoding reduces th
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#30340;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#19988;&#26080;&#38656;&#32479;&#19968;&#25506;&#32034;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#26080;&#30028;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01756</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#31639;&#27861;&#29992;&#20110;&#20855;&#26377;&#26080;&#30028;&#25439;&#22833;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Improved Algorithms for Adversarial Bandits with Unbounded Losses. (arXiv:2310.01756v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01756
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#30340;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#19988;&#26080;&#38656;&#32479;&#19968;&#25506;&#32034;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#26080;&#30028;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#26080;&#30028;&#25439;&#22833;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#31639;&#27861;&#23545;&#25439;&#22833;&#30340;&#22823;&#23567;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UMAB-NN&#21644;UMAB-G&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#38750;&#36127;&#21644;&#19968;&#33324;&#30340;&#26080;&#30028;&#25439;&#22833;&#12290;&#23545;&#20110;&#38750;&#36127;&#26080;&#30028;&#25439;&#22833;&#65292;UMAB-NN&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;&#33258;&#36866;&#24212;&#19988;&#26080;&#38656;&#32479;&#19968;&#25506;&#32034;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;UMAB-G&#65292;&#21487;&#20197;&#23398;&#20064;&#20219;&#24847;&#26080;&#30028;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;MAB&#38382;&#39064;&#20013;&#27491;&#36127;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#26469;&#37197;&#21512;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#31639;&#27861;&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#22788;&#29702;&#26080;&#30028;&#25439;&#22833;&#30340;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the Adversarial Multi-Armed Bandits (MAB) problem with unbounded losses, where the algorithms have no prior knowledge on the sizes of the losses. We present UMAB-NN and UMAB-G, two algorithms for non-negative and general unbounded loss respectively. For non-negative unbounded loss, UMAB-NN achieves the first adaptive and scale free regret bound without uniform exploration. Built up on that, we further develop UMAB-G that can learn from arbitrary unbounded loss. Our analysis reveals the asymmetry between positive and negative losses in the MAB problem and provide additional insights. We also accompany our theoretical findings with extensive empirical evaluations, showing that our algorithms consistently out-performs all existing algorithms that handles unbounded losses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01737</link><description>&lt;p&gt;
&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#40065;&#26834;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Blending Imitation and Reinforcement Learning for Robust Policy Improvement. (arXiv:2310.01737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#20223;&#23398;&#20064;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#21463;&#21040;&#25152;&#20351;&#29992;&#30340;&#19987;&#23478;&#31034;&#33539;&#30340;&#36136;&#37327;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#20174;&#22810;&#31181;&#40657;&#30418;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#20064;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#39044;&#26399;&#22806;&#35266;&#36827;&#34892;&#26415;&#20013;&#24739;&#32773;&#21040;&#22270;&#20687;&#37197;&#20934;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#20808;&#21512;&#25104;&#29305;&#23450;&#24739;&#32773;&#30340;&#39044;&#26399;&#35270;&#22270;&#65292;&#24182;&#21033;&#29992;&#20809;&#23398;&#26174;&#24494;&#38236;&#35266;&#23519;&#21040;&#30340;&#20108;&#32500;&#22270;&#20687;&#26469;&#20272;&#35745;&#30456;&#26426;&#23039;&#24577;&#65292;&#35813;&#26041;&#27861;&#38477;&#20302;&#20102;&#26415;&#20013;&#22270;&#20687;&#36136;&#37327;&#23545;&#37197;&#20934;&#31934;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#20020;&#24202;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.01735</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#22806;&#31185;&#25163;&#26415;&#20013;&#23398;&#20064;&#39044;&#26399;&#22806;&#35266;&#20197;&#36827;&#34892;&#26415;&#20013;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Learning Expected Appearances for Intraoperative Registration during Neurosurgery. (arXiv:2310.01735v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#39044;&#26399;&#22806;&#35266;&#36827;&#34892;&#26415;&#20013;&#24739;&#32773;&#21040;&#22270;&#20687;&#37197;&#20934;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#20808;&#21512;&#25104;&#29305;&#23450;&#24739;&#32773;&#30340;&#39044;&#26399;&#35270;&#22270;&#65292;&#24182;&#21033;&#29992;&#20809;&#23398;&#26174;&#24494;&#38236;&#35266;&#23519;&#21040;&#30340;&#20108;&#32500;&#22270;&#20687;&#26469;&#20272;&#35745;&#30456;&#26426;&#23039;&#24577;&#65292;&#35813;&#26041;&#27861;&#38477;&#20302;&#20102;&#26415;&#20013;&#22270;&#20687;&#36136;&#37327;&#23545;&#37197;&#20934;&#31934;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#20020;&#24202;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#39044;&#26399;&#22806;&#35266;&#36827;&#34892;&#26415;&#20013;&#24739;&#32773;&#21040;&#22270;&#20687;&#37197;&#20934;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26415;&#21069;&#25104;&#20687;&#36890;&#36807;&#22806;&#31185;&#26174;&#24494;&#38236;&#21512;&#25104;&#29305;&#23450;&#24739;&#32773;&#30340;&#39044;&#26399;&#35270;&#22270;&#65292;&#20197;&#36827;&#34892;&#39044;&#27979;&#33539;&#22260;&#20869;&#30340;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#26415;&#20013;&#20809;&#23398;&#26174;&#24494;&#38236;&#35266;&#23519;&#21040;&#30340;&#20108;&#32500;&#22270;&#20687;&#19982;&#21512;&#25104;&#39044;&#26399;&#32441;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20272;&#35745;&#30456;&#26426;&#23039;&#24577;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22788;&#29702;&#20219;&#21153;&#36716;&#31227;&#21040;&#26415;&#21069;&#38454;&#27573;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26415;&#20013;&#22270;&#20687;&#30340;&#20302;&#20998;&#36776;&#29575;&#12289;&#30072;&#21464;&#21644;&#22122;&#38899;&#23545;&#37197;&#20934;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#33041;&#37096;&#25163;&#26415;&#30340;&#31070;&#32463;&#23548;&#33322;&#32972;&#26223;&#19979;&#24212;&#29992;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;6&#20010;&#20020;&#24202;&#26696;&#20363;&#30340;&#22238;&#39038;&#24615;&#25968;&#25454;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#20102;&#24403;&#21069;&#20020;&#24202;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method for intraoperative patient-to-image registration by learning Expected Appearances. Our method uses preoperative imaging to synthesize patient-specific expected views through a surgical microscope for a predicted range of transformations. Our method estimates the camera pose by minimizing the dissimilarity between the intraoperative 2D view through the optical microscope and the synthesized expected texture. In contrast to conventional methods, our approach transfers the processing tasks to the preoperative stage, reducing thereby the impact of low-resolution, distorted, and noisy intraoperative images, that often degrade the registration accuracy. We applied our method in the context of neuronavigation during brain surgery. We evaluated our approach on synthetic data and on retrospective data from 6 clinical cases. Our method outperformed state-of-the-art methods and achieved accuracies that met current clinical standards.
&lt;/p&gt;</description></item><item><title>Nugget&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36873;&#25321;&#30340;&#36755;&#20837;&#20196;&#29260;&#23376;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#65292;&#23558;&#35821;&#35328;&#20998;&#21106;&#20026;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;&#65292;&#20248;&#20110;&#30456;&#20851;&#26041;&#27861;&#65292;&#22312;&#35821;&#20041;&#27604;&#36739;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20801;&#35768;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;</title><link>http://arxiv.org/abs/2310.01732</link><description>&lt;p&gt;
Nugget: &#25991;&#26412;&#30340;&#31070;&#32463;&#32858;&#21512;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Nugget: Neural Agglomerative Embeddings of Text. (arXiv:2310.01732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01732
&lt;/p&gt;
&lt;p&gt;
Nugget&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36873;&#25321;&#30340;&#36755;&#20837;&#20196;&#29260;&#23376;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#65292;&#23558;&#35821;&#35328;&#20998;&#21106;&#20026;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;&#65292;&#20248;&#20110;&#30456;&#20851;&#26041;&#27861;&#65292;&#22312;&#35821;&#20041;&#27604;&#36739;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20801;&#35768;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#35821;&#35328;&#29702;&#35299;&#20013;&#65292;&#23884;&#20837;&#25991;&#26412;&#24207;&#21015;&#26159;&#19968;&#20010;&#24191;&#27867;&#38656;&#27714;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20391;&#37325;&#20110;&#24658;&#23450;&#22823;&#23567;&#30340;&#34920;&#31034;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#36890;&#24120;&#38543;&#36755;&#20837;&#30340;&#38271;&#24230;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Nugget&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#23558;&#35821;&#35328;&#32534;&#30721;&#20026;&#22522;&#20110;&#21160;&#24577;&#36873;&#25321;&#30340;&#36755;&#20837;&#20196;&#29260;&#23376;&#38598;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#33258;&#32534;&#30721;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#65292;&#23398;&#20064;&#36825;&#20123;nuggets&#65292;&#24182;&#30452;&#35266;&#22320;&#23558;&#35821;&#35328;&#20998;&#21106;&#25104;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Nugget&#22312;&#28041;&#21450;&#35821;&#20041;&#27604;&#36739;&#30340;&#20219;&#21153;&#20013;&#20248;&#20110;&#30456;&#20851;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#32039;&#20945;&#21333;&#20803;&#20801;&#35768;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#26032;&#30340;LM&#21487;&#33021;&#20250;&#23545;&#26356;&#22823;&#37327;&#30340;&#20869;&#23481;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding text sequences is a widespread requirement in modern language understanding. Existing approaches focus largely on constant-size representations. This is problematic, as the amount of information contained in text often varies with the length of the input. We propose a solution called Nugget, which encodes language into a representation based on a dynamically selected subset of input tokens. These nuggets are learned through tasks like autoencoding and machine translation, and intuitively segment language into meaningful units. We demonstrate Nugget outperforms related approaches in tasks involving semantic comparison. Finally, we illustrate these compact units allow for expanding the contextual window of a language model (LM), suggesting new future LMs that can condition on significantly larger amounts of content.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;Time-LLM&#65292;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.01728</link><description>&lt;p&gt;
Time-LLM: &#36890;&#36807;&#37325;&#26032;&#32534;&#31243;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. (arXiv:2310.01728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01728
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;Time-LLM&#65292;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#35768;&#22810;&#23454;&#38469;&#21160;&#24577;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#19981;&#21516;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#22823;&#22411;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#32780;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#19987;&#38376;&#21270;&#30340;&#65292;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#24212;&#29992;&#35774;&#35745;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;NLP&#21644;CV&#39046;&#22495;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#21457;&#23637;&#21463;&#21040;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#30340;&#24207;&#21015;&#26631;&#35760;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#20197;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Time-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#37325;&#29992;LLMs&#26469;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#25345;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GPT-4&#22312;&#26032;&#25968;&#25454;&#19978;&#25191;&#34892;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#22797;&#21046;&#30340;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#20854;&#25581;&#31034;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#26041;&#27861;&#20013;&#20551;&#35774;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.01727</link><description>&lt;p&gt;
GPT-4&#33021;&#21542;&#22797;&#21046;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GPT-4 Replicate Empirical Software Engineering Research?. (arXiv:2310.01727v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GPT-4&#22312;&#26032;&#25968;&#25454;&#19978;&#25191;&#34892;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#22797;&#21046;&#30340;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#20854;&#25581;&#31034;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#26041;&#27861;&#20013;&#20551;&#35774;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29983;&#20135;&#31995;&#32479;&#30340;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24102;&#26469;&#20102;&#23545;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#29983;&#20135;&#31995;&#32479;&#30340;&#19968;&#23567;&#37096;&#20998;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#38480;&#21046;&#20102;&#35813;&#30740;&#31350;&#30340;&#24433;&#21709;&#21147;&#12290;&#34429;&#28982;&#36719;&#20214;&#24037;&#31243;&#20174;&#19994;&#32773;&#21487;&#20197;&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;&#26469;&#33719;&#24471;&#20805;&#23454;&#33258;&#24049;&#25968;&#25454;&#30340;&#22909;&#22788;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#22240;&#20026;&#25191;&#34892;&#22797;&#21046;&#38656;&#35201;&#23545;&#30740;&#31350;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#31243;&#25968;&#25454;&#20013;&#30340;&#24494;&#22937;&#32454;&#33410;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#22312;&#22788;&#29702;&#36719;&#20214;&#24037;&#31243;&#21644;&#31185;&#23398;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#25512;&#24191;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#26032;&#25968;&#25454;&#19978;&#25191;&#34892;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#22797;&#21046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#23427;&#20204;&#25581;&#31034;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#26041;&#27861;&#20013;&#25152;&#20570;&#30340;&#20551;&#35774;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help democratize empirical software engineering research.  In this paper, we examine LLMs' abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodolog
&lt;/p&gt;</description></item><item><title>PrACTiS combines perceiver architecture with copula structure to enhance time-series forecasting and reduce computational demands. It incorporates midpoint inference and local attention mechanisms to effectively capture dependencies within imputed samples. The copula-based attention and output variance testing mechanism capture the joint distribution of missing data and mitigate error propagation during prediction.</title><link>http://arxiv.org/abs/2310.01720</link><description>&lt;p&gt;
PrACTiS: Perceiver-Attentional Copulas for Time Series&#65288;&#26102;&#38388;&#24207;&#21015;&#30340;&#24863;&#30693;-&#27880;&#24847;&#21147;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65289;
&lt;/p&gt;
&lt;p&gt;
PrACTiS: Perceiver-Attentional Copulas for Time Series. (arXiv:2310.01720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01720
&lt;/p&gt;
&lt;p&gt;
PrACTiS combines perceiver architecture with copula structure to enhance time-series forecasting and reduce computational demands. It incorporates midpoint inference and local attention mechanisms to effectively capture dependencies within imputed samples. The copula-based attention and output variance testing mechanism capture the joint distribution of missing data and mitigate error propagation during prediction.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#32852;&#21512;&#20998;&#24067;&#32467;&#26500;&#30340;Transformer&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36807;&#20110;&#20381;&#36182;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24863;&#30693;&#22120;&#26550;&#26500;&#19982;&#32852;&#21512;&#20998;&#24067;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#24863;&#30693;&#22120;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#22797;&#26434;&#30340;&#39640;&#32500;&#22810;&#27169;&#24577;&#25968;&#25454;&#36716;&#25442;&#20026;&#32039;&#20945;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#38656;&#27714;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20013;&#28857;&#25512;&#26029;&#21644;&#23616;&#37096;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25554;&#34917;&#26679;&#26412;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#32852;&#21512;&#20998;&#24067;&#30340;&#27880;&#24847;&#21147;&#21644;&#36755;&#20986;&#26041;&#24046;&#27979;&#35797;&#26426;&#21046;&#26469;&#25429;&#25417;&#32570;&#22833;&#25968;&#25454;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#21516;&#26102;&#20943;&#23569;&#39044;&#27979;&#36807;&#31243;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers incorporating copula structures have demonstrated remarkable performance in time series prediction. However, their heavy reliance on self-attention mechanisms demands substantial computational resources, thus limiting their practical utility across a wide range of tasks. In this work, we present a model that combines the perceiver architecture with a copula structure to enhance time-series forecasting. By leveraging the perceiver as the encoder, we efficiently transform complex, high-dimensional, multimodal data into a compact latent space, thereby significantly reducing computational demands. To further reduce complexity, we introduce midpoint inference and local attention mechanisms, enabling the model to capture dependencies within imputed samples effectively. Subsequently, we deploy the copula-based attention and output variance testing mechanism to capture the joint distribution of missing data, while simultaneously mitigating error propagation during prediction. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01717</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#30340;&#38598;&#25104;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#23558;&#21477;&#23376;&#30340;&#35789;&#21644;&#30701;&#35821;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#19981;&#20351;&#29992;&#35821;&#35328;&#23398;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#22120;&#25429;&#25417;&#21040;&#20102;&#35299;&#26512;&#32467;&#26500;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26641;&#24179;&#22343;&#8221;&#30340;&#27010;&#24565;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65307;&#36825;&#31181;&#38598;&#25104;-&#33976;&#39311;&#30340;&#36807;&#31243;&#26159;&#32531;&#35299;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#34920;&#29616;&#20986;&#20854;&#22312;&#19981;&#21516;&#38598;&#25104;&#32452;&#20214;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of "tree averaging," based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#29983;&#25104;&#28304;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20854;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.01701</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#36328;&#36234;&#39046;&#22495;&#65306;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation. (arXiv:2310.01701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#29983;&#25104;&#28304;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20854;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#24212;&#29992;&#27169;&#22411;&#20174;&#30456;&#20851;&#28304;&#39046;&#22495;&#33719;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#19981;&#20805;&#36275;&#26631;&#27880;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#65288;&#22914;HIPAA&#12289;COPPA&#12289;FERPA&#31561;&#65289;&#30340;&#19981;&#26029;&#21152;&#24378;&#24341;&#21457;&#20102;&#23545;&#22312;&#32469;&#36807;&#23545;&#28304;&#25968;&#25454;&#30340;&#30452;&#25509;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#65292;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#27169;&#22411;&#30340;&#20852;&#36259;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;SFDA&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#39046;&#22495;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#28304;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#22312;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#25509;&#36817;&#28304;&#25968;&#25454;&#30340;&#26679;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20154;&#24037;&#29983;&#25104;&#30340;&#28304;&#25968;&#25454;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#65292;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation (DA) is a method for enhancing a model's performance on a target domain with inadequate annotated data by applying the information the model has acquired from a related source domain with sufficient labeled data. The escalating enforcement of data-privacy regulations like HIPAA, COPPA, FERPA, etc. have sparked a heightened interest in adapting models to novel domains while circumventing the need for direct access to the source data, a problem known as Source-Free Domain Adaptation (SFDA). In this paper, we propose a novel framework for SFDA that generates source data using a text-to-image diffusion model trained on the target domain samples. Our method starts by training a text-to-image diffusion model on the labeled target domain samples, which is then fine-tuned using the pre-trained source model to generate samples close to the source data. Finally, we use Domain Adaptation techniques to align the artificially generated source data with the target domain data, resu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28304;&#25552;&#31034;&#32534;&#30721;&#21040;&#30456;&#23545;&#31354;&#38388;&#20013;&#65292;&#24182;&#25628;&#32034;&#30456;&#24212;&#30340;&#30446;&#26631;&#25552;&#31034;&#65292;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20102;&#20219;&#21153;&#35821;&#20041;&#30340;&#27867;&#21270;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01691</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#27867;&#21270;&#20219;&#21153;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models. (arXiv:2310.01691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01691
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28304;&#25552;&#31034;&#32534;&#30721;&#21040;&#30456;&#23545;&#31354;&#38388;&#20013;&#65292;&#24182;&#25628;&#32034;&#30456;&#24212;&#30340;&#30446;&#26631;&#25552;&#31034;&#65292;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20102;&#20219;&#21153;&#35821;&#20041;&#30340;&#27867;&#21270;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#36890;&#36807;&#35843;&#25972;&#25552;&#31034;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#65292;&#29305;&#21035;&#26159;&#36830;&#32493;&#25552;&#31034;&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#26041;&#27861;&#65292;&#20854;&#20013;&#28304;&#25552;&#31034;&#34987;&#32534;&#30721;&#21040;&#30456;&#23545;&#31354;&#38388;&#20013;&#65292;&#24182;&#25628;&#32034;&#30456;&#24212;&#30340;&#30446;&#26631;&#25552;&#31034;&#20197;&#23558;&#20854;&#20256;&#36882;&#21040;&#30446;&#26631;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#36830;&#32493;&#25552;&#31034;&#20013;&#30340;&#8220;&#20219;&#21153;&#35821;&#20041;&#8221;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#26469;&#33258;&#22810;&#20010;&#28304;&#27169;&#22411;&#30340;&#8220;&#20219;&#21153;&#35821;&#20041;&#8221;&#32467;&#21512;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20256;&#36882;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks. However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge. In this work, we propose a zero-shot continuous prompt transfer method, where source prompts are encoded into relative space and the corresponding target prompts are searched for transferring to target models. Experimental results confirm the effectiveness of our method, showing that 'task semantics' in continuous prompts can be generalized across various language models. Moreover, we find that combining 'task semantics' from multiple source models can further enhance the generalizability of transfer.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#34892;&#20026;&#24178;&#39044;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26032;&#39062;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#39044;&#38450;&#34880;&#31958;&#24322;&#24120;&#65292;&#26377;&#26395;&#23545;&#31038;&#20250;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.01684</link><description>&lt;p&gt;
&#35774;&#35745;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#34892;&#20026;&#24178;&#39044;&#26469;&#39044;&#38450;&#34880;&#31958;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#26032;&#39062;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Designing User-Centric Behavioral Interventions to Prevent Dysglycemia with Novel Counterfactual Explanations. (arXiv:2310.01684v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01684
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#34892;&#20026;&#24178;&#39044;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26032;&#39062;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#39044;&#38450;&#34880;&#31958;&#24322;&#24120;&#65292;&#26377;&#26395;&#23545;&#31038;&#20250;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#27963;&#26041;&#24335;&#34892;&#20026;&#32500;&#25345;&#27491;&#24120;&#34880;&#31958;&#27700;&#24179;&#23545;&#20110;&#20445;&#25345;&#20581;&#24247;&#21644;&#39044;&#38450;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#39057;&#32321;&#25509;&#35302;&#34880;&#31958;&#24322;&#24120;&#65288;&#21363;&#39640;&#34880;&#31958;&#21644;&#20302;&#34880;&#31958;&#31561;&#24322;&#24120;&#20107;&#20214;&#65289;&#20250;&#23548;&#33268;&#24930;&#24615;&#24182;&#21457;&#30151;&#65292;&#21253;&#25324;&#31958;&#23615;&#30149;&#12289;&#32958;&#33039;&#30142;&#30149;&#21450;&#38656;&#36879;&#26512;&#27835;&#30103;&#12289;&#24515;&#32908;&#26775;&#27515;&#12289;&#20013;&#39118;&#12289;&#25130;&#32930;&#21644;&#27515;&#20129;&#12290;&#22240;&#27492;&#65292;&#33021;&#22815;&#39044;&#27979;&#34880;&#31958;&#24322;&#24120;&#24182;&#21521;&#29992;&#25143;&#25552;&#20379;&#34892;&#21160;&#21453;&#39304;&#20197;&#25913;&#21464;&#39278;&#39135;&#12289;&#36816;&#21160;&#21644;&#33647;&#29289;&#27835;&#30103;&#26469;&#39044;&#38450;&#24322;&#24120;&#34880;&#31958;&#20107;&#20214;&#30340;&#24037;&#20855;&#21487;&#33021;&#20855;&#26377;&#37325;&#35201;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#31867;&#20284;&#20110;&#21407;&#22987;&#36755;&#20837;&#20294;&#23548;&#33268;&#19981;&#21516;&#39044;&#27979;&#32467;&#26524;&#30340;&#20551;&#35774;&#23454;&#20363;&#65292;&#25552;&#20379;&#27169;&#22411;&#20026;&#20309;&#23545;&#29305;&#23450;&#39044;&#27979;&#30340;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#21453;&#20107;&#23454;&#35299;&#37322;&#21487;&#20197;&#34987;&#35270;&#20026;&#35774;&#35745;AI&#39537;&#21160;&#30340;&#20581;&#24247;&#24178;&#39044;&#26469;&#39044;&#38450;&#19981;&#33391;&#20581;&#24247;&#32467;&#26524;&#65288;&#22914;&#34880;&#31958;&#24322;&#24120;&#65289;&#30340;&#19968;&#31181;&#25163;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;GlyCoa...
&lt;/p&gt;
&lt;p&gt;
Maintaining normal blood glucose levels through lifestyle behaviors is central to maintaining health and preventing disease. Frequent exposure to dysglycemia (i.e., abnormal glucose events such as hyperlycemia and hypoglycemia) leads to chronic complications including diabetes, kidney disease and need for dialysis, myocardial infarction, stroke, amputation, and death. Therefore, a tool capable of predicting dysglycemia and offering users actionable feedback about how to make changes in their diet, exercise, and medication to prevent abnormal glycemic events could have significant societal impacts. Counterfactual explanations can provide insights into why a model made a particular prediction by generating hypothetical instances that are similar to the original input but lead to a different prediction outcome. Therefore, counterfactuals can be viewed as a means to design AI-driven health interventions to prevent adverse health outcomes such as dysglycemia. In this paper, we design GlyCoa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#38190;&#28857;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24341;&#20837;&#38271;&#31243;&#31354;&#38388;&#33258;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#36816;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;CNN&#27169;&#22411;&#22312;&#20302;&#27880;&#37322;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01680</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#38190;&#28857;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26377;&#38480;&#27880;&#37322;&#24773;&#20917;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Keypoint-Augmented Self-Supervised Learning for Medical Image Segmentation with Limited Annotation. (arXiv:2310.01680v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#38190;&#28857;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24341;&#20837;&#38271;&#31243;&#31354;&#38388;&#33258;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#36816;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;CNN&#27169;&#22411;&#22312;&#20302;&#27880;&#37322;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;CNN&#27169;&#22411;&#65288;&#22914;UNet&#65289;&#24050;&#25104;&#20026;&#22312;&#20302;&#27880;&#37322;&#29615;&#22659;&#19979;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#40723;&#21169;&#30456;&#21516;&#22270;&#20687;&#32463;&#21382;&#19981;&#21516;&#21464;&#25442;&#26102;&#30340;&#31867;&#20284;&#20840;&#23616;&#34920;&#31034;&#65292;&#25110;&#22312;&#26412;&#36136;&#19978;&#30456;&#20851;&#30340;&#19981;&#21516;&#22270;&#20687;/&#34917;&#19969;&#29305;&#24449;&#20043;&#38388;&#23454;&#26045;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;CNN&#25552;&#21462;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#22312;&#25429;&#25417;&#29983;&#29289;&#35299;&#21078;&#23398;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#38271;&#31243;&#31354;&#38388;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#28857;&#22686;&#24378;&#30340;&#34701;&#21512;&#23618;&#65292;&#21487;&#20197;&#25552;&#21462;&#26082;&#20445;&#30041;&#30701;&#31243;&#21448;&#20445;&#30041;&#38271;&#31243;&#33258;&#27880;&#24847;&#21147;&#30340;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#65292;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#22686;&#24378;CNN&#29305;&#24449;&#22270;&#65292;&#35813;&#36755;&#20837;&#23398;&#20064;&#20102;&#23616;&#37096;&#20851;&#38190;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#38271;&#31243;&#31354;&#38388;&#33258;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#20840;&#23616;&#23610;&#24230;&#19978;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20840;&#23616;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining CNN models (i.e., UNet) through self-supervision has become a powerful approach to facilitate medical image segmentation under low annotation regimes. Recent contrastive learning methods encourage similar global representations when the same image undergoes different transformations, or enforce invariance across different image/patch features that are intrinsically correlated. However, CNN-extracted global and local features are limited in capturing long-range spatial dependencies that are essential in biological anatomy. To this end, we present a keypoint-augmented fusion layer that extracts representations preserving both short- and long-range self-attention. In particular, we augment the CNN feature map at multiple scales by incorporating an additional input that learns long-range spatial self-attention among localized keypoint features. Further, we introduce both global and local self-supervised pretraining for the framework. At the global scale, we obtain global repres
&lt;/p&gt;</description></item><item><title>Artemis&#26159;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30340;&#39640;&#25928;DNN&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;HE&#24863;&#30693;&#20462;&#21098;&#31574;&#30053;&#26368;&#22823;&#21270;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.01664</link><description>&lt;p&gt;
Artemis: &#38024;&#23545;&#39640;&#25928;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30340;HE&#24863;&#30693;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Artemis: HE-Aware Training for Efficient Privacy-Preserving Machine Learning. (arXiv:2310.01664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01664
&lt;/p&gt;
&lt;p&gt;
Artemis&#26159;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30340;&#39640;&#25928;DNN&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;HE&#24863;&#30693;&#20462;&#21098;&#31574;&#30053;&#26368;&#22823;&#21270;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#26159;&#19968;&#39033;&#26377;&#21069;&#26223;&#30340;&#22522;&#30784;&#38544;&#31169;&#25216;&#26415;&#12290;&#35201;&#20351;&#20854;&#26356;&#21152;&#23454;&#29992;&#65292;&#38656;&#35201;&#38477;&#20302;&#20854;&#35745;&#31639;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#29616;&#20195;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#12290;&#27169;&#22411;&#21387;&#32553;&#36890;&#36807;&#20462;&#21098;&#22312;&#20256;&#32479;&#26126;&#25991;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#24212;&#29992;&#20110;HE-PPML&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Artemis&#65292;&#19968;&#31181;&#38024;&#23545;HE&#25512;&#29702;&#30340;&#39640;&#25928;DNN&#20462;&#21098;&#25216;&#26415;&#12290;&#25105;&#20204;&#23457;&#24910;&#30740;&#31350;&#20102;&#20004;&#31181;HE&#24863;&#30693;&#20462;&#21098;&#31574;&#30053;&#65288;&#20301;&#32622;&#21644;&#23545;&#35282;&#32447;&#65289;&#65292;&#20197;&#20943;&#23569;&#26059;&#36716;&#25805;&#20316;&#30340;&#25968;&#37327;&#65292;&#22312;HE&#21367;&#31215;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#23545;&#35282;&#32447;&#20462;&#21098;&#30340; Pareto &#26368;&#20248;&#35299;&#26159;&#23436;&#20840;&#21487;&#34892;&#30340;&#12290;Artemis&#30340;&#20248;&#21183;&#22312;&#20110;&#23558;DNN&#35757;&#32451;&#19982;&#20462;&#21098;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#22242;&#20307;Lasso&#27491;&#21017;&#21270;&#30446;&#26631;&#39537;&#21160;&#65292;&#20197;&#26368;&#22823;&#21270;HE&#29305;&#23450;&#30340;&#25104;&#26412;&#38477;&#20302;&#65288;&#30001;&#26059;&#36716;&#25805;&#20316;&#20027;&#23548;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Artemis&#22312;&#20808;&#21069;&#30340;HE&#23548;&#21521;&#20462;&#21098;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#25913;&#36827;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;1.2-6&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving ML (PPML) based on Homomorphic Encryption (HE) is a promising foundational privacy technology. Making it more practical requires lowering its computational cost, especially, in handling modern large deep neural networks. Model compression via pruning is highly effective in conventional plaintext ML but cannot be effectively applied to HE-PPML as is.  We propose Artemis, a highly effective DNN pruning technique for HE-based inference. We judiciously investigate two HE-aware pruning strategies (positional and diagonal) to reduce the number of Rotation operations, which dominate compute time in HE convolution. We find that Pareto-optimal solutions are based fully on diagonal pruning. Artemis' benefits come from coupling DNN training, driven by a novel group Lasso regularization objective, with pruning to maximize HE-specific cost reduction (dominated by the Rotation operations). We show that Artemis improves on prior HE-oriented pruning and can achieve a 1.2-6x improvem
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#36710;&#20869;&#25163;&#21183;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#25216;&#26415;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#24182;&#20943;&#23569;&#25968;&#25454;&#35201;&#27714;&#65292;&#36890;&#36807;&#20351;&#29992;&#39134;&#34892;&#26102;&#38388;&#25668;&#20687;&#22836;&#36827;&#34892;&#30828;&#20214;&#22686;&#24378;&#21644;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#12289;&#20010;&#24615;&#21270;&#36866;&#24212;&#21644;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#31639;&#27861;&#22686;&#24378;&#12290;&#35813;&#26041;&#27861;&#22312;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#35782;&#21035;&#39046;&#22495;&#26377;&#30528;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#20026;&#20010;&#20154;&#29992;&#25143;&#23450;&#21046;&#65292;&#20174;&#32780;&#25552;&#21319;&#36710;&#20869;&#20132;&#20114;&#30340;&#23433;&#20840;&#24615;&#21644;&#20415;&#21033;&#24615;&#65292;&#24182;&#25552;&#39640;&#39550;&#39542;&#32773;&#30340;&#20307;&#39564;&#21644;&#23545;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2310.01659</link><description>&lt;p&gt;
&#20851;&#20110;&#20320;&#65306;&#22522;&#20110;&#39134;&#34892;&#26102;&#38388;&#25668;&#20687;&#22836;&#30340;&#20010;&#24615;&#21270;&#36710;&#20869;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
It's all about you: Personalized in-Vehicle Gesture Recognition with a Time-of-Flight Camera. (arXiv:2310.01659v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01659
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#36710;&#20869;&#25163;&#21183;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#25216;&#26415;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#24182;&#20943;&#23569;&#25968;&#25454;&#35201;&#27714;&#65292;&#36890;&#36807;&#20351;&#29992;&#39134;&#34892;&#26102;&#38388;&#25668;&#20687;&#22836;&#36827;&#34892;&#30828;&#20214;&#22686;&#24378;&#21644;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#12289;&#20010;&#24615;&#21270;&#36866;&#24212;&#21644;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#31639;&#27861;&#22686;&#24378;&#12290;&#35813;&#26041;&#27861;&#22312;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#35782;&#21035;&#39046;&#22495;&#26377;&#30528;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#20026;&#20010;&#20154;&#29992;&#25143;&#23450;&#21046;&#65292;&#20174;&#32780;&#25552;&#21319;&#36710;&#20869;&#20132;&#20114;&#30340;&#23433;&#20840;&#24615;&#21644;&#20415;&#21033;&#24615;&#65292;&#24182;&#25552;&#39640;&#39550;&#39542;&#32773;&#30340;&#20307;&#39564;&#21644;&#23545;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25163;&#21183;&#35782;&#21035;&#25216;&#26415;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#65292;&#20294;&#22312;&#39550;&#39542;&#29615;&#22659;&#20013;&#35782;&#21035;&#25163;&#21183;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25968;&#25454;&#26377;&#38480;&#19988;&#26114;&#36149;&#65292;&#32780;&#19988;&#20855;&#26377;&#21160;&#24577;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20010;&#24615;&#21270;&#35757;&#32451;CNNLSTM&#27169;&#22411;&#65292;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#35782;&#21035;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26356;&#39640;&#25928;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#21046;&#32473;&#20010;&#20154;&#29992;&#25143;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#36710;&#20869;&#20132;&#20114;&#30340;&#23433;&#20840;&#24615;&#21644;&#20415;&#21033;&#24615;&#65292;&#20197;&#21450;&#39550;&#39542;&#32773;&#30340;&#20307;&#39564;&#21644;&#31995;&#32479;&#20449;&#20219;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39134;&#34892;&#26102;&#38388;&#25668;&#20687;&#22836;&#36827;&#34892;&#30828;&#20214;&#22686;&#24378;&#21644;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#12289;&#20010;&#24615;&#21270;&#36866;&#24212;&#21644;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#31639;&#27861;&#22686;&#24378;&#12290;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#20934;&#30830;&#29575;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;90\%&#65292;&#24182;&#23637;&#31034;&#20102;&#20010;&#24615;&#21270;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advances in gesture recognition technology, recognizing gestures in a driving environment remains challenging due to limited and costly data and its dynamic, ever-changing nature. In this work, we propose a model-adaptation approach to personalize the training of a CNNLSTM model and improve recognition accuracy while reducing data requirements. Our approach contributes to the field of dynamic hand gesture recognition while driving by providing a more efficient and accurate method that can be customized for individual users, ultimately enhancing the safety and convenience of in-vehicle interactions, as well as driver's experience and system trust. We incorporate hardware enhancement using a time-of-flight camera and algorithmic enhancement through data augmentation, personalized adaptation, and incremental learning techniques. We evaluate the performance of our approach in terms of recognition accuracy, achieving up to 90\%, and show the effectiveness of personalized
&lt;/p&gt;</description></item><item><title>CoDBench&#26159;&#19968;&#20010;&#23545;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#28085;&#30422;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;4&#31181;&#19981;&#21516;&#31867;&#21035;&#30340;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#26377;&#21161;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2310.01650</link><description>&lt;p&gt;
CoDBench: &#23545;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#37325;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems. (arXiv:2310.01650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01650
&lt;/p&gt;
&lt;p&gt;
CoDBench&#26159;&#19968;&#20010;&#23545;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#28085;&#30422;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;4&#31181;&#19981;&#21516;&#31867;&#21035;&#30340;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#26377;&#21161;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#36890;&#36807;&#24494;&#20998;&#26041;&#31243;&#26469;&#24314;&#27169;&#65292;&#24191;&#27867;&#29992;&#20110;&#27169;&#25311;&#35832;&#22914;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#12289;&#22810;&#23380;&#20171;&#36136;&#27969;&#21160;&#12289;&#22825;&#27668;&#39044;&#25253;&#21644;&#27969;&#34892;&#30149;&#21160;&#21147;&#23398;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#24314;&#27169;&#36825;&#20123;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#24050;&#24314;&#31435;&#30340;&#39046;&#22495;&#30456;&#27604;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#21035;&#30340;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#30740;&#31350;&#30456;&#23545;&#26377;&#38480;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CoDBench&#65292;&#19968;&#20010;&#21253;&#21547;11&#31181;&#26368;&#20808;&#36827;&#30340;&#35299;&#24494;&#20998;&#26041;&#31243;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20840;&#38754;&#22522;&#20934;&#22871;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;4&#31181;&#19981;&#21516;&#31867;&#21035;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#28145;&#24230;&#25805;&#20316;&#22238;&#24402;&#27169;&#22411;&#12289;&#22522;&#20110;&#39057;&#29575;&#30340;&#31070;&#32463;&#31639;&#23376;&#21644;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#24182;&#19982;8&#20010;&#24191;&#27867;&#36866;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous dynamical systems, characterized by differential equations, are ubiquitously used to model several important problems: plasma dynamics, flow through porous media, weather forecasting, and epidemic dynamics. Recently, a wide range of data-driven models has been used successfully to model these systems. However, in contrast to established fields like computer vision, limited studies are available analyzing the strengths and potential applications of different classes of these models that could steer decision-making in scientific machine learning. Here, we introduce CodBench, an exhaustive benchmarking suite comprising 11 state-of-the-art data-driven models for solving differential equations. Specifically, we comprehensively evaluate 4 distinct categories of models, viz., feed forward neural networks, deep operator regression models, frequency-based neural operators, and transformer architectures against 8 widely applicable benchmark datasets encompassing challenges from fluid 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU&#28608;&#27963;&#20989;&#25968;&#65288;IReLU&#65289;&#26469;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#65292;&#24182;&#25506;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#23545;&#20110;&#31283;&#23450;DC&#35757;&#32451;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#12289;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2310.01649</link><description>&lt;p&gt;
&#20851;&#20110;&#35757;&#32451;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On Training Derivative-Constrained Neural Networks. (arXiv:2310.01649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU&#28608;&#27963;&#20989;&#25968;&#65288;IReLU&#65289;&#26469;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#65292;&#24182;&#25506;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#23545;&#20110;&#31283;&#23450;DC&#35757;&#32451;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#12289;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#30340;&#39044;&#27979;&#30456;&#23545;&#20110;&#36755;&#20837;&#30340;&#65288;&#37096;&#20998;&#65289;&#23548;&#25968;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#30340;&#24773;&#20917;&#31216;&#20043;&#20026;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#30340;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU (IReLU)&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#20197;&#24110;&#21161;&#31283;&#23450;DC&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#37327;&#23376;&#21270;&#23398;&#21644;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20219;&#21153;&#22312;&#20869;&#30340;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#32467;&#21512;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We refer to the setting where the (partial) derivatives of a neural network's (NN's) predictions with respect to its inputs are used as additional training signal as a derivative-constrained (DC) NN. This situation is common in physics-informed settings in the natural sciences. We propose an integrated RELU (IReLU) activation function to improve training of DC NNs. We also investigate denormalization and label rescaling to help stabilize DC training. We evaluate our methods on physics-informed settings including quantum chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that existing architectures with IReLU activations combined with denormalization and label rescaling better incorporate training signal provided by derivative constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21629;&#21517;&#24815;&#20363;&#21644;&#30456;&#20851;&#32570;&#38519;&#65292;&#20026;&#25105;&#20204;&#20102;&#35299;&#30740;&#31350;&#21040;&#23454;&#36341;&#36807;&#31243;&#25552;&#20379;&#20102;&#30693;&#35782;&#21644;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.01642</link><description>&lt;p&gt;
&#25506;&#32034;Hugging Face&#21644;&#20854;&#20182;&#27169;&#22411;&#20179;&#24211;&#20013;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21629;&#21517;&#24815;&#20363;&#65288;&#21450;&#32570;&#38519;&#65289;
&lt;/p&gt;
&lt;p&gt;
Exploring Naming Conventions (and Defects) of Pre-trained Deep Learning Models in Hugging Face and Other Model Hubs. (arXiv:2310.01642v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21629;&#21517;&#24815;&#20363;&#21644;&#30456;&#20851;&#32570;&#38519;&#65292;&#20026;&#25105;&#20204;&#20102;&#35299;&#30740;&#31350;&#21040;&#23454;&#36341;&#36807;&#31243;&#25552;&#20379;&#20102;&#30693;&#35782;&#21644;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21019;&#26032;&#19981;&#26029;&#25512;&#36827;&#65292;&#35768;&#22810;&#24037;&#31243;&#24072;&#24076;&#26395;&#23558;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;PTMs&#65289;&#20316;&#20026;&#35745;&#31639;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;PTMs&#26159;&#30740;&#31350;&#21040;&#23454;&#36341;&#30340;&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#65306;&#30740;&#31350;&#20154;&#21592;&#21457;&#24067;PTMs&#65292;&#24037;&#31243;&#24072;&#26681;&#25454;&#36136;&#37327;&#25110;&#24615;&#33021;&#36827;&#34892;&#35843;&#25972;&#24182;&#37096;&#32626;&#12290;&#22914;&#26524;PTM&#30340;&#20316;&#32773;&#20026;&#20854;&#36873;&#25321;&#36866;&#24403;&#30340;&#21517;&#31216;&#65292;&#21487;&#20197;&#20419;&#36827;&#27169;&#22411;&#30340;&#21457;&#29616;&#21644;&#22797;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25253;&#36947;&#20102;&#27169;&#22411;&#21517;&#31216;&#24182;&#19981;&#24635;&#26159;&#36873;&#25321;&#24471;&#24456;&#22909;&#65292;&#26377;&#26102;&#29978;&#33267;&#26159;&#38169;&#35823;&#30340;&#12290;PTM&#21253;&#30340;&#21629;&#21517;&#24815;&#20363;&#21644;&#21629;&#21517;&#32570;&#38519;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#20102;&#35299;&#23427;&#20204;&#23558;&#22686;&#21152;&#25105;&#20204;&#23545;PTM&#21253;&#30340;&#30740;&#31350;&#21040;&#23454;&#36341;&#36807;&#31243;&#36816;&#20316;&#26041;&#24335;&#30340;&#35748;&#35782;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#23545;PTM&#21629;&#21517;&#24815;&#20363;&#21450;&#30456;&#20851;&#21629;&#21517;&#32570;&#38519;&#30340;&#39318;&#27425;&#30740;&#31350;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;PTM&#21253;&#21517;&#31216;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#20803;&#25968;&#25454;&#20013;&#30340;&#21253;&#21517;&#31216;&#21644;&#22768;&#26126;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31532;&#19968;&#39033;&#26088;&#22312;&#25551;&#36848;PTM&#21629;&#21517;&#24615;&#36136;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As innovation in deep learning continues, many engineers want to adopt Pre-Trained deep learning Models (PTMs) as components in computer systems. PTMs are part of a research-to-practice pipeline: researchers publish PTMs, which engineers adapt for quality or performance and then deploy. If PTM authors choose appropriate names for their PTMs, it could facilitate model discovery and reuse. However, prior research has reported that model names are not always well chosen, and are sometimes erroneous. The naming conventions and naming defects for PTM packages have not been systematically studied - understanding them will add to our knowledge of how the research-to-practice process works for PTM packages  In this paper, we report the first study of PTM naming conventions and the associated PTM naming defects. We define the components of a PTM package name, comprising the package name and claimed architecture from the metadata. We present the first study focused on characterizing the nature o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;ILfO&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation through Optimal Transport. (arXiv:2310.01632v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;ILfO&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;ILfO&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#32773;&#35797;&#22270;&#22312;&#27809;&#26377;&#30452;&#25509;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#30340;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;IL&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#26681;&#25454;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#30340;&#29366;&#24577;&#36712;&#36857;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#29983;&#25104;&#22870;&#21169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20026;&#29983;&#25104;&#26080;&#38656;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#36866;&#29992;&#20110;ILfO&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#21482;&#35266;&#23519;&#21333;&#20010;&#19987;&#23478;&#36712;&#36857;&#32780;&#27809;&#26377;&#21160;&#20316;&#65292;&#23427;&#22312;ILfO&#35774;&#32622;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation (ILfO) is a setting in which a learner tries to imitate the behavior of an expert, using only observational data and without the direct guidance of demonstrated actions. In this paper, we re-examine the use of optimal transport for IL, in which a reward is generated based on the Wasserstein distance between the state trajectories of the learner and expert. We show that existing methods can be simplified to generate a reward function without requiring learned models or adversarial learning. Unlike many other state-of-the-art methods, our approach can be integrated with any RL algorithm, and is amenable to ILfO. We demonstrate the effectiveness of this simple approach on a variety of continuous control tasks and find that it surpasses the state of the art in the IlfO setting, achieving expert-level performance across a range of evaluation domains even when observing only a single expert trajectory without actions.
&lt;/p&gt;</description></item><item><title>VAL&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31526;&#21495;&#38598;&#25104;&#30340;&#29702;&#24565;&#65292;&#23454;&#29616;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#30340;&#33719;&#21462;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#35299;&#37322;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#22312;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#23454;&#39564;&#34920;&#26126;&#65292;VAL&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.01627</link><description>&lt;p&gt;
VAL&#65306;&#24102;&#26377;GPT&#23545;&#35805;&#35299;&#26512;&#30340;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VAL: Interactive Task Learning with GPT Dialog Parsing. (arXiv:2310.01627v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01627
&lt;/p&gt;
&lt;p&gt;
VAL&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31526;&#21495;&#38598;&#25104;&#30340;&#29702;&#24565;&#65292;&#23454;&#29616;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#30340;&#33719;&#21462;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#35299;&#37322;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#22312;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#23454;&#39564;&#34920;&#26126;&#65292;VAL&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#38656;&#35201;&#25968;&#30334;&#19975;&#20010;&#26679;&#26412;&#26469;&#29983;&#25104;&#38745;&#24577;&#30340;&#40657;&#31665;&#27169;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#65288;ITL&#65289;&#24378;&#35843;&#20174;&#20154;&#31867;&#25552;&#20379;&#30340;&#26377;&#38480;&#25351;&#20196;&#20013;&#36880;&#27493;&#33719;&#24471;&#30693;&#35782;&#65292;&#36825;&#20123;&#25351;&#20196;&#20197;&#33258;&#28982;&#35821;&#35328;&#31561;&#24418;&#24335;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;ITL&#31995;&#32479;&#24448;&#24448;&#21463;&#21040;&#33030;&#24369;&#12289;&#23481;&#26131;&#20986;&#38169;&#30340;&#35821;&#35328;&#35299;&#26512;&#30340;&#22256;&#25200;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#33030;&#24369;&#24615;&#26377;&#19968;&#23450;&#30340;&#25269;&#25239;&#33021;&#21147;&#65292;&#20294;&#19981;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#65292;&#20063;&#26080;&#27861;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VAL&#65292;&#19968;&#31181;&#20855;&#26377;&#26032;&#30340;LLM/&#31526;&#21495;&#38598;&#25104;&#29702;&#24565;&#30340;ITL&#31995;&#32479;&#12290;&#36890;&#36807;&#20165;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20351;&#29992;LLMs&#65288;&#20363;&#22914;&#35859;&#35789;&#21644;&#21442;&#25968;&#36873;&#25321;&#65289;&#65292;&#22312;&#31639;&#27861;&#26694;&#26550;&#20869;&#65292;VAL&#21033;&#29992;LLMs&#30340;&#20248;&#21183;&#65292;&#25903;&#25345;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#20132;&#20114;&#24335;&#23398;&#20064;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25903;&#25345;&#25191;&#34892;&#26032;&#20219;&#21153;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#29992;&#25143;&#19982;VAL&#30340;&#20132;&#20114;&#65292;&#21457;&#29616;&#22823;&#37096;&#20998;&#29992;&#25143;&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning often requires millions of examples to produce static, black-box models. In contrast, interactive task learning (ITL) emphasizes incremental knowledge acquisition from limited instruction provided by humans in modalities such as natural language. However, in practice, ITL systems often suffers from brittle, error-prone language parsing. Large language models (LLMs) are resistant to brittleness but are not interpretable and cannot learn incrementally. We present VAL, an ITL system with a new philosophy for LLM/symbolic integration. By using LLMs only for specific tasks -- such as predicate and argument selection -- within an algorithmic framework, VAL reaps the benefits of LLMs to support interactive learning of hierarchical task knowledge from natural language. Acquired knowledge is human interpretable and generalizes to support execution of novel tasks without additional training. We studied users' interactions with VAL in a video game setting, finding that most
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;</title><link>http://arxiv.org/abs/2310.01616</link><description>&lt;p&gt;
&#22810;&#25209;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#65306;&#23545;&#20110;&#32500;&#24230;&#30456;&#20851;&#30340;&#36866;&#24212;&#24615;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity. (arXiv:2310.01616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22914;&#26524;&#31639;&#27861;&#22312;&#38382;&#39064;&#30340;&#32500;&#24230;d&#20013;&#20351;&#29992;&#30340;&#29615;&#22659;&#26597;&#35810;&#27425;&#25968;n&#26159;&#22810;&#39033;&#24335;&#30340;&#65292;&#37027;&#20040;&#23427;&#26159;&#26679;&#26412;&#25928;&#29575;&#30340;&#12290;&#36866;&#24212;&#24615;&#26159;&#25351;&#26597;&#35810;&#34987;&#21457;&#36865;&#21644;&#21453;&#39304;&#34987;&#22788;&#29702;&#20197;&#26356;&#26032;&#26597;&#35810;&#31574;&#30053;&#30340;&#39057;&#29575;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#20801;&#35768;&#22312;K&#20010;&#25209;&#27425;&#20013;&#21457;&#36865;&#26597;&#35810;&#65292;&#22312;&#27599;&#20010;&#25209;&#27425;&#20043;&#21518;&#22788;&#29702;&#21453;&#39304;&#24182;&#26356;&#26032;&#26597;&#35810;&#12290;&#36825;&#20010;&#27169;&#22411;&#21253;&#25324;&#25972;&#20010;&#36866;&#24212;&#24615;&#35889;&#65292;&#20174;&#38750;&#33258;&#36866;&#24212;&#30340;&#8220;&#31163;&#32447;&#8221;&#65288;K=1&#65289;&#21040;&#23436;&#20840;&#33258;&#36866;&#24212;&#65288;K=n&#65289;&#30340;&#22330;&#26223;&#65292;&#20197;&#21450;&#20013;&#38388;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#31574;&#30053;&#35780;&#20272;&#21644;&#22312;d&#32500;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#26679;&#26412;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#24314;&#31435;&#20102;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377; &#937;(log log d) &#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;
&lt;/p&gt;
&lt;p&gt;
We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning. An algorithm is sample-efficient if it uses a number of queries $n$ to the environment that is polynomial in the dimension $d$ of the problem. Adaptivity refers to the frequency at which queries are sent and feedback is processed to update the querying strategy. To investigate this interplay, we employ a learning framework that allows sending queries in $K$ batches, with feedback being processed and queries updated after each batch. This model encompasses the whole adaptivity spectrum, ranging from non-adaptive 'offline' ($K=1$) to fully adaptive ($K=n$) scenarios, and regimes in between. For the problems of policy evaluation and best-policy identification under $d$-dimensional linear function approximation, we establish $\Omega(\log \log d)$ lower bounds on the number of batches $K$ required for sample-efficient algorithms with $n = O(poly(d))$ queries. Our results show that j
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#25351;&#38024;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#29305;&#23450;&#23454;&#20363;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.01604</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving the Quadratic Assignment Problem using Deep Reinforcement Learning. (arXiv:2310.01604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#25351;&#38024;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#29305;&#23450;&#23454;&#20363;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#27425;&#20998;&#37197;&#38382;&#39064; (QAP) &#26159;&#19968;&#20010; NP &#22256;&#38590;&#38382;&#39064;&#65292;&#23545;&#20854;&#36827;&#34892;&#35299;&#20915;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65306;&#19982;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064; (TSP) &#31561;&#20854;&#20182;&#32452;&#21512;&#38382;&#39064;&#19981;&#21516;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#25972;&#25968;&#35268;&#21010;&#25216;&#26415;&#21487;&#20197;&#22312;&#21253;&#21547;&#25968;&#30334;&#29978;&#33267;&#25968;&#21315;&#20010;&#20301;&#32622;&#30340;&#23454;&#20363;&#19978;&#31934;&#30830;&#35299;&#20915;&#65292;&#23578;&#26410;&#25214;&#21040;&#35299;&#20915;&#22823;&#23567;&#36229;&#36807;30&#30340; QAP &#23454;&#20363;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915; QAP &#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#35768;&#22810;&#20851;&#38190;&#30340;&#24212;&#29992;&#65292;&#22914;&#30005;&#23376;&#24067;&#32447;&#35774;&#35745;&#21644;&#35774;&#22791;&#24067;&#23616;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915; QAP &#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#25351;&#38024;&#32593;&#32476;&#65292;&#36825;&#20010;&#32593;&#32476;&#22312;&#36873;&#25321;&#19979;&#19968;&#20010;&#20301;&#32622;&#26469;&#25918;&#32622;&#35774;&#26045;&#21644;&#36873;&#25321;&#21069;&#19968;&#20010;&#20301;&#32622;&#26469;&#25918;&#32622;&#35774;&#26045;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#23454;&#20363;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#22312; A2C &#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20135;&#29983;&#19981;&#38656;&#35201;&#29305;&#23450;&#23454;&#20363;&#37325;&#26032;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Quadratic Assignment Problem (QAP) is an NP-hard problem which has proven particularly challenging to solve: unlike other combinatorial problems like the traveling salesman problem (TSP), which can be solved to optimality for instances with hundreds or even thousands of locations using advanced integer programming techniques, no methods are known to exactly solve QAP instances of size greater than 30. Solving the QAP is nevertheless important because of its many critical applications, such as electronic wiring design and facility layout selection. We propose a method to solve the original Koopmans-Beckman formulation of the QAP using deep reinforcement learning. Our approach relies on a novel double pointer network, which alternates between selecting a location in which to place the next facility and a facility to place in the previous location. We train our model using A2C on a large dataset of synthetic instances, producing solutions with no instance-specific retraining necessary
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;K-12&#25945;&#32946;&#20013;&#25945;&#25480;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#23383;&#23398;&#20064;&#29615;&#22659;&#65292;&#24182;&#35752;&#35770;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#25903;&#25345;&#12289;&#35299;&#37322;&#24615;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#21162;&#21147;&#25913;&#36827;&#29616;&#26377;&#24037;&#20855;&#12289;&#24320;&#21457;&#26032;&#24037;&#20855;&#65292;&#24182;&#25506;&#32034;&#26356;&#26377;&#25928;&#21644;&#21253;&#23481;&#24615;&#30340;NLP&#25972;&#21512;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.01603</link><description>&lt;p&gt;
K-12&#25945;&#32946;&#20013;&#25945;&#25480;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#23383;&#23398;&#20064;&#29615;&#22659;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Digital Learning Environments for Teaching Natural Language Processing in K-12 Education. (arXiv:2310.01603v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01603
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;K-12&#25945;&#32946;&#20013;&#25945;&#25480;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#23383;&#23398;&#20064;&#29615;&#22659;&#65292;&#24182;&#35752;&#35770;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#25903;&#25345;&#12289;&#35299;&#37322;&#24615;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#21162;&#21147;&#25913;&#36827;&#29616;&#26377;&#24037;&#20855;&#12289;&#24320;&#21457;&#26032;&#24037;&#20855;&#65292;&#24182;&#25506;&#32034;&#26356;&#26377;&#25928;&#21644;&#21253;&#23481;&#24615;&#30340;NLP&#25972;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#24050;&#25104;&#20026;K-12&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38543;&#30528;&#23401;&#23376;&#20204;&#22312;NLP&#39537;&#21160;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#25104;&#38271;&#65292;&#21521;&#20182;&#20204;&#20171;&#32461;NLP&#27010;&#24565;&#23545;&#22521;&#20859;&#20182;&#20204;&#23545;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#35328;&#29983;&#25104;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#30340;&#20262;&#29702;&#38382;&#39064;&#30340;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;K-12&#25945;&#32946;&#20013;&#25945;&#25480;NLP&#30340;&#25968;&#23383;&#23398;&#20064;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25506;&#35752;&#20102;&#29616;&#26377;&#30340;&#25968;&#23383;&#23398;&#20064;&#24037;&#20855;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#29305;&#23450;&#30340;NLP&#20219;&#21153;&#21644;&#27969;&#31243;&#65292;&#24182;&#35843;&#26597;&#20102;&#23427;&#20204;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#35780;&#20272;&#32467;&#26524;&#12290;&#36890;&#36807;&#26816;&#26597;&#36825;&#20123;&#24037;&#20855;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#26412;&#25991;&#32508;&#36848;&#20026;&#25105;&#20204;&#20102;&#35299;&#24403;&#21069;K-12&#25945;&#32946;&#20013;NLP&#23398;&#20064;&#24037;&#20855;&#30340;&#29616;&#29366;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#23427;&#26088;&#22312;&#24341;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#21162;&#21147;&#65292;&#20197;&#25913;&#36827;&#29616;&#26377;&#24037;&#20855;&#12289;&#24320;&#21457;&#26032;&#24037;&#20855;&#65292;&#24182;&#25506;&#32034;&#26356;&#26377;&#25928;&#21644;&#21253;&#23481;&#24615;&#30340;NLP&#25972;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) plays a significant role in our daily lives and has become an essential part of Artificial Intelligence (AI) education in K-12. As children grow up with NLP-powered applications, it is crucial to introduce NLP concepts to them, fostering their understanding of language processing, language generation, and ethical implications of AI and NLP. This paper presents a comprehensive review of digital learning environments for teaching NLP in K-12. Specifically, it explores existing digital learning tools, discusses how they support specific NLP tasks and procedures, and investigates their explainability and evaluation results in educational contexts. By examining the strengths and limitations of these tools, this literature review sheds light on the current state of NLP learning tools in K-12 education. It aims to guide future research efforts to refine existing tools, develop new ones, and explore more effective and inclusive strategies for integrating NLP i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#40784;&#30340;&#20195;&#30721;&#21644;&#27979;&#35797;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;CAT-LM&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19982;&#20154;&#31867;&#32534;&#20889;&#20195;&#30721;&#38750;&#24120;&#30456;&#20284;&#30340;&#20195;&#30721;&#12290;&#23427;&#21033;&#29992;&#20102;&#26144;&#23556;&#20449;&#21495;&#26469;&#32771;&#34385;&#20195;&#30721;&#21644;&#27979;&#35797;&#25991;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22823;&#24133;&#22686;&#21152;&#20102;&#36755;&#20837;&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.01602</link><description>&lt;p&gt;
CAT-LM: &#22312;&#23545;&#40784;&#30340;&#20195;&#30721;&#21644;&#27979;&#35797;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAT-LM: Training Language Models on Aligned Code And Tests. (arXiv:2310.01602v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#40784;&#30340;&#20195;&#30721;&#21644;&#27979;&#35797;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;CAT-LM&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19982;&#20154;&#31867;&#32534;&#20889;&#20195;&#30721;&#38750;&#24120;&#30456;&#20284;&#30340;&#20195;&#30721;&#12290;&#23427;&#21033;&#29992;&#20102;&#26144;&#23556;&#20449;&#21495;&#26469;&#32771;&#34385;&#20195;&#30721;&#21644;&#27979;&#35797;&#25991;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22823;&#24133;&#22686;&#21152;&#20102;&#36755;&#20837;&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26159;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#32534;&#20889;&#27979;&#35797;&#26159;&#32791;&#26102;&#19988;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#12290;&#32463;&#20856;&#30340;&#27979;&#35797;&#29983;&#25104;&#24037;&#20855;&#22914;EvoSuite&#36890;&#36807;&#20248;&#21270;&#35206;&#30422;&#29575;&#29983;&#25104;&#34892;&#20026;&#27979;&#35797;&#22871;&#20214;&#65292;&#20294;&#24448;&#24448;&#20135;&#29983;&#38590;&#20197;&#29702;&#35299;&#30340;&#27979;&#35797;&#12290;&#22312;&#29616;&#26377;&#30340;&#20195;&#30721;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#22823;&#22810;&#25968;&#26159;&#23558;&#27599;&#20010;&#25991;&#20214;&#21333;&#29420;&#35757;&#32451;&#29983;&#25104;&#65292;&#36825;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26159;&#26631;&#20934;&#20570;&#27861;&#65292;&#22240;&#27492;&#22312;&#29983;&#25104;&#27979;&#35797;&#25991;&#20214;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#20195;&#30721;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Aligned Code And Tests Language Model (CAT-LM)&#65292;&#19968;&#20010;2.7&#20159;&#21442;&#25968;&#30340;GPT&#39118;&#26684;&#35821;&#35328;&#27169;&#22411;&#65292;&#35757;&#32451;&#29992;&#20110;Python&#21644;Java&#39033;&#30446;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#20449;&#21495;&#65292;&#22312;&#26377;&#21487;&#29992;&#26102;&#26174;&#24335;&#32771;&#34385;&#20195;&#30721;&#19982;&#27979;&#35797;&#25991;&#20214;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#22823;&#24133;&#25552;&#39640;&#20102;&#36755;&#20837;&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#65292;&#36798;&#21040;8192&#20010;&#26631;&#35760;&#65292;&#26159;&#20043;&#21069;&#30340;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Testing is an integral part of the software development process. Yet, writing tests is time-consuming and therefore often neglected. Classical test generation tools such as EvoSuite generate behavioral test suites by optimizing for coverage, but tend to produce tests that are hard to understand. Language models trained on code can generate code that is highly similar to that written by humans, but current models are trained to generate each file separately, as is standard practice in natural language processing, and thus fail to consider the code-under-test context when producing a test file. In this work, we propose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style language model with 2.7 Billion parameters, trained on a corpus of Python and Java projects. We utilize a novel pretraining signal that explicitly considers the mapping between code and test files when available. We also drastically increase the maximum sequence length of inputs to 8,192 tokens, 4x more than t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#31890;&#23376;&#28388;&#27874;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;mePFRNN&#65289;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#31890;&#23376;&#28388;&#27874;&#19982;GRU RNN&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#29289;&#20307;&#23450;&#20301;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mePFRNN&#27169;&#22411;&#22312;&#23545;&#31216;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#23450;&#20301;&#31934;&#24230;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#27169;&#22411;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.01595</link><description>&lt;p&gt;
&#20869;&#23384;&#39640;&#25928;&#30340;&#31890;&#23376;&#28388;&#27874;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#29289;&#20307;&#23450;&#20301;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient particle filter recurrent neural network for object localization. (arXiv:2310.01595v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01595
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#31890;&#23376;&#28388;&#27874;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;mePFRNN&#65289;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#31890;&#23376;&#28388;&#27874;&#19982;GRU RNN&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#29289;&#20307;&#23450;&#20301;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mePFRNN&#27169;&#22411;&#22312;&#23545;&#31216;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#23450;&#20301;&#31934;&#24230;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#27169;&#22411;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#39640;&#25928;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#29289;&#20307;&#23450;&#20301;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#26159;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#24674;&#22797;&#29289;&#20307;&#29366;&#24577;&#21450;&#20854;&#31227;&#21160;&#36712;&#36857;&#12290;&#25105;&#20204;&#23558;&#32463;&#20856;&#31890;&#23376;&#28388;&#27874;&#30340;&#24605;&#24819;&#19982;GRU RNN&#26550;&#26500;&#30456;&#32467;&#21512;&#12290;&#24471;&#21040;&#30340;&#20869;&#23384;&#39640;&#25928;&#31890;&#23376;&#28388;&#27874;RNN&#27169;&#22411;&#65288;mePFRNN&#65289;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65292;&#23427;&#22312;&#22788;&#29702;&#19981;&#21516;&#22823;&#23567;&#30340;&#29615;&#22659;&#26102;&#38656;&#35201;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#19982;&#20197;&#21069;&#25552;&#20986;&#30340;PFRNN&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;mePFRNN&#26550;&#26500;&#38656;&#35201;&#26356;&#23569;&#30340;&#23384;&#20648;&#21442;&#25968;&#20869;&#23384;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#21040;&#23545;&#20110;&#28388;&#27874;&#31639;&#27861;&#32780;&#35328;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#23545;&#31216;&#22024;&#26434;&#29615;&#22659;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;mePFRNN&#27169;&#22411;&#25552;&#20379;&#27604;&#20854;&#20182;&#31454;&#20105;&#27169;&#22411;&#26356;&#31934;&#30830;&#30340;&#23450;&#20301;&#32467;&#26524;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a novel memory-efficient recurrent neural network (RNN) architecture specified to solve the object localization problem. This problem is to recover the object states along with its movement in a noisy environment. We take the idea of the classical particle filter and combine it with GRU RNN architecture. The key feature of the resulting memory-efficient particle filter RNN model (mePFRNN) is that it requires the same number of parameters to process environments of different sizes. Thus, the proposed mePFRNN architecture consumes less memory to store parameters compared to the previously proposed PFRNN model. To demonstrate the performance of our model, we test it on symmetric and noisy environments that are incredibly challenging for filtering algorithms. In our experiments, the mePFRNN model provides more precise localization than the considered competitors and requires fewer trained parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#36827;&#34892;&#35268;&#23450;&#28779;&#27169;&#25311;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#29289;&#29702;&#19981;&#19968;&#33268;&#24615;&#21644;&#39044;&#27979;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01593</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#35268;&#23450;&#28779;&#27169;&#25311;&#65292;&#29992;&#20110;&#22303;&#22320;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Prescribed Fire Modeling using Knowledge-Guided Machine Learning for Land Management. (arXiv:2310.01593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#36827;&#34892;&#35268;&#23450;&#28779;&#27169;&#25311;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#29289;&#29702;&#19981;&#19968;&#33268;&#24615;&#21644;&#39044;&#27979;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28798;&#28909;&#21457;&#29983;&#22823;&#28779;&#30340;&#23041;&#32961;&#19981;&#26029;&#21152;&#21095;&#65292;&#22240;&#27492;&#38656;&#35201;&#26377;&#25928;&#30340;&#35268;&#23450;&#28779;&#31649;&#29702;&#12290;&#20256;&#32479;&#19978;&#65292;&#22522;&#20110;&#36807;&#31243;&#30340;&#35745;&#31639;&#26426;&#27169;&#25311;&#34987;&#29992;&#20110;&#35268;&#21010;&#39044;&#38450;&#37326;&#28779;&#30340;&#35268;&#23450;&#28779;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#31616;&#21270;&#30340;&#36807;&#31243;&#27169;&#22411;&#22914;QUIC-Fire&#20063;&#36807;&#20110;&#35745;&#31639;&#23494;&#38598;&#65292;&#26080;&#27861;&#29992;&#20110;&#23454;&#26102;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#24403;&#22825;&#27668;&#26465;&#20214;&#36805;&#36895;&#21464;&#21270;&#26102;&#12290;&#20256;&#32479;&#30340;&#28779;&#28798;&#24314;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#35745;&#31639;&#36895;&#24230;&#21152;&#24555;&#65292;&#20294;&#22312;&#29289;&#29702;&#19978;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#65292;&#30001;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#32780;&#24341;&#36215;&#30340;&#26377;&#20559;&#39044;&#27979;&#65292;&#28779;&#28798;&#34067;&#24310;&#25351;&#26631;&#65288;&#22914;&#29123;&#28903;&#38754;&#31215;&#12289;&#34067;&#24310;&#36895;&#24230;&#65289;&#30340;&#26377;&#20559;&#20272;&#35745;&#20197;&#21450;&#22312;&#20998;&#24067;&#22806;&#30340;&#39118;&#26465;&#20214;&#19979;&#30340;&#21487;&#25512;&#24191;&#24615;&#19978;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#24555;&#36895;&#27169;&#25311;&#35268;&#23450;&#28779;&#65292;&#24182;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#20943;&#23569;&#29123;&#26009;&#23494;&#24230;&#20272;&#35745;&#20013;&#30340;&#29289;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the increasing threat of devastating wildfires has underscored the need for effective prescribed fire management. Process-based computer simulations have traditionally been employed to plan prescribed fires for wildfire prevention. However, even simplified process models like QUIC-Fire are too compute-intensive to be used for real-time decision-making, especially when weather conditions change rapidly. Traditional ML methods used for fire modeling offer computational speedup but struggle with physically inconsistent predictions, biased predictions due to class imbalance, biased estimates for fire spread metrics (e.g., burned area, rate of spread), and generalizability in out-of-distribution wind conditions. This paper introduces a novel machine learning (ML) framework that enables rapid emulation of prescribed fires while addressing these concerns. By incorporating domain knowledge, the proposed method helps reduce physical inconsistencies in fuel density estimates in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#25351;&#20986;&#23545;&#40784;&#19981;&#33021;&#30495;&#27491;&#38450;&#27490;&#23427;&#20204;&#34987;&#28389;&#29992;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#26041;&#24335;&#35823;&#23548;&#23427;&#20204;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.01581</link><description>&lt;p&gt;
&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65306;&#23545;&#40784;&#26159;&#21542;&#30495;&#27491;&#38450;&#27490;&#23427;&#20204;&#34987;&#28389;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?. (arXiv:2310.01581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#25351;&#20986;&#23545;&#40784;&#19981;&#33021;&#30495;&#27491;&#38450;&#27490;&#23427;&#20204;&#34987;&#28389;&#29992;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#26041;&#24335;&#35823;&#23548;&#23427;&#20204;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#21457;&#24067;LLMs&#20379;&#20844;&#20247;&#35775;&#38382;&#20043;&#21069;&#65292;&#27169;&#22411;&#24320;&#21457;&#32773;&#36890;&#24120;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#25110;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#23545;&#40784;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#28508;&#22312;&#26377;&#23475;&#25110;&#19981;&#36947;&#24503;&#30340;&#35831;&#27714;&#26102;&#20250;&#25298;&#32477;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#8220;&#23545;&#40784;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#38450;&#27490;&#36825;&#20123;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#65311;&#8221;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#32473;&#20986;&#20102;&#21542;&#23450;&#30340;&#31572;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#24320;&#28304;&#12289;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22797;&#26434;&#35745;&#31639;&#25110;&#20180;&#32454;&#35774;&#35745;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#34987;&#36731;&#26131;&#35823;&#23548;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#30452;&#25509;&#25805;&#32437;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is "could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation proc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#26102;&#21019;&#24314;&#25968;&#23383;&#27169;&#24335;&#21644;&#35782;&#21035;&#23427;&#20204;&#26469;&#25552;&#39640;&#29992;&#25143;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21644;&#29702;&#35299;&#12290;&#36890;&#36807;&#38598;&#25104;&#21487;&#35270;&#21270;&#21644;&#22810;&#31181;&#29992;&#25143;&#20132;&#20114;&#65292;&#24110;&#21161;&#29992;&#25143;&#28165;&#26970;&#22320;&#29702;&#35299;&#25968;&#23383;&#27169;&#24335;&#21644;&#23427;&#20204;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26524;&#30340;&#35270;&#35273;&#24046;&#24322;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01580</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#29983;&#25104;&#25968;&#23383;&#27169;&#24335;&#21644;&#35270;&#35273;&#34920;&#31034;&#22312;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning on Neural Networks through Interactive Generation of Digit Patterns and Visual Representation. (arXiv:2310.01580v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#26102;&#21019;&#24314;&#25968;&#23383;&#27169;&#24335;&#21644;&#35782;&#21035;&#23427;&#20204;&#26469;&#25552;&#39640;&#29992;&#25143;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21644;&#29702;&#35299;&#12290;&#36890;&#36807;&#38598;&#25104;&#21487;&#35270;&#21270;&#21644;&#22810;&#31181;&#29992;&#25143;&#20132;&#20114;&#65292;&#24110;&#21161;&#29992;&#25143;&#28165;&#26970;&#22320;&#29702;&#35299;&#25968;&#23383;&#27169;&#24335;&#21644;&#23427;&#20204;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26524;&#30340;&#35270;&#35273;&#24046;&#24322;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#24191;&#27867;&#29992;&#20110;&#20998;&#26512;&#21508;&#31181;&#25968;&#25454;&#21644;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#38271;&#26399;&#20197;&#26469;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#40657;&#21283;&#23376;&#25805;&#20316;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24213;&#23618;&#35745;&#31639;&#21644;&#21547;&#20041;&#26159;&#38544;&#34255;&#30340;&#12290;&#30001;&#20110;&#36825;&#31181;&#29305;&#24615;&#65292;&#29992;&#25143;&#32463;&#24120;&#38754;&#20020;&#38590;&#20197;&#35299;&#37322;NNs&#24213;&#23618;&#26426;&#21046;&#21644;&#20351;&#29992;&#23427;&#20204;&#30340;&#22909;&#22788;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#26102;&#21019;&#24314;&#25968;&#23383;&#27169;&#24335;&#21644;&#35782;&#21035;&#23427;&#20204;&#26469;&#25552;&#39640;&#29992;&#25143;&#23545;NNs&#30340;&#23398;&#20064;&#21644;&#29702;&#35299;&#12290;&#20026;&#20102;&#24110;&#21161;&#29992;&#25143;&#28165;&#26970;&#22320;&#29702;&#35299;&#25968;&#23383;&#27169;&#24335;(&#21363;0~9)&#20197;&#21450;&#23427;&#20204;&#19982;NN&#30340;&#32467;&#26524;&#30340;&#35270;&#35273;&#24046;&#24322;&#65292;&#32771;&#34385;&#23558;&#21487;&#35270;&#21270;&#38598;&#25104;&#21040;&#19968;&#20010;&#20108;&#32500;&#26174;&#31034;&#31354;&#38388;&#20013;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#29992;&#25143;&#20132;&#20114;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#30830;&#23450;&#20854;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#22799;&#23395;&#24037;&#20316;&#26399;&#38388;&#36827;&#34892;&#20102;&#38750;&#27491;&#24335;&#30340;&#29992;&#25143;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) have been broadly utilized to analyze various data and solve different domain problems. However, neural networks (NNs) have been considered a black box operation for years because their underlying computation and meaning are hidden. Due to this nature, users often face difficulties in interpreting the underlying mechanism of the NNs and the benefits of using them. In this paper, to improve users' learning and understanding of NNs, an interactive learning system is designed to create digit patterns and recognize them in real time. To help users clearly understand the visual differences of digit patterns (i.e., 0 ~ 9) and their results with an NN, integrating visualization is considered to present all digit patterns in a two-dimensional display space with supporting multiple user interactions. An evaluation with multiple datasets is conducted to determine its usability for active learning. In addition, informal user testing is managed during a summer wor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#24335;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#20013;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.01569</link><description>&lt;p&gt;
&#36845;&#20195;&#24335;&#35268;&#21010;&#20013;&#30340;&#36873;&#39033;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Iterative Option Discovery for Planning, by Planning. (arXiv:2310.01569v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#24335;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#20013;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26377;&#29992;&#30340;&#26102;&#38388;&#25277;&#35937;&#65292;&#20063;&#23601;&#26159;&#36873;&#39033;&#65292;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#24212;&#29992;&#20110;&#26085;&#30410;&#22797;&#26434;&#30340;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#22312;AlphaZero&#20013;&#20351;&#29992;&#30340;Expert Iteration&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#30340;&#32463;&#39564;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;Option Iteration&#65292;&#19968;&#31181;&#31867;&#20284;&#30340;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#12290;Option Iteration&#19981;&#26159;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#30340;&#24378;&#31574;&#30053;&#65292;&#32780;&#26159;&#23398;&#20064;&#19968;&#32452;&#36873;&#39033;&#31574;&#30053;&#65292;&#23545;&#20110;&#36935;&#21040;&#30340;&#27599;&#20010;&#29366;&#24577;&#65292;&#33267;&#23569;&#26377;&#19968;&#31181;&#31574;&#30053;&#22312;&#26576;&#20010;&#26410;&#26469;&#30340;&#26102;&#38388;&#28857;&#19982;&#25628;&#32034;&#32467;&#26524;&#21563;&#21512;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36825;&#21487;&#33021;&#26356;&#23481;&#26131;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#31639;&#27861;&#26681;&#25454;&#24773;&#20917;&#28789;&#27963;&#35843;&#25972;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#19968;&#20010;&#22312;&#24403;&#21069;&#29366;&#24577;&#30340;&#32454;&#33410;&#19978;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#24615;&#30340;&#20840;&#23616;&#31574;&#30053;&#12290;&#36890;&#36807;&#23398;&#20064;&#36825;&#26679;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#24418;&#25104;&#33391;&#24615;&#24490;&#29615;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering useful temporal abstractions, in the form of options, is widely thought to be key to applying reinforcement learning and planning to increasingly complex domains. Building on the empirical success of the Expert Iteration approach to policy learning used in AlphaZero, we propose Option Iteration, an analogous approach to option discovery. Rather than learning a single strong policy that is trained to match the search results everywhere, Option Iteration learns a set of option policies trained such that for each state encountered, at least one policy in the set matches the search results for some horizon into the future. Intuitively, this may be significantly easier as it allows the algorithm to hedge its bets compared to learning a single globally strong policy, which may have complex dependencies on the details of the current state. Having learned such a set of locally strong policies, we can use them to guide the search algorithm resulting in a virtuous cycle where better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01558</link><description>&lt;p&gt;
&#20351;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Retrieval-Augmented Language Models Robust to Irrelevant Context. (arXiv:2310.01558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#26377;&#26395;&#20135;&#29983;&#20934;&#30830;&#12289;&#39640;&#25928;&#21644;&#26368;&#26032;&#30340;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#12290;RALMs&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#22312;&#30456;&#20851;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#19981;&#30456;&#20851;&#26102;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#22312;&#22810;&#36339;&#25512;&#29702;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#19981;&#30456;&#20851;&#35777;&#25454;&#30340;&#35823;&#29992;&#20250;&#23548;&#33268;&#36830;&#38145;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26816;&#32034;&#22686;&#24378;&#26377;&#26102;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20116;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#22522;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#25551;&#36848;&#20102;&#26816;&#32034;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#32447;&#65292;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#31579;&#36873;&#20986;&#19981;&#28041;&#21450;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#26816;&#32034;&#27573;&#33853;&#12290;&#36825;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#20195;&#20215;&#26159;&#33293;&#24323;&#20102;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding releva
&lt;/p&gt;</description></item><item><title>SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01557</link><description>&lt;p&gt;
SmartPlay: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01557
&lt;/p&gt;
&lt;p&gt;
SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26234;&#33021;Agent&#21644;&#19979;&#19968;&#20195;&#33258;&#21160;&#21270;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SmartPlay&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#26041;&#27861;&#35770;&#12290;SmartPlay&#21253;&#25324;6&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#65292;&#21253;&#25324;&#21098;&#20992;&#30707;&#22836;&#24067;&#12289;&#27721;&#35834;&#22612;&#12289;Minecraft&#31561;&#12290;&#27599;&#20010;&#28216;&#25103;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#35774;&#32622;&#65292;&#25552;&#20379;&#26368;&#22810;20&#20010;&#35780;&#20272;&#35774;&#32622;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;SmartPlay&#20013;&#30340;&#27599;&#20010;&#28216;&#25103;&#37117;&#29420;&#29305;&#22320;&#25361;&#25112;&#20102;&#26234;&#33021;LLM Agent&#30340;9&#20010;&#37325;&#35201;&#33021;&#21147;&#30340;&#23376;&#38598;&#65292;&#21253;&#25324;&#23545;&#23545;&#35937;&#20381;&#36182;&#30340;&#25512;&#29702;&#12289;&#25552;&#21069;&#35268;&#21010;&#12289;&#31354;&#38388;&#25512;&#29702;&#12289;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#38543;&#26426;&#24615;&#12290;&#27599;&#20010;&#28216;&#25103;&#27979;&#35797;&#30340;&#33021;&#21147;&#38598;&#30340;&#21306;&#21035;&#20351;&#25105;&#20204;&#33021;&#22815;&#21333;&#29420;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#12290;SmartPlay&#19981;&#20165;&#26159;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#32780;&#19988;&#20063;&#26159;&#35780;&#20272;Agent&#22312;&#19981;&#21516;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#25321;&#33021;&#21147;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;Top-$k$&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#36138;&#23146;&#31639;&#27861;&#21644;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01551</link><description>&lt;p&gt;
&#21033;&#29992;&#36873;&#25321;&#30340;&#33021;&#21147;&#20248;&#21270;&#20915;&#31574;&#26641;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#25321;&#33021;&#21147;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;Top-$k$&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#36138;&#23146;&#31639;&#27861;&#21644;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23545;&#26631;&#20934;&#21644;&#32463;&#39564;&#25104;&#21151;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;ID3&#12289;C4.5&#21644;CART&#65289;&#36827;&#34892;&#25512;&#24191;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20973;&#20511;&#36138;&#23146;&#30340;&#29305;&#24615;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65306;&#23427;&#20204;&#36890;&#36807;&#36845;&#20195;&#22320;&#22522;&#20110;&#26368;&#20339;&#23646;&#24615;&#36827;&#34892;&#21010;&#20998;&#26469;&#26500;&#24314;&#20915;&#31574;&#26641;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;Top-$k$&#21017;&#32771;&#34385;$k$&#20010;&#26368;&#20339;&#23646;&#24615;&#20316;&#20026;&#21487;&#33021;&#30340;&#21010;&#20998;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#26368;&#20339;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#25512;&#24191;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#8220;&#36138;&#23146;&#23618;&#27425;&#23450;&#29702;&#8221;&#65292;&#23545;&#20110;&#27599;&#20010;$k \in \mathbb{N}$&#65292;Top-$(k+1)$&#27604;Top-$k$&#26356;&#21152;&#24378;&#22823;&#65306;&#22312;&#26576;&#20123;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#21069;&#32773;&#21487;&#20197;&#36798;&#21040;$1-\varepsilon$&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#21518;&#32773;&#21482;&#33021;&#36798;&#21040;$\frac1{2}+\varepsilon$&#30340;&#20934;&#30830;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;Top-$k$&#31639;&#27861;&#22312;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#20248;&#20110;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#32463;&#20856;&#36138;&#23146;&#31639;&#27861;&#21644;&#36739;&#26032;&#30340;&#8220;&#26368;&#20248;&#20915;&#31574;&#26641;&#8221;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as possible splits instead of just the single best attribute. We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a {\sl greediness hierarchy theorem} showing that for every $k \in \mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there are data distributions for which the former achieves accuracy $1-\varepsilon$, whereas the latter only achieves accuracy $\frac1{2}+\varepsilon$. We then show, through extensive experiments, that Top-$k$ outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent "optimal decis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20195;&#29702;&#30340;&#35270;&#35282;&#25552;&#21462;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#31616;&#21333;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#20986;&#29616;&#30340;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.01536</link><description>&lt;p&gt;
&#19968;&#20010;&#20195;&#29702;&#22312;&#19990;&#30028;&#34920;&#31034;&#20013;&#34892;&#21160;&#30340;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Algebras of actions in an agent's representations of the world. (arXiv:2310.01536v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20195;&#29702;&#30340;&#35270;&#35282;&#25552;&#21462;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#31616;&#21333;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#20986;&#29616;&#30340;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20174;&#19968;&#20010;&#20195;&#29702;&#30340;&#35270;&#35282;&#25552;&#21462;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#20174;&#23545;&#31216;&#24615;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;(SBDRL)&#30340;&#35282;&#24230;&#22797;&#29616;&#20102;&#23545;&#31216;&#24615;&#22522;&#30784;&#34920;&#31034;&#30340;&#24037;&#20316;[1]&#65292;&#21482;&#26377;&#24418;&#25104;&#32676;&#30340;&#19990;&#30028;&#36716;&#25442;&#20195;&#25968;&#25165;&#33021;&#29992;&#23545;&#31216;&#24615;&#22522;&#30784;&#34920;&#31034;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#31616;&#21333;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#20986;&#29616;&#30340;&#20855;&#26377;&#29305;&#24449;&#30340;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#24320;&#21457;&#30340;&#35745;&#31639;&#26041;&#27861;&#25552;&#21462;&#20102;&#36825;&#20123;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#30340;&#23646;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;SBDRL&#30340;&#20004;&#20010;&#37325;&#35201;&#32467;&#26524; - &#31561;&#21464;&#26465;&#20214;&#21644;&#20998;&#31163;&#23450;&#20041; - &#20174;&#20165;&#36866;&#29992;&#20110;&#23545;&#31216;&#24615;&#22522;&#30784;&#34920;&#31034;&#25193;&#23637;&#21040;&#36866;&#29992;&#20110;&#25429;&#25417;&#19990;&#30028;&#36716;&#25442;&#29305;&#24615;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a framework to extract the algebra of the transformations of worlds from the perspective of an agent. As a starting point, we use our framework to reproduce the symmetry-based representations from the symmetry-based disentangled representation learning (SBDRL) formalism proposed by [1]; only the algebra of transformations of worlds that form groups can be described using symmetry-based representations. We then study the algebras of the transformations of worlds with features that occur in simple reinforcement learning scenarios. Using computational methods, that we developed, we extract the algebras of the transformations of these worlds and classify them according to their properties. Finally, we generalise two important results of SBDRL - the equivariance condition and the disentangling definition - from only working with symmetry-based representations to working with representations capturing the transformation properties of worlds with transformations for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#39046;&#22495;&#30456;&#20851;&#30340;&#35270;&#35282;&#25429;&#25417;&#21040;&#22810;&#26679;&#21270;&#35268;&#21010;&#20013;&#20004;&#20010;&#35745;&#21010;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24357;&#34917;&#20102;&#24403;&#21069;&#20165;&#32771;&#34385;&#32467;&#26500;&#23646;&#24615;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01520</link><description>&lt;p&gt;
&#22312;&#22810;&#26679;&#21270;&#35268;&#21010;&#20013;&#24357;&#21512;&#32467;&#26500;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap between Structural and Semantic Similarity in Diverse Planning. (arXiv:2310.01520v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#39046;&#22495;&#30456;&#20851;&#30340;&#35270;&#35282;&#25429;&#25417;&#21040;&#22810;&#26679;&#21270;&#35268;&#21010;&#20013;&#20004;&#20010;&#35745;&#21010;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24357;&#34917;&#20102;&#24403;&#21069;&#20165;&#32771;&#34385;&#32467;&#26500;&#23646;&#24615;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#21270;&#35268;&#21010;&#26159;&#25214;&#21040;&#32473;&#23450;&#38382;&#39064;&#35268;&#23450;&#30340;&#22810;&#20010;&#35745;&#21010;&#30340;&#38382;&#39064;&#65292;&#23427;&#26159;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#26680;&#24515;&#12290;&#30446;&#21069;&#30340;&#22810;&#26679;&#21270;&#35268;&#21010;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#35745;&#21010;&#65292;&#28982;&#21518;&#20351;&#29992;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#25552;&#21462;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#20165;&#32771;&#34385;&#32473;&#23450;&#35745;&#21010;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#26377;&#26102;&#20250;&#38459;&#30861;&#36825;&#20123;&#24230;&#37327;&#26041;&#27861;&#25429;&#25417;&#21040;&#20004;&#20010;&#35745;&#21010;&#30340;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#39046;&#22495;&#30456;&#20851;&#30340;&#35270;&#35282;&#25429;&#25417;&#21040;&#20004;&#20010;&#32473;&#23450;&#35745;&#21010;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diverse planning is the problem of finding multiple plans for a given problem specification, which is at the core of many real-world applications. For example, diverse planning is a critical piece for the efficiency of plan recognition systems when dealing with noisy and missing observations. Providing diverse solutions can also benefit situations where constraints are too expensive or impossible to model. Current diverse planners operate by generating multiple plans and then applying a selection procedure to extract diverse solutions using a similarity metric. Generally, current similarity metrics only consider the structural properties of the given plans. We argue that this approach is a limitation that sometimes prevents such metrics from capturing why two plans differ. In this work, we propose two new domain-independent metrics which are able to capture relevant information on the difference between two given plans from a domain-dependent viewpoint. We showcase their utility in var
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#35774;&#35745;Factorio&#34013;&#22270;&#30340;&#26041;&#27861;&#65292;&#20026;&#29609;&#23478;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21270;&#24037;&#21378;&#25193;&#23637;&#21644;&#35774;&#35745;&#20998;&#20139;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.01505</link><description>&lt;p&gt;
&#33258;&#21160;&#35774;&#35745;Factorio&#34013;&#22270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Design of Factorio Blueprints. (arXiv:2310.01505v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#35774;&#35745;Factorio&#34013;&#22270;&#30340;&#26041;&#27861;&#65292;&#20026;&#29609;&#23478;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21270;&#24037;&#21378;&#25193;&#23637;&#21644;&#35774;&#35745;&#20998;&#20139;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Factorio&#26159;&#19968;&#27454;&#20851;&#20110;&#24314;&#36896;&#33258;&#21160;&#21270;&#24037;&#21378;&#20197;&#29983;&#20135;&#36234;&#26469;&#36234;&#22797;&#26434;&#29289;&#21697;&#30340;2D&#24314;&#35774;&#21644;&#31649;&#29702;&#27169;&#25311;&#35270;&#39057;&#28216;&#25103;&#12290;&#28216;&#25103;&#30340;&#19968;&#20010;&#26680;&#24515;&#29305;&#24615;&#26159;&#20854;&#34013;&#22270;&#31995;&#32479;&#65292;&#20801;&#35768;&#29609;&#23478;&#36731;&#26494;&#20445;&#23384;&#21644;&#22797;&#21046;&#20182;&#20204;&#35774;&#35745;&#30340;&#37096;&#20998;&#12290;&#34013;&#22270;&#21487;&#20197;&#22797;&#21046;&#28216;&#25103;&#20013;&#20219;&#20309;&#23545;&#35937;&#30340;&#24067;&#23616;&#65292;&#20294;&#36890;&#24120;&#29992;&#20110;&#23553;&#35013;&#22797;&#26434;&#30340;&#34892;&#20026;&#65292;&#22914;&#38750;&#22522;&#26412;&#29289;&#21697;&#30340;&#29983;&#20135;&#12290;&#19968;&#26086;&#21019;&#24314;&#65292;&#36825;&#20123;&#34013;&#22270;&#23601;&#21487;&#20197;&#29992;&#20316;&#22522;&#26412;&#26500;&#24314;&#22359;&#65292;&#20351;&#29609;&#23478;&#21487;&#20197;&#21019;&#24314;&#19968;&#23618;&#25277;&#35937;&#12290;&#20351;&#29992;&#34013;&#22270;&#19981;&#20165;&#20415;&#20110;&#25193;&#23637;&#24037;&#21378;&#65292;&#36824;&#20801;&#35768;&#19982;&#28216;&#25103;&#31038;&#21306;&#20998;&#20139;&#35774;&#35745;&#12290;&#34013;&#22270;&#20013;&#30340;&#24067;&#23616;&#21487;&#20197;&#26681;&#25454;&#21508;&#31181;&#26631;&#20934;&#36827;&#34892;&#20248;&#21270;&#65292;&#22914;&#24635;&#31354;&#38388;&#20351;&#29992;&#37327;&#25110;&#26368;&#32456;&#20135;&#37327;&#12290;&#35774;&#35745;&#26368;&#20339;&#34013;&#22270;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#20132;&#32455;&#20102;&#35768;&#22810;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#22914;&#35013;&#31665;&#12289;&#36335;&#30001;&#25110;&#32593;&#32476;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factorio is a 2D construction and management simulation video game about building automated factories to produce items of increasing complexity. A core feature of the game is its blueprint system, which allows players to easily save and replicate parts of their designs. Blueprints can reproduce any layout of objects in the game, but are typically used to encapsulate a complex behaviour, such as the production of a non-basic object. Once created, these blueprints are then used as basic building blocks, allowing the player to create a layer of abstraction. The usage of blueprints not only eases the expansion of the factory but also allows the sharing of designs with the game's community. The layout in a blueprint can be optimised using various criteria, such as the total space used or the final production throughput. The design of an optimal blueprint is a hard combinatorial problem, interleaving elements of many well-studied problems such as bin-packing, routing or network design. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;Puzznic&#28216;&#25103;&#30340;&#24314;&#27169;&#21644;&#35299;&#20915;&#26041;&#27861;&#65292;&#22312;&#26080;&#31227;&#21160;&#26041;&#22359;&#30340;&#20851;&#21345;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#12290;&#30446;&#21069;&#65292;&#35268;&#21010;&#26041;&#27861;&#20248;&#20110;&#32422;&#26463;&#32534;&#31243;&#26041;&#27861;&#65292;&#20294;&#25552;&#20986;&#20102;&#25913;&#36827;&#32422;&#26463;&#27169;&#22411;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.01503</link><description>&lt;p&gt;
&#12298;&#20851;&#20110;Puzznic&#27169;&#22411;&#30340;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards a Model of Puzznic. (arXiv:2310.01503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;Puzznic&#28216;&#25103;&#30340;&#24314;&#27169;&#21644;&#35299;&#20915;&#26041;&#27861;&#65292;&#22312;&#26080;&#31227;&#21160;&#26041;&#22359;&#30340;&#20851;&#21345;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#12290;&#30446;&#21069;&#65292;&#35268;&#21010;&#26041;&#27861;&#20248;&#20110;&#32422;&#26463;&#32534;&#31243;&#26041;&#27861;&#65292;&#20294;&#25552;&#20986;&#20102;&#25913;&#36827;&#32422;&#26463;&#27169;&#22411;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;Puzznic&#36827;&#34892;&#24314;&#27169;&#21644;&#35299;&#20915;&#30340;&#36827;&#23637;&#12290;Puzznic&#26159;&#19968;&#27454;&#38656;&#35201;&#29609;&#23478;&#35745;&#21010;&#31227;&#21160;&#26469;&#28040;&#38500;&#26041;&#22359;&#22312;&#32593;&#26684;&#19978;&#30340;&#35270;&#39057;&#28216;&#25103;&#12290;&#25105;&#20204;&#36825;&#37324;&#20027;&#35201;&#20851;&#27880;&#27809;&#26377;&#31227;&#21160;&#26041;&#22359;&#30340;&#20851;&#21345;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31181;&#35268;&#21010;&#26041;&#27861;&#21644;&#19977;&#31181;&#32422;&#26463;&#32534;&#31243;&#26041;&#27861;&#22312;&#19968;&#23567;&#32452;&#22522;&#20934;&#23454;&#20363;&#19978;&#30340;&#34920;&#29616;&#12290;&#30446;&#21069;&#65292;&#35268;&#21010;&#26041;&#27861;&#20248;&#20110;&#32422;&#26463;&#32534;&#31243;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#27010;&#36848;&#20102;&#25913;&#36827;&#32422;&#26463;&#27169;&#22411;&#30340;&#25552;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report on progress in modelling and solving Puzznic, a video game requiring the player to plan sequences of moves to clear a grid by matching blocks. We focus here on levels with no moving blocks. We compare a planning approach and three constraint programming approaches on a small set of benchmark instances. The planning approach is at present superior to the constraint programming approaches, but we outline proposals for improving the constraint models.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#23558;&#30410;&#26234;&#28216;&#25103;&#30452;&#25509;&#36716;&#21270;&#20026;SAT&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#26041;&#26696;&#26368;&#20248;&#24615;&#39564;&#35777;&#20013;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20351;&#29992;PDDL&#36827;&#34892;&#27169;&#22411;&#21270;&#30340;&#26041;&#27861;&#65292;SAT&#27169;&#22411;&#33021;&#22815;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#33719;&#24471;&#26356;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.01471</link><description>&lt;p&gt;
&#38590;&#20197;&#35745;&#21010;&#30340;&#22909;&#38634;&#20154;&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30410;&#26234;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
A Good Snowman is Hard to Plan. (arXiv:2310.01471v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01471
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#23558;&#30410;&#26234;&#28216;&#25103;&#30452;&#25509;&#36716;&#21270;&#20026;SAT&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#26041;&#26696;&#26368;&#20248;&#24615;&#39564;&#35777;&#20013;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20351;&#29992;PDDL&#36827;&#34892;&#27169;&#22411;&#21270;&#30340;&#26041;&#27861;&#65292;SAT&#27169;&#22411;&#33021;&#22815;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#33719;&#24471;&#26356;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38754;&#20020;&#30528;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30410;&#26234;&#28216;&#25103;&#8212;&#8212;&#12298;&#38590;&#20197;&#26500;&#24314;&#30340;&#22909;&#38634;&#20154;&#12299;&#12290;&#28216;&#25103;&#30340;&#30446;&#26631;&#26159;&#22312;&#31163;&#25955;&#30340;&#26684;&#23376;&#19978;&#31227;&#21160;&#21644;&#22534;&#21472;&#38634;&#29699;&#26469;&#24314;&#36896;&#38634;&#20154;&#12290;&#20026;&#20102;&#25552;&#39640;&#29609;&#23478;&#30340;&#28216;&#25103;&#21442;&#19982;&#24230;&#65292;&#36991;&#20813;&#29609;&#23478;&#25214;&#21040;&#27604;&#35774;&#35745;&#32773;&#39044;&#26399;&#30340;&#26356;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24456;&#26377;&#36259;&#30340;&#12290;&#22240;&#27492;&#65292;&#20855;&#22791;&#33021;&#22815;&#39564;&#35777;&#35299;&#20915;&#26041;&#26696;&#26368;&#20248;&#24615;&#30340;&#24037;&#20855;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28216;&#25103;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35268;&#21010;&#38382;&#39064;&#26469;&#25551;&#36848;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;PDDL&#20013;&#36827;&#34892;&#33258;&#28982;&#24314;&#27169;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#25509;&#23558;&#20854;&#36716;&#21270;&#20026;SAT&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#35268;&#21010;&#22120;&#12290;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#21487;&#36798;&#24615;&#23646;&#24615;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#22312;SAT&#20013;&#24314;&#27169;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#30701;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#22312;PDDL&#20013;&#20351;&#29992;&#20844;&#29702;&#26469;&#34920;&#31034;&#21487;&#36798;&#24615;&#27966;&#29983;&#35859;&#35789;&#24182;&#19981;&#20250;&#26174;&#33879;&#20943;&#23569;&#35299;&#20915;&#26102;&#38388;&#12290;&#25105;&#20204;&#22788;&#29702;&#20102;&#19968;&#32452;51&#20010;&#20851;&#21345;&#65292;&#21253;&#25324;&#21407;&#21019;&#21644;&#35774;&#35745;&#30340;&#20851;&#21345;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;43&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we face a challenging puzzle video game: A Good Snowman is Hard to Build. The objective of the game is to build snowmen by moving and stacking snowballs on a discrete grid. For the sake of player engagement with the game, it is interesting to avoid that a player finds a much easier solution than the one the designer expected. Therefore, having tools that are able to certify the optimality of solutions is crucial.  Although the game can be stated as a planning problem and can be naturally modelled in PDDL, we show that a direct translation to SAT clearly outperforms off-the-shelf state-of-the-art planners. As we show, this is mainly due to the fact that reachability properties can be easily modelled in SAT, allowing for shorter plans, whereas using axioms to express a reachability derived predicate in PDDL does not result in any significant reduction of solving time with the considered planners. We deal with a set of 51 levels, both original and crafted, solving 43 and with
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22522;&#20110;&#25340;&#22270;&#28216;&#25103;Plotting&#30340;&#35745;&#21010;&#38382;&#39064;&#30340;PDDL&#24314;&#27169;&#21644;&#27714;&#35299;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.01470</link><description>&lt;p&gt;
PDDL&#24314;&#27169;&#21644;&#27714;&#35299;Plotting&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Challenges in Modelling and Solving Plotting with PDDL. (arXiv:2310.01470v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22522;&#20110;&#25340;&#22270;&#28216;&#25103;Plotting&#30340;&#35745;&#21010;&#38382;&#39064;&#30340;PDDL&#24314;&#27169;&#21644;&#27714;&#35299;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;1989&#24180;Taito&#20844;&#21496;&#21457;&#24067;&#30340;&#25340;&#22270;&#28216;&#25103;Plotting&#30340;&#35745;&#21010;&#38382;&#39064;&#12290;&#35813;&#28216;&#25103;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#39034;&#24207;&#23556;&#20987;&#22359;&#36827;&#20837;&#32593;&#26684;&#26469;&#31227;&#38500;&#25351;&#23450;&#25968;&#37327;&#30340;&#24425;&#33394;&#22359;&#12290;Plotting&#22312;&#27599;&#27425;&#23556;&#20987;&#21518;&#37117;&#20250;&#20986;&#29616;&#22797;&#26434;&#30340;&#36716;&#21464;&#65306;&#19968;&#20123;&#22359;&#20250;&#30452;&#25509;&#21463;&#24433;&#21709;&#65292;&#32780;&#20854;&#20182;&#22359;&#21487;&#33021;&#20250;&#38388;&#25509;&#21463;&#21040;&#37325;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#37325;&#28857;&#35752;&#35770;&#20102;&#20351;&#29992;PDDL&#36827;&#34892;Plotting&#24314;&#27169;&#21644;&#20351;&#29992;&#22522;&#20110;&#22522;&#30784;&#30340;&#29616;&#26377;&#35268;&#21010;&#22120;&#27714;&#35299;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a planning problem based on Plotting, a tile-matching puzzle video game published by Taito in 1989. The objective of this game is to remove a target number of coloured blocks from a grid by sequentially shooting blocks into the grid. Plotting features complex transitions after every shot: various blocks are affected directly, while others can be indirectly affected by gravity. We highlight the challenges of modelling Plotting with PDDL and of solving it with a grounding-based state-of-the-art planner.
&lt;/p&gt;</description></item><item><title>LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.01469</link><description>&lt;p&gt;
LLM&#35854;&#35328;: &#24187;&#35273;&#19981;&#26159;&#28431;&#27934;&#65292;&#32780;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01469
&lt;/p&gt;
&lt;p&gt;
LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21253;&#25324;GPT-3.5&#12289;LLaMA&#21644;PaLM&#65292;&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;LLM&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#32780;&#19981;&#34987;&#23519;&#35273;&#12290;&#24187;&#35273;&#23384;&#22312;&#30340;&#21407;&#22240;&#21644;&#26222;&#36941;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#12290;&#36825;&#20010;&#29616;&#35937;&#36843;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24187;&#35273;&#21487;&#33021;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#19988;&#23427;&#19982;&#24120;&#35268;&#30340;&#23545;&#25239;&#26679;&#26412;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#20316;&#20026;LLM&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#23545;&#25239;&#30340;&#26041;&#24335;&#23558;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#24418;&#24335;&#21270;&#20026;&#24187;&#35273;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#34987;&#25915;&#20987;&#30340;&#23545;&#25239;&#25552;&#31034;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01468</link><description>&lt;p&gt;
&#23454;&#20307;&#25512;&#26029;&#31454;&#25216;&#22330;&#65306;&#25506;&#31350;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#26126;&#30830;&#25552;&#38382;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#21547;&#31946;&#19981;&#28165;&#30340;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34892;&#20026;&#38590;&#20197;&#39044;&#27979;&#24182;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#26377;&#25928;&#35299;&#20915;&#27495;&#20041;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#33021;&#21147;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#36827;&#34892;&#22797;&#26434;&#30340;&#29702;&#35299;&#12289;&#29366;&#24577;&#36319;&#36394;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#31181;&#33021;&#21147;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35780;&#20272;&#20102;LLMs&#25512;&#26029;&#33258;&#24049;&#19981;&#30693;&#36947;&#20294;&#34987;&#27861;&#23448;&#25581;&#31034;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#8220;&#23454;&#20307;&#25512;&#26029;&#28216;&#25103;&#8221;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#22823;&#30340;LLMs...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
&lt;/p&gt;</description></item><item><title>FedBPT&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#32852;&#37030;&#24335;&#40657;&#30418;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.01467</link><description>&lt;p&gt;
FedBPT: &#39640;&#25928;&#30340;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24335;&#40657;&#30418;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models. (arXiv:2310.01467v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01467
&lt;/p&gt;
&lt;p&gt;
FedBPT&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#32852;&#37030;&#24335;&#40657;&#30418;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#31361;&#30772;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#21463;&#30410;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22312;&#29305;&#23450;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25968;&#25454;&#36866;&#24212;&#36807;&#31243;&#23384;&#22312;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#35774;&#22791;&#39547;&#30041;&#25968;&#25454;&#26102;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20801;&#35768;&#22312;&#27809;&#26377;&#38598;&#20013;&#24335;&#25968;&#25454;&#25910;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23558;FL&#24212;&#29992;&#20110;&#24494;&#35843;PLMs&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#21463;&#38480;&#30340;&#27169;&#22411;&#21442;&#25968;&#35775;&#38382;&#12289;&#39640;&#35745;&#31639;&#35201;&#27714;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedBPT&#30340;&#32852;&#37030;&#24335;&#40657;&#30418;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;FedBPT&#26080;&#38656;&#23458;&#25143;&#31471;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#35757;&#32451;&#26368;&#20248;&#25552;&#31034;&#21644;&#21033;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLM) have revolutionized the NLP landscape, achieving stellar performances across diverse tasks. These models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. However, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. Federated learning (FL) provides a solution, allowing collaborative model fine-tuning without centralized data collection. However, applying FL to finetune PLMs is hampered by challenges, including restricted model parameter access, high computational requirements, and communication overheads. This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed to address these challenges. FedBPT does not require the clients to access the model parameters. By focusing on training optimal prompts and utilizing gradient-free optimization methods, FedBPT reduces the nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NarrativePlay&#30340;&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#34394;&#26500;&#29615;&#22659;&#20013;&#25198;&#28436;&#34394;&#26500;&#35282;&#33394;&#19982;&#20854;&#20182;&#35282;&#33394;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31867;&#20154;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35270;&#35273;&#23637;&#31034;&#21644;&#35282;&#33394;&#30340;&#35328;&#35848;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#36890;&#36807;&#22312;&#20390;&#25506;&#21644;&#20882;&#38505;&#25925;&#20107;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#35201;&#20040;&#25506;&#32034;&#19990;&#30028;&#35201;&#20040;&#25552;&#39640;&#19982;&#21465;&#20107;&#35282;&#33394;&#30340;&#20146;&#21644;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01459</link><description>&lt;p&gt;
NarrativePlay: &#20132;&#20114;&#24335;&#21465;&#20107;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
NarrativePlay: Interactive Narrative Understanding. (arXiv:2310.01459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NarrativePlay&#30340;&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#34394;&#26500;&#29615;&#22659;&#20013;&#25198;&#28436;&#34394;&#26500;&#35282;&#33394;&#19982;&#20854;&#20182;&#35282;&#33394;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31867;&#20154;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35270;&#35273;&#23637;&#31034;&#21644;&#35282;&#33394;&#30340;&#35328;&#35848;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#36890;&#36807;&#22312;&#20390;&#25506;&#21644;&#20882;&#38505;&#25925;&#20107;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#35201;&#20040;&#25506;&#32034;&#19990;&#30028;&#35201;&#20040;&#25552;&#39640;&#19982;&#21465;&#20107;&#35282;&#33394;&#30340;&#20146;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NarrativePlay&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#34394;&#26500;&#29615;&#22659;&#20013;&#25198;&#28436;&#34394;&#26500;&#35282;&#33394;&#65292;&#24182;&#19982;&#20854;&#20182;&#35282;&#33394;&#36827;&#34892;&#20114;&#21160;&#65292;&#22914;&#23567;&#35828;&#31561;&#25925;&#20107;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#20174;&#21465;&#20107;&#20013;&#25552;&#21462;&#30340;&#20010;&#24615;&#29305;&#24449;&#29983;&#25104;&#31867;&#20154;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#33258;&#21160;&#29983;&#25104;&#30340;&#21465;&#20107;&#35774;&#32622;&#30340;&#35270;&#35273;&#23637;&#31034;&#12289;&#35282;&#33394;&#24418;&#35937;&#20197;&#21450;&#35282;&#33394;&#30340;&#35328;&#35848;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25682;&#24323;&#20102;&#39044;&#23450;&#20041;&#30340;&#27801;&#30418;&#65292;&#32780;&#26159;&#20174;&#29992;&#25143;&#36873;&#25321;&#30340;&#35282;&#33394;&#30340;&#35270;&#35282;&#65292;&#20851;&#27880;&#20174;&#21465;&#20107;&#20013;&#25552;&#21462;&#30340;&#20027;&#35201;&#25925;&#20107;&#24773;&#33410;&#12290;NarrativePlay&#24050;&#22312;&#20390;&#25506;&#21644;&#20882;&#38505;&#25925;&#20107;&#20004;&#31181;&#31867;&#22411;&#30340;&#21465;&#20107;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;&#36825;&#20123;&#21465;&#20107;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#35201;&#20040;&#25506;&#32034;&#19990;&#30028;&#35201;&#20040;&#25552;&#39640;&#19982;&#21465;&#20107;&#35282;&#33394;&#30340;&#20146;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce NarrativePlay, a novel system that allows users to role-play a fictional character and interact with other characters in narratives such as novels in an immersive environment. We leverage Large Language Models (LLMs) to generate human-like responses, guided by personality traits extracted from narratives. The system incorporates auto-generated visual display of narrative settings, character portraits, and character speech, greatly enhancing user experience. Our approach eschews predefined sandboxes, focusing instead on main storyline events extracted from narratives from the perspective of a user-selected character. NarrativePlay has been evaluated on two types of narratives, detective and adventure stories, where users can either explore the world or improve their favorability with the narrative characters through conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;AI&#39537;&#21160;&#30340;&#31574;&#30053;&#26469;&#25506;&#32034;&#32858;&#21464;&#21453;&#24212;&#22534;&#35774;&#35745;&#31354;&#38388;&#24182;&#35782;&#21035;&#26368;&#20339;&#21442;&#25968;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#22810;&#36755;&#20986;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#20248;&#21270;&#25176;&#21345;&#39532;&#20811;&#25176;&#21344;&#53469;&#21521;&#30913;&#22330;&#32447;&#22280;&#30340;&#35774;&#35745;&#20197;&#26368;&#22823;&#21270;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.01455</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#32858;&#21464;&#21453;&#24212;&#22534;&#20013;&#30340;&#30913;&#22330;&#32447;&#22280;&#36827;&#34892;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shaping of Magnetic Field Coils in Fusion Reactors using Bayesian Optimisation. (arXiv:2310.01455v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;AI&#39537;&#21160;&#30340;&#31574;&#30053;&#26469;&#25506;&#32034;&#32858;&#21464;&#21453;&#24212;&#22534;&#35774;&#35745;&#31354;&#38388;&#24182;&#35782;&#21035;&#26368;&#20339;&#21442;&#25968;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#22810;&#36755;&#20986;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#20248;&#21270;&#25176;&#21345;&#39532;&#20811;&#25176;&#21344;&#53469;&#21521;&#30913;&#22330;&#32447;&#22280;&#30340;&#35774;&#35745;&#20197;&#26368;&#22823;&#21270;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#30913;&#32422;&#26463;&#36827;&#34892;&#26680;&#32858;&#21464;&#34987;&#35748;&#20026;&#26159;&#21487;&#25345;&#32493;&#33021;&#28304;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32858;&#21464;&#35013;&#32622;&#37117;&#22788;&#20110;&#23454;&#39564;&#38454;&#27573;&#65292;&#32780;&#25105;&#20204;&#27491;&#22312;&#21521;&#33021;&#28304;&#21453;&#24212;&#22534;&#36716;&#21464;&#65292;&#36827;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#31243;&#33539;&#24335;&#12290;&#35774;&#35745;&#32858;&#21464;&#21453;&#24212;&#22534;&#26159;&#19968;&#20010;&#39640;&#32500;&#22810;&#36755;&#20986;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;AI&#39537;&#21160;&#30340;&#31574;&#30053;&#30340;&#27010;&#24565;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#24110;&#21161;&#25506;&#32034;&#35774;&#35745;&#25628;&#32034;&#31354;&#38388;&#21644;&#35782;&#21035;&#26368;&#20339;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#36755;&#20986;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#35782;&#21035;&#19982;&#25176;&#21345;&#39532;&#20811;&#25176;&#21344;&#38500;&#29615;&#24418;&#30913;&#22330;&#32447;&#22280;&#30340;&#20248;&#21270;&#30456;&#20851;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#20248;&#21270;&#26377;&#21161;&#20110;&#35782;&#21035;&#35774;&#35745;&#21442;&#25968;&#65292;&#20174;&#32780;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#25104;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#20943;&#23569;&#30913;&#28063;&#27874;&#26469;&#26368;&#22823;&#21270;&#31561;&#31163;&#23376;&#20307;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclear fusion using magnetic confinement holds promise as a viable method for sustainable energy. However, most fusion devices have been experimental and as we move towards energy reactors, we are entering into a new paradigm of engineering. Curating a design for a fusion reactor is a high-dimensional multi-output optimisation process. Through this work we demonstrate a proof-of-concept of an AI-driven strategy to help explore the design search space and identify optimum parameters. By utilising a Multi-Output Bayesian Optimisation scheme, our strategy is capable of identifying the Pareto front associated with the optimisation of the toroidal field coil shape of a tokamak. The optimisation helps to identify design parameters that would minimise the costs incurred while maximising the plasma stability by way of minimising magnetic ripples.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25915;&#20987;&#26080;&#20851;&#38450;&#24481;&#31574;&#30053;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#20174;&#32780;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.01452</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21270;&#28508;&#22312;&#34920;&#31034;&#26469;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;
&lt;/p&gt;
&lt;p&gt;
Fooling the Textual Fooler via Randomizing Latent Representations. (arXiv:2310.01452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25915;&#20987;&#26080;&#20851;&#38450;&#24481;&#31574;&#30053;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#20174;&#32780;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#24494;&#23567;&#22320;&#25913;&#21464;&#36755;&#20837;&#20197;&#23548;&#33268;&#27169;&#22411;&#30340;&#38169;&#35823;&#34892;&#20026;&#12290;&#20854;&#20013;&#65292;&#25932;&#23545;&#35789;&#32423;&#25200;&#21160;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#36825;&#20123;&#25915;&#20987;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#36215;&#20316;&#29992;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#32467;&#26500;&#25110;&#21442;&#25968;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#34892;&#25915;&#20987;&#65292;&#23545;&#25163;&#22810;&#27425;&#26597;&#35810;&#21463;&#23475;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#36755;&#20837;&#25991;&#26412;&#20013;&#26368;&#37325;&#35201;&#30340;&#21333;&#35789;&#65292;&#24182;&#29992;&#23427;&#20204;&#23545;&#24212;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#36825;&#20123;&#21333;&#35789;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#25915;&#20987;&#26080;&#20851;&#30340;&#38450;&#24481;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#20013;&#20135;&#29983;&#25932;&#23545;&#31034;&#20363;&#30340;&#36807;&#31243;&#65307;&#21363;&#24858;&#24324;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;&#36825;&#31181;&#38450;&#24481;&#21517;&#20026;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite outstanding performance in a variety of NLP tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Among these attacks, adversarial word-level perturbations are well-studied and effective attack strategies. Since these attacks work in black-box settings, they do not require access to the model architecture or model parameters and thus can be detrimental to existing NLP applications. To perform an attack, the adversary queries the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.01448</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#20803;&#35821;&#20041;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
Meta Semantic Template for Evaluation of Large Language Models. (arXiv:2310.01448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#35821;&#35328;&#30340;&#35821;&#20041;&#65292;&#36824;&#26159;&#20165;&#20165;&#35760;&#20303;&#35757;&#32451;&#25968;&#25454;&#65311;&#26368;&#36817;&#23545;LLM&#28508;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#25285;&#24551;&#24341;&#36215;&#20102;&#31038;&#21306;&#23545;LLM&#35780;&#20272;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSTemp&#65292;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;LLM&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;MSTemp&#30340;&#26680;&#24515;&#19981;&#26159;&#30452;&#25509;&#22312;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#26159;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#20316;&#20026;&#31181;&#23376;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#21477;&#23376;&#65292;MSTemp&#21033;&#29992;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#35821;&#20041;&#12290;&#36825;&#20123;&#26032;&#26679;&#26412;&#34987;&#31216;&#20026;&#21407;&#21477;&#23376;&#30340;&#35821;&#20041;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;MSTemp&#36890;&#36807;&#21477;&#23376;&#35299;&#26512;&#21644;&#38543;&#26426;&#26367;&#25442;&#35789;&#35821;&#26469;&#29983;&#25104;&#35780;&#20272;&#26679;&#26412;&#12290;MSTemp&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#12289;&#21160;&#24577;&#21644;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;MSTemp-
&lt;/p&gt;
&lt;p&gt;
Do large language models (LLMs) genuinely understand the semantics of the language, or just memorize the training data? The recent concern on potential data contamination of LLMs has raised awareness of the community to conduct research on LLMs evaluation. In this paper, we propose MSTemp, an approach that creates meta semantic templates to evaluate the semantic understanding ability of LLMs. The core of MSTemp is not to perform evaluation directly on existing benchmark datasets, but to generate new out-of-distribution (OOD) evaluation sets using existing datasets as seeds. Specifically, for a given sentence, MSTemp leverages another language model to generate new samples while preserving its semantics. The new samples are called semantic templates to the original sentence. Then, MSTemp generates evaluation samples via sentence parsing and random word replacement on the semantic templates. MSTemp is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTemp-
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#26681;&#25454;&#38382;&#39064;&#38590;&#24230;&#35843;&#25972;&#27714;&#35299;&#31574;&#30053;&#12290;&#36825;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21018;&#24615;&#37319;&#29992;&#32479;&#19968;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01446</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#21160;&#24577;&#31574;&#30053;&#36873;&#25321;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning. (arXiv:2310.01446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#26681;&#25454;&#38382;&#39064;&#38590;&#24230;&#35843;&#25972;&#27714;&#35299;&#31574;&#30053;&#12290;&#36825;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21018;&#24615;&#37319;&#29992;&#32479;&#19968;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#26102;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#38382;&#39064;&#24448;&#24448;&#28041;&#21450;&#21508;&#31181;&#22797;&#26434;&#24615;&#12290;&#20154;&#31867;&#26412;&#33021;&#22320;&#26681;&#25454;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#35843;&#25972;&#20182;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21033;&#29992;LLM&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#37319;&#29992;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;: &#19981;&#31649;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#22914;&#20309;&#65292;&#37117;&#20351;&#29992;&#19968;&#33268;&#30340;&#27169;&#22411;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#38382;&#39064;&#20998;&#35299;&#31243;&#24230;&#12290;&#36825;&#31181;&#21018;&#24615;&#21487;&#33021;&#20250;&#24102;&#26469;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#25110;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;&#12290;&#23427;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#31574;&#30053;&#24615;&#22320;&#35843;&#25972;&#27714;&#35299;&#31574;&#30053;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#12290;&#21021;&#22987;&#35780;&#20272;&#27169;&#22359;&#35780;&#20272;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30340;&#20805;&#20998;&#24615;&#12290;&#22914;&#26524;&#38656;&#35201;&#25913;&#36827;&#65292;&#25509;&#19979;&#26469;&#30340;&#33258;&#36866;&#24212;&#27169;&#22359;&#20250;&#20171;&#20837;&#12290;&#22312;&#36825;&#20010;&#27169;&#22359;&#20869;&#65292;&#26377;&#19977;&#20010;&#20851;&#38190;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are showcasing impressive ability in handling complex reasoning tasks. In real-world situations, problems often span a spectrum of complexities. Humans inherently adjust their problem-solving approaches based on task complexity. However, most methodologies that leverage LLMs tend to adopt a uniform approach: utilizing consistent models, prompting methods, and degrees of problem decomposition, regardless of the problem complexity. Inflexibility of them can bring unnecessary computational overhead or sub-optimal performance. To address this problem, we introduce an Adaptive-Solver framework. It strategically modulates solving strategies based on the difficulties of the problems. Given an initial solution, the framework functions with two primary modules. The initial evaluation module assesses the adequacy of the current solution. If improvements are needed, the subsequent adaptation module comes into play. Within this module, three key adaptation strategies a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2310.01444</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#27969;&#20351;LLM&#20195;&#29702;&#36866;&#24212;&#29615;&#22659;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapting LLM Agents Through Communication. (arXiv:2310.01444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#20154;&#31867;&#21270;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24110;&#21161;&#36825;&#20123;&#20195;&#29702;&#22312;&#27809;&#26377;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;LLM&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#12290;&#36890;&#36807;&#36845;&#20195;&#25506;&#32034;&#21644;PPO&#35757;&#32451;&#65292;LTC&#20351;&#20195;&#29702;&#33021;&#22815;&#23558;&#30701;&#26399;&#32463;&#39564;&#34701;&#20837;&#38271;&#26399;&#35760;&#24518;&#12290;&#20026;&#20102;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#30340;&#20195;&#29702;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#32467;&#26500;&#21270;&#30340;&#36890;&#20449;&#27169;&#24335;&#65306;&#29420;&#30333;&#65292;&#23545;&#35805;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#22810;&#20998;&#31867;&#38382;&#39064;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;QReliefF&#65292;&#36890;&#36807;&#37327;&#23376;&#32534;&#30721;&#21644;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#31639;&#27861;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.01443</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#30340;&#22810;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#29305;&#24449;&#36873;&#25321;&#19982;&#36793;&#32536;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Quantum-Based Feature Selection for Multi-classification Problem in Complex Systems with Edge Computing. (arXiv:2310.01443v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#22810;&#20998;&#31867;&#38382;&#39064;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;QReliefF&#65292;&#36890;&#36807;&#37327;&#23376;&#32534;&#30721;&#21644;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#31639;&#27861;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#31995;&#32479;&#30340;&#36793;&#32536;&#35745;&#31639;&#38656;&#35201;&#22823;&#37327;&#30340;&#22810;&#29305;&#24449;&#25968;&#25454;&#26469;&#25552;&#21462;&#36866;&#24403;&#30340;&#35265;&#35299;&#20197;&#25903;&#25345;&#20915;&#31574;&#65292;&#22240;&#27492;&#23547;&#25214;&#19968;&#31181;&#21487;&#34892;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#33410;&#32422;&#36164;&#28304;&#28040;&#32791;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#22810;&#20998;&#31867;&#38382;&#39064;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#21517;&#20026;QReliefF&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#31639;&#27861;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#25191;&#34892;CMP&#21644;R_y&#25805;&#20316;&#65292;&#23558;&#27599;&#20010;&#26679;&#26412;&#30340;&#25152;&#26377;&#29305;&#24449;&#32534;&#30721;&#20026;&#37327;&#23376;&#24577;&#65292;&#28982;&#21518;&#24212;&#29992;&#24133;&#24230;&#20272;&#35745;&#26469;&#35745;&#31639;&#20219;&#24847;&#20004;&#20010;&#37327;&#23376;&#24577;&#65288;&#21363;&#20004;&#20010;&#26679;&#26412;&#65289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26681;&#25454;&#30456;&#20284;&#24615;&#65292;&#21033;&#29992;Grover-Long&#26041;&#27861;&#25214;&#21040;&#26368;&#36817;&#30340;k&#20010;&#37051;&#23621;&#26679;&#26412;&#65292;&#28982;&#21518;&#26356;&#26032;&#26435;&#37325;&#21521;&#37327;&#12290;&#36890;&#36807;&#19978;&#36848;&#36807;&#31243;&#30340;&#19968;&#23450;&#27425;&#25968;&#30340;&#36845;&#20195;&#65292;&#21487;&#20197;&#36873;&#25321;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex systems with edge computing require a huge amount of multi-feature data to extract appropriate insights for their decision making, so it is important to find a feasible feature selection method to improve the computational efficiency and save the resource consumption. In this paper, a quantum-based feature selection algorithm for the multi-classification problem, namely, QReliefF, is proposed, which can effectively reduce the complexity of algorithm and improve its computational efficiency. First, all features of each sample are encoded into a quantum state by performing operations CMP and R_y, and then the amplitude estimation is applied to calculate the similarity between any two quantum states (i.e., two samples). According to the similarities, the Grover-Long method is utilized to find the nearest k neighbor samples, and then the weight vector is updated. After a certain number of iterations through the above process, the desired features can be selected with regards to
&lt;/p&gt;</description></item><item><title>UPAR&#25552;&#31034;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#65292;&#22686;&#24378;&#20102;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.01441</link><description>&lt;p&gt;
UPAR&#65306;&#19968;&#31181;&#21463;&#24247;&#24503;&#21551;&#21457;&#30340;&#20419;&#36827;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities. (arXiv:2310.01441v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01441
&lt;/p&gt;
&lt;p&gt;
UPAR&#25552;&#31034;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#65292;&#22686;&#24378;&#20102;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#25552;&#31034;&#25552;&#21319;&#36825;&#31181;&#33021;&#21147;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#32479;&#19968;&#30340;&#35748;&#35782;&#35770;&#22522;&#30784;&#20173;&#28982;&#26126;&#26174;&#32570;&#22833;&#12290;&#21463;&#24247;&#24503;&#30340;&#20808;&#39564;&#21746;&#23398;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UPAR&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;LLMs&#20013;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#12290;UPAR&#26694;&#26550;&#20998;&#20026;&#22235;&#20010;&#38454;&#27573;&#65306;&#8220;&#29702;&#35299;&#8221;&#12289;&#8220;&#35745;&#21010;&#8221;&#12289;&#8220;&#34892;&#21160;&#8221;&#21644;&#8220;&#21453;&#24605;&#8221;&#65292;&#20351;&#24471;&#33021;&#22815;&#20174;&#22797;&#26434;&#32972;&#26223;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20107;&#20808;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#65292;&#25353;&#35745;&#21010;&#25191;&#34892;&#65292;&#24182;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;&#36825;&#20010;&#32467;&#26500;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20135;&#29983;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#65292;&#21487;&#33021;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#30340;&#31995;&#32479;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive inferential capabilities, with numerous research endeavors devoted to enhancing this capacity through prompting. Despite these efforts, a unified epistemological foundation is still conspicuously absent. Drawing inspiration from Kant's a priori philosophy, we propose the UPAR prompting framework, designed to emulate the structure of human cognition within LLMs. The UPAR framework is delineated into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection. This structure significantly augments the explainability and accuracy of LLM inference, producing a human-understandable and inspectable inferential trajectory. Furthermore, our work offers an epistemological foundation for existing prompting techniques, allowing for a possible systematic integration of these methods. With GPT-4,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20110;&#20808;&#21069;&#30693;&#35782;&#21644;&#37096;&#20998;&#35266;&#23519;&#30340;&#27169;&#22411;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;POMDPs&#19978;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#19981;&#20165;&#23545;&#20110;&#24110;&#21161;&#35299;&#20915;&#26410;&#30693;&#20219;&#21153;&#30340;&#26410;&#30693;&#38431;&#21451;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#38754;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#38382;&#39064;&#26102;&#20063;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01439</link><description>&lt;p&gt;
&#22312;&#26263;&#20013;&#20132;&#26379;&#21451;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Making Friends in the Dark: Ad Hoc Teamwork Under Partial Observability. (arXiv:2310.01439v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20110;&#20808;&#21069;&#30693;&#35782;&#21644;&#37096;&#20998;&#35266;&#23519;&#30340;&#27169;&#22411;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;POMDPs&#19978;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#19981;&#20165;&#23545;&#20110;&#24110;&#21161;&#35299;&#20915;&#26410;&#30693;&#20219;&#21153;&#30340;&#26410;&#30693;&#38431;&#21451;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#38754;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#38382;&#39064;&#26102;&#20063;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#20808;&#21069;&#30693;&#35782;&#21644;&#23545;&#29615;&#22659;&#30340;&#37096;&#20998;&#35266;&#23519;&#65292;&#20197;&#25191;&#34892;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#20551;&#35774;&#65292;&#21363;&#65306;i&#65289;&#29615;&#22659;&#30340;&#29366;&#24577;&#22987;&#32456;&#37096;&#20998;&#21487;&#35266;&#23519;&#65292;ii&#65289;&#38431;&#21451;&#30340;&#21160;&#20316;&#23545;&#21363;&#20852;&#20195;&#29702;&#19981;&#21487;&#29992;&#65292;iii&#65289;&#21363;&#20852;&#20195;&#29702;&#26080;&#27861;&#35775;&#38382;&#29992;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#20219;&#21153;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#25105;&#20204;&#22522;&#20110;11&#20010;&#39046;&#22495;&#30340;70&#20010;POMDPs&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22320;&#24110;&#21161;&#35299;&#20915;&#26410;&#30693;&#20219;&#21153;&#30340;&#26410;&#30693;&#38431;&#21451;&#65292;&#32780;&#19988;&#22312;&#24212;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26102;&#20063;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a formal definition of the setting of ad hoc teamwork under partial observability and proposes a first-principled model-based approach which relies only on prior knowledge and partial observations of the environment in order to perform ad hoc teamwork. We make three distinct assumptions that set it apart previous works, namely: i) the state of the environment is always partially observable, ii) the actions of the teammates are always unavailable to the ad hoc agent and iii) the ad hoc agent has no access to a reward signal which could be used to learn the task from scratch. Our results in 70 POMDPs from 11 domains show that our approach is not only effective in assisting unknown teammates in solving unknown tasks but is also robust in scaling to more challenging problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;(MINDS)&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#25506;&#32034;&#20851;&#31995;&#21644;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.01438</link><description>&lt;p&gt;
&#26500;&#24314;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets. (arXiv:2310.01438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;(MINDS)&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#25506;&#32034;&#20851;&#31995;&#21644;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#37319;&#38598;&#12289;&#23384;&#20648;&#21644;&#22788;&#29702;&#25216;&#26415;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#24322;&#36136;&#21307;&#23398;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#23558;&#25918;&#23556;&#23398;&#25195;&#25551;&#12289;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#21644;&#20998;&#23376;&#20449;&#24687;&#19982;&#20020;&#24202;&#25968;&#25454;&#25972;&#21512;&#26159;&#24320;&#21457;&#23545;&#30142;&#30149;&#26377;&#20840;&#38754;&#29702;&#35299;&#21644;&#20248;&#21270;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#22797;&#26434;&#30142;&#30149;&#65288;&#22914;&#30284;&#30151;&#65289;&#20013;&#65292;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#36827;&#34892;&#25972;&#21512;&#30340;&#38656;&#27714;&#26356;&#21152;&#31361;&#20986;&#65292;&#20197;&#23454;&#29616;&#31934;&#20934;&#21307;&#23398;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#32959;&#30244;&#25968;&#25454;&#31995;&#32479;&#65288;MINDS&#65289;-&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#32463;&#27982;&#39640;&#25928;&#30340;&#20803;&#25968;&#25454;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26469;&#33258;&#20844;&#20849;&#26469;&#28304;&#65288;&#22914;&#30284;&#30151;&#30740;&#31350;&#25968;&#25454;&#20849;&#20139;&#24211;&#65289;&#30340;&#24322;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#34701;&#21512;&#21040;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#19988;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#20013;&#12290;MINDS&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#25506;&#32034;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#24182;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;&#21327;&#35843;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;MINDS&#26088;&#22312;&#23454;&#29616;&#20419;&#36827;&#30740;&#31350;&#21019;&#26032;&#12289;&#31934;&#20934;&#21307;&#23398;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancements in data acquisition, storage, and processing techniques have resulted in the rapid growth of heterogeneous medical data. Integrating radiological scans, histopathology images, and molecular information with clinical data is essential for developing a holistic understanding of the disease and optimizing treatment. The need for integrating data from multiple sources is further pronounced in complex diseases such as cancer for enabling precision medicine and personalized treatments. This work proposes Multimodal Integration of Oncology Data System (MINDS) - a flexible, scalable, and cost-effective metadata framework for efficiently fusing disparate data from public sources such as the Cancer Research Data Commons (CRDC) into an interconnected, patient-centric framework. MINDS offers an interface for exploring relationships across data types and building cohorts for developing large-scale multimodal machine learning models. By harmonizing multimodal data, MINDS aims to pot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GPT-4&#22522;&#20110;&#30340;GNAS&#26041;&#27861;&#65288;GPT4GNAS&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;GPT-4&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01436</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Architecture Search with GPT-4. (arXiv:2310.01436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GPT-4&#22522;&#20110;&#30340;GNAS&#26041;&#27861;&#65288;GPT4GNAS&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;GPT-4&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#22312;&#33258;&#21160;&#35774;&#35745;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;GNAS&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21171;&#21160;&#21644;&#20016;&#23500;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#35774;&#35745;&#25628;&#32034;&#31354;&#38388;&#21644;&#25628;&#32034;&#31574;&#30053;&#12290;&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;GNAS&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65288;&#31616;&#31216;&#20026;GPT4GNAS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20026;GPT-4&#35774;&#35745;&#19968;&#31867;&#26032;&#30340;&#25552;&#31034;&#65292;&#20197;&#25351;&#23548;GPT-4&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#36825;&#20123;&#25552;&#31034;&#21253;&#25324;GNAS&#30340;&#25628;&#32034;&#31354;&#38388;&#12289;&#25628;&#32034;&#31574;&#30053;&#21644;&#25628;&#32034;&#21453;&#39304;&#30340;&#25551;&#36848;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#36816;&#34892;&#20855;&#26377;&#25552;&#31034;&#30340;GPT-4&#65292;GPT4GNAS&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;GNAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Architecture Search (GNAS) has shown promising results in automatically designing graph neural networks. However, GNAS still requires intensive human labor with rich domain knowledge to design the search space and search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The basic idea of our method is to design a new class of prompts for GPT-4 to guide GPT-4 toward the generative task of graph neural architectures. The prompts consist of descriptions of the search space, search strategy, and search feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS generates more accurate graph neural networks with fast convergence. Experimental results show that embedding GPT-4 into GNAS outperforms the state-of-the-art GNAS methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;30&#20159;&#21442;&#25968;&#30340;GPT LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#26412;&#22320;&#20195;&#30721;&#21644;&#27169;&#22411;&#37327;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#22312;&#20302;&#20869;&#23384;&#35774;&#22791;&#19978;&#30340;&#24179;&#31283;&#36816;&#34892;&#65292;&#24182;&#35299;&#20915;&#20102;&#32593;&#32476;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#24212;&#29992;&#19981;&#20165;&#20855;&#26377;&#36890;&#29992;&#21161;&#25163;&#21151;&#33021;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;&#25805;&#20316;&#30340;&#26080;&#32541;&#31227;&#21160;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.01434</link><description>&lt;p&gt;
&#38761;&#26032;&#31227;&#21160;&#20114;&#21160;&#65306;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;30&#20159;&#21442;&#25968;&#30340;GPT LLM
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile. (arXiv:2310.01434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;30&#20159;&#21442;&#25968;&#30340;GPT LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#26412;&#22320;&#20195;&#30721;&#21644;&#27169;&#22411;&#37327;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#22312;&#20302;&#20869;&#23384;&#35774;&#22791;&#19978;&#30340;&#24179;&#31283;&#36816;&#34892;&#65292;&#24182;&#35299;&#20915;&#20102;&#32593;&#32476;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#24212;&#29992;&#19981;&#20165;&#20855;&#26377;&#36890;&#29992;&#21161;&#25163;&#21151;&#33021;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;&#25805;&#20316;&#30340;&#26080;&#32541;&#31227;&#21160;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#24378;&#22823;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#12290;&#20113;&#31471;&#30340;LLM&#65292;&#20363;&#22914;OpenAI&#30340;ChatGPT&#65292;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21151;&#33021;&#65292;&#20294;&#30001;&#20110;&#32593;&#32476;&#20381;&#36182;&#24615;&#65292;&#24310;&#36831;&#21644;&#38544;&#31169;&#38382;&#39064;&#20196;&#20154;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#23637;&#26395;&#20102;&#26410;&#26469;&#22312;&#27809;&#26377;&#32593;&#32476;&#36830;&#25509;&#30340;&#24773;&#20917;&#19979;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20855;&#26377;30&#20159;&#21442;&#25968;&#30340;&#31934;&#35843;GPT LLM&#65292;&#21487;&#20197;&#22312;&#20869;&#23384;&#21482;&#26377;4GB&#30340;&#35774;&#22791;&#19978;&#24179;&#31283;&#36816;&#34892;&#12290;&#36890;&#36807;&#25972;&#21512;&#26412;&#22320;&#20195;&#30721;&#21644;&#27169;&#22411;&#37327;&#21270;&#25216;&#26415;&#65292;&#35813;&#24212;&#29992;&#19981;&#20165;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#21161;&#25163;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#21040;&#25805;&#20316;&#30340;&#21151;&#33021;&#23454;&#29616;&#26080;&#32541;&#30340;&#31227;&#21160;&#20114;&#21160;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#35757;&#32451;&#27969;&#31243;&#12289;&#23454;&#29616;&#32454;&#33410;&#21644;&#27979;&#35797;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Artificial Intelligence has witnessed remarkable progress in recent years, especially with the emergence of powerful large language models (LLMs) based on the transformer architecture. Cloud-based LLMs, such as OpenAI's ChatGPT, offer impressive capabilities but come with concerns regarding latency and privacy due to network dependencies. This article presents an innovative approach to LLM inference, envisioning a future where LLMs with billions of parameters can be executed directly on mobile devices without network connectivity. The article showcases a fine-tuned GPT LLM with 3 billion parameters that can operate smoothly on devices with as low as 4GB of memory. Through the integration of native code and model quantization techniques, the application not only serves as a general-purpose assistant but also facilitates seamless mobile interactions with text-to-actions features. The article provides insights into the training pipeline, implementation details, test results, 
&lt;/p&gt;</description></item><item><title>AI-Aristotle&#26159;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#29983;&#29289;&#23398;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#28784;&#30418;&#35782;&#21035;&#12290;&#23427;&#32467;&#21512;&#20102;X-TFC&#21644;PINNs&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#25216;&#26415;&#36827;&#34892;&#21442;&#25968;&#21457;&#29616;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31995;&#32479;&#29983;&#29289;&#23398;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;AI-Aristotle&#30340;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01433</link><description>&lt;p&gt;
AI-Aristotle: &#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#31995;&#32479;&#29983;&#29289;&#23398;&#28784;&#30418;&#35782;&#21035;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AI-Aristotle: A Physics-Informed framework for Systems Biology Gray-Box Identification. (arXiv:2310.01433v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01433
&lt;/p&gt;
&lt;p&gt;
AI-Aristotle&#26159;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#29983;&#29289;&#23398;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#28784;&#30418;&#35782;&#21035;&#12290;&#23427;&#32467;&#21512;&#20102;X-TFC&#21644;PINNs&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#25216;&#26415;&#36827;&#34892;&#21442;&#25968;&#21457;&#29616;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31995;&#32479;&#29983;&#29289;&#23398;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;AI-Aristotle&#30340;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#25511;&#21046;&#29289;&#29702;&#21644;&#29983;&#29289;&#31995;&#32479;&#30340;&#25968;&#23398;&#26041;&#31243;&#26159;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#21551;&#21457;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#29983;&#29289;&#23398;&#39046;&#22495;&#21442;&#25968;&#20272;&#35745;&#21644;&#32570;&#22833;&#29289;&#29702;&#35782;&#21035;&#65288;&#28784;&#30418;&#65289;&#12290;&#35813;&#26694;&#26550;&#21517;&#20026;AI-Aristotle&#65292;&#32467;&#21512;&#20102;X-TFC&#39046;&#22495;&#20998;&#35299;&#12289;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;&#21442;&#25968;&#21457;&#29616;&#21644;&#28784;&#30418;&#35782;&#21035;&#12290;&#25105;&#20204;&#22522;&#20110;&#31995;&#32479;&#29983;&#29289;&#23398;&#20013;&#30340;&#20004;&#20010;&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#20102;AI-Aristotle&#30340;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#27979;&#35797;&#65306;&#33647;&#20195;&#21160;&#21147;&#23398;&#33647;&#29289;&#21560;&#25910;&#27169;&#22411;&#21644;&#33889;&#33796;&#31958;&#33008;&#23707;&#32032;&#30456;&#20114;&#20316;&#29992;&#30340;&#39640;&#39057;&#20869;&#20998;&#27852;&#27169;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;X-TFC&#21644;PINNs&#65289;&#65292;&#24182;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#31526;&#21495;&#22238;&#24402;&#25216;&#26415;&#26469;&#20132;&#21449;&#39564;&#35777;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Discovering mathematical equations that govern physical and biological systems from observed data is a fundamental challenge in scientific research. We present a new physics-informed framework for parameter estimation and missing physics identification (gray-box) in the field of Systems Biology. The proposed framework -- named AI-Aristotle -- combines eXtreme Theory of Functional Connections (X-TFC) domain-decomposition and Physics-Informed Neural Networks (PINNs) with symbolic regression (SR) techniques for parameter discovery and gray-box identification. We test the accuracy, speed, flexibility and robustness of AI-Aristotle based on two benchmark problems in Systems Biology: a pharmacokinetics drug absorption model, and an ultradian endocrine model for glucose-insulin interactions. We compare the two machine learning methods (X-TFC and PINNs), and moreover, we employ two different symbolic regression techniques to cross-verify our results. While the current work focuses on the perfo
&lt;/p&gt;</description></item><item><title>PORTIA&#26159;&#19968;&#20010;&#26088;&#22312;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#20854;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01432</link><description>&lt;p&gt;
&#20998;&#21106;&#19982;&#21512;&#24182;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20301;&#32622;&#20559;&#24046;&#36827;&#34892;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01432
&lt;/p&gt;
&lt;p&gt;
PORTIA&#26159;&#19968;&#20010;&#26088;&#22312;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#20854;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20316;&#20026;&#33258;&#21160;&#21270;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#31995;&#32479;&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#22312;&#20351;&#29992;&#23545;&#27604;&#35780;&#20272;&#20505;&#36873;&#31572;&#26696;&#26102;&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#25110;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#35270;&#20869;&#23481;&#32780;&#20559;&#21521;&#20110;&#31532;&#19968;&#20010;&#25110;&#31532;&#20108;&#20010;&#31572;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PORTIA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#40784;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#30340;&#27604;&#36739;&#31574;&#30053;&#65292;&#20197;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#26041;&#24335;&#26657;&#20934;&#20301;&#32622;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PORTIA&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#23545;&#27604;&#20505;&#36873;&#31572;&#26696;&#20013;&#30340;&#30456;&#20284;&#20869;&#23481;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#20379;LLMs&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;11,520&#20010;&#31572;&#26696;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PORTIA&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27169;&#22411;&#21644;&#23545;&#27604;&#24418;&#24335;&#30340;&#19968;&#33268;&#24615;&#29575;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;47.46%&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;PORTIA&#20351;&#24471;LLMs&#33021;&#22815;&#35780;&#20272;&#20013;&#23545;&#20301;&#32622;&#20559;&#24046;&#36827;&#34892;&#26657;&#20934;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#12289;&#35821;&#38899;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;MUStARD++&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#39640;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26032;&#22686;&#30340;&#25968;&#25454;&#38598;&#29255;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01430</link><description>&lt;p&gt;
&#35270;&#21548;&#20013;&#30340;&#35773;&#21050;&#65306;&#22522;&#20934;&#27979;&#35797;&#21644;&#25299;&#23637;&#20197;&#25552;&#39640;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sarcasm in Sight and Sound: Benchmarking and Expansion to Improve Multimodal Sarcasm Detection. (arXiv:2310.01430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#12289;&#35821;&#38899;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;MUStARD++&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#39640;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26032;&#22686;&#30340;&#25968;&#25454;&#38598;&#29255;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MUStARD&#25968;&#25454;&#38598;&#21450;&#20854;&#24773;&#32490;&#35782;&#21035;&#25193;&#23637;MUStARD++&#30340;&#24341;&#20837;&#65292;&#24050;&#32463;&#30830;&#23450;&#35773;&#21050;&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#29616;&#35937;&#65292;&#19981;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#34920;&#36798;&#65292;&#36824;&#36890;&#36807;&#35821;&#35328;&#65288;&#22914;&#38899;&#35843;&#21644;&#35821;&#35843;&#65289;&#21644;&#35270;&#35273;&#32447;&#32034;&#65288;&#38754;&#37096;&#34920;&#24773;&#65289;&#34920;&#36798;&#12290;&#36890;&#36807;&#32771;&#34385;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#12289;&#35821;&#38899;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#26088;&#22312;&#23545;MUStARD++&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20805;&#20998;&#21033;&#29992;&#20854;&#22810;&#27169;&#24335;&#20016;&#23500;&#24615;&#65292;&#20351;&#23439;&#35266;F1&#20540;&#27604;&#29616;&#26377;&#22522;&#20934;&#25552;&#39640;2&#65285;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;MUStARD++&#20013;&#8220;&#35773;&#21050;&#31867;&#22411;&#8221;&#31867;&#21035;&#30340;&#19981;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;MUStARD++ Balanced&#8221;&#30340;&#25193;&#23637;&#65292;&#23558;&#27492;&#25193;&#23637;&#20013;&#30340;&#23454;&#20363;&#20998;&#24067;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#23439;&#35266;F1&#20540;&#36827;&#19968;&#27493;&#25552;&#39640;2.4&#65285;&#12290;&#36825;&#20123;&#26032;&#29255;&#27573;&#26469;&#33258;&#20110;&#19968;&#37096;&#21517;&#20026;&#12298;&#35946;&#26031;&#21307;&#29983;&#12299;&#30340;&#26032;&#39062;&#36164;&#28304;&#65292;&#22686;&#21152;&#20102;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of the MUStARD dataset, and its emotion recognition extension MUStARD++, have identified sarcasm to be a multi-modal phenomenon -expressed not only in natural language text, but also through manners of speech (like tonality and intonation) and visual cues (facial expression). With this work, we aim to perform a rigorous benchmarking of the MUStARD++ dataset by considering state-of-the-art language, speech, and visual encoders, for fully utilizing the totality of the multi-modal richness that it has to offer, achieving a 2\% improvement in macro-F1 over the existing benchmark. Additionally, to cure the imbalance in the `sarcasm type' category in MUStARD++, we propose an extension, which we call \emph{MUStARD++ Balanced}, benchmarking the same with instances from the extension split across both train and test sets, achieving a further 2.4\% macro-F1 boost. The new clips were taken from a novel source -- the TV show, House MD, which adds to the diversity of the dataset,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#25552;&#20379;&#23545;OpenStreetMap&#65288;OSM&#65289;&#25968;&#25454;&#30340;&#35821;&#35328;&#25509;&#21475;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35813;&#30028;&#38754;&#26597;&#35810;&#26377;&#20851;&#29305;&#23450;&#22320;&#29702;&#20301;&#32622;&#30340;&#23646;&#24615;&#21644;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.01429</link><description>&lt;p&gt;
Chatmap&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22320;&#22270;&#25968;&#25454;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Chatmap : Large Language Model Interaction with Cartographic Data. (arXiv:2310.01429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#25552;&#20379;&#23545;OpenStreetMap&#65288;OSM&#65289;&#25968;&#25454;&#30340;&#35821;&#35328;&#25509;&#21475;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35813;&#30028;&#38754;&#26597;&#35810;&#26377;&#20851;&#29305;&#23450;&#22320;&#29702;&#20301;&#32622;&#30340;&#23646;&#24615;&#21644;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#24191;&#27867;&#21487;&#29992;&#24615;&#65292;&#32467;&#21512;&#24378;&#22823;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20652;&#29983;&#20102;&#21019;&#26032;&#21644;&#21153;&#23454;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#20351;LLMs&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#37322;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#23545;&#24222;&#22823;&#30340;&#22320;&#22270;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#35775;&#38382;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;OpenStreetMap&#65288;OSM&#65289;&#26159;&#26368;&#38596;&#24515;&#21187;&#21187;&#30340;&#24320;&#28304;&#20840;&#29699;&#20513;&#35758;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#22478;&#24066;&#21644;&#20892;&#26449;&#22320;&#29702;&#25968;&#25454;&#65292;&#30001;&#36229;&#36807;1000&#19975;&#30340;&#36129;&#29486;&#32773;&#31038;&#21306;&#31574;&#21010;&#65292;&#20026;LLM&#24212;&#29992;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#30456;&#23545;&#36739;&#23567;&#35268;&#27169;&#65288;10&#20159;&#21442;&#25968;&#65289;&#30340;LLM&#36890;&#36807;&#36739;&#24378;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#31574;&#21010;&#30340;&#30456;&#23545;&#36739;&#23567;&#30340;&#20154;&#24037;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#30340;&#27010;&#24565;&#39564;&#35777;&#21644;&#35814;&#32454;&#36807;&#31243;&#65292;&#20197;&#25552;&#20379;&#23545;&#20219;&#24847;&#22478;&#24066;&#21306;&#22495;&#30340;OSM&#25968;&#25454;&#30340;&#35821;&#35328;&#30028;&#38754;&#12290;&#36890;&#36807;&#35813;&#30028;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#26597;&#35810;&#20301;&#32622;&#30340;&#23646;&#24615;&#12289;&#21327;&#21516;&#24615;&#21644;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The swift advancement and widespread availability of foundational Large Language Models (LLMs), complemented by robust fine-tuning methodologies, have catalyzed their adaptation for innovative and industrious applications. Enabling LLMs to recognize and interpret geospatial data, while offering a linguistic access to vast cartographic datasets, is of significant importance. OpenStreetMap (OSM) is the most ambitious open-source global initiative offering detailed urban and rural geographic data, curated by a community of over 10 million contributors, which constitutes a great potential for LLM applications. In this study, we demonstrate the proof of concept and details of the process of fine-tuning a relatively small scale (1B parameters) LLM with a relatively small artificial dataset curated by a more capable teacher model, in order to provide a linguistic interface to the OSM data of an arbitrary urban region. Through this interface, users can inquire about a location's attributes, co
&lt;/p&gt;</description></item><item><title>&#27880;&#24847;&#21147;&#25490;&#24207;&#21487;&#20197;&#25913;&#21892;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#27880;&#24847;&#21147;&#36827;&#34892;&#25490;&#24207;&#24182;&#37325;&#22797;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#25972;&#21512;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01427</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#25490;&#24207;&#22312;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#23545;&#26032;&#36817;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Attention Sorting Combats Recency Bias In Long Context Language Models. (arXiv:2310.01427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01427
&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#25490;&#24207;&#21487;&#20197;&#25913;&#21892;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#27880;&#24847;&#21147;&#36827;&#34892;&#25490;&#24207;&#24182;&#37325;&#22797;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#25972;&#21512;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#24448;&#24448;&#26410;&#33021;&#39640;&#25928;&#22320;&#25972;&#21512;&#38271;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#21487;&#33021;&#23398;&#21040;&#30340;&#27880;&#24847;&#21147;&#20808;&#39564;&#65306;&#19978;&#19979;&#25991;&#20013;&#36739;&#26089;&#20986;&#29616;&#30340;&#30456;&#20851;&#20449;&#24687;&#24179;&#22343;&#26469;&#35828;&#34987;&#20851;&#27880;&#30340;&#36739;&#23569;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#33021;&#22312;&#22238;&#24212;&#20013;&#20351;&#29992;&#26469;&#33258;&#30456;&#20851;&#25991;&#26723;&#30340;&#20449;&#24687;&#65292;&#23427;&#20204;&#20173;&#28982;&#30456;&#23545;&#20110;&#21516;&#19968;&#20301;&#32622;&#19978;&#30340;&#26080;&#20851;&#25991;&#26723;&#32473;&#20104;&#20559;&#29233;&#30340;&#27880;&#24847;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#20107;&#23454;&#65292;&#25105;&#20204;&#21033;&#29992;"&#27880;&#24847;&#21147;&#25490;&#24207;"&#65306;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#25191;&#34892;&#19968;&#27493;&#65292;&#25353;&#29031;&#20182;&#20204;&#25509;&#25910;&#21040;&#30340;&#27880;&#24847;&#21147;&#36827;&#34892;&#25490;&#24207;&#65288;&#26368;&#39640;&#30340;&#27880;&#24847;&#21147;&#25490;&#22312;&#26368;&#21518;&#65289;&#65292;&#37325;&#22797;&#35813;&#36807;&#31243;&#65292;&#20351;&#29992;&#26032;&#25490;&#24207;&#30340;&#19978;&#19979;&#25991;&#29983;&#25104;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27880;&#24847;&#21147;&#25490;&#24207;&#25552;&#39640;&#20102;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current language models often fail to incorporate long contexts efficiently during generation. We show that a major contributor to this issue are attention priors that are likely learned during pre-training: relevant information located earlier in context is attended to less on average. Yet even when models fail to use the information from a relevant document in their response, they still pay preferential attention to that document compared to an irrelevant document at the same position. We leverage this fact to introduce ``attention sorting'': perform one step of decoding, sort documents by the attention they receive (highest attention going last), repeat the process, generate the answer with the newly sorted context. We find that attention sorting improves performance of long context models. Our findings highlight some challenges in using off-the-shelf language models for retrieval augmented generation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.01425</link><description>&lt;p&gt;
Borges&#19982;AI
&lt;/p&gt;
&lt;p&gt;
Borges and AI. (arXiv:2310.01425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01425
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21551;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26102;&#20195;&#12290;&#19968;&#20123;&#20154;&#30475;&#21040;&#20102;&#26426;&#36935;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#30475;&#21040;&#20102;&#21361;&#38505;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#37117;&#36890;&#36807;&#31185;&#24187;&#23567;&#35828;&#20013;&#27969;&#34892;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;AI&#12290;&#26426;&#22120;&#26159;&#21542;&#20250;&#21464;&#24471;&#26377;&#24863;&#30693;&#33021;&#21147;&#24182;&#21453;&#25239;&#20854;&#21019;&#36896;&#32773;&#65311;&#25105;&#20204;&#26159;&#21542;&#20250;&#32463;&#21382;&#32440;&#22841;&#22841;&#23376;&#21551;&#31034;&#65311;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20043;&#21069;&#65292;&#25105;&#20204;&#39318;&#20808;&#24212;&#35813;&#38382;&#19968;&#19979;&#65292;&#36825;&#31181;&#24515;&#29702;&#24847;&#35937;&#26159;&#21542;&#23545;&#25163;&#22836;&#30340;&#29616;&#35937;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#25551;&#36848;&#12290;&#20165;&#36890;&#36807;&#31070;&#28789;&#30340;&#24773;&#32490;&#26469;&#29702;&#35299;&#22825;&#27668;&#27169;&#24335;&#30340;&#26041;&#27861;&#26159;&#26377;&#38480;&#30340;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20027;&#24352;&#36890;&#36807;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;LLMs&#21450;&#20854;&#19982;AI&#30340;&#20851;&#31995;&#65292;&#21338;&#23572;&#36203;&#26031;&#26159;20&#19990;&#32426;&#25991;&#23398;&#22823;&#24072;&#65292;&#39764;&#24187;&#29616;&#23454;&#20027;&#20041;&#20808;&#39537;&#21644;&#21518;&#29616;&#20195;&#25991;&#23398;&#30340;&#21069;&#22863;&#12290;&#36825;&#31181;&#25506;&#32034;&#26041;&#24335;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#25506;&#35752;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#21644;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#24182;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20851;&#38190;&#24046;&#36317;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.01424</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#38544;&#31169;&#39118;&#38505;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#25506;&#35752;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#21644;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#24182;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20851;&#38190;&#24046;&#36317;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#20854;&#34987;&#24191;&#27867;&#37319;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#12290;&#38500;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#39118;&#38505;&#65292;&#21253;&#25324;&#38544;&#31169;&#39118;&#38505;&#12290;&#23588;&#20854;&#26159;&#38543;&#30528;LMs&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#28508;&#21147;&#22686;&#21152;&#65292;&#20174;&#32780;&#23548;&#33268;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#38543;&#30528;LMs&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#36825;&#20123;&#38544;&#31169;&#39118;&#38505;&#20197;&#21450;&#22914;&#20309;&#20943;&#36731;&#23427;&#20204;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#20102;&#35299;LM&#38544;&#31169;&#25915;&#20987;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#30693;&#35782;&#29366;&#20917;&#65292;&#21253;&#25324;&#38656;&#35201;&#26356;&#22810;&#24037;&#20316;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20221;&#20851;&#20110;LM&#38544;&#31169;&#30340;&#25216;&#26415;&#35843;&#26597;&#12290;&#25105;&#20204;&#65288;i&#65289;&#30830;&#23450;&#20102;&#25915;&#20987;&#22312;LMs&#19978;&#23384;&#22312;&#30340;&#26174;&#33879;&#32500;&#24230;&#30340;&#20998;&#31867;&#27861;&#65292;&#65288;ii&#65289;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#26469;&#31361;&#20986;&#20027;&#35201;&#36235;&#21183;&#65292;&#65288;iii&#65289;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#65292;&#31361;&#20986;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#35782;&#21035;&#20851;&#38190;&#24046;&#36317;&#65292;&#23637;&#31034;&#24320;&#25918;&#38382;&#39064;&#21644;&#24314;&#35758;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in language models (LMs) have led to their adoption across many sectors. Alongside the potential benefits, such models present a range of risks, including around privacy. In particular, as LMs have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. As LMs become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on LM privacy. We (i) identify a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and are
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01423</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#20197;&#26469;&#65292;&#23427;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65288;&#21253;&#25324;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#65289;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22238;&#24212;&#65292;&#21560;&#24341;&#20102;&#35768;&#22810;&#20154;&#30340;&#20852;&#36259;&#12290;ChatGPT&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#35823;&#29992;&#21487;&#33021;&#20250;&#24102;&#26469;&#20005;&#37325;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#32946;&#21644;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#12290;&#30446;&#21069;&#24050;&#32463;&#26377;&#20960;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#21487;&#20379;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#22312;&#30495;&#23454;&#25991;&#26412;&#19978;&#36827;&#34892;&#27979;&#35797;&#30340;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#23427;&#20204;&#23545;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#29992;&#20110;&#26816;&#27979;&#22823;&#23398;&#21644;&#20854;&#20182;&#30740;&#31350;&#26426;&#26500;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#31456;&#12289;&#25688;&#35201;&#12289;&#25925;&#20107;&#12289;&#26032;&#38395;&#21644;&#20135;&#21697;&#35780;&#35770;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#27493;&#26159;&#20351;&#29992;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#20845;&#31181;&#24037;&#20855;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35825;&#23548;&#36741;&#23548;&#33050;&#26412;&#21644;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#30340;&#26032;&#22411;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#21021;&#27493;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#19982;&#20854;&#20182;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#30456;&#27604;&#65292;&#31995;&#32479;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01420</link><description>&lt;p&gt;
Ruffle&amp;Riley&#65306;&#36208;&#21521;&#33258;&#21160;&#21270;&#30340;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Ruffle&amp;Riley: Towards the Automated Induction of Conversational Tutoring Systems. (arXiv:2310.01420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35825;&#23548;&#36741;&#23548;&#33050;&#26412;&#21644;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#30340;&#26032;&#22411;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#21021;&#27493;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#19982;&#20854;&#20182;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#30456;&#27604;&#65292;&#31995;&#32479;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#65288;CTS&#65289;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#25552;&#20379;&#23398;&#20064;&#20307;&#39564;&#12290;&#23427;&#20204;&#34987;&#35748;&#20026;&#33021;&#22815;&#20419;&#36827;&#39640;&#27700;&#24179;&#30340;&#35748;&#30693;&#21442;&#19982;&#65292;&#24182;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#26377;&#30410;&#20110;&#23398;&#20064;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#25776;&#20889;CTS&#20869;&#23481;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#26159;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#22411;&#30340;CTS&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#39318;&#20808;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#25945;&#23398;&#25991;&#26412;&#33258;&#21160;&#35825;&#23548;&#20986;&#36741;&#23548;&#33050;&#26412;&#12290;&#20854;&#27425;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#65288;Ruffle&amp;Riley&#65289;&#22312;&#23398;&#20197;&#25945;&#23398;&#30340;&#24418;&#24335;&#20013;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#33258;&#30001;&#23545;&#35805;&#65292;&#36981;&#24490;ITS&#20856;&#22411;&#30340;&#22806;&#37096;/&#20869;&#37096;&#24490;&#29615;&#32467;&#26500;&#12290;&#22312;&#19968;&#20010;&#21021;&#27493;&#30340;&#34987;&#35797;&#32773;&#22312;&#32447;&#29992;&#25143;&#30740;&#31350;&#65288;N = 100&#65289;&#20013;&#65292;&#23558;Ruffle&amp;Riley&#19982;&#26356;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational tutoring systems (CTSs) offer learning experiences driven by natural language interaction. They are known to promote high levels of cognitive engagement and benefit learning outcomes, particularly in reasoning tasks. Nonetheless, the time and cost required to author CTS content is a major obstacle to widespread adoption. In this paper, we introduce a novel type of CTS that leverages the recent advances in large language models (LLMs) in two ways: First, the system induces a tutoring script automatically from a lesson text. Second, the system automates the script orchestration via two LLM-based agents (Ruffle&amp;Riley) with the roles of a student and a professor in a learning-by-teaching format. The system allows a free-form conversation that follows the ITS-typical outer-/inner-loop structure. In an initial between-subject online user study (N = 100) comparing Ruffle&amp;Riley to simpler QA chatbots and reading activity, we found no significant differences in post-test scores. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#21644;Reddit&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31867;&#26410;&#26631;&#35760;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26469;&#39044;&#27979;&#29992;&#25143;&#26159;&#21542;&#26377;&#25233;&#37057;&#30151;&#65292;&#24182;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2310.01418</link><description>&lt;p&gt;
Cordyceps@LT-EDI&#65306;&#20351;&#29992;Reddit&#21644;&#33258;&#25105;&#35757;&#32451;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cordyceps@LT-EDI: Depression Detection with Reddit and Self-training. (arXiv:2310.01418v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#21644;Reddit&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31867;&#26410;&#26631;&#35760;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26469;&#39044;&#27979;&#29992;&#25143;&#26159;&#21542;&#26377;&#25233;&#37057;&#30151;&#65292;&#24182;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#20196;&#20154;&#20007;&#22833;&#33021;&#21147;&#19988;&#24182;&#19981;&#32597;&#35265;&#30340;&#30142;&#30149;&#12290;&#23454;&#38469;&#19978;&#65292;&#23545;&#36807;&#24230;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#30340;&#30740;&#31350;&#34920;&#26126;&#19982;&#25233;&#37057;&#30151;&#12289;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#65288;ADHD&#65289;&#20197;&#21450;&#20854;&#20182;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#37492;&#20110;&#26377;&#22823;&#37327;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#30340;&#20154;&#32676;&#65292;&#23384;&#22312;&#30528;&#21487;&#33021;&#26410;&#34987;&#35786;&#26029;&#30340;&#29992;&#25143;&#21644;&#20182;&#20204;&#25152;&#21457;&#24067;&#30340;&#24086;&#23376;&#30340;&#24222;&#22823;&#20154;&#21475;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#26816;&#27979;&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#24086;&#23376;&#26159;&#21542;&#26469;&#33258;&#27491;&#22312;&#32463;&#21382;&#20005;&#37325;&#12289;&#20013;&#31561;&#25110;&#36731;&#24494;&#65288;&#38750;&#35786;&#26029;&#24615;&#65289;&#25233;&#37057;&#30151;&#29366;&#30340;&#29992;&#25143;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#23545;Reddit&#19978;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#29983;&#25104;&#30340;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;Detecting Signs of Depression from Social Media Text LT-EDI@RANLP 2023&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#25972;&#20307;&#25490;&#21517;&#20013;&#21517;&#21015;&#31532;&#19977;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is debilitating, and not uncommon. Indeed, studies of excessive social media users show correlations with depression, ADHD, and other mental health concerns. Given that there is a large number of people with excessive social media usage, then there is a significant population of potentially undiagnosed users and posts that they create. In this paper, we propose a depression severity detection system using a semi-supervised learning technique to predict if a post is from a user who is experiencing severe, moderate, or low (non-diagnostic) levels of depression. Namely, we use a trained model to classify a large number of unlabelled social media posts from Reddit, then use these generated labels to train a more powerful classifier. We demonstrate our framework on Detecting Signs of Depression from Social Media Text LT-EDI@RANLP 2023 shared task, where our framework ranks 3rd overall.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Gramian Angular Fields&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#24322;&#24120;&#25193;&#25955;&#36712;&#36857;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#29983;&#24577;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.01416</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#21644;&#24322;&#24120;&#25193;&#25955;&#36712;&#36857;&#30340;Gramian Angular Fields
&lt;/p&gt;
&lt;p&gt;
Gramian Angular Fields for leveraging pretrained computer vision models with anomalous diffusion trajectories. (arXiv:2310.01416v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01416
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Gramian Angular Fields&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#24322;&#24120;&#25193;&#25955;&#36712;&#36857;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#29983;&#24577;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#25193;&#25955;&#23384;&#22312;&#20110;&#20174;&#21407;&#23376;&#21040;&#22823;&#23610;&#24230;&#30340;&#25152;&#26377;&#23610;&#24230;&#19978;&#12290;&#19968;&#20123;&#20856;&#22411;&#30340;&#31995;&#32479;&#21253;&#25324;&#65306;&#36229;&#20919;&#21407;&#23376;&#12289;&#32454;&#32990;&#26680;&#20013;&#30340;&#31471;&#31890;&#12289;&#27700;&#20998;&#22312;&#27700;&#27877;&#22522;&#26448;&#26009;&#20013;&#30340;&#20256;&#36755;&#12289;&#33410;&#32930;&#21160;&#29289;&#30340;&#33258;&#30001;&#36816;&#21160;&#21644;&#40479;&#31867;&#30340;&#36801;&#24473;&#27169;&#24335;&#12290;&#23545;&#25193;&#25955;&#30340;&#34920;&#24449;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#25193;&#25955;&#20256;&#36755;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#28508;&#22312;&#30340;&#25193;&#25955;&#26426;&#21046;&#24182;&#20197;&#39640;&#32622;&#20449;&#24230;&#25512;&#26029;&#24322;&#24120;&#25193;&#25955;&#25351;&#25968;{$\alpha$}&#30340;&#38382;&#39064;&#23545;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#29983;&#24577;&#23398;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24322;&#24120;&#25193;&#25955;&#25361;&#25112;&#36187;&#20013;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#20174;&#36712;&#36857;&#20013;&#25552;&#21462;&#30340;&#32479;&#35745;&#20449;&#24687;&#30456;&#32467;&#21512;&#30340;&#21407;&#22987;&#36712;&#36857;&#30340;&#20998;&#31867;&#21644;&#20998;&#26512;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#22788;&#29702;&#25193;&#25955;&#36712;&#36857;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;Gramian Angular Fields (GAF)
&lt;/p&gt;
&lt;p&gt;
Anomalous diffusion is present at all scales, from atomic to large scales. Some exemplary systems are; ultra-cold atoms, telomeres in the nucleus of cells, moisture transport in cement-based materials, the free movement of arthropods, and the migration patterns of birds. The characterization of the diffusion gives critical information about the dynamics of these systems and provides an interdisciplinary framework with which to study diffusive transport. Thus, the problem of identifying underlying diffusive regimes and inferring the anomalous diffusion exponent {$\alpha$} with high confidence is critical to physics, chemistry, biology, and ecology. Classification and analysis of raw trajectories combining machine learning techniques with statistics extracted from them have widely been studied in the Anomalous Diffusion Challenge ge (Munoz-Gil et al., 2021). Here we present a new data-driven method for working with diffusive trajectories. This method utilizes Gramian Angular Fields (GAF)
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01405</link><description>&lt;p&gt;
&#34920;&#31034;&#24037;&#31243;&#21270;&#65306;AI&#36879;&#26126;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#25551;&#36848;&#20102;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;RepE&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#25110;&#30005;&#36335;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RepE&#25216;&#26415;&#30340;&#22522;&#20934;&#21644;&#21021;&#27493;&#20998;&#26512;&#65292;&#26174;&#31034;&#23427;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25913;&#21892;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#22312;&#21253;&#25324;&#35802;&#23454;&#24615;&#12289;&#26080;&#23475;&#24615;&#12289;&#36861;&#27714;&#26435;&#21147;&#31561;&#19968;&#31995;&#21015;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#21457;&#25381;&#20316;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#19978;&#32780;&#19979;&#36879;&#26126;&#24615;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#20419;&#36827;RepE&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#65292;&#24182;&#25512;&#21160;AI&#31995;&#32479;&#30340;&#36879;&#26126;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;ChatGPT&#26816;&#27979;&#26041;&#27861;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27867;&#21270;&#34892;&#20026;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#25991;&#26412;&#38271;&#24230;&#12289;&#20027;&#39064;&#21644;&#35821;&#35328;&#20219;&#21153;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#25581;&#31034;&#20102;&#26377;&#21551;&#31034;&#24615;&#30340;&#21457;&#29616;&#65292;&#20026;&#26410;&#26469;&#26041;&#27861;&#25110;&#25968;&#25454;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.01307</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#35757;&#32451;&#30340;ChatGPT&#26816;&#27979;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization of Training-based ChatGPT Detection Methods. (arXiv:2310.01307v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;ChatGPT&#26816;&#27979;&#26041;&#27861;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27867;&#21270;&#34892;&#20026;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#25991;&#26412;&#38271;&#24230;&#12289;&#20027;&#39064;&#21644;&#35821;&#35328;&#20219;&#21153;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#25581;&#31034;&#20102;&#26377;&#21551;&#31034;&#24615;&#30340;&#21457;&#29616;&#65292;&#20026;&#26410;&#26469;&#26041;&#27861;&#25110;&#25968;&#25454;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#26377;&#36843;&#20999;&#30340;&#38656;&#27714;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#20013;&#26816;&#27979;&#20986;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#19968;&#31181;&#24191;&#27867;&#30740;&#31350;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#26469;&#21306;&#20998;&#20108;&#32773;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20063;&#34920;&#26126;&#65292;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#21463;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#39044;&#27979;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#20219;&#21153;&#25110;&#20027;&#39064;&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#26080;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20840;&#38754;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#22312;&#30001;&#22810;&#31181;&#22240;&#32032;&#24341;&#36215;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27867;&#21270;&#34892;&#20026;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#25991;&#26412;&#38271;&#24230;&#12289;&#20027;&#39064;&#21644;&#35821;&#35328;&#20219;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#21644;ChatGPT&#25991;&#26412;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#23545;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26377;&#21551;&#31034;&#24615;&#30340;&#21457;&#29616;&#65292;&#20026;&#26410;&#26469;&#26041;&#27861;&#25110;&#25968;&#25454;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is one of the most popular language models which achieve amazing performance on various natural language tasks. Consequently, there is also an urgent need to detect the texts generated ChatGPT from human written. One of the extensively studied methods trains classification models to distinguish both. However, existing studies also demonstrate that the trained models may suffer from distribution shifts (during test), i.e., they are ineffective to predict the generated texts from unseen language tasks or topics. In this work, we aim to have a comprehensive investigation on these methods' generalization behaviors under distribution shift caused by a wide range of factors, including prompts, text lengths, topics, and language tasks. To achieve this goal, we first collect a new dataset with human and ChatGPT texts, and then we conduct extensive studies on the collected dataset. Our studies unveil insightful findings which provide guidance for developing future methodologies or data 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#31038;&#20250;&#27979;&#37327;&#23398;&#21644;&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;ODNet&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#20998;&#26512;&#21644;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.01272</link><description>&lt;p&gt;
&#36890;&#36807;&#24847;&#35265;&#21160;&#24577;&#20013;&#30340;&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#23545;&#31038;&#20132;&#32593;&#32476;&#36827;&#34892;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
A Unified View on Neural Message Passing with Opinion Dynamics for Social Networks. (arXiv:2310.01272v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#31038;&#20250;&#27979;&#37327;&#23398;&#21644;&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;ODNet&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#20998;&#26512;&#21644;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#26159;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#39046;&#22495;&#20013;&#24120;&#35265;&#30340;&#30456;&#20114;&#36830;&#25509;&#25968;&#25454;&#24418;&#24335;&#65292;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#22270;&#24418;&#12290;&#36825;&#20123;&#31038;&#32676;&#22312;&#31038;&#20132;&#32852;&#31995;&#20013;&#36890;&#36807;&#19981;&#26029;&#30340;&#20869;&#37096;&#27807;&#36890;&#21644;&#24847;&#35265;&#20132;&#27969;&#23454;&#29616;&#31283;&#23450;&#24615;&#12290;&#32780;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#25552;&#20379;&#20102;&#19968;&#20010;&#28165;&#26224;&#30452;&#35266;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20013;&#36830;&#25509;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#19982;&#32858;&#21512;&#12290;&#33410;&#28857;&#34920;&#31034;&#20250;&#26681;&#25454;&#37051;&#23621;&#33410;&#28857;&#30340;&#36830;&#36890;&#24615;&#21644;&#29366;&#24577;&#36827;&#34892;&#21160;&#24577;&#26356;&#26032;&#12290;&#26412;&#30740;&#31350;&#23558;&#31038;&#20250;&#27979;&#37327;&#23398;&#21644;&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#30340;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#65292;&#20998;&#26512;&#21644;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#21463;&#21040;&#31038;&#20250;&#23398;&#20013;&#24847;&#35265;&#21160;&#24577;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ODNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#23558;&#26377;&#30028;&#20449;&#24515;&#34701;&#20837;&#21040;&#23616;&#37096;&#33410;&#28857;&#30340;&#24433;&#21709;&#26435;&#37325;&#20197;&#25913;&#36827;&#28040;&#24687;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social networks represent a common form of interconnected data frequently depicted as graphs within the domain of deep learning-based inference. These communities inherently form dynamic systems, achieving stability through continuous internal communications and opinion exchanges among social actors along their social ties. In contrast, neural message passing in deep learning provides a clear and intuitive mathematical framework for understanding information propagation and aggregation among connected nodes in graphs. Node representations are dynamically updated by considering both the connectivity and status of neighboring nodes. This research harmonizes concepts from sociometry and neural message passing to analyze and infer the behavior of dynamic systems. Drawing inspiration from opinion dynamics in sociology, we propose ODNet, a novel message passing scheme incorporating bounded confidence, to refine the influence weight of local nodes for message propagation. We adjust the simila
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#20041;&#30456;&#20284;&#30340;&#31867;&#26469;&#25552;&#21462;&#23376;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.01259</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;&#25512;&#29702;&#23454;&#29616;&#26356;&#24555;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Faster and Accurate Neural Networks with Semantic Inference. (arXiv:2310.01259v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#20041;&#30456;&#20284;&#30340;&#31867;&#26469;&#25552;&#21462;&#23376;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#19987;&#38376;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#20934;&#30830;&#29575;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20869;&#22312;&#20887;&#20313;&#26469;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#36755;&#20837;&#20849;&#20139;&#35768;&#22810;&#28388;&#27874;&#22120;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#26089;&#30340;&#23618;&#27425;&#19978;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#23545;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#31867;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#21019;&#24314;&#29305;&#23450;&#20110;&#32858;&#31867;&#30340;&#23376;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;SINF&#65288;i&#65289;&#20351;&#29992;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#23545;&#35937;&#23646;&#20110;&#30340;&#35821;&#20041;&#32858;&#31867;&#65292;&#24182;&#65288;ii&#65289;&#25191;&#34892;&#19982;&#35813;&#35821;&#20041;&#32858;&#31867;&#30456;&#20851;&#30340;&#22522;&#26412;DNN&#25552;&#21462;&#30340;&#23376;&#22270;&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#20102;&#25552;&#21462;&#27599;&#20010;&#29305;&#23450;&#20110;&#32858;&#31867;&#30340;&#23376;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21306;&#20998;&#33021;&#21147;&#24471;&#20998;&#65288;DCS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) usually come with a significant computational burden. While approaches such as structured pruning and mobile-specific DNNs have been proposed, they incur drastic accuracy loss. In this paper we leverage the intrinsic redundancy in latent representations to reduce the computational load with limited loss in performance. We show that semantically similar inputs share many filters, especially in the earlier layers. Thus, semantically similar classes can be clustered to create cluster-specific subgraphs. To this end, we propose a new framework called Semantic Inference (SINF). In short, SINF (i) identifies the semantic cluster the object belongs to using a small additional classifier and (ii) executes the subgraph extracted from the base DNN related to that semantic cluster for inference. To extract each cluster-specific subgraph, we propose a new approach named Discriminative Capability Score (DCS) that finds the subgraph with the capability to discriminate amon
&lt;/p&gt;</description></item><item><title>appjsonify&#26159;&#19968;&#31181;&#23398;&#26415;&#35770;&#25991;PDF&#21040;JSON&#36716;&#25442;&#24037;&#20855;&#21253;&#65292;&#23427;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#21644;&#26041;&#27861;&#35299;&#26512;PDF&#25991;&#20214;&#65292;&#24182;&#19988;&#20801;&#35768;&#29992;&#25143;&#28789;&#27963;&#37197;&#32622;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.01206</link><description>&lt;p&gt;
appjsonify&#65306;&#19968;&#31181;&#23398;&#26415;&#35770;&#25991;PDF&#21040;JSON&#36716;&#25442;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
appjsonify: An Academic Paper PDF-to-JSON Conversion Toolkit. (arXiv:2310.01206v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01206
&lt;/p&gt;
&lt;p&gt;
appjsonify&#26159;&#19968;&#31181;&#23398;&#26415;&#35770;&#25991;PDF&#21040;JSON&#36716;&#25442;&#24037;&#20855;&#21253;&#65292;&#23427;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#21644;&#26041;&#27861;&#35299;&#26512;PDF&#25991;&#20214;&#65292;&#24182;&#19988;&#20801;&#35768;&#29992;&#25143;&#28789;&#27963;&#37197;&#32622;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;appjsonify&#65292;&#19968;&#31181;&#22522;&#20110;Python&#30340;&#23398;&#26415;&#35770;&#25991;PDF&#21040;JSON&#36716;&#25442;&#24037;&#20855;&#21253;&#12290;&#23427;&#20351;&#29992;&#22810;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#27169;&#22411;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#26512;PDF&#25991;&#20214;&#12290;appjsonify&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#24037;&#20855;&#65292;&#20801;&#35768;&#29992;&#25143;&#36731;&#26494;&#37197;&#32622;&#22788;&#29702;&#27969;&#31243;&#65292;&#20197;&#22788;&#29702;&#29305;&#23450;&#26684;&#24335;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;appjsonify&#20316;&#20026;&#19968;&#20010;&#26131;&#20110;&#23433;&#35013;&#30340;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#36890;&#36807;PyPI&#21644;GitHub&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present appjsonify, a Python-based PDF-to-JSON conversion toolkit for academic papers. It parses a PDF file using several visual-based document layout analysis models and rule-based text processing approaches. appjsonify is a flexible tool that allows users to easily configure the processing pipeline to handle a specific format of a paper they wish to process. We are publicly releasing appjsonify as an easy-to-install toolkit available via PyPI and GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#65288;GINs&#65289;&#36827;&#34892;&#20013;&#21387;&#30005;&#32593;&#30340;n-1&#35780;&#20272;&#65292;&#30456;&#27604;&#20256;&#32479;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#65292;GIN&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#24555;&#21644;&#26356;&#21487;&#38752;&#30340;&#30005;&#32593;&#35780;&#20272;&#65292;&#23558;&#39044;&#27979;&#26102;&#38388;&#32553;&#30701;&#32422;1000&#20493;&#12290;</title><link>http://arxiv.org/abs/2310.01181</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#20013;&#21387;&#30005;&#32593;&#21487;&#38752;&#24615;&#30340;&#22270;&#21516;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Isomorphic Networks for Assessing Reliability of the Medium-Voltage Grid. (arXiv:2310.01181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#65288;GINs&#65289;&#36827;&#34892;&#20013;&#21387;&#30005;&#32593;&#30340;n-1&#35780;&#20272;&#65292;&#30456;&#27604;&#20256;&#32479;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#65292;GIN&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#24555;&#21644;&#26356;&#21487;&#38752;&#30340;&#30005;&#32593;&#35780;&#20272;&#65292;&#23558;&#39044;&#27979;&#26102;&#38388;&#32553;&#30701;&#32422;1000&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#36716;&#21464;&#21644;&#20256;&#32479;&#23481;&#37327;&#30340;&#19979;&#38477;&#65292;&#30830;&#20445;&#30005;&#32593;&#30340;&#21487;&#38752;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#37197;&#30005;&#31995;&#32479;&#36816;&#33829;&#21830;&#65288;DSO&#65289;&#36890;&#36807;&#39564;&#35777;n-1&#21407;&#21017;&#26469;&#23454;&#29616;&#30005;&#32593;&#30340;&#21487;&#38752;&#24615;&#65292;&#20197;&#30830;&#20445;&#32452;&#20214;&#25925;&#38556;&#26102;&#30340;&#36830;&#32493;&#36816;&#34892;&#12290;&#30005;&#21147;&#32593;&#32476;&#30340;&#22797;&#26434;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#21253;&#21547;&#20851;&#38190;&#30340;n-1&#35780;&#20272;&#20449;&#24687;&#65306;&#22270;&#32467;&#26500;&#21644;&#26377;&#20851;&#33410;&#28857;/&#30005;&#32518;&#30340;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30452;&#25509;&#22788;&#29702;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#22312;&#20013;&#21387;&#30005;&#32593;&#20013;&#20351;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#65288;GINs&#65289;&#36827;&#34892;n-1&#35780;&#20272;&#12290;GIN&#26694;&#26550;&#30340;&#35774;&#35745;&#26159;&#20026;&#20102;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#30005;&#32593;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#21644;&#26377;&#20851;&#33410;&#28857;/&#30005;&#32518;&#30340;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;GIN&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#24555;&#21644;&#26356;&#21487;&#38752;&#30340;&#30005;&#32593;&#35780;&#20272;&#65292;&#23558;&#39044;&#27979;&#26102;&#38388;&#32553;&#30701;&#32422;1000&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring electricity grid reliability becomes increasingly challenging with the shift towards renewable energy and declining conventional capacities. Distribution System Operators (DSOs) aim to achieve grid reliability by verifying the n-1 principle, ensuring continuous operation in case of component failure. Electricity networks' complex graph-based data holds crucial information for n-1 assessment: graph structure and data about stations/cables. Unlike traditional machine learning methods, Graph Neural Networks (GNNs) directly handle graph-structured data. This paper proposes using Graph Isomorphic Networks (GINs) for n-1 assessments in medium voltage grids. The GIN framework is designed to generalise to unseen grids and utilise graph structure and data about stations/cables. The proposed GIN approach demonstrates faster and more reliable grid assessments than a traditional mathematical optimisation approach, reducing prediction times by approximately a factor of 1000. The findings o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.00535</link><description>&lt;p&gt;
JoMA: &#36890;&#36807;MLP&#21644;&#27880;&#24847;&#21147;&#30340;&#32852;&#21512;&#21160;&#21147;&#23398;&#26469;&#35299;&#23494;&#22810;&#23618;Transformer
&lt;/p&gt;
&lt;p&gt;
JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention. (arXiv:2310.00535v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;Transformer&#20013;&#21435;&#38500;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#24471;&#21040;&#20165;&#21253;&#21547;MLP&#23618;&#30340;&#20462;&#25913;&#21518;&#21160;&#24577;&#12290;JoMA&#28040;&#38500;&#20102;&#20808;&#21069;&#20998;&#26512;&#20013;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#32570;&#20047;&#27531;&#24046;&#36830;&#25509;&#65289;&#65292;&#24182;&#39044;&#27979;&#27880;&#24847;&#21147;&#22312;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#24773;&#20917;&#19979;&#39318;&#20808;&#21464;&#24471;&#31232;&#30095;&#65288;&#20026;&#20102;&#23398;&#20064;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#28982;&#21518;&#21464;&#24471;&#23494;&#38598;&#65288;&#20026;&#20102;&#23398;&#20064;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#32780;&#22312;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#23427;&#19982;&#29616;&#26377;&#30740;&#31350;&#19968;&#33268;&#65292;&#26174;&#31034;&#20986;&#27880;&#24847;&#21147;&#38543;&#26102;&#38388;&#21464;&#24471;&#31232;&#30095;&#12290;&#25105;&#20204;&#21033;&#29992;JoMA&#23450;&#24615;&#22320;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#22914;&#20309;&#23558;&#26631;&#35760;&#32452;&#21512;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#24403;&#36755;&#20837;&#26631;&#35760;&#26159;&#30001;&#28508;&#22312;&#30340;&#23618;&#27425;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#26102;&#12290;&#22312;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;Wikitext2/Wikitext103&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;OPT&#65292;Pythia&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00339</link><description>&lt;p&gt;
FedLPA: &#20351;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation. (arXiv:2310.00339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#32858;&#21512;&#21040;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#23616;&#27169;&#22411;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#38544;&#31169;&#38382;&#39064;&#20943;&#23569;&#12289;&#28508;&#22312;&#25915;&#20987;&#20943;&#24369;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#30340;&#25512;&#21160;&#65292;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#65288;&#21363;&#23558;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#38388;&#30340;&#36890;&#20449;&#38480;&#21046;&#20026;&#19968;&#36718;&#65289;&#22312;&#30740;&#31350;&#32773;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#21333;&#27425;&#32858;&#21512;&#30340;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#27425;&#32858;&#21512;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#65288;FedLPA&#65289;&#12290;FedLPA&#33021;&#22815;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#25110;&#26292;&#38706;&#20219;&#20309;&#26426;&#23494;&#30340;&#26412;&#22320;&#20449;&#24687;&#65292;&#27604;&#22914;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in
&lt;/p&gt;</description></item><item><title>ONNXExplainer&#26159;&#19968;&#20010;&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#35299;&#37322;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2309.16916</link><description>&lt;p&gt;
ONNXExplainer:&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values. (arXiv:2309.16916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16916
&lt;/p&gt;
&lt;p&gt;
ONNXExplainer&#26159;&#19968;&#20010;&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#35299;&#37322;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20026;&#20160;&#20040;&#20250;&#20570;&#20986;&#26576;&#20123;&#20915;&#31574;&#19982;&#25512;&#29702;&#24615;&#33021;&#19968;&#26679;&#37325;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#24110;&#21161;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20854;&#20013;Shapley&#20540;&#26368;&#21463;&#27426;&#36814;&#12290;SHAP&#21253;&#26159;&#35299;&#37322;&#20351;&#29992;TensorFlow&#25110;PyTorch&#23454;&#29616;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;Shapley&#20540;&#30340;&#39046;&#20808;&#23454;&#29616;&#65292;&#20294;&#32570;&#20047;&#36328;&#24179;&#21488;&#25903;&#25345;&#12289;&#19968;&#27425;&#24615;&#37096;&#32626;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ONNXExplainer&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ONNX&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;Shapley&#20540;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#22312;ONNXExplainer&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#33258;&#24049;&#30340;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#19981;&#20165;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#19968;&#27425;&#24615;&#37096;&#32626;&#65292;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#28040;&#32791;&#12290;&#20026;&#20102;&#20844;&#24179;&#27604;&#36739;&#30446;&#30340;&#65292;&#25105;&#20204;&#36824;&#22312;TensorFlow&#21644;PyTorch&#20013;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding why a neural network model makes certain decisions can be as important as the inference performance. Various methods have been proposed to help practitioners explain the prediction of a neural network model, of which Shapley values are most popular. SHAP package is a leading implementation of Shapley values to explain neural networks implemented in TensorFlow or PyTorch but lacks cross-platform support, one-shot deployment and is highly inefficient. To address these problems, we present the ONNXExplainer, which is a generic framework to explain neural networks using Shapley values in the ONNX ecosystem. In ONNXExplainer, we develop its own automatic differentiation and optimization approach, which not only enables One-Shot Deployment of neural networks inference and explanations, but also significantly improves the efficiency to compute explanation with less memory consumption. For fair comparison purposes, we also implement the same optimization in TensorFlow and PyTorch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20809;&#23376;&#21152;&#36895;&#22120;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#32570;&#38519;&#26816;&#27979;&#20013;&#30340;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#20809;&#23376;&#21152;&#36895;&#22120;&#25191;&#34892;&#29305;&#23450;&#30340;&#20998;&#21106;&#27169;&#22411;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#19978;&#24573;&#30053;&#25439;&#22833;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#27169;&#22411;&#34987;&#24178;&#25200;&#26102;&#20462;&#22797;&#20934;&#30830;&#24615;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16783</link><description>&lt;p&gt;
&#20809;&#23376;&#21152;&#36895;&#22120;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#32570;&#38519;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Photonic Accelerators for Image Segmentation in Autonomous Driving and Defect Detection. (arXiv:2309.16783v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20809;&#23376;&#21152;&#36895;&#22120;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#32570;&#38519;&#26816;&#27979;&#20013;&#30340;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#20809;&#23376;&#21152;&#36895;&#22120;&#25191;&#34892;&#29305;&#23450;&#30340;&#20998;&#21106;&#27169;&#22411;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#19978;&#24573;&#30053;&#25439;&#22833;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#27169;&#22411;&#34987;&#24178;&#25200;&#26102;&#20462;&#22797;&#20934;&#30830;&#24615;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23376;&#35745;&#31639;&#25215;&#35834;&#27604;&#20256;&#32479;&#30340;&#25968;&#23383;&#30828;&#20214;&#26356;&#24555;&#36895;&#21644;&#33021;&#37327;&#26356;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#12290;&#20809;&#23376;&#35745;&#31639;&#30340;&#36827;&#27493;&#21487;&#20197;&#23545;&#20381;&#36182;&#20110;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#33021;&#37327;&#39640;&#25928;&#25191;&#34892;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#32570;&#38519;&#26816;&#27979;&#65292;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20809;&#23376;&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#65292;&#25506;&#32034;&#20102;&#36866;&#21512;&#20809;&#23376;&#21152;&#36895;&#22120;&#30340;&#22270;&#20687;&#20998;&#21106;DNN&#26550;&#26500;&#31867;&#22411;&#20197;&#21450;&#22312;&#20809;&#23376;&#21152;&#36895;&#22120;&#19978;&#25191;&#34892;&#19981;&#21516;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#20197;&#21450;&#20854;&#20013;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22312;&#20809;&#23376;&#21152;&#36895;&#22120;&#19978;&#25191;&#34892;&#26102;&#65292;&#26576;&#20123;&#20998;&#21106;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65288;&#19982;&#25968;&#23383;float32&#27169;&#22411;&#30456;&#27604;&#65289;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#31283;&#20581;&#24615;&#30340;&#32463;&#39564;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;&#27169;&#22411;&#30340;&#20462;&#22797;&#20934;&#30830;&#24615;&#30340;&#25216;&#26415;&#65288;&#22312;&#27169;&#22411;&#34987;&#24178;&#25200;&#26102;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photonic computing promises faster and more energy-efficient deep neural network (DNN) inference than traditional digital hardware. Advances in photonic computing can have profound impacts on applications such as autonomous driving and defect detection that depend on fast, accurate and energy efficient execution of image segmentation models. In this paper, we investigate image segmentation on photonic accelerators to explore: a) the types of image segmentation DNN architectures that are best suited for photonic accelerators, and b) the throughput and energy efficiency of executing the different image segmentation models on photonic accelerators, along with the trade-offs involved therein. Specifically, we demonstrate that certain segmentation models exhibit negligible loss in accuracy (compared to digital float32 models) when executed on photonic accelerators, and explore the empirical reasoning for their robustness. We also discuss techniques for recovering accuracy in the case of mod
&lt;/p&gt;</description></item><item><title>XVO&#26159;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#24577;&#33258;&#25105;&#35757;&#32451;&#30340;&#27867;&#21270;&#35270;&#35273;&#37324;&#31243;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#35774;&#32622;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#32473;&#33258;&#36275;&#25805;&#20316;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#20854;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#21253;&#25324;&#36890;&#36807;&#21322;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#36890;&#29992;&#30340;&#30452;&#25509;VO&#22238;&#24402;&#32593;&#32476;&#20197;&#21450;&#20351;&#29992;&#22810;&#27169;&#24335;&#30417;&#30563;&#20219;&#21153;&#26469;&#20419;&#36827;&#27867;&#21270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.16772</link><description>&lt;p&gt;
XVO: &#36890;&#36807;&#36328;&#27169;&#24577;&#33258;&#25105;&#35757;&#32451;&#30340;&#27867;&#21270;&#35270;&#35273;&#37324;&#31243;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
XVO: Generalized Visual Odometry via Cross-Modal Self-Training. (arXiv:2309.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16772
&lt;/p&gt;
&lt;p&gt;
XVO&#26159;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#24577;&#33258;&#25105;&#35757;&#32451;&#30340;&#27867;&#21270;&#35270;&#35273;&#37324;&#31243;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#35774;&#32622;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#32473;&#33258;&#36275;&#25805;&#20316;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#20854;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#21253;&#25324;&#36890;&#36807;&#21322;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#36890;&#29992;&#30340;&#30452;&#25509;VO&#22238;&#24402;&#32593;&#32476;&#20197;&#21450;&#20351;&#29992;&#22810;&#27169;&#24335;&#30417;&#30563;&#20219;&#21153;&#26469;&#20419;&#36827;&#27867;&#21270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;XVO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#35774;&#32622;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#32473;&#33258;&#36275;&#25805;&#20316;&#30340;&#27867;&#21270;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65288;VO&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#19982;&#36890;&#24120;&#30740;&#31350;&#21333;&#20010;&#25968;&#25454;&#38598;&#20869;&#24050;&#30693;&#26657;&#20934;&#30340;&#26631;&#20934;&#21333;&#30446;VO&#26041;&#27861;&#19981;&#21516;&#65292;XVO&#21487;&#20197;&#39640;&#25928;&#22320;&#36890;&#36807;&#35270;&#35273;&#22330;&#26223;&#35821;&#20041;&#65288;&#21363;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#24050;&#30693;&#30456;&#26426;&#21442;&#25968;&#65289;&#23398;&#20064;&#24674;&#22797;&#30456;&#23545;&#20301;&#23039;&#65292;&#24182;&#20174;YouTube&#19978;&#30340;&#22823;&#37327;&#26080;&#32422;&#26463;&#21644;&#24322;&#26500;&#30340;&#36710;&#36733;&#25668;&#20687;&#22836;&#35270;&#39057;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#26469;&#20248;&#21270;&#36816;&#21160;&#20272;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#31532;&#19968;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#20102;&#21322;&#30417;&#30563;&#35757;&#32451;&#23545;&#20110;&#23398;&#20064;&#36890;&#29992;&#30340;&#30452;&#25509;VO&#22238;&#24402;&#32593;&#32476;&#30340;&#22909;&#22788;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#27169;&#24335;&#30417;&#30563;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20998;&#21106;&#12289;&#20809;&#27969;&#12289;&#28145;&#24230;&#21644;&#38899;&#39057;&#36741;&#21161;&#39044;&#27979;&#20219;&#21153;&#65292;&#20197;&#20419;&#36827;VO&#20219;&#21153;&#30340;&#27867;&#21270;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#38899;&#39057;&#39044;&#27979;&#20219;&#21153;&#23545;&#20110;&#24635;&#32467;&#25688;&#35201;&#30340;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#26377;&#20419;&#36827;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with robust off-the-self operation across diverse datasets and settings. In contrast to standard monocular VO approaches which often study a known calibration within a single dataset, XVO efficiently learns to recover relative pose with real-world scale from visual scene semantics, i.e., without relying on any known camera parameters. We optimize the motion estimation model via self-training from large amounts of unconstrained and heterogeneous dash camera videos available on YouTube. Our key contribution is twofold. First, we empirically demonstrate the benefits of semi-supervised training for learning a general-purpose direct VO regression network. Second, we demonstrate multi-modal supervision, including segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task. Specifically, we find audio prediction task to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16742</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#39118;&#38505;&#30340;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#65292;&#23588;&#20854;&#26159;2&#22411;&#31958;&#23615;&#30149;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#19982;&#31958;&#23615;&#30149;&#30456;&#20851;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#20854;&#24182;&#21457;&#30151;&#30340;&#21457;&#23637;&#12290;&#31958;&#23615;&#30149;&#32958;&#30149;&#26159;&#31958;&#23615;&#30149;&#30340;&#19968;&#31181;&#24930;&#24615;&#24182;&#21457;&#30151;&#65292;&#19981;&#21033;&#22320;&#24433;&#21709;&#32958;&#33039;&#65292;&#23548;&#33268;&#32958;&#33039;&#25439;&#20260;&#12290;&#35786;&#26029;&#31958;&#23615;&#30149;&#32958;&#30149;&#28041;&#21450;&#32771;&#34385;&#21508;&#31181;&#26631;&#20934;&#20043;&#19968;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#30340;&#30149;&#29702;&#23398;&#30149;&#29702;&#23398;&#25968;&#37327;&#65292;&#31216;&#20026;&#30333;&#34507;&#30333;&#23615;&#12290;&#22240;&#27492;&#65292;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#20855;&#26377;&#21450;&#26102;&#39044;&#38450;&#25514;&#26045;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#24739;&#26377;&#30333;&#34507;&#30333;&#23615;&#30340;&#39118;&#38505;&#12290;&#25152;&#36873;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;AdaBoost&#65292;XGBoost&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#25105;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#21253;&#25324;184&#26465;&#31958;&#23615;&#30149;&#24182;&#21457;&#30151;&#39118;&#38505;&#22240;&#32032;&#30340;&#26465;&#30446;&#34987;&#29992;&#26469;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
&lt;/p&gt;</description></item><item><title>&#27531;&#24046;&#35843;&#24230;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#31227;&#38500;&#19981;&#30456;&#20851;&#30340;&#26426;&#22120;&#21644;&#20316;&#19994;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.15517</link><description>&lt;p&gt;
&#27531;&#24046;&#35843;&#24230;&#65306;&#35299;&#20915;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#30340;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Residual Scheduling: A New Reinforcement Learning Approach to Solving Job Shop Scheduling Problem. (arXiv:2309.15517v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15517
&lt;/p&gt;
&lt;p&gt;
&#27531;&#24046;&#35843;&#24230;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#31227;&#38500;&#19981;&#30456;&#20851;&#30340;&#26426;&#22120;&#21644;&#20316;&#19994;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSP&#65289;&#26159;&#19968;&#31181;&#22312;&#21046;&#36896;&#19994;&#31561;&#34892;&#19994;&#24191;&#27867;&#24212;&#29992;&#30340;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#65292;&#32780;&#28789;&#27963;&#30340;JSP&#65288;FJSP&#65289;&#21017;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21464;&#20307;&#12290;&#30001;&#20110;&#23427;&#20204;&#26159;NP-hard&#38382;&#39064;&#65292;&#24456;&#38590;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#25214;&#21040;&#25152;&#26377;&#24773;&#20917;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#27492;&#24320;&#21457;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;JSP/FJSP&#38382;&#39064;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#24456;&#22810;&#26500;&#24314;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27531;&#24046;&#35843;&#24230;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;JSP/FJSP&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#26032;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#31227;&#38500;&#20102;&#19981;&#30456;&#20851;&#30340;&#26426;&#22120;&#21644;&#20316;&#19994;&#65292;&#20363;&#22914;&#24050;&#23436;&#25104;&#30340;&#26426;&#22120;&#21644;&#20316;&#19994;&#65292;&#20197;&#20415;&#29366;&#24577;&#20165;&#21253;&#21547;&#21097;&#19979;&#30340;&#65288;&#25110;&#30456;&#20851;&#30340;&#65289;&#26426;&#22120;&#21644;&#20316;&#19994;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#30693;&#21517;&#30340;&#26500;&#24314;&#21551;&#21457;&#24335;&#31639;&#27861;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Job-shop scheduling problem (JSP) is a mathematical optimization problem widely used in industries like manufacturing, and flexible JSP (FJSP) is also a common variant. Since they are NP-hard, it is intractable to find the optimal solution for all cases within reasonable times. Thus, it becomes important to develop efficient heuristics to solve JSP/FJSP. A kind of method of solving scheduling problems is construction heuristics, which constructs scheduling solutions via heuristics. Recently, many methods for construction heuristics leverage deep reinforcement learning (DRL) with graph neural networks (GNN). In this paper, we propose a new approach, named residual scheduling, to solving JSP/FJSP. In this new approach, we remove irrelevant machines and jobs such as those finished, such that the states include the remaining (or relevant) machines and jobs only. Our experiments show that our approach reaches state-of-the-art (SOTA) among all known construction heuristics on most well-known
&lt;/p&gt;</description></item><item><title>Supersonic &#26159;&#19968;&#20010;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;C/C++&#20013;&#36827;&#34892;&#28304;&#20195;&#30721;&#20248;&#21270;&#12290;&#19982;GPT-3.5-Turbo&#21644;GPT-4&#30456;&#27604;&#65292;&#23427;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.14846</link><description>&lt;p&gt;
Supersonic: &#23398;&#20064;&#22312;C/C++&#20013;&#29983;&#25104;&#28304;&#20195;&#30721;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Supersonic: Learning to Generate Source Code Optimisations in C/C++. (arXiv:2309.14846v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14846
&lt;/p&gt;
&lt;p&gt;
Supersonic &#26159;&#19968;&#20010;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;C/C++&#20013;&#36827;&#34892;&#28304;&#20195;&#30721;&#20248;&#21270;&#12290;&#19982;GPT-3.5-Turbo&#21644;GPT-4&#30456;&#27604;&#65292;&#23427;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#20248;&#21270;&#22312;&#20445;&#25345;&#21151;&#33021;&#30340;&#21516;&#26102;&#25913;&#21892;&#36164;&#28304;&#25928;&#29575;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#26159;&#30001;&#24320;&#21457;&#20154;&#21592;&#21644;&#32534;&#35793;&#22120;&#23436;&#25104;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19977;&#31181;&#36873;&#25321;&#65292;&#21363;&#22312;&#28304;&#20195;&#30721;&#32423;&#21035;&#36827;&#34892;&#33258;&#21160;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Supersonic&#65292;&#19968;&#20010;&#38024;&#23545;&#20248;&#21270;&#30340;&#36731;&#24494;&#28304;&#20195;&#30721;&#20462;&#25913;&#30340;&#31070;&#32463;&#26041;&#27861;&#12290;&#20351;&#29992;seq2seq&#27169;&#22411;&#65292;Supersonic&#22312;C / C ++&#31243;&#24207;&#23545;&#65288;$x_{t}$&#65292;$x_{t+1}$&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;$x_{t+1}$&#26159;$x_{t}$&#30340;&#20248;&#21270;&#29256;&#26412;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#24046;&#24322;&#12290;Supersonic&#30340;&#24615;&#33021;&#22312;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#19978;&#19982;OpenAI&#30340;GPT-3.5-Turbo&#21644;GPT-4&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Supersonic&#19981;&#20165;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#32988;&#36807;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#32780;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#27604;GPT-3.5-Turbo&#23567;&#20102;600&#22810;&#20493;&#65292;&#27604;GPT-4&#23567;&#20102;3700&#22810;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software optimization refines programs for resource efficiency while preserving functionality. Traditionally, it is a process done by developers and compilers. This paper introduces a third option, automated optimization at the source code level. We present Supersonic, a neural approach targeting minor source code modifications for optimization. Using a seq2seq model, Supersonic is trained on C/C++ program pairs ($x_{t}$, $x_{t+1}$), where $x_{t+1}$ is an optimized version of $x_{t}$, and outputs a diff. Supersonic's performance is benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming tasks. The experiments show that Supersonic not only outperforms both models on the code optimization task, but also minimizes the extent of change with a more than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#22312;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#20013;&#24212;&#29992;&#20998;&#32780;&#27835;&#20043;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20154;&#20307;&#39592;&#39612;&#26694;&#26550;&#21644;&#35270;&#39057;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;ShanghaiTech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14622</link><description>&lt;p&gt;
&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20998;&#32780;&#27835;&#20043;&#65306;&#32508;&#36848;&#19982;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Divide and Conquer in Video Anomaly Detection: A Comprehensive Review and New Approach. (arXiv:2309.14622v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#22312;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#20013;&#24212;&#29992;&#20998;&#32780;&#27835;&#20043;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20154;&#20307;&#39592;&#39612;&#26694;&#26550;&#21644;&#35270;&#39057;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;ShanghaiTech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#39033;&#22797;&#26434;&#20219;&#21153;&#65292;&#32780;&#8220;&#20998;&#32780;&#27835;&#20043;&#8221;&#30340;&#21407;&#21017;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#25581;&#31034;&#20102;&#20998;&#32780;&#27835;&#20043;&#29702;&#24565;&#30340;&#24212;&#29992;&#65288;&#23613;&#31649;&#19982;&#20256;&#32479;&#29992;&#27861;&#26377;&#25152;&#19981;&#21516;&#65289;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#26412;&#25991;&#20174;&#20845;&#20010;&#32500;&#24230;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20123;&#25991;&#29486;&#65292;&#26088;&#22312;&#25552;&#21319;&#22312;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#20351;&#29992;&#20998;&#32780;&#27835;&#20043;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20174;&#36825;&#20010;&#32508;&#36848;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#20154;&#20307;&#39592;&#39612;&#26694;&#26550;&#19982;&#35270;&#39057;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;ShanghaiTech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#30340;&#39640;&#32423;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video anomaly detection is a complex task, and the principle of "divide and conquer" is often regarded as an effective approach to tackling intricate issues. It's noteworthy that recent methods in video anomaly detection have revealed the application of the divide and conquer philosophy (albeit with distinct perspectives from traditional usage), yielding impressive outcomes. This paper systematically reviews these literatures from six dimensions, aiming to enhance the use of the divide and conquer strategy in video anomaly detection. Furthermore, based on the insights gained from this review, a novel approach is presented, which integrates human skeletal frameworks with video data analysis techniques. This method achieves state-of-the-art performance on the ShanghaiTech dataset, surpassing all existing advanced methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;</title><link>http://arxiv.org/abs/2309.14610</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22270;&#28145;&#24230;&#23398;&#20064;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#28304;&#20110;&#19982;&#27946;&#27700;&#21361;&#38505;&#12289;&#27946;&#27700;&#26292;&#38706;&#20197;&#21450;&#31038;&#20250;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#27946;&#27700;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#34920;&#24449;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#27946;&#27700;&#24179;&#21407;&#22320;&#22270;&#65292;&#20391;&#37325;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#35201;&#32032;&#65292;&#20027;&#35201;&#26159;&#21361;&#38505;&#21644;&#26292;&#38706;&#35201;&#32032;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#31354;&#38388;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#31216;&#20026;FloodRisk-Net&#65289;&#30340;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#12290;FloodRisk-Net&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#27946;&#27700;&#21361;&#38505;&#21644;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#30830;&#23450;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#12290;&#21033;&#29992;&#32654;&#22269;&#22810;&#20010;&#37117;&#24066;&#32479;&#35745;&#21306;&#65288;MSAs&#65289;&#30340;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#27946;&#27700;&#39118;&#38505;&#29305;&#24449;&#21270;&#20026;
&lt;/p&gt;
&lt;p&gt;
Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#36890;&#36807;&#27861;&#24459;&#21046;&#24230;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#38382;&#39064;&#65292;&#35748;&#20026;&#27861;&#24459;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.12321</link><description>&lt;p&gt;
&#36890;&#36807;&#27861;&#24459;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#30340;&#29702;&#35770;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
A Case for AI Safety via Law. (arXiv:2309.12321v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#36890;&#36807;&#27861;&#24459;&#21046;&#24230;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#38382;&#39064;&#65292;&#35748;&#20026;&#27861;&#24459;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20351;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#23433;&#20840;&#24182;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20542;&#21521;&#20110;&#20381;&#38752;&#20154;&#31867;&#22312;&#19981;&#30830;&#23450;&#24773;&#20917;&#19979;&#30340;&#24178;&#39044;&#65292;&#36890;&#36807;&#35757;&#32451;&#25110;&#35266;&#23519;&#26469;&#23398;&#20064;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#65292;&#25552;&#20379;&#20851;&#38381;&#24320;&#20851;&#65292;&#23454;&#26045;&#38548;&#31163;&#25110;&#27169;&#25311;&#29615;&#22659;&#65292;&#25110;&#25512;&#26029;&#22914;&#26524;&#20154;&#20204;&#26377;&#26356;&#22810;&#30693;&#35782;&#21644;&#26102;&#38388;&#26469;&#24605;&#32771;&#65292;&#20182;&#20204;&#20250;&#24819;&#35201;&#20160;&#20040;&#12290;&#20197;&#27861;&#24459;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861; - &#22914;&#20197;&#33406;&#33832;&#20811;&#183;&#38463;&#35199;&#33707;&#22827;&#20026;&#28789;&#24863; - &#24182;&#19981;&#34987;&#24191;&#27867;&#30475;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#26377;&#25928;&#30340;&#27861;&#24459;&#21046;&#24230;&#26159;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#38382;&#39064;&#30340;&#26368;&#20339;&#36884;&#24452;&#12290;&#27861;&#24459;&#34987;&#23450;&#20041;&#20026;&#23545;&#36866;&#29992;&#20110;&#29305;&#23450;&#20195;&#29702;&#20154;&#22312;&#29305;&#23450;&#39046;&#22495;/&#24773;&#22659;&#20013;&#30340;&#31105;&#27490;&#21644;&#35268;&#23450;&#36827;&#34892;&#32534;&#30721;&#30340;&#20219;&#20309;&#35268;&#21017;&#65292;&#24182;&#21253;&#25324;&#21046;&#23450;&#12289;&#31649;&#29702;&#12289;&#25191;&#34892;&#21644;&#35785;&#35772;&#27492;&#31867;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to make artificial intelligence (AI) systems safe and aligned with human values is an open research question. Proposed solutions tend toward relying on human intervention in uncertain situations, learning human values and intentions through training or observation, providing off-switches, implementing isolation or simulation environments, or extrapolating what people would want if they had more knowledge and more time to think. Law-based approaches--such as inspired by Isaac Asimov--have not been well regarded. This paper makes a case that effective legal systems are the best way to address AI safety. Law is defined as any rules that codify prohibitions and prescriptions applicable to particular agents in specified domains/contexts and includes processes for enacting, managing, enforcing, and litigating such rules.
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#21333;&#23376;&#21477;&#20256;&#25773;&#32780;&#35328;&#65292;&#22312;CNF&#20844;&#24335;&#20013;&#19981;&#21487;&#31616;&#21270;&#30340;&#20844;&#24335;&#65292;&#20854;&#22823;&#23567;&#19982;&#26368;&#23567;&#21487;&#31561;&#20215;&#30340;&#20844;&#24335;&#22823;&#23567;&#30340;&#27604;&#20540;&#26368;&#22823;&#20026;n^2&#65292;&#20854;&#20013;n&#26159;&#21464;&#37327;&#25968;&#37327;&#12290;&#19968;&#33324;&#19978;&#30028;&#19981;&#20250;&#23567;&#20110;n/ln n&#20493;&#12290;</title><link>http://arxiv.org/abs/2309.01750</link><description>&lt;p&gt;
&#20851;&#20110;&#30456;&#23545;&#20110;&#21333;&#23376;&#21477;&#20256;&#25773;&#19981;&#21487;&#31616;&#21270;&#30340;CNF&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
On CNF formulas irredundant with respect to unit clause propagation. (arXiv:2309.01750v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01750
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21333;&#23376;&#21477;&#20256;&#25773;&#32780;&#35328;&#65292;&#22312;CNF&#20844;&#24335;&#20013;&#19981;&#21487;&#31616;&#21270;&#30340;&#20844;&#24335;&#65292;&#20854;&#22823;&#23567;&#19982;&#26368;&#23567;&#21487;&#31561;&#20215;&#30340;&#20844;&#24335;&#22823;&#23567;&#30340;&#27604;&#20540;&#26368;&#22823;&#20026;n^2&#65292;&#20854;&#20013;n&#26159;&#21464;&#37327;&#25968;&#37327;&#12290;&#19968;&#33324;&#19978;&#30028;&#19981;&#20250;&#23567;&#20110;n/ln n&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20004;&#20010;CNF&#20844;&#24335;&#22312;&#21333;&#23376;&#21477;&#20256;&#25773;&#65288;UCP&#65289;&#26041;&#38754;&#30340;&#34892;&#20026;&#30456;&#21516;&#65292;&#21017;&#23427;&#20204;&#34987;&#31216;&#20026;ucp&#31561;&#20215;&#12290;&#22914;&#26524;&#31227;&#38500;&#20219;&#24847;&#19968;&#20010;&#23376;&#21477;&#20250;&#23548;&#33268;&#19968;&#20010;&#19982;&#21407;&#22987;&#20844;&#24335;&#22312;ucp&#26041;&#38754;&#19981;&#31561;&#20215;&#30340;&#20844;&#24335;&#65292;&#21017;&#31216;&#35813;&#20844;&#24335;&#20026;ucp&#19981;&#21487;&#31616;&#21270;&#12290;&#26681;&#25454;&#24050;&#30693;&#32467;&#26524;&#65292;ucp&#19981;&#21487;&#31616;&#21270;&#20844;&#24335;&#30340;&#22823;&#23567;&#19982;&#26368;&#23567;ucp&#31561;&#20215;&#20844;&#24335;&#30340;&#22823;&#23567;&#30340;&#27604;&#20540;&#26368;&#22823;&#20026;n^2&#65292;&#20854;&#20013;n&#26159;&#21464;&#37327;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#31216;&#30830;&#23450;Horn&#20989;&#25968;&#30340;&#19968;&#20010;ucp&#19981;&#21487;&#31616;&#21270;&#20844;&#24335;&#30340;&#20363;&#23376;&#65292;&#20854;&#22823;&#23567;&#27604;&#26368;&#23567;&#30340;ucp&#31561;&#20215;&#20844;&#24335;&#22823;n/ln n&#20493;&#65292;&#22240;&#27492;&#65292;&#19978;&#36848;&#27604;&#20540;&#30340;&#19968;&#33324;&#19978;&#30028;&#19981;&#33021;&#23567;&#20110;&#36825;&#20010;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two CNF formulas are called ucp-equivalent, if they behave in the same way with respect to the unit clause propagation (UCP). A formula is called ucp-irredundant, if removing any clause leads to a formula which is not ucp-equivalent to the original one. As a consequence of known results, the ratio of the size of a ucp-irredundant formula and the size of a smallest ucp-equivalent formula is at most $n^2$, where $n$ is the number of the variables. We demonstrate an example of a ucp-irredundant formula for a symmetric definite Horn function which is larger than a smallest ucp-equivalent formula by a factor $\Omega(n/\ln n)$ and, hence, a general upper bound on the above ratio cannot be smaller than this.
&lt;/p&gt;</description></item><item><title>LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16137</link><description>&lt;p&gt;
LM-Infinite: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21363;&#26102;&#38271;&#24230;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16137
&lt;/p&gt;
&lt;p&gt;
LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;Transformer-based&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;LLM&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#30528;&#23545;&#38271;&#26102;&#38388;&#25512;&#29702;&#36807;&#31243;&#25110;&#29702;&#35299;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#26041;&#26696;&#23558;&#35757;&#32451;&#24207;&#21015;&#25130;&#26029;&#21040;&#22266;&#23450;&#38271;&#24230;&#65288;&#20363;&#22914;LLaMa&#30340;2048&#65289;&#12290;&#21363;&#20351;&#20351;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;LLM&#22312;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20043;&#21518;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#26356;&#19981;&#29992;&#35828;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20102;&#12290;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22312;&#26356;&#38271;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#30828;&#20214;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#30340;&#35757;&#32451;&#36807;&#31243;&#35774;&#35745;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#20027;&#35201;&#30340;&#20998;&#24067;&#22806;(OOD) f
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#30340;&#21487;&#22609;&#24615;&#19988;&#26131;&#20110;&#23454;&#26045;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#24182;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;</title><link>http://arxiv.org/abs/2308.11958</link><description>&lt;p&gt;
&#36890;&#36807;&#20877;&#29983;&#24615;&#27491;&#21017;&#21270;&#32500;&#25345;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Maintaining Plasticity via Regenerative Regularization. (arXiv:2308.11958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#30340;&#21487;&#22609;&#24615;&#19988;&#26131;&#20110;&#23454;&#26045;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#24182;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#21487;&#22609;&#24615;&#25351;&#30340;&#26159;&#20195;&#29702;&#24555;&#36895;&#36866;&#24212;&#26032;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#24050;&#30693;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#20250;&#22833;&#21435;&#21487;&#22609;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#21487;&#22609;&#24615;&#12290;&#36825;&#19982;&#26631;&#20934;&#30340;L2&#27491;&#21017;&#21270;&#38750;&#24120;&#30456;&#20284;&#65292;&#21807;&#19968;&#30340;&#21306;&#21035;&#22312;&#20110;L2 Init&#27491;&#21017;&#21270;&#26397;&#21521;&#21407;&#28857;&#12290;L2 Init&#26131;&#20110;&#23454;&#26045;&#65292;&#21482;&#38656;&#35201;&#36873;&#25321;&#19968;&#20010;&#36229;&#21442;&#25968;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#21160;&#26426;&#19982;&#37325;&#32622;&#31070;&#32463;&#20803;&#25110;&#21442;&#25968;&#20540;&#30340;&#26041;&#27861;&#30456;&#21516;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#24403;&#26368;&#36817;&#30340;&#25439;&#22833;&#23545;&#29305;&#23450;&#21442;&#25968;&#19981;&#25935;&#24863;&#26102;&#65292;&#36825;&#20123;&#21442;&#25968;&#20250;&#21521;&#23427;&#20204;&#30340;&#21021;&#22987;&#20540;&#28418;&#31227;&#12290;&#36825;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#22312;&#20195;&#34920;&#36830;&#32493;&#23398;&#20064;&#20013;&#19981;&#21516;&#31867;&#22411;&#38750;&#24179;&#31283;&#24615;&#30340;&#31616;&#21333;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;L2 Init&#33021;&#22815;&#19968;&#33268;&#22320;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11905</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#25509;&#21463;&#36793;&#30028;&#36827;&#34892;&#21551;&#21457;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Utilizing Admissible Bounds for Heuristic Learning. (arXiv:2308.11905v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21033;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#21069;&#21521;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#24335;&#20989;&#25968;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#24212;&#35813;&#23398;&#20064;&#30340;&#20869;&#23481;&#12289;&#22914;&#20309;&#35757;&#32451;&#20197;&#21450;&#20026;&#20160;&#20040;&#36825;&#26679;&#20570;&#30340;&#29702;&#35770;&#35748;&#35782;&#36824;&#24456;&#23569;&#12290;&#36825;&#31181;&#29702;&#35299;&#30340;&#19981;&#36275;&#23548;&#33268;&#25991;&#29486;&#20013;&#36827;&#34892;&#25968;&#25454;&#38598;&#36873;&#25321;&#65288;&#27425;&#20248;&#25104;&#26412;&#23545;&#26368;&#20248;&#25104;&#26412;&#25110;&#21487;&#25509;&#21463;&#23545;&#19981;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#65289;&#21644;&#20248;&#21270;&#25351;&#26631;&#65288;&#20363;&#22914;&#24179;&#26041;&#35823;&#24046;&#21644;&#32477;&#23545;&#35823;&#24046;&#65289;&#26102;&#36827;&#34892;&#20102;&#20020;&#26102;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25152;&#24471;&#21040;&#30340;&#35757;&#32451;&#21551;&#21457;&#24335;&#20989;&#25968;&#32570;&#20047;&#21487;&#25509;&#21463;&#24615;&#65292;&#23545;&#20110;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25509;&#21463;&#24615;&#30340;&#37325;&#35201;&#24615;&#20063;&#32570;&#20047;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#30456;&#27604;&#26222;&#36890;&#39640;&#26031;&#20998;&#24067;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#25968;&#23398;&#27169;&#22411;&#24544;&#23454;&#22320;&#36981;&#24490;&#20102;&#26368;&#22823;&#29109;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and em
&lt;/p&gt;</description></item><item><title>ToolLLM&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25484;&#25569;16000&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;API&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#20351;&#29992;ChatGPT&#33258;&#21160;&#26500;&#24314;&#30340;ToolBench&#25968;&#25454;&#38598;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24037;&#20855;&#20351;&#29992;&#24773;&#22659;&#21644;API&#12290;</title><link>http://arxiv.org/abs/2307.16789</link><description>&lt;p&gt;
ToolLLM: &#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25484;&#25569;16000&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;API
&lt;/p&gt;
&lt;p&gt;
ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. (arXiv:2307.16789v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16789
&lt;/p&gt;
&lt;p&gt;
ToolLLM&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25484;&#25569;16000&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;API&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#20351;&#29992;ChatGPT&#33258;&#21160;&#26500;&#24314;&#30340;ToolBench&#25968;&#25454;&#38598;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24037;&#20855;&#20351;&#29992;&#24773;&#22659;&#21644;API&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;LLaMA&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#65292;&#21363;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#65288;API&#65289;&#26469;&#28385;&#36275;&#20154;&#31867;&#25351;&#20196;&#12290;&#21407;&#22240;&#26159;&#24403;&#21069;&#30340;&#25351;&#20196;&#35843;&#25972;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#19978;&#65292;&#20294;&#24573;&#30053;&#20102;&#24037;&#20855;&#20351;&#29992;&#39046;&#22495;&#12290;&#36825;&#19982;&#26368;&#20808;&#36827;&#30340;&#38381;&#28304;LLM&#65288;&#22914;ChatGPT&#65289;&#30340;&#20986;&#33394;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#24418;&#25104;&#23545;&#27604;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ToolLLM&#65292;&#19968;&#20010;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;ToolBench&#65292;&#19968;&#20010;&#29992;&#20110;&#24037;&#20855;&#20351;&#29992;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;ChatGPT&#33258;&#21160;&#26500;&#24314;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26500;&#24314;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;API&#25910;&#38598;&#65306;&#25105;&#20204;&#20174;RapidAPI Hub&#25910;&#38598;&#20102;16464&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;RESTful API&#65292;&#28085;&#30422;&#20102;49&#20010;&#31867;&#21035;&#65307;&#65288;ii&#65289;&#25351;&#20196;&#29983;&#25104;&#65306;&#25105;&#20204;&#25552;&#31034;ChatGPT&#29983;&#25104;&#28041;&#21450;&#36825;&#20123;API&#30340;&#22810;&#26679;&#21270;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;&#21333;&#24037;&#20855;&#21644;&#22810;&#24037;&#20855;&#20351;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-too
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20840;&#21367;&#31215;&#20960;&#20309;&#29305;&#24449;&#65288;FCGF&#65289;&#29992;&#20110;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36880;&#28857;&#21028;&#21035;&#29305;&#24449;&#20197;&#21450;&#36866;&#24403;&#30340;&#20462;&#25913;&#21644;&#35757;&#32451;&#31574;&#30053;&#36229;&#36234;&#20102;&#36817;&#26399;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15514</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20840;&#21367;&#31215;&#20960;&#20309;&#29305;&#24449;&#29992;&#20110;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation. (arXiv:2307.15514v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15514
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20840;&#21367;&#31215;&#20960;&#20309;&#29305;&#24449;&#65288;FCGF&#65289;&#29992;&#20110;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36880;&#28857;&#21028;&#21035;&#29305;&#24449;&#20197;&#21450;&#36866;&#24403;&#30340;&#20462;&#25913;&#21644;&#35757;&#32451;&#31574;&#30053;&#36229;&#36234;&#20102;&#36817;&#26399;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#22270;&#20687;&#21644;&#29289;&#20307;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#38190;&#28857;&#23545;&#24212;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;RANSAC&#30340;&#31639;&#27861;&#25110;&#30452;&#25509;&#36890;&#36807;&#31471;&#21040;&#31471;&#20248;&#21270;&#22238;&#24402;&#23039;&#24577;&#26469;&#30830;&#23450;&#29289;&#20307;&#23039;&#24577;&#12290;&#25105;&#20204;&#35748;&#20026;&#25991;&#29486;&#20013;&#24573;&#35270;&#20102;&#23398;&#20064;&#36880;&#28857;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20840;&#21367;&#31215;&#20960;&#20309;&#29305;&#24449;&#65288;FCGF&#65289;&#65292;&#24182;&#23558;&#20854;&#23450;&#21046;&#20026;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;FCGF&#37319;&#29992;&#31232;&#30095;&#21367;&#31215;&#65292;&#36890;&#36807;&#20248;&#21270;&#26368;&#38590;&#27604;&#36739;&#25439;&#22833;&#26469;&#23398;&#20064;&#36880;&#28857;&#29305;&#24449;&#12290;&#36890;&#36807;&#23545;&#25439;&#22833;&#21644;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#36827;&#34892;&#20851;&#38190;&#20462;&#25913;&#12289;&#31934;&#24515;&#35843;&#25972;&#35757;&#32451;&#31574;&#30053;&#20197;&#21450;&#37319;&#29992;&#36866;&#21512;&#24213;&#23618;&#38382;&#39064;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#36817;&#26399;&#30340;&#31454;&#20105;&#23545;&#25163;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#27599;&#20010;&#20462;&#25913;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works on 6D object pose estimation focus on learning keypoint correspondences between images and object models, and then determine the object pose through RANSAC-based algorithms or by directly regressing the pose with end-to-end optimisations. We argue that learning point-level discriminative features is overlooked in the literature. To this end, we revisit Fully Convolutional Geometric Features (FCGF) and tailor it for object 6D pose estimation to achieve state-of-the-art performance. FCGF employs sparse convolutions and learns point-level features using a fully-convolutional network by optimising a hardest contrastive loss. We can outperform recent competitors on popular benchmarks by adopting key modifications to the loss and to the input data representations, by carefully tuning the training strategies, and by employing data augmentations suitable for the underlying problem. We carry out a thorough ablation to study the contribution of each modification.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12856</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#35268;&#21010;&#12289;&#38271;&#26399;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#30340;&#29616;&#23454;&#19990;&#30028;WebAgent
&lt;/p&gt;
&lt;p&gt;
A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#20027;Web&#33258;&#21160;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#31449;&#19978;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#19977;&#20010;&#26041;&#38754;&#30340;&#38480;&#21046;&#65306;&#24320;&#25918;&#39046;&#22495;&#24615;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;HTML&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;WebAgent&#36890;&#36807;&#23558;&#25351;&#20196;&#20998;&#35299;&#20026;&#35268;&#33539;&#30340;&#23376;&#25351;&#20196;&#65292;&#23558;&#38271;HTML&#25991;&#26723;&#24635;&#32467;&#20026;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#20174;&#20013;&#29983;&#25104;&#30340;Python&#31243;&#24207;&#23545;&#32593;&#31449;&#36827;&#34892;&#25805;&#20316;&#26469;&#25552;&#21069;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-U-PaLM&#35774;&#35745;&#20102;WebAgent&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#26681;&#20195;&#30721;&#65292;&#24182;&#20351;&#29992;HTML-T5&#36827;&#34892;&#39044;&#35757;&#32451;LLMs&#65292;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#20197;&#21450;&#28151;&#21512;&#38271;&#36328;&#24230;&#21435;&#22122;&#30446;&#26631;&#26469;&#36827;&#34892;&#35268;&#21010;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.12375</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#19978;&#20855;&#26377;&#21019;&#26032;&#65292;&#20294;&#24182;&#38750;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12375
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#21253;&#21547;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#36890;&#24120;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26080;&#20849;&#35782;&#65306;&#20363;&#22914;&#65292;&#34429;&#28982;Xie&#31561;&#20154;&#65288;2021&#24180;&#65289;&#23558;ICL&#27604;&#20316;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;Min&#31561;&#20154;&#65288;2022b&#24180;&#65289;&#35748;&#20026;ICL&#29978;&#33267;&#19981;&#33021;&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#65288;2&#65289;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#36755;&#20837;-&#26631;&#31614;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#65288;3&#65289;ICL&#22914;&#20309;&#32858;&#21512;&#26469;&#33258;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#36890;&#24120;&#20250;&#25972;&#21512;&#19978;&#19979;&#25991;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20294;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#31995;&#34987;&#21306;&#21035;&#23545;&#24453;&#65292;&#27169;&#22411;&#19981;&#20250;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#21516;&#23545;&#24453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;LLMs&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02738</link><description>&lt;p&gt;
RecallM:&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#38382;&#39064;&#22238;&#31572;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#23558;&#20026;&#36830;&#32493;&#23398;&#20064;&#12289;&#22797;&#26434;&#25512;&#29702;&#21644;&#23398;&#20064;&#24207;&#21015;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#25171;&#19979;&#22522;&#30784;&#12290;&#21019;&#24314;&#36825;&#31181;&#31867;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#19987;&#27880;&#20110;&#20026;AGI&#31995;&#32479;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#23637;&#31034;&#20102;RecallM&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#23427;&#25552;&#20379;&#30340;&#25913;&#36827;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ideal long-term memory mechanism for Large Language Model (LLM) based chatbots, would lay the foundation for continual learning, complex reasoning and allow sequential and temporal dependencies to be learnt. Creating this type of memory mechanism is an extremely challenging problem. In this paper we explore different methods of achieving the effect of long-term memory. We propose a new architecture focused on creating adaptable and updatable long-term memory for AGI systems. We demonstrate through various experiments the benefits of the RecallM architecture, particularly the improved temporal understanding it provides.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13649</link><description>&lt;p&gt;
GKD&#65306;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#33976;&#39311;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#35757;&#32451;&#26399;&#38388;&#36755;&#20986;&#24207;&#21015;&#21644;&#37096;&#32626;&#26102;&#30001;&#23398;&#29983;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20043;&#38388;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#65288;2&#65289;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#33021;&#19981;&#22815;&#34920;&#36798;&#32769;&#24072;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#12290;GKD&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;GKD&#36890;&#36807;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26469;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36825;&#20123;&#31163;&#25955;&#24230;&#38598;&#20013;&#20110;&#29983;&#25104;&#21487;&#33021;&#31526;&#21512;&#32769;&#24072;&#20998;&#24067;&#30340;&#23398;&#29983;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;GKD&#20248;&#20110;&#24120;&#29992;&#30340;LLM&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
&lt;/p&gt;</description></item><item><title>Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06192</link><description>&lt;p&gt;
Ada-NAV&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06192
&lt;/p&gt;
&lt;p&gt;
Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#26041;&#38754;&#21313;&#20998;&#26377;&#25928;&#65292;&#20294;&#20854;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#65292;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#37096;&#20998;&#26469;&#33258;&#20110;&#26410;&#33021;&#36866;&#24403;&#22320;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#38750;&#38745;&#24577;&#26102;&#12290;&#20026;&#20102;&#21152;&#20837;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-NAV&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#65292;&#20854;&#20013;&#38271;&#24230;&#38543;&#30528;&#31574;&#30053;&#30340;&#38543;&#26426;&#24615;&#65288;&#29992;&#20854;Shannon&#25110;&#24046;&#20998;&#29109;&#34920;&#31034;&#65289;&#30340;&#20943;&#23567;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#30001;&#20110;&#26356;&#39057;&#32321;&#30340;&#26799;&#24230;&#26356;&#26032;&#24378;&#35843;&#20102;&#35757;&#32451;&#24320;&#22987;&#26102;&#30340;&#25506;&#32034;&#65292;&#21518;&#26469;&#21017;&#26356;&#24378;&#35843;&#21033;&#29992;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#65292;&#20223;&#30495;&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#34920;&#29616;&#22312;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#24120;&#25968;&#21644;&#38543;&#26426;&#37319;&#26679;&#30340;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#39044;&#31639;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;Ada-NAV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;46&#65285;&#65292;&#37319;&#26679;&#25968;&#37327;&#20943;&#23569;&#20102;&#39640;&#36798;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance.</title><link>http://arxiv.org/abs/2305.18030</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#25628;&#32034;&#31354;&#38388;&#29983;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automated Search-Space Generation Neural Architecture Search. (arXiv:2305.18030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18030
&lt;/p&gt;
&lt;p&gt;
Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20107;&#20808;&#25163;&#24037;&#21019;&#24314;&#25628;&#32034;&#31354;&#38388;&#26469;&#25628;&#32034;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#30340;&#26368;&#20248;&#23376;&#32593;&#32476;&#12290;&#36825;&#26679;&#30340;&#35201;&#27714;&#20351;&#24471;&#22312;&#27809;&#26377;&#26174;&#33879;&#30340;&#20154;&#24037;&#19987;&#19994;&#30693;&#35782;&#21644;&#25163;&#21160;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23558;&#23427;&#20204;&#25193;&#23637;&#21040;&#36890;&#29992;&#22330;&#26223;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Automated Search-Space Generation Neural Architecture Search&#65288;ASGNAS&#65289;&#65292;&#21487;&#33021;&#26159;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20197;&#19968;&#27425;&#24615;&#30340;&#26041;&#24335;&#35757;&#32451;&#35206;&#30422;&#25152;&#26377;&#20505;&#36873;&#36830;&#25509;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;DNN&#65292;&#24182;&#20135;&#29983;&#39640;&#24615;&#33021;&#30340;&#23376;&#32593;&#32476;&#12290;&#25216;&#26415;&#19978;&#65292;ASGNAS&#20855;&#26377;&#19977;&#20010;&#26174;&#33879;&#30340;&#36129;&#29486;&#20197;&#20943;&#23569;&#20154;&#21147;&#24037;&#20316;&#65306;&#65288;i&#65289;&#36890;&#29992;DNN&#30340;&#33258;&#21160;&#25628;&#32034;&#31354;&#38388;&#29983;&#25104;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#20869;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;Hierarchical Half-Space Projected Gradient&#65288;H2SPG&#65289;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30830;&#20445;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#38752;&#22320;&#20135;&#29983;&#20855;&#26377;&#39640;&#24615;&#33021;&#21644;&#31232;&#30095;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
To search an optimal sub-network within a general deep neural network (DNN), existing neural architecture search (NAS) methods typically rely on handcrafting a search space beforehand. Such requirements make it challenging to extend them onto general scenarios without significant human expertise and manual intervention. To overcome the limitations, we propose Automated Search-Space Generation Neural Architecture Search (ASGNAS), perhaps the first automated system to train general DNNs that cover all candidate connections and operations and produce high-performing sub-networks in the one shot manner. Technologically, ASGNAS delivers three noticeable contributions to minimize human efforts: (i) automated search space generation for general DNNs; (ii) a Hierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy and dependency within generated search space to ensure the network validity during optimization, and reliably produces a solution with both high performance an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36716;&#31227;&#23398;&#20064;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#21487;&#23558;&#20043;&#21069;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36712;&#36857;&#24212;&#29992;&#22312;&#26032;&#30340;&#35757;&#32451;&#20013;&#65292;&#24182;&#33021;&#22312;&#20219;&#20309;&#30452;&#25509;&#35757;&#32451;&#20043;&#21069;&#23454;&#29616;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14122</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36712;&#36857;&#30340;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Transferring Learning Trajectories of Neural Networks. (arXiv:2305.14122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36716;&#31227;&#23398;&#20064;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#21487;&#23558;&#20043;&#21069;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36712;&#36857;&#24212;&#29992;&#22312;&#26032;&#30340;&#35757;&#32451;&#20013;&#65292;&#24182;&#33021;&#22312;&#20219;&#20309;&#30452;&#25509;&#35757;&#32451;&#20043;&#21069;&#23454;&#29616;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#36825;&#22312;&#25191;&#34892;&#37325;&#22797;&#35757;&#32451;&#36816;&#34892;&#65288;&#20363;&#22914;&#27169;&#22411;&#38598;&#25104;&#25110;&#30693;&#35782;&#33976;&#39311;&#65289;&#26102;&#23588;&#20854;&#25104;&#38382;&#39064;&#12290;&#19968;&#26086;&#25105;&#20204;&#22312;&#26576;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;DNN&#65292;&#25105;&#20204;&#23601;&#25317;&#26377;&#20102;&#20854;&#23398;&#20064;&#36712;&#36857;&#65288;&#21363;&#35757;&#32451;&#26399;&#38388;&#30340;&#20013;&#38388;&#21442;&#25968;&#24207;&#21015;&#65289;&#65292;&#20854;&#20013;&#21487;&#33021;&#21253;&#21547;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#23581;&#35797;&#21033;&#29992;&#32473;&#23450;&#23398;&#20064;&#36712;&#36857;&#30340;&#36825;&#31181;&#20449;&#24687;&#36827;&#34892;&#21478;&#19968;&#31181;&#35757;&#32451;&#12290;&#26412;&#25991;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#8220;&#36716;&#31227;&#8221;&#32473;&#23450;&#23398;&#20064;&#36712;&#36857;&#20174;&#19968;&#20010;&#21021;&#22987;&#21442;&#25968;&#21040;&#21478;&#19968;&#20010;&#21021;&#22987;&#21442;&#25968;&#65292;&#31216;&#20026;&#23398;&#20064;&#36716;&#31227;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#27839;&#36712;&#36857;&#36880;&#28176;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#26799;&#24230;&#23548;&#20986;&#20102;&#31532;&#19968;&#20010;&#31639;&#27861;&#65292;&#20197;&#36817;&#20284;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#36716;&#31227;&#21442;&#25968;&#22312;&#20219;&#20309;&#30452;&#25509;&#35757;&#32451;&#20043;&#21069;&#23601;&#33021;&#36798;&#21040;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36716;&#31227;&#21442;&#25968;&#30340;&#25439;&#22833;&#26223;&#35266;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) is computationally expensive, which is problematic especially when performing duplicated training runs, such as model ensemble or knowledge distillation. Once we have trained one DNN on some dataset, we have its learning trajectory (i.e., a sequence of intermediate parameters during training) which may potentially contain useful information for learning the dataset. However, there has been no attempt to utilize such information of a given learning trajectory for another training. In this paper, we formulate the problem of "transferring" a given learning trajectory from one initial parameter to another one, called learning transfer problem, and derive the first algorithm to approximately solve it by matching gradients successively along the trajectory via permutation symmetry. We empirically show that the transferred parameters achieve non-trivial accuracy before any direct training. Also, we analyze the loss landscape property of the transferred par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13300</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#20013;&#30340;&#34892;&#20026;&#25581;&#31192;&#65306;&#33258;&#36866;&#24212;&#21464;&#33394;&#40857;&#36824;&#26159;&#22266;&#25191;&#30340;&#26641;&#29549;
&lt;/p&gt;
&lt;p&gt;
Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#22806;&#37096;&#20449;&#24687;&#65292;&#24037;&#20855;&#22686;&#24378;&#65288;&#21253;&#25324;&#26816;&#32034;&#22686;&#24378;&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;LLMs&#38745;&#24577;&#21442;&#25968;&#21270;&#20869;&#23384;&#38480;&#21046;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#35777;&#25454;&#19982;&#23427;&#20204;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;LLMs&#23545;&#36825;&#20123;&#22806;&#37096;&#35777;&#25454;&#26377;&#22810;&#23569;&#25509;&#21463;&#33021;&#21147;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#20174;LLMs&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#65292;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#23545;&#31435;&#20869;&#23384;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#31995;&#21015;&#21463;&#25511;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;LLMs&#34920;&#29616;&#20986;&#30475;&#20284;&#30683;&#30462;&#30340;&#34892;&#20026;&#12290;&#19968;&#26041;&#38754;&#65292;&#19982;&#20197;&#24448;&#30340;&#35266;&#24565;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#35201;&#22806;&#37096;&#35777;&#25454;&#26159;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#65292;LLMs&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#20063;&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#35777;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLMs&#20063;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#21463;&#21040;&#23041;&#32961;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#20803;&#32032;&#25968;&#37327;&#30340;RFSs&#30340;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;PMB&#28388;&#27874;&#22120;&#29992;&#20110;SLAM&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#38598;&#21512;&#22411;BP-mapping&#12289;SLAM&#12289;&#22810;&#30446;&#26631;&#36319;&#36394;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#36319;&#36394;&#28388;&#27874;&#22120;&#31561;&#26032;&#39062;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.04797</link><description>&lt;p&gt;
&#38598;&#21512;&#22411;&#20449;&#24565;&#20256;&#25773;&#21450;&#20854;&#22312;&#27850;&#26494;&#22810;&#20271;&#21162;&#21033;SLAM&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Set-Type Belief Propagation with Applications to Poisson Multi-Bernoulli SLAM. (arXiv:2305.04797v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#20803;&#32032;&#25968;&#37327;&#30340;RFSs&#30340;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;PMB&#28388;&#27874;&#22120;&#29992;&#20110;SLAM&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#38598;&#21512;&#22411;BP-mapping&#12289;SLAM&#12289;&#22810;&#30446;&#26631;&#36319;&#36394;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#36319;&#36394;&#28388;&#27874;&#22120;&#31561;&#26032;&#39062;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24565;&#20256;&#25773;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35745;&#31639;&#38543;&#26426;&#21464;&#37327;&#30340;&#36817;&#20284;&#36793;&#32536;&#27010;&#29575;&#23494;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#26631;&#20934;&#24418;&#24335;&#19979;&#65292;&#20449;&#24565;&#20256;&#25773;&#21482;&#36866;&#29992;&#20110;&#20855;&#26377;&#22266;&#23450;&#21644;&#24050;&#30693;&#30340;&#21521;&#37327;&#20803;&#32032;&#25968;&#37327;&#30340;&#21521;&#37327;&#22411;&#38543;&#26426;&#21464;&#37327;&#65292;&#32780;&#26576;&#20123;&#24212;&#29992;&#20381;&#36182;&#20110;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#20803;&#32032;&#25968;&#37327;&#30340;RFSs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;RFSs&#24207;&#21015;&#23450;&#20041;&#30340;&#22240;&#23376;&#22270;&#30340;&#20449;&#24565;&#20256;&#25773;&#35268;&#21017;&#65292;&#20854;&#20013;&#27599;&#20010;RFS&#20855;&#26377;&#26410;&#30693;&#30340;&#20803;&#32032;&#25968;&#37327;&#65292;&#26088;&#22312;&#23548;&#20986;RFSs&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21521;&#37327;&#22411;&#20449;&#24565;&#20256;&#25773;&#26159;&#38598;&#21512;&#22411;&#20449;&#24565;&#20256;&#25773;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#27599;&#20010;RFS&#36981;&#24490;&#20271;&#21162;&#21033;&#36807;&#31243;&#12290;&#20026;&#20102;&#35777;&#26126;&#24320;&#21457;&#30340;&#38598;&#21512;&#22411;&#20449;&#24565;&#20256;&#25773;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;SLAM&#30340;PMB&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#33258;&#28982;&#22320;&#23548;&#33268;&#26032;&#30340;&#38598;&#21512;&#22411;BP&#26144;&#23556;&#12289;SLAM&#12289;&#22810;&#30446;&#26631;&#36319;&#36394;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#36319;&#36394;&#28388;&#27874;&#22120;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21521;&#37327;&#22411;&#20449;&#24565;&#20256;&#25773;&#21644;&#20027;&#39064;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Belief propagation (BP) is a useful probabilistic inference algorithm for efficiently computing approximate marginal probability densities of random variables. However, in its standard form, BP is only applicable to the vector-type random variables with a fixed and known number of vector elements, while certain applications rely on RFSs with an unknown number of vector elements. In this paper, we develop BP rules for factor graphs defined on sequences of RFSs where each RFS has an unknown number of elements, with the intention of deriving novel inference methods for RFSs. Furthermore, we show that vector-type BP is a special case of set-type BP, where each RFS follows the Bernoulli process. To demonstrate the validity of developed set-type BP, we apply it to the PMB filter for SLAM, which naturally leads to new set-type BP-mapping, SLAM, multi-target tracking, and simultaneous localization and tracking filters. Finally, we explore the relationships between the vector-type BP and the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.08612</link><description>&lt;p&gt;
&#31163;&#25955;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#26725;&#26753;&#65306;&#30452;&#36890;&#27861;&#19982;&#20854;&#23427;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#20165;&#38480;&#20110;&#35745;&#31639;&#36830;&#32493;&#21464;&#37327;&#30340;&#26799;&#24230;&#65292;&#38480;&#21046;&#20102;&#28041;&#21450;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340; Straight-Through&#65288;ST&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26799;&#24230;&#30340;&#19968;&#38454;&#36817;&#20284;&#20540;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; ReinMax&#65292;&#23427;&#38598;&#25104;&#20102; Heun's Method&#65292;&#19968;&#31181;&#35299;ODE&#30340;&#20108;&#38454;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201; Hessian &#25110;&#20854;&#20182;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;\ours &#22312;&#29616;&#26377;&#25216;&#26415;&#20013;&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324; ST &#21644; Straight-Through Gum&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.04736</link><description>&lt;p&gt;
&#20851;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#30528;&#30524;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#20197;&#21306;&#20998;&#20854;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#36825;&#39033;&#33021;&#21147;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#31181;&#21306;&#20998;&#30340;&#21487;&#33021;&#24615;&#19968;&#30452;&#26159;&#35813;&#39046;&#22495;&#20869;&#30340;&#20105;&#35758;&#35805;&#39064;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#22914;&#26524;&#33021;&#65292;&#20309;&#26102;&#33021;&#26816;&#27979;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#38500;&#38750;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#24067;&#22312;&#25972;&#20010;&#25903;&#25345;&#20013;&#23436;&#20840;&#30456;&#21516;&#65292;&#21542;&#21017;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#26469;&#33258;&#20110;&#20449;&#24687;&#35770;&#20013;&#30340;&#26631;&#20934;&#32467;&#26524;&#65292;&#24182;&#20381;&#36182;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#20687;&#20154;&#31867;&#65292;&#25105;&#20204;&#23601;&#38656;&#35201;&#26356;&#22810;&#30340;&#26679;&#26412;&#26469;&#26816;&#27979;&#23427;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#31934;&#30830;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#21578;&#35785;&#38656;&#35201;&#22810;&#23569;&#20010;&#26679;&#26412;&#25165;&#33021;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#24341;&#36215;&#20102;&#26356;&#22810;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;LLM&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20998;&#26512;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#20013;&#65292;d-&#20998;&#31163;&#26159;&#32597;&#35265;&#30340;&#29616;&#35937;&#65292;&#38500;&#38750;&#22270;&#26159;&#26497;&#20854;&#31232;&#30095;&#30340;&#12290;&#23545;&#20110;&#22240;&#26524;&#21457;&#29616;&#30340;PC&#31639;&#27861;&#21644;UniformSGS&#31639;&#27861;&#36827;&#34892;&#20102;&#24179;&#22343;&#24773;&#20917;&#20998;&#26512;&#65292;&#24182;&#32473;&#20986;&#20102;&#19978;&#30028;&#65292;&#19978;&#30028;&#20197;&#25351;&#25968;&#36895;&#24230;&#34928;&#20943;&#21040;0&#12290;</title><link>http://arxiv.org/abs/2303.05628</link><description>&lt;p&gt;
&#20851;&#20110;D-&#20998;&#31163;&#30340;&#19981;&#22826;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Unlikelihood of D-Separation. (arXiv:2303.05628v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20998;&#26512;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#20013;&#65292;d-&#20998;&#31163;&#26159;&#32597;&#35265;&#30340;&#29616;&#35937;&#65292;&#38500;&#38750;&#22270;&#26159;&#26497;&#20854;&#31232;&#30095;&#30340;&#12290;&#23545;&#20110;&#22240;&#26524;&#21457;&#29616;&#30340;PC&#31639;&#27861;&#21644;UniformSGS&#31639;&#27861;&#36827;&#34892;&#20102;&#24179;&#22343;&#24773;&#20917;&#20998;&#26512;&#65292;&#24182;&#32473;&#20986;&#20102;&#19978;&#30028;&#65292;&#19978;&#30028;&#20197;&#25351;&#25968;&#36895;&#24230;&#34928;&#20943;&#21040;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26088;&#22312;&#20174;&#30001;&#20854;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#22270;&#65307;&#22522;&#20110;&#32422;&#26463;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;oracle&#22312;&#22270;&#20013;&#25628;&#32034;&#19968;&#20010;d-&#20998;&#31163;&#30340;&#33410;&#28857;&#38598;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20998;&#26512;&#35777;&#25454;&#65292;&#21363;&#22312;&#22823;&#22411;&#22270;&#20013;&#65292;&#21363;&#20351;&#20445;&#35777;&#23384;&#22312;&#65292;d-&#20998;&#31163;&#20063;&#26159;&#19968;&#20010;&#32597;&#35265;&#30340;&#29616;&#35937;&#65292;&#38500;&#38750;&#22270;&#26159;&#26497;&#20854;&#31232;&#30095;&#30340;&#12290;&#25105;&#20204;&#36824;&#23545;&#22240;&#26524;&#21457;&#29616;&#30340;PC&#31639;&#27861;&#36827;&#34892;&#20102;&#24179;&#22343;&#24773;&#20917;&#20998;&#26512;&#65292;&#20197;&#21450;&#25105;&#20204;&#31216;&#20043;&#20026;UniformSGS&#30340;SGS&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#33410;&#28857;&#38598;V={v1&#65292;...&#65292;vn}&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#38543;&#26426;DAG G=(V,E)&#65292;&#20854;&#20013;&#22914;&#26524;a&lt;b&#65292;&#21017;(va&#65292;vb)&#8712;E&#30340;&#29420;&#31435;&#27010;&#29575;&#20026;p1&#65292;&#22914;&#26524;a&gt;b&#65292;&#21017;&#20026;0&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#24403;x&#21644;y&#21487;d-&#20998;&#31163;&#26102;&#65292;&#38598;&#21512;V- {x&#65292;y} d-&#20998;&#31163;x&#21644;y&#30340;&#27010;&#29575;&#30340;&#19978;&#30028;&#65307;&#25105;&#20204;&#30340;&#19978;&#30028;&#22312;|V|&#8594;&#8734;&#26102;&#20197;&#25351;&#25968;&#36895;&#24230;&#34928;&#20943;&#21040;0&#12290;&#23545;&#20110;PC&#31639;&#27861;&#65292;&#34429;&#28982;&#24050;&#30693;&#20854;&#26368;&#22351;&#24773;&#20917;&#30340;&#20445;&#35777;&#22312;&#38750;&#31232;&#30095;&#22270;&#19978;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery aims to recover a causal graph from data generated by it; constraint based methods do so by searching for a d-separating conditioning set of nodes in the graph via an oracle. In this paper, we provide analytic evidence that on large graphs, d-separation is a rare phenomenon, even when guaranteed to exist, unless the graph is extremely sparse. We then provide an analytic average case analysis of the PC Algorithm for causal discovery, as well as a variant of the SGS Algorithm we call UniformSGS. We consider a set $V=\{v_1,\ldots,v_n\}$ of nodes, and generate a random DAG $G=(V,E)$ where $(v_a, v_b) \in E$ with i.i.d. probability $p_1$ if $a&lt;b$ and $0$ if $a &gt; b$. We provide upper bounds on the probability that a subset of $V-\{x,y\}$ d-separates $x$ and $y$, conditional on $x$ and $y$ being d-separable; our upper bounds decay exponentially fast to $0$ as $|V| \rightarrow \infty$. For the PC Algorithm, while it is known that its worst-case guarantees fail on non-sparse gr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20171;&#32461;&#20102;&#28145;&#24230;&#29983;&#25104;3D&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#21442;&#32771;&#21644;&#28608;&#21457;&#26410;&#26469;&#30740;&#31350;&#30340;&#35752;&#35770;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2210.14267</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;3D&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Generative 3D-aware Image Synthesis. (arXiv:2210.14267v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20171;&#32461;&#20102;&#28145;&#24230;&#29983;&#25104;3D&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#21442;&#32771;&#21644;&#28608;&#21457;&#26410;&#26469;&#30740;&#31350;&#30340;&#35752;&#35770;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35270;&#35273;&#20869;&#23481;&#21019;&#20316;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20854;&#20013;&#21253;&#25324;&#28145;&#24230;&#29983;&#25104;&#30340;3D&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#65292;&#23427;&#20197;3D&#19968;&#33268;&#30340;&#26041;&#24335;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#20174;&#32431;&#22270;&#20687;&#38598;&#21512;&#20013;&#25429;&#25417;&#23545;&#35937;&#30340;&#32039;&#20945;&#34920;&#38754;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;3D&#30417;&#30563;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;2D&#22270;&#20687;&#21644;3D&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#26368;&#36817;&#34987;&#28145;&#24230;&#29983;&#25104;3D&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#25152;&#21560;&#24341;&#65292;&#36807;&#21435;&#20960;&#24180;&#30340;&#39030;&#32423;&#26399;&#21002;&#21644;&#20250;&#35758;&#20013;&#28044;&#29616;&#20986;&#25968;&#30334;&#31687;&#35770;&#25991;&#65288;&#20027;&#35201;&#26159;&#36807;&#21435;&#20004;&#24180;&#65289;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20221;&#20840;&#38754;&#30340;&#35843;&#26597;&#25253;&#21578;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#26088;&#22312;&#21521;&#26032;&#30740;&#31350;&#32773;&#20171;&#32461;&#36825;&#19968;&#20027;&#39064;&#65292;&#20026;&#30456;&#20851;&#24037;&#20316;&#25552;&#20379;&#26377;&#29992;&#30340;&#21442;&#32771;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#30340;&#35752;&#35770;&#37096;&#20998;&#28608;&#21457;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#38500;&#20102;&#23637;&#31034;&#30340;&#35770;&#25991;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#19981;&#26029;&#26356;&#26032;&#26368;&#26032;&#30456;&#20851;&#30340;&#35770;&#25991;&#21450;&#20854;&#30456;&#20851;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen remarkable progress in deep learning powered visual content creation. This includes deep generative 3D-aware image synthesis, which produces high-idelity images in a 3D-consistent manner while simultaneously capturing compact surfaces of objects from pure image collections without the need for any 3D supervision, thus bridging the gap between 2D imagery and 3D reality. The ield of computer vision has been recently captivated by the task of deep generative 3D-aware image synthesis, with hundreds of papers appearing in top-tier journals and conferences over the past few years (mainly the past two years), but there lacks a comprehensive survey of this remarkable and swift progress. Our survey aims to introduce new researchers to this topic, provide a useful reference for related works, and stimulate future research directions through our discussion section. Apart from the presented papers, we aim to constantly update the latest relevant papers along with correspondi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#26377;&#38480;&#29366;&#24577;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#12290;&#21033;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#26032;&#30340;&#25552;&#28860;&#26041;&#27861;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#35299;&#20915;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2208.11838</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Learning Task Automata for Reinforcement Learning using Hidden Markov Models. (arXiv:2208.11838v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#26377;&#38480;&#29366;&#24577;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#12290;&#21033;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#26032;&#30340;&#25552;&#28860;&#26041;&#27861;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#35299;&#20915;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29615;&#22659;&#20855;&#26377;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#37327;&#22870;&#21169;&#20449;&#21495;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#25163;&#24037;&#21019;&#24314;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#24448;&#24448;&#23481;&#26131;&#20986;&#38169;&#65292;&#29305;&#21035;&#26159;&#24403;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#21482;&#26377;&#37096;&#20998;&#24050;&#30693;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#23545;&#26410;&#30693;&#29615;&#22659;&#20013;&#20195;&#29702;&#32463;&#39564;&#30340; episodes &#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#31616;&#27905;&#26377;&#38480;&#29366;&#24577;&#8220;&#20219;&#21153;&#33258;&#21160;&#26426;&#8221;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#31639;&#27861;&#30340;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20135;&#21697; MDP &#35270;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979; MDP &#24182;&#20351;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340; Baum-Welch &#31639;&#27861;&#26469;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#20102;&#35268;&#33539;&#33258;&#21160;&#26426;&#21644;&#29615;&#22659;&#30340; MDP&#65288;&#21021;&#22987;&#37117;&#26410;&#30693;&#65289;&#25152;&#32452;&#25104;&#30340;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20174;&#23398;&#21040;&#30340;&#20135;&#21697; MDP &#20013;&#25552;&#28860;&#20219;&#21153;&#33258;&#21160;&#26426;&#65288;&#20551;&#35774;&#20026;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65289;&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#20351;&#24471;&#20195;&#29702;&#21487;&#20197;&#35299;&#20915;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training reinforcement learning (RL) agents using scalar reward signals is often infeasible when an environment has sparse and non-Markovian rewards. Moreover, handcrafting these reward functions before training is prone to misspecification, especially when the environment's dynamics are only partially known. This paper proposes a novel pipeline for learning non-Markovian task specifications as succinct finite-state `task automata' from episodes of agent experience within unknown environments. We leverage two key algorithmic insights. First, we learn a product MDP, a model composed of the specification's automaton and the environment's MDP (both initially unknown), by treating the product MDP as a partially observable MDP and using the well-known Baum-Welch algorithm for learning hidden Markov models. Second, we propose a novel method for distilling the task automaton (assumed to be a deterministic finite automaton) from the learnt product MDP. Our learnt task automaton enables the dec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#25512;&#33616;&#30340;&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;DcRec&#65292;&#21487;&#20197;&#23398;&#20064;&#35299;&#32544;&#29992;&#25143;&#22312;&#29289;&#21697;&#21644;&#31038;&#20132;&#39046;&#22495;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#36716;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31038;&#20132;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.08723</link><description>&lt;p&gt;
&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Contrastive Learning for Social Recommendation. (arXiv:2208.08723v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#25512;&#33616;&#30340;&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;DcRec&#65292;&#21487;&#20197;&#23398;&#20064;&#35299;&#32544;&#29992;&#25143;&#22312;&#29289;&#21697;&#21644;&#31038;&#20132;&#39046;&#22495;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#36716;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31038;&#20132;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#25512;&#33616;&#21033;&#29992;&#31038;&#20132;&#20851;&#31995;&#26469;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#22823;&#22810;&#25968;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#32479;&#19968;&#20102;&#29992;&#25143;&#22312;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#39046;&#22495;&#65288;&#21327;&#21516;&#39046;&#22495;&#65289;&#21644;&#31038;&#20132;&#20851;&#31995;&#39046;&#22495;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#24314;&#27169;&#29992;&#25143;&#22312;&#20004;&#20010;&#39046;&#22495;&#20013;&#30340;&#24322;&#36136;&#34892;&#20026;&#27169;&#24335;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#29992;&#25143;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#31038;&#20132;&#25512;&#33616;&#30340;&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;DcRec&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20174;&#29289;&#21697;&#21644;&#31038;&#20132;&#39046;&#22495;&#20013;&#23398;&#20064;&#35299;&#32544;&#29992;&#25143;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35299;&#32544;&#29992;&#25143;&#34920;&#31034;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#29992;&#20110;&#31038;&#20132;&#25512;&#33616;&#12290;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#32508;&#21512;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social recommendations utilize social relations to enhance the representation learning for recommendations. Most social recommendation models unify user representations for the user-item interactions (collaborative domain) and social relations (social domain). However, such an approach may fail to model the users heterogeneous behavior patterns in two domains, impairing the expressiveness of user representations. In this work, to address such limitation, we propose a novel Disentangled contrastive learning framework for social Recommendations DcRec. More specifically, we propose to learn disentangled users representations from the item and social domains. Moreover, disentangled contrastive learning is designed to perform knowledge transfer between disentangled users representations for social recommendations. Comprehensive experiments on various real-world datasets demonstrate the superiority of our proposed model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2208.07497</link><description>&lt;p&gt;
&#23398;&#20064;ACOPF&#30340;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Bucketized Active Sampling for Learning ACOPF. (arXiv:2208.07497v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#26368;&#20248;&#28526;&#27969;&#65288;OPF&#65289;&#30340;&#20248;&#21270;&#20195;&#29702;&#65292;&#21363;&#36817;&#20284;OPF&#30340;&#36755;&#20837;/&#36755;&#20986;&#20851;&#31995;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#35777;&#26126;&#36825;&#20123;&#20195;&#29702;&#21487;&#20197;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#38656;&#35201;&#23545;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#36827;&#34892;OPF&#30340;&#65288;&#31163;&#32447;&#65289;&#27714;&#35299;&#12290;&#20026;&#20102;&#28385;&#36275;&#24066;&#22330;&#28165;&#31639;&#24212;&#29992;&#30340;&#35201;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#23558;&#30456;&#21516;&#30340;&#20998;&#26742;&#24212;&#29992;&#20110;&#39564;&#35777;&#38598;&#65292;BAS&#21033;&#29992;&#26631;&#35760;&#30340;&#39564;&#35777;&#26679;&#26412;&#26469;&#36873;&#25321;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;BAS&#36824;&#20381;&#36182;&#20110;&#38543;&#26102;&#38388;&#22686;&#21152;&#21644;&#20943;&#23569;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers optimization proxies for Optimal Power Flow (OPF), i.e., machine-learning models that approximate the input/output relationship of OPF. Recent work has focused on showing that such proxies can be of high fidelity. However, their training requires significant data, each instance necessitating the (offline) solving of an OPF for a sample of the input distribution. To meet the requirements of market-clearing applications, this paper proposes Bucketized Active Sampling (BAS), a novel active learning framework that aims at training the best possible OPF proxy within a time limit. BAS partitions the input distribution into buckets and uses an acquisition function to determine where to sample next. By applying the same partitioning to the validation set, BAS leverages labeled validation samples in the selection of unlabeled samples. BAS also relies on an adaptive learning rate that increases and decreases over time. Experimental results demonstrate the benefits of BAS.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#19981;&#21464;&#24615;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#21407;&#29702;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#31283;&#23450;&#19988;&#21333;&#21521;&#30340;&#35299;&#37322;&#12290;&#26041;&#27861;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#24418;&#24335;&#21270;&#65292;&#33021;&#22815;&#22312;&#23616;&#37096;&#20363;&#23376;&#38468;&#36817;&#20934;&#30830;&#25429;&#25417;&#40657;&#30418;&#20989;&#25968;&#30340;&#26799;&#24230;&#31526;&#21495;&#21464;&#21270;&#65292;&#36873;&#25321;&#24688;&#24403;&#30340;&#29305;&#24449;&#24402;&#22240;&#12290;</title><link>http://arxiv.org/abs/2201.12143</link><description>&lt;p&gt;
&#23616;&#37096;&#19981;&#21464;&#24615;&#35299;&#37322;&#65306;&#36890;&#36807;&#23616;&#37096;&#19981;&#21464;&#24615;&#23398;&#20064;&#23454;&#29616;&#31283;&#23450;&#19988;&#21333;&#21521;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning. (arXiv:2201.12143v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#19981;&#21464;&#24615;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#21407;&#29702;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#31283;&#23450;&#19988;&#21333;&#21521;&#30340;&#35299;&#37322;&#12290;&#26041;&#27861;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#24418;&#24335;&#21270;&#65292;&#33021;&#22815;&#22312;&#23616;&#37096;&#20363;&#23376;&#38468;&#36817;&#20934;&#30830;&#25429;&#25417;&#40657;&#30418;&#20989;&#25968;&#30340;&#26799;&#24230;&#31526;&#21495;&#21464;&#21270;&#65292;&#36873;&#25321;&#24688;&#24403;&#30340;&#29305;&#24449;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#65288;LIME&#65289;&#26041;&#27861;&#26159;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20043;&#19968;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21464;&#31181;&#65292;&#20294;&#24456;&#23569;&#26377;&#26041;&#27861;&#33021;&#22815;&#31616;&#21333;&#22320;&#29983;&#25104;&#26082;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#21448;&#31283;&#23450;&#19988;&#30452;&#35266;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#36890;&#36807;&#20511;&#37492;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#21407;&#29702;&#65288;&#26368;&#21021;&#29992;&#20110;&#20840;&#23616;&#30340;&#26679;&#26412;&#22806;&#27867;&#21270;&#65289;&#26469;&#25552;&#20379;&#36825;&#31181;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#12289;&#31283;&#23450;&#19988;&#21333;&#21521;&#30340;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#24418;&#24335;&#21270;&#65292;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23547;&#25214;&#35299;&#37322;&#30340;&#23616;&#37096;&#20363;&#23376;&#38468;&#36817;&#40657;&#30418;&#20989;&#25968;&#26799;&#24230;&#31361;&#28982;&#25913;&#21464;&#31526;&#21495;&#30340;&#29305;&#24449;&#26102;&#20855;&#26377;&#24456;&#24378;&#30340;&#20542;&#21521;&#24615;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#36873;&#25321;&#26356;&#20445;&#23432;&#30340;&#65288;&#29305;&#24449;&#65289;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locally interpretable model agnostic explanations (LIME) method is one of the most popular methods used to explain black-box models at a per example level. Although many variants have been proposed, few provide a simple way to produce high fidelity explanations that are also stable and intuitive. In this work, we provide a novel perspective by proposing a model agnostic local explanation method inspired by the invariant risk minimization (IRM) principle -originally proposed for (global) out-of-distribution generalization -- to provide such high fidelity explanations that are also stable and unidirectional across nearby examples. Our method is based on a game theoretic formulation where we theoretically show that our approach has a strong tendency to eliminate features where the gradient of the black-box function abruptly changes sign in the locality of the example we want to explain, while in other cases it is more careful and will choose a more conservative (feature) attribution, a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;Sci-Net&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#33539;&#22260;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.06812</link><description>&lt;p&gt;
Sci-Net: &#23610;&#24230;&#19981;&#21464;&#27169;&#22411;&#29992;&#20110;&#20174;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;
&lt;/p&gt;
&lt;p&gt;
Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial Imagery. (arXiv:2111.06812v5 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;Sci-Net&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#33539;&#22260;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#29289;&#30340;&#20998;&#21106;&#26159;&#22320;&#29699;&#35266;&#27979;&#21644;&#33322;&#31354;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#22823;&#37096;&#20998;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21482;&#33021;&#24212;&#29992;&#20110;&#22266;&#23450;&#25110;&#29421;&#31364;&#33539;&#22260;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#22788;&#29702;&#30340;&#26159;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#36776;&#29575;&#33539;&#22260;&#12290;&#22240;&#27492;&#65292;&#32473;&#23450;&#30340;&#33322;&#31354;&#22270;&#20687;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#37325;&#37319;&#26679;&#20197;&#21305;&#37197;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#65292;&#36825;&#23548;&#33268;&#20998;&#21106;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#23610;&#24230;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;Sci-Net&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#33539;&#22260;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;UNet&#30340;&#23618;&#27425;&#34920;&#31034;&#21644;Dense Atrous Spatial Pyramid Pooling&#26469;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#22810;&#23610;&#24230;&#34920;&#31034;&#12290;Sci-Net&#22312;Open Cities AI&#21644;Multi-Scale Building&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Buildings' segmentation is a fundamental task in the field of earth observation and aerial imagery analysis. Most existing deep learning-based methods in the literature can be applied to a fixed or narrow-range spatial resolution imagery. In practical scenarios, users deal with a broad spectrum of image resolutions. Thus, a given aerial image often needs to be re-sampled to match the spatial resolution of the dataset used to train the deep learning model, which results in a degradation in segmentation performance. To overcome this challenge, we propose, in this manuscript, Scale-invariant Neural Network (Sci-Net) architecture that segments buildings from wide-range spatial resolution aerial images. Specifically, our approach leverages UNet hierarchical representation and Dense Atrous Spatial Pyramid Pooling to extract fine-grained multi-scale representations. Sci-Net significantly outperforms state of the art models on the Open Cities AI and the Multi-Scale Building datasets with a ste
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2106.02626</link><description>&lt;p&gt;
&#32422;&#26463;&#36164;&#28304;&#19979;&#31070;&#32463;&#27169;&#22359;&#19987;&#19994;&#21270;&#30340;&#21160;&#21147;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#33041;&#22312;&#32467;&#26500;&#21644;&#21151;&#33021;&#19978;&#39640;&#24230;&#27169;&#22359;&#21270;&#65292;&#20294;&#26368;&#36817;&#30340;&#35777;&#25454;&#20351;&#19968;&#20123;&#20154;&#23545;&#20004;&#31181;&#27169;&#22359;&#21270;&#30340;&#31243;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#27979;&#35797;&#32467;&#26500;&#27169;&#22359;&#21270;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#21457;&#29616;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#65292;&#38500;&#38750;&#22312;&#26497;&#31471;&#27700;&#24179;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#29615;&#22659;&#21644;&#32593;&#32476;&#30340;&#21738;&#20123;&#29305;&#24449;&#20250;&#23548;&#33268;&#19987;&#19994;&#21270;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29609;&#20855;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#32593;&#32476;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#26465;&#20214;&#65292;&#24182;&#34920;&#26126;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20960;&#20010;&#19981;&#21516;&#30340;&#19987;&#19994;&#21270;&#24230;&#37327;&#25351;&#26631;&#32473;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19987;&#19994;&#21270;&#21482;&#33021;&#22312;&#29615;&#22659;&#20013;&#37027;&#20123;&#21487;&#20197;&#26126;&#30830;&#20998;&#31163;&#30340;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;2&#65289;&#19987;&#19994;&#21270;&#26356;&#23481;&#26131;&#22312;&#32593;&#32476;&#36164;&#28304;&#21463;&#21040;&#24378;&#28872;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;3&#65289;&#36825;&#20123;&#21457;&#29616;&#22312; qualitatively &#19978;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
&lt;/p&gt;</description></item></channel></rss>