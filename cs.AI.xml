<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DeepAAT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#26080;&#20154;&#26426;&#24433;&#20687;AAT&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#36890;&#36807;&#32771;&#34385;&#24433;&#20687;&#30340;&#31354;&#38388;&#21644;&#20809;&#35889;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;AAT&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01134</link><description>&lt;p&gt;
DeepAAT: &#24555;&#36895;&#26080;&#20154;&#26426;&#22320;&#22270;&#21046;&#20316;&#30340;&#28145;&#24230;&#33258;&#21160;&#33322;&#31354;&#19977;&#35282;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
DeepAAT: Deep Automated Aerial Triangulation for Fast UAV-based Mapping
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01134
&lt;/p&gt;
&lt;p&gt;
DeepAAT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#26080;&#20154;&#26426;&#24433;&#20687;AAT&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#36890;&#36807;&#32771;&#34385;&#24433;&#20687;&#30340;&#31354;&#38388;&#21644;&#20809;&#35889;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;AAT&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#33322;&#31354;&#19977;&#35282;&#27979;&#37327;&#65288;AAT&#65289;&#26088;&#22312;&#21516;&#26102;&#24674;&#22797;&#22270;&#20687;&#23039;&#24577;&#21644;&#37325;&#24314;&#31232;&#30095;&#28857;&#65292;&#23545;&#22320;&#29699;&#35266;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#20973;&#20511;&#20854;&#22312;&#25668;&#24433;&#27979;&#37327;&#39046;&#22495;&#25968;&#21313;&#24180;&#30340;&#30740;&#31350;&#31215;&#28096;&#65292;AAT&#24050;&#32463;&#21457;&#23637;&#25104;&#20026;&#22823;&#35268;&#27169;&#26080;&#20154;&#26426;&#22320;&#22270;&#21046;&#20316;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#22522;&#26412;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;AAT&#26041;&#27861;&#20173;&#38754;&#20020;&#25928;&#29575;&#20302;&#21644;&#31283;&#20581;&#24615;&#26377;&#38480;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepAAT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#26080;&#20154;&#26426;&#24433;&#20687;AAT&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#12290;DeepAAT&#32771;&#34385;&#20102;&#24433;&#20687;&#30340;&#31354;&#38388;&#21644;&#20809;&#35889;&#29305;&#24449;&#65292;&#22686;&#24378;&#20102;&#35299;&#20915;&#38169;&#35823;&#21305;&#37197;&#23545;&#21644;&#20934;&#30830;&#39044;&#27979;&#22270;&#20687;&#23039;&#24577;&#30340;&#33021;&#21147;&#12290;DeepAAT&#22312;AAT&#30340;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#30830;&#20445;&#20102;&#22330;&#26223;&#30340;&#20840;&#38754;&#35206;&#30422;&#21644;&#31934;&#24230;&#12290;&#20854;&#22788;&#29702;&#36895;&#24230;&#27604;&#22686;&#37327;AAT&#26041;&#27861;&#24555;&#20960;&#30334;&#20493;&#65292;&#27604;&#20840;&#23616;AAT&#26041;&#27861;&#24555;&#20960;&#21313;&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#30340;&#37325;&#24314;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Aerial Triangulation (AAT), aiming to restore image pose and reconstruct sparse points simultaneously, plays a pivotal role in earth observation. With its rich research heritage spanning several decades in photogrammetry, AAT has evolved into a fundamental process widely applied in large-scale Unmanned Aerial Vehicle (UAV) based mapping. Despite its advancements, classic AAT methods still face challenges like low efficiency and limited robustness. This paper introduces DeepAAT, a deep learning network designed specifically for AAT of UAV imagery. DeepAAT considers both spatial and spectral characteristics of imagery, enhancing its capability to resolve erroneous matching pairs and accurately predict image poses. DeepAAT marks a significant leap in AAT's efficiency, ensuring thorough scene coverage and precision. Its processing speed outpaces incremental AAT methods by hundreds of times and global AAT methods by tens of times while maintaining a comparable level of reconstruct
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#26377;&#38480;&#20803;&#20998;&#26512;&#26694;&#26550;&#26469;&#20010;&#24615;&#21270;&#20272;&#35745;&#24515;&#33039;&#32452;&#32455;&#30340;&#34987;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#36890;&#36807;&#23884;&#22871;&#20248;&#21270;&#26041;&#26696;&#30340;&#20351;&#29992;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36924;&#36817;&#21305;&#37197;&#22270;&#20687;&#25968;&#25454;&#30340;&#26448;&#26009;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.02807</link><description>&lt;p&gt;
&#19968;&#20010;&#20010;&#24615;&#21270;&#34987;&#21160;&#24515;&#33039;&#21147;&#23398;&#30340;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Optimization Framework to Personalize Passive Cardiac Mechanics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#26377;&#38480;&#20803;&#20998;&#26512;&#26694;&#26550;&#26469;&#20010;&#24615;&#21270;&#20272;&#35745;&#24515;&#33039;&#32452;&#32455;&#30340;&#34987;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#36890;&#36807;&#23884;&#22871;&#20248;&#21270;&#26041;&#26696;&#30340;&#20351;&#29992;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36924;&#36817;&#21305;&#37197;&#22270;&#20687;&#25968;&#25454;&#30340;&#26448;&#26009;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#24515;&#33039;&#21147;&#23398;&#24314;&#27169;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#24515;&#33039;&#21151;&#33021;&#22312;&#20581;&#24247;&#21644;&#30142;&#30149;&#20013;&#30340;&#29983;&#29289;&#21147;&#23398;&#24182;&#24110;&#21161;&#27835;&#30103;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#20165;&#38480;&#20110;&#20351;&#29992;&#22312;&#21333;&#19968;&#24515;&#33039;&#30456;&#20301;&#33719;&#21462;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#36890;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#20687;&#33719;&#21462;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36870;&#26377;&#38480;&#20803;&#20998;&#26512;&#65288;iFEA&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#26469;&#20272;&#35745;&#24515;&#33039;&#32452;&#32455;&#30340;&#34987;&#21160;&#26426;&#26800;&#29305;&#24615;&#12290;&#35813;iFEA&#26694;&#26550;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#22871;&#20248;&#21270;&#26041;&#26696;&#65292;&#20854;&#20013;&#22806;&#37096;&#36845;&#20195;&#21033;&#29992;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#26469;&#26368;&#20339;&#36924;&#36817;&#21305;&#37197;&#22270;&#20687;&#25968;&#25454;&#30340;&#26448;&#26009;&#21442;&#25968;&#65292;&#32780;&#20869;&#37096;&#36845;&#20195;&#37319;&#29992;&#22686;&#24191;Sellier&#31639;&#27861;&#26469;&#20272;&#35745;&#26080;&#24212;&#21147;&#21442;&#32771;&#26500;&#22411;&#12290;&#37325;&#28857;&#25918;&#22312;&#34920;&#24449;&#34987;&#21160;&#26426;&#26800;&#34892;&#20026;&#19978;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02807v1 Announce Type: cross  Abstract: Personalized cardiac mechanics modeling is a powerful tool for understanding the biomechanics of cardiac function in health and disease and assisting in treatment planning. However, current models are limited to using medical images acquired at a single cardiac phase, often limiting their applicability for processing dynamic image acquisitions. This study introduces an inverse finite element analysis (iFEA) framework to estimate the passive mechanical properties of cardiac tissue using time-dependent medical image data. The iFEA framework relies on a novel nested optimization scheme, in which the outer iterations utilize a traditional optimization method to best approximate material parameters that fit image data, while the inner iterations employ an augmented Sellier's algorithm to estimate the stress-free reference configuration. With a focus on characterizing the passive mechanical behavior, the framework employs structurally based 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#21407;&#29702;&#35770;&#25512;&#29702;&#24378;&#24230;&#21464;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;Problog&#24037;&#20855;&#35745;&#31639;&#25439;&#22833;&#24230;&#37327;&#65292;&#26368;&#32456;&#24471;&#20986;&#20102;&#20851;&#20110;&#19981;&#21516;&#36951;&#24536;&#31574;&#30053;&#24378;&#24230;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#23454;&#38469;&#24212;&#29992;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2404.02454</link><description>&lt;p&gt;
&#34913;&#37327;&#36951;&#24536;&#31574;&#30053;&#30340;&#25512;&#29702;&#24378;&#24230;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Techniques for Measuring the Inferential Strength of Forgetting Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#21407;&#29702;&#35770;&#25512;&#29702;&#24378;&#24230;&#21464;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;Problog&#24037;&#20855;&#35745;&#31639;&#25439;&#22833;&#24230;&#37327;&#65292;&#26368;&#32456;&#24471;&#20986;&#20102;&#20851;&#20110;&#19981;&#21516;&#36951;&#24536;&#31574;&#30053;&#24378;&#24230;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#23454;&#38469;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#36951;&#24536;&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#19988;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#30693;&#35782;&#24037;&#31243;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#30340;&#36951;&#24536;&#31574;&#30053;&#25110;&#19981;&#21516;&#36951;&#24536;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#22914;&#20309;&#24433;&#21709;&#21407;&#29702;&#35770;&#30340;&#25512;&#29702;&#24378;&#24230;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#26681;&#25454;&#27169;&#22411;&#35745;&#25968;&#21644;&#27010;&#29575;&#29702;&#35770;&#30340;&#30452;&#35273;&#23450;&#20041;&#29992;&#20110;&#34913;&#37327;&#25512;&#29702;&#24378;&#24230;&#21464;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#30740;&#31350;&#20102;&#27492;&#31867;&#25439;&#22833;&#24230;&#37327;&#30340;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#30693;&#35782;&#24037;&#31243;&#24037;&#20855;&#65292;&#29992;&#20110;&#20351;&#29992;Problog&#35745;&#31639;&#25439;&#22833;&#24230;&#37327;&#12290;&#35770;&#25991;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#21644;&#30830;&#23450;&#19981;&#21516;&#36951;&#24536;&#31574;&#30053;&#24378;&#24230;&#30340;&#24037;&#20316;&#26041;&#27861;&#65292;&#20197;&#21450;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;Problog&#24212;&#29992;&#29702;&#35770;&#32467;&#26524;&#30340;&#20855;&#20307;&#31034;&#20363;&#12290;&#34429;&#28982;&#37325;&#28857;&#26159;&#36951;&#24536;&#65292;&#20294;&#32467;&#26524;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#24212;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02454v1 Announce Type: new  Abstract: The technique of forgetting in knowledge representation has been shown to be a powerful and useful knowledge engineering tool with widespread application. Yet, very little research has been done on how different policies of forgetting, or use of different forgetting operators, affects the inferential strength of the original theory. The goal of this paper is to define loss functions for measuring changes in inferential strength based on intuitions from model counting and probability theory. Properties of such loss measures are studied and a pragmatic knowledge engineering tool is proposed for computing loss measures using Problog. The paper includes a working methodology for studying and determining the strength of different forgetting policies, in addition to concrete examples showing how to apply the theoretical results using Problog. Although the focus is on forgetting, the results are much more general and should have wider applicati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#30005;&#21160;&#36710;&#36742;&#30340;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#35299;&#20915;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02448</link><description>&lt;p&gt;
&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#29992;&#20110;&#24212;&#24613;&#20379;&#30005;&#65306;&#38754;&#21521;&#30005;&#20449;&#22522;&#31449;&#25937;&#21161;
&lt;/p&gt;
&lt;p&gt;
Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#30005;&#21160;&#36710;&#36742;&#30340;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#35299;&#20915;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#23478;&#30005;&#20449;&#25552;&#20379;&#21830;&#65292;&#25105;&#20204;&#20844;&#21496;&#26377;&#19968;&#20010;&#20851;&#38190;&#20351;&#21629;&#65292;&#21363;&#22312;&#20572;&#30005;&#26399;&#38388;&#20445;&#25345;&#30005;&#20449;&#26381;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#20351;&#21629;&#65292;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#32500;&#25345;&#30005;&#20449;&#22522;&#31449;&#30340;&#30005;&#21147;&#12290;&#26412;&#25991;&#32771;&#34385;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#30005;&#21160;&#36710;&#36742; (EVs) &#30452;&#25509;&#21069;&#24448;&#20854;&#20301;&#32622;&#20026;&#22522;&#31449;&#25552;&#20379;&#30005;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#23567;&#21270;&#25152;&#26377;&#30005;&#21160;&#36710;&#36742;&#30340;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#30340;EV&#36335;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#36335;&#24452;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#26032;&#22411;&#30340;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064; (EVRP) &#21464;&#20307;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#30456;&#32467;&#21512;&#30340;&#27714;&#35299;&#22120;&#12290;&#36710;&#36742;&#36873;&#25321;&#22120;&#30340;&#35268;&#21017;&#30830;&#20445;&#20102;&#25152;&#36873;EV&#24320;&#22987;&#31227;&#21160;&#26102;&#30340;&#30830;&#20999;&#29615;&#22659;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;RL&#27169;&#22411;&#30340;&#33410;&#28857;&#36873;&#25321;&#23454;&#29616;&#20102;&#24555;&#36895;&#36335;&#24452;&#29983;&#25104;&#65292;&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#23588;&#20026;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#19978;&#23545;&#25105;&#20204;&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02448v1 Announce Type: cross  Abstract: As a telecom provider, our company has a critical mission to maintain telecom services even during power outages. To accomplish the mission, it is essential to maintain the power of the telecom base stations. Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations. The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations. In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning (RL)-based node selector. The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move. In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies. We evaluate our solver on bot
&lt;/p&gt;</description></item><item><title>&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#20197;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#20445;&#25345;&#24378;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;</title><link>https://arxiv.org/abs/2404.02090</link><description>&lt;p&gt;
&#24050;&#32463;&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#35777;&#26126;&#23545;&#22122;&#22768;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Already Moderate Population Sizes Provably Yield Strong Robustness to Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02090
&lt;/p&gt;
&lt;p&gt;
&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#20197;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#20445;&#25345;&#24378;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#34920;&#26126;&#65292;&#20856;&#22411;&#30340;&#36827;&#21270;&#31639;&#27861;&#21487;&#20197;&#24456;&#22909;&#22320;&#24212;&#23545;&#35832;&#22914;&#22024;&#26434;&#30340;&#20989;&#25968;&#35780;&#20272;&#31561;&#38543;&#26426;&#24178;&#25200;&#12290;&#22312;&#31532;&#19968;&#27425;&#38024;&#23545;$(1+\lambda)$&#21644;$(1,\lambda)$&#36827;&#21270;&#31639;&#27861;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#30340;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20004;&#31181;&#31639;&#27861;&#37117;&#33021;&#23481;&#24525;&#24658;&#23450;&#30340;&#22122;&#22768;&#27010;&#29575;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;&#12290;&#20026;&#27492;&#65292;&#31181;&#32676;&#35268;&#27169;$\lambda$&#24212;&#33267;&#23569;&#20026;&#38382;&#39064;&#35268;&#27169;$n$&#30340;&#23545;&#25968;&#12290;&#22312;&#36825;&#26041;&#21521;&#19978;&#30340;&#21807;&#19968;&#20808;&#21069;&#32467;&#26524;&#28041;&#21450;&#19981;&#22826;&#29616;&#23454;&#30340;&#19968;&#20301;&#22122;&#22768;&#27169;&#22411;&#65292;&#38656;&#35201;&#36229;&#32447;&#24615;&#30340;&#38382;&#39064;&#35268;&#27169;&#31181;&#32676;&#22823;&#23567;&#65292;&#24182;&#19988;&#23545;&#20110;OneMax&#22522;&#20934;&#35777;&#26126;&#20102;&#22823;&#33268;&#26159;&#26080;&#22122;&#22768;&#36816;&#34892;&#26102;&#38388;&#30340;&#19977;&#27425;&#26041;&#30340;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26174;&#30528;&#26356;&#24378;&#32467;&#26524;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#21363;&#26080;&#22122;&#22768;&#21518;&#20195;&#21487;&#20197;&#30475;&#20316;&#26159;&#29238;&#20195;&#21644;&#26377;&#22122;&#22768;&#30340;&#21518;&#20195;&#20043;&#38388;&#30340;&#26377;&#20559;&#32479;&#19968;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02090v1 Announce Type: cross  Abstract: Experience shows that typical evolutionary algorithms can cope well with stochastic disturbances such as noisy function evaluations.   In this first mathematical runtime analysis of the $(1+\lambda)$ and $(1,\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise, we show that both algorithms can tolerate constant noise probabilities without increasing the asymptotic runtime on the OneMax benchmark. For this, a population size $\lambda$ suffices that is at least logarithmic in the problem size $n$. The only previous result in this direction regarded the less realistic one-bit noise model, required a population size super-linear in the problem size, and proved a runtime guarantee roughly cubic in the noiseless runtime for the OneMax benchmark. Our significantly stronger results are based on the novel proof argument that the noiseless offspring can be seen as a biased uniform crossover between the parent and the noisy o
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#21465;&#20107;&#22312;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#37325;&#35201;&#24615;&#21644;&#25361;&#25112;&#65292;&#24182;&#36992;&#35831;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#19968;&#36215;&#25506;&#35752;&#29983;&#25104;AI&#23545;&#25968;&#25454;&#21465;&#20107;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.01622</link><description>&lt;p&gt;
Gen4DS&#65306;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#25968;&#25454;&#21465;&#20107;&#30740;&#35752;&#20250;
&lt;/p&gt;
&lt;p&gt;
Gen4DS: Workshop on Data Storytelling in an Era of Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01622
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#21465;&#20107;&#22312;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#37325;&#35201;&#24615;&#21644;&#25361;&#25112;&#65292;&#24182;&#36992;&#35831;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#19968;&#36215;&#25506;&#35752;&#29983;&#25104;AI&#23545;&#25968;&#25454;&#21465;&#20107;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21465;&#20107;&#26159;&#19968;&#31181;&#21476;&#32769;&#32780;&#29645;&#36149;&#30340;&#20154;&#31867;&#33021;&#21147;&#65292;&#22312;&#25968;&#23383;&#26102;&#20195;&#24471;&#21040;&#20102;&#26032;&#29983;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#25968;&#25454;&#21465;&#20107;&#30340;&#35748;&#21487;&#21644;&#24212;&#29992;&#26377;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;AI&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#36825;&#19968;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24341;&#21457;&#20102;&#35768;&#22810;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#36992;&#35831;&#22823;&#23478;&#21442;&#21152;&#25105;&#20204;&#30340;&#30740;&#35752;&#20250;&#65288;Gen4DS&#65289;&#65292;&#35752;&#35770;&#29983;&#25104;AI&#22914;&#20309;&#20419;&#36827;&#25968;&#25454;&#21465;&#20107;&#30340;&#21019;&#20316;&#65311;&#29983;&#25104;AI&#22914;&#20309;&#25913;&#21464;&#25968;&#25454;&#21465;&#20107;&#32773;&#30340;&#24037;&#20316;&#27969;&#31243;&#65311;&#22312;&#21465;&#20107;&#20013;&#21152;&#20837;AI&#30340;&#39118;&#38505;&#21644;&#38544;&#24739;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01622v1 Announce Type: cross  Abstract: Storytelling is an ancient and precious human ability that has been rejuvenated in the digital age. Over the last decade, there has been a notable surge in the recognition and application of data storytelling, both in academia and industry. Recently, the rapid development of generative AI has brought new opportunities and challenges to this field, sparking numerous new questions. These questions may not necessarily be quickly transformed into papers, but we believe it is necessary to promptly discuss them to help the community better clarify important issues and research agendas for the future. We thus invite you to join our workshop (Gen4DS) to discuss questions such as: How can generative AI facilitate the creation of data stories? How might generative AI alter the workflow of data storytellers? What are the pitfalls and risks of incorporating AI in storytelling? We have designed both paper presentations and interactive activities (i
&lt;/p&gt;</description></item><item><title>&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#34701;&#21512;&#22810;&#20010;&#35270;&#35282;&#21644;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#20197;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#20013;&#26085;&#24120;&#20449;&#24687;&#33719;&#21462;&#26041;&#24335;&#30340;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2404.00989</link><description>&lt;p&gt;
360+x&#65306;&#19968;&#20010;&#20840;&#26223;&#22810;&#27169;&#24577;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
360+x: A Panoptic Multi-modal Scene Understanding Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#34701;&#21512;&#22810;&#20010;&#35270;&#35282;&#21644;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#20197;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#20013;&#26085;&#24120;&#20449;&#24687;&#33719;&#21462;&#26041;&#24335;&#30340;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23545;&#19990;&#30028;&#30340;&#24863;&#30693;&#21463;&#21040;&#22810;&#31181;&#35270;&#35282;&#21644;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#20174;&#26576;&#31181;&#35270;&#35282;&#65288;&#20363;&#22914;&#33258;&#25105;&#20013;&#24515;&#25110;&#31532;&#19977;&#20154;&#31216;&#35270;&#35282;&#65289;&#29702;&#35299;&#22330;&#26223;&#65292;&#20294;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#26223;&#35270;&#35282;&#65288;&#21363;&#22810;&#35270;&#35282;&#21644;&#22810;&#25968;&#25454;&#27169;&#24577;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25429;&#25417;&#20102;&#31532;&#19977;&#20154;&#31216;&#20840;&#26223;&#21644;&#21069;&#35270;&#22270;&#65292;&#20197;&#21450;&#20855;&#26377;&#35270;&#39057;&#12289;&#22810;&#36890;&#36947;&#38899;&#39057;&#12289;&#23450;&#21521;&#21452;&#32819;&#24310;&#36831;&#12289;&#20301;&#32622;&#25968;&#25454;&#21644;&#25991;&#26412;&#22330;&#26223;&#25551;&#36848;&#31561;&#20016;&#23500;&#27169;&#24577;&#30340;&#33258;&#25105;&#30340;&#21333;&#30524;/&#21452;&#30524;&#35270;&#22270;&#65292;&#21576;&#29616;&#20102;&#23545;&#19990;&#30028;&#30340;&#20840;&#38754;&#35266;&#23519;&#12290;&#22270;1&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;360+x&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;28&#20010;&#22330;&#26223;&#31867;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#35270;&#35282;&#21644;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25968;&#25454;&#24211;&#65292;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#26085;&#24120;&#20449;&#24687;&#30340;&#33719;&#21462;&#26041;&#24335;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#22522;&#20934;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;5&#31181;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00989v1 Announce Type: cross  Abstract: Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. Figure 1 offers a glimpse of all 28 scene categories of our 360+x dataset. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 differe
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00722</link><description>&lt;p&gt;
DRCT&#65306;&#23558;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20445;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#20043;&#22806;
&lt;/p&gt;
&lt;p&gt;
DRCT: Saving Image Super-resolution away from Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00722
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Vision Transformer&#30340;&#20302;&#23618;&#35270;&#35273;&#20219;&#21153;&#24212;&#29992;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Transformer&#26356;&#25797;&#38271;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#21033;&#29992;&#38750;&#23616;&#37096;&#21306;&#22495;&#30340;&#20449;&#24687;&#37325;&#24314;&#22270;&#20687;&#12290;&#22312;&#36229;&#20998;&#36776;&#29575;&#39046;&#22495;&#65292;&#22522;&#20110;Swin Transformer&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#19988;&#20855;&#26377;&#26059;&#36716;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#22312;&#19981;&#21516;&#31383;&#21475;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25193;&#22823;&#24863;&#30693;&#37326;&#25110;&#35774;&#35745;&#22797;&#26434;&#32593;&#32476;&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#21644;&#32593;&#32476;&#25928;&#29575;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#28145;&#24230;&#22686;&#21152;&#65292;&#31354;&#38388;&#20449;&#24687;&#24448;&#24448;&#20250;&#20943;&#23569;&#65292;&#20174;&#32780;&#23548;&#33268;&#31354;&#38388;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#24182;&#26368;&#32456;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00722v1 Announce Type: cross  Abstract: In recent years, Vision Transformer-based applications to low-level vision tasks have achieved widespread success. Unlike CNN-based models, Transformers are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To addr
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;</title><link>https://arxiv.org/abs/2404.00482</link><description>&lt;p&gt;
&#29992;&#20110;&#26031;&#25289;&#22827;&#35821;&#30340;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Named Entity Corpus for Slavic Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00482
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#65288;&#20445;&#21152;&#21033;&#20122;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#12289;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#12289;&#20420;&#35821;&#21644;&#20044;&#20811;&#20848;&#35821;&#65289;&#21629;&#21517;&#23454;&#20307;&#30340;&#35821;&#26009;&#24211;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;2017-2023&#24180;&#38388;&#26031;&#25289;&#22827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#35752;&#20250;&#30340;&#19968;&#31995;&#21015;&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;5017&#20221;&#28085;&#30422;&#19971;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#25991;&#26723;&#26631;&#26377;&#20116;&#31867;&#21629;&#21517;&#23454;&#20307;&#65292;&#27599;&#20010;&#23454;&#20307;&#30001;&#31867;&#21035;&#12289;&#24341;&#29992;&#35789;&#21644;&#21807;&#19968;&#36328;&#35821;&#35328;&#26631;&#35782;&#31526;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#35843;&#25972;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998; - &#21333;&#20010;&#20027;&#39064;&#21010;&#20998;&#21644;&#36328;&#20027;&#39064;&#21010;&#20998;&#12290;&#23545;&#20110;&#27599;&#20010;&#21010;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#32622;&#20102;&#22522;&#20934;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;XLM-RoBERTa-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#25552;&#21450;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;mT5-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;</title><link>https://arxiv.org/abs/2404.00076</link><description>&lt;p&gt;
&#20351;&#29992;&#20498;&#32622;&#26631;&#31614;&#30340;&#21518;&#38376;&#26041;&#27861;&#65306;&#33039;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22768;&#38899;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#32463;&#24120;&#20351;&#29992;&#20844;&#20849;&#25110;&#31532;&#19977;&#26041;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#36825;&#20351;&#24471;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#28508;&#22312;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#12290;&#22312;&#36825;&#31181;&#25915;&#20987;&#31867;&#22411;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#27602;&#21270;&#25968;&#25454;&#26469;&#35757;&#32451;DNN&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#21478;&#19968;&#31181;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#38750;&#24120;&#30456;&#20851;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#31867;&#22411;&#26159;&#26631;&#31614;&#32763;&#36716;&#65292;&#25915;&#20987;&#32773;&#22312;&#20854;&#20013;&#25805;&#32437;&#25968;&#25454;&#23376;&#38598;&#30340;&#26631;&#31614;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#21363;&#20351;&#26159;&#33021;&#21147;&#26377;&#38480;&#30340;&#25915;&#20987;&#32773;&#65292;&#36825;&#20123;&#25915;&#20987;&#20063;&#21487;&#33021;&#26497;&#22823;&#22320;&#38477;&#20302;&#31995;&#32479;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#65292;&#8220;&#26631;&#31614;&#23545;&#26631;&#31614;&#8221;&#65292;&#22312;&#19982;&#30446;&#26631;&#31867;&#21035;&#30456;&#20851;&#30340;&#36873;&#23450;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65288;&#25293;&#25163;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
&lt;/p&gt;</description></item><item><title>FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.20261</link><description>&lt;p&gt;
FABind+: &#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#22686;&#24378;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20261
&lt;/p&gt;
&lt;p&gt;
FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#25216;&#26415;&#20381;&#36182;&#20110;&#21463;&#29289;&#29702;&#21407;&#29702;&#25903;&#37197;&#30340;&#24191;&#27867;&#37319;&#26679;&#21644;&#27169;&#25311;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21069;&#26223;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#22686;&#38271;&#12290;&#24314;&#31435;&#22312;FABind&#30340;&#22522;&#30784;&#24037;&#20316;&#20043;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind+&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22823;&#25552;&#21319;&#20854;&#21069;&#36523;&#24615;&#33021;&#30340;&#22686;&#24378;&#29256;&#12290;&#25105;&#20204;&#30830;&#23450;&#21475;&#34955;&#39044;&#27979;&#26159;&#20998;&#23376;&#23545;&#25509;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#33879;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23545;&#25509;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#23545;&#25509;&#27169;&#22359;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#22686;&#24378;&#20854;&#23039;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#20256;&#32479;&#37319;&#26679;/&#29983;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.20208</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#39044;&#27979;&#34920;&#26684;&#20219;&#21153;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#65292;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#32570;&#22833;&#20540;&#22635;&#20805;&#31561;&#39044;&#27979;&#20219;&#21153;&#26159;&#19982;&#34920;&#26684;&#25968;&#25454;&#30456;&#20851;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#20219;&#21153;&#12290;&#23613;&#31649;LLMs&#25797;&#38271;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25910;&#38598;&#24102;&#26377;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;Llama-2&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;-shot&#39044;&#27979;&#12289;&#23569;-shot&#39044;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;NeuraLunaDTNet&#21327;&#35758;&#65292;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#20102;PRoPHET&#36335;&#30001;&#21327;&#35758;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#21464;&#21270;&#30340;&#26102;&#31354;&#22270;&#20013;&#30340;&#32852;&#31995;&#35745;&#21010;&#26469;&#20248;&#21270;&#26376;&#29699;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.20199</link><description>&lt;p&gt;
NeuraLunaDTNet&#65306;&#22522;&#20110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#24310;&#36831;&#23481;&#24525;&#26376;&#29699;&#36890;&#20449;&#32593;&#32476;&#36335;&#30001;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;NeuraLunaDTNet&#21327;&#35758;&#65292;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#20102;PRoPHET&#36335;&#30001;&#21327;&#35758;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#21464;&#21270;&#30340;&#26102;&#31354;&#22270;&#20013;&#30340;&#32852;&#31995;&#35745;&#21010;&#26469;&#20248;&#21270;&#26376;&#29699;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#36890;&#20449;&#38754;&#20020;&#20005;&#37325;&#24310;&#36831;&#12289;&#38590;&#20197;&#39044;&#27979;&#30340;&#36335;&#24452;&#21644;&#36890;&#20449;&#20013;&#26029;&#31561;&#25361;&#25112;&#12290;&#24310;&#36831;&#23481;&#24525;&#32593;&#32476;&#26550;&#26500;&#26159;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24773;&#20917;&#32780;&#19987;&#38376;&#35774;&#35745;&#30340;&#65292;&#36866;&#29992;&#20110;&#24212;&#23545;&#19968;&#20123;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;DTN&#36335;&#30001;&#21327;&#35758;&#22312;&#20248;&#21270;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#30001;&#20110;&#31354;&#38388;&#36890;&#20449;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#32531;&#35299;&#19968;&#20123;&#36335;&#30001;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#35758;NeuraLunaDTNet&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#21464;&#21270;&#30340;&#26102;&#31354;&#22270;&#20013;&#30340;&#32852;&#31995;&#35745;&#21010;&#26469;&#25552;&#39640;PRoPHET&#36335;&#30001;&#21327;&#35758;&#22312;&#26376;&#29699;&#36890;&#20449;&#20013;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20199v1 Announce Type: cross  Abstract: Space Communication poses challenges such as severe delays, hard-to-predict routes and communication disruptions. The Delay Tolerant Network architecture, having been specifically designed keeping such scenarios in mind, is suitable to address some challenges. The traditional DTN routing protocols fall short of delivering optimal performance, due to the inherent complexities of space communication. Researchers have aimed at using recent advancements in AI to mitigate some routing challenges [9]. We propose utilising a feedforward neural network to develop a novel protocol NeuraLunaDTNet, which enhances the efficiency of the PRoPHET routing protocol for lunar communication, by learning contact plans in dynamically changing spatio-temporal graph.
&lt;/p&gt;</description></item><item><title>TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.20150</link><description>&lt;p&gt;
TFB&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#20840;&#38754;&#19988;&#20844;&#24179;&#30340;&#22522;&#20934;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20150
&lt;/p&gt;
&lt;p&gt;
TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20250;&#22312;&#32463;&#27982;&#12289;&#20132;&#36890;&#12289;&#20581;&#24247;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#39046;&#22495;&#20013;&#20135;&#29983;&#65292;&#23545;&#26410;&#26469;&#25968;&#20540;&#30340;&#39044;&#27979;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#35768;&#22810;&#39044;&#27979;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#20026;&#20102;&#30830;&#20445;&#36827;&#23637;&#65292;&#26377;&#24517;&#35201;&#33021;&#22815;&#20197;&#20840;&#38754;&#19988;&#21487;&#38752;&#30340;&#26041;&#24335;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TFB&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#12290;TFB&#36890;&#36807;&#35299;&#20915;&#19982;&#25968;&#25454;&#38598;&#12289;&#27604;&#36739;&#26041;&#27861;&#21644;&#35780;&#20272;&#31649;&#36947;&#30456;&#20851;&#30340;&#32570;&#28857;&#65292;&#25512;&#21160;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#65306;1&#65289;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#65292;2&#65289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;3&#65289;&#19981;&#19968;&#33268;&#21644;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#39046;&#22495;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#26469;&#33258;10&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65306;&#20132;&#36890;&#12289;&#30005;&#21147;&#12289;&#33021;&#28304;&#12289;&#29615;&#22659;&#12289;&#33258;&#28982;&#12289;&#32463;&#27982;&#12289;&#32929;&#31080;&#24066;&#22330;&#12289;&#38134;&#34892;&#12289;&#20581;&#24247;&#21644;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#20102;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24230;&#37327;&#65292;&#21457;&#29616;&#24182;&#25913;&#36827;&#20102;PAvPU&#26694;&#26550;&#20013;&#30340;&#26680;&#24515;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2403.19826</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19826
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20102;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24230;&#37327;&#65292;&#21457;&#29616;&#24182;&#25913;&#36827;&#20102;PAvPU&#26694;&#26550;&#20013;&#30340;&#26680;&#24515;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#35821;&#20041;&#20998;&#21106;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#24212;&#29992;&#65292;&#23558;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#35821;&#20041;&#31867;&#21035;&#12290;&#36825;&#39033;&#20219;&#21153;&#36890;&#36807;&#21253;&#21547;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#36229;&#36234;&#20256;&#32479;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#65292;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#35780;&#20272;&#27599;&#20010;&#20998;&#21106;&#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35782;&#21035;&#20986;PAvPU&#65288;Patch Accuracy versus Patch Uncertainty&#65289;&#26694;&#26550;&#20013;&#30340;&#19977;&#20010;&#26680;&#24515;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#26088;&#22312;&#25913;&#36827;&#35813;&#24230;&#37327;&#30340;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#22686;&#24378;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19826v1 Announce Type: new  Abstract: In the domain of computer vision, semantic segmentation emerges as a fundamental application within machine learning, wherein individual pixels of an image are classified into distinct semantic categories. This task transcends traditional accuracy metrics by incorporating uncertainty quantification, a critical measure for assessing the reliability of each segmentation prediction. Such quantification is instrumental in facilitating informed decision-making, particularly in applications where precision is paramount. Within this nuanced framework, the metric known as PAvPU (Patch Accuracy versus Patch Uncertainty) has been developed as a specialized tool for evaluating entropy-based uncertainty in image segmentation tasks. However, our investigation identifies three core deficiencies within the PAvPU framework and proposes robust solutions aimed at refining the metric. By addressing these issues, we aim to enhance the reliability and applic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18742</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Learning Dynamics of Alignment with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#27169;&#22411;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#34429;&#28982;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29702;&#35770;&#19978;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#27491;&#24335;&#23637;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#29616;&#35937;&#65292;&#21363;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24403;&#20195;LLMs&#21644;&#23545;&#40784;&#20219;&#21153;&#19978;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24378;&#21270;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23545;&#40784;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#20813;&#36131;&#22768;&#26126;&#65306;&#26412;&#25991;&#21253;&#21547;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17601</link><description>&lt;p&gt;
LASIL&#65306;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#38271;&#26399;&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17601
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#22312;&#20132;&#36890;&#24037;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#21333;&#20010;&#36710;&#36742;&#34892;&#20026;&#21644;&#25972;&#20307;&#20132;&#36890;&#27969;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#19968;&#20010;&#30495;&#23454;&#30340;&#27169;&#25311;&#22120;&#65292;&#31934;&#30830;&#22797;&#21046;&#21508;&#31181;&#20132;&#36890;&#26465;&#20214;&#19979;&#30340;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#20381;&#36182;&#21551;&#21457;&#24335;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#24448;&#24448;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20132;&#36890;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#32780;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#30340;&#27169;&#25311;&#12290;&#30001;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#31283;&#23450;&#30340;&#38271;&#26399;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#19987;&#23478;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#22686;&#24378;&#29366;&#24577;&#24847;&#35782;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17601v1 Announce Type: new  Abstract: Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware 
&lt;/p&gt;</description></item><item><title>&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#23545;&#35805;&#24335;&#25945;&#23398;&#22330;&#26223;&#20013;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#65292;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.16687</link><description>&lt;p&gt;
&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16687
&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#23545;&#35805;&#24335;&#25945;&#23398;&#22330;&#26223;&#20013;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#65292;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#20986;&#29616;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#21069;&#26223;&#12290; LLM&#20855;&#26377;&#35299;&#37322;&#30693;&#35782;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#32771;&#34385;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#20026;&#23398;&#29983;&#25552;&#20379;&#23545;&#35805;&#24335;&#25945;&#23398;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#26816;&#39564;LLM&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#23545;&#35805;&#25945;&#23398;&#22330;&#26223;&#20013;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#25945;&#32946;&#32773;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290; &#26412;&#30740;&#31350;&#25307;&#21215;&#20102;34&#21517;&#26412;&#31185;&#29983;&#20316;&#20026;&#21442;&#19982;&#32773;&#65292;&#38543;&#26426;&#20998;&#20026;&#20004;&#32452;&#12290; &#23454;&#39564;&#32452;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23545;&#35805;&#24335;&#25945;&#23398;&#65292;&#32780;&#25511;&#21046;&#32452;&#19982;&#20154;&#31867;&#25945;&#24072;&#20114;&#21160;&#12290; &#20004;&#32452;&#37117;&#23398;&#20064;&#20102;&#20449;&#24687;&#30456;&#20851;&#35838;&#31243;&#8220;&#25968;&#23383;&#22270;&#20687;&#8221;&#30340;&#30452;&#26041;&#22270;&#22343;&#34913;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16687v1 Announce Type: cross  Abstract: In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course "Digital Ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.15933</link><description>&lt;p&gt;
&#29702;&#35299;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#20013;&#30340;&#22495;&#22823;&#23567;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Domain-Size Generalization in Markov Logic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#65288;MLNs&#65289;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#20851;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#22810;&#20010;&#30740;&#31350;&#27880;&#24847;&#21040;&#65292;&#22312;&#32473;&#23450;&#22495;&#19978;&#23398;&#20064;&#30340;MLNs&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#22495;&#19978;&#27867;&#21270;&#24456;&#24046;&#12290;&#36825;&#31181;&#34892;&#20026;&#28304;&#20110;MLN&#22312;&#19981;&#21516;&#22495;&#22823;&#23567;&#19978;&#20351;&#29992;&#26102;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#23558;&#20854;&#38480;&#21046;&#22312;MLN&#21442;&#25968;&#30340;&#26041;&#24046;&#33539;&#22260;&#20869;&#12290;&#21442;&#25968;&#26041;&#24046;&#36824;&#38480;&#21046;&#20102;&#20174;&#19981;&#21516;&#22495;&#22823;&#23567;&#20013;&#21462;&#20986;&#30340;MLN&#36793;&#32536;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#23637;&#31034;&#65292;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#65292;&#23545;&#24212;&#20110;&#22495;&#22823;&#23567;&#27867;&#21270;&#30340;&#20004;&#20010;&#33258;&#28982;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#25351;&#25968;&#38543;&#26426;&#22270;&#21644;&#20854;&#20182;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;&#30340;&#20851;&#31995;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24050;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#20250;&#20943;&#23569;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15933v1 Announce Type: new  Abstract: We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the varia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;TAI&#65289;&#21644;&#20854;&#21508;&#31181;&#23450;&#20041;&#65292;&#20027;&#24352;&#19981;&#24212;&#23558;&#8220;&#36127;&#36131;&#20219;&#30340;&#8221;&#25110;&#8220;&#36947;&#24503;&#30340;&#8221;&#20154;&#24037;&#26234;&#33021;&#31561;&#26415;&#35821;&#35270;&#20026;TAI&#30340;&#26367;&#20195;&#65292;&#32780;&#26159;&#25552;&#20513;&#20197;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#39118;&#38505;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#20851;&#38190;&#23646;&#24615;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35748;&#35782;&#21040;&#22320;&#32536;&#25919;&#27835;&#21644;&#22320;&#29702;&#21407;&#22240;&#23548;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#24046;&#24322;&#23545;&#36328;&#22269;&#20844;&#21496;&#26500;&#25104;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15457</link><description>&lt;p&gt;
&#36208;&#21521;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#20043;&#26053;-&#31532;&#19968;&#37096;&#20998;&#65306;&#36861;&#27714;&#21153;&#23454;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;TAI&#65289;&#21644;&#20854;&#21508;&#31181;&#23450;&#20041;&#65292;&#20027;&#24352;&#19981;&#24212;&#23558;&#8220;&#36127;&#36131;&#20219;&#30340;&#8221;&#25110;&#8220;&#36947;&#24503;&#30340;&#8221;&#20154;&#24037;&#26234;&#33021;&#31561;&#26415;&#35821;&#35270;&#20026;TAI&#30340;&#26367;&#20195;&#65292;&#32780;&#26159;&#25552;&#20513;&#20197;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#39118;&#38505;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#20851;&#38190;&#23646;&#24615;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35748;&#35782;&#21040;&#22320;&#32536;&#25919;&#27835;&#21644;&#22320;&#29702;&#21407;&#22240;&#23548;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#24046;&#24322;&#23545;&#36328;&#22269;&#20844;&#21496;&#26500;&#25104;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;TAI&#65289;&#21450;&#20854;&#21508;&#31181;&#23450;&#20041;&#12290;&#32771;&#34385;&#21040;&#20219;&#20309;&#31038;&#20250;&#20013;&#23562;&#37325;&#30340;&#21407;&#21017;&#65292;TAI&#36890;&#24120;&#34987;&#19968;&#20123;&#23646;&#24615;&#25152;&#29305;&#24449;&#65292;&#20854;&#20013;&#19968;&#20123;&#23646;&#24615;&#24050;&#23548;&#33268;&#30417;&#31649;&#25110;&#24037;&#31243;&#32972;&#26223;&#19979;&#30340;&#28151;&#28102;&#12290;&#25105;&#20204;&#21453;&#23545;&#20351;&#29992;&#35832;&#22914;&#8220;&#36127;&#36131;&#20219;&#30340;&#8221;&#25110;&#8220;&#36947;&#24503;&#30340;&#8221;&#20154;&#24037;&#26234;&#33021;&#31561;&#26415;&#35821;&#26469;&#26367;&#20195;TAI&#12290;&#20026;&#20102;&#24110;&#21161;&#28548;&#28165;&#20219;&#20309;&#28151;&#20081;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#23427;&#20204;&#25243;&#22312;&#33041;&#21518;&#12290;&#37492;&#20110;TAI&#22266;&#26377;&#30340;&#20027;&#35266;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20027;&#24352;&#37319;&#21462;&#20197;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#39118;&#38505;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#20851;&#38190;&#23646;&#24615;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23457;&#35270;&#20102;&#27491;&#22312;&#36827;&#34892;&#30340;&#30417;&#31649;&#29615;&#22659;&#65292;&#37325;&#28857;&#20851;&#27880;&#27431;&#30431;&#12289;&#20013;&#22269;&#21644;&#32654;&#22269;&#30340;&#20513;&#35758;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#65292;&#22522;&#20110;&#22320;&#32536;&#25919;&#27835;&#21644;&#22320;&#29702;&#21407;&#22240;&#32780;&#19981;&#21516;&#30340;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#23545;&#36328;&#22269;&#20844;&#21496;&#26500;&#25104;&#39069;&#22806;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15457v1 Announce Type: cross  Abstract: This paper reviews Trustworthy Artificial Intelligence (TAI) and its various definitions. Considering the principles respected in any society, TAI is often characterized by a few attributes, some of which have led to confusion in regulatory or engineering contexts. We argue against using terms such as Responsible or Ethical AI as substitutes for TAI. And to help clarify any confusion, we suggest leaving them behind. Given the subjectivity and complexity inherent in TAI, developing a universal framework is deemed infeasible. Instead, we advocate for approaches centered on addressing key attributes and properties such as fairness, bias, risk, security, explainability, and reliability. We examine the ongoing regulatory landscape, with a focus on initiatives in the EU, China, and the USA. We recognize that differences in AI regulations based on geopolitical and geographical reasons pose an additional challenge for multinational companies. 
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.13808</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Pretraining Data Diversity for Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13808
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#26159;&#21807;&#19968;&#26679;&#26412;&#25968;&#37327;&#65292;&#22312;&#22266;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;SSL&#24615;&#33021;&#65292;&#23613;&#31649;&#21482;&#26377;&#24403;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#24456;&#23567;&#30340;&#26102;&#20505;&#25165;&#26159;&#22914;&#27492;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#36890;&#36807;&#32593;&#32476;&#29228;&#34411;&#25110;&#25193;&#25955;&#29983;&#25104;&#30340;&#25968;&#25454;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#24322;&#24120;&#22823;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20998;&#24067;&#36716;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#19971;&#31181;SSL&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#35832;&#22914;ImageNet&#21644;YFCC100M&#31561;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#36229;&#36807;200&#20010;GPU&#22825;&#12290;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#23558;&#22312;https://github.com/hammoudhasan/DiversitySSL &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13808v1 Announce Type: cross  Abstract: We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL .
&lt;/p&gt;</description></item><item><title>&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#36827;&#34892;&#22352;&#26631;&#32423;&#25511;&#21046;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#23558;&#26426;&#22120;&#20154;&#25216;&#33021;&#36801;&#31227;&#21040;&#26032;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13801</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#25919;&#31574;&#65306;&#19982;LLMs&#19968;&#36215;&#36827;&#34892;&#22352;&#26631;&#32423;&#20307;&#24577;&#25511;&#21046;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13801
&lt;/p&gt;
&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#36827;&#34892;&#22352;&#26631;&#32423;&#25511;&#21046;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#23558;&#26426;&#22120;&#20154;&#25216;&#33021;&#36801;&#31227;&#21040;&#26032;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;LLMs&#19968;&#36215;&#35299;&#20915;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#38382;&#39064;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;LLMs&#24050;&#32463;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#23558;&#22797;&#26434;&#30340;&#39640;&#32423;&#25351;&#20196;&#36716;&#25442;&#20026;&#20013;&#32423;&#31574;&#30053;&#20195;&#30721;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#33719;&#21462;&#20219;&#21153;&#21644;&#22330;&#26223;&#23545;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21046;&#23450;&#34892;&#21160;&#35268;&#21010;&#65292;&#24182;&#36755;&#20986;&#22352;&#26631;&#32423;&#25511;&#21046;&#21629;&#20196;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20316;&#20026;&#25919;&#31574;&#30340;&#20013;&#38388;&#34920;&#31034;&#20195;&#30721;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#25552;&#31034;&#20223;&#30495;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#23454;&#39564;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#65292;&#19982;&#32570;&#24109;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26377;&#28508;&#21147;&#23558;&#26426;&#22120;&#20154;&#25216;&#33021;&#20174;&#24050;&#30693;&#20219;&#21153;&#36716;&#31227;&#21040;&#20197;&#21069;&#26410;&#35265;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13801v1 Announce Type: cross  Abstract: We demonstrate experimental results with LLMs that address robotics action planning problems. Recently, LLMs have been applied in robotics action planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes. In contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies. Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks.
&lt;/p&gt;</description></item><item><title>CAL-MAPF&#24341;&#20837;&#20102;&#32531;&#23384;&#26426;&#21046;&#26469;&#22686;&#24378;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21512;&#36866;&#30340;&#36755;&#20837;&#20219;&#21153;&#20998;&#24067;&#12289;&#39640;&#32531;&#23384;&#21629;&#20013;&#29575;&#21644;&#27969;&#30021;&#30340;&#20132;&#36890;&#36825;&#19977;&#20010;&#22240;&#32032;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13421</link><description>&lt;p&gt;
&#22522;&#20110;&#32531;&#23384;&#22686;&#24378;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Caching-Augmented Lifelong Multi-Agent Path Finding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13421
&lt;/p&gt;
&lt;p&gt;
CAL-MAPF&#24341;&#20837;&#20102;&#32531;&#23384;&#26426;&#21046;&#26469;&#22686;&#24378;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21512;&#36866;&#30340;&#36755;&#20837;&#20219;&#21153;&#20998;&#24067;&#12289;&#39640;&#32531;&#23384;&#21629;&#20013;&#29575;&#21644;&#27969;&#30021;&#30340;&#20132;&#36890;&#36825;&#19977;&#20010;&#22240;&#32032;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010; (Multi-Agent Path Finding&#65292;MAPF) &#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#28041;&#21450;&#20026;&#22810;&#20010;&#26426;&#22120;&#20154;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#32456;&#36523;MAPF&#20250;&#22312;&#20195;&#29702;&#23436;&#25104;&#20854;&#21021;&#22987;&#30446;&#26631;&#21518;&#31435;&#21363;&#23558;&#30446;&#26631;&#37325;&#26032;&#20998;&#37197;&#32473;&#20195;&#29702;&#65292;&#25552;&#20379;&#23545;&#23454;&#38469;&#20179;&#24211;&#35268;&#21010;&#30340;&#26356;&#20934;&#30830;&#36924;&#36817;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32531;&#23384;&#22686;&#24378;&#32456;&#36523;MAPF (CAL-MAPF)&#30340;&#26032;&#26426;&#21046;&#65292;&#26088;&#22312;&#25552;&#39640;&#32456;&#36523;MAPF&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#32531;&#23384;&#30340;&#20020;&#26102;&#29289;&#21697;&#23384;&#20648;&#21644;&#26367;&#25442;&#30340;&#26032;&#31867;&#22411;&#22320;&#22270;&#32593;&#26684;&#65292;&#24182;&#20026;&#20854;&#35774;&#35745;&#20102;&#19968;&#31181;&#38145;&#23450;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#32531;&#23384;&#26426;&#21046;&#32463;&#36807;&#21508;&#31181;&#32531;&#23384;&#26367;&#25442;&#31574;&#30053;&#21644;&#19968;&#31995;&#21015;&#36755;&#20837;&#20219;&#21153;&#20998;&#24067;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#30830;&#23450;&#20102;&#19977;&#20010;&#26174;&#33879;&#24433;&#21709;CAL-MAPF&#24615;&#33021;&#30340;&#20027;&#35201;&#22240;&#32032;&#65306;&#21512;&#36866;&#30340;&#36755;&#20837;&#20219;&#21153;&#20998;&#24067;&#12289;&#39640;&#32531;&#23384;&#21629;&#20013;&#29575;&#21644;&#27969;&#30021;&#30340;&#20132;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13421v2 Announce Type: replace-cross  Abstract: Multi-Agent Path Finding (MAPF), which involves finding collision-free paths for multiple robots, is crucial in various applications. Lifelong MAPF, where targets are reassigned to agents as soon as they complete their initial targets, offers a more accurate approximation of real-world warehouse planning. In this paper, we present a novel mechanism named Caching-Augmented Lifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF. We have developed a new type of map grid called cache for temporary item storage and replacement, and designed a locking mechanism for it to improve the stability of the planning solution. This cache mechanism was evaluated using various cache replacement policies and a spectrum of input task distributions. We identified three main factors significantly impacting CAL-MAPF performance through experimentation: suitable input task distribution, high cache hit rate, and smooth traffic.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#65292;&#36890;&#36807;&#21327;&#20316;&#23398;&#20064;&#22810;&#20010;&#23398;&#20064;&#32773;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13245</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#19982;&#38646;&#27425;&#36890;&#29992;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated reinforcement learning for robot motion planning with zero-shot generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#65292;&#36890;&#36807;&#21327;&#20316;&#23398;&#20064;&#22810;&#20010;&#23398;&#20064;&#32773;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#38646;&#27425;&#36890;&#29992;&#21270;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#36827;&#34892;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#37096;&#32626;&#23398;&#20064;&#31574;&#30053;&#21040;&#26032;&#29615;&#22659;&#26102;&#19981;&#38656;&#35201;&#25968;&#25454;&#25910;&#38598;&#21644;&#31574;&#30053;&#35843;&#25972;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#23398;&#20064;&#32773;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#65288;&#20113;&#31471;&#65289;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#32780;&#19981;&#20998;&#20139;&#21407;&#22987;&#25968;&#25454;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#27599;&#20010;&#23398;&#20064;&#32773;&#23558;&#20854;&#26412;&#22320;&#25511;&#21046;&#31574;&#30053;&#21644;&#30456;&#24212;&#30340;&#20272;&#35745;&#24402;&#19968;&#21270;&#21040;&#36798;&#26102;&#38388;&#19978;&#20256;&#33267;&#20113;&#31471;&#65292;&#28982;&#21518;&#20113;&#31471;&#22312;&#23398;&#20064;&#32773;&#38388;&#35745;&#31639;&#20840;&#23616;&#26368;&#20248;&#24182;&#23558;&#26368;&#20248;&#31574;&#30053;&#24191;&#25773;&#32473;&#23398;&#20064;&#32773;&#12290;&#27599;&#20010;&#23398;&#20064;&#32773;&#28982;&#21518;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20174;&#20854;&#26412;&#22320;&#25511;&#21046;&#31574;&#30053;&#21644;&#20113;&#31471;&#20013;&#36873;&#25321;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#21040;&#36798;&#26102;&#38388;&#21644;&#23433;&#20840;&#24615;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#20445;&#35777;&#12290;&#23545;&#20110;&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#65292;&#20960;&#20046;&#19968;&#33268;&#24615;&#65292;Pare&#30340;&#29702;&#35770;&#20445;&#35777;//}
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13245v1 Announce Type: cross  Abstract: This paper considers the problem of learning a control policy for robot motion planning with zero-shot generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments. We develop a federated reinforcement learning framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data. In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners. Each learner then selects between its local control policy and that from the Cloud for next iteration. The proposed framework leverages on the derived zero-shot generalization guarantees on arrival time and safety. Theoretical guarantees on almost-sure convergence, almost consensus, Pare
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;</title><link>https://arxiv.org/abs/2403.13041</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#31169;&#23494;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Provable Privacy with Non-Private Pre-Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20998;&#26512;&#24046;&#20998;&#31169;&#23494;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#25968;&#25454;&#30456;&#20851;&#30340;&#39044;&#22788;&#29702;&#30340;&#28508;&#22312;&#38544;&#31169;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#26032;&#30340;&#25216;&#26415;&#27010;&#24565;&#24314;&#31435;&#20102;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;&#65306;&#19968;&#31181;&#31216;&#20026;&#24179;&#28369;DP&#30340;DP&#21464;&#20307;&#20197;&#21450;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
&lt;/p&gt;</description></item><item><title>DetToolChain&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#21487;&#20197;&#37322;&#25918;MLLM&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26816;&#27979;&#38142;&#24335;&#24605;&#32500;&#33258;&#21160;&#21270;&#20219;&#21153;&#20998;&#35299;&#21644;&#36880;&#27493;&#26694;&#32454;&#21270;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.12488</link><description>&lt;p&gt;
DetToolChain&#65306;&#19968;&#31181;&#37322;&#25918;MLLM&#26816;&#27979;&#33021;&#21147;&#30340;&#26032;&#25552;&#31034;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12488
&lt;/p&gt;
&lt;p&gt;
DetToolChain&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#21487;&#20197;&#37322;&#25918;MLLM&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26816;&#27979;&#38142;&#24335;&#24605;&#32500;&#33258;&#21160;&#21270;&#20219;&#21153;&#20998;&#35299;&#21644;&#36880;&#27493;&#26694;&#32454;&#21270;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;DetToolChain&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#29992;&#20110;&#37322;&#25918;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#22914;GPT-4V&#21644;Gemini&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21463;&#39640;&#31934;&#24230;&#26816;&#27979;&#20808;&#39564;&#21551;&#21457;&#30340;&#26816;&#27979;&#25552;&#31034;&#24037;&#20855;&#21253;&#21644;&#19968;&#20010;&#23454;&#29616;&#36825;&#20123;&#25552;&#31034;&#30340;&#26032;Chain-of-Thought&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24037;&#20855;&#21253;&#20013;&#30340;&#25552;&#31034;&#26088;&#22312;&#24341;&#23548;MLLM&#38598;&#20013;&#22312;&#21306;&#22495;&#20449;&#24687;&#19978;&#65288;&#20363;&#22914;&#65292;&#25918;&#22823;&#65289;&#65292;&#25353;&#29031;&#27979;&#37327;&#26631;&#20934;&#38405;&#35835;&#22352;&#26631;&#65288;&#20363;&#22914;&#65292;&#21472;&#21152;&#26631;&#23610;&#21644;&#25351;&#21335;&#38024;&#65289;&#65292;&#24182;&#20174;&#19978;&#19979;&#25991;&#20449;&#24687;&#20013;&#25512;&#26029;&#65288;&#20363;&#22914;&#65292;&#21472;&#21152;&#22330;&#26223;&#22270;&#65289;&#12290;&#22522;&#20110;&#36825;&#20123;&#24037;&#20855;&#65292;&#26032;&#30340;&#26816;&#27979;Chain-of-Thought&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#65292;&#35786;&#26029;&#39044;&#27979;&#65292;&#24182;&#35268;&#21010;&#36880;&#27493;&#26694;&#32454;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#31995;&#21015;&#26816;&#27979;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#23454;&#65292;&#29305;&#21035;&#26159;&#22312;&#22256;&#38590;&#24773;&#20917;&#19979;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12488v1 Announce Type: cross  Abstract: We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini. Our approach consists of a detection prompting toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these prompts. Specifically, the prompts in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene graphs). Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements. The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases. Compared to existing state-of-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11169</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Correcting misinformation on social media with a large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20449;&#24687;&#20250;&#30772;&#22351;&#20844;&#20247;&#23545;&#31185;&#23398;&#21644;&#27665;&#20027;&#30340;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#19981;&#20934;&#30830;&#20449;&#24687;&#20250;&#36805;&#36895;&#20256;&#25773;&#12290;&#19987;&#23478;&#21644;&#26222;&#36890;&#20154;&#36890;&#36807;&#25163;&#21160;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#20934;&#30830;&#20449;&#24687;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#32416;&#27491;&#35823;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#25285;&#24551;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31561;&#25216;&#26415;&#20351;&#35823;&#20449;&#24687;&#26356;&#23481;&#26131;&#29983;&#25104;&#12290;LLMs&#36824;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#65292;&#21487;&#20197;&#21152;&#36895;&#32416;&#27491;&#35823;&#20449;&#24687;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30001;&#20110;&#32570;&#20047;&#26368;&#26032;&#20449;&#24687;&#12289;&#20542;&#21521;&#20110;&#29983;&#25104;&#20284;&#26159;&#32780;&#38750;&#30340;&#20869;&#23481;&#21644;&#24341;&#29992;&#20197;&#21450;&#26080;&#27861;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#32780;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUSE&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#26368;&#26032;&#20449;&#24687;&#35775;&#38382;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;LLM&#12290;&#36890;&#36807;&#26816;&#32034;&#19978;&#19979;&#25991;&#35777;&#25454;&#21644;&#21453;&#39539;&#65292;MUSE&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#21487;&#20449;&#30340;&#35299;&#37322;&#21644;&#21442;&#32771;&#12290;&#23427;&#36824;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
&lt;/p&gt;</description></item><item><title>&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.09722</link><description>&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Prediction of readmission of patients by extracting biomedical concepts from clinical texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09722
&lt;/p&gt;
&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#20026;&#36827;&#34892;&#26088;&#22312;&#25913;&#21892;&#20026;&#24739;&#32773;&#25552;&#20379;&#30340;&#21307;&#30103;&#26381;&#21153;&#24182;&#38477;&#20302;&#21307;&#30103;&#31995;&#32479;&#25104;&#26412;&#30340;&#30740;&#31350;&#21019;&#36896;&#20102;&#28508;&#22312;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#21307;&#23398;&#39046;&#22495;&#22791;&#21463;&#20851;&#27880;&#30340;&#19968;&#20010;&#35805;&#39064;&#26159;&#35782;&#21035;&#20986;&#21018;&#20174;&#21307;&#38498;&#20986;&#38498;&#21518;&#21487;&#33021;&#24456;&#24555;&#20877;&#27425;&#20837;&#38498;&#30340;&#24739;&#32773;&#12290;&#36825;&#31181;&#35782;&#21035;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#21644;&#23545;&#24739;&#32773;&#30005;&#23376;&#25991;&#20214;&#20013;&#30340;&#20986;&#38498;&#25253;&#21578;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#26469;&#39044;&#27979;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#20004;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65306;&#35789;&#34955;&#27169;&#22411;&#21644;&#27010;&#24565;&#34955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09722v1 Announce Type: cross  Abstract: Today, the existence of a vast amount of electronic health data has created potential capacities for conducting studies aiming to improve the medical services provided to patients and reduce the costs of the healthcare system. One of the topics that has been receiving attention in the field of medicine in recent years is the identification of patients who are likely to be re-hospitalized shortly after being discharged from the hospital. This identification can help doctors choose appropriate treatment methods, thereby reducing the rate of patient re-hospitalization and resulting in effective treatment cost reduction. In this study, the prediction of patient re-hospitalization using text mining approaches and the processing of discharge report texts in the patient's electronic file has been discussed. To this end, the performance of various machine learning models has been evaluated using two approaches: bag of word and bag of concept, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#26041;&#27861;&#65292;&#22312;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#25552;&#21319;&#65292;&#20197;&#21450;&#35757;&#32451;&#26102;&#38388;&#21644;&#25512;&#29702;&#26102;&#38388;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2403.09680</link><description>&lt;p&gt;
&#39044;&#25490;&#24207;Tsetlin&#26426;&#22120;&#65288;&#22522;&#22240;K-Medoid&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#26041;&#27861;&#65292;&#22312;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#25552;&#21319;&#65292;&#20197;&#21450;&#35757;&#32451;&#26102;&#38388;&#21644;&#25512;&#29702;&#26102;&#38388;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#24555;&#36895;&#36951;&#20256;&#31639;&#27861;&#20174;&#25968;&#25454;&#38598;&#20013;&#30830;&#23450;N&#20010;&#25968;&#25454;&#28857;&#65292;&#20197;&#35299;&#20915;&#26368;&#22823;&#31163;&#25955;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34987;&#29992;&#20316;&#36816;&#34892;K-Medoid&#32858;&#31867;&#31639;&#27861;&#30340;&#21021;&#22987;&#25918;&#32622;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#24555;&#36895;&#36951;&#20256;&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#27721;&#26126;&#36317;&#31163;&#26469;&#23545;&#40784;N&#20010;&#29420;&#31435;&#30340;Tsetlin Machines&#12290;&#23545;&#20110;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#32467;&#26524;&#26174;&#31034;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;&#39640;&#36798;10&#65285;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#32422;383&#20493;&#65292;&#25512;&#29702;&#26102;&#38388;&#20943;&#23569;&#20102;&#32422;86&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09680v1 Announce Type: cross  Abstract: This paper proposes a machine learning pre-sort stage to traditional supervised learning using Tsetlin Machines. Initially, N data-points are identified from the dataset using an expedited genetic algorithm to solve the maximum dispersion problem. These are then used as the initial placement to run the K-Medoid clustering algorithm. Finally, an expedited genetic algorithm is used to align N independent Tsetlin Machines by maximising hamming distance. For MNIST level classification problems, results demonstrate up to 10% improvement in accuracy, approx. 383X reduction in training time and approx. 86X reduction in inference time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Keyformer&#8221;&#30340;&#21019;&#26032;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#26631;&#35760;&#26469;&#20943;&#23569;KV&#32531;&#23384;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09054</link><description>&lt;p&gt;
Keyformer&#65306;&#36890;&#36807;&#20851;&#38190;&#26631;&#35760;&#36873;&#25321;&#20943;&#23569;KV&#32531;&#23384;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#29983;&#25104;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Keyformer&#8221;&#30340;&#21019;&#26032;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#26631;&#35760;&#26469;&#20943;&#23569;KV&#32531;&#23384;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#22522;&#30784;&#26550;&#26500;&#12290;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25512;&#26029;&#36807;&#31243;&#28041;&#21450;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;&#25552;&#31034;&#22788;&#29702;&#21644;&#26631;&#35760;&#29983;&#25104;&#12290;&#26631;&#35760;&#29983;&#25104;&#65292;&#26500;&#25104;&#20102;&#22823;&#37096;&#20998;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20027;&#35201;&#28041;&#21450;&#21521;&#37327;-&#30697;&#38453;&#20056;&#27861;&#21644;&#19982;&#38190;-&#20540;(KV)&#32531;&#23384;&#20132;&#20114;&#12290;&#30001;&#20110;&#20174;&#23384;&#20648;&#31995;&#32479;&#20256;&#36755;&#26435;&#37325;&#21644;KV&#32531;&#23384;&#20540;&#21040;&#35745;&#31639;&#21333;&#20803;&#30340;&#24320;&#38144;&#65292;&#36825;&#19968;&#38454;&#27573;&#21463;&#21040;&#20869;&#23384;&#24102;&#23485;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#20869;&#23384;&#29942;&#39048;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#21644;&#22823;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#24212;&#29992;&#20013;&#23588;&#20026;&#31361;&#20986;&#65292;&#36825;&#20004;&#32773;&#23545;LLMs&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;  &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#8220;Keyformer&#8221;&#65292;&#20197;&#32531;&#35299;&#19982;KV&#32531;&#23384;&#22823;&#23567;&#21644;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;Keyformer&#21033;&#29992;&#20102;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#22823;&#32422;90
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces "Keyformer", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.05297</link><description>&lt;p&gt;
PEEB&#65306;&#20855;&#26377;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#35821;&#35328;&#29942;&#39048;&#30340;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05297
&lt;/p&gt;
&lt;p&gt;
PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#20998;&#31867;&#22120;&#20381;&#36182;&#20110;&#21253;&#21547;{text encoder&#24050;&#30693;&#30340;&#31867;&#21517;&#31216;}&#30340;&#25552;&#31034;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;CLIP&#22312;&#26032;&#31867;&#21035;&#25110;&#20854;&#21517;&#31216;&#24456;&#23569;&#22312;&#20114;&#32852;&#32593;&#19978;&#20986;&#29616;&#30340;&#31867;&#21035;&#65288;&#20363;&#22914;&#40479;&#31867;&#30340;&#23398;&#21517;&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#38024;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEEB - &#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#65288;1&#65289;&#23558;&#31867;&#21035;&#21517;&#31216;&#34920;&#36798;&#20026;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65307;&#21644;&#65288;2&#65289;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#35745;&#31639;&#29992;&#20110;&#20998;&#31867;&#30340;&#36923;&#36753;&#20998;&#25968;&#12290;&#22312;&#19968;&#20010;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#31867;&#21035;&#21517;&#31216;&#26159;&#26410;&#30693;&#30340;&#65292;PEEB&#22312;&#20934;&#30830;&#24615;&#19978;&#22823;&#24133;&#20248;&#20110;CLIP&#65288;&#32422;&#20026;10&#20493;&#65289;&#12290;&#19982;&#22522;&#20110;&#37096;&#20998;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;PEEB&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#19978;&#26159;&#26368;&#20808;&#36827;&#30340;&#65288;88.80%&#20934;&#30830;&#29575;&#65289;&#65292;&#32780;&#19988;&#36824;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#35753;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#20197;&#24418;&#25104;&#26032;&#30340;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05297v1 Announce Type: cross  Abstract: CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.04810</link><description>&lt;p&gt;
&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Restricted Bayesian Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20316;&#20026;&#40657;&#30418;&#27169;&#22411;&#30340;&#36816;&#34892;&#26041;&#24335;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#12289;&#36807;&#25311;&#21512;&#12289;&#27424;&#25311;&#21512;&#12289;&#26799;&#24230;&#28040;&#22833;&#31561;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#31283;&#20581;&#30340;&#25910;&#25947;&#20540;&#65292;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#23588;&#20854;&#26159;&#24403;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#30340;&#20984;&#24615;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04810v1 Announce Type: cross  Abstract: Modern deep learning tools are remarkably effective in addressing intricate problems. However, their operation as black-box models introduces increased uncertainty in predictions. Additionally, they contend with various challenges, including the need for substantial storage space in large networks, issues of overfitting, underfitting, vanishing gradients, and more. This study explores the concept of Bayesian Neural Networks, presenting a novel architecture designed to significantly alleviate the storage space complexity of a network. Furthermore, we introduce an algorithm adept at efficiently handling uncertainties, ensuring robust convergence values without becoming trapped in local optima, particularly when the objective function lacks perfect convexity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;CREEM&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#21435;&#35760;&#24518;&#24182;&#24341;&#20837;&#23436;&#21892;&#36807;&#31243;&#26469;&#23454;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04787</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#21644;&#23436;&#21892;&#36807;&#21435;&#26469;&#19981;&#26029;&#28436;&#36827;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Ever-Evolving Memory by Blending and Refining the Past
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04787
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;CREEM&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#21435;&#35760;&#24518;&#24182;&#24341;&#20837;&#23436;&#21892;&#36807;&#31243;&#26469;&#23454;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31867;&#20284;&#20154;&#31867;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#26500;&#24314;&#38271;&#26399;&#35760;&#24518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26500;&#24314;&#35760;&#24518;&#30340;&#19968;&#20010;&#22825;&#30495;&#26041;&#27861;&#21487;&#33021;&#21482;&#26159;&#21015;&#20986;&#24635;&#32467;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#24403;&#35828;&#35805;&#32773;&#30340;&#29366;&#24577;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#65292;&#36825;&#26679;&#20570;&#21487;&#33021;&#20250;&#23548;&#33268;&#38382;&#39064;&#65292;&#24182;&#31215;&#32047;&#30683;&#30462;&#20449;&#24687;&#12290;&#35760;&#24518;&#20445;&#25345;&#26377;&#32452;&#32455;&#23545;&#20110;&#38477;&#20302;&#22238;&#24212;&#29983;&#25104;&#22120;&#30340;&#28151;&#20081;&#24456;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;&#65292;CREEM&#12290;&#19982;&#20165;&#22522;&#20110;&#24403;&#21069;&#23545;&#35805;&#26500;&#24314;&#35760;&#24518;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35760;&#24518;&#24418;&#25104;&#36807;&#31243;&#20013;&#28151;&#21512;&#36807;&#21435;&#30340;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23436;&#21892;&#36807;&#31243;&#26469;&#22788;&#29702;&#22810;&#20313;&#25110;&#36807;&#26102;&#20449;&#24687;&#12290;&#36825;&#31181;&#21019;&#26032;&#24615;&#26041;&#27861;&#36890;&#36807;&#30830;&#20445;&#19968;&#20010;&#26356;&#21152;&#30693;&#24773;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#26088;&#22312;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04787v1 Announce Type: cross  Abstract: For a human-like chatbot, constructing a long-term memory is crucial. A naive approach for making a memory could be simply listing the summarized dialogue. However, this can lead to problems when the speaker's status change over time and contradictory information gets accumulated. It is important that the memory stays organized to lower the confusion for the response generator. In this paper, we propose a novel memory scheme for long-term conversation, CREEM. Unlike existing approaches that construct memory based solely on current sessions, our proposed model blending past memories during memory formation. Additionally, we introduce refining process to handle redundant or outdated information. This innovative approach seeks for overall improvement and coherence of chatbot responses by ensuring a more informed and dynamically evolving long-term memory.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.03894</link><description>&lt;p&gt;
IRCoder: &#20013;&#38388;&#34920;&#31034;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03894
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#24050;&#36805;&#36895;&#25104;&#20026;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26368;&#21463;&#27426;&#36814;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#33258;&#28982;&#35821;&#35328;LM&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#23545;&#20195;&#30721;-LMs&#65288;&#21363;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;LMs&#65289;&#30340;&#22810;&#35821;&#35328;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#22914;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#25968;&#25454;&#22686;&#24378;&#20197;&#21450;&#20107;&#21518;LM&#35843;&#25972;&#65292;&#20197;&#21450;&#21033;&#29992;&#21407;&#22987;&#25991;&#26412;&#20869;&#23481;&#20043;&#22806;&#30340;&#25968;&#25454;&#28304;&#65292;&#35201;&#31232;&#23569;&#24471;&#22810;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;&#20195;&#30721;-LMs&#20165;&#22312;&#28304;&#20195;&#30721;&#25991;&#20214;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#29616;&#25104;&#30340;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;&#36328;&#32534;&#31243;&#35821;&#35328;&#20849;&#20139;&#65289;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#24182;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#21069;&#26223;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;SLTrans&#65292;&#19968;&#20010;&#30001;&#36817;400&#19975;&#20010;&#33258;&#21253;&#21547;&#28304;&#20195;&#30721;&#25991;&#20214;&#32452;&#25104;&#30340;&#24182;&#34892;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 Announce Type: new  Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;G2ST&#30340;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#23558;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03689</link><description>&lt;p&gt;
&#36890;&#29992;&#21040;&#19987;&#19994;&#30340;&#30005;&#23376;&#21830;&#21153;LLMs&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
General2Specialized LLMs Translation for E-commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;G2ST&#30340;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#23558;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#20027;&#35201;&#22788;&#29702;&#36890;&#29992;&#39046;&#22495;&#30340;&#32763;&#35793;&#65292;&#24573;&#30053;&#20102;&#20855;&#26377;&#29305;&#27530;&#20889;&#20316;&#20844;&#24335;&#30340;&#39046;&#22495;&#65292;&#27604;&#22914;&#30005;&#23376;&#21830;&#21153;&#21644;&#27861;&#24459;&#25991;&#20214;&#12290;&#20197;&#30005;&#23376;&#21830;&#21153;&#20026;&#20363;&#65292;&#25991;&#26412;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#39046;&#22495;&#30456;&#20851;&#35789;&#27719;&#65292;&#24182;&#19988;&#23384;&#22312;&#26356;&#22810;&#30340;&#35821;&#27861;&#38382;&#39064;&#65292;&#36825;&#23548;&#33268;&#24403;&#21069;NMT&#26041;&#27861;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#19968;&#32452;&#26415;&#35821;&#23545;&#65288;&#23545;&#40784;&#30340;&#20013;&#33521;&#21452;&#35821;&#26415;&#35821;&#65289;&#21644;&#19968;&#20010;&#38024;&#23545;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#36827;&#34892;&#27880;&#37322;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65288;&#21517;&#20026;G2ST&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#65292;&#20197;&#23558;&#19968;&#20010;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#12290;&#35813;&#33539;&#24335;&#36866;&#29992;&#20110;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;NMT&#27169;&#22411;&#12290;&#23545;&#30495;&#23454;&#30005;&#23376;&#21830;&#21153;&#26631;&#39064;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#20102;&#21331;&#36234;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03689v1 Announce Type: cross  Abstract: Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and 
&lt;/p&gt;</description></item><item><title>Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01232</link><description>&lt;p&gt;
Polynormer: &#22810;&#39033;&#24335;&#34920;&#36798;&#30340;&#32447;&#24615;&#26102;&#38388;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Polynormer: Polynomial-Expressive Graph Transformer in Linear Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01232
&lt;/p&gt;
&lt;p&gt;
Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#36716;&#25442;&#22120;&#65288;GTs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26550;&#26500;&#65292;&#29702;&#35770;&#19978;&#23427;&#27604;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26356;&#20855;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;GT&#27169;&#22411;&#33267;&#23569;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#22240;&#27492;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#32447;&#24615;GTs&#65292;&#20294;&#23427;&#20204;&#22312;&#20960;&#20010;&#28909;&#38376;&#22270;&#25968;&#25454;&#38598;&#19978;&#20173;&#33853;&#21518;&#20110;GNN&#23545;&#24212;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#23454;&#38469;&#34920;&#29616;&#21147;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#20102;&#24179;&#34913;GTs&#30340;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Polynormer&#65292;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#12290;Polynormer&#26500;&#24314;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#36755;&#20837;&#29305;&#24449;&#19978;&#23398;&#20064;&#39640;&#27425;&#22810;&#39033;&#24335;&#12290;&#20026;&#20102;&#20351;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#20998;&#24320;&#38598;&#25104;&#65292;&#20174;&#32780;&#20135;&#29983;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#20851;&#27880;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;Polynormer&#37319;&#29992;&#20102;&#32447;&#24615;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#20851;&#27880;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#23545;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#20013;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00284</link><description>&lt;p&gt;
&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
A Survey of Route Recommendations: Methods, Applications, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00284
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#23545;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#20013;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#65292;&#38543;&#30528;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#37096;&#32626;&#22312;&#25972;&#20010;&#22478;&#24066;&#65292;&#22823;&#37327;&#25968;&#25454;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#27491;&#22312;&#20351;&#29616;&#20195;&#22478;&#24066;&#21457;&#23637;&#26234;&#33021;&#21270;&#12290;&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36335;&#32447;&#25512;&#33616;&#21450;&#20854;&#24212;&#29992;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#30452;&#25509;&#24433;&#21709;&#24066;&#27665;&#30340;&#20986;&#34892;&#20064;&#24815;&#12290;&#22522;&#20110;&#22823;&#25968;&#25454;&#65288;&#21487;&#33021;&#26159;&#22810;&#27169;&#24335;&#65289;&#24320;&#21457;&#26234;&#33021;&#39640;&#25928;&#30340;&#20986;&#34892;&#36335;&#32447;&#24050;&#25104;&#20026;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#23427;&#20998;&#20026;&#20197;&#19979;&#19977;&#20010;&#37096;&#20998;&#65306;1&#65289;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#23545;&#22823;&#37327;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#23427;&#20204;&#30340;&#21382;&#21490;&#20851;&#31995;&#24182;&#25581;&#31034;&#26368;&#26032;&#36827;&#23637;&#12290;2&#65289;&#24212;&#29992;&#26041;&#38754;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#37327;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#20013;&#36335;&#32447;&#25512;&#33616;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#12290;3&#65289;&#25105;&#20204;&#36842;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00284v1 Announce Type: new  Abstract: Nowadays, with advanced information technologies deployed citywide, large data volumes and powerful computational resources are intelligentizing modern city development. As an important part of intelligent transportation, route recommendation and its applications are widely used, directly influencing citizens` travel habits. Developing smart and efficient travel routes based on big data (possibly multi-modal) has become a central challenge in route recommendation research. Our survey offers a comprehensive review of route recommendation work based on urban computing. It is organized by the following three parts: 1) Methodology-wise. We categorize a large volume of traditional machine learning and modern deep learning methods. Also, we discuss their historical relations and reveal the edge-cutting progress. 2) Application\-wise. We present numerous novel applications related to route commendation within urban computing scenarios. 3) We di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16311</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#20013;&#25991;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Chinese Sentence Pattern Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#21477;&#24335;&#32467;&#26500;&#65288;SPS&#65289;&#35299;&#26512;&#26159;&#19968;&#31181;&#20027;&#35201;&#29992;&#20110;&#35821;&#35328;&#25945;&#23398;&#30340;&#21477;&#27861;&#20998;&#26512;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;SPS&#35299;&#26512;&#22120;&#20027;&#35201;&#20381;&#36182;&#20110;&#25945;&#31185;&#20070;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#65292;&#32570;&#20047;&#36328;&#39046;&#22495;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#25105;&#35757;&#32451;&#26694;&#26550;&#20869;&#12290;&#20174;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#37096;&#20998;&#21477;&#27861;&#35268;&#21017;&#65292;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#32467;&#21512;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#35299;&#26512;&#22120;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;F1&#25351;&#26631;&#27604;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;&#39640;&#20986;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 Announce Type: cross  Abstract: Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#24191;&#21578;&#31454;&#26631;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20256;&#32479;RL&#31639;&#27861;&#22312;&#22312;&#32447;&#29615;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15102</link><description>&lt;p&gt;
&#36712;&#36857;&#24335;&#36845;&#20195;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#31454;&#26631;
&lt;/p&gt;
&lt;p&gt;
Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15102
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24191;&#21578;&#31454;&#26631;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20256;&#32479;RL&#31639;&#27861;&#22312;&#22312;&#32447;&#29615;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#24191;&#21578;&#20013;&#65292;&#24191;&#21578;&#20027;&#21442;&#19982;&#24191;&#21578;&#31454;&#25293;&#20197;&#33719;&#21462;&#24191;&#21578;&#26426;&#20250;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#38656;&#27714;&#26041;&#24179;&#21488;(DSPs)&#25552;&#20379;&#30340;&#33258;&#21160;&#31454;&#26631;&#24037;&#20855;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#31454;&#26631;&#31639;&#27861;&#36890;&#24120;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;RL&#30340;&#33258;&#21160;&#31454;&#26631;&#31574;&#30053;&#26159;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#37096;&#32626;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21487;&#20197;&#24182;&#34892;&#37096;&#32626;&#22810;&#20010;&#33258;&#21160;&#31454;&#26631;&#20195;&#29702;&#20197;&#25910;&#38598;&#22823;&#37327;&#20132;&#20114;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#21033;&#29992;&#31163;&#32447;RL&#31639;&#27861;&#35757;&#32451;&#26032;&#31574;&#30053;&#12290;&#35757;&#32451;&#21518;&#30340;&#31574;&#30053;&#38543;&#21518;&#21487;&#20197;&#37096;&#32626;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#36845;&#20195;&#35757;&#32451;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#36845;&#20195;&#31163;&#32447;RL&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31181;&#36845;&#20195;&#31163;&#32447;RL&#26694;&#26550;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#20854;&#26681;&#28304;&#22312;&#20110;&#30001;&#20110;&#20869;&#22312;&#21407;&#22240;&#32780;&#23548;&#33268;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15102v1 Announce Type: cross  Abstract: In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ reinforcement learning (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in simulation, leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inhe
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STORM&#30340;&#20889;&#20316;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#21512;&#25104;&#20027;&#39064;&#27010;&#35201;&#65292;&#20197;&#36741;&#21161;&#20174;&#22836;&#24320;&#22987;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#31456;&#12290;</title><link>https://arxiv.org/abs/2402.14207</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#36741;&#21161;&#25776;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;
&lt;/p&gt;
&lt;p&gt;
Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STORM&#30340;&#20889;&#20316;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#21512;&#25104;&#20027;&#39064;&#27010;&#35201;&#65292;&#20197;&#36741;&#21161;&#20174;&#22836;&#24320;&#22987;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#25776;&#20889;&#22522;&#20110;&#20107;&#23454;&#21644;&#26377;&#26465;&#29702;&#30340;&#38271;&#31687;&#25991;&#31456;&#65292;&#20351;&#20854;&#22312;&#24191;&#24230;&#21644;&#28145;&#24230;&#19978;&#19982;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#21487;&#23218;&#32654;&#12290;&#36825;&#19968;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#22312;&#25776;&#20889;&#21069;&#38454;&#27573;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22914;&#20309;&#30740;&#31350;&#20027;&#39064;&#24182;&#20934;&#22791;&#22823;&#32434;&#20197;&#20415;&#25776;&#20889;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STORM&#65292;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#36827;&#34892;&#20027;&#39064;&#27010;&#35201;&#21512;&#25104;&#30340;&#20889;&#20316;&#31995;&#32479;&#12290;STORM&#27169;&#25311;&#20102;&#25776;&#20889;&#21069;&#38454;&#27573;&#65292;&#20854;&#20013;&#65288;1&#65289;&#21457;&#29616;&#30740;&#31350;&#32473;&#23450;&#20027;&#39064;&#30340;&#22810;&#26679;&#21270;&#35266;&#28857;&#65292;&#65288;2&#65289;&#27169;&#25311;&#20250;&#35805;&#65292;&#25776;&#20889;&#25345;&#26377;&#19981;&#21516;&#35266;&#28857;&#30340;&#20316;&#32773;&#21521;&#22522;&#20110;&#21487;&#20449;&#20114;&#32852;&#32593;&#26469;&#28304;&#30340;&#20027;&#39064;&#19987;&#23478;&#25552;&#38382;&#65292;&#65288;3&#65289;&#25972;&#29702;&#25910;&#38598;&#21040;&#30340;&#20449;&#24687;&#20197;&#21019;&#24314;&#22823;&#32434;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;FreshWiki&#65292;&#19968;&#20010;&#21253;&#21547;&#26368;&#26032;&#39640;&#36136;&#37327;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#22823;&#32434;&#35780;&#20272;&#25351;&#26631;&#20197;&#35780;&#20272;&#25776;&#20889;&#21069;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14207v1 Announce Type: cross  Abstract: We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.   For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.10100</link><description>&lt;p&gt;
&#35843;&#35856;&#65306;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#38480;&#21046;&#26465;&#20214;&#26159;&#20197;&#21453;&#26144;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#25910;&#38598;&#30340;&#23567;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21253;&#25324;DenseNet&#21644;ConvNeXt&#22312;&#20869;&#30340;CNN&#27169;&#22411;&#65292;&#20197;&#21450;ViT&#12289;SWIN&#21644;AST&#31561;&#36716;&#25442;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35832;&#22914;YAMNet&#21644;VGGish&#30340;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#20020;&#24202;&#25968;&#25454;&#19978;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#20174;&#21330;&#20013;&#24739;&#32773;&#20013;&#26032;&#25910;&#38598;&#20102;&#20004;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#24739;&#32773;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#21457;&#29616;&#22522;&#20110;&#23427;&#20204;&#20174;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;RGB&#21644;&#28784;&#24230;&#35889;&#22270;&#36716;&#25442;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#21487;&#20197;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#65292;&#20854;&#20013;DenseNet-Contrastive&#21644;AST&#27169;&#22411;&#34920;&#29616;&#31361;&#20986;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#23454;&#26045;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#30456;&#27604;&#65292;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#12289;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07689</link><description>&lt;p&gt;
OrderBkd: &#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#36827;&#34892;&#30340;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
OrderBkd: Textual backdoor attack through repositioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#23454;&#26045;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#30456;&#27604;&#65292;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#12289;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31532;&#19977;&#26041;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;NLP&#31995;&#32479;&#26500;&#25104;&#23041;&#32961;&#65292;&#21487;&#33021;&#38544;&#34255;&#21518;&#38376;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#21253;&#25324;&#25554;&#20837;&#26631;&#35760;&#25110;&#21477;&#23376;&#37325;&#36848;&#31561;&#27745;&#26579;&#25968;&#25454;&#26679;&#26412;&#65292;&#36825;&#35201;&#20040;&#25913;&#21464;&#20102;&#21407;&#22987;&#25991;&#26412;&#30340;&#35821;&#20041;&#65292;&#35201;&#20040;&#21487;&#20197;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#25105;&#20204;&#19982;&#20197;&#24448;&#24037;&#20316;&#30340;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#65292;&#25105;&#20204;&#20351;&#29992;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#36890;&#36807;&#35774;&#35745;&#24182;&#24212;&#29992;&#22522;&#20110;&#35789;&#24615;&#30340;&#35268;&#21017;&#26469;&#36873;&#25321;&#36825;&#20123;&#26631;&#35760;&#65292;&#25105;&#20204;&#22312;SST-2&#21644;AG&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22312;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#35770;&#25991;&#20013;&#30340;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/alekseevskaia/OrderBkd&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06716</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06716
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23427;&#20204;&#25658;&#24102;&#30528;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#27169;&#24335;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#34920;&#31034;&#23398;&#20064;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#20869;&#22312;&#30340;&#21160;&#24577;&#24615;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DGNNs&#23637;&#31034;&#20102;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;DGIB&#65289;&#26694;&#26550;&#26469;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#20511;&#21161;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#21407;&#29702;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#26399;&#26395;&#30340;&#26368;&#20248;&#34920;&#31034;&#24212;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#65288;MSC&#65289;&#26465;&#20214;&#12290;&#20026;&#20102;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#21644;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;DGIB&#36845;&#20195;&#22320;&#24341;&#23548;&#21644;&#25913;&#36827;&#36890;&#36807;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#12290;&#20026;&#20102;&#28385;&#36275;MSC&#26465;&#20214;&#65292;&#25105;&#20204;&#23558;&#25972;&#20307;IB&#30446;&#26631;&#20998;&#35299;&#20026;DGIB$_{MS}$&#21644;DGIB$_C$&#65292;&#20854;&#20013;DGIB$_{MS}$&#36890;&#36947;&#30340;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05713</link><description>&lt;p&gt;
&#26126;&#26126;&#23601;&#22312;&#30524;&#21069;&#65306;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#36827;&#34892;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#21095;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#30340;&#20020;&#24202;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#23637;&#31034;&#30340;&#20559;&#35265;&#30340;&#37327;&#21270;&#65292;&#20294;&#38024;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#20197;&#21450;&#20854;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38024;&#23545;&#20154;&#21475;&#32479;&#35745;&#23398;&#26631;&#31614;&#30340;&#27602;&#21270;&#25915;&#20987;&#21487;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#23545;&#34987;&#20302;&#20272;&#32676;&#20307;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#21644;&#20154;&#21475;&#32676;&#20307;&#65288;&#22914;&#24615;&#21035;&#12289;&#24180;&#40836;&#20197;&#21450;&#20854;&#20132;&#21449;&#23376;&#32676;&#65289;&#19978;&#34920;&#26126;&#65292;&#32676;&#20307;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#19982;&#20854;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#24449;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05125</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#19982;LLMs
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Clinical Trial Patient Matching with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#25512;&#20986;&#26032;&#33647;&#30340;&#20851;&#38190;&#38590;&#39064;&#12290;&#30446;&#21069;&#65292;&#35782;&#21035;&#31526;&#21512;&#35797;&#39564;&#20837;&#36873;&#26631;&#20934;&#30340;&#24739;&#32773;&#26159;&#39640;&#24230;&#25163;&#21160;&#30340;&#65292;&#27599;&#20301;&#24739;&#32773;&#38656;&#33457;&#36153;&#38271;&#36798;1&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#31579;&#36873;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#19968;&#20010;&#24739;&#32773;&#30340;&#30149;&#21490;&#20316;&#20026;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#26102;&#65292;&#35780;&#20272;&#35813;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#19968;&#32452;&#21253;&#21547;&#26631;&#20934;&#65288;&#20063;&#20197;&#33258;&#30001;&#25991;&#26412;&#24418;&#24335;&#25351;&#23450;&#65289;&#12290;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#31995;&#32479;&#22312;n2c2 2018&#38431;&#21015;&#36873;&#25321;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#65292;&#35813;&#31574;&#30053;&#19982;&#29616;&#29366;&#30456;&#27604;&#21487;&#20197;&#23558;&#24739;&#32773;&#21305;&#37197;&#26102;&#38388;&#21644;&#25104;&#26412;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26816;&#32034;&#27969;&#31243;&#65292;&#20943;&#23569;&#20102;&#21305;&#37197;&#28040;&#38500;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PVAW&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#24182;&#22312;&#20998;&#26512;RSV&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27169;&#22411;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.01987</link><description>&lt;p&gt;
&#22312;&#32447;&#36716;&#31227;&#23398;&#20064;&#29992;&#20110;RSV&#30149;&#20363;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Transfer Learning for RSV Case Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01987
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PVAW&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#24182;&#22312;&#20998;&#26512;RSV&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27169;&#22411;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#26102;&#65292;&#20250;&#38754;&#20020;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#26631;&#27880;&#20449;&#24687;&#21294;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#39044;&#27979;&#20307;&#31215;&#33258;&#36866;&#24212;&#21152;&#26435;&#65288;PVAW&#65289;&#12290;PVAW&#22312;&#25972;&#21512;&#27169;&#22411;&#20013;&#21019;&#36896;&#24615;&#22320;&#23454;&#29616;&#20102;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#65292;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#36129;&#29486;&#24230;&#33258;&#21160;&#35843;&#25972;&#26435;&#37325;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21305;&#20857;&#22561;&#22823;&#23398;&#21307;&#23398;&#20013;&#24515;&#25910;&#38598;&#30340;&#22810;&#20010;&#23395;&#33410;&#30340;&#21628;&#21560;&#36947;&#21512;&#32990;&#30149;&#27602;&#65288;RSV&#65289;&#25968;&#25454;&#19978;&#24212;&#29992;PVAW&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#31361;&#20986;&#20102;&#22312;&#32447;&#36716;&#31227;&#23398;&#20064;&#22312;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become a pivotal technique in machine learning, renowned for its effectiveness in various real-world applications. However, a significant challenge arises when applying this approach to sequential epidemiological data, often characterized by a scarcity of labeled information. To address this challenge, we introduce Predictive Volume-Adaptive Weighting (PVAW), a novel online multi-source transfer learning method. PVAW innovatively implements a dynamic weighting mechanism within an ensemble model, allowing for the automatic adjustment of weights based on the relevance and contribution of each source and target model. We demonstrate the effectiveness of PVAW through its application in analyzing Respiratory Syncytial Virus (RSV) data, collected over multiple seasons at the University of Pittsburgh Medical Center. Our method showcases significant improvements in model performance over existing baselines, highlighting the potential of online transfer learning in handlin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17548</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#65306;&#20174;&#39046;&#20808;&#25351;&#26631;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29420;&#31435;&#20110;&#36890;&#36947;&#30340;&#26041;&#27861;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#20294;&#23427;&#20204;&#38169;&#36807;&#20102;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#23616;&#37096;&#24179;&#31283;&#30340;&#39046;&#20808;-&#28382;&#21518;&#20851;&#31995;&#65292;&#21363;&#19968;&#20123;&#28382;&#21518;&#21464;&#37327;&#22312;&#30701;&#26102;&#38388;&#20869;&#21487;&#33021;&#36981;&#24490;&#39046;&#20808;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#31181;&#36890;&#36947;&#30456;&#20851;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#39046;&#20808;&#25351;&#26631;&#25552;&#20379;&#20102;&#20808;&#36827;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#28382;&#21518;&#21464;&#37327;&#30340;&#39044;&#27979;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFT&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#39640;&#25928;&#22320;&#20272;&#35745;&#39046;&#20808;&#25351;&#26631;&#21450;&#20854;&#39046;&#20808;&#27493;&#39588;&#65292;&#28982;&#21518;&#24039;&#22937;&#22320;&#20801;&#35768;&#28382;&#21518;&#21464;&#37327;&#21033;&#29992;&#26469;&#33258;&#39046;&#20808;&#25351;&#26631;&#30340;&#20808;&#36827;&#20449;&#24687;&#12290;LIFT&#20316;&#20026;&#19968;&#20010;&#25554;&#20214;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#21327;&#20316;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LIFT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#22238;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#21644;&#36127;&#38754;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#23398;&#20250;&#22914;&#20309;&#22238;&#36991;&#36127;&#38754;&#29305;&#24449;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2401.17390</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Customizing Language Model Responses with Contrastive In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#22238;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#21644;&#36127;&#38754;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#23398;&#20250;&#22914;&#20309;&#22238;&#36991;&#36127;&#38754;&#29305;&#24449;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#19982;&#25105;&#20204;&#30340;&#24847;&#22270;&#23545;&#40784;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#25105;&#20204;&#24076;&#26395;&#29983;&#25104;&#20248;&#20110;&#20854;&#20182;&#20869;&#23481;&#30340;&#20869;&#23481;&#65292;&#25110;&#32773;&#24403;&#25105;&#20204;&#24076;&#26395;LLMs&#20197;&#19968;&#31181;&#38590;&#20197;&#25551;&#36848;&#30340;&#39118;&#26684;&#25110;&#35821;&#27668;&#36827;&#34892;&#22238;&#24212;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#26356;&#22909;&#22320;&#25551;&#36848;&#25105;&#20204;&#30340;&#24847;&#22270;&#30340;&#26041;&#27861;&#12290;&#36825;&#28041;&#21450;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#26469;&#35828;&#26126;&#30495;&#23454;&#30340;&#24847;&#22270;&#65292;&#20197;&#21450;&#36127;&#38754;&#31034;&#20363;&#26469;&#23637;&#31034;&#25105;&#20204;&#24076;&#26395;LLMs&#36991;&#20813;&#30340;&#29305;&#24449;&#12290;&#36127;&#38754;&#31034;&#20363;&#21487;&#20197;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#26816;&#32034;&#65292;&#30001;&#20154;&#24037;&#32534;&#20889;&#65292;&#25110;&#30001;LLMs&#33258;&#21160;&#29983;&#25104;&#12290;&#22312;&#29983;&#25104;&#31572;&#26696;&#20043;&#21069;&#65292;&#25105;&#20204;&#35201;&#27714;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#31034;&#20363;&#65292;&#20197;&#25945;&#20250;&#33258;&#24049;&#36991;&#20813;&#20160;&#20040;&#12290;&#36825;&#20010;&#25512;&#29702;&#27493;&#39588;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#19982;&#29992;&#25143;&#38656;&#27714;&#30456;&#20851;&#30340;&#36866;&#24403;&#34920;&#36798;&#65292;&#24182;&#24341;&#23548;&#20854;&#29983;&#25104;&#26356;&#22909;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#23618;&#20132;&#21449;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;MLCA-AVSR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#38899;&#39057;/&#35270;&#35273;&#32534;&#30721;&#22120;&#19978;&#34701;&#21512;&#27169;&#24577;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.03424</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23618;&#20132;&#21449;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;MLCA-AVSR&#65289;
&lt;/p&gt;
&lt;p&gt;
MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#23618;&#20132;&#21449;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;MLCA-AVSR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#38899;&#39057;/&#35270;&#35273;&#32534;&#30721;&#22120;&#19978;&#34701;&#21512;&#27169;&#24577;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#26126;&#26174;&#36864;&#21270;&#65292;&#32780;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;AVSR&#65289;&#31995;&#32479;&#26088;&#22312;&#29992;&#25239;&#22122;&#38899;&#30340;&#35270;&#35273;&#32447;&#32034;&#34917;&#20805;&#38899;&#39057;&#27969;&#65292;&#24182;&#25552;&#39640;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#34701;&#21512;&#22909;&#23398;&#20064;&#30340;&#27169;&#24577;&#29305;&#24449;&#65292;&#22914;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#27169;&#24577;&#29305;&#24449;&#23398;&#20064;&#26399;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23618;&#20132;&#21449;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;AVSR&#65288;MLCA-AVSR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#38899;&#39057;/&#35270;&#35273;&#32534;&#30721;&#22120;&#19978;&#34701;&#21512;&#23427;&#20204;&#26469;&#20419;&#36827;&#27599;&#20010;&#27169;&#24577;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#23545;MISP2022-AVSR&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;Eval&#38598;&#19978;&#23454;&#29616;&#20102;30.57%&#30340;&#25340;&#25509;&#26368;&#23567;&#32622;&#25442;&#23383;&#31526;&#35823;&#24046;&#29575;&#65288;cpCER&#65289;&#65292;&#30456;&#23545;&#20110;&#25105;&#20204;&#30340;p&#21462;&#24471;&#20102;&#39640;&#36798;3.17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03424v2 Announce Type: replace-cross  Abstract: While automatic speech recognition (ASR) systems degrade significantly in noisy environments, audio-visual speech recognition (AVSR) systems aim to complement the audio stream with noise-invariant visual cues and improve the system's robustness. However, current studies mainly focus on fusing the well-learned modality features, like the output of modality-specific encoders, without considering the contextual relationship during the modality feature learning. In this study, we propose a multi-layer cross-attention fusion based AVSR (MLCA-AVSR) approach that promotes representation learning of each modality by fusing them at different levels of audio/visual encoders. Experimental results on the MISP2022-AVSR Challenge dataset show the efficacy of our proposed system, achieving a concatenated minimum permutation character error rate (cpCER) of 30.57% on the Eval set and yielding up to 3.17% relative improvement compared with our p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;AI&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#39044;&#27979;&#20840;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.12462</link><description>&lt;p&gt;
&#23454;&#29616;&#31471;&#21040;&#31471;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards an end-to-end artificial intelligence driven global weather forecasting system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;AI&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#39044;&#27979;&#20840;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#23545;&#31185;&#23398;&#21644;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20110;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;AI&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#20381;&#36182;&#20110;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#31995;&#32479;&#30340;&#20998;&#26512;&#25110;&#20877;&#20998;&#26512;&#20135;&#21697;&#20316;&#20026;&#39044;&#27979;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#21021;&#22987;&#29366;&#24577;&#36890;&#24120;&#30001;&#20256;&#32479;&#25968;&#25454;&#21516;&#21270;&#32452;&#20214;&#29983;&#25104;&#65292;&#36825;&#26159;&#35745;&#31639;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#25968;&#25454;&#21516;&#21270;&#27169;&#22411;&#65292;&#21363;Adas&#65292;&#29992;&#20110;&#20840;&#29699;&#22825;&#27668;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;Adas&#19982;&#20808;&#36827;&#30340;&#22522;&#20110;AI&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65288;&#21363;FengWu&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#22522;&#20110;AI&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65306;FengWu-Adas&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Adas&#33021;&#22815;&#21516;&#21270;&#31232;&#30095;&#30340;&#20840;&#29699;&#35266;&#27979;&#25968;&#25454;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#31283;&#23450;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12462v2 Announce Type: replace-cross  Abstract: The weather forecasting system is important for science and society, and significant achievements have been made in applying artificial intelligence (AI) to medium-range weather forecasting. However, existing AI-based weather forecasting models rely on analysis or reanalysis products from the traditional numerical weather prediction (NWP) systems as initial conditions for making predictions. Initial states are typically generated by traditional data assimilation component, which is computational expensive and time-consuming. Here we present an AI-based data assimilation model, i.e., Adas, for global weather variables. And we combine Adas with the advanced AI-based weather forecasting model (i.e., FengWu) to construct the first end-to-end AI-based global weather forecasting system: FengWu-Adas. We demonstrate that Adas can assimilate sparse global observations to produce high-quality analysis, enabling the system operate stably 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#20197;&#30740;&#31350;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#20250;&#37319;&#21462;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2311.09702</link><description>&lt;p&gt;
&#25512;&#29702;&#38142;&#19978;&#30340;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#65306;&#27169;&#22411;&#22312;&#27809;&#26377;&#24187;&#35273;&#30340;&#24773;&#20917;&#19979;&#33021;&#36208;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#20197;&#30740;&#31350;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#20250;&#37319;&#21462;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#22312;&#20247;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#23384;&#22312;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30001;&#35821;&#20041;&#20851;&#32852;&#24341;&#36215;&#30340;&#24187;&#35273;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#25552;&#31034;&#20013;&#26159;&#21542;&#20250;&#22240;&#20026;&#26576;&#20123;&#20851;&#38190;&#23383;/&#23454;&#20307;&#20559;&#35265;&#32780;&#37319;&#21462;&#25463;&#24452;&#65292;&#32780;&#19981;&#26159;&#36981;&#24490;&#27491;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EureQA&#30340;&#26032;&#22411;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20174;LLMs&#20250;&#20197;&#32477;&#23545;&#30830;&#23450;&#24615;&#27491;&#30830;&#22238;&#31572;&#30340;&#38382;&#39064;&#24320;&#22987;&#65292;&#28982;&#21518;&#36882;&#24402;&#22320;&#29992;&#35777;&#25454;&#21477;&#23376;&#36974;&#34109;&#37325;&#35201;&#23454;&#20307;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#25214;&#21040;&#26681;&#25454;&#35777;&#25454;&#38142;&#26465;&#36974;&#34109;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09702v2 Announce Type: replace-cross  Abstract: Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path. To quantify this phenomenon, we propose a novel probing method and benchmark called EureQA. We start from questions that LLMs will answer correctly with utmost certainty, and mask the important entity with evidence sentence recursively, asking models to find masked entities according to a chain of evidence before answering the question.   During the construction of the evidence, we purposefully replace semantic clues (entities) that may lead to the correct answer with distractor
&lt;/p&gt;</description></item><item><title>&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#31867;&#20027;&#20307;&#20855;&#26377;&#31867;&#20284;&#30340;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.07092</link><description>&lt;p&gt;
&#25581;&#31034;&#30495;&#30456;&#65306;&#27450;&#39575;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
To Tell The Truth: Language of Deception and Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07092
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#31867;&#20027;&#20307;&#20855;&#26377;&#31867;&#20284;&#30340;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07092v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-cross &#25688;&#35201;&#65306;&#22522;&#20110;&#25991;&#26412;&#30340;&#38169;&#35823;&#20449;&#24687;&#28183;&#36879;&#21040;&#22312;&#32447;&#35752;&#35770;&#20013;&#65292;&#28982;&#32780;&#20154;&#20204;&#33021;&#22815;&#20174;&#36825;&#31181;&#27450;&#39575;&#24615;&#25991;&#26412;&#20869;&#23481;&#20013;&#36776;&#21035;&#30495;&#30456;&#30340;&#35777;&#25454;&#21364;&#24456;&#23569;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#26723;&#26032;&#39062;&#30340;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#65292;&#20854;&#20013;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#30456;&#20114;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#30446;&#26631;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#23548;&#33268;&#35854;&#35328;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27450;&#39575;&#35821;&#35328;&#28508;&#22312;&#21487;&#39564;&#35777;&#35821;&#35328;&#32447;&#32034;&#22312;&#23458;&#35266;&#30495;&#30456;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#36825;&#26159;&#20197;&#24448;&#22522;&#20110;&#25991;&#26412;&#30340;&#27450;&#39575;&#25968;&#25454;&#38598;&#20013;&#32570;&#23569;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#31867;&#25506;&#27979;&#22120;&#65288;&#31639;&#27861;&#65289;&#65292;&#20854;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#19982;&#20154;&#31867;&#20027;&#20307;&#30456;&#20284;&#65292;&#21363;&#20351;&#21069;&#32773;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#32780;&#21518;&#32773;&#21017;&#36890;&#36807;&#23436;&#20840;&#35775;&#38382;&#25152;&#26377;&#28508;&#22312;&#32447;&#32034;&#28304;&#65288;&#35821;&#35328;&#21644;&#35270;&#21548;&#65289;&#36827;&#34892;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24314;&#31435;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#65292;&#37319;&#29992;&#29942;&#39048;&#26694;&#26550;&#26469;&#23398;&#20064;&#21487;&#36776;&#21035;&#30340;&#32447;&#32034;&#65292;&#20197;&#30830;&#23450;&#30495;&#30456;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07092v2 Announce Type: replace-cross  Abstract: Text-based misinformation permeates online discourses, yet evidence of people's ability to discern truth from such deceptive textual content is scarce. We analyze a novel TV game show data where conversations in a high-stake environment between individuals with conflicting objectives result in lies. We investigate the manifestation of potentially verifiable language cues of deception in the presence of objective truth, a distinguishing feature absent in previous text-based deception datasets. We show that there exists a class of detectors (algorithms) that have similar truth detection performance compared to human subjects, even when the former accesses only the language cues while the latter engages in conversations with complete access to all potential sources of cues (language and audio-visual). Our model, built on a large language model, employs a bottleneck framework to learn discernible cues to determine truth, an act of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#27169;&#22411;Bridge&#65292;&#32467;&#21512;&#19987;&#23478;&#30340;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#65292;&#25104;&#21151;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2310.10648</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#27169;&#22411;&#24357;&#34917;&#26032;&#25163;&#19982;&#19987;&#23478;&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#20197;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#27169;&#22411;Bridge&#65292;&#32467;&#21512;&#19987;&#23478;&#30340;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#65292;&#25104;&#21151;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#36741;&#23548;&#35268;&#27169;&#21270;&#20173;&#28982;&#26159;&#25945;&#32946;&#20013;&#30340;&#19968;&#39033;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#38656;&#27714;&#22686;&#38271;&#65292;&#35768;&#22810;&#24179;&#21488;&#32856;&#29992;&#26032;&#25163;&#23548;&#24072;&#65292;&#20182;&#20204;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#25945;&#32946;&#24037;&#20316;&#32773;&#19981;&#21516;&#65292;&#38590;&#20197;&#35299;&#20915;&#23398;&#29983;&#30340;&#38169;&#35823;&#65292;&#22240;&#27492;&#26080;&#27861;&#25235;&#20303;&#20027;&#35201;&#30340;&#23398;&#20064;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#20043;&#38388;&#30693;&#35782;&#24046;&#36317;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;Bridge&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#23558;&#19987;&#23478;&#30340;&#28508;&#22312;&#24605;&#32500;&#36807;&#31243;&#36716;&#21270;&#20026;&#32416;&#27491;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36825;&#28041;&#21450;&#19987;&#23478;&#35782;&#21035;(A)&#23398;&#29983;&#30340;&#38169;&#35823;&#12289;(B)&#32416;&#27491;&#31574;&#30053;&#21644;(C)&#29983;&#25104;&#22238;&#24212;&#20043;&#21069;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;700&#20010;&#30495;&#23454;&#36741;&#23548;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26631;&#27880;&#20102;&#20182;&#20204;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;&#24182;&#21457;&#29616;&#19987;&#23478;&#30340;&#20915;&#31574;&#27169;&#22411;&#23545;LLMs&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65306;&#22238;&#24212;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10648v2 Announce Type: replace-cross  Abstract: Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert's latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student's error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert's decision-making model is critical for LLMs to close the gap: responses f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38271;&#23485;&#27604;&#25935;&#24863;&#30340;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#22120;ARS-DETR&#65292;&#37319;&#29992;&#39640;&#31934;&#24230;&#24230;&#37327;AP$_{75}$&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#35282;&#24230;&#20998;&#31867;&#26041;&#27861;&#21644;&#26059;&#36716;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2303.04989</link><description>&lt;p&gt;
ARS-DETR: &#38754;&#21521;&#38271;&#23485;&#27604;&#25935;&#24863;&#30340;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#19982;Transformer
&lt;/p&gt;
&lt;p&gt;
ARS-DETR: Aspect Ratio Sensitive Oriented Object Detection with Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.04989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38271;&#23485;&#27604;&#25935;&#24863;&#30340;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#22120;ARS-DETR&#65292;&#37319;&#29992;&#39640;&#31934;&#24230;&#24230;&#37327;AP$_{75}$&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#35282;&#24230;&#20998;&#31867;&#26041;&#27861;&#21644;&#26059;&#36716;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#24230;&#37327;AP$_{50}$&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;AP$_{50}$&#22312;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#20013;&#26412;&#36136;&#19978;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#23545;&#35282;&#24230;&#20559;&#24046;&#20855;&#26377;&#36739;&#22823;&#30340;&#23481;&#24525;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#20351;&#29992;&#39640;&#31934;&#24230;&#24230;&#37327;&#65292;&#20363;&#22914;AP$_{75}$&#65292;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;Transformer&#30340;&#38754;&#21521;&#38271;&#23485;&#27604;&#25935;&#24863;&#30340;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#31216;&#20026;ARS-DETR&#65292;&#22312;&#39640;&#31934;&#24230;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;&#38754;&#21521;&#38271;&#23485;&#27604;&#30340;&#22278;&#28369;&#26631;&#31614;&#65288;AR-CSL&#65289;&#65292;&#20197;&#26356;&#21512;&#29702;&#22320;&#24179;&#28369;&#35282;&#24230;&#26631;&#31614;&#65292;&#24182;&#20002;&#24323;&#20808;&#21069;&#24037;&#20316;&#24341;&#20837;&#30340;&#36229;&#21442;&#25968;&#65288;&#20363;&#22914;CSL&#65289;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26059;&#36716;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#26681;&#25454;&#30456;&#24212;&#35282;&#24230;&#26059;&#36716;&#37319;&#26679;&#28857;&#65292;&#24182;&#28040;&#38500;&#20043;&#21069;&#24037;&#20316;&#24341;&#20837;&#30340;&#26679;&#26412;&#28857;&#19982;&#29305;&#24449;&#28857;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.04989v2 Announce Type: replace-cross  Abstract: Existing oriented object detection methods commonly use metric AP$_{50}$ to measure the performance of the model. We argue that AP$_{50}$ is inherently unsuitable for oriented object detection due to its large tolerance in angle deviation. Therefore, we advocate using high-precision metric, e.g. AP$_{75}$, to measure the performance of models. In this paper, we propose an Aspect Ratio Sensitive Oriented Object Detector with Transformer, termed ARS-DETR, which exhibits a competitive performance in high-precision oriented object detection. Specifically, a new angle classification method, calling Aspect Ratio aware Circle Smooth Label (AR-CSL), is proposed to smooth the angle label in a more reasonable way and discard the hyperparameter that introduced by previous work (e.g. CSL). Then, a rotated deformable attention module is designed to rotate the sampling points with the corresponding angles and eliminate the misalignment betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.13034</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39640;&#25928;&#30340;&#31232;&#30095;&#26356;&#26032;&#65292;&#22312;&#24179;&#34913;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#20248;&#21270;&#25311;&#21512;&#25152;&#26377;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#30340;Stackelberg&#21338;&#24328;&#26694;&#26550;&#65292;&#22312;&#26080;&#20154;&#26426;Metaverse&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#21452;&#32990;&#32974;&#36801;&#31227;&#65292;&#20197;&#25552;&#20379;&#26080;&#32541;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.09680</link><description>&lt;p&gt;
&#23567;&#22411;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26080;&#20154;&#26426;Metaverse&#20013;&#30340;&#21452;&#32990;&#32974;&#36801;&#31227;&#65306;&#19968;&#31181;&#22810;&#39046;&#23548;&#32773;&#22810;&#20174;&#23646;&#32773;Stackelberg&#21338;&#24328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A Multi-Leader Multi-Follower Stackelberg Game Approach. (arXiv:2401.09680v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#30340;Stackelberg&#21338;&#24328;&#26694;&#26550;&#65292;&#22312;&#26080;&#20154;&#26426;Metaverse&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#21452;&#32990;&#32974;&#36801;&#31227;&#65292;&#20197;&#25552;&#20379;&#26080;&#32541;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#19982;Metaverse&#30340;&#21327;&#21516;&#20316;&#29992;&#27491;&#22312;&#20652;&#29983;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#31216;&#20026;&#26080;&#20154;&#26426;Metaverse&#65292;&#23427;&#21019;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#34701;&#21512;&#20102;&#29289;&#29702;&#21644;&#34394;&#25311;&#31354;&#38388;&#65292;&#25913;&#21464;&#20102;&#26080;&#20154;&#26426;&#30340;&#20132;&#20114;&#21644;&#34394;&#25311;&#25506;&#32034;&#12290;&#26080;&#20154;&#26426;&#21452;&#32990;&#32974;&#65288;UTs&#65289;&#20316;&#20026;&#26080;&#20154;&#26426;&#30340;&#25968;&#23383;&#23402;&#29983;&#21697;&#65292;&#36890;&#36807;&#20351;&#20854;&#26356;&#20855;&#27785;&#28024;&#24863;&#12289;&#30495;&#23454;&#24863;&#21644;&#20449;&#24687;&#20016;&#23500;&#24615;&#65292;&#38761;&#26032;&#26080;&#20154;&#26426;&#24212;&#29992;&#12290;UTs&#37096;&#32626;&#22312;&#22320;&#38754;&#22522;&#31449;&#65288;&#20363;&#22914;&#36947;&#36335;&#36793;&#32536;&#21333;&#20803;&#65288;RSUs&#65289;&#65289;&#19978;&#65292;&#24182;&#36890;&#36807;&#20026;&#26080;&#20154;&#26426;Metaverse&#29992;&#25143;&#65288;UMUs&#65289;&#25552;&#20379;Metaverse&#26381;&#21153;&#12290;&#30001;&#20110;&#26080;&#20154;&#26426;&#30340;&#21160;&#24577;&#31227;&#21160;&#24615;&#21644;RSUs&#30340;&#26377;&#38480;&#36890;&#20449;&#35206;&#30422;&#33539;&#22260;&#65292;&#36827;&#34892;&#23454;&#26102;&#30340;UT&#36801;&#31227;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;UMUs&#30340;&#26080;&#32541;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;RSUs&#24182;&#20248;&#21270;&#25152;&#38656;&#24102;&#23485;&#23545;&#20110;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;UT&#36801;&#31227;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#21098;&#25216;&#26415;&#30340;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;Stackelberg&#21338;&#24328;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;UT&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
The synergy between Unmanned Aerial Vehicles (UAVs) and metaverses is giving rise to an emerging paradigm named UAV metaverses, which create a unified ecosystem that blends physical and virtual spaces, transforming drone interaction and virtual exploration. UAV Twins (UTs), as the digital twins of UAVs that revolutionize UAV applications by making them more immersive, realistic, and informative, are deployed and updated on ground base stations, e.g., RoadSide Units (RSUs), to offer metaverse services for UAV Metaverse Users (UMUs). Due to the dynamic mobility of UAVs and limited communication coverages of RSUs, it is essential to perform real-time UT migration to ensure seamless immersive experiences for UMUs. However, selecting appropriate RSUs and optimizing the required bandwidth is challenging for achieving reliable and efficient UT migration. To address the challenges, we propose a tiny machine learning-based Stackelberg game framework based on pruning techniques for efficient UT 
&lt;/p&gt;</description></item><item><title>DiffSHEG&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23454;&#26102;&#35821;&#38899;&#39537;&#21160;&#30340;&#25972;&#20307;&#19977;&#32500;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#29983;&#25104;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20219;&#24847;&#38271;&#24207;&#21015;&#29983;&#25104;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.04747</link><description>&lt;p&gt;
DiffSHEG:&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23454;&#26102;&#35821;&#38899;&#39537;&#21160;&#30340;&#25972;&#20307;&#19977;&#32500;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation. (arXiv:2401.04747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04747
&lt;/p&gt;
&lt;p&gt;
DiffSHEG&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23454;&#26102;&#35821;&#38899;&#39537;&#21160;&#30340;&#25972;&#20307;&#19977;&#32500;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#29983;&#25104;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20219;&#24847;&#38271;&#24207;&#21015;&#29983;&#25104;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DiffSHEG&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#35821;&#38899;&#39537;&#21160;&#30340;&#25972;&#20307;&#19977;&#32500;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#38271;&#24230;&#30340;&#35821;&#38899;&#36755;&#20837;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32852;&#21512;&#29983;&#25104;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#65292;&#32780;&#19981;&#26159;&#20998;&#21035;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#32852;&#21160;&#29983;&#25104;&#21464;&#25442;&#22120;&#65292;&#20351;&#24471;&#20174;&#34920;&#24773;&#21040;&#25163;&#21183;&#30340;&#21333;&#21521;&#20449;&#24687;&#27969;&#26356;&#21152;&#39034;&#30021;&#65292;&#26377;&#21033;&#20110;&#21305;&#37197;&#32852;&#21512;&#30340;&#34920;&#24773;&#21644;&#25163;&#21183;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20219;&#24847;&#38271;&#24207;&#21015;&#29983;&#25104;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#30001;&#35821;&#38899;&#39537;&#21160;&#30340;&#39640;&#36136;&#37327;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#30740;&#31350;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting-based sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.03955</link><description>&lt;p&gt;
&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs): &#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#24378;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#30340;&#24555;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015; (TS) &#20013;&#38754;&#20020;&#30528;&#22810;&#26679;&#24615;&#21644;&#20844;&#24320;&#39044;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#21508;&#31181;&#36866;&#24212;&#30340;&#36235;&#21183;&#36880;&#28176;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#65292;&#20986;&#22855;&#22320;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38750;&#24120;&#32531;&#24930;&#19988;&#24222;&#22823;&#65288;&#22823;&#32422;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23618;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTM)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423; TSMixer &#32467;&#26500;&#30340;&#26174;&#33879;&#23567;&#22411;&#27169;&#22411;&#12290;TTM &#26159;&#39318;&#20010;&#25104;&#21151;&#24320;&#21457;&#30340;&#24494;&#22411;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#8804;100&#19975;&#20010;&#21442;&#25968;&#65289;&#65292;&#19987;&#38376;&#22312;&#20844;&#24320;TS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65288;&#20165;&#38656;4-8&#23567;&#26102;&#65289;&#65292;&#20855;&#26377;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#22312;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#22270;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#19978;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2312.10401</link><description>&lt;p&gt;
&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective. (arXiv:2312.10401v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#22312;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#22270;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#19978;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#22270;&#20013;&#25429;&#25417;&#19981;&#21464;&#20449;&#24687;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#25506;&#32034;&#22270;&#30340;&#32467;&#26500;&#29702;&#35770;&#65292;&#20174;&#32780;&#22686;&#21152;&#19981;&#21464;&#20449;&#24687;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22270;&#27169;&#22411;&#26397;&#21521;&#35299;&#37322;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#38169;&#35823;&#23398;&#20064;&#65292;&#22240;&#27492;&#23398;&#20064;&#21040;&#30340;&#22122;&#22768;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#24178;&#25200;&#20102;&#22270;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#25506;&#32034;&#22270;&#30340;&#20869;&#22312;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#19978;&#36848;&#36335;&#24452;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#38416;&#26126;&#32500;&#24230;&#29702;&#35770;&#23545;&#24615;&#33021;&#25913;&#36827;&#30340;&#20869;&#22312;&#26426;&#21046;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning is a general learning paradigm excelling at capturing invariant information from diverse perturbations in graphs. Recent works focus on exploring the structural rationale from graphs, thereby increasing the discriminability of the invariant information. However, such methods may incur in the mis-learning of graph models towards the interpretability of graphs, and thus the learned noisy and task-agnostic information interferes with the prediction of graphs. To this end, with the purpose of exploring the intrinsic rationale of graphs, we accordingly propose to capture the dimensional rationale from graphs, which has not received sufficient attention in the literature. The conducted exploratory experiments attest to the feasibility of the aforementioned roadmap. To elucidate the innate mechanism behind the performance improvement arising from the dimensional rationale, we rethink the dimensional rationale in graph contrastive learning from a causal perspective a
&lt;/p&gt;</description></item><item><title>QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08041</link><description>&lt;p&gt;
QLLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#39640;&#25928;&#20302;&#20301;&#23485;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08041
&lt;/p&gt;
&lt;p&gt;
QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#25152;&#38656;&#36164;&#28304;&#36807;&#22823;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;Quantization-Aware Training&#65292;QAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;Post-Training Quantization&#65292;PTQ&#65289;&#25104;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26356;&#23454;&#38469;&#30340;&#26041;&#27861;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#29305;&#23450;&#36890;&#36947;&#20013;&#30340;&#28608;&#27963;&#31163;&#32676;&#20540;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#21518;&#35757;&#32451;&#37327;&#21270;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QLLM&#65292;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;QLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#36890;&#36947;&#25286;&#20998;&#21644;&#36890;&#36947;&#32452;&#35013;&#65292;&#22312;&#20445;&#35777;&#20302;&#20301;&#23485;&#30340;&#24773;&#20917;&#19979;&#23558;&#31163;&#32676;&#36890;&#36947;&#20998;&#35299;&#25104;&#22810;&#20010;&#23376;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25552;&#31034;&#24037;&#31243;&#23545;ChatGPT&#22312;&#26080;&#30417;&#30563;&#23454;&#20307;&#35299;&#26512;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#23454;&#20307;&#35299;&#26512;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.06174</link><description>&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;ChatGPT&#22312;&#26080;&#30417;&#30563;&#23454;&#20307;&#35299;&#26512;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#26159;&#22914;&#20309;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?. (arXiv:2310.06174v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25552;&#31034;&#24037;&#31243;&#23545;ChatGPT&#22312;&#26080;&#30417;&#30563;&#23454;&#20307;&#35299;&#26512;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#23454;&#20307;&#35299;&#26512;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#35299;&#26512;&#65288;ER&#65289;&#26159;&#19968;&#31181;&#21322;&#33258;&#21160;&#30830;&#23450;&#20004;&#20010;&#23454;&#20307;&#26159;&#21542;&#25351;&#21521;&#30456;&#21516;&#22522;&#30784;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#30005;&#23376;&#21830;&#21153;&#12290;&#20256;&#32479;&#30340;ER&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#30456;&#24403;&#22810;&#30340;&#25163;&#21160;&#19987;&#19994;&#30693;&#35782;&#65292;&#21253;&#25324;&#29305;&#24449;&#24037;&#31243;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#30340;&#35782;&#21035;&#21644;&#31574;&#21010;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#25216;&#26415;&#39640;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#26377;&#26426;&#20250;&#20351;ER&#26356;&#21152;&#26080;&#32541;&#21644;&#39046;&#22495;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;LLMs&#21487;&#33021;&#23384;&#22312;&#39118;&#38505;&#65292;&#20854;&#36755;&#20986;&#36136;&#37327;&#21487;&#33021;&#21462;&#20915;&#20110;&#25152;&#35859;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#20351;&#29992;&#20687;ChatGPT&#36825;&#26679;&#30340;LLMs&#35299;&#20915;ER&#30340;&#19981;&#21516;&#25552;&#31034;&#26041;&#27861;&#30340;&#24433;&#21709;&#30340;&#31995;&#32479;&#23454;&#39564;&#30740;&#31350;&#36824;&#32570;&#20047;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#36827;&#34892;&#36825;&#26679;&#19968;&#39033;&#30740;&#31350;&#12290;&#23613;&#31649;&#36825;&#21482;&#26159;&#21021;&#27493;&#24615;&#36136;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#23454;&#20307;&#35299;&#26512;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Resolution (ER) is the problem of semi-automatically determining when two entities refer to the same underlying entity, with applications ranging from healthcare to e-commerce. Traditional ER solutions required considerable manual expertise, including feature engineering, as well as identification and curation of training data. In many instances, such techniques are highly dependent on the domain. With recent advent in large language models (LLMs), there is an opportunity to make ER much more seamless and domain-independent. However, it is also well known that LLMs can pose risks, and that the quality of their outputs can depend on so-called prompt engineering. Unfortunately, a systematic experimental study on the effects of different prompting methods for addressing ER, using LLMs like ChatGPT, has been lacking thus far. This paper aims to address this gap by conducting such a study. Although preliminary in nature, our results show that prompting can significantly affect the qu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22914;&#20309;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10182</link><description>&lt;p&gt;
&#38899;&#20048;&#20135;&#21697;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Positive and Risky Message Assessment for Music Products. (arXiv:2309.10182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22914;&#20309;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#35282;&#24230;&#22810;&#32423;&#38899;&#20048;&#20869;&#23481;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#20248;&#20110;&#24378;&#22823;&#30340;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#23545;&#24212;&#26041;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel research problem: assessing positive and risky messages from music products. We first establish a benchmark for multi-angle multi-level music content assessment and then present an effective multi-task prediction model with ordinality-enforcement to solve this problem. Our result shows the proposed method not only significantly outperforms strong task-specific counterparts but can concurrently evaluate multiple aspects.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#20840;&#26041;&#20301;&#24863;&#30693;&#29983;&#25104;&#32593;&#32476;&#65288;AOG-Net&#65289;&#29992;&#20110;&#29983;&#25104;360&#24230;&#22270;&#20687;&#65292;&#36890;&#36807;&#28176;&#36827;&#22320;&#22806;&#25193;&#19981;&#23436;&#25972;&#30340;360&#24230;&#22270;&#20687;&#65292;&#24182;&#19982;&#31364;&#35270;&#22330;&#65288;NFoV&#65289;&#21644;&#25991;&#26412;&#24341;&#23548;&#30456;&#32467;&#21512;&#25110;&#21333;&#29420;&#20351;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#32454;&#33268;&#21644;&#19982;&#25991;&#26412;&#19968;&#33268;&#30340;&#27169;&#24335;&#65292;&#24182;&#20026;&#29992;&#25143;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#28789;&#27963;&#32534;&#36753;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.03467</link><description>&lt;p&gt;
Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#20840;&#26041;&#20301;&#24863;&#30693;&#29983;&#25104;&#32593;&#32476;&#65288;AOG-Net&#65289;&#29992;&#20110;&#29983;&#25104;360&#24230;&#22270;&#20687;&#65292;&#36890;&#36807;&#28176;&#36827;&#22320;&#22806;&#25193;&#19981;&#23436;&#25972;&#30340;360&#24230;&#22270;&#20687;&#65292;&#24182;&#19982;&#31364;&#35270;&#22330;&#65288;NFoV&#65289;&#21644;&#25991;&#26412;&#24341;&#23548;&#30456;&#32467;&#21512;&#25110;&#21333;&#29420;&#20351;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#32454;&#33268;&#21644;&#19982;&#25991;&#26412;&#19968;&#33268;&#30340;&#27169;&#24335;&#65292;&#24182;&#20026;&#29992;&#25143;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#28789;&#27963;&#32534;&#36753;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
360&#24230;&#65288;&#20840;&#26041;&#20301;&#65289;&#22270;&#20687;&#25552;&#20379;&#20102;&#19968;&#20010;&#22330;&#26223;&#30340;&#20840;&#26223;&#29699;&#29366;&#35270;&#22270;&#12290;&#26368;&#36817;&#65292;&#22312;&#20174;&#30001;&#25968;&#23383;&#30456;&#26426;&#21644;&#26234;&#33021;&#25163;&#26426;&#25429;&#33719;&#30340;&#20256;&#32479;&#31364;&#35270;&#22330;&#65288;NFoV&#65289;&#22270;&#20687;&#21512;&#25104;360&#24230;&#22270;&#20687;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#25552;&#20379;&#36523;&#20020;&#20854;&#22659;&#30340;&#20307;&#39564;&#65292;&#22914;&#34394;&#25311;&#29616;&#23454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#21512;&#25104;&#22797;&#26434;&#30340;&#35270;&#35273;&#32454;&#33410;&#25110;&#30830;&#20445;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#20840;&#26041;&#20301;&#24863;&#30693;&#29983;&#25104;&#32593;&#32476;&#65288;AOG-Net&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;NFoV&#21644;&#25991;&#26412;&#24341;&#23548;&#30456;&#32467;&#21512;&#25110;&#21333;&#29420;&#36827;&#34892;&#28176;&#36827;&#22320;&#22806;&#25193;&#19981;&#23436;&#25972;&#30340;360&#24230;&#22270;&#20687;&#26469;&#36827;&#34892;360&#24230;&#22270;&#20687;&#29983;&#25104;&#12290;&#36825;&#31181;&#33258;&#22238;&#24402;&#26041;&#26696;&#19981;&#20165;&#20801;&#35768;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#21644;&#35843;&#25972;&#36807;&#31243;&#26469;&#33719;&#21462;&#26356;&#31934;&#32454;&#21644;&#19982;&#25991;&#26412;&#19968;&#33268;&#30340;&#27169;&#24335;&#65292;&#36824;&#20026;&#29992;&#25143;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#32534;&#36753;&#26465;&#20214;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A 360-degree (omni-directional) image provides an all-encompassing spherical view of a scene. Recently, there has been an increasing interest in synthesising 360-degree images from conventional narrow field of view (NFoV) images captured by digital cameras and smartphones, for providing immersive experiences in various scenarios such as virtual reality. Yet, existing methods typically fall short in synthesizing intricate visual details or ensure the generated images align consistently with user-provided prompts. In this study, autoregressive omni-aware generative network (AOG-Net) is proposed for 360-degree image generation by out-painting an incomplete 360-degree image progressively with NFoV and text guidances joinly or individually. This autoregressive scheme not only allows for deriving finer-grained and text-consistent patterns by dynamically generating and adjusting the process but also offers users greater flexibility to edit their conditions throughout the generation process. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#30340;&#26816;&#32034;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#31572;&#26696;&#29983;&#25104;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#30340;&#36830;&#25509;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#21253;&#25324;&#20004;&#31181;&#21333;&#36718;&#26041;&#27861;&#21644;&#20004;&#31181;&#22810;&#36718;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.12574</link><description>&lt;p&gt;
&#25506;&#32034;&#26816;&#32034;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Exploring the Integration Strategies of Retriever and Large Language Models. (arXiv:2308.12574v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#30340;&#26816;&#32034;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#31572;&#26696;&#29983;&#25104;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#30340;&#36830;&#25509;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#21253;&#25324;&#20004;&#31181;&#21333;&#36718;&#26041;&#27861;&#21644;&#20004;&#31181;&#22810;&#36718;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#30340;&#25972;&#21512;&#20026;&#25552;&#39640;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20316;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#34701;&#20837;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#26368;&#20339;&#26041;&#27861;&#20173;&#28982;&#32570;&#20047;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#32467;&#21512;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#31572;&#26696;&#29983;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#24120;&#29992;&#30340;&#36830;&#25509;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#27491;&#30830;&#30340;&#25991;&#26723;&#22312;&#21069;k&#20010;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#20250;&#29983;&#25104;&#8220;&#26410;&#30693;&#8221;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22235;&#31181;&#23558;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#30340;&#26367;&#20195;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#21253;&#25324;&#20004;&#31181;&#21033;&#29992;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#21333;&#36718;&#26041;&#27861;&#21644;&#20004;&#31181;&#21033;&#29992;&#21453;&#39304;&#24490;&#29615;&#30340;&#22810;&#36718;&#31574;&#30053;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
The integration of retrieved passages and large language models (LLMs), such as ChatGPTs, has significantly contributed to improving open-domain question answering. However, there is still a lack of exploration regarding the optimal approach for incorporating retrieved passages into the answer generation process. This paper aims to fill this gap by investigating different methods of combining retrieved passages with LLMs to enhance answer generation. We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating "unknown" outputs, even when the correct document is among the top-k retrieved passages. To address this issue, we explore four alternative strategies for integrating the retrieved passages with the LLMs. These strategies include two single-round methods that utilize chain-of-thought reasoning and two multi-round strategies that incorporate feedback loops. Through comprehensive analyses and experiments, w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#29702;&#35770;&#65292;&#35752;&#35770;&#20102;&#26234;&#33021;&#30340;&#26680;&#24515;&#35201;&#32032;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#29702;&#35770;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#20154;&#31867;&#26234;&#33021;&#65292;&#24182;&#19982;&#26426;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#30446;&#30340;&#26159;&#20026;&#26356;&#24191;&#27867;&#30340;&#29983;&#21629;&#12289;&#38598;&#21512;&#20307;&#21644;&#38750;&#35774;&#35745;&#30340;&#29289;&#29702;&#21270;&#23398;&#31995;&#32479;&#25552;&#20379;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12411</link><description>&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#31181;&#29702;&#35770;&#65306;&#27010;&#24565;&#12289;&#27169;&#22411;&#21644;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
A Theory of Intelligences: Concepts, Models, Implications. (arXiv:2308.12411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12411
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#29702;&#35770;&#65292;&#35752;&#35770;&#20102;&#26234;&#33021;&#30340;&#26680;&#24515;&#35201;&#32032;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#29702;&#35770;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#20154;&#31867;&#26234;&#33021;&#65292;&#24182;&#19982;&#26426;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#30446;&#30340;&#26159;&#20026;&#26356;&#24191;&#27867;&#30340;&#29983;&#21629;&#12289;&#38598;&#21512;&#20307;&#21644;&#38750;&#35774;&#35745;&#30340;&#29289;&#29702;&#21270;&#23398;&#31995;&#32479;&#25552;&#20379;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#26159;&#20154;&#31867;&#29992;&#26469;&#34920;&#31034;&#23454;&#29616;&#30446;&#26631;&#33021;&#21147;&#30340;&#27010;&#24565;&#12290;&#32473;&#20104;&#36825;&#20010;&#24191;&#27867;&#30340;&#33539;&#30068;&#65292;&#26234;&#33021;&#24050;&#32463;&#34987;&#26080;&#25968;&#27425;&#23450;&#20041;&#65292;&#20197;&#21508;&#31181;&#26041;&#24335;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#27979;&#37327;&#26041;&#27861;&#36827;&#34892;&#37327;&#21270;&#12290;&#29702;&#35299;&#26234;&#33021;&#26368;&#32456;&#38656;&#35201;&#29702;&#35770;&#21644;&#37327;&#21270;&#65292;&#20294;&#36825;&#20004;&#32773;&#37117;&#24456;&#38590;&#25417;&#25720;&#12290;&#25105;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#23450;&#26234;&#33021;&#30340;&#19968;&#20123;&#26680;&#24515;&#35201;&#32032;&#65292;&#35752;&#35770;&#20854;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#29702;&#35770;&#12290;&#25105;&#20027;&#35201;&#20851;&#27880;&#20197;&#20154;&#31867;&#20026;&#23450;&#20041;&#21644;&#21442;&#29031;&#23545;&#35937;&#30340;&#26234;&#33021;&#65292;&#24120;&#24120;&#19982;&#26426;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24847;&#22270;&#20026;&#29983;&#21629;&#12289;&#38598;&#21512;&#20307;&#12289;&#20154;&#24037;&#26234;&#33021;&#31561;&#38750;&#35774;&#35745;&#30340;&#29289;&#29702;&#21644;&#21270;&#23398;&#31995;&#32479;&#25552;&#20379;&#26356;&#19968;&#33324;&#21270;&#30340;&#25551;&#36848;&#12290;&#25105;&#35752;&#35770;&#20102;&#26234;&#33021;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#21253;&#25324;&#36335;&#24452;&#25928;&#29575;&#21644;&#30446;&#26631;&#20934;&#30830;&#24615;&#12289;&#26234;&#33021;&#20316;&#20026;&#40657;&#30418;&#12289;&#29615;&#22659;&#24433;&#21709;&#12289;&#22788;&#29702;&#24847;&#22806;&#24773;&#20917;&#30340;&#28789;&#27963;&#24615;&#12289;&#26234;&#33021;&#30340;&#22238;&#24402;&#21644;&#30456;&#23545;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligence is a human construct to represent the ability to achieve goals. Given this wide berth, intelligence has been defined countless times, studied in a variety of ways and quantified using numerous measures. Understanding intelligence ultimately requires theory and quantification, both of which are elusive. My main objectives are to identify some of the central elements in and surrounding intelligence, discuss some of its challenges and propose a theory based on first principles. I focus on intelligence as defined by and for humans, frequently in comparison to machines, with the intention of setting the stage for more general characterizations in life, collectives, human designs such as AI and in non-designed physical and chemical systems. I discuss key features of intelligence, including path efficiency and goal accuracy, intelligence as a Black Box, environmental influences, flexibility to deal with surprisal, the regress of intelligence, the relativistic nature of intelligen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.03929</link><description>&lt;p&gt;
ChatGPT&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#25991;&#26412;&#20013;&#24314;&#31435;&#20449;&#20219;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#30340;&#21307;&#23398;&#25991;&#29486;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#26500;&#24314;&#20102;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998;&#20107;&#23454;&#20449;&#24687;&#21644;&#26410;&#32463;&#39564;&#35777;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#8220;&#20154;&#31867;&#30142;&#30149;&#21644;&#30151;&#29366;&#8221;&#26597;&#35810;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#32534;&#35793;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#27169;&#25311;&#25991;&#31456;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#65288;PubMed&#21644;ChatGPT&#65289;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;10&#32452;&#27599;&#32452;250&#20010;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#31181;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#65288;DOID&#65289;&#21644;&#30151;&#29366;&#26412;&#20307;&#65288;SYMP&#65289;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#20559;&#24046;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;GPT&#30142;&#30149;-&#30151;&#29366;&#38142;&#25509;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#22312;&#22122;&#22768;&#12289;&#20551;&#35774;&#21644;&#37325;&#35201;&#21457;&#29616;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#65306;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;ChatGPT&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;PubMed&#35745;&#25968;&#33719;&#24471;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09913</link><description>&lt;p&gt;
&#25506;&#32034;&#20855;&#26377;&#25551;&#36848;&#36923;&#36753;&#29305;&#24449;&#30340;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#30340;&#38750;&#27491;&#21017;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features. (arXiv:2307.09913v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09913
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#23545;&#35937;&#26159;ALCreg&#21644;ALCvpl&#65292;&#20998;&#21035;&#26159;&#20351;&#29992;&#27491;&#21017;&#21644;&#21487;&#35265;&#25512;&#19979;&#35821;&#35328;&#30340;&#36335;&#24452;&#34920;&#36798;&#24335;&#30340;&#25193;&#23637;&#12290;&#31532;&#19968;&#20010;ALCreg&#26159;Fischer&#21644;Ladner&#25152;&#29087;&#30693;&#30340;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#30340;&#19968;&#31181;&#21464;&#31181;&#12290;&#31532;&#20108;&#20010;ALCvpl&#26159;&#30001;Loding&#21644;Serre&#22312;2007&#24180;&#24341;&#20837;&#21644;&#30740;&#31350;&#30340;&#12290;ALCvpl&#36923;&#36753;&#24191;&#20041;&#19978;&#25512;&#24191;&#20102;&#35768;&#22810;&#24050;&#30693;&#30340;&#21487;&#20915;&#23450;&#24615;&#38750;&#27491;&#21017;&#25193;&#23637;&#30340;ALCreg&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28155;&#21152;&#30475;&#20284;&#26080;&#23475;&#30340;Self&#25805;&#20316;&#31526;&#21518;&#65292;&#23545;&#20110;ALCvpl&#20013;&#30340;&#27010;&#24565;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#21487;&#20915;&#23450;&#24615;&#20007;&#22833;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#22312;ALCvpl&#20013;&#28155;&#21152;&#20010;&#20307;&#35789;&#30340;&#27010;&#24565;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#19981;&#21487;&#20915;&#23450;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#19981;&#21487;&#20915;&#23450;&#24615;&#35777;&#26126;&#21482;&#20381;&#36182;&#20110;&#19968;&#20010;&#21333;&#19968;&#30340;&#38750;&#27491;&#21017;&#65288;&#21487;&#35265;&#25512;&#19979;&#65289;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the impact of non-regular path expressions on the decidability of satisfiability checking and querying in description logics extending ALC. Our primary objects of interest are ALCreg and ALCvpl, the extensions of with path expressions employing, respectively, regular and visibly-pushdown languages. The first one, ALCreg, is a notational variant of the well-known Propositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, was introduced and investigated by Loding and Serre in 2007. The logic ALCvpl generalises many known decidable non-regular extensions of ALCreg.  We provide a series of undecidability results. First, we show that decidability of the concept satisfiability problem for ALCvpl is lost upon adding the seemingly innocent Self operator. Second, we establish undecidability for the concept satisfiability problem for ALCvpl extended with nominals. Interestingly, our undecidability proof relies only on one single non-regular (visibly-pushdown) langu
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10125</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects. (arXiv:2306.10125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10125
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;SSL&#26368;&#31361;&#20986;&#30340;&#20248;&#21183;&#26159;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#19982;&#35768;&#22810;&#20851;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#32508;&#36848;&#30456;&#27604;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;SSL&#30340;&#32508;&#36848;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#22238;&#39038;&#20102;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#29616;&#26377;&#32508;&#36848;&#65292;&#28982;&#21518;&#36890;&#36807;&#24635;&#32467;&#20174;&#29983;&#25104;&#22411;&#12289;&#23545;&#27604;&#22411;&#21644;&#23545;&#25239;&#22411;&#19977;&#20010;&#35282;&#24230;&#23545;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#26041;&#27861;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#21313;&#20010;&#23376;&#31867;&#65292;&#35814;&#32454;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#30452;&#35273;&#12289;&#20027;&#35201;&#26694;&#26550;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages an
&lt;/p&gt;</description></item><item><title>DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.09862</link><description>&lt;p&gt;
DoubleAdapt&#65306;&#19968;&#31181;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09862
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#26159;&#37327;&#21270;&#25237;&#36164;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#20934;&#30830;&#39044;&#27979;&#20215;&#26684;&#36235;&#21183;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20316;&#20026;&#19968;&#39033;&#22312;&#32447;&#26381;&#21153;&#65292;&#32929;&#31080;&#25968;&#25454;&#38543;&#26102;&#38543;&#22320;&#25345;&#32493;&#21040;&#36798;&#12290;&#20351;&#29992;&#26368;&#26032;&#25968;&#25454;&#23545;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#26356;&#26032;&#26159;&#23454;&#29992;&#32780;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#26032;&#25968;&#25454;&#21487;&#33021;&#25581;&#31034;&#20102;&#26410;&#26469;&#32929;&#31080;&#24066;&#22330;&#20013;&#20250;&#37325;&#22797;&#20986;&#29616;&#30340;&#19968;&#20123;&#26032;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#24067;&#28418;&#31227;&#65288;&#21363;&#27010;&#24565;&#28418;&#31227;&#65289;&#30340;&#25361;&#25112;&#65292;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#38543;&#30528;&#32929;&#31080;&#24066;&#22330;&#21160;&#24577;&#28436;&#21464;&#65292;&#26410;&#26469;&#25968;&#25454;&#30340;&#20998;&#24067;&#21487;&#33021;&#20250;&#19982;&#22686;&#37327;&#25968;&#25454;&#31245;&#24494;&#25110;&#26174;&#30528;&#22320;&#19981;&#21516;&#65292;&#20174;&#32780;&#38459;&#30861;&#22686;&#37327;&#26356;&#26032;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20004;&#20010;&#36866;&#37197;&#22120;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#8212;&#8212;DoubleAdapt&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distri
&lt;/p&gt;</description></item><item><title>Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.09750</link><description>&lt;p&gt;
Fedstellar&#65306;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09750
&lt;/p&gt;
&lt;p&gt;
Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2016&#24180;&#65292;&#35895;&#27468;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#36328;&#32852;&#30431;&#21442;&#19982;&#32773;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#34429;&#28982;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#12289;&#21333;&#28857;&#25925;&#38556;&#21644;&#23545;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#31561;&#23616;&#38480;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36890;&#36807;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#21644;&#26368;&#23567;&#21270;&#23545;&#20013;&#22830;&#23454;&#20307;&#30340;&#20381;&#36182;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35757;&#32451;DFL&#27169;&#22411;&#30340;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fedstellar&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#22312;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#19981;&#21516;&#32852;&#30431;&#20013;&#20197;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
&lt;/p&gt;</description></item><item><title>&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17328</link><description>&lt;p&gt;
&#12298;Zero-TPrune: &#22522;&#20110;&#39044;&#35757;&#32451;Transformers&#20851;&#27880;&#22270;&#30340;&#38646;&#23556;&#20987;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17328
&lt;/p&gt;
&lt;p&gt;
&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;Transformer&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#27169;&#22411;&#30340;&#20307;&#31215;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#25512;&#29702;&#25104;&#26412;&#21017;&#38543;&#36755;&#20837;&#24207;&#21015;&#20013;&#20196;&#29260;&#25968;&#37327;&#30340;&#24179;&#26041;&#25552;&#39640;&#12290;&#20196;&#29260;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#26131;&#20110;&#22312;&#21508;&#31181;Transformer&#25903;&#25345;&#30340;&#27169;&#22411;&#19978;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#38656;&#35201;&#22312;&#21098;&#26525;&#21518;&#25110;&#26399;&#38388;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25506;&#35752;&#20102;&#27809;&#26377;&#24494;&#35843;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;Transformer&#30340;&#21098;&#26525;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Zero-TPrune&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#26082;&#32771;&#34385;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21448;&#32771;&#34385;&#30456;&#20284;&#24615;&#26469;&#25191;&#34892;&#20196;&#29260;&#21098;&#26525;&#12290;Zero-TPrune&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#20026;&#20196;&#29260;&#29983;&#25104;&#19968;&#20010;&#37325;&#35201;&#24615;&#25490;&#21517;&#24182;&#31227;&#38500;&#20449;&#24687;&#36739;&#23569;&#30340;&#20196;&#29260;&#12290;&#27880;&#24847;&#30697;&#38453;&#21487;&#29992;&#20110;&#25512;&#26029;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployment of Transformer models on the edge is increasingly challenging due to the exponentially growing model size and inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require a computationally-expensive fine-tuning process after or during pruning, which is not desirable in many cases. Some recent works explore pruning of off-the-shelf pre-trained Transformers without fine-tuning. However, they only take the importance of tokens into consideration. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. Zero-TPrune leverages the attention graph of pre-trained Transformer models to produce an importance rank for tokens and removes the less informative tokens. The attention matrix can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09620</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#35843;&#26597;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#35266;&#28857;&#39044;&#27979;&#20013;&#65292;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#65292;&#22238;&#28335;&#25512;&#29702;&#21644;&#38646;&#27425;&#39044;&#27979;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#23558;&#35843;&#26597;&#38382;&#39064;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#31070;&#32463;&#23884;&#20837;&#24341;&#20837;&#21040;&#35266;&#28857;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;LLMs&#20013;&#12290;&#22312;1972&#24180;&#21040;2021&#24180;&#30340;&#8220;&#24120;&#35268;&#31038;&#20250;&#35843;&#26597;&#8221;&#20013;&#65292;&#25105;&#20204;&#20174;68,846&#21517;&#32654;&#22269;&#20154;&#20013;&#33719;&#24471;&#20102;3,110&#20010;&#20108;&#36827;&#21046;&#35266;&#28857;&#65292;&#22312;Alpaca-7b&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#26524;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65288;AUC=0.87&#65292;&#20844;&#24320;&#35266;&#28857;&#39044;&#27979;&#20026;$\rho$=0.99&#65289;&#21644;&#22238;&#28335;&#25512;&#29702;&#65288;AUC=0.86&#65292;$\rho$=0.98&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26174;&#33879;&#30340;&#39044;&#27979;&#33021;&#21147;&#33021;&#22815;&#20197;&#39640;&#32622;&#20449;&#24230;&#22635;&#34917;&#32570;&#22833;&#30340;&#36235;&#21183;&#65292;&#24182;&#26631;&#26126;&#20844;&#20247;&#24577;&#24230;&#20309;&#26102;&#21457;&#29983;&#21464;&#21270;&#65292;&#22914;&#21516;&#24615;&#23130;&#23035;&#30340;&#33719;&#21462;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#21463;&#21040;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;</title><link>http://arxiv.org/abs/2304.13567</link><description>&lt;p&gt;
&#20301;&#32622;&#20559;&#24046;&#23545;token&#20998;&#31867;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13567
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#25110;&#35789;&#24615;&#26631;&#27880;&#31561;&#19979;&#28216;&#20219;&#21153;&#24050;&#30693;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#36127;&#31034;&#20363;&#30340;&#27604;&#20363;&#21644;&#31867;&#19981;&#24179;&#34913;&#26041;&#38754;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#65292;&#21363;token&#20998;&#31867;&#20219;&#21153;&#20013;&#27491;&#31034;&#20363;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;Token&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20301;&#32622;&#20559;&#24046;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;CoNLL03&#21644;OntoNote5.0&#29992;&#20110;NER&#65292;English Tree Bank UD_en&#21644;TweeBank&#29992;&#20110;POS&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;Transformer&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#20687;BERT&#12289;ERNIE&#12289;ELECTRA&#36825;&#26679;&#30340;&#32534;&#30721;&#22120;&#21644;&#20687;GPT2 &#21644;BLOOM&#36825;&#26679;&#30340;&#35299;&#30721;&#22120;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#23558;&#35299;&#20915;&#26041;&#26696;&#35270;&#20026;&#29992;&#25143;&#65292;&#36890;&#36807;&#27748;&#26222;&#26862;&#25277;&#26679;&#21644;&#39640;&#26031;&#36807;&#31243;&#36924;&#36817; Pareto-&#26368;&#20248;&#21069;&#27839;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04067</link><description>&lt;p&gt;
&#22522;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Recommender System Approach for Very Large-scale Multiobjective Optimization. (arXiv:2304.04067v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#23558;&#35299;&#20915;&#26041;&#26696;&#35270;&#20026;&#29992;&#25143;&#65292;&#36890;&#36807;&#27748;&#26222;&#26862;&#25277;&#26679;&#21644;&#39640;&#26031;&#36807;&#31243;&#36924;&#36817; Pareto-&#26368;&#20248;&#21069;&#27839;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23450;&#20041;&#20102;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#36229;&#36807;100,000&#20010;&#32428;&#24230;&#30340;&#38382;&#39064;&#20026;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#30001;&#20110;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#38656;&#35201;&#20248;&#21270;&#21313;&#19975;&#32423;&#21035;&#30340;&#21464;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#36827;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31181;&#35268;&#27169;&#38750;&#24120;&#22823;&#30340;&#38382;&#39064;&#26102;&#23384;&#22312;&#19981;&#36275;&#12290;&#21463;&#21040;&#29616;&#26377;&#25512;&#33616;&#31995;&#32479;&#25104;&#21151;&#22788;&#29702;&#21382;&#21490;&#20132;&#20114;&#26377;&#38480;&#30340;&#22823;&#35268;&#27169;&#29289;&#21697;&#30340;&#21551;&#21457;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#22522;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#8221;&#65288;VMORS&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#24605;&#24819;&#26159;&#23558;&#36825;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#20197;&#30001;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#35299;&#20915;&#26041;&#26696;&#34987;&#35270;&#20026;&#29992;&#25143;&#65292;&#19981;&#21516;&#30340;&#36827;&#21270;&#26041;&#21521;&#26159;&#31561;&#24453;&#25512;&#33616;&#30340;&#39033;&#30446;&#12290;&#25105;&#20204;&#20351;&#29992;&#27748;&#26222;&#26862;&#25277;&#26679;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#36924;&#36817; Pareto-&#26368;&#20248;&#21069;&#27839;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20197;&#25512;&#33616;&#36827;&#21270;&#26041;&#21521;&#12290;&#23545;&#22522;&#20934;&#38382;&#39064;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define very large multi-objective optimization problems to be multiobjective optimization problems in which the number of decision variables is greater than 100,000 dimensions. This is an important class of problems as many real-world problems require optimizing hundreds of thousands of variables. Existing evolutionary optimization methods fall short of such requirements when dealing with problems at this very large scale. Inspired by the success of existing recommender systems to handle very large-scale items with limited historical interactions, in this paper we propose a method termed Very large-scale Multiobjective Optimization through Recommender Systems (VMORS). The idea of the proposed method is to transform the defined such very large-scale problems into a problem that can be tackled by a recommender system. In the framework, the solutions are regarded as users, and the different evolution directions are items waiting for the recommendation. We use Thompson sampling to recom
&lt;/p&gt;</description></item></channel></rss>