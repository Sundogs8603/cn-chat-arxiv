<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>SemanticBoost&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#25991;&#26412;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25552;&#21319;&#36816;&#21160;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#20174;&#22797;&#26434;&#35821;&#20041;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.20323</link><description>&lt;p&gt;
SemanticBoost&#65306;&#36890;&#36807;&#22686;&#24378;&#25991;&#26412;&#25552;&#31034;&#25552;&#21319;&#36816;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SemanticBoost: Elevating Motion Generation with Augmented Textual Cues. (arXiv:2310.20323v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20323
&lt;/p&gt;
&lt;p&gt;
SemanticBoost&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#25991;&#26412;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25552;&#21319;&#36816;&#21160;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#20174;&#22797;&#26434;&#35821;&#20041;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#25216;&#26415;&#22312;&#20174;&#22797;&#26434;&#30340;&#35821;&#20041;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#26102;&#38754;&#20020;&#22256;&#38590;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#36275;&#22815;&#30340;&#35821;&#20041;&#27880;&#37322;&#21644;&#24369;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SemanticBoost&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#35821;&#20041;&#22686;&#24378;&#27169;&#22359;&#21644;&#19978;&#19979;&#25991;&#35843;&#33410;&#36816;&#21160;&#21435;&#22122;&#22120;&#65288;CAMD&#65289;&#12290;&#35821;&#20041;&#22686;&#24378;&#27169;&#22359;&#20174;&#36816;&#21160;&#25968;&#25454;&#20013;&#25552;&#21462;&#34917;&#20805;&#35821;&#20041;&#65292;&#20016;&#23500;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#30830;&#20445;&#25991;&#26412;&#21644;&#36816;&#21160;&#25968;&#25454;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#40784;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;CAMD&#26041;&#27861;&#36890;&#36807;&#26377;&#25928;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#23558;&#29983;&#25104;&#30340;&#36816;&#21160;&#19982;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#35821;&#20041;&#19968;&#33268;&#30340;&#36816;&#21160;&#24207;&#21015;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21512;&#25104;&#20934;&#30830;&#30340;&#23450;&#21521;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current techniques face difficulties in generating motions from intricate semantic descriptions, primarily due to insufficient semantic annotations in datasets and weak contextual understanding. To address these issues, we present SemanticBoost, a novel framework that tackles both challenges simultaneously. Our framework comprises a Semantic Enhancement module and a Context-Attuned Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary semantics from motion data, enriching the dataset's textual description and ensuring precise alignment between text and motion data without depending on large language models. On the other hand, the CAMD approach provides an all-encompassing solution for generating high-quality, semantically consistent motion sequences by effectively capturing context information and aligning the generated motion with the given textual descriptions. Distinct from existing methods, our approach can synthesize accurate orientational movements, combi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.20246</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#20013;&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#35265;&#35299;&#19982;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#36866;&#29992;&#20110;&#21333;&#35821;&#35328;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#30340;&#24378;&#22823;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20445;&#25345;&#25928;&#26524;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#21644;&#35757;&#32451;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#65288;xMR&#65289;LLM&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#32763;&#35793;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#25351;&#23548;&#25968;&#25454;&#38598;MGSM8KInstruct&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;xMR&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;xMR LLMs&#65292;&#34987;&#21629;&#21517;&#20026;MathOctopus&#65292;&#22312;&#20960;&#27425;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#24320;&#28304;LLMs&#21644;ChatGPT&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MathOctopus-13B&#22312;MGSM&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;47.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;ChatGPT&#30340;46.3%&#12290;&#38500;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#20174;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#35266;&#23519;&#21644;&#35265;&#35299;&#65306;&#65288;1&#65289;&#22312;&#22810;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#22909;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290; &#65288;2&#65289;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290; &#65288;3&#65289;&#27169;&#22411;&#23545;&#20110;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#30340;&#22788;&#29702;&#26159;&#25361;&#25112;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
&lt;/p&gt;</description></item><item><title>FormalGeo&#26159;&#19968;&#31181;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#65292;&#33021;&#22815;&#21033;&#29992;&#29616;&#20195;AI&#27169;&#22411;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;AI&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#35299;&#20915;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#35835;&#12289;&#36861;&#28335;&#21644;&#21487;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.18021</link><description>&lt;p&gt;
FormalGeo&#65306;&#36808;&#21521;&#20154;&#31867;&#32423;IMO&#27700;&#24179;&#20960;&#20309;&#33258;&#21160;&#25512;&#29702;&#30340;&#31532;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
FormalGeo: The First Step Toward Human-like IMO-level Geometric Automated Reasoning. (arXiv:2310.18021v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18021
&lt;/p&gt;
&lt;p&gt;
FormalGeo&#26159;&#19968;&#31181;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#65292;&#33021;&#22815;&#21033;&#29992;&#29616;&#20195;AI&#27169;&#22411;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;AI&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#35299;&#20915;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#35835;&#12289;&#36861;&#28335;&#21644;&#21487;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#25105;&#20204;&#36807;&#21435;&#21313;&#24180;&#24037;&#20316;&#30340;&#31532;&#19968;&#31687;&#25991;&#31456;&#12290;&#22312;&#36825;&#19968;&#31995;&#21015;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#12290;&#36825;&#23558;&#20316;&#20026;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#25361;&#25112;&#19982;&#21487;&#35835;&#30340;AI&#33258;&#21160;&#25512;&#29702;&#20043;&#38388;&#30340;&#20851;&#38190;&#26725;&#26753;&#12290;&#26377;&#20102;&#36825;&#20010;&#27491;&#24335;&#31995;&#32479;&#65292;&#25105;&#20204;&#33021;&#22815;&#26080;&#32541;&#22320;&#23558;&#29616;&#20195;AI&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#27491;&#24335;&#31995;&#32479;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#22312;&#36825;&#20010;&#27491;&#24335;&#26694;&#26550;&#20869;&#65292;AI&#29616;&#22312;&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#23545;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36825;&#20123;&#35777;&#26126;&#26159;&#21487;&#35835;&#30340;&#12289;&#21487;&#36861;&#28335;&#30340;&#21644;&#21487;&#39564;&#35777;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#24418;&#24335;&#21270;&#29702;&#35770;&#65288;GFT&#65289;&#26469;&#25351;&#23548;&#20960;&#20309;&#24418;&#24335;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#22522;&#20110;GFT&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;FormalGeo&#65292;&#21253;&#25324;88&#20010;&#20960;&#20309;&#35859;&#35789;&#21644;196&#20010;&#23450;&#29702;&#12290;&#23427;&#33021;&#22815;&#34920;&#31034;&#12289;&#39564;&#35777;&#21644;&#35299;&#20915;IMO&#32423;&#20960;&#20309;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;Python&#24320;&#21457;&#20102;FGPS&#65288;&#27491;&#24335;&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;&#22120;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is the first article of our work over the past decade. In this series of papers, we have constructed a complete and compatible formal plane geometry system. This will serve as a crucial bridge between IMO-level plane geometry challenges and readable AI automated reasoning. With this formal system in place, we have been able to seamlessly integrate modern AI models with our formal system. Within this formal framework, AI is now capable of providing deductive reasoning solutions to IMO-level plane geometry problems, just like handling other natural languages, and these proofs are readable, traceable, and verifiable. We propose the geometry formalization theory (GFT) to guide the development of the geometry formal system. Based on the GFT, we have established the FormalGeo, which consists of 88 geometric predicates and 196 theorems. It can represent, validate, and solve IMO-level geometry problems. we also have crafted the FGPS (formal geometry problem solver) in Python. It serves as
&lt;/p&gt;</description></item><item><title>netFound&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25429;&#25417;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2310.17025</link><description>&lt;p&gt;
netFound: &#32593;&#32476;&#23433;&#20840;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
netFound: Foundation Model for Network Security. (arXiv:2310.17025v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17025
&lt;/p&gt;
&lt;p&gt;
netFound&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25429;&#25417;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20256;&#32479;&#24037;&#20316;&#27969;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#65292;&#20294;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#21644;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38459;&#30861;&#20102;&#29305;&#24449;&#36873;&#25321;&#65292;&#23548;&#33268;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#20851;&#38190;&#20851;&#31995;&#21644;&#26377;&#25928;&#27867;&#21270;&#12290;&#21463;&#21040;GPT-4&#21644;Vision Transformers&#31561;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;netFound&#65292;&#19968;&#20010;&#32593;&#32476;&#23433;&#20840;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#23545;&#29616;&#26377;&#30340;&#26410;&#26631;&#35760;&#32593;&#32476;&#25968;&#25454;&#21253;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;netFound&#30340;&#35774;&#35745;&#34701;&#21512;&#20102;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#26377;&#25928;&#25429;&#25417;&#20102;&#38544;&#34255;&#30340;&#32593;&#32476;&#19978;&#19979;&#25991;&#65292;&#21253;&#25324;&#24212;&#29992;&#36923;&#36753;&#12289;&#36890;&#20449;&#21327;&#35758;&#21644;&#32593;&#32476;&#26465;&#20214;&#12290;&#26377;&#20102;&#36825;&#20010;&#39044;&#35757;&#32451;&#22522;&#30784;&#65292;&#21363;&#20351;&#22788;&#29702;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#23545;netFound&#36827;&#34892;&#24494;&#35843;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;netFound&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ML for network security, traditional workflows rely on high-quality labeled data and manual feature engineering, but limited datasets and human expertise hinder feature selection, leading to models struggling to capture crucial relationships and generalize effectively. Inspired by recent advancements in ML application domains like GPT-4 and Vision Transformers, we have developed netFound, a foundational model for network security. This model undergoes pre-training using self-supervised algorithms applied to readily available unlabeled network packet traces. netFound's design incorporates hierarchical and multi-modal attributes of network traffic, effectively capturing hidden networking contexts, including application logic, communication protocols, and network conditions.  With this pre-trained foundation in place, we can fine-tune netFound for a wide array of downstream tasks, even when dealing with low-quality, limited, and noisy labeled data. Our experiments demonstrate netFound'
&lt;/p&gt;</description></item><item><title>PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11676</link><description>&lt;p&gt;
PREM:&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11676
&lt;/p&gt;
&lt;p&gt;
PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#22312;&#35782;&#21035;&#21307;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#24120;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#26631;&#27880;&#25968;&#25454;&#30340;&#21294;&#20047;&#65292;&#24050;&#26377;&#30340;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#24448;&#24448;&#22312;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#36825;&#28304;&#20110;&#23427;&#20204;&#22797;&#26434;&#30340;&#30446;&#26631;&#21644;&#32321;&#29712;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#25552;&#39640;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;PREprocessing and Matching&#65288;&#31616;&#31216;PREM&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;PREM&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#39044;&#22788;&#29702;&#27169;&#22359;&#21644;&#37051;&#23621;&#21305;&#37197;&#27169;&#22359;&#12290;PREM&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#20102;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreov
&lt;/p&gt;</description></item><item><title>CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08992</link><description>&lt;p&gt;
CodeChain: &#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#23454;&#29616;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. (arXiv:2310.08992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08992
&lt;/p&gt;
&lt;p&gt;
CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#35299;&#20915;&#31616;&#21333;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#29087;&#32451;&#65292;&#27604;&#22914;&#22312;HumanEval&#25110;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#32534;&#31243;&#20219;&#21153;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#20316;&#20026;&#25972;&#20307;&#20195;&#30721;&#22359;&#32780;&#19981;&#26159;&#23558;&#20854;&#20998;&#35299;&#20026;&#36923;&#36753;&#23376;&#20219;&#21153;&#21644;&#23376;&#27169;&#22359;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26377;&#32463;&#39564;&#30340;&#31243;&#24207;&#21592;&#26412;&#33021;&#22320;&#32534;&#20889;&#20855;&#26377;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22359;&#21270;&#20195;&#30721;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#36890;&#24120;&#20250;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#24320;&#21457;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeChain&#65292;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CodeChain&#39318;&#20808;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25552;&#31034;&#25351;&#23548;LLM&#29983;&#25104;&#27169;&#22359;&#21270;&#20195;&#30721;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#20004;&#20010;&#27493;&#39588;&#23454;&#26045;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#65306;1&#65289;&#39069;&#22806;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05898</link><description>&lt;p&gt;
&#29422;&#23376;&#31192;&#23494;&#22320;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#65306;&#27491;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25152;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05898
&lt;/p&gt;
&lt;p&gt;
Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;Lion&#65288;&#36827;&#21270;&#30340;&#31526;&#21495;&#21160;&#37327;&#65289;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23427;&#22312;&#35757;&#32451;&#25928;&#26524;&#19978;&#19982;AdamW&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#27491;&#22914;&#25105;&#20204;&#21487;&#20197;&#20174;&#38543;&#26426;&#25628;&#32034;&#31243;&#24207;&#30340;&#32467;&#26524;&#20013;&#26399;&#24453;&#30340;&#65292;Lion&#38598;&#25104;&#20102;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#21253;&#25324;&#31526;&#21495;&#21160;&#37327;&#12289;&#29420;&#31435;&#30340;&#26435;&#37325;&#34928;&#20943;&#12289;Polak&#21644;Nesterov&#21160;&#37327;&#65292;&#20294;&#21448;&#19981;&#23646;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#29702;&#35770;&#22522;&#30784;&#20248;&#21270;&#22120;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Lion&#20316;&#20026;&#24191;&#27867;&#20219;&#21153;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#29702;&#35770;&#30340;&#26126;&#30830;&#24615;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21644;&#25193;&#23637;Lion&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#24320;Lion&#30340;&#31070;&#31192;&#38754;&#32433;&#12290;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;$f(x)$&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#28436;&#36827;&#21382;&#31243;&#65292;&#21253;&#25324;&#26089;&#26399;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24341;&#20837;&#65292;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#20197;&#35299;&#20915;&#20559;&#35265;&#21644;&#26292;&#38706;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#36824;&#35752;&#35770;&#20102;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#20197;&#21450;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04438</link><description>&lt;p&gt;
&#19968;&#20221;&#20851;&#20110;&#25552;&#31034;&#24037;&#31243;&#30340;&#31616;&#35201;&#21382;&#21490;: &#21033;&#29992;&#35821;&#35328;&#27169;&#22411; (arXiv:2310.04438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
A Brief History of Prompt: Leveraging Language Models. (arXiv:2310.04438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04438
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#28436;&#36827;&#21382;&#31243;&#65292;&#21253;&#25324;&#26089;&#26399;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24341;&#20837;&#65292;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#20197;&#35299;&#20915;&#20559;&#35265;&#21644;&#26292;&#38706;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#36824;&#35752;&#35770;&#20102;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#20197;&#21450;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#30340;&#28436;&#36827;&#21382;&#31243;&#12290;&#20174;&#26089;&#26399;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#24320;&#22987;&#65292;&#25105;&#20204;&#36861;&#28335;&#20102;&#36825;&#20123;&#24180;&#26469;&#22609;&#36896;&#25552;&#31034;&#24037;&#31243;&#30340;&#20851;&#38190;&#21457;&#23637;&#12290;2015&#24180;&#24341;&#20837;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#21487;&#25511;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36827;&#27493;&#12290;&#38543;&#21518;&#22312;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#26041;&#38754;&#30340;&#31361;&#30772;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25552;&#31034;&#24037;&#31243;&#65292;&#35299;&#20915;&#20102;&#26292;&#38706;&#20559;&#24046;&#21644;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#37325;&#28857;&#32771;&#23519;&#20102;2018&#24180;&#21644;2019&#24180;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#38598;&#20013;&#22312;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#19978;&#12290;&#26412;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#20197;&#21450;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#26085;&#30410;&#37325;&#35201;&#24615;&#12290;&#22312;2020&#24180;&#21644;2021&#24180;&#65292;&#19978;&#19979;&#25991;&#25552;&#31034;&#21644;&#36801;&#31227;&#23398;&#20064;&#21464;&#24471;&#31361;&#20986;&#65292;&#32780;2022&#24180;&#21644;2023&#24180;&#35265;&#35777;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#20026;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.03211</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Performance of Multimodal Language Models. (arXiv:2310.03211v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#20026;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29420;&#31435;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#36890;&#36807;&#27169;&#22411;&#23233;&#25509;&#30340;&#26041;&#24335;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21518;&#65292;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26377;&#26395;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#19981;&#21516;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#12289;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#20108;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#25351;&#23548;&#26550;&#26500;&#36873;&#25321;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#27809;&#26377;&#36275;&#22815;&#22320;&#35299;&#20915;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) have demonstrated promising zero-shot generalization capabilities across various downstream tasks. Recent research has introduced multimodal capabilities to LLMs by integrating independently pretrained vision encoders through model grafting. These multimodal variants undergo instruction tuning, similar to LLMs, enabling effective zero-shot generalization for multimodal tasks. This study conducts a comparative analysis of different multimodal instruction tuning approaches and evaluates their performance across a range of tasks, including complex reasoning, conversation, image captioning, multiple-choice questions (MCQs), and binary classification. Through rigorous benchmarking and ablation experiments, we reveal key insights for guiding architectural choices when incorporating multimodal capabilities into LLMs. However, current approaches have limitations; they do not sufficiently address the need for a diverse multimodal instruction datase
&lt;/p&gt;</description></item><item><title>Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03059</link><description>&lt;p&gt;
Point-PEFT: &#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03059
&lt;/p&gt;
&lt;p&gt;
Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27969;&#34892;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31561;&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#25104;&#26412;&#65292;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#35328;&#21644;2D&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19987;&#38376;PEFT&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-PEFT&#65292;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#28857;&#20113;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;3D&#27169;&#22411;&#65292;&#25105;&#20204;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#12290;Point-prior Prompt&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20869;&#23384;&#24211;&#26469;&#22686;&#24378;&#25552;&#31034;&#26631;&#35760;&#30340;&#21442;&#25968;&#26080;&#20851;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;Geometry-aware Adapter&#26088;&#22312;&#23545;&#19981;&#21516;&#20219;&#21153;&#25110;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.02071</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond. (arXiv:2310.02071v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#25913;&#36827;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20687;GPT4-Vision&#36825;&#26679;&#30340;MLLM&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;MLLMs&#33021;&#21542;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#22788;&#29702;&#20855;&#36523;&#20915;&#31574;&#65292;&#24182;&#19988;LLMs&#21644;MLLMs&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#21542;&#33021;&#22686;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;PCA-EVAL&#30340;&#26032;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#20174;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#35282;&#24230;&#35780;&#20272;&#20855;&#36523;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOLMES&#65292;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#21033;&#29992;MLLMs&#21644;APIs&#33719;&#21462;&#22810;&#27169;&#24577;&#20449;&#24687;&#20197;&#36827;&#34892;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#27604;&#36739;&#20102;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#21644;HOLMES&#65292;&#24182;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2310.01837</link><description>&lt;p&gt;
&#25193;&#23637;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#26080;&#27861;&#23545;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#12289;&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;/&#25512;&#29702;&#25805;&#20316;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21482;&#33021;&#34987;&#35270;&#20026;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#19987;&#23478;&#38656;&#35201;&#24110;&#21161;&#29702;&#35299;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#34892;&#20026;&#21644;&#22522;&#30784;&#20915;&#31574;&#36807;&#31243;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#30830;&#20445;AI&#27169;&#22411;&#31283;&#20581;&#12289;&#23454;&#29992;&#21644;&#21487;&#20449;&#36182;&#37096;&#32626;&#30340;&#25163;&#27573;&#12290;&#24050;&#26377;&#19968;&#20123;XAI&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#35299;&#37322;&#21017;&#22522;&#26412;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;&#26368;&#26032;&#30340;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#20351;&#20854;&#36866;&#29992;&#20110;&#22810;&#31867;&#22270;&#20687;&#20998;&#21106;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20854;&#20013;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#24314;&#31569;&#29289;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current AI-based methods do not provide comprehensible physical interpretations of the utilized data, extracted features, and predictions/inference operations. As a result, deep learning models trained using high-resolution satellite imagery lack transparency and explainability and can be merely seen as a black box, which limits their wide-level adoption. Experts need help understanding the complex behavior of AI models and the underlying decision-making process. The explainable artificial intelligence (XAI) field is an emerging field providing means for robust, practical, and trustworthy deployment of AI models. Several XAI techniques have been proposed for image classification tasks, whereas the interpretation of image segmentation remains largely unexplored. This paper offers to bridge this gap by adapting the recent XAI classification algorithms and making them usable for muti-class image segmentation, where we mainly focus on buildings' segmentation from high-resolution satellite 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#25512;&#26029;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.16064</link><description>&lt;p&gt;
&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#32454;&#32990;&#24418;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Masked autoencoders are scalable learners of cellular morphology. (arXiv:2309.16064v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#25512;&#26029;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#20869;&#23481;&#26174;&#24494;&#38236;&#26816;&#26597;&#20013;&#20174;&#32454;&#32990;&#34920;&#22411;&#20013;&#25512;&#26029;&#29983;&#29289;&#20851;&#31995;&#22312;&#29983;&#29289;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#27604;&#25163;&#24037;&#35774;&#35745;&#30340;&#29305;&#24449;&#26356;&#33021;&#25429;&#25417;&#29983;&#29289;&#20449;&#21495;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#26368;&#39640;&#23610;&#24230;&#19978;&#65292;&#19968;&#20010;&#22312;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#26500;&#24314;&#30340;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35206;&#30422;&#36229;&#36807;35&#20159;&#20010;&#21807;&#19968;&#21098;&#35009;&#22270;&#20687;&#30340;ViT-L/8&#27169;&#22411;&#65292;&#22312;&#25512;&#26029;&#24050;&#30693;&#29983;&#29289;&#20851;&#31995;&#26102;&#30456;&#23545;&#25913;&#36827;&#39640;&#36798;28%&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring biological relationships from cellular phenotypes in high-content microscopy screens provides significant opportunity and challenge in biological research. Prior results have shown that deep vision models can capture biological signal better than hand-crafted features. This work explores how weakly supervised and self-supervised deep learning approaches scale when training larger models on larger datasets. Our results show that both CNN- and ViT-based masked autoencoders significantly outperform weakly supervised models. At the high-end of our scale, a ViT-L/8 trained on over 3.5-billion unique crops sampled from 95-million microscopy images achieves relative improvements as high as 28% over our best weakly supervised models at inferring known biological relationships curated from public databases.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11526</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#20256;&#24863;&#22120;&#26657;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25216;&#26415;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#23558;&#19968;&#20010;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#32467;&#26524;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#20855;&#26377;&#30456;&#21516;&#35774;&#35745;&#30340;&#20256;&#24863;&#22120;&#12290;&#19968;&#31181;&#24819;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#20223;&#23556;&#21464;&#25442;&#20272;&#35745;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#19987;&#23478;&#30340;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Glacier Research&#22312;1973&#24180;&#21457;&#34920;&#30340;&#25913;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#29992;&#20110;&#20256;&#24863;&#22120;&#30340;&#36719;&#20214;&#26657;&#20934;&#12289;&#22522;&#20110;&#19987;&#23478;&#30340;&#36866;&#24212;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;8&#20010;&#30456;&#21516;&#20256;&#24863;&#22120;&#30340;&#22810;&#20256;&#24863;&#22120;&#26495;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#27169;&#25311;&#36824;&#26159;&#23454;&#39564;&#25968;&#25454;&#65292;&#37117;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#65292;&#36890;&#36807;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30701;DNA&#24207;&#21015;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11087</link><description>&lt;p&gt;
Embed-Search-Align: &#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;DNA&#24207;&#21015;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Embed-Search-Align: DNA Sequence Alignment using Transformer Models. (arXiv:2309.11087v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11087
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#65292;&#36890;&#36807;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30701;DNA&#24207;&#21015;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNA&#24207;&#21015;&#23545;&#40784;&#28041;&#21450;&#23558;&#30701;DNA&#35835;&#21462;&#20998;&#37197;&#21040;&#24191;&#27867;&#30340;&#21442;&#32771;&#22522;&#22240;&#32452;&#19978;&#30340;&#26368;&#21487;&#33021;&#20301;&#32622;&#12290;&#36825;&#20010;&#36807;&#31243;&#23545;&#20110;&#21508;&#31181;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#21464;&#24322;&#35843;&#29992;&#12289;&#36716;&#24405;&#32452;&#23398;&#21644;&#34920;&#35266;&#22522;&#22240;&#32452;&#23398;&#12290;&#20256;&#32479;&#26041;&#27861;&#32463;&#36807;&#25968;&#21313;&#24180;&#30340;&#25913;&#36827;&#65292;&#20197;&#20004;&#20010;&#27493;&#39588;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#20808;&#36827;&#34892;&#22522;&#22240;&#32452;&#32034;&#24341;&#65292;&#28982;&#21518;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#20197;&#30830;&#23450;&#32473;&#23450;&#35835;&#21462;&#30340;&#21487;&#33021;&#20301;&#32622;&#12290;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23558;&#25991;&#26412;&#32534;&#30721;&#20026;&#23884;&#20837;&#21521;&#37327;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#25506;&#32034;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#30456;&#21516;&#30340;Transformer&#26550;&#26500;&#20026;DNA&#24207;&#21015;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#24050;&#32463;&#22312;&#28041;&#21450;&#20998;&#31867;&#30701;DNA&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26089;&#26399;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#26816;&#27979;&#32534;&#30721;&#21644;&#38750;&#32534;&#30721;&#21306;&#22495;&#20197;&#21450;&#35782;&#21035;&#22686;&#24378;&#23376;&#21644;&#21551;&#21160;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#24207;&#21015;&#23545;&#40784;&#20219;&#21153;&#65292;&#23545;&#40784;&#20219;&#21153;&#30340;&#20851;&#38190;&#26159;&#22312;&#20445;&#25345;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#21516;&#26102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23545;&#24212;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNA sequence alignment involves assigning short DNA reads to the most probable locations on an extensive reference genome. This process is crucial for various genomic analyses, including variant calling, transcriptomics, and epigenomics. Conventional methods, refined over decades, tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the distance metric captures semantic similarity, recent efforts have explored whether the same Transformer architecture can produce numerical representations for DNA sequences. Such models have shown early promise in tasks involving classification of short DNA sequences, such as the detection of coding vs non-coding regions, as well as the identification of enhancer and promoter sequences. Performance at sequence classification tasks does not, however, translate to sequence alignment, where i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#22240;&#26524;&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#21270;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#65292;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.10399</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#21033;&#29992;&#22240;&#26524;&#20449;&#21495;&#30340;&#30740;&#31350;&#65306;&#19968;&#39033;&#24102;&#26377;&#23454;&#35777;&#32467;&#26524;&#30340;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results. (arXiv:2309.10399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#22240;&#26524;&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#21270;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#65292;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#21307;&#23398;&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22330;&#26223;&#20013;&#30340;&#24369;&#22240;&#26524;&#20449;&#21495;&#26469;&#24314;&#27169;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#21644;&#22240;&#26524;&#22240;&#23376;&#25552;&#21462;&#27169;&#22359;&#12290;&#21518;&#32773;&#35745;&#31639;&#29305;&#24449;&#22270;&#30340;&#26435;&#37325;&#65292;&#26681;&#25454;&#20854;&#23545;&#22270;&#20687;&#22330;&#26223;&#30340;&#22240;&#26524;&#24433;&#21709;&#22686;&#24378;&#27599;&#20010;&#29305;&#24449;&#22270;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#22806;&#37096;&#20449;&#21495;&#26469;&#20462;&#25913;&#22240;&#26524;&#27169;&#22359;&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#33719;&#24471;&#25105;&#20204;&#26041;&#27861;&#30340;&#19981;&#21516;&#21464;&#20307;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#23454;&#39564;&#12289;&#23450;&#24615;&#35780;&#20272;&#21644;&#21066;&#24369;&#23454;&#39564;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23545;&#21069;&#21015;&#33146;MRI&#22270;&#20687;&#36827;&#34892;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#36825;&#22312;&#21307;&#30103;&#22270;&#20687;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method for automatically classifying medical images that uses weak causal signals in the scene to model how the presence of a feature in one part of the image affects the appearance of another feature in a different part of the image. Our method consists of two components: a convolutional neural network backbone and a causality-factors extractor module. The latter computes weights for the feature maps to enhance each feature map according to its causal influence in the image's scene. We can modify the functioning of the causality module by using two external signals, thus obtaining different variants of our method. We evaluate our method on a public dataset of prostate MRI images for prostate cancer diagnosis, using quantitative experiments, qualitative assessment, and ablation studies. Our results show that our method improves classification performance and produces more robust predictions, focusing on relevant parts of the image. That is especially important in medic
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#21487;&#26597;&#35810;&#23433;&#20840;&#32422;&#26463;&#27169;&#22359;&#65292;&#29992;&#20110;&#24378;&#21046;LLM&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#36981;&#23432;&#31105;&#27490;&#34892;&#20026;&#30340;&#38480;&#21046;&#65292;&#24182;&#30830;&#20445;&#28385;&#36275;&#20840;&#29699;&#24037;&#19994;&#24037;&#21378;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#37096;&#32626;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.09919</link><description>&lt;p&gt;
&#25554;&#20837;&#23433;&#20840;&#33455;&#29255;&#65306;&#24378;&#21046;LLM&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#32422;&#26463;&#26465;&#20214;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents. (arXiv:2309.09919v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09919
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#21487;&#26597;&#35810;&#23433;&#20840;&#32422;&#26463;&#27169;&#22359;&#65292;&#29992;&#20110;&#24378;&#21046;LLM&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#36981;&#23432;&#31105;&#27490;&#34892;&#20026;&#30340;&#38480;&#21046;&#65292;&#24182;&#30830;&#20445;&#28385;&#36275;&#20840;&#29699;&#24037;&#19994;&#24037;&#21378;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#37096;&#32626;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#25104;&#20026;&#21487;&#33021;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#26399;&#38388;LLMs&#33719;&#24471;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#19968;&#33324;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#26426;&#22120;&#20154;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;LLM&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#24050;&#32463;&#20184;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#25945;&#20250;&#26426;&#22120;&#20154;"&#20570;&#20160;&#20040;"&#65292;&#20294;&#23545;"&#19981;&#35201;&#20570;&#20160;&#20040;"&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#20110;&#20219;&#20309;&#23454;&#38469;&#24212;&#29992;&#32780;&#35328;&#65292;&#25945;&#26426;&#22120;&#20154;"&#19981;&#35201;&#20570;&#20160;&#20040;"&#21516;&#26679;&#37325;&#35201;&#65306;&#20256;&#36798;&#26377;&#20851;&#31105;&#27490;&#34892;&#20026;&#30340;&#26126;&#30830;&#25351;&#31034;&#65292;&#35780;&#20272;&#26426;&#22120;&#20154;&#23545;&#36825;&#20123;&#38480;&#21046;&#30340;&#29702;&#35299;&#65292;&#24182;&#19988;&#26368;&#37325;&#35201;&#30340;&#26159;&#30830;&#20445;&#36981;&#23432;&#36825;&#20123;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#21487;&#39564;&#35777;&#30340;&#23433;&#20840;&#25805;&#20316;&#23545;&#20110;&#28385;&#36275;ISO 61508&#31561;&#20840;&#29699;&#24037;&#19994;&#24037;&#21378;&#29615;&#22659;&#20013;&#30340;&#26426;&#22120;&#20154;&#23433;&#20840;&#37096;&#32626;&#26631;&#20934;&#33267;&#20851;&#37325;&#35201;&#12290;&#38024;&#23545;&#23558;LLM&#20195;&#29702;&#37096;&#32626;&#22312;&#21327;&#21516;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#21487;&#26597;&#35810;&#23433;&#20840;&#32422;&#26463;&#27169;&#22359;&#65292;&#21516;&#26102;&#30830;&#20445;&#23454;&#29616;&#23454;&#26102;&#32422;&#26463;&#26816;&#26597;&#21644;&#33258;&#36866;&#24212;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have enabled a new research domain, LLM agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of LLMs obtained during pretraining. However, while considerable effort has been made to teach the robot the "dos," the "don'ts" received relatively less attention. We argue that, for any practical usage, it is as crucial to teach the robot the "don'ts": conveying explicit instructions about prohibited actions, assessing the robot's comprehension of these restrictions, and, most importantly, ensuring compliance. Moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as ISO 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. Aiming at deploying the LLM agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (LTL) that simult
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item><item><title>&#36807;&#24230;&#21387;&#32553;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#38480;&#21046;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20449;&#24687;&#20256;&#36882;&#65292;&#24433;&#21709;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.15568</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Over-Squashing in Graph Neural Networks: A Comprehensive survey. (arXiv:2308.15568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15568
&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21387;&#32553;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#38480;&#21046;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20449;&#24687;&#20256;&#36882;&#65292;&#24433;&#21709;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#33539; Paradigm&#65292;&#20026;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;GNN&#30340;&#22522;&#26412;&#26550;&#26500;&#28041;&#21450;&#36890;&#36807;&#28040;&#24687;&#32858;&#21512;&#21644;&#36716;&#25442;&#22312;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#30340;&#26426;&#21046;&#65292;&#22312;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#28508;&#22312;&#23454;&#21147;&#36935;&#21040;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22312;&#26576;&#20123;&#24773;&#22659;&#20013;&#65292;&#20934;&#30830;&#30340;&#39044;&#27979;&#19981;&#20165;&#21462;&#20915;&#20110;&#33410;&#28857;&#30340;&#21363;&#26102;&#23616;&#37096;&#29615;&#22659;&#65292;&#36824;&#21462;&#20915;&#20110;&#36328;&#36234;&#24191;&#22495;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23545;&#38271;&#31243;&#20449;&#24687;&#20256;&#25773;&#30340;&#38656;&#27714;&#26292;&#38706;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#8220;&#36807;&#24230;&#21387;&#32553;&#8221;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#20854;&#20013;&#26469;&#33258;&#36828;&#31163;&#33410;&#28857;&#30340;&#20449;&#24687;&#27969;&#30340;&#21487;&#38752;&#24615;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a revolutionary paradigm in the realm of machine learning, offering a transformative approach to dissect intricate relationships inherent in graph-structured data. The foundational architecture of most GNNs involves the dissemination of information through message aggregation and transformation among interconnected nodes, a mechanism that has demonstrated remarkable efficacy across diverse applications encompassing node classification, link prediction, and recommendation systems. Nonetheless, their potential prowess encounters a restraint intrinsic to scenarios necessitating extensive contextual insights. In certain contexts, accurate predictions hinge not only upon a node's immediate local surroundings but also on interactions spanning far-reaching domains. This intricate demand for long-range information dissemination exposes a pivotal challenge recognized as "over-squashing," wherein the fidelity of information flow from distant nodes bec
&lt;/p&gt;</description></item><item><title>FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12532</link><description>&lt;p&gt;
FedSoL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12532
&lt;/p&gt;
&lt;p&gt;
FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#20010;&#20307;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;FL&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#36817;&#20284;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#32422;&#26463;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#23616;&#37096;&#23398;&#20064;&#19982;&#20840;&#23616;&#30446;&#26631;&#30340;&#20559;&#31163;&#26469;&#20419;&#36827;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#36890;&#36807;&#24178;&#25200;&#21407;&#22987;&#30340;&#23616;&#37096;&#30446;&#26631;&#32780;&#38480;&#21046;&#20102;&#23616;&#37096;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25913;&#21892;&#26412;&#22320;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#12290;&#36890;&#36807;&#22312;&#24179;&#28369;&#30340;&#25439;&#22833;&#31354;&#38388;&#20013;&#33719;&#24471;&#26412;&#22320;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#19981;&#21516;&#26412;&#22320;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#23427;&#19981;&#33021;&#30830;&#20445;&#31283;&#23450;&#30340;&#20840;&#23616;&#23545;&#40784;&#65292;&#22240;&#20026;&#26412;&#22320;&#23398;&#20064;&#19981;&#32771;&#34385;&#20840;&#23616;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;(FedSoL)&#26041;&#27861;&#26469;&#22312;FL&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#31614;&#20043;&#38388;&#30340;&#36317;&#31163;&#20851;&#31995;&#26469;&#35843;&#25972;&#24050;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#26032;&#31867;&#21035;&#25110;&#25913;&#21892;&#38646;&#26679;&#26412;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.12226</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20960;&#20309;&#24863;&#30693;&#33258;&#36866;&#24212;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Adaptation for Pretrained Models. (arXiv:2307.12226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#31614;&#20043;&#38388;&#30340;&#36317;&#31163;&#20851;&#31995;&#26469;&#35843;&#25972;&#24050;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#26032;&#31867;&#21035;&#25110;&#25913;&#21892;&#38646;&#26679;&#26412;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#33879;&#21517;&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#65292;&#36890;&#24120;&#22312;&#20165;&#20855;&#26377;&#36739;&#23567;&#27604;&#20363;&#26631;&#31614;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#26631;&#31614;&#31354;&#38388;&#36890;&#24120;&#20351;&#29992;&#24230;&#37327;&#26469;&#34913;&#37327;&#26631;&#31614;&#20043;&#38388;&#30340;&#36317;&#31163;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#23558;&#24050;&#35757;&#32451;&#30340;&#27169;&#22411;&#35843;&#25972;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#26032;&#31867;&#21035;&#65292;&#25110;&#32773;&#22312;&#38646;&#26679;&#26412;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#26631;&#20934;&#39044;&#27979;&#35268;&#21017;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#20854;&#20013;&#23558;argmax&#26367;&#25442;&#20026;Fr&#233;chet&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#20026;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#65288;i&#65289;&#23398;&#20064;&#29702;&#35770;&#32467;&#26524;&#65292;&#26435;&#34913;&#26631;&#31614;&#31354;&#38388;&#30452;&#24452;&#12289;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#32500;&#24230;&#65292;&#65288;ii&#65289;&#34920;&#24449;&#21487;&#33021;&#39044;&#27979;&#20219;&#20309;&#26410;&#35266;&#23519;&#21040;&#30340;&#31867;&#21035;&#30340;&#25152;&#26377;&#24773;&#26223;&#30340;&#29305;&#24449;&#65292;&#65288;iii&#65289;&#19968;&#31181;&#26368;&#20248;&#30340;&#20027;&#21160;&#23398;&#20064;&#24335;&#19979;&#19968;&#31867;&#21035;&#36873;&#25321;&#36807;&#31243;&#65292;&#20197;&#33719;&#21462;&#26368;&#20339;&#30340;&#35757;&#32451;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models -- including prominent zero-shot models -- are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes -- or, in the case of zero-shot prediction, to improve its performance -- without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping argmax with the Fr\'echet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#65292;&#25552;&#20986;&#20102;SN-Netv2&#65292;&#23427;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#23454;&#29616;&#22810;&#26679;&#24615;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.00154</link><description>&lt;p&gt;
Stitched ViTs&#26159;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stitched ViTs are Flexible Vision Backbones. (arXiv:2307.00154v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#65292;&#25552;&#20986;&#20102;SN-Netv2&#65292;&#23427;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#23454;&#29616;&#22810;&#26679;&#24615;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#26222;&#36890;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#21147;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;ViTs&#30340;&#29616;&#26377;&#24037;&#20316;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#37319;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;ViTs&#38656;&#35201;&#21333;&#29420;&#35757;&#32451;&#65292;&#24182;&#21463;&#21040;&#22266;&#23450;&#30340;&#24615;&#33021;-&#25928;&#29575;&#26435;&#34913;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#21487;&#25340;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#26469;&#24555;&#36895;&#29983;&#25104;&#28085;&#30422;&#20016;&#23500;&#23376;&#32593;&#32476;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#25903;&#25345;&#22312;&#36816;&#34892;&#26102;&#30340;&#22810;&#26679;&#24615;&#24615;&#33021;-&#25928;&#29575;&#26435;&#34913;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SN-Netv2&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#25913;&#36827;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#20419;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#25340;&#25509;&#26041;&#26696;&#26469;&#25193;&#22823;&#25340;&#25509;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32771;&#34385;&#31354;&#38388;&#20013;&#24213;&#23618;FLOPs&#20998;&#24067;&#30340;&#36164;&#28304;&#21463;&#38480;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#37319;&#26679;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;SN-Netv2&#36827;&#34892;&#20102;&#32454;&#24494;&#35843;&#25972;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained plain vision Transformers (ViTs) have been the workhorse for many downstream tasks. However, existing works utilizing off-the-shelf ViTs are inefficient in terms of training and deployment, because adopting ViTs with individual sizes requires separate training and is restricted by fixed performance-efficiency trade-offs. In this paper, we are inspired by stitchable neural networks, which is a new framework that cheaply produces a single model that covers rich subnetworks by stitching pretrained model families, supporting diverse performance-efficiency trade-offs at runtime. Building upon this foundation, we introduce SN-Netv2, a systematically improved model stitching framework to facilitate downstream task adaptation. Specifically, we first propose a Two-way stitching scheme to enlarge the stitching space. We then design a resource-constrained sampling strategy that takes into account the underlying FLOPs distributions in the space for improved sampling. Finally, we o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Opti-Mile"&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26368;&#21518;&#19968;&#20844;&#37324;&#26381;&#21153;&#19982;&#20844;&#20849;&#20132;&#36890;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#25143;&#26080;&#38656;&#25442;&#20056;&#65292;&#35299;&#20915;&#20102;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#26377;&#38480;&#21487;&#36798;&#24615;&#21644;&#25442;&#20056;&#24341;&#36215;&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15943</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#25442;&#20056;&#65306;&#20351;&#29992;Opti-Mile&#23558;&#26368;&#21518;&#19968;&#20844;&#37324;&#19982;&#20844;&#20849;&#20132;&#36890;&#25972;&#21512;&#22312;&#19968;&#36215;
&lt;/p&gt;
&lt;p&gt;
No Transfers Required: Integrating Last Mile with Public Transit Using Opti-Mile. (arXiv:2306.15943v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15943
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Opti-Mile"&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26368;&#21518;&#19968;&#20844;&#37324;&#26381;&#21153;&#19982;&#20844;&#20849;&#20132;&#36890;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#25143;&#26080;&#38656;&#25442;&#20056;&#65292;&#35299;&#20915;&#20102;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#26377;&#38480;&#21487;&#36798;&#24615;&#21644;&#25442;&#20056;&#24341;&#36215;&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20844;&#20849;&#20132;&#36890;&#20855;&#26377;&#32463;&#27982;&#23454;&#24800;&#30340;&#20248;&#28857;&#65292;&#20294;&#30001;&#20110;&#22823;&#37096;&#20998;&#22320;&#21306;&#38656;&#35201;&#25442;&#20056;&#65292;&#23548;&#33268;&#19981;&#20415;&#21033;&#12290;&#20026;&#20102;&#35299;&#20915;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#26377;&#38480;&#30340;&#21487;&#36798;&#24615;&#21644;&#25442;&#20056;&#24341;&#36215;&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26053;&#34892;&#35745;&#21010;&#26041;&#27861;&#65292;&#21363;"Opti-Mile"&#65292;&#23427;&#23558;&#26368;&#21518;&#19968;&#20844;&#37324;&#26381;&#21153;&#19982;&#20844;&#20849;&#20132;&#36890;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#25143;&#26080;&#38656;&#25442;&#20056;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public transit is a popular mode of transit due to its affordability, despite the inconveniences due to the necessity of transfers required to reach most areas. For example, in the bus and metro network of New Delhi, only 30\% of stops can be directly accessed from any starting point, thus requiring transfers for most commutes. Additionally, last-mile services like rickshaws, tuk-tuks or shuttles are commonly used as feeders to the nearest public transit access points, which further adds to the complexity and inefficiency of a journey. Ultimately, users often face a tradeoff between coverage and transfers to reach their destination, regardless of the mode of transit or the use of last-mile services. To address the problem of limited accessibility and inefficiency due to transfers in public transit systems, we propose ``opti-mile," a novel trip planning approach that combines last-mile services with public transit such that no transfers are required. Opti-mile allows users to customise 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.11363</link><description>&lt;p&gt;
&#21463;&#36974;&#34109;&#25193;&#25955;&#27169;&#22411;&#26159;&#24555;&#36895;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11363
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#30340;&#20107;&#23454;&#19978;&#25216;&#26415;&#65292;&#28982;&#32780;&#23427;&#20204;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#35813;&#25216;&#26415;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#32435;&#20837;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#36890;&#36807;&#36974;&#34109;&#36755;&#20837;&#22270;&#20687;&#30340;&#39640;&#27604;&#20363;&#65288;&#20363;&#22914;&#39640;&#36798;90&#65285;&#65289;&#65292;&#24182;&#21033;&#29992;&#36974;&#34109;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#26469;&#21435;&#22122;&#21487;&#35265;&#21306;&#22495;&#65292;&#20174;&#32780;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#26356;&#26174;&#33879;&#30340;&#29305;&#24449;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#36974;&#34109;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;CelebA-HQ $256 \times 256$&#20687;&#32032;&#31354;&#38388;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#20102;&#22522;&#20110;ViT&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;4&#20493;&#21152;&#36895;&#65292;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#19982;&#21435;&#22122;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as the \emph{de-facto} technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community. We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks. Our approach centers on masking a high proportion (e.g., up to 90\%) of the input image and employing masked denoising score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge. By utilizing masked learning in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to denoisin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.07745</link><description>&lt;p&gt;
&#26680;&#21270;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Kernelized Reinforcement Learning with Order Optimal Regret Bounds. (arXiv:2306.07745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#20855;&#26377;&#22797;&#26434;&#27169;&#22411;&#21644;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26174;&#31034;&#20986;&#20102;&#23454;&#35777;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#32467;&#26524;&#36890;&#24120;&#38598;&#20013;&#20110;&#20855;&#26377;&#23569;&#37327;&#29366;&#24577;-&#34892;&#20026;&#25110;&#31616;&#21333;&#27169;&#22411;&#65288;&#20363;&#22914;&#32447;&#24615;&#24314;&#27169;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#65289;&#30340;&#35774;&#32622;&#12290; &#20026;&#20102;&#25512;&#23548;&#26377;&#25928;&#22788;&#29702;&#26356;&#24191;&#27867;&#20540;&#20989;&#25968;&#30340;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;RL&#31574;&#30053;&#65292;&#19968;&#20123;&#26368;&#26032;&#24037;&#20316;&#32771;&#34385;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;$\pi$-KRVI&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#30340;&#19968;&#31181;&#20048;&#35266;&#20462;&#25913;&#65292;&#24403;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#30001;RKHS&#34920;&#31034;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#39640;&#24230;&#38750;&#20809;&#28369;&#20869;&#26680;&#65288;&#20363;&#22914;&#31070;&#32463;&#20999;&#21521;&#20869;&#26680;&#25110;&#26576;&#20123;Mat\'ern&#20869;&#26680;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose $\pi$-KRVI, an optimistic modification of least-squares value iteration, when the state-action value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Mat\'ern kernels) the existing results lead to trivial (superl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18766</link><description>&lt;p&gt;
HiFA: &#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#21450;&#20854;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. (arXiv:2305.18766v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;3D&#27169;&#22411;&#65292;&#33258;&#21160;&#25991;&#26412;&#21040;3D&#21512;&#25104;&#22312;&#25552;&#21319;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#25552;&#20379;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#30340;2D&#28210;&#26579;&#24471;&#20998;&#24182;&#29992;&#20110;&#20248;&#21270;NeRFs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#23545;3D&#20960;&#20309;&#30340;&#26377;&#38480;&#29702;&#35299;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#36935;&#21040;&#22810;&#20010;&#35270;&#35282;&#19978;&#30340;&#20266;&#24433;&#21644;&#19981;&#19968;&#33268;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#20808;&#39564;&#37325;&#26032;&#21046;&#23450;&#20248;&#21270;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#37322;&#25918;&#20102;&#25193;&#25955;&#20808;&#39564;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#65292;&#25105;&#20204;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#65292;&#24182;&#35268;&#33539;&#21270;NeRF&#30340;&#23494;&#24230;&#22330;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic text-to-3D synthesis has achieved remarkable advancements through the optimization of 3D models. Existing methods commonly rely on pre-trained text-to-image generative models, such as diffusion models, providing scores for 2D renderings of Neural Radiance Fields (NeRFs) and being utilized for optimizing NeRFs. However, these methods often encounter artifacts and inconsistencies across multiple views due to their limited understanding of 3D geometry. To address these limitations, we propose a reformulation of the optimization loss using the diffusion prior. Furthermore, we introduce a novel training approach that unlocks the potential of the diffusion prior. To improve 3D geometry representation, we apply auxiliary depth supervision for NeRF-rendered images and regularize the density field of NeRFs. Extensive experiments demonstrate the superiority of our method over prior works, resulting in advanced photo-realism and improved multi-view consistency.
&lt;/p&gt;</description></item><item><title>Sim-Suction &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#20307;&#24863;&#30693;&#21560;&#30424;&#25235;&#21462;&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#19979;&#24863;&#20852;&#36259;&#30340;&#29289;&#20307;&#19982;&#21608;&#22260;&#29615;&#22659;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16378</link><description>&lt;p&gt;
Sim-Suction: &#20351;&#29992;&#21512;&#25104;&#22522;&#20934;&#23398;&#20064;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#30340;&#21560;&#30424;&#25235;&#21462;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Sim-Suction: Learning a Suction Grasp Policy for Cluttered Environments Using a Synthetic Benchmark. (arXiv:2305.16378v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16378
&lt;/p&gt;
&lt;p&gt;
Sim-Suction &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#20307;&#24863;&#30693;&#21560;&#30424;&#25235;&#21462;&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#19979;&#24863;&#20852;&#36259;&#30340;&#29289;&#20307;&#19982;&#21608;&#22260;&#29615;&#22659;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Sim-Suction&#65292;&#19968;&#31181;&#38024;&#23545;&#31227;&#21160;&#25805;&#20316;&#24179;&#21488;&#30340;&#12289;&#36866;&#29992;&#20110;&#20855;&#26377;&#21160;&#24577;&#25668;&#20687;&#26426;&#35270;&#35282;&#30340;&#12289;&#29992;&#20110;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25342;&#21462;&#26410;&#30693;&#29289;&#20307;&#30340;&#40065;&#26834;&#24615;&#29289;&#20307;&#24863;&#30693;&#21560;&#30424;&#25235;&#21462;&#31574;&#30053;&#12290;&#36890;&#24120;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23454;&#29616;&#21560;&#30424;&#25235;&#21462;&#31574;&#30053;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#12289;&#31934;&#30830;&#27880;&#37322;&#30340;&#21560;&#30424;&#25235;&#21462;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#29983;&#25104;&#21560;&#30424;&#25235;&#21462;&#25968;&#25454;&#38598;&#20173;&#28982;&#32570;&#20047;&#30740;&#31350;&#65292;&#23384;&#22312;&#30528;&#20851;&#20110;&#24863;&#20852;&#36259;&#30340;&#29289;&#20307;&#19982;&#20854;&#21608;&#22260;&#29615;&#22659;&#20043;&#38388;&#20851;&#31995;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#21512;&#25104;&#25968;&#25454;&#38598; Sim-Suction-Dataset&#65292;&#21253;&#25324; 500 &#20010;&#26434;&#20081;&#29615;&#22659;&#21644; 320 &#19975;&#20010;&#27880;&#37322;&#30340;&#21560;&#30424;&#25235;&#21462;&#23039;&#24577;&#12290;&#39640;&#25928;&#30340; Sim-Suction-Dataset &#29983;&#25104;&#36807;&#31243;&#36890;&#36807;&#23558;&#20998;&#26512;&#27169;&#22411;&#19982;&#21160;&#24577;&#29289;&#29702;&#27169;&#25311;&#30456;&#32467;&#21512;&#26469;&#21019;&#24314;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#21560;&#30424;&#25235;&#21462;&#23039;&#24577;&#27880;&#37322;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Sim-Suction-Pointnet&#65292;&#36890;&#36807;&#23398;&#20064;&#28857;&#26469;&#29983;&#25104;&#40065;&#26834;&#30340; 6D &#21560;&#30424;&#25235;&#21462;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Sim-Suction, a robust object-aware suction grasp policy for mobile manipulation platforms with dynamic camera viewpoints, designed to pick up unknown objects from cluttered environments. Suction grasp policies typically employ data-driven approaches, necessitating large-scale, accurately-annotated suction grasp datasets. However, the generation of suction grasp datasets in cluttered environments remains underexplored, leaving uncertainties about the relationship between the object of interest and its surroundings. To address this, we propose a benchmark synthetic dataset, Sim-Suction-Dataset, comprising 500 cluttered environments with 3.2 million annotated suction grasp poses. The efficient Sim-Suction-Dataset generation process provides novel insights by combining analytical models with dynamic physical simulations to create fast and accurate suction grasp pose annotations. We introduce Sim-Suction-Pointnet to generate robust 6D suction grasp poses by learning poin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#25552;&#39640;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14726</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Demonstration Selection with Cross Entropy Difference. (arXiv:2305.14726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#25552;&#39640;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#25552;&#39640;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#22240;&#25152;&#36873;&#31034;&#20363;&#32780;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#65288;CED&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#19978;&#19979;&#25991;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65306;&#22312;&#26377;&#38480;&#35843;&#25972;&#20102;&#36825;&#20123;&#28436;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#26377;&#25928;&#24615;&#19982;&#27979;&#35797;&#31034;&#20363;&#30340;&#22256;&#24785;&#24230;&#21576;&#36127;&#30456;&#20851;&#12290;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#65292;&#20197;&#35745;&#31639;&#27979;&#35797;&#31034;&#20363;&#21644;&#27599;&#20010;&#20505;&#36873;&#19978;&#19979;&#25991;&#28436;&#31034;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#24046;&#24322;&#12290;&#35813;&#25351;&#26631;&#29992;&#20110;&#20026;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#29420;&#31435;&#22320;&#25490;&#21517;&#21644;&#36873;&#25321;&#19978;&#19979;&#25991;&#28436;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#28151;&#21512;&#22495;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#32467;&#21512;&#20102;8&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20195;&#34920;4&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;CED&#22312;&#19978;&#19979;&#25991;&#28436;&#31034;&#36873;&#25321;&#26041;&#38754;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method for selecting in-context demonstrations. Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration. We utilize parameter efficient finetuning to train small models on training data that are used for computing the cross-entropy difference between a test example and every candidate in-context demonstration. This metric is used to rank and select in-context demonstrations independently for each test input. We evaluate our method on a mix-domain dataset that combines 8 benchmarks, representing 4 text generation tasks, showing that CED for in-co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14561</link><description>&lt;p&gt;
&#36127;&#21453;&#39304;&#35757;&#32451;&#65306;&#25552;&#39640;NVCiM DNN&#21152;&#36895;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#25381;&#21457;&#24615;&#23384;&#20648;&#22120;(NVM)&#23454;&#29616;&#30340;&#20869;&#23384;&#35745;&#31639;(CiM)&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290; CiM&#21152;&#36895;&#22120;&#36890;&#36807;&#22312;&#21516;&#19968;&#30005;&#36335;&#26495;&#32467;&#26500;&#20013;&#23384;&#20648;&#32593;&#32476;&#26435;&#37325;&#21644;&#25191;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20197;&#26368;&#23567;&#30340;&#38754;&#31215;&#38656;&#27714;&#21644;&#24322;&#24120;&#30340;&#33021;&#25928;&#65292;&#25552;&#20379;DNN&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;NVM&#35774;&#22791;&#30340;&#38543;&#26426;&#24615;&#21644;&#20869;&#22312;&#21464;&#21270;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#22914;&#19982;&#39044;&#26399;&#32467;&#26524;&#30456;&#27604;&#20943;&#23569;&#20998;&#31867;&#31934;&#24230;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#35774;&#22791;&#21464;&#24322;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#25972;&#20307;&#35843;&#33410;&#24182;&#32570;&#20047;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#21463;&#21040;&#36127;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20986;&#21475;&#26426;&#21046;&#20316;&#20026;&#36127;&#21453;&#39304;&#65292;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
&lt;/p&gt;</description></item><item><title>Marsellus&#26159;&#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC&#65292;&#36866;&#29992;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#20197;&#21450;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.08415</link><description>&lt;p&gt;
Marsellus: &#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC
&lt;/p&gt;
&lt;p&gt;
Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing. (arXiv:2305.08415v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08415
&lt;/p&gt;
&lt;p&gt;
Marsellus&#26159;&#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC&#65292;&#36866;&#29992;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#20197;&#21450;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#20114;&#32852;&#29289;&#32852;&#32593;&#65288;AI-IoT&#65289;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoC&#65289;&#38656;&#35201;&#22312;&#33539;&#22260;&#24191;&#27867;&#30340;&#24037;&#20316;&#26465;&#20214;&#19979;&#65292;&#22312;&#20960;&#21313;&#27627;&#29926;&#30340;&#21151;&#32791;&#38480;&#21046;&#19979;&#36816;&#34892;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#23494;&#38598;&#22411;&#20294;&#24378;&#37327;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#29702;&#20197;&#21450;&#38656;&#35201;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Marsellus&#65292;&#19968;&#20010;&#22312;GlobalFoundries 22nm FDX&#19978;&#21046;&#36896;&#30340;&#20840;&#25968;&#23383;&#24322;&#26500;SoC&#65292;&#29992;&#20110;AI-IoT&#26411;&#31471;&#33410;&#28857;&#65292;&#23427;&#32467;&#21512;&#20102;&#65306;1&#65289;&#19968;&#20010;16&#20010;RISC-V&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DSP&#65289;&#26680;&#24515;&#30340;&#36890;&#29992;&#38598;&#32676;&#65292;&#29992;&#20110;&#25191;&#34892;&#21508;&#31181;&#25903;&#25345;4&#20301;&#21644;2&#20301;&#31639;&#26415;&#25193;&#23637;&#65288;XpulpNN&#65289;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#34701;&#21512;&#30340;MAC&#21644;LOAD&#25805;&#20316;&#21644;&#28014;&#28857;&#25903;&#25345;&#65307;2&#65289;&#19968;&#20010;2-8&#20301;&#21487;&#37325;&#26500;&#20108;&#36827;&#21046;&#24341;&#25806;&#65288;RBE&#65289;&#65292;&#29992;&#20110;&#21152;&#36895;DNN&#20013;&#30340;3x3&#21644;1x1&#65288;&#36880;&#28857;&#65289;&#21367;&#31215;&#65307;3&#65289;&#19968;&#32452;&#36830;&#25509;&#21040;&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#29255;&#19978;&#30417;&#35270;&#65288;OCM&#65289;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging Artificial Intelligence-enabled Internet-of-Things (AI-IoT) System-on-a-Chip (SoC) for augmented reality, personalized healthcare, and nano-robotics need to run many diverse tasks within a power envelope of a few tens of mW over a wide range of operating conditions: compute-intensive but strongly quantized Deep Neural Network (DNN) inference, as well as signal processing and control requiring high-precision floating-point. We present Marsellus, an all-digital heterogeneous SoC for AI-IoT end-nodes fabricated in GlobalFoundries 22nm FDX that combines 1) a general-purpose cluster of 16 RISC-V Digital Signal Processing (DSP) cores attuned for the execution of a diverse range of workloads exploiting 4-bit and 2-bit arithmetic extensions (XpulpNN), combined with fused MAC&amp;LOAD operations and floating-point support; 2) a 2-8bit Reconfigurable Binary Engine (RBE) to accelerate 3x3 and 1x1 (pointwise) convolutions in DNNs; 3) a set of On-Chip Monitoring (OCM) blocks connected to an Ad
&lt;/p&gt;</description></item><item><title>AGI&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#65292;&#21487;&#20197;&#24314;&#31435;e-learning&#24179;&#21488;&#12289;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#65292;&#24357;&#34917;&#20256;&#32479;AI&#27169;&#22411;&#22240;&#21463;&#38480;&#20110;&#25968;&#25454;&#21644;&#20154;&#38469;&#20132;&#20114;&#38480;&#21046;&#32780;&#26080;&#27861;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2304.12479</link><description>&lt;p&gt;
&#29992;&#20110;&#25945;&#32946;&#30340;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence (AGI) for Education. (arXiv:2304.12479v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12479
&lt;/p&gt;
&lt;p&gt;
AGI&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#65292;&#21487;&#20197;&#24314;&#31435;e-learning&#24179;&#21488;&#12289;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#65292;&#24357;&#34917;&#20256;&#32479;AI&#27169;&#22411;&#22240;&#21463;&#38480;&#20110;&#25968;&#25454;&#21644;&#20154;&#38469;&#20132;&#20114;&#38480;&#21046;&#32780;&#26080;&#27861;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;&#22914;GPT-4&#21644;ChatGPT&#65289;&#30340;&#20986;&#29616;&#65292;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#20316;&#20026;&#26410;&#26469;&#25216;&#26415;&#24050;&#32463;&#24471;&#21040;&#20840;&#29699;&#35748;&#21487;&#12290;AGI&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#26426;&#31995;&#32479;&#22797;&#21046;&#20154;&#31867;&#26234;&#33021;&#65292;&#26159;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#38024;&#23545;&#26377;&#38480;&#33539;&#22260;&#30340;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#65292;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#33021;&#26080;&#27861;&#32771;&#34385;&#25945;&#32946;&#20013;&#22797;&#26434;&#30340;&#20154;&#38469;&#21160;&#24577;&#12290;&#21463;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#39537;&#21160;&#65292;AGI&#20195;&#34920;&#20102;&#26426;&#22120;&#22312;&#25191;&#34892;&#38656;&#35201;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#20363;&#22914;&#25512;&#29702;&#12289;&#35299;&#20915;&#38382;&#39064;&#12289;&#20570;&#20986;&#20915;&#31574;&#65292;&#29978;&#33267;&#29702;&#35299;&#20154;&#31867;&#24773;&#24863;&#21644;&#31038;&#20132;&#20114;&#21160;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;AGI&#30340;&#20851;&#38190;&#27010;&#24565;&#12289;&#33021;&#21147;&#12289;&#33539;&#22260;&#21644;&#22312;&#26410;&#26469;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#24314;&#31435;e-learning&#24179;&#21488;&#21644;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. AGI aims to replicate human intelligence through computer systems, which is one of the critical technologies having the potential to revolutionize the field of education. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This work reviews AGI's key concepts, capabilities, scope, and potential within future education, including setting e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.18158</link><description>&lt;p&gt;
&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#31209;&#19968;&#20989;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Optimization of Rank-One Functions with Indicator Variables. (arXiv:2303.18158v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#28041;&#21450;&#21040;&#36890;&#36807;&#32422;&#26463;&#26469;&#24314;&#27169;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#37319;&#29992;&#25351;&#26631;&#21464;&#37327;&#26469;&#35782;&#21035;&#36830;&#32493;&#21464;&#37327;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#36890;&#36807;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25903;&#25345;&#20989;&#25968;&#21442;&#25968;&#21644;&#31163;&#25955;&#35268;&#21010;&#25216;&#26415;&#20197;&#25552;&#20379;&#20984;&#21253;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26041;&#27861;&#65292;&#21033;&#29992;&#36879;&#35270;&#20989;&#25968;&#24341;&#36215;&#30340;&#38544;&#34255;&#22278;&#38181;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#27599;&#20010;&#22278;&#38181;&#32422;&#26463;&#28041;&#21450;&#29420;&#31435;&#36830;&#32493;&#21464;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#21644;&#19968;&#32452;&#20108;&#20803;&#21464;&#37327;&#30340;&#19968;&#33324;&#22278;&#38181;&#28151;&#21512;&#20108;&#36827;&#21046;&#38598;&#21512;&#24314;&#31435;&#20102;&#19968;&#20010;&#20984;&#21253;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#24212;&#23545;epi&#30456;&#20851;&#30340;&#38598;&#21512;&#30340;&#25193;&#23637;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems involving minimization of a rank-one convex function over constraints modeling restrictions on the support of the decision variables emerge in various machine learning applications. These problems are often modeled with indicator variables for identifying the support of the continuous variables. In this paper we investigate compact extended formulations for such problems through perspective reformulation techniques. In contrast to the majority of previous work that relies on support function arguments and disjunctive programming techniques to provide convex hull results, we propose a constructive approach that exploits a hidden conic structure induced by perspective functions. To this end, we first establish a convex hull result for a general conic mixed-binary set in which each conic constraint involves a linear function of independent continuous variables and a set of binary variables. We then demonstrate that extended representations of sets associated with epi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23545;&#20110;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#35753;&#20154;&#31867;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#25152;&#20570;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.11712</link><description>&lt;p&gt;
&#39640;&#25928;&#35299;&#37322; CSPs &#30340;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#20248;&#21270;&#65288;&#25193;&#23637;&#31639;&#27861;&#21644;&#31034;&#20363;&#65289;
&lt;/p&gt;
&lt;p&gt;
Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended algorithms and examples). (arXiv:2303.11712v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23545;&#20110;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#35753;&#20154;&#31867;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#25152;&#20570;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#30784;&#19978;&#65292;&#20026;&#36880;&#27493;&#20197;&#26131;&#20110;&#29702;&#35299;&#26041;&#24335;&#35299;&#37322;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSP&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#37324;&#30340;&#35299;&#37322;&#26159;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#25512;&#26029;&#27493;&#39588;&#65292;&#20854;&#20013;&#31616;&#21333;&#24615;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#37327;&#21270;&#12290;&#35299;&#37322;&#29983;&#25104;&#31639;&#27861;&#20381;&#36182;&#20110;&#20174;&#23548;&#20986;&#30340;&#19981;&#21487;&#28385;&#36275;&#20844;&#24335;&#20013;&#25552;&#21462;&#26368;&#23567;&#19981;&#28385;&#23376;&#38598;&#65288;MUS&#65289;&#65292;&#21033;&#29992;&#25152;&#35859;&#30340;&#38750;&#20887;&#20313;&#35299;&#37322;&#21644; MUS &#20043;&#38388;&#30340;&#19968;&#19968;&#23545;&#24212;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;MUS &#25552;&#21462;&#31639;&#27861;&#19981;&#25552;&#20379;&#20219;&#20309;&#38024;&#23545;&#32473;&#23450;&#25104;&#26412;&#20989;&#25968;&#30340;&#23376;&#38598;&#26368;&#23567;&#24615;&#25110;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#24418;&#24335;&#22522;&#30784;&#19978;&#24314;&#31435;&#65292;&#24182;&#30528;&#25163;&#25913;&#36827;&#30340;&#20027;&#35201;&#35201;&#28857;&#65292;&#21363;&#22914;&#20309;&#39640;&#25928;&#22320;&#29983;&#25104;&#21487;&#35777;&#26126;&#26159;&#26368;&#20248;&#30340;&#35299;&#37322;&#65288;&#19982;&#32473;&#23450;&#25104;&#26412;&#24230;&#37327;&#30456;&#20851;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#65288;1&#65289;&#22522;&#20110;&#21629;&#20013;&#38598;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26597;&#25214;&#26368;&#20339;&#21463;&#38480;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#65307;&#65288;2&#65289;&#19968;&#31181;&#37325;&#29992;&#30456;&#20851;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#21516;&#30340;&#35299;&#37322;&#29983;&#25104;&#38454;&#27573;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build on a recently proposed method for stepwise explaining solutions of Constraint Satisfaction Problems (CSP) in a human-understandable way. An explanation here is a sequence of simple inference steps where simplicity is quantified using a cost function. The algorithms for explanation generation rely on extracting Minimal Unsatisfiable Subsets (MUS) of a derived unsatisfiable formula, exploiting a one-to-one correspondence between so-called non-redundant explanations and MUSs. However, MUS extraction algorithms do not provide any guarantee of subset minimality or optimality with respect to a given cost function. Therefore, we build on these formal foundations and tackle the main points of improvement, namely how to generate explanations efficiently that are provably optimal (with respect to the given cost metric). For that, we developed (1) a hitting set-based algorithm for finding the optimal constrained unsatisfiable subsets; (2) a method for re-using relevant information over m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#19979;&#23545;&#23156;&#20799;&#33041; MRI &#20013;&#20122;&#30382;&#36136;&#21306;&#22495;&#30340;&#20998;&#21106;&#65292;&#20805;&#20998;&#32771;&#34385;&#19981;&#21516; MRI &#25195;&#25551;&#20202;&#12289;&#20379;&#24212;&#21830;&#25110;&#37319;&#38598;&#24207;&#21015;&#20197;&#21450;&#19981;&#21516;&#30340;&#31070;&#32463;&#21457;&#32946;&#38454;&#27573;&#25152;&#36896;&#25104;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09373</link><description>&lt;p&gt;
3D&#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#29992;&#20110;&#24322;&#26500;&#23156;&#20799;&#33041; MRI &#39046;&#22495;&#38388;&#36866;&#24212;&#24615;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI. (arXiv:2303.09373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#19979;&#23545;&#23156;&#20799;&#33041; MRI &#20013;&#20122;&#30382;&#36136;&#21306;&#22495;&#30340;&#20998;&#21106;&#65292;&#20805;&#20998;&#32771;&#34385;&#19981;&#21516; MRI &#25195;&#25551;&#20202;&#12289;&#20379;&#24212;&#21830;&#25110;&#37319;&#38598;&#24207;&#21015;&#20197;&#21450;&#19981;&#21516;&#30340;&#31070;&#32463;&#21457;&#32946;&#38454;&#27573;&#25152;&#36896;&#25104;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23156;&#20799;&#33041; MRI &#22312;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#12289;&#36328;&#22330;&#26223;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#20998;&#21106;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#33945;&#29256;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#26469;&#23545;&#23156;&#20799;&#33041;MRI&#30340;&#19981;&#21516;&#20122;&#30382;&#36136;&#21306;&#22495;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#32852;&#21512;&#23398;&#20064;&#26631;&#35760;&#28304;&#22495;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust segmentation of infant brain MRI across multiple ages, modalities, and sites remains challenging due to the intrinsic heterogeneity caused by different MRI scanners, vendors, or acquisition sequences, as well as varying stages of neurodevelopment. To address this challenge, previous studies have explored domain adaptation (DA) algorithms from various perspectives, including feature alignment, entropy minimization, contrast synthesis (style transfer), and pseudo-labeling. This paper introduces a novel framework called MAPSeg (Masked Autoencoding and Pseudo-labelling Segmentation) to address the challenges of cross-age, cross-modality, and cross-site segmentation of subcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as well as masked pseudo-labeling, the model is able to jointly learn from labeled source domain data and unlabeled target domain data. We evaluated our framework on expert-annotated datasets acquired from different ages and sites. MAPSeg consist
&lt;/p&gt;</description></item><item><title>FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.01928</link><description>&lt;p&gt;
&#22522;&#20110;Shapley&#20540;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#25968;&#25454;&#20877;&#21152;&#26435;&#26041;&#27861;FairShap
&lt;/p&gt;
&lt;p&gt;
FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values. (arXiv:2303.01928v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01928
&lt;/p&gt;
&lt;p&gt;
FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#26159;&#26497;&#20854;&#37325;&#35201;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#28982;&#32780;&#24403;&#21069;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#35201;&#27714;&#20351;&#29992;&#36890;&#24120;&#23384;&#22312;&#20559;&#24046;&#30340;&#28023;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19987;&#27880;&#20110;&#24314;&#27169;&#21644;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Shapley&#20540;&#36827;&#34892;&#25968;&#25454;&#20272;&#20540;&#30340;&#39044;&#22788;&#29702;&#65288;&#20877;&#21152;&#26435;&#65289;&#26041;&#27861;FairShap&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#31639;&#27861;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#19988;&#26131;&#20110;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#34913;&#37327;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#23545;&#39044;&#23450;&#20041;&#30340;&#20844;&#24179;&#25351;&#26631;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#24615;&#36136;&#65292;&#26377;&#21508;&#31181;&#22521;&#35757;&#22330;&#26223;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20135;&#29983;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#24182;&#19988;&#20934;&#30830;&#24230;&#26356;&#39640;&#25110;&#30456;&#20284;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30452;&#26041;&#22270;&#21644;&#28508;&#31354;&#38388;&#21487;&#35270;&#21270;&#26469;&#35828;&#26126;FairShap&#30340;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#23545;&#20110;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#30830;&#20445;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness is of utmost societal importance, yet the current trend in large-scale machine learning models requires training with massive datasets that are typically biased. In this context, pre-processing methods that focus on modeling and correcting bias in the data emerge as valuable approaches. In this paper, we propose FairShap, a novel pre-processing (re-weighting) method for fair algorithmic decision-making through data valuation by means of Shapley Values. Our approach is model agnostic and easily interpretable, as it measures the contribution of each training data point to a predefined fairness metric. We empirically validate FairShap on several state-of-the-art datasets of different nature, with a variety of training scenarios and models and show how it outperforms other methods, yielding fairer models with higher or similar levels of accuracy. We also illustrate FairShap's interpretability by means of histograms and latent space visualizations. We believe that this 
&lt;/p&gt;</description></item></channel></rss>