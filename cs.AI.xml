<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#38024;&#23545;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#23433;&#20840;&#27169;&#22359;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#23494;&#25216;&#26415;&#21644;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#25216;&#26415;&#26469;&#23545;&#25239;&#36890;&#20449;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.11730</link><description>&lt;p&gt;
&#36890;&#36807;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#20943;&#36731;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#23433;&#20840;&#27169;&#22359;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#23494;&#25216;&#26415;&#21644;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#25216;&#26415;&#26469;&#23545;&#25239;&#36890;&#20449;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#30340;&#20852;&#36215;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#32852;&#37030;&#21442;&#19982;&#26041;&#20043;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#20419;&#36827;&#20102;&#20998;&#25955;&#24335;&#27169;&#22411;&#32858;&#21512;&#24182;&#20943;&#23569;&#23545;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#36825;&#20123;&#25361;&#25112;&#20027;&#35201;&#28304;&#20110;&#32858;&#21512;&#36807;&#31243;&#30340;&#20998;&#25955;&#24615;&#36136;&#12289;&#21442;&#19982;&#32773;&#30340;&#22810;&#26679;&#21270;&#35282;&#33394;&#21644;&#36131;&#20219;&#20197;&#21450;&#32570;&#20047;&#30417;&#31649;&#21644;&#32531;&#35299;&#23041;&#32961;&#30340;&#20013;&#22830;&#26426;&#26500;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#39318;&#20808;&#30028;&#23450;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;DFL&#36890;&#20449;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20123;&#30830;&#23450;&#30340;&#39118;&#38505;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#20026;DFL&#24179;&#21488;&#35774;&#35745;&#30340;&#23433;&#20840;&#27169;&#22359;&#26469;&#23545;&#25239;&#22522;&#20110;&#36890;&#20449;&#30340;&#25915;&#20987;&#12290;&#35813;&#27169;&#22359;&#32467;&#21512;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#21152;&#23494;&#31561;&#23433;&#20840;&#25216;&#26415;&#19982;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#65288;MTD&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36127;&#38754;&#38472;&#36848;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#35780;&#20272;&#32771;&#34385;&#36127;&#38754;&#38472;&#36848;&#30340;&#26041;&#27861;&#30340;&#22256;&#38590;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#20010;&#20851;&#31995;&#39044;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#20004;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.11719</link><description>&lt;p&gt;
&#20855;&#26377;&#36127;&#38754;&#38472;&#36848;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Benchmark datasets for biomedical knowledge graphs with negative statements. (arXiv:2307.11719v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11719
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36127;&#38754;&#38472;&#36848;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#35780;&#20272;&#32771;&#34385;&#36127;&#38754;&#38472;&#36848;&#30340;&#26041;&#27861;&#30340;&#22256;&#38590;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#20010;&#20851;&#31995;&#39044;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#20004;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#20851;&#20110;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20107;&#23454;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20107;&#23454;&#34987;&#23450;&#20041;&#20026;&#27491;&#38754;&#38472;&#36848;&#12290;&#36127;&#38754;&#38472;&#36848;&#22312;&#24320;&#25918;&#19990;&#30028;&#20551;&#35774;&#19979;&#38750;&#24120;&#31232;&#32570;&#20294;&#39640;&#24230;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#25913;&#21892;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#25903;&#25345;&#32771;&#34385;&#36825;&#20123;&#36127;&#38754;&#38472;&#36848;&#30340;&#26041;&#27861;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#20851;&#31995;&#39044;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#12289;&#22522;&#22240;-&#30142;&#30149;&#20851;&#32852;&#39044;&#27979;&#21644;&#30142;&#30149;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#26500;&#24314;&#20855;&#26377;&#36127;&#38754;&#38472;&#36848;&#30340;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;&#30340;&#22256;&#38590;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20004;&#20010;&#25104;&#21151;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#65292;&#22522;&#22240;&#26412;&#20307;&#21644;&#20154;&#31867;&#34920;&#22411;&#26412;&#20307;&#30340;&#25968;&#25454;&#65292;&#24182;&#28155;&#21152;&#20102;&#36127;&#38754;&#38472;&#36848;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20004;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#29983;&#25104;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs represent facts about real-world entities. Most of these facts are defined as positive statements. The negative statements are scarce but highly relevant under the open-world assumption. Furthermore, they have been demonstrated to improve the performance of several applications, namely in the biomedical domain. However, no benchmark dataset supports the evaluation of the methods that consider these negative statements.  We present a collection of datasets for three relation prediction tasks protein-protein interaction prediction, gene-disease association prediction and disease prediction - that aim at circumventing the difficulties in building benchmarks for knowledge graphs with negative statements. These datasets include data from two successful biomedical ontologies, Gene Ontology and Human Phenotype Ontology, enriched with negative statements.  We also generate knowledge graph embeddings for each dataset with two popular path-based methods and evaluate the perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#21477;&#30340;&#35760;&#24518;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#21270;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#23558;&#23376;&#31243;&#24207;&#20316;&#20026;&#21333;&#20010;&#21333;&#20301;&#22788;&#29702;&#65292;&#20294;&#20107;&#23454;&#19978;&#20195;&#30721;&#30340;&#34892;&#20026;&#21462;&#20915;&#20110;&#35821;&#21477;&#20043;&#38388;&#30340;&#27969;&#21160;&#12290;</title><link>http://arxiv.org/abs/2307.11709</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#21477;&#30340;&#35760;&#24518;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#21270;
&lt;/p&gt;
&lt;p&gt;
Statement-based Memory for Neural Source Code Summarization. (arXiv:2307.11709v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11709
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#21477;&#30340;&#35760;&#24518;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#21270;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#23558;&#23376;&#31243;&#24207;&#20316;&#20026;&#21333;&#20010;&#21333;&#20301;&#22788;&#29702;&#65292;&#20294;&#20107;&#23454;&#19978;&#20195;&#30721;&#30340;&#34892;&#20026;&#21462;&#20915;&#20110;&#35821;&#21477;&#20043;&#38388;&#30340;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#25688;&#35201;&#21270;&#26159;&#23558;&#28304;&#20195;&#30721;&#34892;&#20026;&#20889;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20219;&#21153;&#12290;&#20195;&#30721;&#25688;&#35201;&#21270;&#20026;&#31243;&#24207;&#21592;&#25552;&#20379;&#20102;&#36719;&#20214;&#25991;&#26723;&#30340;&#22522;&#30784;&#12290;&#20195;&#30721;&#30340;&#31616;&#30701;&#25551;&#36848;&#21487;&#20197;&#24110;&#21161;&#31243;&#24207;&#21592;&#24555;&#36895;&#29702;&#35299;&#31243;&#24207;&#65292;&#32780;&#19981;&#24517;&#38405;&#35835;&#20195;&#30721;&#26412;&#36523;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#21270;&#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#21270;&#20195;&#30721;&#25688;&#35201;&#21270;&#25216;&#26415;&#30740;&#31350;&#30340;&#21069;&#27839;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#26368;&#21463;&#27426;&#36814;&#30340;&#25688;&#35201;&#21270;&#30446;&#26631;&#26159;&#31243;&#24207;&#23376;&#31243;&#24207;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#23601;&#26159;&#20351;&#29992;&#20174;&#20195;&#30721;&#20179;&#24211;&#20013;&#25552;&#21462;&#30340;&#22823;&#37327;&#23376;&#31243;&#24207;&#31034;&#20363;&#26469;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#26550;&#26500;&#12290;&#32534;&#30721;&#22120;&#34920;&#31034;&#20195;&#30721;&#65292;&#35299;&#30721;&#22120;&#34920;&#31034;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#23581;&#35797;&#23558;&#23376;&#31243;&#24207;&#35270;&#20026;&#21333;&#20010;&#21333;&#20301;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#23376;&#31243;&#24207;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;Transformer&#25110;&#22522;&#20110;RNN&#30340;&#32534;&#30721;&#22120;&#12290;&#20294;&#20195;&#30721;&#34892;&#20026;&#24448;&#24448;&#21462;&#20915;&#20110;&#35821;&#21477;&#20043;&#38388;&#30340;&#27969;&#21160;&#12290;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#21160;&#24577;&#20998;&#26512;&#26469;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Source code summarization is the task of writing natural language descriptions of source code behavior. Code summarization underpins software documentation for programmers. Short descriptions of code help programmers understand the program quickly without having to read the code itself. Lately, neural source code summarization has emerged as the frontier of research into automated code summarization techniques. By far the most popular targets for summarization are program subroutines. The idea, in a nutshell, is to train an encoder-decoder neural architecture using large sets of examples of subroutines extracted from code repositories. The encoder represents the code and the decoder represents the summary. However, most current approaches attempt to treat the subroutine as a single unit. For example, by taking the entire subroutine as input to a Transformer or RNN-based encoder. But code behavior tends to depend on the flow from statement to statement. Normally dynamic analysis may she
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#24182;&#36827;&#34892;&#33647;&#29289;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#22411;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#65292;&#19981;&#20381;&#36182;&#20110;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#20998;&#23376;&#25351;&#32441;&#25110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11694</link><description>&lt;p&gt;
SynerGPT:&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#21644;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design. (arXiv:2307.11694v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#24182;&#36827;&#34892;&#33647;&#29289;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#22411;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#65292;&#19981;&#20381;&#36182;&#20110;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#20998;&#23376;&#25351;&#32441;&#25110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33647;&#29289;&#30340;&#21327;&#21516;&#32452;&#21512;&#21487;&#20197;&#21152;&#36895;&#30284;&#30151;&#27835;&#30103;&#30340;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#27963;&#26816;&#32454;&#32990;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#21644;&#27169;&#22411;&#29992;&#20110;&#19978;&#19979;&#25991;&#20013;&#30340;&#33647;&#29289;&#21327;&#21516;&#23398;&#20064;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#23567;&#30340;&#8220;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#8221;&#65292;&#20854;&#20013;&#21253;&#21547;&#29305;&#23450;&#30284;&#30151;&#38774;&#32454;&#32990;&#19978;&#19979;&#25991;&#20013;&#30340;10-20&#20010;&#33647;&#29289;&#21327;&#21516;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#35813;&#19978;&#19979;&#25991;&#20013;&#30340;&#39069;&#22806;&#33647;&#29289;&#21327;&#21516;&#20851;&#31995;&#12290;&#21463;&#26368;&#36817;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#35813;&#24037;&#20316;&#36890;&#36807;&#39044;&#35757;&#32451;GPT&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#24120;&#35265;&#30340;&#21151;&#33021;&#31867;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181; &#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;GPT&#27169;&#22411;&#33021;&#22815;&#19978;&#19979;&#25991;&#23398;&#20064;&#8220;&#33647;&#29289;&#21327;&#21516;&#21151;&#33021;&#8221;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411; - &#19981;&#20351;&#29992;&#20219;&#20309;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#20998;&#23376;&#25351;&#32441;&#65292;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25110;&#20219;&#20309;&#20854;&#20182;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782; - &#33021;&#22815;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#26041;&#27861;&#19982;&#36951;&#20256;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#25552;&#31034;&#24182;&#36873;&#25321;&#21327;&#21516;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small "personalized dataset" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to "in-context learn" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn "drug synergy functions". Our model -- which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge -- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39318;&#27425;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#30740;&#31350;&#36890;&#29992;&#20195;&#25968;&#30340;&#29468;&#24819;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#32593;&#32476;&#20197;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11688</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#32593;&#32476;&#29992;&#20110;&#24418;&#25104;&#36890;&#29992;&#20195;&#25968;&#29468;&#24819;
&lt;/p&gt;
&lt;p&gt;
Interpretable Graph Networks Formulate Universal Algebra Conjectures. (arXiv:2307.11688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11688
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39318;&#27425;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#30740;&#31350;&#36890;&#29992;&#20195;&#25968;&#30340;&#29468;&#24819;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#32593;&#32476;&#20197;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#23835;&#36215;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#31350;&#20960;&#21313;&#24180;&#26469;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#22256;&#38590;&#25968;&#23398;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#36890;&#29992;&#20195;&#25968;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#23436;&#20840;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#27425;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#30740;&#31350;&#36890;&#29992;&#20195;&#25968;&#30340;&#29468;&#24819;&#65292;&#32467;&#21512;&#31561;&#24335;&#21644;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#12290;&#34429;&#28982;&#25299;&#25169;&#34920;&#31034;&#27861;&#21487;&#20197;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20998;&#26512;&#36825;&#20123;&#23646;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26377;&#38480;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30452;&#25509;&#29992;&#20110;&#39564;&#35777;&#29616;&#26377;&#29468;&#24819;&#25110;&#25552;&#20986;&#26032;&#29468;&#24819;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#31639;&#27861;&#65292;&#22522;&#20110;&#36890;&#29992;&#20195;&#25968;&#30340;&#29468;&#24819;&#29983;&#25104;&#21487;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#23618;&#26469;&#26500;&#24314;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#22270;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35299;&#37322;&#30340;&#22270;&#32593;&#32476;&#33021;&#22815;&#22686;&#24378;...
&lt;/p&gt;
&lt;p&gt;
The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA) -- one of the fields laying the foundations of modern mathematics -- is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.11661</link><description>&lt;p&gt;
&#29992;GPT-4&#22686;&#24378;CLIP&#65306;&#21033;&#29992;&#35270;&#35273;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;VLMs&#36890;&#36807;&#35774;&#35745;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#25552;&#31034;&#26469;0-shot&#36866;&#24212;&#19979;&#28216;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#25552;&#31034;&#24037;&#31243;&#21033;&#29992;&#20102;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#29992;&#20316;&#20808;&#36827;&#30340;&#20114;&#32852;&#32593;&#25628;&#32034;&#24037;&#20855;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#34987;&#25805;&#20316;&#20197;&#25552;&#20379;&#20219;&#20309;&#32467;&#26500;&#21270;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#65288;&#22914;EuroSAT&#65288;~7&#65285;&#65289;&#12289;DTD&#65288;~7&#65285;&#65289;&#12289;SUN397&#65288;~4.6&#65285;&#65289;&#21644;CUB&#65288;~3.3&#65285;&#65289;&#65289;&#19978;&#26174;&#31034;&#20986;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23569;&#37327;&#26679;&#26412;&#36866;&#37197;&#22120;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#30340;s
&lt;/p&gt;
&lt;p&gt;
Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11655</link><description>&lt;p&gt;
&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#24378;&#30423;&#21453;&#39304;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#30830;&#23450;&#24615;&#28436;&#21270;&#21644;&#19981;&#21487;&#35266;&#27979;&#30340;&#29366;&#24577;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#22312;&#27599;&#19968;&#36718;&#33719;&#24471;&#30340;&#22870;&#21169;&#26159;&#36873;&#25321;&#34892;&#21160;&#30340;&#30701;&#26399;&#22870;&#21169;&#21644;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#31243;&#24230;&#65288;&#21363;&#36890;&#36807;&#20854;&#29366;&#24577;&#27979;&#37327;&#65289;&#30340;&#20989;&#25968;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24179;&#21488;&#20174;&#29992;&#25143;&#23545;&#29305;&#23450;&#31867;&#22411;&#20869;&#23481;&#30340;&#21442;&#19982;&#20013;&#33719;&#24471;&#30340;&#22870;&#21169;&#19981;&#20165;&#21462;&#20915;&#20110;&#20855;&#20307;&#20869;&#23481;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#36824;&#21462;&#20915;&#20110;&#29992;&#25143;&#19982;&#24179;&#21488;&#19978;&#20854;&#20182;&#31867;&#22411;&#20869;&#23481;&#20114;&#21160;&#21518;&#20854;&#20559;&#22909;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#955;&#8712;[0,1]&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22240;&#20808;&#21069;&#20869;&#23481;&#28040;&#36153;&#32780;&#24555;&#36895;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#25512;&#29702;&#22120;&#30340;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#32570;&#38519;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20915;&#31574;&#26641;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#25991;&#23383;&#35828;&#26126;&#35299;&#37322;&#22522;&#20110;&#25513;&#27169;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#25552;&#20379;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#20197;&#25552;&#21319;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11643</link><description>&lt;p&gt;
&#22522;&#20110;&#24418;&#24577;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;AI&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.11643v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#25512;&#29702;&#22120;&#30340;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#32570;&#38519;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20915;&#31574;&#26641;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#25991;&#23383;&#35828;&#26126;&#35299;&#37322;&#22522;&#20110;&#25513;&#27169;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#25552;&#20379;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#20197;&#25552;&#21319;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#24037;&#31243;&#21644;&#21046;&#36896;&#31561;&#34892;&#19994;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#36879;&#26126;&#30340;&#25512;&#29702;&#20197;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AI&#25512;&#29702;&#22120;&#65292;&#35813;&#25512;&#29702;&#22120;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#32570;&#38519;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20915;&#31574;&#26641;&#23545;&#32570;&#38519;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;AI&#25512;&#29702;&#22120;&#36890;&#36807;&#21487;&#35270;&#21270;&#22270;&#34920;&#21644;&#25991;&#23383;&#35828;&#26126;&#25552;&#20379;&#23545;&#22522;&#20110;&#25513;&#27169;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#35299;&#37322;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;AI&#25512;&#29702;&#22120;&#22312;&#20351;&#29992;366&#24352;&#21547;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#38598;&#21512;&#19978;&#27979;&#35797;&#20102;&#35299;&#37322;IE Mask R-CNN&#27169;&#22411;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#25512;&#29702;&#22120;&#22312;&#35299;&#37322;IE Mask R-CNN&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25152;&#25552;&#20986;&#30340;AI&#25512;&#29702;&#22120;&#20026;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of artificial intelligent (AI) models becomes more prevalent in industries such as engineering and manufacturing, it is essential that these models provide transparent reasoning behind their predictions. This paper proposes the AI-Reasoner, which extracts the morphological characteristics of defects (DefChars) from images and utilises decision trees to reason with the DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e. charts) and textual explanations to provide insights into outputs made by masked-based defect detection and classification models. It also provides effective mitigation strategies to enhance data pre-processing and overall model performance. The AI-Reasoner was tested on explaining the outputs of an IE Mask R-CNN model using a set of 366 images containing defects. The results demonstrated its effectiveness in explaining the IE Mask R-CNN model's predictions. Overall, the proposed AI-Reasoner provides a solution for improving the performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#19987;&#23478;&#20013;&#24515;&#26412;&#20307;&#35774;&#35745;&#38598;&#25104;&#21040;CRISP-DM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20844;&#21496;&#22312;&#24212;&#29992;&#25968;&#25454;&#39537;&#21160;&#39033;&#30446;&#26102;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.11637</link><description>&lt;p&gt;
&#23558;&#39046;&#22495;&#19987;&#23478;&#20013;&#24515;&#26412;&#20307;&#35774;&#35745;&#38598;&#25104;&#21040;&#38754;&#21521;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#30340;CRISP-DM&#20013;
&lt;/p&gt;
&lt;p&gt;
Integration of Domain Expert-Centric Ontology Design into the CRISP-DM for Cyber-Physical Production Systems. (arXiv:2307.11637v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#19987;&#23478;&#20013;&#24515;&#26412;&#20307;&#35774;&#35745;&#38598;&#25104;&#21040;CRISP-DM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20844;&#21496;&#22312;&#24212;&#29992;&#25968;&#25454;&#39537;&#21160;&#39033;&#30446;&#26102;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;4.0&#21644;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#39046;&#22495;&#65292;&#22823;&#37327;&#26377;&#28508;&#22312;&#20215;&#20540;&#30340;&#25968;&#25454;&#34987;&#29983;&#25104;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#22797;&#26434;&#21644;&#38544;&#21547;&#30340;&#27169;&#24335;&#26041;&#38754;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#25152;&#24471;&#21040;&#30340;&#30693;&#35782;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#35832;&#22914;&#35786;&#26029;&#25110;&#32500;&#25252;&#35745;&#21010;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#39537;&#21160;&#39033;&#30446;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#29702;&#35299;&#21644;&#20934;&#22791;&#25968;&#25454;&#65292;&#22240;&#27492;&#24120;&#24120;&#23548;&#33268;&#22833;&#36133;&#12290;&#22312;&#22788;&#29702;&#24037;&#19994;4.0&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#26102;&#65292;&#39046;&#22495;&#29305;&#23450;&#26412;&#20307;&#30340;&#24212;&#29992;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#30340;&#26412;&#20307;&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#21644;&#24037;&#20214;&#23578;&#26410;&#31995;&#32479;&#22320;&#38598;&#25104;&#21040;CRISP-DM&#20013;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#20805;&#20998;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#20013;&#24515;&#26412;&#20307;&#35774;&#35745;&#22312;CRISP-DM&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the age of Industry 4.0 and Cyber-Physical Production Systems (CPPSs) vast amounts of potentially valuable data are being generated. Methods from Machine Learning (ML) and Data Mining (DM) have proven to be promising in extracting complex and hidden patterns from the data collected. The knowledge obtained can in turn be used to improve tasks like diagnostics or maintenance planning. However, such data-driven projects, usually performed with the Cross-Industry Standard Process for Data Mining (CRISP-DM), often fail due to the disproportionate amount of time needed for understanding and preparing the data. The application of domain-specific ontologies has demonstrated its advantageousness in a wide variety of Industry 4.0 application scenarios regarding the aforementioned challenges. However, workflows and artifacts from ontology design for CPPSs have not yet been systematically integrated into the CRISP-DM. Accordingly, this contribution intends to present an integrated approach so t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21452;&#20998;&#21306;&#26497;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25511;&#21046;&#23454;&#20363;&#30340;&#26497;&#21270;&#31243;&#24230;&#19982;&#35299;&#20915;&#36825;&#20123;&#23454;&#20363;&#30340;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23454;&#20363;&#30340;&#26497;&#21270;&#31243;&#24230;&#36234;&#39640;&#65292;&#35299;&#20915;&#38382;&#39064;&#36234;&#23481;&#26131;&#12290;</title><link>http://arxiv.org/abs/2307.11621</link><description>&lt;p&gt;
&#20851;&#20110;&#21452;&#20998;&#21306;&#26497;&#21270;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#30740;&#31350;&#65306;&#20174;&#20013;&#31435;&#21040;&#39640;&#24230;&#26497;&#21270;&#30340;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
On the Complexity of the Bipartite Polarization Problem: from Neutral to Highly Polarized Discussions. (arXiv:2307.11621v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21452;&#20998;&#21306;&#26497;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25511;&#21046;&#23454;&#20363;&#30340;&#26497;&#21270;&#31243;&#24230;&#19982;&#35299;&#20915;&#36825;&#20123;&#23454;&#20363;&#30340;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23454;&#20363;&#30340;&#26497;&#21270;&#31243;&#24230;&#36234;&#39640;&#65292;&#35299;&#20915;&#38382;&#39064;&#36234;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#20998;&#21306;&#26497;&#21270;&#38382;&#39064;&#26159;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#22312;&#34920;&#31034;&#36890;&#36807;&#31038;&#20132;&#32593;&#32476;&#21457;&#23637;&#36215;&#26469;&#30340;&#36777;&#35770;&#30340;&#24102;&#26435;&#37325;&#21644;&#26631;&#31614;&#30340;&#22270;&#19978;&#25214;&#21040;&#26368;&#39640;&#26497;&#21270;&#30340;&#21452;&#20998;&#21306;&#12290;&#22270;&#20013;&#30340;&#33410;&#28857;&#20195;&#34920;&#29992;&#25143;&#30340;&#35266;&#28857;&#65292;&#36793;&#20195;&#34920;&#29992;&#25143;&#20043;&#38388;&#30340;&#19968;&#33268;&#25110;&#19981;&#19968;&#33268;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#30475;&#20316;&#26159;&#26368;&#22823;&#21106;&#38382;&#39064;&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#24050;&#32463;&#25214;&#21040;&#20102;&#19968;&#20123;&#36817;&#20284;&#35299;&#21644;&#31934;&#30830;&#35299;&#65292;&#36825;&#20123;&#35299;&#26159;&#20174;Reddit&#30340;&#35752;&#35770;&#20013;&#24471;&#21040;&#30340;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#23454;&#20363;&#20284;&#20046;&#24456;&#23481;&#26131;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#35813;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#35813;&#27169;&#22411;&#30340;&#19968;&#20010;&#21442;&#25968;&#25511;&#21046;&#20102;&#23454;&#20363;&#30340;&#26497;&#21270;&#31243;&#24230;&#65292;&#32780;&#36825;&#19982;&#35299;&#20915;&#36825;&#20123;&#23454;&#20363;&#30340;&#24179;&#22343;&#22797;&#26434;&#24230;&#26377;&#20851;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#24179;&#22343;&#22797;&#26434;&#24230;&#30340;&#32467;&#26524;&#19982;&#25105;&#20204;&#30340;&#20551;&#35774;&#19968;&#33268;&#65306;&#23454;&#20363;&#30340;&#26497;&#21270;&#31243;&#24230;&#36234;&#39640;&#65292;&#25214;&#21040;&#23545;&#24212;&#30340;&#35299;&#36234;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bipartite Polarization Problem is an optimization problem where the goal is to find the highest polarized bipartition on a weighted and labelled graph that represents a debate developed through some social network, where nodes represent user's opinions and edges agreement or disagreement between users. This problem can be seen as a generalization of the maxcut problem, and in previous work approximate solutions and exact solutions have been obtained for real instances obtained from Reddit discussions, showing that such real instances seem to be very easy to solve. In this paper, we investigate further the complexity of this problem, by introducing an instance generation model where a single parameter controls the polarization of the instances in such a way that this correlates with the average complexity to solve those instances. The average complexity results we obtain are consistent with our hypothesis: the higher the polarization of the instance, the easier is to find the corres
&lt;/p&gt;</description></item><item><title>CausE&#26159;&#19968;&#20010;&#37319;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#23884;&#20837;&#35299;&#32544;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11610</link><description>&lt;p&gt;
CausE: &#26397;&#21521;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
CausE: Towards Causal Knowledge Graph Embedding. (arXiv:2307.11610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11610
&lt;/p&gt;
&lt;p&gt;
CausE&#26159;&#19968;&#20010;&#37319;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#23884;&#20837;&#35299;&#32544;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#30340;&#37325;&#28857;&#26159;&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#36830;&#32493;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#30340;&#19977;&#20803;&#32452;&#20197;&#23454;&#29616;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#65288;KGC&#65289;&#12290;&#28982;&#32780;&#65292;KGE&#27169;&#22411;&#36890;&#24120;&#21482;&#26159;&#31616;&#21333;&#22320;&#23398;&#20064;&#19977;&#20803;&#32452;&#25968;&#25454;&#30340;&#32467;&#26500;&#20851;&#32852;&#65292;&#24182;&#19988;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;KG&#20013;&#65292;&#23884;&#20837;&#21487;&#33021;&#20250;&#34987;&#24494;&#19981;&#36275;&#36947;&#30340;&#27169;&#24335;&#21644;&#22122;&#22768;&#38142;&#25509;&#25152;&#35823;&#23548;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22240;&#26524;&#24615;&#21644;&#23884;&#20837;&#35299;&#32544;&#26041;&#38754;&#24314;&#31435;&#20102;KGE&#30340;&#26032;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Causality-enhanced knowledge graph Embedding&#65288;CausE&#65289;&#26694;&#26550;&#12290;CausE&#20351;&#29992;&#22240;&#26524;&#24178;&#39044;&#26469;&#20272;&#35745;&#28151;&#26434;&#23884;&#20837;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CausE&#21487;&#20197;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;KGC&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;https://github.com/zjukg/CausE&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) focuses on representing the entities and relations of a knowledge graph (KG) into the continuous vector spaces, which can be employed to predict the missing triples to achieve knowledge graph completion (KGC). However, KGE models often only briefly learn structural correlations of triple data and embeddings would be misled by the trivial patterns and noisy links in real-world KGs. To address this issue, we build the new paradigm of KGE in the context of causality and embedding disentanglement. We further propose a Causality-enhanced knowledge graph Embedding (CausE) framework. CausE employs causal intervention to estimate the causal effect of the confounder embeddings and design new training objectives to make stable predictions. Experimental results demonstrate that CausE could outperform the baseline models and achieve state-of-the-art KGC performance. We release our code in https://github.com/zjukg/CausE.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DeepFeature&#26041;&#27861;&#65292;&#20174;&#29305;&#24449;&#22270;&#23618;&#38754;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27979;&#35797;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27979;&#35797;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#31070;&#32463;&#20803;&#32780;&#24573;&#30053;&#29305;&#24449;&#22270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11563</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Feature Map Testing for Deep Neural Networks. (arXiv:2307.11563v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DeepFeature&#26041;&#27861;&#65292;&#20174;&#29305;&#24449;&#22270;&#23618;&#38754;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27979;&#35797;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27979;&#35797;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#31070;&#32463;&#20803;&#32780;&#24573;&#30053;&#29305;&#24449;&#22270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#27979;&#35797;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#25110;&#20351;&#29992;&#27979;&#35797;&#25351;&#26631;&#36873;&#25321;&#30340;&#27979;&#35797;&#29992;&#20363;&#34987;&#36755;&#20837;&#27169;&#22411;&#20013;&#65292;&#20197;&#25214;&#21040;&#23548;&#33268;&#25925;&#38556;&#30340;&#27979;&#35797;&#21333;&#20803;&#65288;&#22914;&#31070;&#32463;&#20803;&#21644;&#29305;&#24449;&#22270;&#65292;&#28608;&#27963;&#23427;&#20204;&#20960;&#20046;&#32943;&#23450;&#20250;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#65289;&#65292;&#24182;&#23558;&#20854;&#25253;&#21578;&#32473;DNN&#24320;&#21457;&#32773;&#65292;&#38543;&#21518;&#24320;&#21457;&#32773;&#37325;&#26032;&#20462;&#22797;&#23427;&#20204;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;&#27979;&#35797;&#29992;&#20363;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#27979;&#35797;&#25351;&#26631;&#20027;&#35201;&#20851;&#27880;&#31070;&#32463;&#20803;&#65292;&#36825;&#24847;&#21619;&#30528;&#36890;&#36807;&#24341;&#23548;&#27169;&#31946;&#27979;&#35797;&#25110;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#36873;&#25321;&#21457;&#29616;&#30340;&#27979;&#35797;&#29992;&#20363;&#20027;&#35201;&#38598;&#20013;&#20110;&#26816;&#27979;&#23548;&#33268;&#25925;&#38556;&#30340;&#31070;&#32463;&#20803;&#65292;&#32780;&#24573;&#30053;&#20102;&#23548;&#33268;&#25925;&#38556;&#30340;&#29305;&#24449;&#22270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepFeature&#65292;&#23427;&#20174;&#29305;&#24449;&#22270;&#23618;&#38754;&#23545;DNN&#36827;&#34892;&#27979;&#35797;&#12290;&#24403;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;DeepFeature&#23558;&#20180;&#32454;&#26816;&#26597;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#20869;&#37096;&#29305;&#24449;&#22270;&#65292;&#24182;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the widespread application of deep neural networks~(DNNs) in safety-critical tasks, deep learning testing has drawn increasing attention. During the testing process, test cases that have been fuzzed or selected using test metrics are fed into the model to find fault-inducing test units (e.g., neurons and feature maps, activating which will almost certainly result in a model error) and report them to the DNN developer, who subsequently repair them~(e.g., retraining the model with test cases). Current test metrics, however, are primarily concerned with the neurons, which means that test cases that are discovered either by guided fuzzing or selection with these metrics focus on detecting fault-inducing neurons while failing to detect fault-inducing feature maps.  In this work, we propose DeepFeature, which tests DNNs from the feature map level. When testing is conducted, DeepFeature will scrutinize every internal feature map in the model and identify vulnerabilities that can be enh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CycleIK&#65292;&#19968;&#31181;&#31070;&#32463;&#26426;&#22120;&#20154;&#36870;&#36816;&#21160;&#23398;&#26041;&#27861;&#65292;&#23427;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26550;&#26500;&#12290;&#36890;&#36807;&#23884;&#20837;&#28151;&#21512;&#31070;&#32463;&#36951;&#20256;&#36870;&#36816;&#21160;&#23398;&#27969;&#27700;&#32447;&#24182;&#20351;&#29992;&#31264;&#23494;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36870;&#36816;&#21160;&#23398;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#36825;&#20351;&#24471;CycleIK&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#21040;&#26426;&#22120;&#20154;&#30828;&#20214;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.11554</link><description>&lt;p&gt;
CycleIK&#65306;&#33041;&#31070;&#32463;&#21551;&#21457;&#30340;&#36870;&#36816;&#21160;&#23398;
&lt;/p&gt;
&lt;p&gt;
CycleIK: Neuro-inspired Inverse Kinematics. (arXiv:2307.11554v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CycleIK&#65292;&#19968;&#31181;&#31070;&#32463;&#26426;&#22120;&#20154;&#36870;&#36816;&#21160;&#23398;&#26041;&#27861;&#65292;&#23427;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26550;&#26500;&#12290;&#36890;&#36807;&#23884;&#20837;&#28151;&#21512;&#31070;&#32463;&#36951;&#20256;&#36870;&#36816;&#21160;&#23398;&#27969;&#27700;&#32447;&#24182;&#20351;&#29992;&#31264;&#23494;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36870;&#36816;&#21160;&#23398;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#36825;&#20351;&#24471;CycleIK&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#21040;&#26426;&#22120;&#20154;&#30828;&#20214;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CycleIK&#65292;&#19968;&#31181;&#31070;&#32463;&#26426;&#22120;&#20154;&#36870;&#36816;&#21160;&#23398;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26550;&#26500;&#20004;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#21551;&#21457;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21333;&#29420;&#24212;&#29992;&#65292;&#20294;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#23427;&#20204;&#23884;&#20837;&#28151;&#21512;&#31070;&#32463;&#36951;&#20256;&#36870;&#36816;&#21160;&#23398;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#39034;&#24207;&#26368;&#23567;&#20108;&#20056;&#35268;&#21010;&#65288;SLSQP&#65289;&#25110;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#26032;&#22411;&#33041;&#31070;&#32463;&#21551;&#21457;&#21327;&#20316;&#26426;&#22120;&#20154;&#65288;NICOL&#65289;&#30340;&#38543;&#26426;&#26426;&#22120;&#20154;&#37197;&#32622;&#25910;&#38598;&#30340;&#31264;&#23494;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#35813;&#26426;&#22120;&#20154;&#20855;&#26377;&#20004;&#20010;&#20887;&#20313;&#30340;8&#33258;&#30001;&#24230;&#25805;&#20316;&#22120;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;BioIK&#26041;&#27861;&#20013;&#30340;&#21152;&#26435;&#22810;&#30446;&#26631;&#20989;&#25968;&#26469;&#25903;&#25345;&#35757;&#32451;&#36807;&#31243;&#21644;&#28151;&#21512;&#31070;&#32463;&#36951;&#20256;&#26550;&#26500;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#31070;&#32463;&#27169;&#22411;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#36870;&#36816;&#21160;&#23398;&#26041;&#27861;&#31454;&#20105;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#21040;&#26426;&#22120;&#20154;&#30828;&#20214;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces CycleIK, a neuro-robotic approach that wraps two novel neuro-inspired methods for the inverse kinematics (IK) task, a Generative Adversarial Network (GAN), and a Multi-Layer Perceptron architecture. These methods can be used in a standalone fashion, but we also show how embedding these into a hybrid neuro-genetic IK pipeline allows for further optimization via sequential least-squares programming (SLSQP) or a genetic algorithm (GA). The models are trained and tested on dense datasets that were collected from random robot configurations of the new Neuro-Inspired COLlaborator (NICOL), a semi-humanoid robot with two redundant 8-DoF manipulators. We utilize the weighted multi-objective function from the state-of-the-art BioIK method to support the training process and our hybrid neuro-genetic architecture. We show that the neural models can compete with state-of-the-art IK approaches, which allows for deployment directly to robotic hardware. Additionally, it is shown t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CSE-CIC-IDS2018&#25968;&#25454;&#38598;&#20013;&#30456;&#20851;&#29305;&#24449;&#30340;&#35782;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#36890;&#36807;&#24212;&#29992;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21644;&#20998;&#31867;&#31639;&#27861;&#65292;&#30830;&#23450;&#20102;&#26368;&#32456;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11544</link><description>&lt;p&gt;
CSE-CIC-IDS2018&#25968;&#25454;&#38598;&#20013;&#30456;&#20851;&#29305;&#24449;&#30340;&#35782;&#21035;&#29992;&#20110;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Identifying Relevant Features of CSE-CIC-IDS2018 Dataset for the Development of an Intrusion Detection System. (arXiv:2307.11544v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CSE-CIC-IDS2018&#25968;&#25454;&#38598;&#20013;&#30456;&#20851;&#29305;&#24449;&#30340;&#35782;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#36890;&#36807;&#24212;&#29992;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21644;&#20998;&#31867;&#31639;&#27861;&#65292;&#30830;&#23450;&#20102;&#26368;&#32456;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#26159;IT&#31995;&#32479;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20854;&#20851;&#38190;&#32452;&#20214;&#26159;&#19968;&#20010;&#20998;&#31867;&#27169;&#22359;&#65292;&#19981;&#26029;&#35780;&#20272;&#32593;&#32476;&#27969;&#37327;&#30340;&#29305;&#24449;&#24182;&#35782;&#21035;&#21487;&#33021;&#30340;&#23041;&#32961;&#12290;&#20854;&#25928;&#29575;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27491;&#30830;&#36873;&#25321;&#35201;&#30417;&#25511;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#22312;&#24320;&#21457;IDS&#30340;&#36807;&#31243;&#20013;&#65292;&#30830;&#23450;&#19968;&#20010;&#26368;&#23567;&#30340;&#29305;&#24449;&#38598;&#21512;&#26469;&#23433;&#20840;&#21306;&#20998;&#24694;&#24847;&#27969;&#37327;&#21644;&#33391;&#24615;&#27969;&#37327;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;CSE-CIC-IDS2018 on AWS&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#24037;&#20316;&#27969;&#31243;&#21450;&#20854;&#32467;&#26524;&#65292;&#37325;&#28857;&#20851;&#27880;&#20116;&#31181;&#25915;&#20987;&#31867;&#22411;&#12290;&#20026;&#20102;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#65292;&#24212;&#29992;&#20102;&#20845;&#31181;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#30340;&#24179;&#22343;&#24471;&#20998;&#21046;&#23450;&#20102;&#26368;&#32456;&#30340;&#29305;&#24449;&#25490;&#24207;&#12290;&#25509;&#19979;&#26469;&#65292;&#22522;&#20110;&#19981;&#21516;&#30340;&#25490;&#24207;&#38408;&#20540;&#20540;&#65292;&#24418;&#25104;&#20102;&#20960;&#20010;&#29305;&#24449;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#20116;&#31181;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrusion detection systems (IDSs) are essential elements of IT systems. Their key component is a classification module that continuously evaluates some features of the network traffic and identifies possible threats. Its efficiency is greatly affected by the right selection of the features to be monitored. Therefore, the identification of a minimal set of features that are necessary to safely distinguish malicious traffic from benign traffic is indispensable in the course of the development of an IDS. This paper presents the preprocessing and feature selection workflow as well as its results in the case of the CSE-CIC-IDS2018 on AWS dataset, focusing on five attack types. To identify the relevant features, six feature selection methods were applied, and the final ranking of the features was elaborated based on their average score. Next, several subsets of the features were formed based on different ranking threshold values, and each subset was tried with five classification algorithms
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32467;&#21512;&#27431;&#30431;&#27861;&#35268;&#19982;&#26368;&#26032;&#30740;&#31350;&#36235;&#21183;&#30340;&#21487;&#35777;&#26126;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25253;&#21578;&#30340;&#24314;&#35758;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#21270;&#30340;&#21345;&#29255;&#35760;&#24405;AI&#24212;&#29992;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#29992;&#20363;&#21644;&#25805;&#20316;&#21345;&#20197;&#28385;&#36275;&#30417;&#31649;&#35201;&#27714;&#65292;&#26088;&#22312;&#24320;&#21457;&#23433;&#20840;&#21487;&#20449;&#30340;AI&#31995;&#32479;&#24182;&#25903;&#25345;&#31532;&#19977;&#26041;&#23457;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.11525</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25253;&#21578;&#65306;&#23558;&#27431;&#30431;&#27861;&#35268;&#34701;&#20837;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#30340;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation into AI Development. (arXiv:2307.11525v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32467;&#21512;&#27431;&#30431;&#27861;&#35268;&#19982;&#26368;&#26032;&#30740;&#31350;&#36235;&#21183;&#30340;&#21487;&#35777;&#26126;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25253;&#21578;&#30340;&#24314;&#35758;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#21270;&#30340;&#21345;&#29255;&#35760;&#24405;AI&#24212;&#29992;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#29992;&#20363;&#21644;&#25805;&#20316;&#21345;&#20197;&#28385;&#36275;&#30417;&#31649;&#35201;&#27714;&#65292;&#26088;&#22312;&#24320;&#21457;&#23433;&#20840;&#21487;&#20449;&#30340;AI&#31995;&#32479;&#24182;&#25903;&#25345;&#31532;&#19977;&#26041;&#23457;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21487;&#35299;&#37322;&#21644;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20174;&#19994;&#32773;&#20173;&#28982;&#38754;&#20020;&#32570;&#20047;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#27861;&#35268;&#21644;&#26631;&#20934;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#27431;&#30431;&#26368;&#26032;&#30340;&#27861;&#35268;&#21162;&#21147;&#19982;&#26368;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25351;&#21335;&#25552;&#20986;&#30340;&#24314;&#35758;&#19982;&#30740;&#31350;&#30340;&#26368;&#26032;&#36235;&#21183;&#30456;&#32467;&#21512;&#65306;&#25968;&#25454;&#21345;&#21644;&#27169;&#22411;&#21345;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26631;&#20934;&#21270;&#30340;&#21345;&#29255;&#26469;&#35760;&#24405;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#29992;&#20363;&#21644;&#25805;&#20316;&#21345;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#21644;&#27169;&#22411;&#21345;&#30340;&#26356;&#26032;&#20197;&#28385;&#36275;&#30417;&#31649;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#21345;&#29255;&#20013;&#24341;&#29992;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#20197;&#21450;&#27861;&#35268;&#30340;&#26469;&#28304;&#65292;&#24182;&#23613;&#21487;&#33021;&#25552;&#20379;&#20102;&#39069;&#22806;&#25903;&#25345;&#26448;&#26009;&#21644;&#24037;&#20855;&#31665;&#30340;&#21442;&#32771;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#20986;&#33021;&#22815;&#24110;&#21161;&#20174;&#19994;&#32773;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#24320;&#21457;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21345;&#29255;&#65292;&#21516;&#26102;&#20351;&#31532;&#19977;&#26041;&#23457;&#35745;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#26356;&#21152;&#39640;&#25928;&#65292;&#26131;&#20110;&#29702;&#35299;&#65292;&#24182;&#24314;&#31435;&#23545;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32467;&#21512;&#20102;&#26469;&#33258;&#8230;&#65288;&#37096;&#20998;&#20869;&#23481;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Despite large progress in Explainable and Safe AI, practitioners suffer from a lack of regulation and standards for AI safety. In this work we merge recent regulation efforts by the European Union and first proposals for AI guidelines with recent trends in research: data and model cards. We propose the use of standardized cards to document AI applications throughout the development process. Our main contribution is the introduction of use-case and operation cards, along with updates for data and model cards to cope with regulatory requirements. We reference both recent research as well as the source of the regulation in our cards and provide references to additional support material and toolboxes whenever possible. The goal is to design cards that help practitioners develop safe AI systems throughout the development process, while enabling efficient third-party auditing of AI applications, being easy to understand, and building trust in the system. Our work incorporates insights from i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#12289;&#38899;&#39057;&#29305;&#24449;&#20540;&#21644;&#25991;&#26412;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26469;&#26816;&#27979;&#35270;&#39057;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.11519</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Hate Speech Detection using Machine Learning. (arXiv:2307.11519v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#12289;&#38899;&#39057;&#29305;&#24449;&#20540;&#21644;&#25991;&#26412;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26469;&#26816;&#27979;&#35270;&#39057;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#29992;&#25143;&#21644;&#23186;&#20307;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#36861;&#36394;&#38899;&#39057;&#21644;&#35270;&#39057;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#23558;&#35270;&#39057;&#25110;&#38899;&#39057;&#36716;&#25442;&#20026;&#25991;&#26412;&#24182;&#19981;&#33021;&#20934;&#30830;&#26816;&#27979;&#21040;&#20167;&#24680;&#35328;&#35770;&#65292;&#22240;&#20026;&#20154;&#20204;&#26377;&#26102;&#20250;&#23558;&#20167;&#24680;&#35789;&#27719;&#20316;&#20026;&#24189;&#40664;&#25110;&#24841;&#24555;&#30340;&#24847;&#21619;&#20351;&#29992;&#65292;&#24182;&#22312;&#35270;&#39057;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#22768;&#35843;&#25110;&#23637;&#31034;&#19981;&#21516;&#30340;&#21160;&#20316;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#22823;&#22810;&#26159;&#22522;&#20110;&#21333;&#19968;&#27169;&#24577;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#12289;&#38899;&#39057;&#20013;&#30340;&#29305;&#24449;&#20540;&#12289;&#25991;&#26412;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#26816;&#27979;&#35270;&#39057;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous growth of internet users and media content, it is very hard to track down hateful speech in audio and video. Converting video or audio into text does not detect hate speech accurately as human sometimes uses hateful words as humorous or pleasant in sense and also uses different voice tones or show different action in the video. The state-ofthe-art hate speech detection models were mostly developed on a single modality. In this research, a combined approach of multimodal system has been proposed to detect hate speech from video contents by extracting feature images, feature values extracted from the audio, text and used machine learning and Natural language processing.
&lt;/p&gt;</description></item><item><title>IndigoVX&#26159;&#19968;&#31181;&#23558;&#20154;&#31867;&#26234;&#24935;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#30340;&#26368;&#20248;&#20915;&#31574;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#24490;&#29615;&#65292;&#21033;&#29992;&#20154;&#31867;&#19987;&#23478;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#27934;&#35265;&#65292;&#21046;&#23450;&#21644;&#20248;&#21270;&#26397;&#30528;&#26126;&#30830;&#23450;&#20041;&#30340;&#30446;&#26631;&#30340;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#23450;&#37327;&#21270;&#30340;&#19977;&#20998;&#25968;&#27169;&#24335;&#36827;&#34892;&#35780;&#20272;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.11516</link><description>&lt;p&gt;
IndigoVX: &#20154;&#31867;&#26234;&#24935;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#30340;&#26368;&#20248;&#20915;&#31574;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
IndigoVX: Where Human Intelligence Meets AI for Optimal Decision Making. (arXiv:2307.11516v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11516
&lt;/p&gt;
&lt;p&gt;
IndigoVX&#26159;&#19968;&#31181;&#23558;&#20154;&#31867;&#26234;&#24935;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#30340;&#26368;&#20248;&#20915;&#31574;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#24490;&#29615;&#65292;&#21033;&#29992;&#20154;&#31867;&#19987;&#23478;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#27934;&#35265;&#65292;&#21046;&#23450;&#21644;&#20248;&#21270;&#26397;&#30528;&#26126;&#30830;&#23450;&#20041;&#30340;&#30446;&#26631;&#30340;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#23450;&#37327;&#21270;&#30340;&#19977;&#20998;&#25968;&#27169;&#24335;&#36827;&#34892;&#35780;&#20272;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#24935;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#26368;&#20248;&#30446;&#26631;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;Indigo&#65292;&#26159;&#25351;&#36890;&#36807;&#36845;&#20195;&#30446;&#26631;&#23548;&#21521;&#20248;&#21270;&#36827;&#34892;&#30693;&#24773;&#25968;&#20540;&#20915;&#31574;&#12290;&#24403;&#19982;&#20154;&#31867;&#21327;&#20316;&#32773;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32852;&#21512;&#31995;&#32479;&#21629;&#21517;&#20026;IndigoVX&#65292;&#21363;&#34394;&#25311;&#19987;&#23478;&#12290;&#35813;&#31995;&#32479;&#27010;&#24565;&#31616;&#21333;&#12290;&#25105;&#20204;&#35774;&#24819;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#28216;&#25103;&#25110;&#21830;&#19994;&#31574;&#30053;&#65292;&#20154;&#31867;&#25552;&#20379;&#25112;&#30053;&#32972;&#26223;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#26368;&#20339;&#30340;&#25968;&#25454;&#39537;&#21160;&#31227;&#21160;&#12290;Indigo&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#24490;&#29615;&#36816;&#20316;&#65292;&#21033;&#29992;&#20154;&#31867;&#19987;&#23478;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#27934;&#35265;&#65292;&#21046;&#23450;&#21644;&#20248;&#21270;&#26397;&#30528;&#26126;&#30830;&#23450;&#20041;&#30340;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;&#23450;&#37327;&#21270;&#30340;&#19977;&#20998;&#25968;&#27169;&#24335;&#65292;&#36825;&#31181;&#28151;&#21512;&#21270;&#20801;&#35768;&#32852;&#21512;&#22242;&#38431;&#35780;&#20272;&#31574;&#30053;&#24182;&#25913;&#36827;&#35745;&#21010;&#65292;&#21516;&#26102;&#23454;&#26102;&#36866;&#24212;&#25361;&#25112;&#21644;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper defines a new approach for augmenting human intelligence with AI for optimal goal solving. Our proposed AI, Indigo, is an acronym for Informed Numerical Decision-making through Iterative Goal-Oriented optimization. When combined with a human collaborator, we term the joint system IndigoVX, for Virtual eXpert. The system is conceptually simple. We envisage this method being applied to games or business strategies, with the human providing strategic context and the AI offering optimal, data-driven moves. Indigo operates through an iterative feedback loop, harnessing the human expert's contextual knowledge and the AI's data-driven insights to craft and refine strategies towards a well-defined goal. Using a quantified three-score schema, this hybridization allows the combined team to evaluate strategies and refine their plan, while adapting to challenges and changes in real-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21327;&#21464;&#37327;&#20559;&#31227;&#33258;&#36866;&#24212;&#20013;&#30340;&#19968;&#33324;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#24050;&#26377;&#32467;&#26524;&#24471;&#21040;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;&#22312;&#24369;&#24179;&#28369;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#23454;&#29616;&#19982;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#20013;&#30456;&#21516;&#31934;&#24230;&#25152;&#38656;&#30340;&#26679;&#26412;&#37327;&#35201;&#27604;&#29616;&#26377;&#20998;&#26512;&#35777;&#26126;&#30340;&#23569;&#12290;</title><link>http://arxiv.org/abs/2307.11503</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#33258;&#36866;&#24212;&#20013;&#30340;&#19968;&#33324;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
General regularization in covariate shift adaptation. (arXiv:2307.11503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21327;&#21464;&#37327;&#20559;&#31227;&#33258;&#36866;&#24212;&#20013;&#30340;&#19968;&#33324;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#24050;&#26377;&#32467;&#26524;&#24471;&#21040;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;&#22312;&#24369;&#24179;&#28369;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#23454;&#29616;&#19982;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#20013;&#30456;&#21516;&#31934;&#24230;&#25152;&#38656;&#30340;&#26679;&#26412;&#37327;&#35201;&#27604;&#29616;&#26377;&#20998;&#26512;&#35777;&#26126;&#30340;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26679;&#26412;&#37325;&#21152;&#26435;&#26159;&#32416;&#27491;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#20013;&#30001;&#26410;&#26469;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#24341;&#36215;&#30340;&#26368;&#23567;&#20108;&#20056;&#23398;&#20064;&#31639;&#27861;&#38169;&#35823;&#30340;&#26368;&#24120;&#29992;&#26041;&#27861;&#20043;&#19968;&#12290;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#26679;&#26412;&#26435;&#37325;&#26159;&#30001;&#26410;&#26469;&#25968;&#25454;&#20998;&#24067;&#23545;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#20272;&#35745;Radon-Nikod\'ym&#23548;&#25968;&#30340;&#20540;&#30830;&#23450;&#30340;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#22312;RKHS&#20013;&#37325;&#26032;&#21152;&#26435;&#26680;&#22238;&#24402;&#30340;&#24050;&#30693;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#24471;&#21040;&#26032;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#24369;&#24179;&#28369;&#26465;&#20214;&#19979;&#34920;&#26126;&#65292;&#20026;&#20102;&#23454;&#29616;&#19982;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#30456;&#21516;&#31934;&#24230;&#30340;&#26679;&#26412;&#25968;&#30446;&#35201;&#27604;&#29616;&#26377;&#30340;&#20998;&#26512;&#35777;&#26126;&#30340;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample reweighting is one of the most widely used methods for correcting the error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS), that is caused by future data distributions that are different from the training data distribution. In practical situations, the sample weights are determined by values of the estimated Radon-Nikod\'ym derivative, of the future data distribution w.r.t.~the training data distribution. In this work, we review known error bounds for reweighted kernel regression in RKHS and obtain, by combination, novel results. We show under weak smoothness conditions, that the amount of samples, needed to achieve the same order of accuracy as in the standard supervised learning without differences in data distributions, is smaller than proven by state-of-the-art analyses.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;ResNet&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#36827;&#34892;&#20998;&#24067;&#24335;&#25512;&#29702;&#12290;&#36890;&#36807;&#35782;&#21035;&#21487;&#21024;&#38500;&#20294;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#30340;&#36830;&#25509;&#65292;&#23454;&#29616;&#23545;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#32553;&#20943;&#12290;&#35813;&#26550;&#26500;&#33021;&#22815;&#38477;&#20302;&#24310;&#36831;&#21644;&#33021;&#32791;&#65292;&#24182;&#25552;&#39640;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11499</link><description>&lt;p&gt;
&#36866;&#24212;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#30340;&#33258;&#36866;&#24212;ResNet&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Adaptive ResNet Architecture for Distributed Inference in Resource-Constrained IoT Systems. (arXiv:2307.11499v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11499
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;ResNet&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#36827;&#34892;&#20998;&#24067;&#24335;&#25512;&#29702;&#12290;&#36890;&#36807;&#35782;&#21035;&#21487;&#21024;&#38500;&#20294;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#30340;&#36830;&#25509;&#65292;&#23454;&#29616;&#23545;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#32553;&#20943;&#12290;&#35813;&#26550;&#26500;&#33021;&#22815;&#38477;&#20302;&#24310;&#36831;&#21644;&#33021;&#32791;&#65292;&#24182;&#25552;&#39640;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#26029;&#25193;&#23637;&#21644;&#22797;&#26434;&#21270;&#65292;&#22823;&#22810;&#25968;&#36793;&#32536;&#35774;&#22791;&#26080;&#27861;&#22788;&#29702;&#20854;&#24222;&#22823;&#30340;&#22788;&#29702;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#24335;&#25512;&#29702;&#30340;&#27010;&#24565;&#23545;&#20110;&#22312;&#33410;&#28857;&#38598;&#32676;&#20013;&#20998;&#37197;&#31070;&#32463;&#32593;&#32476;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#21487;&#33021;&#23548;&#33268;&#39069;&#22806;&#30340;&#33021;&#32791;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#20381;&#36182;&#65292;&#36825;&#20123;&#35774;&#22791;&#24448;&#24448;&#21463;&#21040;&#19981;&#31283;&#23450;&#30340;&#20256;&#36755;&#36895;&#29575;&#30340;&#24433;&#21709;&#12290;&#19981;&#31283;&#23450;&#30340;&#20256;&#36755;&#36895;&#29575;&#20250;&#24433;&#21709;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#23454;&#26102;&#24615;&#33021;&#65292;&#23548;&#33268;&#20302;&#24310;&#36831;&#12289;&#39640;&#33021;&#32791;&#21644;&#28508;&#22312;&#30340;&#25925;&#38556;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#21160;&#24577;&#31995;&#32479;&#26469;&#35828;&#65292;&#24517;&#39035;&#26377;&#19968;&#20010;&#20855;&#26377;&#33258;&#36866;&#24212;&#26550;&#26500;&#30340;&#24377;&#24615;DNN&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#29992;&#36164;&#28304;&#35843;&#25972;&#22823;&#23567;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#35782;&#21035;&#20102;&#21487;&#20197;&#22312;ResNet&#20013;&#21024;&#38500;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#36830;&#25509;&#65292;&#20197;&#22312;&#36164;&#28304;&#30701;&#32570;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20998;&#24067;&#12290;&#22522;&#20110;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#24310;&#36831;&#21644;...
&lt;/p&gt;
&lt;p&gt;
As deep neural networks continue to expand and become more complex, most edge devices are unable to handle their extensive processing requirements. Therefore, the concept of distributed inference is essential to distribute the neural network among a cluster of nodes. However, distribution may lead to additional energy consumption and dependency among devices that suffer from unstable transmission rates. Unstable transmission rates harm real-time performance of IoT devices causing low latency, high energy usage, and potential failures. Hence, for dynamic systems, it is necessary to have a resilient DNN with an adaptive architecture that can downsize as per the available resources. This paper presents an empirical study that identifies the connections in ResNet that can be dropped without significantly impacting the model's performance to enable distribution in case of resource shortage. Based on the results, a multi-objective optimization problem is formulated to minimize latency and ma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11494</link><description>&lt;p&gt;
&#39044;&#27979;&#12289;&#25913;&#36827;&#12289;&#21512;&#25104;&#65306;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#39044;&#27979;&#25110;&#22635;&#34917;&#20219;&#21153;&#30340;&#26465;&#20214;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38754;&#21521;&#22810;&#31181;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TSDiff&#65292;&#19968;&#31181;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#26465;&#20214;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#33258;&#24341;&#23548;&#26426;&#21046;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#24471;TSDiff&#33021;&#22815;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#26465;&#20214;&#35774;&#32622;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#32593;&#32476;&#25110;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;TSDiff&#19982;&#20960;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26465;&#20214;&#39044;&#27979;&#26041;&#27861;&#30456;&#31454;&#20105;&#65288;&#39044;&#27979;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;TSDiff&#23398;&#21040;&#30340;&#38544;&#24615;&#27010;&#29575;&#23494;&#24230;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;p
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#30693;&#35782;&#33976;&#39311;&#20013;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#26377;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#24182;&#23545;&#29305;&#24449;&#21644;&#20998;&#31867;&#36827;&#34892;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20986;&#30340;&#23398;&#29983;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11469</link><description>&lt;p&gt;
&#20351;&#29992;&#32593;&#32476;&#25910;&#38598;&#30340;&#22270;&#20687;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Distribution Shift Matters for Knowledge Distillation with Webly Collected Images. (arXiv:2307.11469v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#30693;&#35782;&#33976;&#39311;&#20013;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#26377;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#24182;&#23545;&#29305;&#24449;&#21644;&#20998;&#31867;&#36827;&#34892;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20986;&#30340;&#23398;&#29983;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26088;&#22312;&#20174;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#25945;&#24072;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23398;&#29983;&#32593;&#32476;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#36890;&#24120;&#22312;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#21644;&#25968;&#25454;&#31649;&#29702;&#32771;&#34385;&#19981;&#21487;&#29992;&#26102;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#36890;&#36807;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#35757;&#32451;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#21644;&#32593;&#32476;&#25910;&#38598;&#25968;&#25454;&#20043;&#38388;&#30340;&#24120;&#35265;&#20998;&#24067;&#20559;&#31227;&#65292;&#24433;&#21709;&#20102;&#35757;&#32451;&#22909;&#30340;&#23398;&#29983;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19981;&#21516;&#20998;&#24067;&#19979;&#30340;&#30693;&#35782;&#33976;&#39311;&#8221;&#65288;KD$^{3}$&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#30001;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#25945;&#24072;&#32593;&#32476;&#21644;&#23398;&#29983;&#32593;&#32476;&#30340;&#32508;&#21512;&#39044;&#27979;&#21160;&#24577;&#36873;&#25321;&#26377;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#21152;&#26435;&#29305;&#24449;&#21644;&#20998;&#31867;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation aims to learn a lightweight student network from a pre-trained teacher network. In practice, existing knowledge distillation methods are usually infeasible when the original training data is unavailable due to some privacy issues and data management considerations. Therefore, data-free knowledge distillation approaches proposed to collect training instances from the Internet. However, most of them have ignored the common distribution shift between the instances from original training data and webly collected data, affecting the reliability of the trained student network. To solve this problem, we propose a novel method dubbed ``Knowledge Distillation between Different Distributions" (KD$^{3}$), which consists of three components. Specifically, we first dynamically select useful training instances from the webly collected data according to the combined predictions of teacher network and student network. Subsequently, we align both the weighted features and classif
&lt;/p&gt;</description></item><item><title>&#22312;6G&#32593;&#32476;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24179;&#21488;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#26234;&#33021;&#31995;&#32479;&#25903;&#25345;&#37096;&#32626;&#38646;&#25509;&#35302;&#30340;PAI&#21363;&#26381;&#21153;&#65292;&#23454;&#29616;&#20102;&#26222;&#36866;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#31995;&#32479;&#21270;&#20998;&#21457;&#12290;</title><link>http://arxiv.org/abs/2307.11468</link><description>&lt;p&gt;
&#22312;6G&#32593;&#32476;&#20013;&#23454;&#29616;&#38646;&#25509;&#35302;&#30340;&#26222;&#36866;&#20154;&#24037;&#26234;&#33021;&#21363;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks. (arXiv:2307.11468v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11468
&lt;/p&gt;
&lt;p&gt;
&#22312;6G&#32593;&#32476;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24179;&#21488;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#26234;&#33021;&#31995;&#32479;&#25903;&#25345;&#37096;&#32626;&#38646;&#25509;&#35302;&#30340;PAI&#21363;&#26381;&#21153;&#65292;&#23454;&#29616;&#20102;&#26222;&#36866;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#31995;&#32479;&#21270;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;6G&#25216;&#26415;&#30340;&#24895;&#26223;&#26159;&#36890;&#36807;&#36229;&#23494;&#38598;&#32593;&#32476;&#12289;&#20302;&#24310;&#36831;&#21644;&#24555;&#36895;&#25968;&#25454;&#20256;&#36755;&#26469;&#25903;&#25345;&#26222;&#36866;&#20154;&#24037;&#26234;&#33021;&#65288;PAI&#65289;&#65292;&#24182;&#20351;&#29992;&#38646;&#25509;&#35302;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#33258;&#25105;X&#65288;&#20363;&#22914;&#65292;&#33258;&#37197;&#32622;&#12289;&#33258;&#30417;&#27979;&#21644;&#33258;&#24840;&#65289;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;6G&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#30446;&#21069;&#21482;&#23436;&#25104;&#20102;&#23545;&#20854;&#35774;&#35745;&#30340;&#27010;&#24565;&#21270;&#12289;&#23454;&#26045;&#30340;&#35843;&#26597;&#21644;&#29992;&#20363;&#30340;&#35268;&#21010;&#30340;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36880;&#28176;&#20174;AI&#20998;&#21457;&#30340;&#29702;&#35770;&#30740;&#31350;&#36716;&#21521;&#23454;&#38469;&#37096;&#32626;&#21644;&#26631;&#20934;&#21270;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#26469;&#36890;&#36807;&#20801;&#35768;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#36741;&#21161;&#30340;&#38646;&#25509;&#35302;&#26381;&#21153;&#37197;&#32622;&#26356;&#23481;&#26131;&#35775;&#38382;AI&#26381;&#21153;&#30340;&#20998;&#21457;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24179;&#21488;&#26550;&#26500;&#65292;&#22312;6G&#32593;&#32476;&#20013;&#20351;&#29992;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#26234;&#33021;&#31995;&#32479;&#25903;&#25345;&#37096;&#32626;&#38646;&#25509;&#35302;&#30340;PAI&#21363;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vision of the upcoming 6G technologies, characterized by ultra-dense network, low latency, and fast data rate is to support Pervasive AI (PAI) using zero-touch solutions enabling self-X (e.g., self-configuration, self-monitoring, and self-healing) services. However, the research on 6G is still in its infancy, and only the first steps have been taken to conceptualize its design, investigate its implementation, and plan for use cases. Toward this end, academia and industry communities have gradually shifted from theoretical studies of AI distribution to real-world deployment and standardization. Still, designing an end-to-end framework that systematizes the AI distribution by allowing easier access to the service using a third-party application assisted by a zero-touch service provisioning has not been well explored. In this context, we introduce a novel platform architecture to deploy a zero-touch PAI-as-a-Service (PAIaaS) in 6G networks supported by a blockchain-based smart system.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#38169;&#35823;&#24230;&#37327;&#65292;&#25913;&#21892;&#20102;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#30340;&#20559;&#21521;&#20110;&#30701;&#26399;&#35760;&#24518;&#30340;&#38382;&#39064;&#65292;&#24182;&#32531;&#35299;&#20102;&#26799;&#24230;&#28040;&#22833;&#30340;&#22256;&#25200;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#36866;&#24403;&#30340;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#38169;&#35823;&#22312;&#26377;&#25928;&#30340;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11462</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#37325;&#26032;&#32553;&#25918;&#38169;&#35823;&#20197;&#25913;&#21892;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improve Long-term Memory Learning Through Rescaling the Error Temporally. (arXiv:2307.11462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#38169;&#35823;&#24230;&#37327;&#65292;&#25913;&#21892;&#20102;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#30340;&#20559;&#21521;&#20110;&#30701;&#26399;&#35760;&#24518;&#30340;&#38382;&#39064;&#65292;&#24182;&#32531;&#35299;&#20102;&#26799;&#24230;&#28040;&#22833;&#30340;&#22256;&#25200;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#36866;&#24403;&#30340;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#38169;&#35823;&#22312;&#26377;&#25928;&#30340;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#20013;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#30340;&#38169;&#35823;&#24230;&#37327;&#36873;&#25321;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#24120;&#29992;&#38169;&#35823;&#24230;&#37327;&#65288;&#21253;&#25324;&#24179;&#22343;&#32477;&#23545;/&#24179;&#26041;&#35823;&#24046;&#65289;&#23545;&#30701;&#26399;&#35760;&#24518;&#30340;&#20559;&#21521;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#26102;&#65292;&#25152;&#26377;&#26102;&#38388;&#19978;&#26377;&#27491;&#26435;&#37325;&#30340;&#38169;&#35823;&#37117;&#20559;&#21521;&#20110;&#30701;&#26399;&#35760;&#24518;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#20559;&#24046;&#24182;&#25913;&#21892;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#30340;&#38169;&#35823;&#12290;&#38500;&#20102;&#20943;&#23569;&#23545;&#30701;&#26399;&#35760;&#24518;&#30340;&#20559;&#21521;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#21644;&#24207;&#21015;&#27169;&#22411;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25968;&#20540;&#32467;&#26524;&#30830;&#35748;&#20102;&#36866;&#24403;&#30340;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#35823;&#24046;&#23545;&#20110;&#26377;&#25928;&#30340;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#23450;&#37327;&#20998;&#26512;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#38169;&#35823;&#23545;&#30701;&#26399;&#35760;&#24518;&#20559;&#21521;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the error metric selection for long-term memory learning in sequence modelling. We examine the bias towards short-term memory in commonly used errors, including mean absolute/squared error. Our findings show that all temporally positive-weighted errors are biased towards short-term memory in learning linear functionals. To reduce this bias and improve long-term memory learning, we propose the use of a temporally rescaled error. In addition to reducing the bias towards short-term memory, this approach can also alleviate the vanishing gradient issue. We conduct numerical experiments on different long-memory tasks and sequence models to validate our claims. Numerical results confirm the importance of appropriate temporally rescaled error for effective long-term memory learning. To the best of our knowledge, this is the first work that quantitatively analyzes different errors' memory bias towards short-term memory in sequence modelling.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33521;&#22303;&#25991;&#23398;&#32763;&#35793;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#37319;&#29992;&#32763;&#35793;&#23478;&#30340;&#39118;&#26684;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#24471;&#20102;&#39640;&#24230;&#36824;&#21407;&#20154;&#31867;&#35793;&#32773;&#39118;&#26684;&#30340;&#26426;&#22120;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11457</link><description>&lt;p&gt;
&#23558;&#20154;&#31867;&#32763;&#35793;&#39118;&#26684;&#34701;&#20837;&#33521;&#22303;&#25991;&#23398;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Incorporating Human Translator Style into English-Turkish Literary Machine Translation. (arXiv:2307.11457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33521;&#22303;&#25991;&#23398;&#32763;&#35793;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#37319;&#29992;&#32763;&#35793;&#23478;&#30340;&#39118;&#26684;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#24471;&#20102;&#39640;&#24230;&#36824;&#21407;&#20154;&#31867;&#35793;&#32773;&#39118;&#26684;&#30340;&#26426;&#22120;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#19968;&#33324;&#39046;&#22495;&#65292;&#20294;&#23384;&#22312;&#23558;&#36825;&#20123;&#31995;&#32479;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#25991;&#23398;&#32763;&#35793;&#65289;&#30340;&#36235;&#21183;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#33521;&#22303;&#25991;&#23398;&#32763;&#35793;&#65292;&#24182;&#24320;&#21457;&#32771;&#34385;&#32763;&#35793;&#23478;&#39118;&#26684;&#29305;&#24449;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#23545;&#40784;&#30340;&#29305;&#23450;&#35793;&#32773;&#20316;&#21697;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#25163;&#21160;&#23545;&#40784;&#12289;&#33258;&#21160;&#23545;&#40784;&#12289;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#35821;&#26009;&#24211;&#22823;&#23567;&#23545;&#32763;&#35793;&#25928;&#26524;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#35793;&#32773;&#22312;&#36755;&#20986;&#32763;&#35793;&#20013;&#30340;&#39118;&#26684;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#27169;&#22411;&#36866;&#24212;&#21040;&#35793;&#32773;&#30340;&#39118;&#26684;&#65292;&#21487;&#20197;&#39640;&#24230;&#37325;&#29616;&#20154;&#31867;&#35793;&#32773;&#30340;&#39118;&#26684;&#22312;&#30446;&#26631;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although machine translation systems are mostly designed to serve in the general domain, there is a growing tendency to adapt these systems to other domains like literary translation. In this paper, we focus on English-Turkish literary translation and develop machine translation models that take into account the stylistic features of translators. We fine-tune a pre-trained machine translation model by the manually-aligned works of a particular translator. We make a detailed analysis of the effects of manual and automatic alignments, data augmentation methods, and corpus size on the translations. We propose an approach based on stylistic features to evaluate the style of a translator in the output translations. We show that the human translator style can be highly recreated in the target machine translations by adapting the models to the style of the translator.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#32493;&#23545;&#35805;&#30340;&#26041;&#24335;&#65292;&#23558;&#20010;&#24615;&#21270;&#35299;&#37322;&#20256;&#36798;&#32473;&#34987;&#35299;&#37322;&#32773;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21482;&#35201;&#23384;&#22312;&#19968;&#20010;&#34987;&#35299;&#37322;&#32773;&#29702;&#35299;&#24182;&#19988;&#35299;&#37322;&#32773;&#30693;&#26195;&#30340;&#23545;&#20110;&#21021;&#22987;&#20027;&#24352;&#30340;&#35299;&#37322;&#65292;&#23545;&#35805;&#23558;&#22240;&#20026;&#34987;&#35299;&#37322;&#32773;&#23545;&#21021;&#22987;&#20027;&#24352;&#30340;&#35777;&#26126;&#32780;&#32456;&#27490;&#12290;</title><link>http://arxiv.org/abs/2307.11452</link><description>&lt;p&gt;
&#25552;&#20379;&#20010;&#24615;&#21270;&#35299;&#37322;&#65306;&#19968;&#31181;&#23545;&#35805;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Providing personalized Explanations: a Conversational Approach. (arXiv:2307.11452v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#32493;&#23545;&#35805;&#30340;&#26041;&#24335;&#65292;&#23558;&#20010;&#24615;&#21270;&#35299;&#37322;&#20256;&#36798;&#32473;&#34987;&#35299;&#37322;&#32773;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21482;&#35201;&#23384;&#22312;&#19968;&#20010;&#34987;&#35299;&#37322;&#32773;&#29702;&#35299;&#24182;&#19988;&#35299;&#37322;&#32773;&#30693;&#26195;&#30340;&#23545;&#20110;&#21021;&#22987;&#20027;&#24352;&#30340;&#35299;&#37322;&#65292;&#23545;&#35805;&#23558;&#22240;&#20026;&#34987;&#35299;&#37322;&#32773;&#23545;&#21021;&#22987;&#20027;&#24352;&#30340;&#35777;&#26126;&#32780;&#32456;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#33021;&#20855;&#26377;&#21508;&#31181;&#30693;&#35782;&#21644;&#32972;&#26223;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#38656;&#35201;&#23545;&#20854;&#34892;&#20026;&#36827;&#34892;&#20010;&#24615;&#21270;&#35299;&#37322;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#19981;&#20165;&#33021;&#24110;&#21161;&#35299;&#37322;&#32773;&#20102;&#35299;&#34987;&#35299;&#37322;&#32773;&#30340;&#32972;&#26223;&#65292;&#36824;&#33021;&#35753;&#34987;&#35299;&#37322;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19982;&#34987;&#35299;&#37322;&#32773;&#36827;&#34892;&#36830;&#32493;&#23545;&#35805;&#26469;&#21521;&#20854;&#20256;&#36798;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21482;&#35201;&#23384;&#22312;&#19968;&#20010;&#34987;&#35299;&#37322;&#32773;&#29702;&#35299;&#24182;&#19988;&#35299;&#37322;&#32773;&#30693;&#26195;&#30340;&#23545;&#20110;&#21021;&#22987;&#20027;&#24352;&#30340;&#35299;&#37322;&#65292;&#37027;&#20040;&#23545;&#35805;&#23558;&#22240;&#20026;&#34987;&#35299;&#37322;&#32773;&#23545;&#21021;&#22987;&#20027;&#24352;&#30340;&#35777;&#26126;&#32780;&#32456;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing applications of AI systems require personalized explanations for their behaviors to various stakeholders since the stakeholders may have various knowledge and backgrounds. In general, a conversation between explainers and explainees not only allows explainers to obtain the explainees' background, but also allows explainees to better understand the explanations. In this paper, we propose an approach for an explainer to communicate personalized explanations to an explainee through having consecutive conversations with the explainee. We prove that the conversation terminates due to the explainee's justification of the initial claim as long as there exists an explanation for the initial claim that the explainee understands and the explainer is aware of.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AIGC&#65288;GPT&#65289;&#22312;&#30005;&#20449;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#30005;&#20449;&#22686;&#24378;&#35748;&#30693;&#33021;&#21147;&#31995;&#32479;&#65292;&#20026;&#30005;&#20449;&#26381;&#21153;&#30340;&#26500;&#24314;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11449</link><description>&lt;p&gt;
AIGC&#36171;&#33021;&#30005;&#20449;&#34892;&#19994;&#30333;&#30382;&#20070;
&lt;/p&gt;
&lt;p&gt;
AIGC Empowering Telecom Sector White Paper. (arXiv:2307.11449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AIGC&#65288;GPT&#65289;&#22312;&#30005;&#20449;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#30005;&#20449;&#22686;&#24378;&#35748;&#30693;&#33021;&#21147;&#31995;&#32479;&#65292;&#20026;&#30005;&#20449;&#26381;&#21153;&#30340;&#26500;&#24314;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;GPT&#28909;&#28526;&#20013;&#65292;&#20154;&#20204;&#28145;&#20999;&#24847;&#35782;&#21040;&#20316;&#20026;&#19968;&#39033;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#25216;&#26415;&#21644;&#32463;&#27982;&#31038;&#20250;&#21457;&#23637;&#30340;&#20851;&#38190;&#21147;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#23558;&#32473;&#20840;&#29699;&#20135;&#19994;&#24102;&#26469;&#24040;&#22823;&#30340;&#39134;&#36291;&#21644;&#31361;&#30772;&#65292;&#24182;&#28145;&#21051;&#24433;&#21709;&#26410;&#26469;&#30340;&#31454;&#20105;&#26684;&#23616;&#12290;&#20316;&#20026;&#20449;&#24687;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#30340;&#24314;&#35774;&#32773;&#21644;&#36816;&#33829;&#21830;&#65292;&#30005;&#20449;&#34892;&#19994;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#22522;&#30784;&#25903;&#25345;&#65292;&#29978;&#33267;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#12290;&#22914;&#20309;&#23454;&#29616;AIGC&#65288;GPT&#65289;&#24212;&#29992;&#24182;&#22312;&#30005;&#20449;&#34892;&#19994;&#20013;&#23454;&#26045;AIGC&#26159;&#30005;&#20449;&#20174;&#19994;&#32773;&#24517;&#39035;&#24605;&#32771;&#21644;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;GPT&#20316;&#20026;AIGC&#30340;&#20856;&#22411;&#20195;&#34920;&#65292;&#20316;&#32773;&#20998;&#26512;&#20102;GPT&#22914;&#20309;&#36890;&#36807;&#22330;&#26223;&#36171;&#33021;&#30005;&#20449;&#34892;&#19994;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;GPT&#36890;&#29992;&#27169;&#22411;&#19982;&#30005;&#20449;&#26381;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#30005;&#20449;&#22686;&#24378;&#35748;&#30693;&#33021;&#21147;&#31995;&#32479;&#65292;&#22238;&#31572;&#20102;&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;&#30005;&#20449;&#26381;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the global craze of GPT, people have deeply realized that AI, as a transformative technology and key force in economic and social development, will bring great leaps and breakthroughs to the global industry and profoundly influence the future world competition pattern. As the builder and operator of information and communication infrastructure, the telecom sector provides infrastructure support for the development of AI, and even takes the lead in the implementation of AI applications. How to enable the application of AIGC (GPT) and implement AIGC in the telecom sector are questions that telecom practitioners must ponder and answer. Through the study of GPT, a typical representative of AIGC, the authors have analyzed how GPT empowers the telecom sector in the form of scenarios, discussed the gap between the current GPT general model and telecom services, proposed for the first time a Telco Augmented Cognition capability system, provided answers to how to construct a telecom service 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25512;&#29702;&#38454;&#27573;&#24341;&#20837;&#25209;&#22788;&#29702;&#23545;&#33021;&#28304;&#28040;&#32791;&#21644;&#21709;&#24212;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#25209;&#22788;&#29702;&#23545;&#36825;&#20004;&#20010;&#25351;&#26631;&#37117;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.11434</link><description>&lt;p&gt;
&#20026;&#29615;&#20445;&#20154;&#24037;&#26234;&#33021;&#32780;&#25209;&#22788;&#29702; - &#25506;&#32034;&#25512;&#29702;&#36807;&#31243;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Batching for Green AI -- An Exploratory Study on Inference. (arXiv:2307.11434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11434
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25512;&#29702;&#38454;&#27573;&#24341;&#20837;&#25209;&#22788;&#29702;&#23545;&#33021;&#28304;&#28040;&#32791;&#21644;&#21709;&#24212;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#25209;&#22788;&#29702;&#23545;&#36825;&#20004;&#20010;&#25351;&#26631;&#37117;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#25209;&#22823;&#23567;&#26159;&#19968;&#20010;&#38656;&#35201;&#35843;&#25972;&#30340;&#37325;&#35201;&#21442;&#25968;&#12290;&#38500;&#20102;&#20854;&#20182;&#36136;&#37327;&#25351;&#26631;&#22806;&#65292;&#23427;&#23545;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#12289;&#35757;&#32451;&#26102;&#38388;&#21644;&#24182;&#34892;&#24615;&#20855;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#20107;&#23454;&#26159;&#20247;&#25152;&#21608;&#30693;&#24182;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#38454;&#27573;&#65292;&#24403;&#27169;&#22411;&#34987;&#26368;&#32456;&#29992;&#25143;&#29992;&#20110;&#25512;&#29702;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#24341;&#20837;&#25209;&#22823;&#23567;&#30340;&#28508;&#22312;&#22909;&#22788;&#23384;&#22312;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#36755;&#20837;&#25209;&#22788;&#29702;&#23545;&#20110;&#20116;&#20010;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#34987;&#35748;&#20026;&#26159;&#26368;&#20808;&#36827;&#30340;&#23436;&#20840;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#28304;&#28040;&#32791;&#21644;&#21709;&#24212;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25209;&#22788;&#29702;&#23545;&#36825;&#20004;&#20010;&#25351;&#26631;&#37117;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21576;&#29616;&#20102;&#36807;&#21435;&#21313;&#24180;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#26102;&#38388;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#19978;&#65292;&#33021;&#28304;&#28040;&#32791;&#22312;&#36825;&#27573;&#26102;&#38388;&#20869;&#26126;&#26174;&#19978;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The batch size is an essential parameter to tune during the development of new neural networks. Amongst other quality indicators, it has a large degree of influence on the model's accuracy, generalisability, training times and parallelisability. This fact is generally known and commonly studied. However, during the application phase of a deep learning model, when the model is utilised by an end-user for inference, we find that there is a disregard for the potential benefits of introducing a batch size. In this study, we examine the effect of input batching on the energy consumption and response times of five fully-trained neural networks for computer vision that were considered state-of-the-art at the time of their publication. The results suggest that batching has a significant effect on both of these metrics. Furthermore, we present a timeline of the energy efficiency and accuracy of neural networks over the past decade. We find that in general, energy consumption rises at a much ste
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;OpenPose&#26694;&#26550;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#20998;&#26512;&#32771;&#35797;&#35270;&#39057;&#24182;&#39640;&#25928;&#26377;&#25928;&#22320;&#26816;&#27979;&#21487;&#30097;&#27963;&#21160;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.11413</link><description>&lt;p&gt;
&#37319;&#29992;OpenPose&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#32771;&#35797;&#21487;&#30097;&#27963;&#21160;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Video-based Detector for Suspicious Activity in Examination with OpenPose. (arXiv:2307.11413v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;OpenPose&#26694;&#26550;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#20998;&#26512;&#32771;&#35797;&#35270;&#39057;&#24182;&#39640;&#25928;&#26377;&#25928;&#22320;&#26816;&#27979;&#21487;&#30097;&#27963;&#21160;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#35797;&#26159;&#23398;&#20064;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#65292;&#23398;&#26415;&#26426;&#26500;&#25237;&#20837;&#22823;&#37327;&#36164;&#28304;&#32500;&#25252;&#20854;&#23436;&#25972;&#24615;&#65292;&#38450;&#27490;&#23398;&#29983;&#25110;&#30417;&#32771;&#21592;&#20316;&#24330;&#12290;&#28982;&#32780;&#65292;&#20316;&#24330;&#22312;&#32771;&#35797;&#20013;&#21464;&#24471;&#29462;&#29527;&#65292;&#36825;&#25439;&#23475;&#20102;&#32771;&#35797;&#30340;&#23436;&#25972;&#24615;&#12290;&#20381;&#38752;&#30417;&#32771;&#21592;&#30417;&#35270;&#27599;&#20010;&#23398;&#29983;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#20999;&#23454;&#38469;&#19988;&#26080;&#25928;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#36830;&#32493;&#35760;&#24405;&#32771;&#35797;&#36807;&#31243;&#20197;&#30417;&#25511;&#23398;&#29983;&#30340;&#21487;&#30097;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24405;&#20687;&#36890;&#24120;&#36807;&#38271;&#20197;&#33267;&#20110;&#30417;&#32771;&#21592;&#26080;&#27861;&#26377;&#25928;&#20998;&#26512;&#65292;&#30130;&#21171;&#21487;&#33021;&#23548;&#33268;&#20182;&#20204;&#38169;&#36807;&#37325;&#35201;&#32454;&#33410;&#12290;&#20026;&#25193;&#22823;&#30417;&#25511;&#33539;&#22260;&#65292;&#30417;&#32771;&#21592;&#21487;&#20197;&#20351;&#29992;&#22266;&#23450;&#26550;&#35774;&#22312;&#39640;&#22788;&#25110;&#20329;&#25140;&#30340;&#25668;&#20687;&#22836;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#33258;&#21160;&#21270;&#20998;&#26512;&#35270;&#39057;&#24182;&#26377;&#25928;&#39640;&#25928;&#22320;&#26816;&#27979;&#32771;&#35797;&#20013;&#21487;&#30097;&#27963;&#21160;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;OpenPose&#26694;&#26550;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#35782;&#21035;&#23398;&#29983;&#20132;&#25442;&#30340;
&lt;/p&gt;
&lt;p&gt;
Examinations are a crucial part of the learning process, and academic institutions invest significant resources into maintaining their integrity by preventing cheating from students or facilitators. However, cheating has become rampant in examination setups, compromising their integrity. The traditional method of relying on invigilators to monitor every student is impractical and ineffective. To address this issue, there is a need to continuously record exam sessions to monitor students for suspicious activities. However, these recordings are often too lengthy for invigilators to analyze effectively, and fatigue may cause them to miss significant details. To widen the coverage, invigilators could use fixed overhead or wearable cameras. This paper introduces a framework that uses automation to analyze videos and detect suspicious activities during examinations efficiently and effectively. We utilized the OpenPose framework and Convolutional Neural Network (CNN) to identify students exch
&lt;/p&gt;</description></item><item><title>EMS-YOLO&#26159;&#19968;&#31181;&#30452;&#25509;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#32780;&#19981;&#26159;ANN-SNN&#36716;&#25442;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#28145;&#24230;SNN&#30340;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11411</link><description>&lt;p&gt;
&#28145;&#24230;&#30452;&#25509;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Directly-Trained Spiking Neural Networks for Object Detection. (arXiv:2307.11411v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11411
&lt;/p&gt;
&lt;p&gt;
EMS-YOLO&#26159;&#19968;&#31181;&#30452;&#25509;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#32780;&#19981;&#26159;ANN-SNN&#36716;&#25442;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#28145;&#24230;SNN&#30340;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#19968;&#31181;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#39640;&#33021;&#25928;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26102;&#31354;&#21160;&#24577;&#26469;&#32534;&#30721;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#35757;&#32451;&#30340;&#28145;&#24230;SNN&#22312;&#23569;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#26469;&#35299;&#20915;&#30446;&#26631;&#26816;&#27979;&#22238;&#24402;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EMS-YOLO&#65292;&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#26032;&#22411;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#26694;&#26550;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#32780;&#19981;&#26159;ANN-SNN&#36716;&#25442;&#31574;&#30053;&#26469;&#35757;&#32451;&#28145;&#24230;SNN&#30340;&#23581;&#35797;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#33033;&#20914;&#27531;&#24046;&#22359;EMS-ResNet&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#25193;&#23637;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#30340;&#28145;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20855;&#26377;&#20302;&#21151;&#32791;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35777;&#26126;&#20102;EMS-ResNet&#21487;&#20197;&#36991;&#20813;&#26799;&#24230;&#28040;&#22833;&#25110;&#26799;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;ANN-SNN&#36716;&#25442;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) are brain-inspired energy-efficient models that encode information in spatiotemporal dynamics. Recently, deep SNNs trained directly have shown great success in achieving high performance on classification tasks with very few time steps. However, how to design a directly-trained SNN for the regression task of object detection still remains a challenging problem. To address this problem, we propose EMS-YOLO, a novel directly-trained SNN framework for object detection, which is the first trial to train a deep SNN with surrogate gradients for object detection rather than ANN-SNN conversion strategies. Specifically, we design a full-spike residual block, EMS-ResNet, which can effectively extend the depth of the directly-trained SNN with low power consumption. Furthermore, we theoretically analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding. The results demonstrate that our approach outperforms the state-of-the-art ANN-SNN conversion me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27169;&#22411;Pionono&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#21464;&#24322;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25429;&#25417;&#27599;&#20010;&#26631;&#35760;&#32773;&#30340;&#26631;&#35760;&#34892;&#20026;&#24182;&#23558;&#20854;&#19982;&#22270;&#20687;&#29305;&#24449;&#38598;&#25104;&#65292;&#20135;&#29983;&#27010;&#29575;&#20998;&#21106;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;Pionono&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#19968;&#33268;&#30340;&#20998;&#21106;&#22270;&#65292;&#20026;&#35786;&#26029;&#36807;&#31243;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.11397</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#21464;&#24322;&#30340;&#27010;&#29575;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation. (arXiv:2307.11397v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27169;&#22411;Pionono&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#21464;&#24322;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25429;&#25417;&#27599;&#20010;&#26631;&#35760;&#32773;&#30340;&#26631;&#35760;&#34892;&#20026;&#24182;&#23558;&#20854;&#19982;&#22270;&#20687;&#29305;&#24449;&#38598;&#25104;&#65292;&#20135;&#29983;&#27010;&#29575;&#20998;&#21106;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;Pionono&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#19968;&#33268;&#30340;&#20998;&#21106;&#22270;&#65292;&#20026;&#35786;&#26029;&#36807;&#31243;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#30340;&#21464;&#24322;&#24615;&#65292;&#21363;&#20351;&#26159;&#22312;&#21307;&#23398;&#19987;&#23478;&#20043;&#38388;&#20063;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#27010;&#29575;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#21464;&#24322;&#32593;&#32476;&#65288;Pionono&#65289;&#12290;&#23427;&#36890;&#36807;&#22810;&#32500;&#27010;&#29575;&#20998;&#24067;&#25429;&#25417;&#27599;&#20010;&#26631;&#35760;&#32773;&#30340;&#26631;&#35760;&#34892;&#20026;&#65292;&#24182;&#23558;&#27492;&#20449;&#24687;&#19982;&#22270;&#20687;&#30340;&#29305;&#24449;&#22270;&#38598;&#25104;&#36215;&#26469;&#65292;&#20135;&#29983;&#27010;&#29575;&#20998;&#21106;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;STAPLE&#12289;&#27010;&#29575;U-Net&#21644;&#22522;&#20110;&#28151;&#28102;&#30697;&#38453;&#30340;&#27169;&#22411;&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;Pionono&#39044;&#27979;&#22810;&#20010;&#19968;&#33268;&#30340;&#20998;&#21106;&#22270;&#65292;&#27169;&#25311;&#20102;&#35780;&#20998;&#32773;&#30340;&#19987;&#19994;&#24847;&#35265;&#65292;&#20026;&#35786;&#26029;&#36807;&#31243;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;&#22312;&#30495;&#23454;&#30340;&#30284;&#30151;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;Pionono&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#21307;&#23398;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation is a challenging task, particularly due to interand intra-observer variability, even between medical experts. In this paper, we propose a novel model, called Probabilistic Inter-Observer and iNtra-Observer variation NetwOrk (Pionono). It captures the labeling behavior of each rater with a multidimensional probability distribution and integrates this information with the feature maps of the image to produce probabilistic segmentation predictions. The model is optimized by variational inference and can be trained end-to-end. It outperforms state-of-the-art models such as STAPLE, Probabilistic U-Net, and models based on confusion matrices. Additionally, Pionono predicts multiple coherent segmentation maps that mimic the rater's expert opinion, which provides additional valuable information for the diagnostic process. Experiments on real-world cancer segmentation datasets demonstrate the high accuracy and efficiency of Pionono, making it a powerful tool for med
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#32763;&#36716;&#35838;&#22530;&#20934;&#22791;&#23398;&#20064;&#20013;&#30340;&#23398;&#29983;&#25552;&#20379;&#21363;&#26102;&#21453;&#39304;&#12290;&#35813;&#31995;&#32479;&#19981;&#20165;&#35299;&#20915;&#20102;&#23398;&#29983;&#22312;&#35266;&#30475;&#35838;&#22530;&#35270;&#39057;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#36824;&#36890;&#36807;&#23545;&#22238;&#31572;&#19982;&#19978;&#19979;&#25991;&#30340;&#23545;&#40784;&#21644;&#25945;&#24072;&#31572;&#26696;&#30340;&#25910;&#38598;&#65292;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#23398;&#20064;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.11388</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#20026;&#32763;&#36716;&#35838;&#22530;&#20934;&#22791;&#23398;&#20064;&#25552;&#20379;&#21363;&#26102;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-based System to Provide Immediate Feedback to Students in Flipped Classroom Preparation Learning. (arXiv:2307.11388v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#32763;&#36716;&#35838;&#22530;&#20934;&#22791;&#23398;&#20064;&#20013;&#30340;&#23398;&#29983;&#25552;&#20379;&#21363;&#26102;&#21453;&#39304;&#12290;&#35813;&#31995;&#32479;&#19981;&#20165;&#35299;&#20915;&#20102;&#23398;&#29983;&#22312;&#35266;&#30475;&#35838;&#22530;&#35270;&#39057;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#36824;&#36890;&#36807;&#23545;&#22238;&#31572;&#19982;&#19978;&#19979;&#25991;&#30340;&#23545;&#40784;&#21644;&#25945;&#24072;&#31572;&#26696;&#30340;&#25910;&#38598;&#65292;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#23398;&#20064;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#32763;&#36716;&#35838;&#22530;&#20934;&#22791;&#23398;&#20064;&#20013;&#30340;&#23398;&#29983;&#25552;&#20379;&#21363;&#26102;&#21453;&#39304;&#30340;&#31995;&#32479;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32763;&#36716;&#35838;&#22530;&#27169;&#24335;&#20013;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#30830;&#20445;&#23398;&#29983;&#22312;&#24773;&#24863;&#19978;&#21442;&#19982;&#24182;&#20445;&#25345;&#23398;&#20064;&#31215;&#26497;&#24615;&#12290;&#22312;&#32763;&#36716;&#35838;&#22530;&#20934;&#22791;&#20013;&#65292;&#23398;&#29983;&#32463;&#24120;&#23545;&#35838;&#22530;&#35270;&#39057;&#30340;&#20869;&#23481;&#26377;&#30097;&#38382;&#65292;&#20294;&#25945;&#24072;&#24456;&#38590;&#31435;&#21363;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#20351;&#29992;&#20102;ChatGPT API&#65292;&#22312;&#23454;&#38469;&#23454;&#36341;&#20013;&#29992;&#20110;&#20934;&#22791;&#23398;&#20064;&#30340;&#35270;&#39057;&#35266;&#30475;&#25903;&#25345;&#31995;&#32479;&#19978;&#36827;&#34892;&#24320;&#21457;&#12290;ChatGPT&#30340;&#22238;&#31572;&#24448;&#24448;&#19982;&#23398;&#29983;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22238;&#31572;&#19982;&#19978;&#19979;&#25991;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25910;&#38598;&#25945;&#24072;&#23545;&#23398;&#29983;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#23398;&#29983;&#30340;&#39069;&#22806;&#25351;&#23548;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25152;&#25552;&#20986;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a system that uses large language models to provide immediate feedback to students in flipped classroom preparation learning. This study aimed to solve challenges in the flipped classroom model, such as ensuring that students are emotionally engaged and motivated to learn. Students often have questions about the content of lecture videos in the preparation of flipped classrooms, but it is difficult for teachers to answer them immediately. The proposed system was developed using the ChatGPT API on a video-watching support system for preparation learning that is being used in real practice. Answers from ChatGPT often do not align with the context of the student's question. Therefore, this paper also proposes a method to align the answer with the context. This paper also proposes a method to collect the teacher's answers to the students' questions and use them as additional guides for the students. This paper discusses the design and implementation of the proposed syst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11373</link><description>&lt;p&gt;
&#36890;&#36807;Fenchel&#23545;&#20598;&#23454;&#29616;&#22810;&#26679;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Diverse Offline Imitation via Fenchel Duality. (arXiv:2307.11373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#39046;&#22495;&#65292;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21508;&#31181;&#24037;&#20316;&#25552;&#20986;&#20102;&#20197;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#65292;&#20316;&#20026;&#20869;&#22312;&#39537;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#38656;&#35201;&#22312;&#32447;&#29615;&#22659;&#35775;&#38382;&#30340;&#31639;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;\textit{&#31163;&#32447;}&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#32771;&#34385;&#20102;&#22312;KL-&#25955;&#24230;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30446;&#26631;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#32422;&#26463;&#30830;&#20445;&#27599;&#20010;&#25216;&#33021;&#30340;&#29366;&#24577;&#21344;&#29992;&#20445;&#25345;&#22312;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#29366;&#24577;&#25805;&#20316;&#35206;&#30422;&#29575;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#33539;&#22260;&#20869;&#19982;&#19987;&#23478;&#30340;&#29366;&#24577;&#21344;&#29992;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36830;&#25509;Fenchel&#23545;&#20598;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
&lt;/p&gt;</description></item><item><title>CohortGPT&#26159;&#19968;&#31181;&#22686;&#24378;&#22411;GPT&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#25307;&#21215;&#20219;&#21153;&#12290;&#27492;&#30740;&#31350;&#21457;&#29616;&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#19968;&#33324;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#23427;&#20204;&#24573;&#30053;&#20102;&#35821;&#35328;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.11346</link><description>&lt;p&gt;
CohortGPT&#65306;&#19968;&#31181;&#29992;&#20110;&#20020;&#24202;&#30740;&#31350;&#21442;&#19982;&#32773;&#25307;&#21215;&#30340;&#22686;&#24378;&#22411;GPT
&lt;/p&gt;
&lt;p&gt;
CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study. (arXiv:2307.11346v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11346
&lt;/p&gt;
&lt;p&gt;
CohortGPT&#26159;&#19968;&#31181;&#22686;&#24378;&#22411;GPT&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#25307;&#21215;&#20219;&#21153;&#12290;&#27492;&#30740;&#31350;&#21457;&#29616;&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#19968;&#33324;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#23427;&#20204;&#24573;&#30053;&#20102;&#35821;&#35328;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#21307;&#30103;&#25991;&#26412;&#65288;&#22914;&#20020;&#24202;&#35760;&#24405;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#65289;&#36827;&#34892;&#21442;&#19982;&#32773;&#25307;&#21215;&#26159;&#20020;&#24202;&#30740;&#31350;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#27979;&#35797;&#23427;&#20204;&#22312;&#35299;&#20915;&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#25307;&#21215;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#39064;&#35774;&#32622;&#65288;&#22914;&#21307;&#30103;&#25991;&#26412;&#20998;&#31867;&#65289;&#26102;&#65292;LLMs&#30340;&#24615;&#33021;&#19968;&#33324;&#12290;&#21487;&#33021;&#30340;&#35299;&#37322;&#26159;LLMs&#20165;&#20351;&#29992;&#21307;&#30103;&#25991;&#26412;&#65292;&#24573;&#30053;&#20102;&#35821;&#35328;&#20013;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Participant recruitment based on unstructured medical texts such as clinical notes and radiology reports has been a challenging yet important task for the cohort establishment in clinical research. Recently, Large Language Models (LLMs) such as ChatGPT have achieved tremendous success in various downstream tasks thanks to their promising performance in language understanding, inference, and generation. It is then natural to test their feasibility in solving the cohort recruitment task, which involves the classification of a given paragraph of medical text into disease label(s). However, when applied to knowledge-intensive problem settings such as medical text classification, where the LLMs are expected to understand the decision made by human experts and accurately identify the implied disease labels, the LLMs show a mediocre performance. A possible explanation is that, by only using the medical text, the LLMs neglect to use the rich context of additional information that languages aff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#24494;&#35843;&#31574;&#30053;&#65292;&#26088;&#22312;&#22522;&#20110;Maniskill2&#22522;&#20934;&#36827;&#19968;&#27493;&#25552;&#21319;&#20855;&#26377;&#36523;&#20307;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;ManiSkill2&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#31532;&#19968;&#21517;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11343</link><description>&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#36523;&#20307;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#36890;&#29992;&#25805;&#20316;&#25216;&#33021;&#30340;&#20004;&#38454;&#27573;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI. (arXiv:2307.11343v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#24494;&#35843;&#31574;&#30053;&#65292;&#26088;&#22312;&#22522;&#20110;Maniskill2&#22522;&#20934;&#36827;&#19968;&#27493;&#25552;&#21319;&#20855;&#26377;&#36523;&#20307;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;ManiSkill2&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#31532;&#19968;&#21517;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chat-GPT&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#20110;&#20855;&#26377;&#36523;&#20307;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#20855;&#26377;&#36523;&#20307;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#19982;&#35757;&#32451;&#29615;&#22659;&#30340;&#22823;&#37327;&#20132;&#20114;&#65292;&#36825;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;Maniskill2&#24341;&#20837;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#29289;&#29702;&#27169;&#25311;&#22522;&#20934;&#65292;&#29992;&#20110;&#25805;&#20316;&#21508;&#31181;3D&#23545;&#35937;&#12290;&#35813;&#22522;&#20934;&#20351;&#24471;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#28436;&#31034;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20182;&#20204;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#36866;&#24212;&#26410;&#35265;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#24494;&#35843;&#31574;&#30053;&#65292;&#26088;&#22312;&#22522;&#20110;Maniskill2&#22522;&#20934;&#36827;&#19968;&#27493;&#25552;&#21319;&#25105;&#20204;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;ManiSkill2&#25361;&#25112;&#36187;&#30340;&#19977;&#20010;&#36187;&#36947;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#25552;&#39640;&#20855;&#26377;&#36523;&#20307;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Chat-GPT has led to a surge of interest in Embodied AI. However, many existing Embodied AI models heavily rely on massive interactions with training environments, which may not be practical in real-world situations. To this end, the Maniskill2 has introduced a full-physics simulation benchmark for manipulating various 3D objects. This benchmark enables agents to be trained using diverse datasets of demonstrations and evaluates their ability to generalize to unseen scenarios in testing environments. In this paper, we propose a novel two-stage fine-tuning strategy that aims to further enhance the generalization capability of our model based on the Maniskill2 benchmark. Through extensive experiments, we demonstrate the effectiveness of our approach by achieving the 1st prize in all three tracks of the ManiSkill2 Challenge. Our findings highlight the potential of our method to improve the generalization abilities of Embodied AI models and pave the way for their ractical appli
&lt;/p&gt;</description></item><item><title>OpenGDA&#26159;&#19968;&#20010;&#29992;&#20110;&#36328;&#32593;&#32476;&#23398;&#20064;&#30340;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#20197;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11341</link><description>&lt;p&gt;
OpenGDA:&#29992;&#20110;&#36328;&#32593;&#32476;&#23398;&#20064;&#30340;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OpenGDA: Graph Domain Adaptation Benchmark for Cross-network Learning. (arXiv:2307.11341v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11341
&lt;/p&gt;
&lt;p&gt;
OpenGDA&#26159;&#19968;&#20010;&#29992;&#20110;&#36328;&#32593;&#32476;&#23398;&#20064;&#30340;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#20197;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#22312;&#36328;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#26088;&#22312;&#20256;&#36882;&#26631;&#31614;&#25110;&#32467;&#26500;&#30693;&#35782;&#12290;&#30446;&#21069;&#65292;&#22312;&#35780;&#20272;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#26102;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#20027;&#35201;&#29992;&#20110;&#29305;&#23450;&#30340;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#23545;&#20110;&#36793;&#32423;&#21644;&#22270;&#32423;&#20219;&#21153;&#30340;&#35752;&#35770;&#30456;&#23545;&#36739;&#23569;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#20027;&#35201;&#22312;&#26377;&#38480;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#22914;&#31038;&#20132;&#32593;&#32476;&#25110;&#24341;&#29992;&#32593;&#32476;&#65292;&#32570;&#20047;&#22312;&#26356;&#20016;&#23500;&#22330;&#26223;&#20013;&#27979;&#35797;&#27169;&#22411;&#33021;&#21147;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#65292;&#25552;&#39640;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OpenGDA&#30340;&#22522;&#20934;&#12290;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#39044;&#22788;&#29702;&#21644;&#32479;&#19968;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#65288;&#33410;&#28857;&#65292;&#36793;&#65292;&#22270;&#65289;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#20110;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#28085;&#30422;&#20102;&#32593;&#32476;&#20449;&#24687;&#31995;&#32479;&#65292;&#22478;&#24066;&#31995;&#32479;&#21644;&#33258;&#28982;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#38598;&#25104;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#21644;&#26368;&#32456;&#30340;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph domain adaptation models are widely adopted in cross-network learning tasks, with the aim of transferring labeling or structural knowledge. Currently, there mainly exist two limitations in evaluating graph domain adaptation models. On one side, they are primarily tested for the specific cross-network node classification task, leaving tasks at edge-level and graph-level largely under-explored. Moreover, they are primarily tested in limited scenarios, such as social networks or citation networks, lacking validation of model's capability in richer scenarios. As comprehensively assessing models could enhance model practicality in real-world applications, we propose a benchmark, known as OpenGDA. It provides abundant pre-processed and unified datasets for different types of tasks (node, edge, graph). They originate from diverse scenarios, covering web information systems, urban systems and natural systems. Furthermore, it integrates state-of-the-art models with standardized and end-to
&lt;/p&gt;</description></item><item><title>Tri-MipRF &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#21521; Mip &#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#28388;&#27874;&#30340;&#19977;&#32500;&#29305;&#24449;&#31354;&#38388;&#20998;&#35299;&#20026;&#27491;&#20132;&#30340; mipmaps&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#21363;&#26102;&#37325;&#24314;&#21644;&#25239;&#38191;&#40831;&#39640;&#20445;&#30495;&#28210;&#26579;&#12290;</title><link>http://arxiv.org/abs/2307.11335</link><description>&lt;p&gt;
Tri-MipRF: &#39640;&#25928;&#25239;&#38191;&#40831;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#19977;&#21521; Mip &#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields. (arXiv:2307.11335v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11335
&lt;/p&gt;
&lt;p&gt;
Tri-MipRF &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#21521; Mip &#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#28388;&#27874;&#30340;&#19977;&#32500;&#29305;&#24449;&#31354;&#38388;&#20998;&#35299;&#20026;&#27491;&#20132;&#30340; mipmaps&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#21363;&#26102;&#37325;&#24314;&#21644;&#25239;&#38191;&#40831;&#39640;&#20445;&#30495;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#25225;&#25321;&#65292;&#20363;&#22914;&#65292;MipNeRF&#33021;&#22815;&#21576;&#29616;&#31934;&#32454;&#35814;&#32454;&#21644;&#25239;&#38191;&#40831;&#30340;&#28210;&#26579;&#25928;&#26524;&#65292;&#20294;&#38656;&#35201;&#25968;&#22825;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#32780;Instant-ngp&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#23436;&#25104;&#37325;&#24314;&#65292;&#20294;&#22312;&#28210;&#26579;&#19981;&#21516;&#36317;&#31163;&#25110;&#20998;&#36776;&#29575;&#26102;&#20250;&#20986;&#29616;&#27169;&#31946;&#25110;&#38191;&#40831;&#21270;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#24573;&#30053;&#20102;&#37319;&#26679;&#21306;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#21521; Mip &#32534;&#30721;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#21363;&#26102;&#37325;&#24314;&#21644;&#25239;&#38191;&#40831;&#39640;&#20445;&#30495;&#28210;&#26579;&#12290;&#20851;&#38190;&#22312;&#20110;&#23558;&#39044;&#28388;&#27874;&#30340;&#19977;&#32500;&#29305;&#24449;&#31354;&#38388;&#20998;&#35299;&#20026;&#19977;&#20010;&#27491;&#20132;&#30340; mipmaps&#12290;&#36890;&#36807;&#21033;&#29992;&#20108;&#32500;&#39044;&#28388;&#27874;&#29305;&#24449;&#22270;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#19977;&#32500;&#21306;&#22495;&#37319;&#26679;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#28210;&#26579;&#36136;&#37327;&#32780;&#19981;&#25439;&#22833;&#25928;&#29575;&#12290;&#20026;&#24212;&#23545;&#26032;&#30340;&#19977;&#21521; Mip &#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38181;&#25237;&#23556;&#28210;&#26579;&#25216;&#26415;&#65292;&#20197;&#39640;&#25928;&#37319;&#26679;&#25239;&#38191;&#40831;&#30340;&#19977;&#32500;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous progress in neural radiance fields (NeRF), we still face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF presents fine-detailed and anti-aliased renderings but takes days for training, while Instant-ngp can accomplish the reconstruction in a few minutes but suffers from blurring or aliasing when rendering at various distances or resolutions due to ignoring the sampling area. To this end, we propose a novel Tri-Mip encoding that enables both instant reconstruction and anti-aliased high-fidelity rendering for neural radiance fields. The key is to factorize the pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can efficiently perform 3D area sampling by taking advantage of 2D pre-filtered feature maps, which significantly elevates the rendering quality without sacrificing efficiency. To cope with the novel Tri-Mip representation, we propose a cone-casting rendering technique to efficiently sample anti-aliased 3D fea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#33832;&#36203;&#21202;&#20197;&#21335;&#38750;&#27954;&#35937;&#31227;&#21160;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#23395;&#33410;&#21464;&#21270;&#21644;&#38477;&#38632;&#27169;&#24335;&#31561;&#21160;&#24577;&#39537;&#21160;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#39044;&#27979;&#29983;&#24577;&#22240;&#32032;&#23545;&#35937;&#36801;&#24473;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#24182;&#20026;&#21046;&#23450;&#20445;&#25252;&#31574;&#30053;&#25552;&#20379;&#20102;&#32508;&#21512;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2307.11325</link><description>&lt;p&gt;
&#33832;&#36203;&#21202;&#20197;&#21335;&#38750;&#27954;&#35937;&#31227;&#21160;&#30340;&#20998;&#26512;&#65306;&#29983;&#24577;&#23398;&#12289;&#27668;&#20505;&#21644;&#20445;&#25252;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives. (arXiv:2307.11325v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#33832;&#36203;&#21202;&#20197;&#21335;&#38750;&#27954;&#35937;&#31227;&#21160;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#23395;&#33410;&#21464;&#21270;&#21644;&#38477;&#38632;&#27169;&#24335;&#31561;&#21160;&#24577;&#39537;&#21160;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#39044;&#27979;&#29983;&#24577;&#22240;&#32032;&#23545;&#35937;&#36801;&#24473;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#24182;&#20026;&#21046;&#23450;&#20445;&#25252;&#31574;&#30053;&#25552;&#20379;&#20102;&#32508;&#21512;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35937;&#19982;&#29615;&#22659;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#29983;&#24577;&#23398;&#21644;&#20445;&#25252;&#31574;&#30053;&#37117;&#26377;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#26469;&#35299;&#35835;&#33832;&#36203;&#21202;&#20197;&#21335;&#38750;&#27954;&#35937;&#31227;&#21160;&#30340;&#22797;&#26434;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#23395;&#33410;&#21464;&#21270;&#21644;&#38477;&#38632;&#27169;&#24335;&#31561;&#20851;&#38190;&#29983;&#24577;&#39537;&#21160;&#22240;&#32032;&#12290;&#23613;&#31649;&#22260;&#32469;&#36825;&#20123;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#22240;&#32032;&#23384;&#22312;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#38750;&#27954;&#21160;&#24577;&#26223;&#35266;&#32972;&#26223;&#19979;&#35937;&#36801;&#24473;&#34892;&#20026;&#30340;&#20840;&#38754;&#35270;&#35282;&#12290;&#25105;&#20204;&#32508;&#21512;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#36825;&#20123;&#29983;&#24577;&#20915;&#23450;&#22240;&#32032;&#23545;&#35937;&#36801;&#24473;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#36825;&#26159;&#24314;&#31435;&#30693;&#24773;&#30340;&#20445;&#25252;&#31574;&#30053;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#32771;&#34385;&#21040;&#20840;&#29699;&#27668;&#20505;&#21464;&#21270;&#23545;&#23395;&#33410;&#21644;&#38477;&#38632;&#27169;&#24335;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#39044;&#27979;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#26410;&#26469;&#21487;&#33021;&#20250;&#23545;&#35937;&#30340;&#34892;&#21160;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#26524;&#26088;&#22312;&#19981;&#20165;&#25512;&#36827;&#23545;&#31227;&#21160;&#29983;&#24577;&#23398;&#30340;&#29702;&#35299;&#65292;&#21516;&#26102;&#20063;&#20026;&#20445;&#25252;&#23454;&#36341;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interaction between elephants and their environment has profound implications for both ecology and conservation strategies. This study presents an analytical approach to decipher the intricate patterns of elephant movement in Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal variations and rainfall patterns. Despite the complexities surrounding these influential factors, our analysis provides a holistic view of elephant migratory behavior in the context of the dynamic African landscape. Our comprehensive approach enables us to predict the potential impact of these ecological determinants on elephant migration, a critical step in establishing informed conservation strategies. This projection is particularly crucial given the impacts of global climate change on seasonal and rainfall patterns, which could substantially influence elephant movements in the future. The findings of our work aim to not only advance the understanding of movement ecology but also f
&lt;/p&gt;</description></item><item><title>HVDetFusion&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#26694;&#26550;&#65292;&#25903;&#25345;&#32431;&#30456;&#26426;&#25968;&#25454;&#21644;&#30456;&#26426;+&#38647;&#36798;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#26816;&#27979;&#65292;&#36890;&#36807;&#20462;&#25913;&#26694;&#26550;&#21644;&#21033;&#29992;&#20808;&#39564;&#20449;&#24687;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#36807;&#28388;&#35823;&#25253;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.11323</link><description>&lt;p&gt;
HVDetFusion&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework. (arXiv:2307.11323v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11323
&lt;/p&gt;
&lt;p&gt;
HVDetFusion&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#26694;&#26550;&#65292;&#25903;&#25345;&#32431;&#30456;&#26426;&#25968;&#25454;&#21644;&#30456;&#26426;+&#38647;&#36798;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#26816;&#27979;&#65292;&#36890;&#36807;&#20462;&#25913;&#26694;&#26550;&#21644;&#21033;&#29992;&#20808;&#39564;&#20449;&#24687;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#36807;&#28388;&#35823;&#25253;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;3D&#30446;&#26631;&#26816;&#27979;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#24863;&#30693;&#27169;&#22359;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;SOTA&#31639;&#27861;&#32467;&#21512;&#20102;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20256;&#24863;&#22120;&#65292;&#20294;&#21463;&#21040;&#26114;&#36149;&#30340;&#28608;&#20809;&#38647;&#36798;&#20215;&#26684;&#30340;&#38480;&#21046;&#65292;&#24403;&#21069;&#20027;&#27969;&#30340;&#26041;&#26696;&#26159;&#32431;&#30456;&#26426;&#20256;&#24863;&#22120;&#25110;&#30456;&#26426;+&#38647;&#36798;&#20256;&#24863;&#22120;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#31639;&#27861;HVDetFusion&#65292;&#23427;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#26816;&#27979;&#31639;&#27861;&#65292;&#19981;&#20165;&#25903;&#25345;&#32431;&#30456;&#26426;&#25968;&#25454;&#20316;&#20026;&#26816;&#27979;&#36755;&#20837;&#65292;&#36824;&#21487;&#20197;&#23558;&#38647;&#36798;&#25968;&#25454;&#21644;&#30456;&#26426;&#25968;&#25454;&#36827;&#34892;&#34701;&#21512;&#36755;&#20837;&#12290;&#30456;&#26426;&#25968;&#25454;&#27969;&#19981;&#20381;&#36182;&#20110;&#38647;&#36798;&#25968;&#25454;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20043;&#21069;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#22312;&#32431;&#30456;&#26426;&#25968;&#25454;&#27969;&#20013;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;Bevdet4D&#26694;&#26550;&#20197;&#25552;&#39640;&#24863;&#30693;&#25928;&#26524;&#21644;&#26356;&#39640;&#25928;&#30340;&#25512;&#26029;&#65292;&#24182;&#19988;&#35813;&#25968;&#25454;&#27969;&#26377;&#23436;&#25972;&#30340;3D&#26816;&#27979;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#34701;&#21512;&#38647;&#36798;&#20449;&#21495;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#29289;&#20307;&#20301;&#32622;&#30340;&#20808;&#39564;&#20449;&#24687;&#26469;&#36807;&#28388;&#21407;&#22987;&#38647;&#36798;&#25968;&#25454;&#30340;&#35823;&#25253;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of autonomous driving, 3D object detection is a very important perception module. Although the current SOTA algorithm combines Camera and Lidar sensors, limited by the high price of Lidar, the current mainstream landing schemes are pure Camera sensors or Camera+Radar sensors. In this study, we propose a new detection algorithm called HVDetFusion, which is a multi-modal detection algorithm that not only supports pure camera data as input for detection, but also can perform fusion input of radar data and camera data. The camera stream does not depend on the input of Radar data, thus addressing the downside of previous methods. In the pure camera stream, we modify the framework of Bevdet4D for better perception and more efficient inference, and this stream has the whole 3D detection output. Further, to incorporate the benefits of Radar signals, we use the prior information of different object positions to filter the false positive information of the original radar data, accor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#20041;&#24120;&#35782;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#27169;&#31946;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#25972;&#29702;&#26700;&#23376;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#21487;&#20197;&#25512;&#29702;&#20986;&#20154;&#31867;&#34892;&#20026;&#30340;&#24120;&#35782;&#12290;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21463;&#38480;&#65292;&#20294;&#36890;&#36807;&#32771;&#34385;&#24863;&#30693;&#21644;&#20302;&#32423;&#25511;&#21046;&#22240;&#32032;&#65292;&#21487;&#20197;&#35299;&#20915;&#25972;&#29702;&#26700;&#23376;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.11319</link><description>&lt;p&gt;
&#22914;&#20309;&#25972;&#29702;&#19968;&#24352;&#26700;&#23376;&#65306;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#20041;&#24120;&#35782;&#25512;&#29702;&#35299;&#20915;&#20855;&#26377;&#27169;&#31946;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
How to Tidy Up a Table: Fusing Visual and Semantic Commonsense Reasoning for Robotic Tasks with Vague Objectives. (arXiv:2307.11319v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#20041;&#24120;&#35782;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#27169;&#31946;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#25972;&#29702;&#26700;&#23376;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#21487;&#20197;&#25512;&#29702;&#20986;&#20154;&#31867;&#34892;&#20026;&#30340;&#24120;&#35782;&#12290;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21463;&#38480;&#65292;&#20294;&#36890;&#36807;&#32771;&#34385;&#24863;&#30693;&#21644;&#20302;&#32423;&#25511;&#21046;&#22240;&#32032;&#65292;&#21487;&#20197;&#35299;&#20915;&#25972;&#29702;&#26700;&#23376;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#27169;&#31946;&#30340;&#30446;&#26631;&#32473;&#26426;&#22120;&#20154;&#25216;&#26415;&#24102;&#26469;&#20102;&#38271;&#26399;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#24456;&#38590;&#23450;&#20041;&#35268;&#21017;&#12289;&#22870;&#21169;&#25110;&#32422;&#26463;&#20197;&#36827;&#34892;&#20248;&#21270;&#12290;&#20687;&#25972;&#29702;&#19968;&#24352;&#20940;&#20081;&#30340;&#26700;&#23376;&#36825;&#26679;&#30340;&#20219;&#21153;&#23545;&#20154;&#31867;&#26469;&#35828;&#21487;&#33021;&#24456;&#31616;&#21333;&#65292;&#20294;&#30001;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#27495;&#20041;&#21644;&#28789;&#27963;&#24615;&#65292;&#34920;&#36798;&#25972;&#27905;&#30340;&#26631;&#20934;&#21364;&#24456;&#22797;&#26434;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#20013;&#65292;&#25105;&#20204;&#26377;&#20102;&#36890;&#36807;&#36825;&#20123;&#27169;&#31946;&#30446;&#26631;&#36827;&#34892;&#25512;&#29702;&#30340;&#26426;&#20250;&#65306;LLM&#36890;&#36807;&#23398;&#20064;&#22823;&#37327;&#20154;&#31867;&#25968;&#25454;&#26469;&#25429;&#25417;&#26377;&#20851;&#20154;&#31867;&#34892;&#20026;&#30340;&#26377;&#24847;&#20041;&#30340;&#24120;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#20165;&#35757;&#32451;&#20110;&#35821;&#35328;&#36755;&#20837;&#65292;&#23427;&#20204;&#22312;&#24863;&#30693;&#21644;&#20302;&#32423;&#25511;&#21046;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#22240;&#27492;&#21487;&#33021;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25972;&#29702;&#26700;&#23376;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#27169;&#31946;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25972;&#29702;&#19968;&#24352;&#26700;&#23376;&#30340;&#20219;&#21153;&#19981;&#20165;&#28041;&#21450;&#25353;&#29031;&#31867;&#22411;&#21644;&#21151;&#33021;&#23545;&#29289;&#20307;&#36827;&#34892;&#32858;&#31867;&#20197;&#23454;&#29616;&#35821;&#20041;&#25972;&#27905;&#65292;&#36824;&#38656;&#35201;&#32771;&#34385;&#24863;&#30693;&#21644;&#20302;&#32423;&#25511;&#21046;&#26041;&#38754;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vague objectives in many real-life scenarios pose long-standing challenges for robotics, as defining rules, rewards, or constraints for optimization is difficult. Tasks like tidying a messy table may appear simple for humans, but articulating the criteria for tidiness is complex due to the ambiguity and flexibility in commonsense reasoning. Recent advancement in Large Language Models (LLMs) offers us an opportunity to reason over these vague objectives: learned from extensive human data, LLMs capture meaningful common sense about human behavior. However, as LLMs are trained solely on language input, they may struggle with robotic tasks due to their limited capacity to account for perception and low-level controls. In this work, we propose a simple approach to solve the task of table tidying, an example of robotic tasks with vague objectives. Specifically, the task of tidying a table involves not just clustering objects by type and functionality for semantic tidiness but also considerin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; XLDA &#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#36793;&#32536;&#19978;&#36827;&#34892;&#26497;&#31471;&#20998;&#31867;&#65292;&#20854;&#20013;&#20351;&#29992;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#20998;&#31867;&#22120;&#31561;&#25928;&#20110;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#65292;&#36890;&#36807;&#20248;&#21270;&#23454;&#29616;&#20102;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26497;&#31471;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.11317</link><description>&lt;p&gt;
XLDA: &#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#29992;&#20110;&#22312;&#36793;&#32536;&#19978;&#36827;&#34892;&#26497;&#31471;&#20998;&#31867;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge. (arXiv:2307.11317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; XLDA &#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#36793;&#32536;&#19978;&#36827;&#34892;&#26497;&#31471;&#20998;&#31867;&#65292;&#20854;&#20013;&#20351;&#29992;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#20998;&#31867;&#22120;&#31561;&#25928;&#20110;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#65292;&#36890;&#36807;&#20248;&#21270;&#23454;&#29616;&#20102;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26497;&#31471;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#22312;&#36793;&#32536;&#19978;&#37096;&#32626;&#21463;&#38480;&#31867;&#21035;&#65288;&#26368;&#22810;1000&#20010;&#65289;&#30340;&#22686;&#37327;&#23398;&#20064;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#26497;&#24230;&#20998;&#31867;&#22330;&#26223;&#20013;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#65306;&#65288;a&#65289;XLDA&#65292;&#19968;&#31181;&#29992;&#20110;&#36793;&#32536;&#37096;&#32626;&#20013;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;LDA&#20998;&#31867;&#22120;&#34987;&#35777;&#26126;&#19982;FC&#23618;&#31561;&#25928;&#65292;&#21253;&#25324;&#22312;&#26497;&#24230;&#20998;&#31867;&#22330;&#26223;&#20013;&#65307;&#65288;b&#65289;&#20248;&#21270;&#20197;&#23454;&#29616;&#22522;&#20110;XLDA&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#20854;&#20013;&#23384;&#22312;&#21487;&#29992;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25209;&#22788;&#29702;&#35757;&#32451;&#26041;&#27861;&#19979;&#30340;42&#20493;&#21152;&#36895;&#21644;&#26368;&#36817;&#37051;&#25628;&#32034;&#22312;AliProducts&#65288;50k&#31867;&#21035;&#65289;&#21644;Google Landmarks V2&#65288;81k&#31867;&#21035;&#65289;&#31561;&#26497;&#31471;&#25968;&#25454;&#38598;&#19978;&#30340;5&#20493;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming Linear Discriminant Analysis (LDA) while proven in Class-incremental Learning deployments at the edge with limited classes (upto 1000), has not been proven for deployment in extreme classification scenarios. In this paper, we present: (a) XLDA, a framework for Class-IL in edge deployment where LDA classifier is proven to be equivalent to FC layer including in extreme classification scenarios, and (b) optimizations to enable XLDA-based training and inference for edge deployment where there is a constraint on available compute resources. We show up to 42x speed up using a batched training approach and up to 5x inference speedup with nearest neighbor search on extreme datasets like AliProducts (50k classes) and Google Landmarks V2 (81k classes)
&lt;/p&gt;</description></item><item><title>DPM-OT&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#28508;&#21464;&#37327;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;DPMs&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32422;10&#20010;&#20989;&#25968;&#35780;&#20272;&#20869;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.11308</link><description>&lt;p&gt;
DPM-OT:&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport. (arXiv:2307.11308v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11308
&lt;/p&gt;
&lt;p&gt;
DPM-OT&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#28508;&#21464;&#37327;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;DPMs&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32422;10&#20010;&#20989;&#25968;&#35780;&#20272;&#20869;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#20013;&#37319;&#26679;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#20998;&#27573;&#20998;&#24067;&#21464;&#25442;&#65292;&#36890;&#24120;&#38656;&#35201;&#20960;&#30334;&#25110;&#20960;&#21315;&#20010;&#36870;&#25193;&#25955;&#36712;&#36857;&#27493;&#39588;&#65292;&#25165;&#33021;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#26368;&#36817;&#22312;&#35774;&#35745;DPMs&#30340;&#24555;&#36895;&#21462;&#26679;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25110;&#35843;&#25972;&#26041;&#24046;&#36827;&#21270;&#26354;&#32447;&#25110;&#38477;&#22122;&#26041;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#21462;&#26679;&#36895;&#24230;&#21644;&#26679;&#26412;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#19981;&#33021;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#30701;&#36317;&#31163;&#19978;&#32463;&#24120;&#36935;&#21040;&#27169;&#24335;&#28151;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#36870;&#25193;&#25955;&#35270;&#20026;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#28508;&#21464;&#37327;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;DPM-OT&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;OT&#26144;&#23556;&#34920;&#31034;&#30340;&#39640;&#36895;DPMs&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32422;10&#20010;&#20989;&#25968;&#35780;&#20272;&#20869;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#36890;&#36807;&#35745;&#31639;&#25968;&#25454;&#28508;&#21464;&#37327;&#21644;&#30333;&#22122;&#22768;&#20043;&#38388;&#30340;&#21322;&#31163;&#25955;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#26465;&#20174;&#21069;&#33267;&#21518;&#30340;&#24555;&#36895;&#25463;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling from diffusion probabilistic models (DPMs) can be viewed as a piecewise distribution transformation, which generally requires hundreds or thousands of steps of the inverse diffusion trajectory to get a high-quality image. Recent progress in designing fast samplers for DPMs achieves a trade-off between sampling speed and sample quality by knowledge distillation or adjusting the variance schedule or the denoising equation. However, it can't be optimal in both aspects and often suffer from mode mixture in short steps. To tackle this problem, we innovatively regard inverse diffusion as an optimal transport (OT) problem between latents at different stages and propose the DPM-OT, a unified learning framework for fast DPMs with a direct expressway represented by OT map, which can generate high-quality samples within around 10 function evaluations. By calculating the semi-discrete optimal transport map between the data latents and the white noise, we obtain an expressway from the prio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20869;&#26680;&#30340;&#31163;&#32447;&#32972;&#26223;&#21452;&#21521;&#31454;&#26631;&#32773;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#19978;&#19979;&#25991;&#30340;&#30456;&#20284;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.11288</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;&#30340;&#31163;&#32447;&#32972;&#26223;&#21452;&#21521;&#31454;&#26631;&#32773;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kernelized Offline Contextual Dueling Bandits. (arXiv:2307.11288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20869;&#26680;&#30340;&#31163;&#32447;&#32972;&#26223;&#21452;&#21521;&#31454;&#26631;&#32773;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#19978;&#19979;&#25991;&#30340;&#30456;&#20284;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#21453;&#39304;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#20123;&#24212;&#29992;&#20013;&#26080;&#27861;&#30452;&#25509;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#20154;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#26368;&#26032;&#23454;&#20363;&#12290;&#23545;&#20110;&#35768;&#22810;&#36825;&#20123;&#24212;&#29992;&#65292;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#30340;&#25104;&#26412;&#21487;&#33021;&#30456;&#24403;&#39640;&#29978;&#33267;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#20195;&#29702;&#36890;&#24120;&#21487;&#20197;&#36873;&#25321;&#33719;&#24471;&#20154;&#31867;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#26368;&#39640;&#25928;&#22320;&#30830;&#23450;&#19968;&#20010;&#33391;&#22909;&#31574;&#30053;&#65292;&#24182;&#24341;&#20837;&#20102;&#31163;&#32447;&#32972;&#26223;&#21452;&#21521;&#31454;&#26631;&#32773;&#35774;&#32622;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#35774;&#32622;&#25552;&#20379;&#20102;&#19968;&#20010;&#19978;&#30028;&#32622;&#20449;&#21306;&#38388;&#26679;&#24335;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#36951;&#25022;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#32463;&#39564;&#35777;&#23454;&#36825;&#31181;&#26041;&#27861;&#32988;&#36807;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#19978;&#19979;&#25991;&#30340;&#31867;&#20284;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31867;&#20284;AFT&#30340;&#26041;&#27861;&#35770;&#65292;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#35745;&#31639;&#30340;&#19978;&#30028;&#26356;&#31934;&#30830;&#22320;&#25429;&#25417;&#28151;&#21512;MKNF&#30693;&#35782;&#24211;&#30340;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.11286</link><description>&lt;p&gt;
&#28040;&#38500;&#28151;&#21512;&#25512;&#29702;&#31995;&#32479;&#20013;&#24847;&#22806;&#30340;&#31283;&#23450;&#22266;&#23450;&#28857;
&lt;/p&gt;
&lt;p&gt;
Eliminating Unintended Stable Fixpoints for Hybrid Reasoning Systems. (arXiv:2307.11286v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11286
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31867;&#20284;AFT&#30340;&#26041;&#27861;&#35770;&#65292;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#35745;&#31639;&#30340;&#19978;&#30028;&#26356;&#31934;&#30830;&#22320;&#25429;&#25417;&#28151;&#21512;MKNF&#30693;&#35782;&#24211;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#38750;&#21333;&#35843;&#35821;&#20041;&#21487;&#20197;&#34987;&#34920;&#36798;&#20026;&#22312;AFT&#65288;Approximation Fixpoint Theory&#65289;&#19979;&#23450;&#20041;&#30340;&#36817;&#20284;&#35745;&#31639;&#22120;&#12290;&#20351;&#29992;&#20256;&#32479;&#30340;AFT&#29702;&#35770;&#65292;&#26080;&#27861;&#23450;&#20041;&#20381;&#36182;&#20110;&#20808;&#21069;&#36845;&#20195;&#35745;&#31639;&#30340;&#31283;&#23450;&#20462;&#35746;&#20449;&#24687;&#30340;&#36817;&#20284;&#35745;&#31639;&#22120;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23558;&#32463;&#20856;&#21542;&#23450;&#32435;&#20837;&#38750;&#21333;&#35843;&#25512;&#29702;&#30340;&#35821;&#20041;&#26469;&#35828;&#65292;&#36825;&#20123;&#20449;&#24687;&#24456;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31867;&#20284;AFT&#30340;&#26041;&#27861;&#35770;&#65292;&#21487;&#20197;&#21033;&#29992;&#20808;&#21069;&#35745;&#31639;&#30340;&#19978;&#30028;&#26356;&#31934;&#30830;&#22320;&#25429;&#25417;&#35821;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#26368;&#26032;&#30340;&#36817;&#20284;&#35745;&#31639;&#22120;&#65292;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#36866;&#29992;&#20110;&#28151;&#21512;MKNF&#65288;&#26368;&#23567;&#30693;&#35782;&#21644;&#21542;&#23450;&#20026;&#22833;&#36133;&#65289;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide variety of nonmonotonic semantics can be expressed as approximators defined under AFT (Approximation Fixpoint Theory). Using traditional AFT theory, it is not possible to define approximators that rely on information computed in previous iterations of stable revision. However, this information is rich for semantics that incorporate classical negation into nonmonotonic reasoning. In this work, we introduce a methodology resembling AFT that can utilize priorly computed upper bounds to more precisely capture semantics. We demonstrate our framework's applicability to hybrid MKNF (minimal knowledge and negation as failure) knowledge bases by extending the state-of-the-art approximator.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;3D&#25216;&#26415;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21512;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#38598;&#33719;&#21462;&#22256;&#38590;&#38382;&#39064;&#65292;&#24182;&#22312;&#32467;&#32928;&#24687;&#32905;&#20998;&#21106;&#20219;&#21153;&#19978;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11253</link><description>&lt;p&gt;
&#32852;&#21512;&#21333;&#20391;&#21512;&#25104;&#26080;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#21644;&#32467;&#32928;&#30284;&#39044;&#38450;&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention. (arXiv:2307.11253v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11253
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;3D&#25216;&#26415;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21512;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#38598;&#33719;&#21462;&#22256;&#38590;&#38382;&#39064;&#65292;&#24182;&#22312;&#32467;&#32928;&#24687;&#32905;&#20998;&#21106;&#20219;&#21153;&#19978;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#12289;&#26631;&#20934;&#21270;&#38382;&#39064;&#21644;&#32570;&#20047;&#27880;&#37322;&#65292;&#25968;&#25454;&#38598;&#24456;&#38590;&#33719;&#24471;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;3D&#25216;&#26415;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#32467;&#21512;&#20135;&#29983;&#36924;&#30495;&#30340;&#21512;&#25104;&#22270;&#20687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CUT-seg&#65292;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#20998;&#21106;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#19968;&#36215;&#35757;&#32451;&#20135;&#29983;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#23398;&#20064;&#20998;&#21106;&#24687;&#32905;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#21333;&#20391;&#36716;&#25442;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#30340;&#20869;&#23384;&#26174;&#33879;&#36739;&#23569;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#28155;&#21152;&#19968;&#20010;&#20998;&#21106;&#27169;&#22411;&#12290;&#30456;&#27604;&#20854;&#20182;&#20869;&#23384;&#23494;&#38598;&#22411;&#30340;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#38656;&#35201;&#20004;&#38454;&#27573;&#35757;&#32451;&#65292;CUT-seg&#34920;&#29616;&#26356;&#22909;&#65292;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#19968;&#20010;&#30495;&#23454;&#22270;&#20687;&#21644;&#38646;&#30495;&#23454;&#27880;&#37322;&#23601;&#21487;&#20197;&#22312;&#20116;&#20010;&#30495;&#23454;&#24687;&#32905;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20316;&#20026;&#26412;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Deep learning has shown excellent performance in analysing medical images. However, datasets are difficult to obtain due privacy issues, standardization problems, and lack of annotations. We address these problems by producing realistic synthetic images using a combination of 3D technologies and generative adversarial networks. We propose CUT-seg, a joint training where a segmentation model and a generative model are jointly trained to produce realistic images while learning to segment polyps. We take advantage of recent one-sided translation models because they use significantly less memory, allowing us to add a segmentation model in the training loop. CUT-seg performs better, is computationally less expensive, and requires less real images than other memory-intensive image translation approaches that require two stage training. Promising results are achieved on five real polyp segmentation datasets using only one real image and zero real annotations. As a part of this study we releas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#21033;&#29992;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#27169;&#22411;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#28388;&#27874;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20256;&#20837;&#30340;&#30005;&#33655;&#27874;&#24418;&#36716;&#25442;&#20026;&#20108;&#20540;&#20107;&#20214;&#27969;&#65292;&#24182;&#20248;&#21270;SNN&#30340;&#31995;&#32479;&#35774;&#35745;&#21644;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20449;&#21495;&#26377;&#25928;&#29575;&#32422;&#20026;91%&#30340;SNN&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;&#20960;&#20046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#21322;&#12290;</title><link>http://arxiv.org/abs/2307.11242</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#28388;&#27874;&#22312;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments. (arXiv:2307.11242v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#21033;&#29992;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#27169;&#22411;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#28388;&#27874;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20256;&#20837;&#30340;&#30005;&#33655;&#27874;&#24418;&#36716;&#25442;&#20026;&#20108;&#20540;&#20107;&#20214;&#27969;&#65292;&#24182;&#20248;&#21270;SNN&#30340;&#31995;&#32479;&#35774;&#35745;&#21644;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20449;&#21495;&#26377;&#25928;&#29575;&#32422;&#20026;91%&#30340;SNN&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;&#20960;&#20046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#21033;&#29992;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#27169;&#22411;&#23545;&#22312;&#39640;&#20142;&#24230;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#36827;&#34892;&#30340;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#28388;&#27874;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21457;&#32039;&#20945;&#22411;&#31070;&#32463;&#24418;&#24577;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#31890;&#23376;&#30340;&#27178;&#21521;&#21160;&#37327;&#26469;&#28388;&#38500;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#30446;&#26631;&#26159;&#20943;&#23569;&#20256;&#36755;&#21040;&#19979;&#28216;&#30005;&#23376;&#35774;&#22791;&#30340;&#25968;&#25454;&#37327;&#12290;&#20256;&#20837;&#30340;&#30005;&#33655;&#27874;&#24418;&#34987;&#36716;&#25442;&#20026;&#20108;&#20540;&#20107;&#20214;&#27969;&#65292;&#28982;&#21518;&#30001;SNN&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#38024;&#23545;&#30828;&#20214;&#37096;&#32626;&#36827;&#34892;&#20248;&#21270;&#30340;&#20934;&#30830;&#19988;&#32039;&#20945;&#30340;SNN&#30340;&#21508;&#31181;&#31995;&#32479;&#35774;&#35745;&#36873;&#25321;&#65292;&#20174;&#25968;&#25454;&#32534;&#30721;&#21040;&#35757;&#32451;&#31639;&#27861;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#36827;&#21270;&#31639;&#27861;&#21644;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#35757;&#32451;&#30340;SNN&#22312;&#20449;&#21495;&#26377;&#25928;&#29575;&#26041;&#38754;&#22823;&#32422;&#36798;&#21040;91%&#65292;&#21442;&#25968;&#25968;&#37327;&#20960;&#20046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work describes the investigation of neuromorphic computing-based spiking neural network (SNN) models used to filter data from sensor electronics in high energy physics experiments conducted at the High Luminosity Large Hadron Collider. We present our approach for developing a compact neuromorphic model that filters out the sensor data based on the particle's transverse momentum with the goal of reducing the amount of data being sent to the downstream electronics. The incoming charge waveforms are converted to streams of binary-valued events, which are then processed by the SNN. We present our insights on the various system design choices - from data encoding to optimal hyperparameters of the training algorithm - for an accurate and compact SNN optimized for hardware deployment. Our results show that an SNN trained with an evolutionary algorithm and an optimized set of hyperparameters obtains a signal efficiency of about 91% with nearly half as many parameters as a deep neural netw
&lt;/p&gt;</description></item><item><title>Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11224</link><description>&lt;p&gt;
Jina Embeddings:&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11224
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#30001;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#32452;&#25104;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#25991;&#26412;&#36755;&#20837;&#36716;&#21270;&#20026;&#25968;&#20540;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#22312;&#23494;&#38598;&#26816;&#32034;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20174;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#21644;&#19977;&#20803;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#25454;&#28165;&#29702;&#22312;&#25968;&#25454;&#38598;&#20934;&#22791;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#26368;&#21518;&#21033;&#29992;Massive Textual Embedding Benchmark&#65288;MTEB&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#25277;&#35937;&#23545;&#35937;&#30340;&#20855;&#35937;&#21270;&#21644;&#25215;&#35748;&#27010;&#24565;&#21644;&#31867;&#22411;&#20043;&#38388;&#30340;&#26412;&#20307;&#21306;&#21035;&#65292;&#24314;&#31435;&#36215;&#19968;&#20010;&#22522;&#20110;&#26412;&#20307;&#21644;&#35821;&#35328;&#26080;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#20013;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2307.11206</link><description>&lt;p&gt;
&#36808;&#21521;&#26412;&#20307;&#22522;&#30784;&#21644;&#35821;&#35328;&#26080;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Towards Ontologically Grounded and Language-Agnostic Knowledge Graphs. (arXiv:2307.11206v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#25277;&#35937;&#23545;&#35937;&#30340;&#20855;&#35937;&#21270;&#21644;&#25215;&#35748;&#27010;&#24565;&#21644;&#31867;&#22411;&#20043;&#38388;&#30340;&#26412;&#20307;&#21306;&#21035;&#65292;&#24314;&#31435;&#36215;&#19968;&#20010;&#22522;&#20110;&#26412;&#20307;&#21644;&#35821;&#35328;&#26080;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#20013;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#24050;&#25104;&#20026;&#24212;&#29992;&#20013;&#34920;&#31034;&#20107;&#23454;&#20449;&#24687;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#22914;&#25512;&#33616;&#24341;&#25806;&#12289;&#25628;&#32034;&#21644;&#38382;&#31572;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#25345;&#32493;&#26356;&#26032;KGs&#20197;&#21450;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#21644;&#19981;&#21516;&#35821;&#35328;&#30340;KGs&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#27492;&#24314;&#35758;&#36890;&#36807;&#25277;&#35937;&#23545;&#35937;&#30340;&#20855;&#35937;&#21270;&#21644;&#25215;&#35748;&#27010;&#24565;&#21644;&#31867;&#22411;&#20043;&#38388;&#30340;&#26412;&#20307;&#21306;&#21035;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#22522;&#20110;&#26412;&#20307;&#21644;&#35821;&#35328;&#26080;&#20851;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#21487;&#20197;&#32531;&#35299;KGs&#25972;&#21512;&#20013;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have become the standard technology for the representation of factual information in applications such as recommendation engines, search, and question-answering systems. However, the continual updating of KGs, as well as the integration of KGs from different domains and KGs in different languages, remains to be a major challenge. What we suggest here is that by a reification of abstract objects and by acknowledging the ontological distinction between concepts and types, we arrive at an ontologically grounded and language-agnostic representation that can alleviate the difficulties in KG integration.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;MuJoCo&#29615;&#22659;&#20013;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#65292;&#21457;&#29616;DDPG&#22312;&#23569;&#37327;episode&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11166</link><description>&lt;p&gt;
&#22312;MuJoCo&#29615;&#22659;&#20013;&#25506;&#32034;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment. (arXiv:2307.11166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;MuJoCo&#29615;&#22659;&#20013;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#65292;&#21457;&#29616;DDPG&#22312;&#23569;&#37327;episode&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#24555;&#36895;&#30340;&#29289;&#29702;&#27169;&#25311;&#22120;MuJoCo&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#36816;&#34892;&#20219;&#21153;&#65292;&#24182;&#25581;&#31034;&#27599;&#20010;&#20219;&#21153;&#30340;&#35266;&#27979;&#31354;&#38388;&#12289;&#21160;&#20316;&#31354;&#38388;&#12289;&#22870;&#21169;&#31561;&#35814;&#32454;&#20449;&#24687;&#12290;&#36890;&#36807;&#27604;&#36739;&#31163;&#25955;&#21270;&#26041;&#27861;&#20013;&#30340;Q&#23398;&#20064;&#21644;SARSA&#65292;&#23558;&#20540;&#22522;&#26041;&#27861;&#24212;&#29992;&#20110;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#20316;&#20026;&#22522;&#20934;&#65292;&#36880;&#27493;&#21521;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;DDPG&#36807;&#28193;&#12290;&#22312;&#22823;&#37327;&#30340;episode&#20013;&#65292;Q&#23398;&#20064;&#30340;&#24471;&#20998;&#36229;&#36807;&#20102;SARSA&#65292;&#20294;DDPG&#22312;&#23569;&#37327;&#30340;episode&#20013;&#34920;&#29616;&#20248;&#20110;&#20004;&#32773;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;&#26399;&#26395;&#22312;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39044;&#26399;&#26032;&#35774;&#35745;&#30340;DDPG&#23558;&#22823;&#24133;&#25552;&#39640;&#24615;&#33021;&#65292;&#28982;&#32780;&#22312;&#21482;&#26377;&#23569;&#25968;episode&#20043;&#21518;&#65292;&#25105;&#20204;&#24050;&#32463;&#33021;&#22815;&#36798;&#21040;&#19981;&#38169;&#30340;&#24179;&#22343;&#22870;&#21169;&#12290;&#25105;&#20204;&#26399;&#26395;&#22312;&#36275;&#22815;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We leverage the fast physics simulator, MuJoCo to run tasks in a continuous control environment and reveal details like the observation space, action space, rewards, etc. for each task. We benchmark value-based methods for continuous control by comparing Q-learning and SARSA through a discretization approach, and using them as baselines, progressively moving into one of the state-of-the-art deep policy gradient method DDPG. Over a large number of episodes, Qlearning outscored SARSA, but DDPG outperformed both in a small number of episodes. Lastly, we also fine-tuned the model hyper-parameters expecting to squeeze more performance but using lesser time and resources. We anticipated that the new design for DDPG would vastly improve performance, yet after only a few episodes, we were able to achieve decent average rewards. We expect to improve the performance provided adequate time and computational resources.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#34892;&#20026;&#32463;&#27982;&#23398;&#35282;&#24230;&#65292;&#23545;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;AI&#23545;&#40784;&#20013;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;AI&#23433;&#20840;&#38382;&#39064;&#19981;&#20165;&#28041;&#21450;&#35774;&#35745;&#32773;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#36824;&#28041;&#21450;&#21040;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#19982;&#25928;&#29992;&#20989;&#25968;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;</title><link>http://arxiv.org/abs/2307.11137</link><description>&lt;p&gt;
&#27169;&#22411;&#19982;&#38177;&#20154;&#20043;&#38388;&#8212;&#8212;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;AI&#23545;&#40784;&#20013;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#30340;&#34892;&#20026;&#32463;&#27982;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Of Models and Tin Men -- a behavioural economics study of principal-agent problems in AI alignment using large-language models. (arXiv:2307.11137v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#34892;&#20026;&#32463;&#27982;&#23398;&#35282;&#24230;&#65292;&#23545;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;AI&#23545;&#40784;&#20013;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;AI&#23433;&#20840;&#38382;&#39064;&#19981;&#20165;&#28041;&#21450;&#35774;&#35745;&#32773;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#36824;&#28041;&#21450;&#21040;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#19982;&#25928;&#29992;&#20989;&#25968;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#19968;&#20010;&#35774;&#35745;&#32773;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35774;&#35745;&#32773;&#35797;&#22270;&#30830;&#20445;&#20195;&#29702;&#30340;&#34892;&#20026;&#19982;&#20854;&#30446;&#30340;&#19968;&#33268;&#65292;&#24182;&#19988;&#39118;&#38505;&#20165;&#20165;&#26159;&#30001;&#20110;&#35774;&#35745;&#32773;&#24847;&#22270;&#20013;&#30340;&#25928;&#29992;&#20989;&#25968;&#19982;&#20195;&#29702;&#30340;&#20869;&#37096;&#25928;&#29992;&#20989;&#25968;&#20043;&#38388;&#30340;&#24847;&#22806;&#38169;&#20301;&#32780;&#23548;&#33268;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#20363;&#21270;&#30340;&#20195;&#29702;&#30340;&#20986;&#29616;&#65292;&#36825;&#31181;&#25551;&#36848;&#19981;&#33021;&#25429;&#25417;&#21040;AI&#23433;&#20840;&#30340;&#26680;&#24515;&#26041;&#38754;&#65292;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#20013;&#35774;&#35745;&#32773;&#19982;&#20195;&#29702;&#20043;&#38388;&#24182;&#27809;&#26377;&#19968;&#23545;&#19968;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#32780;&#19988;&#35768;&#22810;&#20195;&#29702;&#65292;&#26080;&#35770;&#26159;&#20154;&#24037;&#26234;&#33021;&#36824;&#26159;&#20154;&#31867;&#65292;&#37117;&#20855;&#26377;&#22810;&#26679;&#30340;&#20215;&#20540;&#35266;&#12290;&#22240;&#27492;&#65292;AI&#23433;&#20840;&#20855;&#26377;&#32463;&#27982;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI Alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. With the advent of agents instantiated with large-language models (LLMs), which are typically pre-trained, we argue this does not capture the essential aspects of AI safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. Therefore, there is an economic aspect to AI safety and the principal-agent problem is likely to arise. In a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11133</link><description>&lt;p&gt;
&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#36827;&#34892;&#23545;&#27604;&#22270;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Graph Pooling for Explainable Classification of Brain Networks. (arXiv:2307.11133v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27979;&#37327;&#31070;&#32463;&#27963;&#21160;&#30340;&#25216;&#26415;&#12290;&#20854;&#24212;&#29992;&#22312;&#35782;&#21035;&#24085;&#37329;&#26862;&#30149;&#12289;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#33258;&#38381;&#30151;&#31561;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;fMRI&#25968;&#25454;&#20998;&#26512;&#23558;&#22823;&#33041;&#24314;&#27169;&#20026;&#22270;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;fMRI&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#35201;&#27714;&#23545;GNN&#36827;&#34892;&#29305;&#27530;&#35774;&#35745;&#12290;&#23450;&#21046;GNN&#20197;&#29983;&#25104;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21452;&#27880;&#24847;&#22359;&#21644;&#21487;&#24494;&#20998;&#22270;&#27744;&#21270;&#26041;&#27861;ContrastPool&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;GNN&#20998;&#26512;&#33041;&#32593;&#32476;&#65292;&#28385;&#36275;fMRI&#30340;&#29305;&#27530;&#35201;&#27714;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#30340;3&#31181;&#30142;&#30149;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#30340;&#27169;&#24335;&#19982;&#31070;&#32463;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinson's, Alzheimer's, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literatu
&lt;/p&gt;</description></item><item><title>&#36817;&#20284;&#35745;&#31639;&#26159;&#19968;&#31181;&#33021;&#22815;&#35843;&#25972;&#31995;&#32479;&#35774;&#35745;&#32467;&#26524;&#36136;&#37327;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#21644;/&#25110;&#24615;&#33021;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#65292;&#24050;&#21560;&#24341;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#20851;&#20110;&#24212;&#29992;&#29305;&#23450;&#21644;&#26550;&#26500;&#36817;&#20284;&#25216;&#26415;&#30340;&#35843;&#26597;&#30340;&#31532;&#20108;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.11128</link><description>&lt;p&gt;
&#36817;&#20284;&#35745;&#31639;&#35843;&#26597;&#65292;&#31532;&#20108;&#37096;&#20998;&#65306;&#24212;&#29992;&#29305;&#23450;&#21644;&#26550;&#26500;&#36817;&#20284;&#25216;&#26415;&#21450;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Approximate Computing Survey, Part II: Application-Specific &amp; Architectural Approximation Techniques and Applications. (arXiv:2307.11128v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11128
&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#35745;&#31639;&#26159;&#19968;&#31181;&#33021;&#22815;&#35843;&#25972;&#31995;&#32479;&#35774;&#35745;&#32467;&#26524;&#36136;&#37327;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#21644;/&#25110;&#24615;&#33021;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#65292;&#24050;&#21560;&#24341;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#20851;&#20110;&#24212;&#29992;&#29305;&#23450;&#21644;&#26550;&#26500;&#36817;&#20284;&#25216;&#26415;&#30340;&#35843;&#26597;&#30340;&#31532;&#20108;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#23494;&#38598;&#22411;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#37096;&#32626;&#65292;&#22914;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DSP&#65289;&#65292;&#36843;&#20351;&#35745;&#31639;&#31995;&#32479;&#30028;&#25506;&#32034;&#26032;&#30340;&#35774;&#35745;&#26041;&#27861;&#12290;&#36817;&#20284;&#35745;&#31639;&#25104;&#20026;&#19968;&#31181;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#65292;&#20801;&#35768;&#22312;&#31995;&#32479;&#35774;&#35745;&#20013;&#35843;&#25972;&#32467;&#26524;&#30340;&#36136;&#37327;&#65292;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#21644;/&#25110;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#36825;&#31181;&#26681;&#26412;&#24615;&#30340;&#33539;&#24335;&#36716;&#21464;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#20852;&#36259;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#35745;&#23618;&#38754;&#65288;&#20174;&#31995;&#32479;&#21040;&#38598;&#25104;&#30005;&#36335;&#65289;&#19978;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#36817;&#20284;&#25216;&#26415;&#21644;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#21463;&#36817;&#20284;&#35745;&#31639;&#22312;&#36807;&#21435;10&#24180;&#30340;&#24191;&#27867;&#21560;&#24341;&#21147;&#30340;&#39537;&#20351;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#30340;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#20851;&#38190;&#26041;&#38754;&#65288;&#22914;&#26415;&#35821;&#21644;&#24212;&#29992;&#65289;&#24182;&#22238;&#39038;&#20102;&#20256;&#32479;&#35745;&#31639;&#22534;&#26632;&#30340;&#21508;&#20010;&#23618;&#38754;&#30340;&#26368;&#26032;&#36817;&#20284;&#25216;&#26415;&#12290;&#22312;&#25105;&#20204;&#30340;&#35843;&#26597;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#23545;&#24212;&#29992;&#29305;&#23450;&#21644;&#26550;&#26500;&#36817;&#20284;&#25216;&#26415;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20998;&#31867;&#21644;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenging deployment of compute-intensive applications from domains such Artificial Intelligence (AI) and Digital Signal Processing (DSP), forces the community of computing systems to explore new design approaches. Approximate Computing appears as an emerging solution, allowing to tune the quality of results in the design of a system in order to improve the energy efficiency and/or performance. This radical paradigm shift has attracted interest from both academia and industry, resulting in significant research on approximation techniques and methodologies at different design layers (from system down to integrated circuits). Motivated by the wide appeal of Approximate Computing over the last 10 years, we conduct a two-part survey to cover key aspects (e.g., terminology and applications) and review the state-of-the art approximation techniques from all layers of the traditional computing stack. In Part II of our survey, we classify and present the technical details of application-s
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#30340;&#26412;&#36136;&#26159;&#19968;&#31995;&#21015;&#36890;&#36807;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#24314;&#31435;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#26469;&#26368;&#23567;&#21270;&#31995;&#32479;&#29109;&#30340;&#25968;&#23398;&#20989;&#25968;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.11114</link><description>&lt;p&gt;
&#26234;&#33021;&#30340;&#26412;&#36136;
&lt;/p&gt;
&lt;p&gt;
Nature of Intelligence. (arXiv:2307.11114v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11114
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#26412;&#36136;&#26159;&#19968;&#31995;&#21015;&#36890;&#36807;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#24314;&#31435;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#26469;&#26368;&#23567;&#21270;&#31995;&#32479;&#29109;&#30340;&#25968;&#23398;&#20989;&#25968;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#22823;&#33041;&#65292;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#20855;&#26377;&#23398;&#20064;&#33021;&#21147;&#24182;&#25191;&#34892;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20219;&#21153;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30001;&#22810;&#20010;&#35745;&#31639;&#23618;&#32452;&#25104;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#34920;&#31034;&#24182;&#22312;&#35768;&#22810;&#35782;&#21035;&#39046;&#22495;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#30340;&#26412;&#36136;&#65292;&#21363;&#36890;&#36807;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20849;&#21516;&#20195;&#34920;&#30340;&#26234;&#33021;&#30340;&#26412;&#36136;&#65292;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#26234;&#33021;&#30340;&#26412;&#36136;&#26159;&#19968;&#31995;&#21015;&#36890;&#36807;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#24314;&#31435;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#26469;&#26368;&#23567;&#21270;&#31995;&#32479;&#29109;&#30340;&#25968;&#23398;&#20989;&#25968;&#36807;&#31243;&#12290;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20197;&#19968;&#31181;&#21463;&#24378;&#21270;&#26041;&#24335;&#28040;&#32791;&#33021;&#37327;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#20123;&#20943;&#29109;&#36807;&#31243;&#12290;&#26681;&#25454;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20851;&#20110;&#35821;&#35328;&#12289;&#26080;&#24847;&#35782;&#21644;&#24847;&#35782;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#24037;&#31243;&#23454;&#29616;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain is the substrate for human intelligence. By simulating the human brain, artificial intelligence builds computational models that have learning capabilities and perform intelligent tasks approaching the human level. Deep neural networks consist of multiple computation layers to learn representations of data and improve the state-of-the-art in many recognition domains. However, the essence of intelligence commonly represented by both humans and AI is unknown. Here, we show that the nature of intelligence is a series of mathematically functional processes that minimize system entropy by establishing functional relationships between datasets over space and time. Humans and AI have achieved intelligence by implementing these entropy-reducing processes in a reinforced manner that consumes energy. With this hypothesis, we establish mathematical models of language, unconsciousness and consciousness, predicting the evidence to be found by neuroscience and achieved by AI engineer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;AAA&#28216;&#25103;&#27979;&#35797;&#20013;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#24110;&#21161;&#28216;&#25103;&#34892;&#19994;&#37319;&#29992;&#36825;&#39033;&#25216;&#26415;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.11105</link><description>&lt;p&gt;
&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;AAA&#28216;&#25103;&#27979;&#35797;&#30340;&#25216;&#26415;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games. (arXiv:2307.11105v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;AAA&#28216;&#25103;&#27979;&#35797;&#20013;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#24110;&#21161;&#28216;&#25103;&#34892;&#19994;&#37319;&#29992;&#36825;&#39033;&#25216;&#26415;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30740;&#31350;&#21040;&#23454;&#38469;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#22797;&#26434;&#36719;&#20214;&#31995;&#32479;&#26469;&#35828;&#65292;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#22823;&#35268;&#27169;&#28216;&#25103;&#21046;&#20316;&#20013;&#65292;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#26159;&#24320;&#21457;&#29615;&#22659;&#21487;&#33021;&#19982;&#26368;&#32456;&#20135;&#21697;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#12290;&#26412;&#25216;&#26415;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22312;&#29616;&#26377;&#22522;&#20110;&#33050;&#26412;&#26426;&#22120;&#20154;&#30340;&#33258;&#21160;&#21270;&#28216;&#25103;&#27979;&#35797;&#35299;&#20915;&#26041;&#26696;&#20013;&#28155;&#21152;&#23454;&#39564;&#24615;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#21162;&#21147;&#65292;&#20197;&#22686;&#21152;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22914;&#20309;&#38598;&#25104;&#36825;&#20010;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#22686;&#21152;&#31867;&#20284;[1]&#30340;AAA&#28216;&#25103;&#65288;&#21253;&#25324;Battlefield 2042&#21644;Dead Space&#65288;2023&#65289;&#65289;&#30340;&#27979;&#35797;&#35206;&#30422;&#29575;&#12290;&#26412;&#25216;&#26415;&#35770;&#25991;&#26088;&#22312;&#23637;&#31034;&#22312;&#28216;&#25103;&#21046;&#20316;&#20013;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#26696;&#20363;&#65292;&#24182;&#20171;&#32461;&#20102;&#24076;&#26395;&#22312;&#28216;&#25103;&#20013;&#36827;&#34892;&#30456;&#21516;&#25506;&#32034;&#30340;&#20154;&#21487;&#33021;&#20250;&#36935;&#21040;&#30340;&#26368;&#22823;&#26102;&#38388;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24110;&#21161;&#28216;&#25103;&#34892;&#19994;&#26356;&#24555;&#22320;&#37319;&#29992;&#36825;&#39033;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#25105;&#20204;&#35748;&#20026;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Going from research to production, especially for large and complex software systems, is fundamentally a hard problem. In large-scale game production, one of the main reasons is that the development environment can be very different from the final product. In this technical paper we describe an effort to add an experimental reinforcement learning system to an existing automated game testing solution based on scripted bots in order to increase its capacity. We report on how this reinforcement learning system was integrated with the aim to increase test coverage similar to [1] in a set of AAA games including Battlefield 2042 and Dead Space (2023). The aim of this technical paper is to show a use-case of leveraging reinforcement learning in game production and cover some of the largest time sinks anyone who wants to make the same journey for their game may encounter. Furthermore, to help the game industry to adopt this technology faster, we propose a few research directions that we believ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#30340;&#21453;&#28436;&#31934;&#24230;&#65292;&#32780;&#19988;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11099</link><description>&lt;p&gt;
&#29992;&#23398;&#20064;&#30340;&#20195;&#29702;&#21644;&#32422;&#26463;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving multiphysics-based inverse problems with learned surrogates and constraints. (arXiv:2307.11099v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#30340;&#21453;&#28436;&#31934;&#24230;&#65292;&#32780;&#19988;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#36136;&#30899;&#23553;&#23384;&#30417;&#27979;&#20013;&#65292;&#24403;&#22810;&#27169;&#24577;&#26102;&#21464;&#25968;&#25454;&#26114;&#36149;&#19988;&#25968;&#20540;&#27169;&#25311;&#25104;&#26412;&#39640;&#26114;&#26102;&#65292;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;&#23398;&#20064;&#20195;&#29702;&#19982;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#31181;&#32452;&#21512;&#19981;&#20165;&#33021;&#22815;&#22823;&#22823;&#25913;&#21892;&#23545;&#37325;&#35201;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#65288;&#28183;&#36879;&#29575;&#65289;&#30340;&#21453;&#28436;&#65292;&#36824;&#33021;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#21253;&#25324;&#20117;&#27979;&#37327;&#21644;&#20027;&#21160;&#28304;&#26102;&#21464;&#22320;&#38663;&#25968;&#25454;&#65289;&#25552;&#20379;&#19968;&#20010;&#33258;&#28982;&#30340;&#24179;&#21488;&#12290;&#36890;&#36807;&#28155;&#21152;&#23398;&#20064;&#32422;&#26463;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#35745;&#31639;&#21487;&#34892;&#30340;&#21453;&#28436;&#26041;&#27861;&#65292;&#20854;&#31934;&#24230;&#20173;&#28982;&#20934;&#30830;&#12290;&#36825;&#36890;&#36807;&#21253;&#21547;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;&#24402;&#19968;&#21270;&#27969;&#65289;&#65292;&#20351;&#27169;&#22411;&#36845;&#20195;&#20445;&#25345;&#22312;&#20998;&#24067;&#20869;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#20316;&#20026;&#20195;&#29702;&#30340;&#32463;&#36807;&#35757;&#32451;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#31639;&#23376;&#29992;&#20110;&#20195;&#26367;&#28041;&#21450;&#37096;&#20998;&#35745;&#31639;&#26114;&#36149;&#30340;&#22810;&#30456;&#27969;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving multiphysics-based inverse problems for geological carbon storage monitoring can be challenging when multimodal time-lapse data are expensive to collect and costly to simulate numerically. We overcome these challenges by combining computationally cheap learned surrogates with learned constraints. Not only does this combination lead to vastly improved inversions for the important fluid-flow property, permeability, it also provides a natural platform for inverting multimodal data including well measurements and active-source time-lapse seismic data. By adding a learned constraint, we arrive at a computationally feasible inversion approach that remains accurate. This is accomplished by including a trained deep neural network, known as a normalizing flow, which forces the model iterates to remain in-distribution, thereby safeguarding the accuracy of trained Fourier neural operators that act as surrogates for the computationally expensive multiphase flow simulations involving partia
&lt;/p&gt;</description></item><item><title>&#23494;&#38598;&#26679;&#26412;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#26088;&#22312;&#25581;&#31034;&#23398;&#20064;&#26426;&#21046;&#21644;&#34920;&#31034;&#30340;&#26410;&#30693;&#29305;&#24615;&#65292;&#24182;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38544;&#34255;&#21333;&#20803;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10991</link><description>&lt;p&gt;
&#23494;&#38598;&#26679;&#26412;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dense Sample Deep Learning. (arXiv:2307.10991v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10991
&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26679;&#26412;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#26088;&#22312;&#25581;&#31034;&#23398;&#20064;&#26426;&#21046;&#21644;&#34920;&#31034;&#30340;&#26410;&#30693;&#29305;&#24615;&#65292;&#24182;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38544;&#34255;&#21333;&#20803;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26159;20&#19990;&#32426;80&#24180;&#20195;&#25552;&#20986;&#30340;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#30340;&#21464;&#20307;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#35821;&#35328;&#32763;&#35793;&#12289;&#34507;&#30333;&#36136;&#25240;&#21472;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#20197;&#21450;&#26368;&#36817;&#30340;&#31867;&#20154;&#35821;&#35328;&#27169;&#22411;&#65288;CHATbots&#65289;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#23545;&#20110;&#20351;&#36825;&#20123;&#32593;&#32476;&#22312;&#22914;&#27492;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#26377;&#25928;&#30340;&#23398;&#20064;&#26426;&#21046;&#21644;&#34920;&#31034;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#37096;&#20998;&#21407;&#22240;&#21487;&#33021;&#26159;&#20854;&#22823;&#35268;&#27169;&#26550;&#26500;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#20351;&#29992;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#30340;&#26412;&#36136;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#30693;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20855;&#26377;&#25968;&#30334;&#19975;&#25110;&#25968;&#21313;&#20159;&#20010;&#26631;&#35760;&#30340;&#35757;&#32451;&#38598;&#23384;&#22312;&#26410;&#30693;&#30340;&#32452;&#21512;&#26041;&#24335;&#65292;&#21516;&#26102;&#25968;&#30334;&#19975;&#25110;&#25968;&#21313;&#20159;&#20010;&#38544;&#34255;&#21333;&#20803;&#30340;&#32593;&#32476;&#38590;&#20197;&#21487;&#35270;&#21270;&#65292;&#20854;&#26426;&#21046;&#20063;&#38590;&#20197;&#25581;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#26679;&#26412;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) , a variant of the neural network algorithms originally proposed in the 1980s, has made surprising progress in Artificial Intelligence (AI), ranging from language translation, protein folding, autonomous cars, and more recently human-like language models (CHATbots), all that seemed intractable until very recently. Despite the growing use of Deep Learning (DL) networks, little is actually understood about the learning mechanisms and representations that makes these networks effective across such a diverse range of applications. Part of the answer must be the huge scale of the architecture and of course the large scale of the data, since not much has changed since 1987. But the nature of deep learned representations remain largely unknown. Unfortunately training sets with millions or billions of tokens have unknown combinatorics and Networks with millions or billions of hidden units cannot easily be visualized and their mechanisms cannot be easily revealed. In this pap
&lt;/p&gt;</description></item><item><title>AdjointDPM&#26159;&#19968;&#31181;&#26032;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#35299;&#20915;&#20102;DPM&#23450;&#21046;&#21270;&#20013;&#20869;&#23384;&#28040;&#32791;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.10711</link><description>&lt;p&gt;
AdjointDPM: &#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10711
&lt;/p&gt;
&lt;p&gt;
AdjointDPM&#26159;&#19968;&#31181;&#26032;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#35299;&#20915;&#20102;DPM&#23450;&#21046;&#21270;&#20013;&#20869;&#23384;&#28040;&#32791;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23450;&#21046;&#21270;&#26041;&#27861;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#26679;&#20363;&#26469;&#23558;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#27010;&#24565;&#23545;&#40784;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#24403;&#21807;&#19968;&#21487;&#29992;&#30340;&#30417;&#30563;&#26159;&#23450;&#20041;&#22312;&#29983;&#25104;&#20869;&#23481;&#19978;&#30340;&#21487;&#24494;&#24230;&#37327;&#26102;&#30340;DPM&#23450;&#21046;&#21270;&#25361;&#25112;&#12290;&#30001;&#20110;DPM&#30340;&#37319;&#26679;&#36807;&#31243;&#28041;&#21450;&#23545;&#21435;&#22122;UNet&#30340;&#36882;&#24402;&#35843;&#29992;&#65292;&#26420;&#32032;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#23384;&#20648;&#25152;&#26377;&#36845;&#20195;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#23548;&#33268;&#20869;&#23384;&#28040;&#32791;&#26497;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;AdjointDPM&#65292;&#39318;&#20808;&#36890;&#36807;&#27714;&#35299;&#30456;&#24212;&#30340;&#27010;&#29575;&#27969;ODE&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#28982;&#21518;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#36890;&#36807;&#27714;&#35299;&#21478;&#19968;&#20010;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;(&#21253;&#25324;&#35843;&#21046;&#20449;&#21495;&#12289;&#32593;&#32476;&#26435;&#37325;&#21644;&#21021;&#22987;&#22122;&#22768;)&#12290;&#20026;&#20102;&#20943;&#23569;&#27491;&#21521;&#29983;&#25104;&#21644;&#21453;&#21521;&#20256;&#25773;&#20013;&#30340;&#25968;&#20540;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10617</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26816;&#27979;&#34394;&#20551;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#32447;&#35780;&#35770;&#22312;&#25512;&#24191;&#20219;&#20309;&#20135;&#21697;&#25110;&#26381;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20225;&#19994;&#21487;&#33021;&#20250;&#23884;&#20837;&#34394;&#20551;&#35780;&#35770;&#20197;&#21560;&#24341;&#23458;&#25143;&#36141;&#20080;&#20182;&#20204;&#30340;&#20135;&#21697;&#12290;&#20182;&#20204;&#29978;&#33267;&#21487;&#33021;&#31361;&#20986;&#24378;&#35843;&#33258;&#24049;&#20135;&#21697;&#30340;&#20248;&#28857;&#25110;&#25209;&#35780;&#31454;&#20105;&#23545;&#25163;&#30340;&#20135;&#21697;&#12290;&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#12289;&#24191;&#21578;&#21830;&#21644;&#20854;&#20182;&#22312;&#32447;&#21830;&#19994;&#29992;&#25143;&#26377;&#21160;&#26426;&#20026;&#20182;&#20204;&#24819;&#35201;&#25512;&#24191;&#30340;&#20135;&#21697;&#32534;&#20889;&#34394;&#20551;&#30340;&#27491;&#38754;&#35780;&#35770;&#65292;&#25110;&#32773;&#20026;&#20182;&#20204;&#30495;&#27491;&#19981;&#21916;&#27426;&#30340;&#20135;&#21697;&#25552;&#20379;&#34394;&#20551;&#30340;&#36127;&#38754;&#35780;&#35770;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#25345;&#32493;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#19968;&#20010;&#39184;&#39302;&#35780;&#35770;&#30340;&#34394;&#20551;&#24847;&#35265;&#22403;&#22334;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22810;&#27425;&#23454;&#39564;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;n-gram&#27169;&#22411;&#21644;&#26368;&#22823;&#29305;&#24449;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, online reviews play a vital role for promoting any kind of product or services. Businesses may embed fake reviews in order to attract customers to purchase their products. They may even highlight the benefits of their own product or criticize the competition's product. Marketers, advertisers, and other online business users have incentive to create fake positive reviews for products which they want to promote or give fake negative reviews for products which they really don't like. So now-a-days writing a deceptive review is inevitable thing for promoting their own business or degrading competitor's reputation. Thus, identifying deceptive reviews is an intense and on-going research area. This research paper proposes machine learning model approach to identify deceptive reviews. The paper investigates the performance of the several experiments done on a Deceptive Opinion Spam Corpus dataset of restaurants reviews. We developed a n-gram model and max features to identify 
&lt;/p&gt;</description></item><item><title>Ethosight&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#32852;&#24230;&#35745;&#31639;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#32454;&#24494;&#34892;&#20026;&#21644;&#22330;&#26223;&#32454;&#33410;&#30340;&#20934;&#30830;&#24863;&#30693;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#39044;&#20808;&#23384;&#22312;&#31526;&#21495;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.10577</link><description>&lt;p&gt;
Ethosight: &#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23884;&#20837;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#32852;&#24230;&#24230;&#37327;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#36827;&#34892;&#32454;&#33268;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning. (arXiv:2307.10577v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10577
&lt;/p&gt;
&lt;p&gt;
Ethosight&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#32852;&#24230;&#35745;&#31639;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#32454;&#24494;&#34892;&#20026;&#21644;&#22330;&#26223;&#32454;&#33410;&#30340;&#20934;&#30830;&#24863;&#30693;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#39044;&#20808;&#23384;&#22312;&#31526;&#21495;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21162;&#21147;&#26469;&#36827;&#34892;&#25968;&#25454;&#33719;&#21462;&#21644;&#39564;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;&#26816;&#27979;&#32454;&#24494;&#30340;&#34892;&#20026;&#32454;&#33410;&#25110;&#20107;&#20214;&#26102;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21306;&#20998;&#24120;&#35268;&#34892;&#20026;&#21644;&#28508;&#22312;&#39118;&#38505;&#30340;&#22256;&#38590;&#65292;&#22914;&#21306;&#20998;&#24120;&#35268;&#36141;&#29289;&#21644;&#28508;&#22312;&#25170;&#31363;&#65292;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Ethosight&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#12290;Ethosight&#28040;&#38500;&#20102;&#23545;&#39044;&#20808;&#23384;&#22312;&#30340;&#31526;&#21495;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#20174;&#29992;&#25143;&#38656;&#27714;&#21644;&#24863;&#20852;&#36259;&#30340;&#35821;&#20041;&#30693;&#35782;&#20986;&#21457;&#36827;&#34892;&#33258;&#20027;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#26631;&#31614;&#20851;&#32852;&#24230;&#35745;&#31639;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#24490;&#29615;&#65292;Ethosight&#25512;&#26029;&#22330;&#26223;&#32454;&#33410;&#24182;&#36845;&#20195;&#22320;&#20248;&#21270;&#26631;&#31614;&#38598;&#12290;&#25512;&#29702;&#26426;&#21046;&#21487;&#20197;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;GPT4&#12289;&#31526;&#21495;&#25512;&#29702;&#22120;&#22914;OpenNARS&#25110;&#28151;&#21512;&#31995;&#32479;&#12290;Ethosight&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;ImageBind&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional computer vision models often require extensive manual effort for data acquisition and validation, particularly when detecting subtle behavioral nuances or events. The difficulty in distinguishing routine behaviors from potential risks in real-world applications, like differentiating routine shopping from potential shoplifting, further complicates the process.  We present Ethosight, a novel zero-shot computer vision algorithm. Ethosight eradicates the need for pre-existing symbolic knowledge, initiating from a clean slate based on user requirements and semantic knowledge of interest. Using localized label affinity calculations and a reasoning-guided iterative learning loop, Ethosight infers scene details and iteratively refines the label set. Reasoning mechanisms can be derived from large language models like GPT4, symbolic reasoners like OpenNARS, or hybrid systems.  Ethosight further capitalizes on the capabilities of a pre-trained multi-modal model, ImageBind, generating 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.09782</link><description>&lt;p&gt;
ZeroQuant-FP: &#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#30340;&#19968;&#39033;&#39134;&#36291;
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09782
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22797;&#26434;&#39046;&#22495;&#20013;&#65292;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#35752;&#28014;&#28857;&#65288;FP&#65289;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;FP8&#21644;FP4&#65292;&#20197;&#24212;&#23545;&#22343;&#21248;&#37327;&#21270;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22788;&#29702;&#31163;&#32676;&#20540;&#65292;&#24182;&#21463;&#21040;NVIDIA H100&#30828;&#20214;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;LLMs&#20013;&#65292;FP8&#28608;&#27963;&#22987;&#32456;&#20248;&#20110;&#20854;&#25972;&#25968;&#65288;INT8&#65289;&#31561;&#25928;&#65292;&#24615;&#33021;&#20248;&#21183;&#22312;&#21253;&#21547;&#36229;&#36807;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#23545;&#20110;&#26435;&#37325;&#37327;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FP4&#30340;&#24615;&#33021;&#19982;INT4&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#20248;&#65292;&#31616;&#21270;&#20102;&#22312;&#20687;H100&#36825;&#26679;&#25903;&#25345;FP&#30340;&#30828;&#20214;&#19978;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20943;&#23569;&#30001;&#26435;&#37325;&#21644;&#28608;&#27963;&#20043;&#38388;&#24046;&#24322;&#24341;&#36215;&#30340;&#31934;&#24230;&#23545;&#40784;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#32553;&#25918;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints
&lt;/p&gt;</description></item><item><title>Ord2Seq&#26159;&#19968;&#31181;&#23558;&#24207;&#22238;&#24402;&#38382;&#39064;&#36716;&#21270;&#20026;&#26631;&#31614;&#24207;&#21015;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#30340;&#20108;&#20803;&#20998;&#31867;&#27493;&#39588;&#24494;&#22937;&#22320;&#21306;&#20998;&#30456;&#37051;&#31867;&#21035;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#36798;&#21040;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.09004</link><description>&lt;p&gt;
Ord2Seq: &#23558;&#24207;&#22238;&#24402;&#35270;&#20026;&#26631;&#31614;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ord2Seq: Regard Ordinal Regression as Label Sequence Prediction. (arXiv:2307.09004v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09004
&lt;/p&gt;
&lt;p&gt;
Ord2Seq&#26159;&#19968;&#31181;&#23558;&#24207;&#22238;&#24402;&#38382;&#39064;&#36716;&#21270;&#20026;&#26631;&#31614;&#24207;&#21015;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#30340;&#20108;&#20803;&#20998;&#31867;&#27493;&#39588;&#24494;&#22937;&#22320;&#21306;&#20998;&#30456;&#37051;&#31867;&#21035;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#36798;&#21040;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#22238;&#24402;&#26159;&#23558;&#23545;&#35937;&#23454;&#20363;&#20998;&#20026;&#24207;&#21015;&#31867;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#22914;&#21307;&#30103;&#30142;&#30149;&#20998;&#32423;&#12289;&#30005;&#24433;&#35780;&#20998;&#31561;&#26041;&#38754;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#23398;&#20064;&#31867;&#38388;&#24207;&#20851;&#31995;&#65292;&#20294;&#22312;&#21306;&#20998;&#30456;&#37051;&#31867;&#21035;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;Ord2Seq&#65292;&#23427;&#39318;&#27425;&#23558;&#27599;&#20010;&#24207;&#31867;&#21035;&#26631;&#31614;&#36716;&#21270;&#20026;&#29305;&#27530;&#30340;&#26631;&#31614;&#24207;&#21015;&#65292;&#20174;&#32780;&#23558;&#24207;&#22238;&#24402;&#20219;&#21153;&#35270;&#20026;&#24207;&#21015;&#39044;&#27979;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;&#24207;&#22238;&#24402;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36882;&#24402;&#30340;&#20108;&#20803;&#20998;&#31867;&#27493;&#39588;&#65292;&#20197;&#24494;&#22937;&#22320;&#21306;&#20998;&#30456;&#37051;&#31867;&#21035;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21306;&#20998;&#30456;&#37051;&#31867;&#21035;&#23545;&#24615;&#33021;&#25913;&#36827;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#20195;&#30721;&#23558;&#22312;&#23436;&#25104;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ordinal regression refers to classifying object instances into ordinal categories. It has been widely studied in many scenarios, such as medical disease grading, movie rating, etc. Known methods focused only on learning inter-class ordinal relationships, but still incur limitations in distinguishing adjacent categories thus far. In this paper, we propose a simple sequence prediction framework for ordinal regression called Ord2Seq, which, for the first time, transforms each ordinal category label into a special label sequence and thus regards an ordinal regression task as a sequence prediction process. In this way, we decompose an ordinal regression task into a series of recursive binary classification steps, so as to subtly distinguish adjacent categories. Comprehensive experiments show the effectiveness of distinguishing adjacent categories for performance improvement and our new approach exceeds state-of-the-art performances in four different scenarios. Codes will be available upon a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32534;&#35793;&#26102;&#38388;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.08167</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Computing the gradients with respect to all parameters of a quantum neural network using a single circuit. (arXiv:2307.08167v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08167
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32534;&#35793;&#26102;&#38388;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#21442;&#25968;&#24179;&#31227;&#35268;&#21017;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#26102;&#65292;&#38656;&#35201;&#23545;&#32593;&#32476;&#30340;&#21333;&#20010;&#21487;&#35843;&#21442;&#25968;&#35745;&#31639;&#20004;&#27425;&#20195;&#20215;&#20989;&#25968;&#12290;&#24403;&#21442;&#25968;&#24635;&#25968;&#36739;&#39640;&#26102;&#65292;&#38656;&#35201;&#35843;&#25972;&#21644;&#36816;&#34892;&#22810;&#27425;&#29992;&#20110;&#35745;&#31639;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#19968;&#20010;&#30005;&#36335;&#35745;&#31639;&#25152;&#26377;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32463;&#20856;&#23492;&#23384;&#22120;&#12290;&#25105;&#20204;&#36824;&#22312;&#30495;&#23454;&#37327;&#23376;&#30828;&#20214;&#21644;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#30005;&#36335;&#32534;&#35793;&#26102;&#38388;&#26126;&#26174;&#32553;&#30701;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
When computing the gradients of a quantum neural network using the parameter-shift rule, the cost function needs to be calculated twice for the gradient with respect to a single adjustable parameter of the network. When the total number of parameters is high, the quantum circuit for the computation has to be adjusted and run for many times. Here we propose an approach to compute all the gradients using a single circuit only, with a much reduced circuit depth and less classical registers. We also demonstrate experimentally, on both real quantum hardware and simulator, that our approach has the advantages that the circuit takes a significantly shorter time to compile than the conventional approach, resulting in a speedup on the total runtime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06092</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#37327;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#23485;&#24230;&#19982;&#22823;&#24120;&#25968; $n$ &#25104;&#27604;&#20363;&#12290;&#22312;&#38750;&#32447;&#24615;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#26377;&#38480;&#32500;&#20998;&#24067;&#36824;&#26159;&#25972;&#20010;&#36807;&#31243;&#65292;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;&#21450;&#20854;&#23548;&#25968;&#65289;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#37117;&#20250;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#20854;&#20013; $\gamma&gt;0$&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#29992;&#20110;&#24230;&#37327;&#24046;&#24322;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20379;&#30340;&#20219;&#20309;&#30028;&#38480;&#37117;&#35201;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#24335;&#21307;&#23398;&#35786;&#26029;&#30340;&#22823;&#36793;&#32536;&#31232;&#30095;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35282;&#24230;&#36793;&#32536;&#21644;&#33258;&#36866;&#24212;&#23610;&#24230;&#65292;&#20351;&#31639;&#27861;&#33021;&#22815;&#27491;&#30830;&#20998;&#31867;&#24050;&#30693;&#31867;&#21035;&#24182;&#35782;&#21035;&#26410;&#30693;&#31867;&#21035;&#65292;&#20197;&#36827;&#19968;&#27493;&#25903;&#25345;&#21307;&#23398;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2307.04541</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#24335;&#21307;&#23398;&#35786;&#26029;&#30340;&#22823;&#36793;&#32536;&#31232;&#30095;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis. (arXiv:2307.04541v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#24335;&#21307;&#23398;&#35786;&#26029;&#30340;&#22823;&#36793;&#32536;&#31232;&#30095;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35282;&#24230;&#36793;&#32536;&#21644;&#33258;&#36866;&#24212;&#23610;&#24230;&#65292;&#20351;&#31639;&#27861;&#33021;&#22815;&#27491;&#30830;&#20998;&#31867;&#24050;&#30693;&#31867;&#21035;&#24182;&#35782;&#21035;&#26410;&#30693;&#31867;&#21035;&#65292;&#20197;&#36827;&#19968;&#27493;&#25903;&#25345;&#21307;&#23398;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#21160;&#19979;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#21463;&#25511;&#30340;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#31639;&#27861;&#21487;&#33021;&#38754;&#20020;&#22810;&#31181;&#25361;&#25112;&#12290;&#24320;&#25918;&#24335;&#35782;&#21035;(OSR)&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#34920;&#31034;&#22312;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21487;&#33021;&#20986;&#29616;&#22312;&#27979;&#35797;&#20013;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#23436;&#20840;&#25910;&#38598;&#20197;&#21450;&#19981;&#26029;&#20986;&#29616;&#30340;&#26032;&#30340;&#25110;&#32597;&#35265;&#30340;&#30142;&#30149;&#12290;OSR&#38656;&#35201;&#19968;&#20010;&#31639;&#27861;&#19981;&#20165;&#33021;&#27491;&#30830;&#20998;&#31867;&#24050;&#30693;&#31867;&#21035;&#65292;&#36824;&#33021;&#35782;&#21035;&#26410;&#30693;&#31867;&#21035;&#65292;&#24182;&#23558;&#20854;&#36716;&#21457;&#32473;&#19987;&#23478;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35786;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;OSR&#38382;&#39064;&#65292;&#25105;&#20204;&#20551;&#35774;&#24050;&#30693;&#31867;&#21035;&#21487;&#33021;&#23494;&#38598;&#22320;&#21344;&#25454;&#20102;&#23884;&#20837;&#31354;&#38388;&#30340;&#23567;&#37096;&#20998;&#65292;&#32780;&#20854;&#20313;&#30340;&#31232;&#30095;&#21306;&#22495;&#21487;&#33021;&#34987;&#35782;&#21035;&#20026;&#26410;&#30693;&#31867;&#21035;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32479;&#19968;&#20004;&#31181;&#26426;&#21046;&#30340;&#24320;&#25918;&#36793;&#32536;&#20313;&#24358;&#25439;&#22833;(OMCL)&#12290;&#21069;&#32773;&#34987;&#31216;&#20026;&#20855;&#26377;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#36793;&#32536;&#25439;&#22833;(MLAS)&#65292;&#24341;&#20837;&#20102;&#35282;&#24230;&#36793;&#32536;&#20197;&#22686;&#24378;&#31867;&#20869;&#32039;&#23494;&#24230;&#21644;&#31867;&#38388;&#21487;&#20998;&#31163;&#24615;&#65292;&#24182;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fueled by deep learning, computer-aided diagnosis achieves huge advances. However, out of controlled lab environments, algorithms could face multiple challenges. Open set recognition (OSR), as an important one, states that categories unseen in training could appear in testing. In medical fields, it could derive from incompletely collected training datasets and the constantly emerging new or rare diseases. OSR requires an algorithm to not only correctly classify known classes, but also recognize unknown classes and forward them to experts for further diagnosis. To tackle OSR, we assume that known classes could densely occupy small parts of the embedding space and the remaining sparse regions could be recognized as unknowns. Following it, we propose Open Margin Cosine Loss (OMCL) unifying two mechanisms. The former, called Margin Loss with Adaptive Scale (MLAS), introduces angular margin for reinforcing intra-class compactness and inter-class separability, together with an adaptive scali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.03512</link><description>&lt;p&gt;
&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#32771;&#21476;&#30740;&#31350;&#20013;&#30340;&#36965;&#24863;&#25968;&#25454;&#26102;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#38556;&#30861;&#26159;&#36866;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#32463;&#24120;&#34987;&#29992;&#26469;&#20943;&#36731;&#36825;&#20010;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#24517;&#35201;&#25506;&#32034;&#22312;&#19981;&#21516;&#32771;&#21476;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20256;&#36755;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#20004;&#20010;&#35821;&#20041;&#20998;&#21106;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#20010;LiDAR&#25968;&#25454;&#38598;&#19978;&#30340;&#21508;&#31181;&#20256;&#36755;&#23398;&#20064;&#37197;&#32622;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20256;&#36755;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#32771;&#21476;&#23398;&#20013;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#23613;&#31649;&#23578;&#26410;&#35266;&#23519;&#21040;&#31995;&#32479;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27492;&#31867;&#25216;&#26415;&#26377;&#25928;&#24615;&#30340;&#20855;&#20307;&#35265;&#35299;&#65292;&#21487;&#20316;&#20026;&#26410;&#26469;&#24037;&#20316;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applying deep learning to remote sensing data in archaeological research, a notable obstacle is the limited availability of suitable datasets for training models. The application of transfer learning is frequently employed to mitigate this drawback. However, there is still a need to explore its effectiveness when applied across different archaeological datasets. This paper compares the performance of various transfer learning configurations using two semantic segmentation deep neural networks on two LiDAR datasets. The experimental results indicate that transfer learning-based approaches in archaeology can lead to performance improvements, although a systematic enhancement has not yet been observed. We provide specific insights about the validity of such techniques that can serve as a baseline for future works.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#30693;&#35782;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#23567;&#21306;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#36890;&#36947;&#35821;&#20041;&#33719;&#21462;&#21644;&#22810;&#29992;&#25143;&#27874;&#26463;&#36171;&#24418;&#65292;&#21487;&#25552;&#39640;&#23460;&#22806;&#26080;&#32447;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25903;&#25345;&#25193;&#23637;&#29616;&#23454;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.03070</link><description>&lt;p&gt;
&#28151;&#21512;&#30693;&#35782;&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#36947;&#35821;&#20041;&#33719;&#21462;&#21644;&#27874;&#26463;&#36171;&#24418;&#29992;&#20110;&#26080;&#23567;&#21306;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hybrid Knowledge-Data Driven Channel Semantic Acquisition and Beamforming for Cell-Free Massive MIMO. (arXiv:2307.03070v1 [eess.SP] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#30693;&#35782;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#23567;&#21306;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#36890;&#36947;&#35821;&#20041;&#33719;&#21462;&#21644;&#22810;&#29992;&#25143;&#27874;&#26463;&#36171;&#24418;&#65292;&#21487;&#25552;&#39640;&#23460;&#22806;&#26080;&#32447;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25903;&#25345;&#25193;&#23637;&#29616;&#23454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25512;&#21160;&#23460;&#22806;&#26080;&#32447;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#26222;&#36941;&#23384;&#22312;&#30340;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#24212;&#29992;&#65292;&#24182;&#32553;&#23567;&#19982;&#24403;&#21069;&#23460;&#20869;&#26080;&#32447;&#20256;&#36755;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#30693;&#35782;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#23567;&#21306;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31995;&#32479;&#20013;&#30340;&#36890;&#36947;&#35821;&#20041;&#33719;&#21462;&#21644;&#22810;&#29992;&#25143;&#27874;&#26463;&#36171;&#24418;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;-Mixer&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#36890;&#36947;&#35821;&#20041;&#33719;&#21462;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#32852;&#21512;&#20248;&#21270;&#20102;&#23548;&#39057;&#20449;&#21495;&#12289;&#20449;&#36947;&#35821;&#20041;&#23884;&#20837;&#30340;CSI&#37327;&#21270;&#22120;&#20197;&#21450;&#20449;&#36947;&#35821;&#20041;&#25552;&#21462;&#30340;CSI&#37325;&#26500;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#33719;&#21462;&#30340;&#36890;&#36947;&#35821;&#20041;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23637;&#24320;&#22810;&#29992;&#25143;&#27874;&#26463;&#36171;&#24418;&#22120;&#65292;&#22312;&#23460;&#22806;XR&#22330;&#26223;&#20013;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#30340;&#39057;&#35889;&#25928;&#29575;&#65292;&#24182;&#23545;&#19981;&#23436;&#32654;&#30340;CSI&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23637;&#24320;&#20256;&#32479;&#30340;&#36845;&#20195;&#36229;&#26494;&#24347;&#65288;SOR&#65289;&#32447;&#24615;&#27874;&#26463;&#36171;&#24418;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on advancing outdoor wireless systems to better support ubiquitous extended reality (XR) applications, and close the gap with current indoor wireless transmission capabilities. We propose a hybrid knowledge-data driven method for channel semantic acquisition and multi-user beamforming in cell-free massive multiple-input multiple-output (MIMO) systems. Specifically, we firstly propose a data-driven multiple layer perceptron (MLP)-Mixer-based auto-encoder for channel semantic acquisition, where the pilot signals, CSI quantizer for channel semantic embedding, and CSI reconstruction for channel semantic extraction are jointly optimized in an end-to-end manner. Moreover, based on the acquired channel semantic, we further propose a knowledge-driven deep-unfolding multi-user beamformer, which is capable of achieving good spectral efficiency with robustness to imperfect CSI in outdoor XR scenarios. By unfolding conventional successive over-relaxation (SOR)-based linear beamf
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15079</link><description>&lt;p&gt;
&#20174;$O(\sqrt{n})$&#21040;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming. (arXiv:2306.15079v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#25968;&#20540;&#20248;&#21270;&#29702;&#35770;&#19968;&#30452;&#23384;&#22312;&#19968;&#20010;&#22256;&#25200;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#20840;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#21644;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#20197;&#26377;&#30028;&#30418;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65288;Box-QP&#65289;&#20026;&#36215;&#28857;&#65292;&#35768;&#22810;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23545;&#20598;&#29702;&#35770;&#36716;&#21270;&#20026;Box-QP&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;QP&#31639;&#27861;&#65292;&#23588;&#20854;&#26159;&#20854;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#30452;&#25509;&#8221;&#26041;&#27861;&#65306;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#31934;&#30830;&#20540;&#20026;$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#22312;&#24403;&#20170;&#30340;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
A "dark cloud" hangs over numerical optimization theory for decades, namely, whether an optimization algorithm $O(\log(n))$ iteration complexity exists. "Yes", this paper answers, with a new optimization algorithm and strict theory proof. It starts with box-constrained quadratic programming (Box-QP), and many practical optimization problems fall into Box-QP. Smooth quadratic programming (QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It is the first time to present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which behaves like a "direct" method: the required number of iterations is deterministic with exact value $\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$. This significant breakthrough enables us to transition from the $O(\sqrt{n})$ to the $O(\log(n))$ optimization algorithm, whose amazing scalability is particularly relevant in today's era of big data and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.14096</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#23454;&#20307;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#23376;&#20219;&#21153;&#65292;&#30446;&#21069;&#38754;&#20020;&#30528;&#20247;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24320;&#21457;&#26377;&#25928;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#25152;&#38656;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#27169;&#24335;&#21305;&#37197;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;&#29616;&#26377;&#24320;&#28304;LLMs&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#21160;&#30495;&#23454;&#19990;&#30028;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#36866;&#29992;&#20110;&#20154;&#24037;&#20915;&#31574;&#36741;&#21161;&#30340;&#26234;&#33021;&#24178;&#39044;&#26041;&#26696;&#65292;&#22312;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#26681;&#25454;&#38382;&#39064;&#21644;&#29992;&#25143;&#30340;&#23646;&#24615;&#33258;&#36866;&#24212;&#22320;&#23637;&#31034;AI&#36741;&#21161;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07458</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#20154;&#24037;&#20915;&#31574;&#36741;&#21161;&#30340;&#26234;&#33021;&#24178;&#39044;&#26041;&#26696;&#65306;&#26082;&#32771;&#34385;&#20934;&#30830;&#24615;&#21448;&#20860;&#39038;&#26102;&#38388;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adaptive interventions for both accuracy and time in AI-assisted human decision making. (arXiv:2306.07458v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#36866;&#29992;&#20110;&#20154;&#24037;&#20915;&#31574;&#36741;&#21161;&#30340;&#26234;&#33021;&#24178;&#39044;&#26041;&#26696;&#65292;&#22312;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#26681;&#25454;&#38382;&#39064;&#21644;&#29992;&#25143;&#30340;&#23646;&#24615;&#33258;&#36866;&#24212;&#22320;&#23637;&#31034;AI&#36741;&#21161;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#35201;&#39640;&#20934;&#30830;&#24615;&#20294;&#21516;&#26102;&#26102;&#38388;&#21448;&#32039;&#36843;&#30340;&#29615;&#22659;&#19979;&#65292;&#20363;&#22914;&#22312;&#24613;&#35786;&#23460;&#24037;&#20316;&#30340;&#21307;&#29983;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#65292;&#26082;&#33021;&#25552;&#39640;&#20934;&#30830;&#24615;&#21448;&#33021;&#20943;&#23569;&#26102;&#38388;&#12290;&#20294;&#26159;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#21151;&#33021;&#24102;&#26469;&#30340;&#22909;&#22788;&#26159;&#19981;&#21516;&#30340;&#65306;&#19968;&#20123;&#33021;&#22815;&#20943;&#23569;&#26102;&#38388;&#65292;&#20294;&#20250;&#22686;&#21152;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#30456;&#21453;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24076;&#26395;&#26681;&#25454;&#38382;&#39064;&#21644;&#29992;&#25143;&#30340;&#21508;&#31181;&#23646;&#24615;&#65288;&#22914;&#30693;&#35782;&#27700;&#24179;&#65289;&#26469;&#33258;&#36866;&#24212;&#22320;&#23637;&#31034;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#65292;&#20197;&#20415;&#22312;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24615;&#20043;&#38388;&#20570;&#20986;&#26368;&#20339;&#26435;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#29992;&#25143;&#38656;&#35201;&#20026;&#22806;&#26143;&#20154;&#24320;&#33647;&#26041;&#30340;&#30740;&#31350;&#26469;&#25506;&#32034;&#33258;&#36866;&#24212;AI&#36741;&#21161;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#26681;&#25454;&#38382;&#39064;&#33258;&#36866;&#24212;AI&#36741;&#21161;&#26159;&#26377;&#30410;&#30340;&#65292;&#21487;&#20197;&#36798;&#21040;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#32771;&#34385;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#24378;&#21270;&#23398;&#20064;&#65289;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In settings where users are both time-pressured and need high accuracy, such as doctors working in Emergency Rooms, we want to provide AI assistance that both increases accuracy and reduces time. However, different types of AI assistance have different benefits: some reduce time taken while increasing overreliance on AI, while others do the opposite. We therefore want to adapt what AI assistance we show depending on various properties (of the question and of the user) in order to best tradeoff our two objectives. We introduce a study where users have to prescribe medicines to aliens, and use it to explore the potential for adapting AI assistance. We find evidence that it is beneficial to adapt our AI assistance depending on the question, leading to good tradeoffs between time taken and accuracy. Future work would consider machine-learning algorithms (such as reinforcement learning) to automatically adapt quickly.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;</title><link>http://arxiv.org/abs/2306.03933</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#32622;&#25442;&#19981;&#21464;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#22256;&#38590;&#65292;&#26032;&#29289;&#29702;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#32500;&#31354;&#38388;&#12290;&#29305;&#21035;&#26159;&#22312;&#25104;&#20998;&#32423;&#21035;&#19978;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#31561;&#33391;&#22909;&#24615;&#36136;&#21512;&#24182;&#21040;&#27969;&#34892;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#32622;&#25442;&#19981;&#21464;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#29992;&#20316;&#32622;&#25442;&#19981;&#21464;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#30340;&#21943;&#27880;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#27604;&#19982;&#34987;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#33719;&#24471;&#30340;&#23494;&#24230;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18451</link><description>&lt;p&gt;
&#20855;&#26377;&#22240;&#26524;&#20122;&#32467;&#26500;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#24341;&#36215;&#20102;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#30446;&#26631;&#26159;&#39044;&#27979;&#20998;&#23376;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#23427;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20551;&#23450;&#22522;&#20110;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#26500;&#24314;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#25581;&#31034;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;SCM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#20854;&#24178;&#39044;&#26159;&#22522;&#20110;&#25104;&#23545;&#20998;&#23376;&#26465;&#20214;&#30340;&#12290;&#20351;&#29992;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#20174;&#22240;&#26524;&#20122;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#20102;&#19982;&#21270;&#23398;&#21453;&#24212;&#34394;&#20551;&#30456;&#20851;&#30340;&#24555;&#25463;&#20122;&#32467;&#26500;&#30340;&#28151;&#28102;&#25928;&#24212;&#12290;&#26412;&#25991;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and sy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26550;&#26500;&#26469;&#22686;&#24378;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;&#65292;&#24182;&#22312;&#20854;&#20182;&#35780;&#20215;&#25351;&#26631;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12851</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#22686;&#24378;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Coherence of Extractive Summarization with Multitask Learning. (arXiv:2305.12851v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26550;&#26500;&#26469;&#22686;&#24378;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;&#65292;&#24182;&#22312;&#20854;&#20182;&#35780;&#20215;&#25351;&#26631;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26550;&#26500;&#26469;&#22686;&#24378;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26550;&#26500;&#21253;&#21547;&#19968;&#20010;&#25277;&#21462;&#24335;&#25688;&#35201;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#36830;&#36143;&#24615;&#21028;&#21035;&#22120;&#27169;&#22359;&#12290;&#36830;&#36143;&#24615;&#21028;&#21035;&#22120;&#36890;&#36807;&#22312;&#32447;&#35757;&#32451;&#22686;&#24378;&#20102;&#23545;&#36755;&#20837;&#21477;&#23376;&#36830;&#36143;&#24615;&#30340;&#21028;&#26029;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#26356;&#26032;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#21442;&#25968;&#26469;&#26368;&#22823;&#21270;&#36830;&#36143;&#24615;&#21028;&#21035;&#22120;&#30340;&#36830;&#36143;&#24615;&#20998;&#25968;&#12290;&#20026;&#20102;&#33021;&#22815;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#35757;&#32451;&#25277;&#21462;&#24335;&#25688;&#35201;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#36716;&#25442;&#27169;&#22411;&#21644;&#36716;&#25442;&#30697;&#38453;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#24182;&#21477;&#23376;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25277;&#21462;&#24335;&#25688;&#35201;&#20013;&#36830;&#36143;&#21477;&#23376;&#22312;&#21407;&#22987;&#25991;&#31456;&#20013;&#25353;&#20301;&#32622;&#30340;&#27604;&#20363;&#65288;&#21363;&#33258;&#21160;&#21477;&#23376;&#32423;&#36830;&#36143;&#24230;&#25351;&#26631;&#65289;&#65292;&#32780;&#22312;&#20854;&#20182;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a multitask learning architecture for extractive summarization with coherence boosting. The architecture contains an extractive summarizer and coherent discriminator module. The coherent discriminator is trained online on the sentence vectors of the augmented textual input, thus improving its general ability of judging whether the input sentences are coherent. Meanwhile, we maximize the coherent scores from the coherent discriminator by updating the parameters of the summarizer. To make the extractive sentences trainable in a differentiable manner, we introduce two strategies, including pre-trained converting model (model-based) and converting matrix (MAT-based) that merge sentence representations. Experiments show that our proposed method significantly improves the proportion of consecutive sentences in the extracted summaries based on their positions in the original article (i.e., automatic sentence-level coherence metric), while the goodness in terms of other aut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#22238;&#39038;AI&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;30&#31687;&#20132;&#32455;&#24615;&#35752;&#35770;&#65292;&#25581;&#31034;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#32570;&#20047;&#23545;&#20132;&#32455;&#24615;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#20854;&#19968;&#26041;&#38754;&#23558;&#20854;&#32553;&#23567;&#20026;&#22312;&#32676;&#20307;&#23376;&#32452;&#19978;&#36827;&#34892;&#20844;&#24179;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#21478;&#19968;&#26041;&#38754;&#21017;&#22312;&#31038;&#20250;&#32972;&#26223;&#21644;&#26435;&#21147;&#32467;&#26500;&#30340;&#35752;&#35770;&#26041;&#38754;&#23384;&#22312;&#27424;&#32570;&#12290;</title><link>http://arxiv.org/abs/2303.17555</link><description>&lt;p&gt;
&#23545;&#21387;&#36843;&#30697;&#38453;&#30340;&#20998;&#35299;:&#25581;&#31034;&#20132;&#32455;&#24615;&#22312;AI&#20844;&#24179;&#24615;&#20013;&#30340;&#20316;&#29992;&#30340;&#25209;&#21028;&#24615;&#22238;&#39038;&#19982;&#20877;&#24819;&#35937;
&lt;/p&gt;
&lt;p&gt;
Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness. (arXiv:2303.17555v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#22238;&#39038;AI&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;30&#31687;&#20132;&#32455;&#24615;&#35752;&#35770;&#65292;&#25581;&#31034;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#32570;&#20047;&#23545;&#20132;&#32455;&#24615;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#20854;&#19968;&#26041;&#38754;&#23558;&#20854;&#32553;&#23567;&#20026;&#22312;&#32676;&#20307;&#23376;&#32452;&#19978;&#36827;&#34892;&#20844;&#24179;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#21478;&#19968;&#26041;&#38754;&#21017;&#22312;&#31038;&#20250;&#32972;&#26223;&#21644;&#26435;&#21147;&#32467;&#26500;&#30340;&#35752;&#35770;&#26041;&#38754;&#23384;&#22312;&#27424;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#32455;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#26597;&#21644;&#23454;&#36341;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26816;&#26597;&#31038;&#20250;&#19981;&#24179;&#31561;&#22914;&#20309;&#36890;&#36807;&#32467;&#26500;&#21644;&#32426;&#24459;&#39046;&#22495;&#25345;&#32493;&#23384;&#22312;&#12290;&#22312;AI&#20844;&#24179;&#30340;&#29702;&#24565;&#20013;&#65292;&#8220;&#20844;&#24179;&#24615;&#8221;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#37319;&#29992;&#20132;&#32455;&#24615;&#20316;&#20026;&#20998;&#26512;&#26694;&#26550;&#23545;&#20110;&#26377;&#25928;&#22320;&#23454;&#29616;&#20844;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23545;AI&#20844;&#24179;&#25991;&#29486;&#20013;30&#31687;&#20851;&#20110;&#20132;&#32455;&#24615;&#30340;&#35752;&#35770;&#36827;&#34892;&#25209;&#21028;&#24615;&#22238;&#39038;&#65292;&#25105;&#20204;&#24402;&#32435;&#21644;&#28436;&#32462;&#20986;:1)&#20132;&#32455;&#24615;&#25351;&#23548;&#22914;&#20309;&#22312;AI&#20844;&#24179;&#33539;&#20363;&#20013;&#25805;&#20316;&#65292;2)&#25581;&#31034;&#20132;&#32455;&#24615;&#30340;&#27010;&#24565;&#21270;&#21644;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#23558;&#20132;&#32455;&#24615;&#32553;&#20943;&#20026;&#38024;&#23545;&#20154;&#21475;&#20122;&#32452;&#30340;&#20844;&#24179;&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#20182;&#20204;&#20063;&#26410;&#33021;&#35752;&#35770;&#23427;&#20204;&#30340;&#31038;&#20250;&#32972;&#26223;&#65292;&#24403;&#25552;&#21040;&#26435;&#21147;&#26102;&#65292;&#20182;&#20204;&#20027;&#35201;&#23558;&#20854;&#32622;&#20110;AI&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#36827;&#19968;&#27493;&#38416;&#36848;&#24182;&#35780;&#20272;&#36825;&#20123;&#24046;&#36317;&#23545;&#20110;&#20020;&#24202;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intersectionality is a critical framework that, through inquiry and praxis, allows us to examine how social inequalities persist through domains of structure and discipline. Given AI fairness' raison d'\^etre of ``fairness,'' we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness. Through a critical review of how intersectionality is discussed in 30 papers from the AI fairness literature, we deductively and inductively: 1) map how intersectionality tenets operate within the AI fairness paradigm and 2) uncover gaps between the conceptualization and operationalization of intersectionality. We find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups. They also fail to discuss their social context and when mentioning power, they mostly situate it only within the AI pipeline. We: 3) outline and assess the implications of these gaps for critical inquiry and prax
&lt;/p&gt;</description></item><item><title>BoxSnake&#26159;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26694;&#27880;&#37322;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#25513;&#33180;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;BoxSnake&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11630</link><description>&lt;p&gt;
BoxSnake&#65306;&#20351;&#29992;&#26694;&#27880;&#37322;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
BoxSnake: Polygonal Instance Segmentation with Box Supervision. (arXiv:2303.11630v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11630
&lt;/p&gt;
&lt;p&gt;
BoxSnake&#26159;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26694;&#27880;&#37322;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#25513;&#33180;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;BoxSnake&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26694;&#27880;&#37322;&#30340;&#23454;&#20363;&#20998;&#21106;&#22240;&#21482;&#38656;&#35201;&#31616;&#21333;&#30340;&#26694;&#26631;&#27880;&#32780;&#38750;&#26114;&#36149;&#30340;&#25513;&#33180;&#25110;&#22810;&#36793;&#24418;&#26631;&#27880;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24102;&#26694;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#25513;&#33180;&#30340;&#26694;&#26550;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25216;&#26415;BoxSnake&#65292;&#39318;&#27425;&#20165;&#20351;&#29992;&#26694;&#27880;&#37322;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#65306;&#65288;1&#65289;&#22522;&#20110;&#28857;&#30340;&#21333;&#20803;&#25439;&#22833;&#65292;&#32422;&#26463;&#39044;&#27979;&#22810;&#36793;&#24418;&#30340;&#36793;&#30028;&#26694;&#20197;&#23454;&#29616;&#31895;&#30053;&#20998;&#21106;&#65307;&#65288;2&#65289;&#36317;&#31163;&#24863;&#30693;&#30340;&#25104;&#23545;&#25439;&#22833;&#65292;&#20419;&#20351;&#39044;&#27979;&#30340;&#22810;&#36793;&#24418;&#36148;&#21512;&#29289;&#20307;&#36793;&#30028;&#12290;&#19982;&#22522;&#20110;&#25513;&#33180;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;BoxSnake&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#39044;&#27979;&#20998;&#21106;&#19982;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#22312;Cityscapes&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Box-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new end-to-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that constrains the bounding box of predicted polygons to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss that encourages the predicted polygons to fit the object boundaries. Compared with the mask-based weakly-supervised methods, BoxSnake further reduces the performance gap between the predicted segmentation and the bounding box, and shows significant superiority on the Cityscapes dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20197;&#27133;&#20026;&#20013;&#24515;&#30340;&#21442;&#32771;&#26694;&#26550;&#26469;&#25913;&#36827;&#23545;&#35937;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Slot Attention&#20013;&#34701;&#20837;&#31354;&#38388;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25972;&#20307;&#23545;&#35937;&#21457;&#29616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04973</link><description>&lt;p&gt;
&#19981;&#21464;&#30340;&#27133;&#27880;&#24847;&#21147;: &#36890;&#36807;&#20197;&#27133;&#20026;&#20013;&#24515;&#30340;&#21442;&#32771;&#26694;&#26550;&#36827;&#34892;&#23545;&#35937;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames. (arXiv:2302.04973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20197;&#27133;&#20026;&#20013;&#24515;&#30340;&#21442;&#32771;&#26694;&#26550;&#26469;&#25913;&#36827;&#23545;&#35937;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Slot Attention&#20013;&#34701;&#20837;&#31354;&#38388;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25972;&#20307;&#23545;&#35937;&#21457;&#29616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#24863;&#30693;&#25968;&#25454;&#20013;&#33258;&#21160;&#21457;&#29616;&#21487;&#32452;&#21512;&#30340;&#25277;&#35937;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27133;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#23545;&#35937;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#22312;&#20805;&#20998;&#25429;&#25417;&#35270;&#35273;&#19990;&#30028;&#20013;&#30340;&#31354;&#38388;&#23545;&#31216;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#23548;&#33268;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#65292;&#27604;&#22914;&#22312;&#32416;&#32467;&#23545;&#35937;&#22806;&#35266;&#21644;&#23039;&#24577;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#27133;&#20026;&#20013;&#24515;&#30340;&#21442;&#32771;&#26694;&#26550;&#26469;&#34701;&#20837;&#31354;&#38388;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#23558;&#31561;&#21464;&#24615;&#24341;&#20837;&#21040;Slot Attention&#30340;&#27880;&#24847;&#21147;&#21644;&#29983;&#25104;&#26426;&#21046;&#20013;&#65292;&#36890;&#36807;&#24179;&#31227;&#12289;&#32553;&#25918;&#21644;&#26059;&#36716;&#20301;&#32622;&#32534;&#30721;&#26469;&#23454;&#29616;&#23545;&#27599;&#20010;&#23545;&#35937;&#23039;&#24577;&#21464;&#25442;&#30340;&#31561;&#21464;&#24615;&#12290;&#36825;&#20123;&#25913;&#21464;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#25972;&#20307;&#23545;&#35937;&#21457;&#29616;&#26041;&#38754;&#21462;&#24471;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#35821;&#27861;&#33539;&#22260;&#20869;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically discovering composable abstractions from raw perceptual data is a long-standing challenge in machine learning. Recent slot-based neural networks that learn about objects in a self-supervised manner have made exciting progress in this direction. However, they typically fall short at adequately capturing spatial symmetries present in the visual world, which leads to sample inefficiency, such as when entangling object appearance and pose. In this paper, we present a simple yet highly effective method for incorporating spatial symmetries via slot-centric reference frames. We incorporate equivariance to per-object pose transformations into the attention and generation mechanism of Slot Attention by translating, scaling, and rotating position encodings. These changes result in little computational overhead, are easy to implement, and can result in large gains in terms of data efficiency and overall improvements to object discovery. We evaluate our method on a wide range of synt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;&#26041;&#27861;SpArX&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26356;&#24544;&#23454;&#21644;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2301.09559</link><description>&lt;p&gt;
SpArX: &#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SpArX: Sparse Argumentative Explanations for Neural Networks. (arXiv:2301.09559v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;&#26041;&#27861;SpArX&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26356;&#24544;&#23454;&#21644;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20294;&#35299;&#37322;&#23427;&#20204;&#30340;&#20915;&#31574;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#35299;&#37322;&#25913;&#21464;&#21333;&#20010;&#36755;&#20837;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#19968;&#33268;&#30340;&#35299;&#37322;&#26410;&#24517;&#24544;&#23454;&#20110;&#20854;&#23454;&#38469;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#26426;&#21046;&#21019;&#24314;&#20102;&#35770;&#35777;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;SpArX&#26041;&#27861;&#39318;&#20808;&#23558;&#22810;&#23618;&#24863;&#30693;&#22120;&#31232;&#30095;&#21270;&#65292;&#21516;&#26102;&#20445;&#25345;&#23613;&#21487;&#33021;&#22810;&#30340;&#21407;&#22987;&#32467;&#26500;&#12290;&#28982;&#21518;&#23558;&#31232;&#30095;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#65292;&#20197;&#25581;&#31034;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#65292;&#20135;&#29983;&#20840;&#23616;&#21644;/&#25110;&#23616;&#37096;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SpArX&#27604;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#32473;&#20986;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#23454;&#38469;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) have various applications in AI, but explaining their decisions remains challenging. Existing approaches often focus on explaining how changing individual inputs affects NNs' outputs. However, an explanation that is consistent with the input-output behaviour of an NN is not necessarily faithful to the actual mechanics thereof. In this paper, we exploit relationships between multi-layer perceptrons (MLPs) and quantitative argumentation frameworks (QAFs) to create argumentative explanations for the mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining as much of the original structure as possible. It then translates the sparse MLP into an equivalent QAF to shed light on the underlying decision process of the MLP, producing global and/or local explanations. We demonstrate experimentally that SpArX can give more faithful explanations than existing approaches, while simultaneously providing deeper insights into the actual reasoning process of M
&lt;/p&gt;</description></item><item><title>NusaCrowd&#26159;&#19968;&#20010;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;&#65292;&#24050;&#27719;&#38598;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#65292;&#20026;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#22810;&#31181;&#23454;&#39564;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2212.09648</link><description>&lt;p&gt;
NusaCrowd&#65306;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;
&lt;/p&gt;
&lt;p&gt;
NusaCrowd: Open Source Initiative for Indonesian NLP Resources. (arXiv:2212.09648v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09648
&lt;/p&gt;
&lt;p&gt;
NusaCrowd&#26159;&#19968;&#20010;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;&#65292;&#24050;&#27719;&#38598;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#65292;&#20026;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#22810;&#31181;&#23454;&#39564;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NusaCrowd&#65292;&#36825;&#26159;&#19968;&#20010;&#21327;&#20316;&#20513;&#35758;&#65292;&#26088;&#22312;&#25910;&#38598;&#21644;&#32479;&#19968;&#21360;&#23612;&#35821;&#35328;&#30340;&#29616;&#26377;&#36164;&#28304;&#65292;&#21253;&#25324;&#24320;&#25918;&#20197;&#21069;&#38750;&#20844;&#24320;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#35813;&#20513;&#35758;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#26631;&#20934;&#21270;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#12290;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#24050;&#32463;&#32463;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#23427;&#20204;&#30340;&#20215;&#20540;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;NusaCrowd&#30340;&#25968;&#25454;&#25910;&#38598;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#25512;&#36827;&#23545;&#22312;&#20351;&#29992;&#24191;&#27867;&#30340;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#20174;&#32780;&#20351;&#20043;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments. NusaCrowd's data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31995;&#21015;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#26041;&#27861;&#23558;&#35821;&#38899;&#34920;&#31034;&#19982;&#21457;&#38899;&#36712;&#36857;&#30456;&#32852;&#31995;&#12290;&#32467;&#26524;&#26174;&#31034;&#22768;&#36947;&#21457;&#38899;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.11723</link><description>&lt;p&gt;
&#35777;&#25454;&#26174;&#31034;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#22768;&#36947;&#21457;&#38899;
&lt;/p&gt;
&lt;p&gt;
Evidence of Vocal Tract Articulation in Self-Supervised Learning of Speech. (arXiv:2210.11723v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11723
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31995;&#21015;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#26041;&#27861;&#23558;&#35821;&#38899;&#34920;&#31034;&#19982;&#21457;&#38899;&#36712;&#36857;&#30456;&#32852;&#31995;&#12290;&#32467;&#26524;&#26174;&#31034;&#22768;&#36947;&#21457;&#38899;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#33021;&#22815;&#23398;&#20064;&#21040;&#20016;&#23500;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#26041;&#20415;&#22320;&#34987;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21033;&#29992;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#31181;&#25928;&#29992;&#65292;&#23545;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#21508;&#31181;&#20998;&#26512;&#65292;&#20197;&#25581;&#31034;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#26159;&#21738;&#20123;&#20197;&#21450;&#22914;&#20309;&#32534;&#30721;&#30340;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#20998;&#26512;&#33539;&#22260;&#28085;&#30422;&#20102;&#22768;&#23398;&#12289;&#38899;&#20301;&#21644;&#35821;&#20041;&#31561;&#26041;&#38754;&#65292;&#20294;&#23545;&#35821;&#38899;&#20135;&#29983;&#30340;&#29289;&#29702;&#22522;&#30784;&#30340;&#20851;&#27880;&#36824;&#19981;&#22815;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#23558;&#35821;&#38899;&#34920;&#31034;&#19982;&#30005;&#30913;&#35013;&#32622;&#35760;&#24405;&#30340;&#21457;&#38899;&#36712;&#36857;&#30456;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#32447;&#24615;&#25506;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#20197;EMA&#32447;&#24615;&#26144;&#23556;&#30340;&#24179;&#22343;&#30456;&#20851;&#24615;&#27979;&#37327;&#21457;&#38899;&#24471;&#20998;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;SUPERB&#22522;&#20934;&#27979;&#35797;&#25490;&#34892;&#27036;&#20013;&#36873;&#25321;&#30340;&#19968;&#32452;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23545;&#26368;&#25104;&#21151;&#30340;&#20004;&#20010;&#27169;&#22411;Wav2Vec 2.0&#21644;HuBERT&#36827;&#34892;&#20102;&#26356;&#36827;&#19968;&#27493;&#30340;&#36880;&#23618;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent self-supervised learning (SSL) models have proven to learn rich representations of speech, which can readily be utilized by diverse downstream tasks. To understand such utilities, various analyses have been done for speech SSL models to reveal which and how information is encoded in the learned representations. Although the scope of previous analyses is extensive in acoustic, phonetic, and semantic perspectives, the physical grounding by speech production has not yet received full attention. To bridge this gap, we conduct a comprehensive analysis to link speech representations to articulatory trajectories measured by electromagnetic articulography (EMA). Our analysis is based on a linear probing approach where we measure articulatory score as an average correlation of linear mapping to EMA. We analyze a set of SSL models selected from the leaderboard of the SUPERB benchmark and perform further layer-wise analyses on two most successful models, Wav2Vec 2.0 and HuBERT. Surprisingl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37327;&#21270;&#20998;&#26512;&#20102;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36827;&#32780;&#32771;&#34385;&#20102;3&#31181;&#25919;&#31574;&#20462;&#25913;&#21450;&#20854;&#24433;&#21709;&#65292;&#26088;&#22312;&#20026;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#25490;&#24207;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2207.07392</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#22768;&#26126;&#24615;&#27169;&#22411;&#8212;&#8212;&#36879;&#26126;&#24230;&#30340;&#24314;&#27169;&#21644;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A simple declarative model of the Federal Disaster Assistance Policy -- modelling and measuring transparency. (arXiv:2207.07392v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37327;&#21270;&#20998;&#26512;&#20102;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36827;&#32780;&#32771;&#34385;&#20102;3&#31181;&#25919;&#31574;&#20462;&#25913;&#21450;&#20854;&#24433;&#21709;&#65292;&#26088;&#22312;&#20026;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#25490;&#24207;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19977;&#20010;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#65292;&#23545;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#36827;&#34892;&#20102;&#25968;&#37327;&#20998;&#26512;&#12290;&#36825;&#31181;&#25968;&#37327;&#20998;&#26512;&#26041;&#27861;&#26159;&#26032;&#30340;&#65292;&#22312;&#20854;&#20182;&#39046;&#22495;&#22914;&#19994;&#21153;&#21644;&#21307;&#30103;&#27969;&#31243;&#20013;&#20063;&#26377;&#24212;&#29992;&#12290;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#36807;&#31243;&#36879;&#26126;&#24230;&#24456;&#24863;&#20852;&#36259;&#65292;&#20294;&#27599;&#20010;&#20154;&#23545;&#36879;&#26126;&#24230;&#30340;&#20855;&#20307;&#23450;&#20041;&#37117;&#26377;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19977;&#31181;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#20462;&#25913;&#65292;&#24182;&#20998;&#26512;&#20102;&#20174;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#30475;&#65292;&#27599;&#31181;&#25919;&#31574;&#19979;&#21033;&#30410;&#30456;&#20851;&#32773;&#28385;&#24847;&#24230;&#30340;&#21464;&#21270;&#24773;&#20917;&#12290;&#36825;&#31181;&#20998;&#26512;&#34987;&#29992;&#26469;&#23545;&#22235;&#31181;&#25919;&#31574;&#30340;&#20559;&#22909;&#36827;&#34892;&#25490;&#24207;&#65292;&#20197;&#20415;&#32771;&#34385;&#21040;&#25152;&#26377;&#38598;&#20307;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we will provide a quantitative analysis of a simple model of the Federal Disaster Assistance policy from the viewpoint of three different stakeholders. This quantitative methodology is new and has applications to other areas such as business and healthcare processes. The stakeholders are interested in process transparency but each has a different opinion on precisely what constitutes transparency. We will also consider three modifications to the Federal Disaster Assistance policy and analyse, from a stakeholder viewpoint, how stakeholder satisfaction changes from process to process. This analysis is used to rank the favourability of four policies with respect to all collective stakeholder preferences.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#27963;&#21160;&#30340;&#21464;&#21270;&#19982;&#36830;&#25509;&#21040;&#19979;&#19968;&#23618;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#21464;&#21270;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#20598;&#20851;&#31995;&#12290;&#36890;&#36807;&#36825;&#31181;&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#21270;&#26144;&#23556;&#21040;&#23545;&#24212;&#30340;&#26435;&#37325;&#21464;&#21270;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#29305;&#24449;&#26041;&#21521;&#30340;&#20960;&#20309;&#22240;&#23376;&#30340;&#20056;&#31215;&#26469;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2203.10736</link><description>&lt;p&gt;
&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27963;&#21160;-&#26435;&#37325;&#23545;&#20598;&#24615;&#65306;&#27867;&#21270;&#24615;&#30340;&#20960;&#20309;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
The activity-weight duality in feed forward neural networks: The geometric determinants of generalization. (arXiv:2203.10736v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10736
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#27963;&#21160;&#30340;&#21464;&#21270;&#19982;&#36830;&#25509;&#21040;&#19979;&#19968;&#23618;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#21464;&#21270;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#20598;&#20851;&#31995;&#12290;&#36890;&#36807;&#36825;&#31181;&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#21270;&#26144;&#23556;&#21040;&#23545;&#24212;&#30340;&#26435;&#37325;&#21464;&#21270;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#29305;&#24449;&#26041;&#21521;&#30340;&#20960;&#20309;&#22240;&#23376;&#30340;&#20056;&#31215;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#27867;&#21270;&#24615;&#12290;&#22312;&#20855;&#26377;&#22823;&#37327;&#26435;&#37325;&#65288;&#21442;&#25968;&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#25214;&#21040;&#24456;&#22810;&#35299;&#26469;&#24456;&#22909;&#22320;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#21738;&#20010;&#35299;&#33021;&#22815;&#25551;&#36848;&#19981;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#20219;&#20309;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#23494;&#38598;&#36830;&#25509;&#23618;&#20013;&#65292;&#32473;&#23450;&#23618;&#31070;&#32463;&#20803;&#27963;&#21160;&#30340;&#21464;&#21270;&#19982;&#36830;&#25509;&#21040;&#19979;&#19968;&#23618;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#21464;&#21270;&#20043;&#38388;&#30340;&#30830;&#20999;&#23545;&#20598;&#65288;&#31561;&#20215;&#65289;&#20851;&#31995;&#30340;&#21457;&#29616;&#12290;&#27963;&#21160;-&#26435;&#37325;&#65288;A-W&#65289;&#23545;&#20598;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36755;&#20837;&#65288;&#25968;&#25454;&#65289;&#30340;&#21464;&#21270;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#23545;&#20598;&#26435;&#37325;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26144;&#23556;&#65292;&#25105;&#20204;&#34920;&#26126;&#27867;&#21270;&#25439;&#22833;&#21487;&#20197;&#20998;&#35299;&#20026;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#19981;&#21516;&#29305;&#24449;&#26041;&#21521;&#30340;&#36129;&#29486;&#20043;&#21644;&#12290;&#32473;&#23450;&#29305;&#24449;&#26041;&#21521;&#30340;&#36129;&#29486;&#26159;&#20004;&#20010;&#20960;&#20309;&#22240;&#23376;&#65288;&#34892;&#21015;&#24335;&#65289;&#30340;&#20056;&#31215;&#65306;&#23574;&#38160;&#24230;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental problems in machine learning is generalization. In neural network models with a large number of weights (parameters), many solutions can be found to fit the training data equally well. The key question is which solution can describe testing data not in the training set. Here, we report the discovery of an exact duality (equivalence) between changes in activities in a given layer of neurons and changes in weights that connect to the next layer of neurons in a densely connected layer in any feed forward neural network. The activity-weight (A-W) duality allows us to map variations in inputs (data) to variations of the corresponding dual weights. By using this mapping, we show that the generalization loss can be decomposed into a sum of contributions from different eigen-directions of the Hessian matrix of the loss function at the solution in weight space. The contribution from a given eigen-direction is the product of two geometric factors (determinants): the sharpn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#22312;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#24615;&#25506;&#32034;&#34892;&#20026;&#26469;&#25913;&#21892;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#36830;&#36890;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.08227</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#23376;&#22270;&#23398;&#20064;&#34920;&#26684;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-agent Skills for Tabular Reinforcement Learning using Factor Graphs. (arXiv:2201.08227v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#22312;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#24615;&#25506;&#32034;&#34892;&#20026;&#26469;&#25913;&#21892;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#33021;&#21457;&#29616;&#34987;&#24320;&#21457;&#29992;&#20110;&#25913;&#21892;&#21333;&#26234;&#33021;&#20307;&#24773;&#26223;&#20013;&#31232;&#30095;&#22870;&#21169;&#20449;&#21495;&#30340;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#33021;&#21147;&#65292;&#36890;&#36807;&#36830;&#25509;&#29366;&#24577;&#36716;&#31227;&#22270;&#30340;Fiedler&#21521;&#37327;&#25552;&#20379;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#26368;&#36828;&#30340;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#65292;&#22240;&#20026;&#31995;&#32479;&#20013;&#30340;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#65292;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#37319;&#29992;&#25216;&#33021;&#20173;&#28982;&#20381;&#36182;&#20110;&#21333;&#26234;&#33021;&#20307;&#25216;&#33021;&#21457;&#29616;&#65292;&#24182;&#26410;&#30452;&#25509;&#21457;&#29616;&#33021;&#22815;&#25913;&#21892;&#26234;&#33021;&#20307;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#36830;&#36890;&#24615;&#30340;&#32852;&#21512;&#25216;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#21512;&#20316;&#24615;&#25506;&#32034;&#34892;&#20026;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#30452;&#25509;&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#30340;&#21487;&#34892;&#24615;&#65292;&#21516;&#26102;&#20173;&#28982;&#20139;&#21463;&#20998;&#35299;&#30340;&#20415;&#21033;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#36924;&#36817;&#20026;Kronecker&#22270;&#8212;&#8212;Kronecker
&lt;/p&gt;
&lt;p&gt;
Covering skill (a.k.a., option) discovery has been developed to improve the exploration of reinforcement learning in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. However, these option discovery methods cannot be directly extended to multi-agent scenarios, since the joint state space grows exponentially with the number of agents in the system. Thus, existing researches on adopting options in multi-agent scenarios still rely on single-agent option discovery and fail to directly discover the joint options that can improve the connectivity of the joint state space of agents. In this paper, we show that it is indeed possible to directly compute multi-agent options with collaborative exploratory behaviors among the agents, while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph -- the Kronecker 
&lt;/p&gt;</description></item></channel></rss>