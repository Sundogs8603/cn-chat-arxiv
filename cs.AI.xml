<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ClothesNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;3D&#26381;&#35013;&#23545;&#35937;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26381;&#35013;&#29305;&#24449;&#12289;&#36793;&#30028;&#32447;&#21644;&#20851;&#38190;&#28857;&#30340;&#20016;&#23500;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#20419;&#36827;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#27169;&#25311;&#26381;&#35013;&#29615;&#22659;&#65292;ClothesNet&#36824;&#21487;&#20197;&#36827;&#34892;&#21253;&#25324;&#20998;&#31867;&#12289;&#36793;&#30028;&#32447;&#20998;&#21106;&#12289;&#20851;&#38190;&#28857;&#26816;&#27979;&#22312;&#20869;&#30340;&#26381;&#35013;&#24863;&#30693;&#20219;&#21153;&#20197;&#21450;&#21253;&#25324;&#37325;&#26032;&#25670;&#25918;&#12289;&#25240;&#21472;&#12289;&#25346;&#34915;&#21644;&#31359;&#34915;&#31561;&#22312;&#20869;&#30340;&#26426;&#22120;&#20154;&#20132;&#20114;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ClothesNet&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09987</link><description>&lt;p&gt;
ClothesNet&#65306;&#19968;&#20010;&#20855;&#26377;&#27169;&#25311;&#26381;&#35013;&#29615;&#22659;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;3D&#26381;&#35013;&#27169;&#22411;&#24211;
&lt;/p&gt;
&lt;p&gt;
ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment. (arXiv:2308.09987v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09987
&lt;/p&gt;
&lt;p&gt;
ClothesNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;3D&#26381;&#35013;&#23545;&#35937;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26381;&#35013;&#29305;&#24449;&#12289;&#36793;&#30028;&#32447;&#21644;&#20851;&#38190;&#28857;&#30340;&#20016;&#23500;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#20419;&#36827;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#27169;&#25311;&#26381;&#35013;&#29615;&#22659;&#65292;ClothesNet&#36824;&#21487;&#20197;&#36827;&#34892;&#21253;&#25324;&#20998;&#31867;&#12289;&#36793;&#30028;&#32447;&#20998;&#21106;&#12289;&#20851;&#38190;&#28857;&#26816;&#27979;&#22312;&#20869;&#30340;&#26381;&#35013;&#24863;&#30693;&#20219;&#21153;&#20197;&#21450;&#21253;&#25324;&#37325;&#26032;&#25670;&#25918;&#12289;&#25240;&#21472;&#12289;&#25346;&#34915;&#21644;&#31359;&#34915;&#31561;&#22312;&#20869;&#30340;&#26426;&#22120;&#20154;&#20132;&#20114;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ClothesNet&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ClothesNet&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21253;&#21547;&#20855;&#26377;&#20449;&#24687;&#20016;&#23500;&#27880;&#37322;&#30340;3D&#26381;&#35013;&#23545;&#35937;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#32422;4400&#20010;&#28085;&#30422;11&#20010;&#31867;&#21035;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26631;&#27880;&#20102;&#26381;&#35013;&#29305;&#24449;&#12289;&#36793;&#30028;&#32447;&#21644;&#20851;&#38190;&#28857;&#12290;ClothesNet&#21487;&#20197;&#29992;&#20110;&#20419;&#36827;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#20219;&#21153;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26381;&#35013;&#24863;&#30693;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#36793;&#30028;&#32447;&#20998;&#21106;&#21644;&#20851;&#38190;&#28857;&#26816;&#27979;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#20110;&#26426;&#22120;&#20154;&#20132;&#20114;&#20219;&#21153;&#30340;&#27169;&#25311;&#26381;&#35013;&#29615;&#22659;&#65292;&#21253;&#25324;&#37325;&#26032;&#25670;&#25918;&#12289;&#25240;&#21472;&#12289;&#25346;&#34915;&#21644;&#31359;&#34915;&#31561;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;ClothesNet&#22312;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#34917;&#20805;&#26448;&#26009;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#39029;&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ClothesNet: a large-scale dataset of 3D clothes objects with information-rich annotations. Our dataset consists of around 4400 models covering 11 categories annotated with clothes features, boundary lines, and keypoints. ClothesNet can be used to facilitate a variety of computer vision and robot interaction tasks. Using our dataset, we establish benchmark tasks for clothes perception, including classification, boundary line segmentation, and keypoint detection, and develop simulated clothes environments for robotic interaction tasks, including rearranging, folding, hanging, and dressing. We also demonstrate the efficacy of our ClothesNet in real-world experiments. Supplemental materials and dataset are available on our project webpage.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#27431;&#27954;&#20844;&#27665;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#12289;&#24577;&#24230;&#21644;&#20449;&#20219;&#65292;&#21457;&#29616;&#23613;&#31649;&#24847;&#35782;&#27700;&#24179;&#36739;&#20302;&#65292;&#20294;&#19968;&#21322;&#20197;&#19978;&#30340;&#20154;&#23545;&#20154;&#24037;&#26234;&#33021;&#25345;&#31215;&#26497;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.09979</link><description>&lt;p&gt;
&#27431;&#27954;&#33539;&#22260;&#20869;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#23545;&#24847;&#35782;&#12289;&#24577;&#24230;&#21644;&#20449;&#20219;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence across Europe: A Study on Awareness, Attitude and Trust. (arXiv:2308.09979v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#27431;&#27954;&#20844;&#27665;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#12289;&#24577;&#24230;&#21644;&#20449;&#20219;&#65292;&#21457;&#29616;&#23613;&#31649;&#24847;&#35782;&#27700;&#24179;&#36739;&#20302;&#65292;&#20294;&#19968;&#21322;&#20197;&#19978;&#30340;&#20154;&#23545;&#20154;&#24037;&#26234;&#33021;&#25345;&#31215;&#26497;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#24191;&#27867;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#35843;&#26597;&#20102;&#26469;&#33258;&#20843;&#20010;&#19981;&#21516;&#22269;&#23478;&#65288;&#27861;&#22269;&#12289;&#24503;&#22269;&#12289;&#24847;&#22823;&#21033;&#12289;&#33655;&#20848;&#12289;&#27874;&#20848;&#12289;&#32599;&#39532;&#23612;&#20122;&#12289;&#35199;&#29677;&#29273;&#21644;&#29790;&#20856;&#65289;&#30340;4,006&#21517;&#27431;&#27954;&#20844;&#27665;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#35266;&#28857;&#12290;&#30740;&#31350;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#27431;&#27954;&#19978;&#19979;&#25991;&#20013;&#20154;&#20204;&#30340;&#35266;&#28857;&#21644;&#35748;&#30693;&#65292;&#36825;&#24050;&#32463;&#21463;&#21040;&#37325;&#35201;&#25919;&#31574;&#34892;&#21160;&#21644;&#30417;&#31649;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35843;&#26597;&#27431;&#27954;&#20844;&#27665;&#30340;&#35266;&#24565;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#39564;&#35777;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#21367;&#65288;PAICE&#65289;&#65292;&#22260;&#32469;&#20154;&#20204;&#30340;&#24847;&#35782;&#12289;&#24577;&#24230;&#21644;&#20449;&#20219;&#19977;&#20010;&#32500;&#24230;&#36827;&#34892;&#26500;&#24314;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#24847;&#35782;&#27700;&#24179;&#33258;&#25105;&#35780;&#20272;&#36739;&#20302;&#65292;&#20294;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#24577;&#24230;&#22312;&#21322;&#25968;&#20197;&#19978;&#30340;&#20154;&#21475;&#20013;&#38750;&#24120;&#31215;&#26497;&#12290;&#36890;&#36807;&#23545;&#25910;&#38598;&#21040;&#30340;&#32467;&#26524;&#36827;&#34892;&#21453;&#24605;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#38544;&#21547;&#30340;&#30683;&#30462;&#28857;&#65292;&#24182;&#30830;&#23450;&#20102;&#21487;&#33021;&#24433;&#21709;&#20449;&#20219;&#29983;&#24577;&#31995;&#32479;&#30340;&#24314;&#31435;&#21644;&#21253;&#23481;&#24615;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of an extensive study investigating the opinions on Artificial Intelligence (AI) of a sample of 4,006 European citizens from eight distinct countries (France, Germany, Italy, Netherlands, Poland, Romania, Spain, and Sweden). The aim of the study is to gain a better understanding of people's views and perceptions within the European context, which is already marked by important policy actions and regulatory processes. To survey the perceptions of the citizens of Europe we design and validate a new questionnaire (PAICE) structured around three dimensions: people's awareness, attitude, and trust. We observe that while awareness is characterized by a low level of self-assessed competency, the attitude toward AI is very positive for more than half of the population. Reflecting upon the collected results, we highlight implicit contradictions and identify trends that may interfere with the creation of an ecosystem of trust and the development of inclusive AI po
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#26102;&#38388;&#23884;&#20837;&#30340;&#32423;&#32852;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#20449;&#24687;&#27969;&#34892;&#24230;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#26102;&#38388;&#23884;&#20837;&#26041;&#27861;&#23558;&#26102;&#38388;&#23646;&#24615;&#34701;&#20837;&#33410;&#28857;&#29305;&#24449;&#20013;&#65292;&#28982;&#21518;&#37319;&#29992;&#32423;&#32852;&#22270;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#32423;&#32852;&#24207;&#21015;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#26469;&#20805;&#20998;&#23398;&#20064;&#32423;&#32852;&#22270;&#21644;&#32423;&#32852;&#24207;&#21015;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.09976</link><description>&lt;p&gt;
&#26174;&#24335;&#26102;&#38388;&#23884;&#20837;&#30340;&#32423;&#32852;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#20449;&#24687;&#27969;&#34892;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explicit Time Embedding Based Cascade Attention Network for Information Popularity Prediction. (arXiv:2308.09976v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#26102;&#38388;&#23884;&#20837;&#30340;&#32423;&#32852;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#20449;&#24687;&#27969;&#34892;&#24230;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#26102;&#38388;&#23884;&#20837;&#26041;&#27861;&#23558;&#26102;&#38388;&#23646;&#24615;&#34701;&#20837;&#33410;&#28857;&#29305;&#24449;&#20013;&#65292;&#28982;&#21518;&#37319;&#29992;&#32423;&#32852;&#22270;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#32423;&#32852;&#24207;&#21015;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#26469;&#20805;&#20998;&#23398;&#20064;&#32423;&#32852;&#22270;&#21644;&#32423;&#32852;&#24207;&#21015;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#39044;&#27979;&#20449;&#24687;&#32423;&#32852;&#30340;&#27969;&#34892;&#24230;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25429;&#25417;&#26102;&#38388;&#23646;&#24615;&#21644;&#32423;&#32852;&#35282;&#33394;&#20449;&#24687;&#65288;&#22914;&#32423;&#32852;&#22270;&#21644;&#32423;&#32852;&#24207;&#21015;&#65289;&#23545;&#20110;&#29702;&#35299;&#20449;&#24687;&#32423;&#32852;&#26159;&#24517;&#35201;&#30340;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#24456;&#23569;&#20851;&#27880;&#23558;&#36825;&#20123;&#20449;&#24687;&#32479;&#19968;&#36215;&#26469;&#36827;&#34892;&#27969;&#34892;&#24230;&#39044;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#22320;&#23545;&#32423;&#32852;&#30340;&#20840;&#37096;&#23646;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#32423;&#32852;&#27880;&#24847;&#21147;&#32593;&#32476;(TCAN)&#20316;&#20026;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#20449;&#24687;&#32593;&#32476;&#30340;&#26032;&#22411;&#27969;&#34892;&#24230;&#39044;&#27979;&#26550;&#26500;&#12290;TCAN&#36890;&#36807;&#19968;&#31181;&#36890;&#29992;&#30340;&#26102;&#38388;&#23884;&#20837;&#26041;&#27861;(TC)&#23558;&#26102;&#38388;&#23646;&#24615;&#65288;&#21608;&#26399;&#24615;&#12289;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#32553;&#25918;&#65289;&#34701;&#20837;&#33410;&#28857;&#29305;&#24449;&#20013;&#65292;&#28982;&#21518;&#37319;&#29992;&#32423;&#32852;&#22270;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;(CGAT)&#21644;&#32423;&#32852;&#24207;&#21015;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;(CSAT)&#26469;&#20805;&#20998;&#23398;&#20064;&#32423;&#32852;&#22270;&#21644;&#32423;&#32852;&#24207;&#21015;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting information cascade popularity is a fundamental problem in social networks. Capturing temporal attributes and cascade role information (e.g., cascade graphs and cascade sequences) is necessary for understanding the information cascade. Current methods rarely focus on unifying this information for popularity predictions, which prevents them from effectively modeling the full properties of cascades to achieve satisfactory prediction performances. In this paper, we propose an explicit Time embedding based Cascade Attention Network (TCAN) as a novel popularity prediction architecture for large-scale information networks. TCAN integrates temporal attributes (i.e., periodicity, linearity, and non-linear scaling) into node features via a general time embedding approach (TE), and then employs a cascade graph attention encoder (CGAT) and a cascade sequence attention encoder (CSAT) to fully learn the representation of cascade graphs and cascade sequences. We use two real-world dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#25286;&#21368;&#24335;&#36801;&#31227;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#24341;&#20837;&#26799;&#24230;&#30896;&#25758;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#36951;&#24536;&#28304;&#20219;&#21153;&#32780;&#19981;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09971</link><description>&lt;p&gt;
&#21487;&#25286;&#21368;&#24335;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#36873;&#25321;&#24615;&#28304;&#20219;&#21153;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Disposable Transfer Learning for Selective Source Task Unlearning. (arXiv:2308.09971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#25286;&#21368;&#24335;&#36801;&#31227;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#24341;&#20837;&#26799;&#24230;&#30896;&#25758;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#36951;&#24536;&#28304;&#20219;&#21153;&#32780;&#19981;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#34987;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#21363;&#20351;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#21518;&#65292;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#24615;&#33021;&#20173;&#28982;&#20445;&#30041;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#34987;&#35270;&#20026;&#25152;&#26377;&#32773;&#30340;&#31169;&#26377;&#36130;&#20135;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#23547;&#27714;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#24191;&#20041;&#24615;&#33021;&#30340;&#29420;&#21344;&#26435;&#26159;&#21512;&#29702;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#25286;&#21368;&#24335;&#36801;&#31227;&#23398;&#20064;(DTL)&#30340;&#26032;&#33539;&#24335;&#65292;&#23427;&#21482;&#22788;&#29702;&#28304;&#20219;&#21153;&#32780;&#19981;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#30693;&#35782;&#30340;&#36951;&#24536;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#26799;&#24230;&#30896;&#25758;&#25439;&#22833;(GC&#25439;&#22833;)&#12290;GC&#25439;&#22833;&#36890;&#36807;&#24341;&#23548;&#19981;&#21516;mini-batches&#30340;&#26799;&#24230;&#21521;&#37327;&#26397;&#19981;&#21516;&#26041;&#21521;&#21069;&#36827;&#65292;&#36873;&#25321;&#24615;&#22320;&#36951;&#24536;&#28304;&#30693;&#35782;&#12290;&#27169;&#22411;&#26159;&#21542;&#25104;&#21151;&#36951;&#24536;&#28304;&#20219;&#21153;&#36890;&#36807;&#8220;&#32972;&#39534;&#24335;&#23398;&#20064;&#20934;&#30830;&#24615;&#8221;(PL&#20934;&#30830;&#24615;)&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20869;&#24515;&#29420;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65288;IMMO&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#28151;&#21512;&#34701;&#21512;&#21644;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09970</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20869;&#24515;&#29420;&#30333;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Tackling Vision Language Tasks Through Learning Inner Monologues. (arXiv:2308.09970v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09970
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20869;&#24515;&#29420;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65288;IMMO&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#28151;&#21512;&#34701;&#21512;&#21644;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#38656;&#35201;AI&#27169;&#22411;&#23545;&#35270;&#35273;&#21644;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#29702;&#35299;&#21644;&#25512;&#29702;&#12290;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#21147;&#37327;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#65288;1&#65289;LLM&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20043;&#38388;&#30340;&#28151;&#21512;&#34701;&#21512;&#65292;&#20854;&#20013;&#35270;&#35273;&#36755;&#20837;&#39318;&#20808;&#34987;VLM&#36716;&#21270;&#20026;&#35821;&#35328;&#25551;&#36848;&#65292;&#25104;&#20026;LLM&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#30340;&#36755;&#20837;&#65307;&#65288;2&#65289;&#35821;&#35328;&#31354;&#38388;&#20013;&#30340;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#65292;&#20854;&#20013;&#35270;&#35273;&#36755;&#20837;&#34987;&#32534;&#30721;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#30417;&#30563;&#24494;&#35843;&#23558;&#20854;&#25237;&#24433;&#21040;LLM&#30340;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20855;&#26377;&#36731;&#37327;&#32423;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#24456;&#38590;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#29305;&#24449;&#23545;&#40784;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20869;&#24515;&#29420;&#30333;&#22810;&#27169;&#24577;&#20248;&#21270;&#65288;IMMO&#65289;&#65292;&#36890;&#36807;&#27169;&#25311;&#24605;&#32500;&#36807;&#31243;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating in
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36935;&#21040;&#26410;&#30693;&#23545;&#35937;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#23545;&#40784;&#30340;&#30072;&#21464;&#24863;&#30693;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#36890;&#36807;&#20943;&#23567;OoD&#25968;&#25454;&#21644;&#39550;&#39542;&#22330;&#26223;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;OoD&#21512;&#25104;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#8220;&#32473;&#23450;&#31867;&#21035;&#20043;&#22806;&#8221;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#24322;&#24120;&#20998;&#21106;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09965</link><description>&lt;p&gt;
&#22522;&#20110;&#39118;&#26684;&#23545;&#40784;&#30340;&#30072;&#21464;&#24863;&#30693;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation. (arXiv:2308.09965v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09965
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36935;&#21040;&#26410;&#30693;&#23545;&#35937;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#23545;&#40784;&#30340;&#30072;&#21464;&#24863;&#30693;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#36890;&#36807;&#20943;&#23567;OoD&#25968;&#25454;&#21644;&#39550;&#39542;&#22330;&#26223;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;OoD&#21512;&#25104;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#8220;&#32473;&#23450;&#31867;&#21035;&#20043;&#22806;&#8221;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#24322;&#24120;&#20998;&#21106;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#32972;&#26223;&#19979;&#65292;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#36935;&#21040;&#26410;&#30693;&#23545;&#35937;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#23558;&#26631;&#20934;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#37197;&#22791;&#24322;&#24120;&#24863;&#30693;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#21512;&#25104;&#30340;&#31163;&#32676;&#20998;&#24067;&#65288;OoD&#65289;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20943;&#23567;OoD&#25968;&#25454;&#21644;&#39550;&#39542;&#22330;&#26223;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#22521;&#35757;&#36807;&#31243;&#20013;&#21487;&#33021;&#20316;&#20026;&#26126;&#26174;&#25463;&#24452;&#30340;&#39118;&#26684;&#24046;&#24322;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;OoD&#21512;&#25104;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24494;&#35843;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#25928;&#22320;&#23548;&#33268;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#29983;&#25104;&#8220;&#32473;&#23450;&#31867;&#21035;&#20043;&#22806;&#8221;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#21033;&#29992;&#27599;&#20010;&#20687;&#32032;&#30340;OoD&#20998;&#25968;&#36827;&#34892;&#24322;&#24120;&#20998;&#21106;&#12290;&#36890;&#36807;&#26368;&#23567;&#30340;&#24494;&#35843;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#20351;&#24471;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#29992;&#20110;&#24322;&#24120;&#20998;&#21106;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the context of autonomous driving, encountering unknown objects becomes inevitable during deployment in the open world. Therefore, it is crucial to equip standard semantic segmentation models with anomaly awareness. Many previous approaches have utilized synthetic out-of-distribution (OoD) data augmentation to tackle this problem. In this work, we advance the OoD synthesis process by reducing the domain gap between the OoD data and driving scenes, effectively mitigating the style difference that might otherwise act as an obvious shortcut during training. Additionally, we propose a simple fine-tuning loss that effectively induces a pre-trained semantic segmentation model to generate a ``none of the given classes" prediction, leveraging per-pixel OoD scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline enables the use of pre-trained models for anomaly segmentation while maintaining the performance on the original task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#20005;&#37325;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#21457;&#29616;few-shot&#24341;&#23548;&#23545;&#20110;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#20013;&#36716;&#21518;&#65292;&#36825;&#31181;&#24046;&#24322;&#28040;&#22833;&#20102;&#12290;&#22312;WebNLG 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;few-shot + &#32763;&#35793;&#31995;&#32479;&#30340;&#21464;&#20307;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#37117;&#20248;&#20110;&#31454;&#20105;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09957</link><description>&lt;p&gt;
&#20351;&#29992;GPT-3.5&#22312;&#36164;&#28304;&#20005;&#37325;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;:&#38656;&#35201;&#35895;&#27468;&#32763;&#35793;&#30340;&#19968;&#28857;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Data-to-text Generation for Severely Under-Resourced Languages with GPT-3.5: A Bit of Help Needed from Google Translate. (arXiv:2308.09957v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#20005;&#37325;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#21457;&#29616;few-shot&#24341;&#23548;&#23545;&#20110;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#20013;&#36716;&#21518;&#65292;&#36825;&#31181;&#24046;&#24322;&#28040;&#22833;&#20102;&#12290;&#22312;WebNLG 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;few-shot + &#32763;&#35793;&#31995;&#32479;&#30340;&#21464;&#20307;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#37117;&#20248;&#20110;&#31454;&#20105;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#65288;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT&#65289;&#22312;&#22788;&#29702;&#33521;&#35821;&#30456;&#20851;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20013;&#33521;&#35821;&#30340;&#27604;&#20363;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#22914;&#20309;&#24212;&#23545;&#37027;&#20123;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20005;&#37325;&#19981;&#36275;&#30340;&#35821;&#35328;&#65292;&#20855;&#20307;&#21253;&#25324;&#29233;&#23572;&#20848;&#35821;&#12289;&#39532;&#32819;&#20182;&#35821;&#12289;&#23041;&#23572;&#22763;&#35821;&#21644;&#24067;&#37324;&#22810;&#23612;&#35821;&#12290;&#25105;&#20204;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#36827;&#34892;&#20102;&#24341;&#23548;&#24335;&#24037;&#31243;&#38454;&#27573;&#30340;&#27979;&#35797;&#65292;&#20351;&#29992;&#20102;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#36755;&#20837;/&#36755;&#20986;&#23545;&#30340;&#21508;&#31181;&#24341;&#23548;&#31867;&#22411;&#21644;&#26684;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#20004;&#31181;&#22330;&#26223;&#20013;&#23545;&#20004;&#20010;&#26368;&#26377;&#24076;&#26395;&#30340;&#24341;&#23548;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65306;&#65288;i&#65289;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#65288;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65289;&#65292;&#65288;ii&#65289;&#29983;&#25104;&#33521;&#35821;&#21518;&#20877;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#30452;&#25509;&#29983;&#25104;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#26041;&#38754;&#65292;few-shot&#24341;&#23548;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#20013;&#36716;&#21518;&#65292;&#36825;&#31181;&#24046;&#24322;&#28040;&#22833;&#20102;&#12290;&#25105;&#20204;&#23558;few-shot + &#32763;&#35793;&#31995;&#32479;&#30340;&#21464;&#20307;&#25552;&#20132;&#21040;&#20102;WebNLG 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#65292;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#37117;&#27604;&#31454;&#20105;&#31995;&#32479;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs like GPT are great at tasks involving English which dominates in their training data. In this paper, we look at how they cope with tasks involving languages that are severely under-represented in their training data, in the context of data-to-text generation for Irish, Maltese, Welsh and Breton. During the prompt-engineering phase we tested a range of prompt types and formats on GPT-3.5 and~4 with a small sample of example input/output pairs. We then fully evaluated the two most promising prompts in two scenarios: (i) direct generation into the under-resourced language, and (ii) generation into English followed by translation into the under-resourced language. We find that few-shot prompting works better for direct generation into under-resourced languages, but that the difference disappears when pivoting via English. The few-shot + translation system variants were submitted to the WebNLG 2023 shared task where they outperformed competitor systems by substantial margins in all lan
&lt;/p&gt;</description></item><item><title>Eva-KELLM&#26159;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#21644;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#26469;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#25910;&#38598;&#25104;&#26412;&#39640;&#12289;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#22256;&#38590;&#12289;&#35780;&#20272;&#35270;&#35282;&#21463;&#38480;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09954</link><description>&lt;p&gt;
Eva-KELLM&#65306;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs. (arXiv:2308.09954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09954
&lt;/p&gt;
&lt;p&gt;
Eva-KELLM&#26159;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#21644;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#26469;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#25910;&#38598;&#25104;&#26412;&#39640;&#12289;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#22256;&#38590;&#12289;&#35780;&#20272;&#35270;&#35282;&#21463;&#38480;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30528;&#20016;&#23500;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30693;&#35782;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21487;&#33021;&#21464;&#24471;&#36807;&#26102;&#25110;&#19981;&#21512;&#36866;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#24182;&#35780;&#20272;&#20854;&#25928;&#26524;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20107;&#23454;&#19977;&#20803;&#32452;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#65292;&#36825;&#19981;&#20165;&#22312;&#25910;&#38598;&#19978;&#20135;&#29983;&#39640;&#25104;&#26412;&#65292;&#32780;&#19988;&#22312;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#26102;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30740;&#31350;&#22312;&#35780;&#20272;&#35270;&#35282;&#19978;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Eva-KELLM&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#39318;&#20808;&#35201;&#27714;LLM&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#65292;&#19982;&#20351;&#29992;&#20107;&#23454;&#19977;&#20803;&#32452;&#30456;&#27604;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26041;&#20415;&#12289;&#26356;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#20174;&#22810;&#20010;&#35282;&#24230;&#35780;&#20272;&#26356;&#26032;&#21518;&#30340;LLM&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) possess a wealth of knowledge encoded in their parameters. However, this knowledge may become outdated or unsuitable over time. As a result, there has been a growing interest in knowledge editing for LLMs and evaluating its effectiveness. Existing studies primarily focus on knowledge editing using factual triplets, which not only incur high costs for collection but also struggle to express complex facts. Furthermore, these studies are often limited in their evaluation perspectives. In this paper, we propose Eva-KELLM, a new benchmark for evaluating knowledge editing of LLMs. This benchmark includes an evaluation framework and a corresponding dataset. Under our framework, we first ask the LLM to perform knowledge editing using raw documents, which provides a more convenient and universal approach compared to using factual triplets. We then evaluate the updated LLM from multiple perspectives. In addition to assessing the effectiveness of knowledge editing and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#23637;&#31034;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#25552;&#21319;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#23558;&#20854;&#27604;&#21947;&#20026;&#19968;&#31181;&#20725;&#30828;&#24863;&#30693;&#27493;&#38271;&#36866;&#37197;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.09939</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#21147;&#31995;&#32479;&#35282;&#24230;&#29702;&#35299;&#33258;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Understanding Self-attention Mechanism via Dynamical System Perspective. (arXiv:2308.09939v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#23637;&#31034;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#25552;&#21319;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#23558;&#20854;&#27604;&#21947;&#20026;&#19968;&#31181;&#20725;&#30828;&#24863;&#30693;&#27493;&#38271;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#65288;SAM&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#25552;&#21319;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#35813;&#26426;&#21046;&#30340;&#35299;&#37322;&#20027;&#35201;&#22522;&#20110;&#30452;&#35273;&#21644;&#32463;&#39564;&#65292;&#32780;&#32570;&#20047;&#23545;SAM&#22914;&#20309;&#24110;&#21161;&#24615;&#33021;&#30340;&#30452;&#25509;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22522;&#20110;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#21147;&#31995;&#32479;&#35270;&#35282;&#65292;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#30340;&#39640;&#31934;&#24230;&#35299;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20869;&#22312;&#20725;&#30828;&#29616;&#35937;&#65288;SP&#65289;&#20063;&#23384;&#22312;&#20110;&#39640;&#24615;&#33021;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20013;&#12290;&#22240;&#27492;&#65292;NN&#22312;&#29305;&#24449;&#23618;&#38754;&#19978;&#27979;&#37327;SP&#30340;&#33021;&#21147;&#23545;&#20110;&#33719;&#24471;&#39640;&#24615;&#33021;&#26159;&#24517;&#35201;&#30340;&#65292;&#24182;&#19988;&#26159;&#35757;&#32451;NN&#22256;&#38590;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#31867;&#20284;&#20110;&#35299;&#20915;&#20725;&#30828;ODE&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SAM&#20063;&#26159;&#19968;&#31181;&#33021;&#22815;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#30340;&#20725;&#30828;&#24863;&#30693;&#27493;&#38271;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention mechanism (SAM) is widely used in various fields of artificial intelligence and has successfully boosted the performance of different models. However, current explanations of this mechanism are mainly based on intuitions and experiences, while there still lacks direct modeling for how the SAM helps performance. To mitigate this issue, in this paper, based on the dynamical system perspective of the residual neural network, we first show that the intrinsic stiffness phenomenon (SP) in the high-precision solution of ordinary differential equations (ODEs) also widely exists in high-performance neural networks (NN). Thus the ability of NN to measure SP at the feature level is necessary to obtain high performance and is an important factor in the difficulty of training NN. Similar to the adaptive step-size method which is effective in solving stiff ODEs, we show that the SAM is also a stiffness-aware step size adaptor that can enhance the model's representational ability t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;East&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23433;&#20840;Transformer&#25512;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#24536;&#21364;&#20998;&#27573;&#22810;&#39033;&#24335;&#27714;&#20540;&#31639;&#27861;&#26469;&#20248;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#36890;&#20449;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#23433;&#20840;&#21327;&#35758;&#26469;&#22788;&#29702;softmax&#21644;&#23618;&#24402;&#19968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.09923</link><description>&lt;p&gt;
East: &#39640;&#25928;&#20934;&#30830;&#30340;&#23433;&#20840;Transformer&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
East: Efficient and Accurate Secure Transformer Framework for Inference. (arXiv:2308.09923v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09923
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;East&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23433;&#20840;Transformer&#25512;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#24536;&#21364;&#20998;&#27573;&#22810;&#39033;&#24335;&#27714;&#20540;&#31639;&#27861;&#26469;&#20248;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#36890;&#20449;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#23433;&#20840;&#21327;&#35758;&#26469;&#22788;&#29702;softmax&#21644;&#23618;&#24402;&#19968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;ChatGPT&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#26381;&#21153;&#36807;&#31243;&#20013;&#65292;&#29992;&#25143;&#30340;&#36755;&#20837;&#20250;&#27844;&#28431;&#32473;&#27169;&#22411;&#25552;&#20379;&#21830;&#12290;&#38543;&#30528;&#20154;&#20204;&#23545;&#38544;&#31169;&#30340;&#20851;&#27880;&#65292;&#38544;&#31169;&#20445;&#25252;&#30340;Transformer&#25512;&#29702;&#22312;&#36825;&#31867;&#26381;&#21153;&#20013;&#38656;&#27714;&#37327;&#22823;&#12290;&#23545;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;Transformer&#25512;&#29702;&#26469;&#35828;&#65292;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#23433;&#20840;&#21327;&#35758;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30740;&#31350;&#24471;&#19981;&#22810;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#23454;&#29992;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#23433;&#20840;&#21327;&#35758;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#32780;&#35328;&#24456;&#22256;&#38590;&#20294;&#24456;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\emph{East}&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23433;&#20840;Transformer&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24536;&#21364;&#20998;&#27573;&#22810;&#39033;&#24335;&#27714;&#20540;&#31639;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#28608;&#27963;&#20989;&#25968;&#65292;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#23558;GELU&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#36890;&#20449;&#37327;&#20998;&#21035;&#20943;&#23569;&#20102;1.5&#20493;&#21644;2.5&#20493;&#20197;&#19978;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#29992;&#20110;softmax&#21644;&#23618;&#24402;&#19968;&#21270;&#30340;&#23433;&#20840;&#21327;&#35758;&#65292;&#20197;&#24544;&#23454;&#22320;&#20445;&#25345;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer has been successfully used in practical applications, such as ChatGPT, due to its powerful advantages. However, users' input is leaked to the model provider during the service. With people's attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework \emph{East} to enable efficient and accurate secure Transformer inference. Firstly, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5$\times$ and 2.5$\times$, compared to prior arts. Secondly, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Deepfake&#26816;&#27979;&#27169;&#22411;Recap&#65292;&#36890;&#36807;&#24674;&#22797;&#38754;&#37096;&#24182;&#26144;&#23556;&#24674;&#22797;&#30340;&#38754;&#37096;&#26292;&#38706;&#20102;&#38750;&#29305;&#23450;&#30340;&#38754;&#37096;&#19981;&#19968;&#33268;&#24615;&#65292;&#25193;&#22823;&#20102;&#30495;&#23454;&#21644;&#20266;&#36896;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.09921</link><description>&lt;p&gt;
&#32047;&#35745;: &#36890;&#36807;&#24674;&#22797;&#38754;&#37096;&#24182;&#26144;&#23556;&#24674;&#22797;&#30340;&#38754;&#37096;&#26816;&#27979;&#19981;&#21487;&#39044;&#27979;&#31713;&#25913;&#30165;&#36857;&#30340;Deepfake&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Recap: Detecting Deepfake Video with Unpredictable Tampered Traces via Recovering Faces and Mapping Recovered Faces. (arXiv:2308.09921v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09921
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Deepfake&#26816;&#27979;&#27169;&#22411;Recap&#65292;&#36890;&#36807;&#24674;&#22797;&#38754;&#37096;&#24182;&#26144;&#23556;&#24674;&#22797;&#30340;&#38754;&#37096;&#26292;&#38706;&#20102;&#38750;&#29305;&#23450;&#30340;&#38754;&#37096;&#19981;&#19968;&#33268;&#24615;&#65292;&#25193;&#22823;&#20102;&#30495;&#23454;&#21644;&#20266;&#36896;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#20351;&#29992;Deepfake&#25216;&#26415;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;Deepfake&#26816;&#27979;&#30340;&#37325;&#35201;&#30740;&#31350;&#20852;&#36259;&#12290;Deepfake&#25805;&#32437;&#32463;&#24120;&#24341;&#20837;&#38543;&#26426;&#31713;&#25913;&#30165;&#36857;&#65292;&#22312;&#19981;&#21516;&#30340;&#38754;&#37096;&#21306;&#22495;&#20135;&#29983;&#19981;&#21487;&#39044;&#27979;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#20266;&#36896;&#25351;&#26631;&#65292;&#38543;&#30528;&#20266;&#36896;&#27169;&#24335;&#30340;&#25913;&#36827;&#65292;&#36825;&#20123;&#30165;&#36857;&#21464;&#24471;&#36234;&#26469;&#36234;&#38543;&#26426;&#21270;&#65292;&#23548;&#33268;&#20381;&#36182;&#20110;&#29305;&#23450;&#20266;&#36896;&#30165;&#36857;&#30340;&#26041;&#27861;&#30340;&#26816;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Deepfake&#26816;&#27979;&#27169;&#22411;Recap&#65292;&#36890;&#36807;&#24674;&#22797;&#38754;&#37096;&#21644;&#26144;&#23556;&#24674;&#22797;&#30340;&#38754;&#37096;&#65292;&#26292;&#38706;&#20102;&#38750;&#29305;&#23450;&#30340;&#38754;&#37096;&#19981;&#19968;&#33268;&#24615;&#65292;&#25193;&#22823;&#20102;&#30495;&#23454;&#21644;&#20266;&#36896;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#24674;&#22797;&#38454;&#27573;&#65292;&#27169;&#22411;&#19987;&#27880;&#20110;&#38543;&#26426;&#36974;&#30422;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#24182;&#22312;&#27809;&#26377;&#19981;&#21487;&#39044;&#27979;&#30340;&#31713;&#25913;&#30165;&#36857;&#30340;&#24773;&#20917;&#19979;&#37325;&#24314;&#30495;&#23454;&#30340;&#38754;&#37096;&#65292;&#20174;&#32780;&#20026;&#30495;&#23454;&#30340;&#38754;&#37096;&#20135;&#29983;&#30456;&#23545;&#36739;&#22909;&#30340;&#24674;&#22797;&#25928;&#26524;&#65292;&#32780;&#20026;&#20266;&#36896;&#30340;&#38754;&#37096;&#20135;&#29983;&#36739;&#24046;&#30340;&#24674;&#22797;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploitation of Deepfake techniques for malicious intentions has driven significant research interest in Deepfake detection. Deepfake manipulations frequently introduce random tampered traces, leading to unpredictable outcomes in different facial regions. However, existing detection methods heavily rely on specific forgery indicators, and as the forgery mode improves, these traces become increasingly randomized, resulting in a decline in the detection performance of methods reliant on specific forgery traces. To address the limitation, we propose Recap, a novel Deepfake detection model that exposes unspecific facial part inconsistencies by recovering faces and enlarges the differences between real and fake by mapping recovered faces. In the recovering stage, the model focuses on randomly masking regions of interest (ROIs) and reconstructing real faces without unpredictable tampered traces, resulting in a relatively good recovery effect for real faces while a poor recovery effect fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#35270;&#35273;&#34920;&#31034;&#25429;&#33719;&#30005;&#23376;&#26174;&#24494;&#38236;&#23454;&#20363;&#20998;&#21106;&#20013;&#20307;&#32032;&#32423;&#21644;&#29305;&#24449;&#32423;&#19968;&#33268;&#24615;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.09917</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#23610;&#24230;&#19968;&#33268;&#24615;&#30340;&#33258;&#30417;&#30563;&#30005;&#23376;&#26174;&#24494;&#38236;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation. (arXiv:2308.09917v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#35270;&#35273;&#34920;&#31034;&#25429;&#33719;&#30005;&#23376;&#26174;&#24494;&#38236;&#23454;&#20363;&#20998;&#21106;&#20013;&#20307;&#32032;&#32423;&#21644;&#29305;&#24449;&#32423;&#19968;&#33268;&#24615;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23454;&#20363;&#30340;&#22797;&#26434;&#24418;&#24577;&#21644;&#19981;&#20805;&#36275;&#30340;&#27880;&#37322;&#65292;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;EM&#65289;&#20307;&#31215;&#20013;&#30340;&#23454;&#20363;&#20998;&#21106;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#26368;&#36817;&#20986;&#29616;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#33719;&#21462;&#23545;&#20110;EM&#23454;&#20363;&#20998;&#21106;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#32990;&#32452;&#32455;&#32467;&#26500;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#25429;&#25417;&#22797;&#26434;&#30340;&#35270;&#35273;&#27169;&#24335;&#21644;&#20307;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25152;&#33719;&#21462;&#30340;&#20808;&#39564;&#30693;&#35782;&#19981;&#36275;&#20197;&#24212;&#29992;&#20110;&#19979;&#28216;&#30340;EM&#20998;&#26512;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#30340;&#35270;&#35273;&#34920;&#31034;&#26469;&#25429;&#25417;EM&#20307;&#31215;&#20013;&#30340;&#20307;&#32032;&#32423;&#21644;&#29305;&#24449;&#32423;&#30340;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#37325;&#26500;&#20989;&#25968;&#24378;&#21046;&#23454;&#26045;Siamese&#32593;&#32476;&#36755;&#20986;&#20043;&#38388;&#30340;&#20307;&#32032;&#32423;&#19968;&#33268;&#24615;&#65292;&#24182;&#32467;&#21512;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#36719;&#29305;&#24449;&#21305;&#37197;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#29305;&#24449;&#32423;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation in electron microscopy (EM) volumes poses a significant challenge due to the complex morphology of instances and insufficient annotations. Self-supervised learning has recently emerged as a promising solution, enabling the acquisition of prior knowledge of cellular tissue structures that are essential for EM instance segmentation. However, existing pretraining methods often lack the ability to capture complex visual patterns and relationships between voxels, which results in the acquired prior knowledge being insufficient for downstream EM analysis tasks. In this paper, we propose a novel pretraining framework that leverages multiscale visual representations to capture both voxel-level and feature-level consistency in EM volumes. Specifically, our framework enforces voxel-level consistency between the outputs of a Siamese network by a reconstruction function, and incorporates a cross-attention mechanism for soft feature matching to achieve fine-grained feature-lev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22870;&#21169;&#32553;&#25918;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22238;&#35775;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#35774;&#32622;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09909</link><description>&lt;p&gt;
&#19981;&#20877;&#37325;&#22797;&#25506;&#32034;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Never Explore Repeatedly in Multi-Agent Reinforcement Learning. (arXiv:2308.09909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09909
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22870;&#21169;&#32553;&#25918;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22238;&#35775;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#35774;&#32622;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#20869;&#22312;&#21160;&#26426;&#24050;&#32463;&#25104;&#20026;&#25506;&#32034;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23613;&#31649;&#35745;&#31639;&#35768;&#22810;&#20869;&#22312;&#22870;&#21169;&#20381;&#36182;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#22120;&#20272;&#35745;&#21464;&#20998;&#21518;&#39564;&#65292;&#20294;&#30001;&#20110;&#36825;&#20123;&#31070;&#32463;&#32479;&#35745;&#36924;&#36817;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25361;&#25112;&#23450;&#20026;"&#22238;&#35775;"&#38382;&#39064;&#65292;&#21363;&#26234;&#33021;&#20307;&#21453;&#22797;&#25506;&#32034;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#26377;&#38480;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22870;&#21169;&#32553;&#25918;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#31283;&#23450;&#20808;&#21069;&#25506;&#32034;&#21306;&#22495;&#20869;&#30340;&#20869;&#22312;&#22870;&#21169;&#30340;&#26126;&#26174;&#27874;&#21160;&#65292;&#24182;&#20419;&#36827;&#26356;&#24191;&#27867;&#30340;&#25506;&#32034;&#65292;&#26377;&#25928;&#22320;&#25233;&#21046;&#22238;&#35775;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#24378;&#35843;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20687;Google Research Football&#21644;StarCraft II&#24494;&#25805;&#20316;&#20219;&#21153;&#36825;&#26679;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#35774;&#32622;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the "revisitation" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09908</link><description>&lt;p&gt;
LEGO: &#23545;&#20110;&#22522;&#20110;&#28857;&#20113;&#30340;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds. (arXiv:2308.09908v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#65292;&#25968;&#25454;&#20851;&#32852;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#65288;LEGO&#65289;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;LEGO&#36319;&#36394;&#22120;&#38598;&#25104;&#20102;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21046;&#23450;&#20851;&#32852;&#35780;&#20998;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#39640;&#25928;&#30340;&#30446;&#26631;&#21305;&#37197;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#29366;&#24577;&#26356;&#26032;&#36807;&#31243;&#65292;&#26412;&#25991;&#36824;&#28155;&#21152;&#20102;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#23558;&#23545;&#35937;&#29366;&#24577;&#30340;&#26102;&#38388;&#36830;&#36143;&#24615;&#32435;&#20837;&#36319;&#36394;&#20013;&#65292;&#30830;&#20445;&#19968;&#33268;&#30340;&#36319;&#36394;&#12290;&#19982;&#20854;&#20182;&#22312;&#32447;&#36319;&#36394;&#26041;&#27861;&#65288;&#21253;&#25324;&#22522;&#20110;LiDAR&#21644;&#22522;&#20110;LiDAR-&#30456;&#26426;&#34701;&#21512;&#30340;&#26041;&#27861;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20165;&#21033;&#29992;LiDAR&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#25552;&#20132;&#32467;&#26524;&#33267;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#25490;&#34892;&#27036;&#26102;&#65292;LEGO&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating the accurate and efficient matching of objects across time frames. To further enhance the state update process, the Kalman filter is added to ensure consistent tracking by incorporating temporal coherence in the object states. Our proposed method utilizing LiDAR alone has shown exceptional performance compared to other online tracking approaches, including LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at the time of submitting results to KITTI object tracking evaluation ranki
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;RAH&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21161;&#25163;&#65292;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20010;&#24615;&#21270;&#21453;&#39304;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09904</link><description>&lt;p&gt;
RAH&#65281;RecSys-Assistant-Human&#65306;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#20013;&#24515;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RAH! RecSys-Assistant-Human: A Human-Central Recommendation Framework with Large Language Models. (arXiv:2308.09904v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;RAH&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21161;&#25163;&#65292;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20010;&#24615;&#21270;&#21453;&#39304;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#29983;&#24577;&#31995;&#32479;&#28041;&#21450;&#21040;&#25512;&#33616;&#31995;&#32479;&#65288;&#35745;&#31639;&#26426;&#65289;&#21644;&#29992;&#25143;&#65288;&#20154;&#31867;&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#35282;&#24230;&#19981;&#21516;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#29992;&#25143;&#30340;&#35282;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#21152;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;RAH&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12289;&#21161;&#25163;&#21644;&#20154;&#31867;&#12290;&#21161;&#25163;&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#20010;&#20154;&#20195;&#29702;&#65292;&#29992;&#20110;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#21161;&#25163;&#25198;&#28436;&#38750;&#20405;&#20837;&#24615;&#30340;&#35282;&#33394;&#65292;RAH&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#29992;&#25143;&#32676;&#20307;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#35780;&#20272;&#20102;RAH&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#20195;&#29702;&#20154;&#31867;&#21453;&#39304;&#12290;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#20351;&#29992;&#23398;&#20064;-&#34892;&#21160;-&#35780;&#35770;&#23478;&#21644;&#21453;&#24605;&#26426;&#21046;&#21487;&#20197;&#23548;&#33268;&#26356;&#21152;&#19968;&#33268;&#30340;&#20010;&#24615;&#65292;&#65288;2&#65289;&#25105;&#20204;&#30340;&#21161;&#25163;&#21487;&#20197;&#26377;&#25928;&#22320;&#20195;&#29702;&#20154;&#31867;&#21453;&#39304;&#24182;&#24110;&#21161;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;RAH&#26694;&#26550;&#20013;&#36827;&#19968;&#27493;&#35299;&#20915;&#20154;&#31867;&#20013;&#24515;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#29992;&#25143;``&#22842;&#26435;''&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation ecosystem involves interactions between recommender systems(Computer) and users(Human). Orthogonal to the perspective of recommender systems, we attempt to utilize LLMs from the perspective of users and propose a more human-central recommendation framework named RAH, which consists of Recommender system, Assistant and Human. The assistant is a LLM-based and personal proxy for a human to achieve user satisfaction. The assistant plays a non-invasion role and the RAH framework can adapt to different recommender systems and user groups. Subsequently, we implement and evaluate the RAH framework for learning user personalities and proxy human feedback. The experiment shows that (1) using learn-action-critic and reflection mechanisms can lead more aligned personality and (2) our assistant can effectively proxy human feedback and help adjust recommender systems. Finally, we discuss further strategies in the RAH framework to address human-central concerns including user contr
&lt;/p&gt;</description></item><item><title>SwinLSTM&#26159;&#19968;&#31181;&#23558;Swin Transformer&#21644;LSTM&#32467;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26032;&#24490;&#29615;&#21333;&#20803;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2308.09891</link><description>&lt;p&gt;
SwinLSTM&#65306;&#20351;&#29992;Swin Transformer&#21644;LSTM&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM. (arXiv:2308.09891v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09891
&lt;/p&gt;
&lt;p&gt;
SwinLSTM&#26159;&#19968;&#31181;&#23558;Swin Transformer&#21644;LSTM&#32467;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26032;&#24490;&#29615;&#21333;&#20803;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;CNN&#21644;RNN&#38598;&#25104;&#20197;&#25429;&#25417;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26159;&#26102;&#31354;&#39044;&#27979;&#20219;&#21153;&#20013;&#24120;&#29992;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;CNN&#23398;&#20064;&#23616;&#37096;&#31354;&#38388;&#20449;&#24687;&#30340;&#23646;&#24615;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#25429;&#25417;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24490;&#29615;&#21333;&#20803;SwinLSTM&#65292;&#23427;&#23558;Swin Transformer&#22359;&#21644;&#31616;&#21270;&#30340;LSTM&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#23558;ConvLSTM&#20013;&#30340;&#21367;&#31215;&#32467;&#26500;&#26367;&#25442;&#20026;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20197;SwinLSTM&#21333;&#20803;&#20026;&#26680;&#24515;&#30340;&#26102;&#31354;&#39044;&#27979;&#32593;&#32476;&#12290;SwinLSTM&#22312;Moving MNIST&#65292;Human3.6m&#65292;TaxiBJ&#21644;KTH&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#31454;&#20105;&#24615;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#20840;&#23616;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#23545;&#20110;&#27169;&#22411;&#25429;&#25417;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26356;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating CNNs and RNNs to capture spatiotemporal dependencies is a prevalent strategy for spatiotemporal prediction tasks. However, the property of CNNs to learn local spatial information decreases their efficiency in capturing spatiotemporal dependencies, thereby limiting their prediction accuracy. In this paper, we propose a new recurrent cell, SwinLSTM, which integrates Swin Transformer blocks and the simplified LSTM, an extension that replaces the convolutional structure in ConvLSTM with the self-attention mechanism. Furthermore, we construct a network with SwinLSTM cell as the core for spatiotemporal prediction. Without using unique tricks, SwinLSTM outperforms state-of-the-art methods on Moving MNIST, Human3.6m, TaxiBJ, and KTH datasets. In particular, it exhibits a significant improvement in prediction accuracy compared to ConvLSTM. Our competitive experimental results demonstrate that learning global spatial dependencies is more advantageous for models to capture spatiotempo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#65288;IBL&#65289;&#65292;&#23427;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#20195;&#30721;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#21040;&#25552;&#31034;&#20013;&#65292;&#36755;&#20986;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2308.09890</link><description>&lt;p&gt;
&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;: &#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Inductive-bias Learning: Generating Code Models with Large Language Model. (arXiv:2308.09890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09890
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#65288;IBL&#65289;&#65292;&#23427;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#20195;&#30721;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#21040;&#25552;&#31034;&#20013;&#65292;&#36755;&#20986;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22240;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26041;&#38754;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;ICL&#25216;&#26415;&#22312;&#19981;&#26356;&#26032;LLM&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#21040;&#25552;&#31034;&#20013;&#21363;&#21487;&#23454;&#29616;&#22522;&#20110;&#35268;&#21017;&#30340;&#39640;&#20934;&#30830;&#24615;&#25512;&#29702;&#12290;&#34429;&#28982;ICL&#26159;&#19968;&#20010;&#21457;&#23637;&#20013;&#30340;&#39046;&#22495;&#65292;&#36824;&#26377;&#35768;&#22810;&#26410;&#35299;&#31572;&#30340;&#38382;&#39064;&#65292;&#20294;LLMs&#26412;&#36523;&#20316;&#20026;&#25512;&#29702;&#27169;&#22411;&#20284;&#20046;&#23454;&#29616;&#20102;&#19981;&#38656;&#35201;&#26126;&#30830;&#25351;&#20986;"&#24402;&#32435;&#20559;&#24046;"&#30340;&#25512;&#29702;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20195;&#30721;&#29983;&#25104;&#20063;&#26159;LLMs&#30340;&#19968;&#39033;&#37325;&#35201;&#24212;&#29992;&#12290;&#20195;&#30721;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#22823;&#22823;&#25552;&#39640;&#65292;&#20351;&#24471;&#21363;&#20351;&#38750;&#24037;&#31243;&#24072;&#20063;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#29983;&#25104;&#25191;&#34892;&#25152;&#38656;&#20219;&#21153;&#30340;&#20195;&#30721;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#23398;&#20064;&#8221;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#65288;IBL&#65289;&#8221;&#65292;&#23427;&#32467;&#21512;&#20102;ICL&#21644;&#20195;&#30721;&#29983;&#25104;&#30340;&#25216;&#26415;&#12290;IBL&#30340;&#24605;&#24819;&#24456;&#30452;&#35266;&#12290;&#19982;ICL&#31867;&#20284;&#65292;IBL&#23558;&#35757;&#32451;&#25968;&#25454;&#36755;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#24182;&#36755;&#20986;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models(LLMs) have been attracting attention due to a ability called in-context learning(ICL). ICL, without updating the parameters of a LLM, it is possible to achieve highly accurate inference based on rules ``in the context'' by merely inputting a training data into the prompt. Although ICL is a developing field with many unanswered questions, LLMs themselves serves as a inference model, seemingly realizing inference without explicitly indicate ``inductive bias''. On the other hand, a code generation is also a highlighted application of LLMs. The accuracy of code generation has dramatically improved, enabling even non-engineers to generate code to perform the desired tasks by crafting appropriate prompts. In this paper, we propose a novel ``learning'' method called an ``Inductive-Bias Learning (IBL)'', which combines the techniques of ICL and code generation. An idea of IBL is straightforward. Like ICL, IBL inputs a training data into the prompt and outputs a code with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#21644;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#25913;&#36827;&#20102;&#20248;&#21270;&#21644;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36824;&#25193;&#23637;&#20102;&#26694;&#26550;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.09858</link><description>&lt;p&gt;
&#24352;&#37327;&#21387;&#32553;&#30340;&#21453;&#21521;&#20256;&#25773;&#20813;&#36153;&#35757;&#32451;&#65288;&#29289;&#29702;&#20449;&#24687;&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks. (arXiv:2308.09858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#21644;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#25913;&#36827;&#20102;&#20248;&#21270;&#21644;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36824;&#25193;&#23637;&#20102;&#26694;&#26550;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#35745;&#31639;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#30828;&#20214;&#21644;&#36719;&#20214;&#36164;&#28304;&#26469;&#25903;&#25345;&#33258;&#21160;&#24494;&#20998;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;BP&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#22823;&#22823;&#22686;&#21152;&#20102;&#35774;&#22791;&#19978;&#35757;&#32451;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#19978;&#24066;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;BP&#30340;&#26694;&#26550;&#65292;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#25773;&#23601;&#21487;&#20197;&#35757;&#32451;&#23454;&#38469;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#38646;&#38454;&#65288;ZO&#65289;&#20248;&#21270;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#22823;&#20110;&#20197;&#21069;ZO&#26041;&#27861;&#33021;&#21147;&#30340;&#32593;&#32476;&#23610;&#23544;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;ZO&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31232;&#30095;&#26684;&#26041;&#27861;&#26469;&#25193;&#23637;&#25105;&#20204;&#30340;BP-free&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;epsilon-ProVe&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36817;&#20284;&#30340;&#26041;&#27861;&#26469;&#26522;&#20030;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23433;&#20840;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.09842</link><description>&lt;p&gt;
&#29992;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26522;&#20030;&#23433;&#20840;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees. (arXiv:2308.09842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09842
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;epsilon-ProVe&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36817;&#20284;&#30340;&#26041;&#27861;&#26469;&#26522;&#20030;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23433;&#20840;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#23433;&#20840;&#21306;&#22495;&#26159;&#20445;&#35777;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#31995;&#32479;&#30340;&#20449;&#20219;&#30340;&#20851;&#38190;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AllDNN-Verification&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#23433;&#20840;&#23646;&#24615;&#21644;&#19968;&#20010;DNN&#65292;&#26522;&#20030;&#23646;&#24615;&#36755;&#20837;&#22495;&#30340;&#25152;&#26377;&#23433;&#20840;&#21306;&#22495;&#65292;&#21363;&#23646;&#24615;&#25104;&#31435;&#30340;&#21306;&#22495;&#12290;&#30001;&#20110;&#38382;&#39064;&#30340;#P&#38590;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#21483;&#20570;epsilon-ProVe&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32479;&#35745;&#39044;&#27979;&#23481;&#38480;&#38480;&#21046;&#33719;&#24471;&#21487;&#25511;&#20302;&#20272;&#30340;&#36755;&#20986;&#21487;&#36798;&#38598;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#23433;&#20840;&#21306;&#22495;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#20026;&#36825;&#31181;&#26032;&#22411;&#30340;DNN&#39564;&#35777;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21327;&#21516;&#26041;&#27861;&#20114;&#34917;&#21508;&#33258;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#21644;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09830</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#30340;&#21327;&#21516;&#38598;&#25104;&#23545;&#20110;&#31283;&#20581;&#20154;&#24037;&#26234;&#33021;&#30340;&#25506;&#32034;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis. (arXiv:2308.09830v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21327;&#21516;&#26041;&#27861;&#20114;&#34917;&#21508;&#33258;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#21644;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#26500;&#24314;&#34920;&#29616;&#20986;&#26234;&#33021;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26102;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#35748;&#30693;&#26550;&#26500;(CAs) &#36827;&#34892;&#38598;&#25104;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#29702;&#35770;&#27169;&#22411;&#30340;&#25351;&#23548;&#19979;&#65292;&#36890;&#36807;&#21021;&#27493;&#30340;&#32463;&#39564;&#25968;&#25454;&#25903;&#25345;&#65292;&#25105;&#20204;&#20551;&#35774;&#19981;&#21516;&#30340;&#21327;&#21516;&#26041;&#27861;&#21487;&#20197;&#20114;&#34917;&#23427;&#20204;&#21508;&#33258;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#20174;&#32780;&#22521;&#32946;&#20986;&#26356;&#31283;&#20581;&#21644;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#25152;&#28041;&#21450;&#30340;&#26435;&#34913;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores alternatives for integrating two subdisciplines of AI in the construction of artificial agents that exhibit intelligent behavior: Large Language Models (LLMs) and Cognitive Architectures (CAs). Guided by theoretical models and supported by preliminary empirical data, we hypothesize how diverse synergistic approaches can mutually compensate for their respective weaknesses and limitations, ultimately fostering more robust and sophisticated artificial intelligence systems. Additionally, we discuss the tradeoffs and challenges associated with each approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OASIS&#65292;&#19968;&#20010;&#29992;&#20110;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#21464;&#24418;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#20195;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24694;&#24847;&#20869;&#23481;&#30340;&#20256;&#25773;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09810</link><description>&lt;p&gt;
&#19968;&#24133;&#22270;&#20687;&#32988;&#36807;&#21315;&#35328;&#19975;&#35821;&#65306;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#21464;&#24418;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth a Thousand Toxic Words: A Metamorphic Testing Framework for Content Moderation Software. (arXiv:2308.09810v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OASIS&#65292;&#19968;&#20010;&#29992;&#20110;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#21464;&#24418;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#20195;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24694;&#24847;&#20869;&#23481;&#30340;&#20256;&#25773;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#25351;&#25968;&#22686;&#38271;&#20026;&#20154;&#31867;&#31038;&#20250;&#30340;&#27807;&#36890;&#21644;&#20869;&#23481;&#20256;&#25773;&#24102;&#26469;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24179;&#21488;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#28389;&#29992;&#26469;&#20256;&#25773;&#26377;&#23475;&#20869;&#23481;&#65292;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#12289;&#24694;&#24847;&#24191;&#21578;&#21644;&#33394;&#24773;&#20869;&#23481;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#36127;&#38754;&#21518;&#26524;&#65292;&#22914;&#23545;&#38738;&#23569;&#24180;&#24515;&#29702;&#20581;&#24247;&#30340;&#20260;&#23475;&#12290;&#23613;&#31649;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#23457;&#26680;&#26041;&#27861;&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#20294;&#24694;&#24847;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23558;&#25991;&#23383;&#23884;&#20837;&#22270;&#20687;&#20013;&#26469;&#35268;&#36991;&#23457;&#26680;&#65292;&#20363;&#22914;&#25991;&#23383;&#30340;&#25130;&#22270;&#65292;&#36890;&#24120;&#24102;&#26377;&#19968;&#20123;&#24178;&#25200;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#20195;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#23545;&#27492;&#31867;&#24694;&#24847;&#36755;&#20837;&#30340;&#24615;&#33021;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OASIS&#65292;&#19968;&#20010;&#29992;&#20110;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#21464;&#24418;&#27979;&#35797;&#26694;&#26550;&#12290;OASIS&#37319;&#29992;&#20102;21&#20010;&#36716;&#25442;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#26159;&#20174;&#25105;&#20204;&#23545;&#26469;&#33258;4&#20010;&#27969;&#34892;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#65288;&#21253;&#25324;Twitter&#12289;Instagram&#65289;&#25910;&#38598;&#30340;5000&#20010;&#30495;&#23454;&#26377;&#23475;&#20869;&#23481;&#30340;&#35797;&#39564;&#30740;&#31350;&#24635;&#32467;&#20986;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media platforms has brought about a revolution in communication and content dissemination in human society. Nevertheless, these platforms are being increasingly misused to spread toxic content, including hate speech, malicious advertising, and pornography, leading to severe negative consequences such as harm to teenagers' mental health. Despite tremendous efforts in developing and deploying textual and image content moderation methods, malicious users can evade moderation by embedding texts into images, such as screenshots of the text, usually with some interference. We find that modern content moderation software's performance against such malicious inputs remains underexplored. In this work, we propose OASIS, a metamorphic testing framework for content moderation software. OASIS employs 21 transform rules summarized from our pilot study on 5,000 real-world toxic contents collected from 4 popular social media applications, including Twitter, Instagram,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VL-PET&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#31890;&#24230;&#25511;&#21046;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#20462;&#25913;&#36827;&#34892;&#26377;&#25928;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#22312;&#24615;&#33021;&#21644;&#21151;&#33021;&#24046;&#36317;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.09804</link><description>&lt;p&gt;
VL-PET&#65306;&#36890;&#36807;&#31890;&#24230;&#25511;&#21046;&#23454;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control. (arXiv:2308.09804v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VL-PET&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#31890;&#24230;&#25511;&#21046;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#20462;&#25913;&#36827;&#34892;&#26377;&#25928;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#22312;&#24615;&#33021;&#21644;&#21151;&#33021;&#24046;&#36317;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#27169;&#22411;&#35268;&#27169;&#36805;&#36895;&#22686;&#38271;&#65292;&#20840;&#38754;&#24494;&#35843;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#23384;&#20648;&#26041;&#38754;&#21464;&#24471;&#20195;&#20215;&#39640;&#26114;&#12290;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#65288;VL&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#25216;&#26415;&#65292;&#23558;&#27169;&#22359;&#21270;&#20462;&#25913;&#65288;&#20363;&#22914;Adapter&#21644;LoRA&#65289;&#38598;&#25104;&#21040;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;PLMs&#20013;&#12290;&#36890;&#36807;&#35843;&#25972;&#19968;&#23567;&#32452;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#24615;&#33021;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#30340;&#27169;&#22359;&#21270;&#20462;&#25913;&#21644;&#24573;&#35270;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#21151;&#33021;&#24046;&#36317;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#32780;&#29616;&#26377;&#30340;PET&#25216;&#26415;&#65288;&#20363;&#22914;VL-Adapter&#65289;&#24573;&#35270;&#20102;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-and-Language Parameter-Efficient Tuning&#65288;VL-PET&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31890;&#24230;&#25511;&#21046;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#20462;&#25913;&#36827;&#34892;&#26377;&#25928;&#25511;&#21046;&#12290;&#36890;&#36807;&#32771;&#34385;&#30001;&#36825;&#31181;&#26426;&#21046;&#29983;&#25104;&#30340;&#19981;&#21516;&#31890;&#24230;&#25511;&#21046;&#30697;&#38453;&#65292;&#21487;&#20197;&#23454;&#20363;&#21270;&#22810;&#31181;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;VL-PET&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the model size of pre-trained language models (PLMs) grows rapidly, full fine-tuning becomes prohibitively expensive for model training and storage. In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques perform on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated fr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#65292;&#36890;&#36807;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#20449;&#24687;&#26356;&#26032;&#26469;&#25429;&#25417;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09780</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction. (arXiv:2308.09780v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#65292;&#36890;&#36807;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#20449;&#24687;&#26356;&#26032;&#26469;&#25429;&#25417;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20844;&#21496;&#22312;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#23558;&#30003;&#35831;&#21738;&#20123;&#31867;&#22411;&#30340;&#19987;&#21033;&#33021;&#22815;&#25581;&#31034;&#20986;&#23427;&#20204;&#30340;&#21457;&#23637;&#25112;&#30053;&#65292;&#24182;&#24110;&#21161;&#20854;&#25552;&#21069;&#21457;&#29616;&#28508;&#22312;&#30340;&#21512;&#20316;&#20249;&#20276;&#25110;&#31454;&#20105;&#23545;&#25163;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#20844;&#21496;&#19981;&#26029;&#21464;&#21270;&#30340;&#20559;&#22909;&#21644;&#23545;&#20998;&#31867;&#20195;&#30721;&#30340;&#35821;&#20041;&#20851;&#32852;&#30340;&#24314;&#27169;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#40092;&#26377;&#28041;&#21450;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#20844;&#21496;&#21644;&#19987;&#21033;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#22522;&#30784;&#19978;&#12290;&#24403;&#35266;&#23519;&#21040;&#19968;&#20010;&#26032;&#30340;&#19987;&#21033;&#26102;&#65292;&#30456;&#20851;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#34920;&#31034;&#26681;&#25454;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#32534;&#30721;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#25429;&#25417;&#19987;&#21033;&#20998;&#31867;&#20195;&#30721;&#30340;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09765</link><description>&lt;p&gt;
&#21463;&#20919;&#33853;: &#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#21453;&#24046;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#29289;&#20307;&#21521;&#37327;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#27969;&#34892;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#65288;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#23545;&#65292;&#24182;&#24573;&#30053;&#20102;&#20174;&#20013;&#25552;&#21462;&#23545;&#35937;&#30340;&#20998;&#24067;&#12290;&#20154;&#31867;&#23545;&#29289;&#20307;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#26174;&#33879;&#21462;&#20915;&#20110;&#23545;&#35937;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#25972;&#20307;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20102;&#20154;&#31867;&#24863;&#30693;&#30340;&#21453;&#24046;&#25928;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#20998;&#25968;&#37327;&#21270;&#20102;&#22312;&#20004;&#20010;&#20803;&#32032;&#20043;&#38388;&#25214;&#21040;&#32473;&#23450;&#30456;&#20284;&#24230;&#30340;&#24778;&#21916;&#65292;&#30456;&#23545;&#20110;&#25104;&#23545;&#30340;&#25972;&#20307;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#24230;&#37327;&#65292;&#36890;&#24120;&#21457;&#29616;&#19982;&#21407;&#22987;&#20313;&#24358;&#30456;&#20284;&#24230;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;10-15\%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#30340;&#32972;&#26223;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.09764</link><description>&lt;p&gt;
&#21435;&#38500;&#32972;&#26223;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#23578;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation. (arXiv:2308.09764v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#30340;&#32972;&#26223;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#29702;&#35299;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28909;&#38376;&#35805;&#39064;&#65292;&#22312;&#24066;&#22330;&#19978;&#20855;&#26377;&#24456;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#30001;&#20110;&#26381;&#35013;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#20197;&#21450;&#21508;&#31181;&#22330;&#26223;&#21644;&#32972;&#26223;&#30340;&#23384;&#22312;&#65292;&#26102;&#23578;&#29702;&#35299;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20173;&#28982;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#20013;&#30340;&#32972;&#26223;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#26174;&#33879;&#24615;&#29289;&#20307;&#26816;&#27979;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#26102;&#23578;&#25968;&#25454;&#36827;&#34892;&#32972;&#26223;&#21435;&#38500;&#12290;&#34987;&#21435;&#38500;&#32972;&#26223;&#30340;&#26102;&#23578;&#22270;&#20687;&#19982;&#26102;&#23578;&#25968;&#25454;&#38598;&#20013;&#30340;&#21407;&#22987;&#22270;&#20687;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#21253;&#25324;&#27169;&#22411;&#26550;&#26500;&#12289;&#27169;&#22411;&#21021;&#22987;&#21270;&#12289;&#19982;&#20854;&#20182;&#35757;&#32451;&#25216;&#24039;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#20860;&#23481;&#24615;&#20197;&#21450;&#30446;&#26631;&#20219;&#21153;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the "rembg" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#20581;&#31574;&#30053;&#24341;&#23548;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32447;&#28436;&#21270;&#19968;&#20010;&#20984;&#35206;&#30422;&#31574;&#30053;&#38598;&#65292;&#21516;&#26102;&#28385;&#36275;&#30446;&#26631;&#20559;&#22909;&#31354;&#38388;&#30340;&#25506;&#32034;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.09734</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#20581;&#31574;&#30053;&#24341;&#23548;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments. (arXiv:2308.09734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09734
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#20581;&#31574;&#30053;&#24341;&#23548;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32447;&#28436;&#21270;&#19968;&#20010;&#20984;&#35206;&#30422;&#31574;&#30053;&#38598;&#65292;&#21516;&#26102;&#28385;&#36275;&#30446;&#26631;&#20559;&#22909;&#31354;&#38388;&#30340;&#25506;&#32034;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#31867;&#38656;&#35201;&#28385;&#36275;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#36807;&#31243;&#29305;&#24615;&#30340;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#34701;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#38745;&#24577;&#29615;&#22659;&#26102;&#36866;&#24212;&#24615;&#19981;&#36275;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21457;&#23637;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32447;&#25506;&#32034;&#23450;&#20041;&#30340;&#30446;&#26631;&#20559;&#22909;&#31354;&#38388;&#65292;&#21516;&#26102;&#28436;&#21270;&#31574;&#30053;&#35206;&#30422;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#31283;&#20581;&#22320;&#22312;&#32447;&#28436;&#21270;&#19968;&#20010;&#20984;&#35206;&#30422;&#31574;&#30053;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity to evolve a coverage set of policies that can solve the problem. This paper introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the prop
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36890;&#29992;&#30340;&#25216;&#33021;&#38598;&#65292;&#20351;&#24471;&#31574;&#30053;&#35206;&#30422;&#38598;&#33021;&#22815;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#25345;&#32493;&#28436;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09733</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#20869;&#22312;&#21160;&#26426;&#23618;&#27425;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes. (arXiv:2308.09733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09733
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36890;&#29992;&#30340;&#25216;&#33021;&#38598;&#65292;&#20351;&#24471;&#31574;&#30053;&#35206;&#30422;&#38598;&#33021;&#22815;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#25345;&#32493;&#28436;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26159;&#28041;&#21450;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#36825;&#20123;&#20989;&#25968;&#26080;&#27861;&#22312;&#27809;&#26377;&#22949;&#21327;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#38382;&#39064;&#26080;&#27861;&#20687;&#20256;&#32479;&#24773;&#20917;&#19979;&#37027;&#26679;&#36890;&#36807;&#21333;&#20010;&#26368;&#20248;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21457;&#23637;&#20102;&#19968;&#20010;&#21487;&#20197;&#28385;&#36275;&#35299;&#20915;&#38382;&#39064;&#20013;&#25152;&#26377;&#21487;&#33021;&#20559;&#22909;&#30340;&#26368;&#20248;&#31574;&#30053;&#35206;&#30422;&#38598;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23558;&#20854;&#35206;&#30422;&#38598;&#25512;&#24191;&#21040;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#24037;&#20316;&#12290;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#65292;&#29366;&#24577;&#36716;&#31227;&#21644;&#22870;&#21169;&#20998;&#24067;&#30340;&#21442;&#25968;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36825;&#38480;&#21046;&#23548;&#33268;&#20102;&#36827;&#21270;&#31574;&#30053;&#38598;&#30340;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#38656;&#35201;&#23398;&#20064;&#19968;&#32452;&#36890;&#29992;&#30340;&#25216;&#33021;&#65292;&#21487;&#20197;&#22312;&#29615;&#22659;&#21160;&#24577;&#21464;&#21270;&#26102;&#24341;&#23548;&#31574;&#30053;&#35206;&#30422;&#38598;&#30340;&#28436;&#21464;&#65292;&#20174;&#32780;&#20419;&#36827;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Markov decision processes are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problems cannot be solved by a single optimal policy as in the conventional case. Alternatively, multi-objective reinforcement learning methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in non-stationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skill set that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics therefore, it can facilitate a continuous learning 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;Baird&#21453;&#20363;&#19978;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.09732</link><description>&lt;p&gt;
Baird&#21453;&#20363;&#24050;&#35299;&#20915;&#65306;&#20197;&#35843;&#35797;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#31639;&#27861;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm. (arXiv:2308.09732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09732
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;Baird&#21453;&#20363;&#19978;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Baird&#21453;&#20363;&#26159;&#30001;Leemon Baird&#22312;1995&#24180;&#25552;&#20986;&#30340;&#65292;&#39318;&#20808;&#29992;&#20110;&#35777;&#26126;Temporal Difference (TD(0))&#31639;&#27861;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#21457;&#25955;&#12290;&#20174;&#37027;&#26102;&#36215;&#65292;&#23427;&#32463;&#24120;&#34987;&#29992;&#26469;&#27979;&#35797;&#21644;&#27604;&#36739;&#31163;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#12290;&#26799;&#24230;TD&#31639;&#27861;&#35299;&#20915;&#20102;TD&#22312;Baird&#21453;&#20363;&#19978;&#30340;&#21457;&#25955;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#30340;&#25910;&#25947;&#20173;&#28982;&#38750;&#24120;&#32531;&#24930;&#65292;&#32780;&#19988;&#32531;&#24930;&#30340;&#26412;&#36136;&#36824;&#19981;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#29305;&#21035;&#29702;&#35299;&#20026;&#20160;&#20040;TDC&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#24930;&#65292;&#24182;&#25552;&#20379;&#35843;&#35797;&#20998;&#26512;&#26469;&#29702;&#35299;&#36825;&#31181;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#35843;&#35797;&#25216;&#26415;&#21487;&#20197;&#29992;&#26469;&#30740;&#31350;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26368;&#36817;&#30340;Impression GTD&#31639;&#27861;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#25910;&#25947;&#38750;&#24120;&#24555;&#65292;&#20107;&#23454;&#19978;&#26159;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;Baird&#21453;&#20363;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#35299;&#20915;&#20102;&#65292;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;TD&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).  This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#22312;&#20020;&#24202;&#20915;&#31574;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#35774;&#35745;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#24182;&#20197;&#39046;&#22495;&#30693;&#35782;&#20026;&#22522;&#30784;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#21307;&#30103;&#19987;&#23478;&#65292;&#25552;&#21462;&#20851;&#38190;&#35265;&#35299;&#24182;&#36741;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#19968;&#39046;&#22495;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#32467;&#21512;&#22312;&#21019;&#24314;&#26356;&#20855;&#27934;&#23519;&#21147;&#30340;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#30340;&#21160;&#24577;&#65292;&#24182;&#39564;&#35777;&#20102;ChatGPT&#22312;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.09731</link><description>&lt;p&gt;
ChatGPT-HealthPrompt. &#21033;&#29992;ChatGPT&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20013;&#21457;&#25381;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT. (arXiv:2308.09731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#22312;&#20020;&#24202;&#20915;&#31574;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#35774;&#35745;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#24182;&#20197;&#39046;&#22495;&#30693;&#35782;&#20026;&#22522;&#30784;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#21307;&#30103;&#19987;&#23478;&#65292;&#25552;&#21462;&#20851;&#38190;&#35265;&#35299;&#24182;&#36741;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#19968;&#39046;&#22495;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#32467;&#21512;&#22312;&#21019;&#24314;&#26356;&#20855;&#27934;&#23519;&#21147;&#30340;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#30340;&#21160;&#24577;&#65292;&#24182;&#39564;&#35777;&#20102;ChatGPT&#22312;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#20020;&#24202;&#20915;&#31574;&#65292;&#37325;&#28857;&#20851;&#27880;OpenAI&#30340;ChatGPT&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#20351;&#29992;&#65292;&#31574;&#30053;&#24615;&#22320;&#35774;&#35745;&#21253;&#25324;&#20219;&#21153;&#25551;&#36848;&#12289;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#19988;&#20851;&#38190;&#22320;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#20415;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#20174;&#39640;&#24615;&#33021;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#24471;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#25552;&#31034;&#35774;&#35745;&#20013;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#21307;&#30103;&#19987;&#23478;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20851;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20851;&#38190;&#35265;&#35299;&#65292;&#20197;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#12290;&#39046;&#22495;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#22312;&#21019;&#24314;&#26356;&#20855;&#27934;&#23519;&#21147;&#30340;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LLMs&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#30340;&#21160;&#24577;&#12290;&#36890;&#36807;&#27604;&#36739;OpenAI&#30340;ChatGPT&#19982;&#20256;&#32479;&#30340;supervised&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool.  Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#36481;&#21160;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#20844;&#24179;&#30446;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#22312;&#25968;&#23383;&#20581;&#24247;&#31561;&#39046;&#22495;&#20013;&#21487;&#20197;&#25552;&#39640;&#25968;&#20493;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09726</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#19981;&#36481;&#21160;&#22810;&#33218;&#36172;&#21338;&#26426;&#65306;&#21463;&#25968;&#23383;&#20581;&#24247;&#21551;&#21457;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health. (arXiv:2308.09726v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#36481;&#21160;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#20844;&#24179;&#30446;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#22312;&#25968;&#23383;&#20581;&#24247;&#31561;&#39046;&#22495;&#20013;&#21487;&#20197;&#25552;&#39640;&#25968;&#20493;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#36481;&#21160;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;RMABs&#65289;&#26159;&#19968;&#31181;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36830;&#32493;&#29615;&#22659;&#20013;&#36827;&#34892;&#31639;&#27861;&#20915;&#31574;&#30340;&#27969;&#34892;&#26694;&#26550;&#12290;RMABs&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#12289;&#27835;&#30103;&#23433;&#25490;&#12289;&#21453;&#20599;&#29454;&#31561;&#25935;&#24863;&#20915;&#31574;&#65292;&#32780;&#26412;&#30740;&#31350;&#30340;&#21160;&#26426;&#27491;&#26159;&#25968;&#23383;&#20581;&#24247;&#12290;&#22312;&#36825;&#20123;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#20915;&#31574;&#24517;&#39035;&#25913;&#21892;&#32467;&#26524;&#24182;&#36991;&#20813;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#24046;&#36317;&#65288;&#20363;&#22914;&#65292;&#30830;&#20445;&#20581;&#24247;&#20844;&#24179;&#65289;&#12290;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;RMABs&#30340;&#20844;&#24179;&#30446;&#26631;&#65288;ERMABs&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#24179;&#25991;&#29486;&#20013;&#20004;&#31181;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#30446;&#26631;&#65292;&#26368;&#23567;&#21270;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#26368;&#22823;&#21270;&#32435;&#20160;&#31119;&#21033;&#12290;&#25105;&#20204;&#20026;&#35299;&#20915;&#36825;&#20004;&#20010;&#30446;&#26631;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#8212;&#8212;&#23545;&#20110;&#21069;&#32773;&#20351;&#29992;&#20102;&#27700;&#20301;&#22635;&#20805;&#31639;&#27861;&#65292;&#23545;&#20110;&#21518;&#32773;&#20351;&#29992;&#20102;&#29702;&#35770;&#19978;&#26377;&#21160;&#26426;&#30340;&#36138;&#24515;&#31639;&#27861;&#26469;&#24179;&#34913;&#19981;&#21516;&#32676;&#20307;&#22823;&#23567;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#27169;&#25311;&#39046;&#22495;&#65288;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#25968;&#23383;&#20581;&#24247;&#27169;&#22411;&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#21487;&#20197;&#27604;&#24403;&#21069;&#26041;&#27861;&#25552;&#39640;&#25968;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless multi-armed bandits (RMABs) are a popular framework for algorithmic decision making in sequential settings with limited resources. RMABs are increasingly being used for sensitive decisions such as in public health, treatment scheduling, anti-poaching, and -- the motivation for this work -digital health. For such high stakes settings, decisions must both improve outcomes and prevent disparities between groups (e.g., ensure health equity). We study equitable objectives for RMABs (ERMABs) for the first time. We consider two equity-aligned objectives from the fairness literature, minimax reward and max Nash welfare. We develop efficient algorithms for solving each -- a water filling algorithm for the former, and a greedy algorithm with theoretically motivated nuance to balance disparate group sizes for the latter. Finally, we demonstrate across three simulation domains, including a new digital health model, that our approaches can be multiple times more equitable than the curren
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoCLIM&#30340;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#20013;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09725</link><description>&lt;p&gt;
MoCLIM: &#29992;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#21644;&#32452;&#23398;&#25512;&#29702;&#24314;&#27169;&#23454;&#29616;&#20934;&#30830;&#30340;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling. (arXiv:2308.09725v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoCLIM&#30340;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#20013;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#21307;&#23398;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#30284;&#30151;&#20122;&#22411;&#30340;&#29983;&#21270;&#26426;&#21046;&#19982;&#30142;&#30149;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#22522;&#20110;&#32452;&#23398;&#30340;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#32452;&#23398;&#35760;&#24405;&#20102;&#30284;&#30151;&#20013;&#22810;&#27493;&#39588;&#36807;&#31243;&#30340;&#29983;&#21270;&#20135;&#29289;&#12290;&#26412;&#25991;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#26469;&#25913;&#21892;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#32467;&#26524;&#65292;&#22240;&#27492;&#24320;&#21457;&#20102;MoCLIM&#65292;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;MoCLIM&#29420;&#31435;&#22320;&#20174;&#19981;&#21516;&#30340;&#32452;&#23398;&#27169;&#24335;&#20013;&#25552;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#32452;&#23398;&#27169;&#24335;&#20043;&#38388;&#30340;&#23545;&#27604;&#23398;&#20064;&#25152;&#24471;&#21040;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#22312;&#32473;&#23450;&#30284;&#30151;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#20122;&#22411;&#24456;&#22909;&#22320;&#32858;&#31867;&#21040;&#36739;&#20302;&#30340;&#28508;&#31354;&#38388;&#20013;&#12290;&#36825;&#31181;&#23545;&#27604;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#29983;&#29289;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#32452;&#38469;&#25512;&#29702;&#30340;&#25237;&#24433;&#12290;&#22312;&#20845;&#20010;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36739;&#23569;&#30340;&#39640;&#32500;&#30284;&#30151;&#25968;&#25454;&#25311;&#21512;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precision medicine fundamentally aims to establish causality between dysregulated biochemical mechanisms and cancer subtypes. Omics-based cancer subtyping has emerged as a revolutionary approach, as different level of omics records the biochemical products of multistep processes in cancers. This paper focuses on fully exploiting the potential of multi-omics data to improve cancer subtyping outcomes, and hence developed MoCLIM, a representation learning framework. MoCLIM independently extracts the informative features from distinct omics modalities. Using a unified representation informed by contrastive learning of different omics modalities, we can well-cluster the subtypes, given cancer, into a lower latent space. This contrast can be interpreted as a projection of inter-omics inference observed in biological networks. Experimental results on six cancer datasets demonstrate that our approach significantly improves data fit and subtyping performance in fewer high-dimensional cancer ins
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#21551;&#21457;&#30340;&#23376;&#39046;&#22495;&#36866;&#24212;&#65288;KISA&#65289;&#26694;&#26550;&#22312;&#20132;&#21449;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#20013;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.09724</link><description>&lt;p&gt;
&#30693;&#35782;&#21551;&#21457;&#30340;&#23376;&#39046;&#22495;&#36866;&#24212;&#29992;&#20110;&#20132;&#21449;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Knowledge-inspired Subdomain Adaptation for Cross-Domain Knowledge Transfer. (arXiv:2308.09724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09724
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#21551;&#21457;&#30340;&#23376;&#39046;&#22495;&#36866;&#24212;&#65288;KISA&#65289;&#26694;&#26550;&#22312;&#20132;&#21449;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#20013;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#20197;&#20840;&#23616;&#26041;&#24335;&#23545;&#40784;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#26679;&#26412;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#40784;&#21518;&#65292;&#27599;&#20010;&#28304;&#26679;&#26412;&#37117;&#26399;&#26395;&#19982;&#20219;&#20309;&#30446;&#26631;&#26679;&#26412;&#30456;&#20284;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20840;&#23616;&#23545;&#40784;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#26368;&#20248;&#25110;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#32454;&#31890;&#24230;&#30340;&#39046;&#22495;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#21551;&#21457;&#30340;&#23376;&#39046;&#22495;&#36866;&#24212;&#65288;KISA&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;KISA&#26368;&#23567;&#21270;&#20849;&#20139;&#39044;&#26399;&#25439;&#22833;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#36825;&#26159;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#25104;&#21151;&#30340;&#21069;&#25552;&#12290;&#65288;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#21551;&#21457;&#30340;&#23376;&#39046;&#22495;&#21010;&#20998;&#38382;&#39064;&#65292;&#36825;&#22312;&#32454;&#31890;&#24230;&#30340;&#39046;&#22495;&#36866;&#24212;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art deep domain adaptation techniques align source and target samples in a global fashion. That is, after alignment, each source sample is expected to become similar to any target sample. However, global alignment may not always be optimal or necessary in practice. For example, consider cross-domain fraud detection, where there are two types of transactions: credit and non-credit. Aligning credit and non-credit transactions separately may yield better performance than global alignment, as credit transactions are unlikely to exhibit patterns similar to non-credit transactions. To enable such fine-grained domain adaption, we propose a novel Knowledge-Inspired Subdomain Adaptation (KISA) framework. In particular, (1) We provide the theoretical insight that KISA minimizes the shared expected loss which is the premise for the success of domain adaptation methods. (2) We propose the knowledge-inspired subdomain division problem that plays a crucial role in fine-grained doma
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2308.09720</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24847;&#24819;&#19981;&#21040;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09720
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#23637;&#31034;&#20986;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#65288;&#39044;&#27979;&#20154;&#31867;&#20070;&#20889;&#25991;&#26412;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#65289;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#30340;&#24615;&#36136;&#21450;&#20854;&#19982;&#20854;&#20182;&#24050;&#30693;&#38388;&#25509;&#36807;&#31243;&#30340;&#20851;&#31995;&#12290;&#25991;&#31456;&#20027;&#24352;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#30340;&#19968;&#20010;&#37325;&#35201;&#21103;&#20316;&#29992;&#26159;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#24320;&#21457;&#30340;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#21487;&#39044;&#27979;&#30340;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#31616;&#35201;&#35752;&#35770;&#20102;&#36825;&#20123;&#31995;&#32479;&#25152;&#33719;&#24471;&#30340;&#35748;&#30693;&#25216;&#33021;&#19982;&#20154;&#31867;&#35748;&#30693;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I discuss the nature of this indirect acquisition process and its relation to other known indirect processes. I argue that an important side effect of such indirect acquisition is the development of integrated abilities. I discuss the extent to which the abilities developed by large language models are predictable. Finally, I briefly discuss the relation between the cognitive skills acquired by these systems and human cognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;COVID-19&#24863;&#26579;&#39118;&#38505;&#26412;&#20307;&#65288;CIRO&#65289;&#33258;&#21160;&#35780;&#20272;&#27599;&#20010;&#20154;&#30340;&#24863;&#26579;&#39118;&#38505;&#65292;&#20197;&#20943;&#36731;&#20844;&#20849;&#21355;&#29983;&#23448;&#21592;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2308.09719</link><description>&lt;p&gt;
CIRO: COVID-19&#24863;&#26579;&#39118;&#38505;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
CIRO: COVID-19 infection risk ontology. (arXiv:2308.09719v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;COVID-19&#24863;&#26579;&#39118;&#38505;&#26412;&#20307;&#65288;CIRO&#65289;&#33258;&#21160;&#35780;&#20272;&#27599;&#20010;&#20154;&#30340;&#24863;&#26579;&#39118;&#38505;&#65292;&#20197;&#20943;&#36731;&#20844;&#20849;&#21355;&#29983;&#23448;&#21592;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#21355;&#29983;&#26426;&#26500;&#36827;&#34892;&#25509;&#35302;&#36861;&#36394;&#20197;&#35782;&#21035;&#19982;&#24863;&#26579;&#30149;&#20363;&#30340;&#23494;&#20999;&#25509;&#35302;&#32773;&#12290;&#28982;&#32780;&#65292;&#22312;&#30001;2019&#24180;&#20896;&#29366;&#30149;&#27602;&#30149;&#65288;COVID-19&#65289;&#24341;&#36215;&#30340;&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#39640;&#30149;&#20363;&#37327;&#22269;&#23478;&#27809;&#26377;&#37319;&#29992;&#36825;&#31181;&#25805;&#20316;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26085;&#26412;&#25919;&#24220;&#36827;&#34892;&#20102;&#36825;&#39033;&#25805;&#20316;&#65292;&#20197;&#20943;&#23569;&#20844;&#20849;&#21355;&#29983;&#23448;&#21592;&#30340;&#33392;&#36763;&#25163;&#24037;&#21171;&#21160;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25511;&#21046;&#24863;&#26579;&#12290;&#20026;&#20943;&#36731;&#23448;&#21592;&#30340;&#36127;&#25285;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;COVID-19&#24863;&#26579;&#39118;&#38505;&#26412;&#20307;&#65288;CIRO&#65289;&#30340;&#26412;&#20307;&#26469;&#33258;&#21160;&#35780;&#20272;&#27599;&#20010;&#20154;&#30340;&#24863;&#26579;&#39118;&#38505;&#12290;&#36825;&#20010;&#26412;&#20307;&#20351;&#29992;&#36164;&#28304;&#25551;&#36848;&#26694;&#26550;&#65288;RDF&#65289;&#21644;SPARQL&#65288;SPARQL&#21327;&#35758;&#21644;RDF&#26597;&#35810;&#35821;&#35328;&#65289;&#26597;&#35810;&#34920;&#36798;&#20102;&#26085;&#26412;&#25919;&#24220;&#23545;COVID-19&#30340;&#24863;&#26579;&#39118;&#38505;&#65292;&#20197;&#20415;&#33258;&#21160;&#35780;&#20272;&#20010;&#20307;&#30340;&#24863;&#26579;&#39118;&#38505;&#12290;&#36890;&#36807;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26500;&#24314;&#30340;&#30693;&#35782;&#22270;&#21487;&#20197;&#25512;&#26029;&#20986;&#30001;t
&lt;/p&gt;
&lt;p&gt;
Public health authorities perform contact tracing for highly contagious agents to identify close contacts with the infected cases. However, during the pandemic caused by coronavirus disease 2019 (COVID-19), this operation was not employed in countries with high patient volumes. Meanwhile, the Japanese government conducted this operation, thereby contributing to the control of infections, at the cost of arduous manual labor by public health officials. To ease the burden of the officials, this study attempted to automate the assessment of each person's infection risk through an ontology, called COVID-19 Infection Risk Ontology (CIRO). This ontology expresses infection risks of COVID-19 formulated by the Japanese government, toward automated assessment of infection risks of individuals, using Resource Description Framework (RDF) and SPARQL (SPARQL Protocol and RDF Query Language) queries. For evaluation, we demonstrated that the knowledge graph built could infer the risks, formulated by t
&lt;/p&gt;</description></item><item><title>&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2308.09687</link><description>&lt;p&gt;
&#24819;&#27861;&#22270;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09687
&lt;/p&gt;
&lt;p&gt;
&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#27861;&#22270;&#65288;Graph of Thoughts&#65292;GoT&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;Chain-of-Thought&#25110;Tree of Thoughts&#65288;ToT&#65289;&#31561;&#33539;&#24335;&#12290;GoT&#30340;&#20851;&#38190;&#24605;&#24819;&#21644;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#23558;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#20854;&#20013;&#20449;&#24687;&#21333;&#20803;&#65288;"LLM&#24819;&#27861;"&#65289;&#26159;&#39030;&#28857;&#65292;&#36793;&#34920;&#31034;&#36825;&#20123;&#39030;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#23558;&#20219;&#24847;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#12289;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#25490;&#24207;&#20219;&#21153;&#19978;&#36136;&#37327;&#25552;&#39640;&#20102;62%&#65292;&#21516;&#26102;&#25104;&#26412;&#38477;&#20302;&#20102;&#36229;&#36807;31%&#12290;&#25105;&#20204;&#30830;&#20445;GoT&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#24320;&#21019;&#26032;&#30340;&#25552;&#31034;&#26041;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;&#24418;&#28151;&#21512;&#24605;&#32500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#36339;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09658</link><description>&lt;p&gt;
&#26641;&#24418;&#28151;&#21512;&#24605;&#32500;: &#23558;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#32467;&#21512;&#29992;&#20110;&#22810;&#36339;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning. (arXiv:2308.09658v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;&#24418;&#28151;&#21512;&#24605;&#32500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#36339;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31867;&#20284;&#20195;&#30721;&#30340;&#35745;&#21010;&#65292;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#35270;&#35273;&#25512;&#29702;&#65289;&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36235;&#21183;&#12290;&#36825;&#31181;&#34987;&#31216;&#20026;LLM-based planning&#30340;&#33539;&#24335;&#22312;&#38382;&#39064;&#35299;&#20915;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#31616;&#21333;&#38382;&#39064;&#30340;&#22522;&#26412;&#24773;&#26223;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#30452;&#25509;&#22238;&#31572;&#20986;&#26469;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#25512;&#29702;&#27493;&#39588;&#12290;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#30340;&#35745;&#21010;&#21046;&#35746;&#36824;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#22810;&#36339;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#21644;&#35745;&#21010;&#25628;&#32034;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#21464;&#24471;&#26174;&#33879;&#12290;&#30446;&#21069;&#30340;&#31639;&#27861;&#35201;&#20040;&#36890;&#36807;&#37319;&#29992;&#24555;&#36895;&#19968;&#27425;&#24615;&#29983;&#25104;&#26469;&#35299;&#20915;&#25928;&#29575;&#38382;&#39064;&#65292;&#35201;&#20040;&#37319;&#29992;&#22797;&#26434;&#30340;&#36845;&#20195;&#29983;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20294;&#20004;&#31181;&#26041;&#27861;&#37117;&#26080;&#27861;&#24179;&#34913;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#38656;&#27714;&#12290;&#21463;&#21040;&#20154;&#33041;&#20013;&#30340;&#21452;&#31995;&#32479;&#35748;&#30693;&#65288;&#24555;&#36895;&#24605;&#32771;&#21644;&#24930;&#36895;&#24605;&#32771;&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;&#24418;&#28151;&#21512;&#24605;&#32500;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There emerges a promising trend of using large language models (LLMs) to generate code-like plans for complex inference tasks such as visual reasoning. This paradigm, known as LLM-based planning, provides flexibility in problem solving and endows better interpretability. However, current research is mostly limited to basic scenarios of simple questions that can be straightforward answered in a few inference steps. Planning for the more challenging multi-hop visual reasoning tasks remains under-explored. Specifically, under multi-hop reasoning situations, the trade-off between accuracy and the complexity of plan-searching becomes prominent. The prevailing algorithms either address the efficiency issue by employing the fast one-stop generation or adopt a complex iterative generation method to improve accuracy. Both fail to balance the need for efficiency and performance. Drawing inspiration from the dual system of cognition in the human brain, the fast and the slow think processes, we pr
&lt;/p&gt;</description></item><item><title>&#27602;&#31661;&#34521;&#26159;&#19968;&#31181;&#26080;&#26631;&#31614;&#25915;&#20987;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#30446;&#26631;&#31867;&#21035;&#30340;&#30693;&#35782;&#12290;&#23427;&#20855;&#26377;&#20302;&#20013;&#27602;&#29575;&#21644;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.09487</link><description>&lt;p&gt;
&#27602;&#31661;&#34521;&#65306;&#19968;&#31181;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#20302;&#20013;&#27602;&#29575;&#21644;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#26080;&#26631;&#31614;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data. (arXiv:2308.09487v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09487
&lt;/p&gt;
&lt;p&gt;
&#27602;&#31661;&#34521;&#26159;&#19968;&#31181;&#26080;&#26631;&#31614;&#25915;&#20987;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#30446;&#26631;&#31867;&#21035;&#30340;&#30693;&#35782;&#12290;&#23427;&#20855;&#26377;&#20302;&#20013;&#27602;&#29575;&#21644;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25104;&#21151;&#21457;&#21160;&#21518;&#38376;&#25915;&#20987;&#65292;&#27880;&#20837;&#30340;&#25968;&#25454;&#38656;&#35201;&#34987;&#27491;&#30830;&#26631;&#35760;&#65292;&#21542;&#21017;&#65292;&#21363;&#20351;&#26159;&#22522;&#26412;&#30340;&#25968;&#25454;&#36807;&#28388;&#22120;&#20063;&#33021;&#36731;&#26131;&#26816;&#27979;&#20986;&#26469;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#20102;&#26080;&#26631;&#31614;&#25915;&#20987;&#30340;&#27010;&#24565;&#65292;&#36825;&#31181;&#25915;&#20987;&#26356;&#21152;&#21361;&#38505;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#25913;&#21464;&#27880;&#20837;&#25968;&#25454;&#30340;&#26631;&#31614;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26080;&#26631;&#31614;&#21518;&#38376;&#25915;&#20987;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#23545;&#25972;&#20010;&#35757;&#32451;&#38598;&#25110;&#20854;&#20013;&#19968;&#37096;&#20998;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25915;&#20987;&#32773;&#24456;&#38590;&#25317;&#26377;&#36825;&#20123;&#29702;&#35299;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#36890;&#24120;&#26469;&#33258;&#22810;&#20010;&#29420;&#31435;&#30340;&#26469;&#28304;&#12290;&#19982;&#25152;&#26377;&#24403;&#21069;&#30340;&#26080;&#26631;&#31614;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26631;&#31614;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;&#27602;&#31661;&#34521;&#8221;&#12290;&#27602;&#31661;&#34521;&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#20102;&#35299;&#25915;&#20987;&#30446;&#26631;&#31867;&#21035;&#65292;&#27604;&#22914;&#8220;&#34521;&#8221;&#12290;&#22312;CIFAR10&#12289;Tiny-ImageNet&#21644;TSRD&#19978;&#65292;&#20165;&#38656;&#35201;&#20998;&#21035;&#21344;&#35757;&#32451;&#38598;&#22823;&#23567;&#30340;0.1%&#12289;0.025%&#21644;0.4%&#30340;&#20013;&#27602;&#29575;&#65292;&#27602;&#31661;&#34521;&#23601;&#33021;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
To successfully launch backdoor attacks, injected data needs to be correctly labeled; otherwise, they can be easily detected by even basic data filters. Hence, the concept of clean-label attacks was introduced, which is more dangerous as it doesn't require changing the labels of injected data. To the best of our knowledge, the existing clean-label backdoor attacks largely relies on an understanding of the entire training set or a portion of it. However, in practice, it is very difficult for attackers to have it because of training datasets often collected from multiple independent sources. Unlike all current clean-label attacks, we propose a novel clean label method called 'Poison Dart Frog'. Poison Dart Frog does not require access to any training data; it only necessitates knowledge of the target class for the attack, such as 'frog'. On CIFAR10, Tiny-ImageNet, and TSRD, with a mere 0.1\%, 0.025\%, and 0.4\% poisoning rate of the training set size, respectively, Poison Dart Frog achie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36830;&#25509;&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;CLIP&#12289;CLAP&#21644;AudioLDM&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#21548;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09300</link><description>&lt;p&gt;
V2A-Mapper&#65306;&#36890;&#36807;&#36830;&#25509;&#22522;&#30784;&#27169;&#22411;&#23454;&#29616;&#36731;&#37327;&#32423;&#30340;&#35270;&#21548;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models. (arXiv:2308.09300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36830;&#25509;&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;CLIP&#12289;CLAP&#21644;AudioLDM&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#21548;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#32452;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#26500;&#24314;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22823;&#37327;&#25968;&#25454;&#23398;&#20064;&#24471;&#21040;&#30340;&#20195;&#34920;&#24615;&#21644;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#21644;&#36801;&#31227;&#33267;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20174;&#22836;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#38899;&#39057;&#27169;&#24577;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#20013;&#65292;&#21033;&#29992;FMs&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#33258;&#21160;&#29983;&#25104;&#35821;&#20041;&#30456;&#20851;&#30340;&#22768;&#38899;&#26159;&#36328;&#27169;&#24577;&#29983;&#25104;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35270;&#21548;&#29983;&#25104;&#65288;V2A&#65289;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#35268;&#27169;&#36866;&#20013;&#30340;&#25968;&#25454;&#38598;&#20174;&#22836;&#35774;&#35745;&#21644;&#26500;&#24314;&#22797;&#26434;&#30340;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;CLIP&#12289;CLAP&#21644;AudioLDM&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#35270;&#35273;CLIP&#27169;&#22411;&#21644;&#21548;&#35273;CLAP&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;LLM&#29983;&#25104;&#30340;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#34920;&#31034;&#20026;&#25512;&#29702;&#22270;&#65292;&#20174;&#32780;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.09267</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#39564;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach. (arXiv:2308.09267v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;LLM&#29983;&#25104;&#30340;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#34920;&#31034;&#20026;&#25512;&#29702;&#22270;&#65292;&#20174;&#32780;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#31561;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#21463;&#29305;&#23450;&#35774;&#35745;&#30340;&#25552;&#31034;&#25351;&#23548;&#19979;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#36825;&#19981;&#20165;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36824;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#38382;&#39064;&#27714;&#35299;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#25552;&#21319;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;LLM&#36755;&#20986;&#39564;&#35777;&#22120;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#36825;&#20123;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#24819;&#19968;&#20010;&#25512;&#29702;&#20219;&#21153;&#30340;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#30001;LLM&#29983;&#25104;&#65292;&#21487;&#20197;&#30001;&#25512;&#29702;&#22270;&#34920;&#31034;&#65292;&#22240;&#20026;&#19981;&#21516;&#25512;&#29702;&#36335;&#24452;&#30340;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#23384;&#22312;&#36923;&#36753;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25512;&#29702;&#22270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased impressive reasoning capabilities, particularly when guided by specifically designed prompts in complex reasoning tasks such as math word problems. These models typically solve tasks using a chain-of-thought approach, which not only bolsters their reasoning abilities but also provides valuable insights into their problem-solving process. However, there is still significant room for enhancing the reasoning abilities of LLMs. Some studies suggest that the integration of an LLM output verifier can boost reasoning accuracy without necessitating additional model training. In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs. We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths. Therefore, we propose the Reasoning Graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31216;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#25277;&#26679;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#8220;&#22495;&#21487;&#25552;&#21319;&#25277;&#26679;&#8221;&#30340;&#20869;&#23481;&#65292;&#35777;&#26126;&#20102;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#38382;&#39064;&#20063;&#23384;&#22312;&#21487;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.08828</link><description>&lt;p&gt;
&#23545;&#31216;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#25277;&#26679;&#30340;&#25552;&#21319;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lifted Algorithms for Symmetric Weighted First-Order Model Sampling. (arXiv:2308.08828v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31216;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#25277;&#26679;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#8220;&#22495;&#21487;&#25552;&#21319;&#25277;&#26679;&#8221;&#30340;&#20869;&#23481;&#65292;&#35777;&#26126;&#20102;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#38382;&#39064;&#20063;&#23384;&#22312;&#21487;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#27169;&#22411;&#35745;&#25968;&#65288;WMC&#65289;&#26159;&#35745;&#31639;&#21629;&#39064;&#20844;&#24335;&#30340;&#25152;&#26377;&#28385;&#36275;&#35299;&#65288;&#27169;&#22411;&#65289;&#30340;&#26435;&#37325;&#20043;&#21644;&#30340;&#20219;&#21153;&#12290;&#31867;&#20284;&#22320;&#65292;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#65288;WMS&#65289;&#26088;&#22312;&#20197;&#27010;&#29575;&#19982;&#23427;&#20204;&#30456;&#24212;&#30340;&#26435;&#37325;&#25104;&#27604;&#20363;&#22320;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#12290;WMC&#21644;WMS&#37117;&#38590;&#20197;&#31934;&#30830;&#27714;&#35299;&#65292;&#23646;&#20110;\#P-hard&#22797;&#26434;&#24230;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#22914;&#26524;&#21629;&#39064;&#20844;&#24335;&#21487;&#20197;&#20197;&#32039;&#20945;&#30340;&#26041;&#24335;&#34920;&#31034;&#24182;&#29992;&#19968;&#38454;&#36923;&#36753;&#34920;&#36798;&#65292;&#26377;&#26102;&#35745;&#25968;&#38382;&#39064;&#21487;&#33021;&#26159;&#21487;&#20197;&#22788;&#29702;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#21487;&#20197;&#22312;&#22495;&#22823;&#23567;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#65292;&#24182;&#34987;&#31216;&#20026;&#8220;&#22495;&#21487;&#25552;&#21319;&#8221;&#12290;&#28982;&#21518;&#65292;&#23601;&#20250;&#20986;&#29616;&#20197;&#19979;&#38382;&#39064;&#65306;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#26159;&#21542;&#20063;&#26159;&#22914;&#27492;&#65311;&#26412;&#25991;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#23427;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#8220;&#22495;&#21487;&#25552;&#21319;&#25277;&#26679;&#8221;&#30340;&#20869;&#23481;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted model counting (WMC) is the task of computing the weighted sum of all satisfying assignments (i.e., models) of a propositional formula. Similarly, weighted model sampling (WMS) aims to randomly generate models with probability proportional to their respective weights. Both WMC and WMS are hard to solve exactly, falling under the \#P-hard complexity class. However, it is known that the counting problem may sometimes be tractable, if the propositional formula can be compactly represented and expressed in first-order logic. In such cases, model counting problems can be solved in time polynomial in the domain size, and are known as \textit{domain-liftable}. The following question then arises: Is it also the case for weighted model sampling? This paper addresses this question and answers it affirmatively. Specifically, we prove the \textit{domain-liftability under sampling} for the two-variables fragment of first-order logic with counting quantifiers in this paper, by devising an e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#20998;&#25104;&#23376;&#38598;&#24182;&#32452;&#21512;&#21508;&#23376;&#38598;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08780</link><description>&lt;p&gt;
&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Exploring Demonstration Ensembling for In-context Learning. (arXiv:2308.08780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#20998;&#25104;&#23376;&#38598;&#24182;&#32452;&#21512;&#21508;&#23376;&#38598;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#36890;&#36807;&#21521;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#31034;&#20363;&#26469;&#36827;&#34892;&#25805;&#20316;&#65292;&#21363;&#28436;&#31034;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#23558;&#28436;&#31034;&#19982;&#27979;&#35797;&#36755;&#20837;&#36830;&#25509;&#36215;&#26469;&#25552;&#31034;&#32473;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36830;&#25509;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#27599;&#20010;&#28436;&#31034;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#36129;&#29486;&#12290;&#24403;&#19968;&#20123;&#28436;&#31034;&#19982;&#27979;&#35797;&#31034;&#20363;&#26080;&#20851;&#26102;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#26576;&#20123;&#21464;&#25442;&#22120;&#27169;&#22411;&#23545;&#36755;&#20837;&#38271;&#24230;&#26377;&#38480;&#21046;&#65292;&#23558;&#35768;&#22810;&#31034;&#20363;&#25918;&#20837;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#20219;&#21153;&#26102;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28436;&#31034;&#38598;&#25104;&#65288;DENSE&#65289;&#20316;&#20026;&#31616;&#21333;&#36830;&#25509;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#27169;&#22411;&#20351;&#29992;&#28436;&#31034;&#30340;&#23376;&#38598;&#65288;&#21363;bucket&#65289;&#26469;&#39044;&#27979;&#36755;&#20986;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#23376;&#38598;&#24471;&#21040;&#30340;&#36755;&#20986;&#27010;&#29575;&#32452;&#21512;&#36215;&#26469;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-j&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) operates by showing language models (LMs) examples of input-output pairs for a given task, i.e., demonstrations. The standard approach for ICL is to prompt the LM with concatenated demonstrations followed by the test input. This approach suffers from some issues. First, concatenation offers almost no control over the contribution of each demo to the model prediction. This can be sub-optimal when some demonstrations are irrelevant to the test example. Second, due to the input length limit of some transformer models, it might be infeasible to fit many examples into the context, especially when dealing with long-input tasks. In this work, we explore Demonstration Ensembling (DENSE) as an alternative to simple concatenation. \model predicts outputs using subsets (i.e., buckets) of the demonstrations and then combines the output probabilities resulting from each subset to produce the final prediction. We study different ensembling methods using GPT-j and experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2308.08708</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24847;&#35782;&#65306;&#26469;&#33258;&#24847;&#35782;&#31185;&#23398;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25110;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#33021;&#20855;&#26377;&#24847;&#35782;&#25104;&#20026;&#31185;&#23398;&#30028;&#20851;&#27880;&#30340;&#35805;&#39064;&#65292;&#20063;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#25285;&#24551;&#12290;&#26412;&#25253;&#21578;&#25552;&#20986;&#24182;&#20030;&#20363;&#20102;&#19968;&#31181;&#20005;&#35880;&#19988;&#32463;&#39564;&#22522;&#30784;&#30340;&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26041;&#27861;&#65306;&#26681;&#25454;&#25105;&#20204;&#30446;&#21069;&#26368;&#21487;&#20449;&#30340;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#23545;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20960;&#31181;&#24191;&#27867;&#35748;&#21487;&#30340;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#65292;&#21253;&#25324;&#24490;&#29615;&#22788;&#29702;&#29702;&#35770;&#12289;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#12289;&#39640;&#38454;&#29702;&#35770;&#12289;&#39044;&#27979;&#22788;&#29702;&#29702;&#35770;&#21644;&#27880;&#24847;&#27169;&#24335;&#29702;&#35770;&#12290;&#20174;&#36825;&#20123;&#29702;&#35770;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20123;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#20855;&#22791;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25351;&#31034;&#24615;&#29305;&#24449;&#26469;&#35780;&#20272;&#20102;&#20960;&#20010;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#31995;&#32479;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20855;&#26377;&#24847;&#35782;&#65292;&#20294;&#21516;&#26102;&#20063;&#26174;&#31034;&#20986;&#27809;&#26377;&#26126;&#26174;&#30340;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TeCH&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#26041;&#27861;&#26469;&#37325;&#24314;&#36924;&#30495;&#30340;&#26381;&#39280;&#20154;&#29289;&#12290;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#8220;&#26410;&#26366;&#30475;&#35265;&#30340;&#21306;&#22495;&#8221;&#24182;&#28155;&#21152;&#39640;&#32423;&#32454;&#33410;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;DMTet&#30340;&#28151;&#21512;3D&#34920;&#31034;&#20197;&#36798;&#21040;&#26356;&#20302;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.08545</link><description>&lt;p&gt;
TeCH: &#25991;&#26412;&#24341;&#23548;&#30340;&#36924;&#30495;&#26381;&#39280;&#20154;&#29289;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
TeCH: Text-guided Reconstruction of Lifelike Clothed Humans. (arXiv:2308.08545v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08545
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TeCH&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#26041;&#27861;&#26469;&#37325;&#24314;&#36924;&#30495;&#30340;&#26381;&#39280;&#20154;&#29289;&#12290;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#8220;&#26410;&#26366;&#30475;&#35265;&#30340;&#21306;&#22495;&#8221;&#24182;&#28155;&#21152;&#39640;&#32423;&#32454;&#33410;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;DMTet&#30340;&#28151;&#21512;3D&#34920;&#31034;&#20197;&#36798;&#21040;&#26356;&#20302;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#22312;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#37325;&#24314;&#30528;&#35013;&#20154;&#29289;&#26041;&#38754;&#21462;&#24471;&#20102;&#30740;&#31350;&#36827;&#23637;&#65292;&#20294;&#20934;&#30830;&#24674;&#22797;&#8220;&#26410;&#26366;&#30475;&#35265;&#30340;&#21306;&#22495;&#8221;&#24182;&#28155;&#21152;&#39640;&#32423;&#32454;&#33410;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#32570;&#20047;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#20250;&#29983;&#25104;&#36807;&#20110;&#24179;&#28369;&#30340;&#32972;&#38754;&#34920;&#38754;&#21644;&#27169;&#31946;&#30340;&#32441;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TeCH&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;1&#65289;&#36890;&#36807;&#26381;&#35013;&#35299;&#26512;&#27169;&#22411;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#25551;&#36848;&#24615;&#25991;&#26412;&#25552;&#31034;&#65288;&#20363;&#22914;&#65292;&#26381;&#39280;&#12289;&#39068;&#33394;&#12289;&#21457;&#22411;&#65289;&#65307;2&#65289;&#32463;&#36807;&#20010;&#24615;&#21270;&#24494;&#35843;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;T2I&#65289;&#26469;&#23398;&#20064;&#8220;&#26080;&#27861;&#25551;&#36848;&#8221;&#30340;&#22806;&#35266;&#65292;&#20174;&#32780;&#23545;3D&#20154;&#31867;&#24418;&#35937;&#36827;&#34892;&#37325;&#24314;&#12290;&#20026;&#20102;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#34920;&#31034;&#39640;&#20998;&#36776;&#29575;&#30340;3D&#26381;&#39280;&#20154;&#29289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;DMTet&#30340;&#28151;&#21512;3D&#34920;&#31034;&#65292;&#23427;&#30001;&#26126;&#30830;&#30340;&#36523;&#20307;&#24418;&#29366;&#32593;&#26684;&#21644;&#19968;&#20010;&#8230;&#8230;&#65288;&#25688;&#35201;&#36739;&#38271;&#65292;&#30465;&#30053;&#37096;&#20998;&#20869;&#23481;&#65289;
&lt;/p&gt;
&lt;p&gt;
Despite recent research advancements in reconstructing clothed humans from a single image, accurately restoring the "unseen regions" with high-level details remains an unsolved challenge that lacks attention. Existing methods often generate overly smooth back-side surfaces with a blurry texture. But how to effectively capture all visual attributes of an individual from a single image, which are sufficient to reconstruct unseen areas (e.g., the back view)? Motivated by the power of foundation models, TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the "indescribable" appearance. To represent high-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D representation based on DMTet, which consists of an explicit body shape grid and an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20013;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#35752;&#35770;&#20102;AIGC&#30340;&#22522;&#30784;&#25216;&#26415;&#12289;&#24066;&#22330;&#29366;&#20917;&#21644;&#21457;&#23637;&#36712;&#36857;&#65292;&#24182;&#37325;&#28857;&#24378;&#35843;&#20102;AIGC&#30340;&#29983;&#24577;&#24314;&#35774;&#12290;&#39044;&#27979;&#20102;&#34892;&#19994;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.08451</link><description>&lt;p&gt;
&#20013;&#22269;AIGC&#30340;&#29616;&#29366;&#21450;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
AIGC In China: Current Developments And Future Outlook. (arXiv:2308.08451v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20013;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#35752;&#35770;&#20102;AIGC&#30340;&#22522;&#30784;&#25216;&#26415;&#12289;&#24066;&#22330;&#29366;&#20917;&#21644;&#21457;&#23637;&#36712;&#36857;&#65292;&#24182;&#37325;&#28857;&#24378;&#35843;&#20102;AIGC&#30340;&#29983;&#24577;&#24314;&#35774;&#12290;&#39044;&#27979;&#20102;&#34892;&#19994;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#24050;&#32463;&#22312;&#26085;&#24120;&#29983;&#27963;&#12289;&#24037;&#19994;&#21046;&#36896;&#21644;&#23398;&#26415;&#30028;&#31561;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#37492;&#20110;AIGC&#21457;&#23637;&#30340;&#20840;&#29699;&#36235;&#21183;&#21644;&#31454;&#20105;&#21147;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#20013;&#22269;&#22312;&#35813;&#39046;&#22495;&#30340;&#29616;&#29366;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#27010;&#36848;&#20102;AIGC&#30340;&#22522;&#30784;&#25216;&#26415;&#21644;&#24403;&#21069;&#24212;&#29992;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#20851;&#38190;&#35789;&#25628;&#32034;&#35782;&#21035;&#30456;&#20851;&#23398;&#26415;&#35770;&#25991;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#20013;&#22269;&#30340;AIGC&#24066;&#22330;&#29366;&#20917;&#12289;&#25919;&#31574;&#29615;&#22659;&#21644;&#21457;&#23637;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20840;&#38754;&#23457;&#35270;&#20102;AIGC&#20135;&#21697;&#21450;&#20854;&#30456;&#24212;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;AIGC&#30340;&#29983;&#24577;&#24314;&#35774;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;AIGC&#34892;&#19994;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#65292;&#24182;&#26681;&#25454;AIGC&#30340;&#31454;&#20105;&#24615;&#27934;&#23519;&#23637;&#26395;&#20102;&#34892;&#19994;&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing attention given to AI Generated Content (AIGC) has brought a profound impact on various aspects of daily life, industrial manufacturing, and the academic sector. Recognizing the global trends and competitiveness in AIGC development, this study aims to analyze China's current status in the field. The investigation begins with an overview of the foundational technologies and current applications of AIGC. Subsequently, the study delves into the market status, policy landscape, and development trajectory of AIGC in China, utilizing keyword searches to identify relevant scholarly papers. Furthermore, the paper provides a comprehensive examination of AIGC products and their corresponding ecosystem, emphasizing the ecological construction of AIGC. Finally, this paper discusses the challenges and risks faced by the AIGC industry while presenting a forward-looking perspective on the industry's future based on competitive insights in AIGC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniZoomer&#65292;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;M&#246;bius&#21464;&#25442;&#25972;&#21512;&#21040;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#22312;&#20840;&#21521;&#22270;&#20687;&#19978;&#36827;&#34892;&#31227;&#21160;&#21644;&#32553;&#25918;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#21464;&#25442;&#29305;&#24449;&#22270;&#65292;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#22686;&#21152;&#30340;&#36793;&#32536;&#26354;&#29575;&#24182;&#20943;&#36731;&#27169;&#31946;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#28151;&#21472;&#38382;&#39064;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.08114</link><description>&lt;p&gt;
OmniZoomer: &#23398;&#20064;&#22312;&#39640;&#20998;&#36776;&#29575;&#29699;&#38754;&#19978;&#31227;&#21160;&#21644;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution. (arXiv:2308.08114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniZoomer&#65292;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;M&#246;bius&#21464;&#25442;&#25972;&#21512;&#21040;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#22312;&#20840;&#21521;&#22270;&#20687;&#19978;&#36827;&#34892;&#31227;&#21160;&#21644;&#32553;&#25918;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#21464;&#25442;&#29305;&#24449;&#22270;&#65292;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#22686;&#21152;&#30340;&#36793;&#32536;&#26354;&#29575;&#24182;&#20943;&#36731;&#27169;&#31946;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#28151;&#21472;&#38382;&#39064;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#21521;&#22270;&#20687;&#65288;ODI&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22823;&#35270;&#37326;&#21487;&#20197;&#35753;&#35266;&#20247;&#22312;&#34394;&#25311;&#29616;&#23454;&#31561;&#27785;&#28024;&#24335;&#29615;&#22659;&#20013;&#33258;&#30001;&#36873;&#25321;&#35270;&#35282;&#12290;&#36890;&#24120;&#20351;&#29992;M&#246;bius&#21464;&#25442;&#22312;ODI&#19978;&#25552;&#20379;&#31227;&#21160;&#21644;&#32553;&#25918;&#30340;&#26426;&#20250;&#65292;&#20294;&#23558;&#20854;&#24212;&#29992;&#22312;&#22270;&#20687;&#32423;&#21035;&#19978;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#31946;&#25928;&#26524;&#21644;&#28151;&#21472;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;OmniZoomer&#65292;&#23558;M&#246;bius&#21464;&#25442;&#25972;&#21512;&#21040;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#22312;ODI&#19978;&#36827;&#34892;&#31227;&#21160;&#21644;&#32553;&#25918;&#12290;&#36890;&#36807;&#23398;&#20064;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#21508;&#31181;&#21464;&#25442;&#29305;&#24449;&#22270;&#65292;&#22686;&#24378;&#20102;&#32593;&#32476;&#22788;&#29702;&#22686;&#21152;&#30340;&#36793;&#32536;&#26354;&#29575;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27169;&#31946;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#28151;&#21472;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#24357;&#34917;&#25551;&#36848;&#26354;&#32447;&#25152;&#38656;&#20687;&#32032;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#22270;&#65292;&#24182;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
Omnidirectional images (ODIs) have become increasingly popular, as their large field-of-view (FoV) can offer viewers the chance to freely choose the view directions in immersive environments such as virtual reality. The M\"obius transformation is typically employed to further provide the opportunity for movement and zoom on ODIs, but applying it to the image level often results in blurry effect and aliasing problem. In this paper, we propose a novel deep learning-based approach, called \textbf{OmniZoomer}, to incorporate the M\"obius transformation into the network for movement and zoom on ODIs. By learning various transformed feature maps under different conditions, the network is enhanced to handle the increasing edge curvatures, which alleviates the blurry effect. Moreover, to address the aliasing problem, we propose two key components. Firstly, to compensate for the lack of pixels for describing curves, we enhance the feature maps in the high-resolution (HR) space and calculate the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#26469;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#21644;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07575</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#26041;&#24335;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Story Visualization by Online Text Augmentation with Context Memory. (arXiv:2308.07575v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#26469;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#21644;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#21487;&#35270;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#38590;&#28857;&#22312;&#20110;&#19981;&#20165;&#38656;&#35201;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21576;&#29616;&#35270;&#35273;&#32454;&#33410;&#65292;&#36824;&#38656;&#35201;&#23545;&#36328;&#22810;&#20010;&#21477;&#23376;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#20026;&#27599;&#20010;&#21477;&#23376;&#29983;&#25104;&#35821;&#20041;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#20294;&#22312;&#32473;&#23450;&#27573;&#33853;&#20013;&#32534;&#30721;&#19978;&#19979;&#25991;&#20197;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#35828;&#26381;&#21147;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#65292;&#27491;&#30830;&#30340;&#35282;&#33394;&#25110;&#36866;&#24403;&#30340;&#22330;&#26223;&#32972;&#26223;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#65292;&#29992;&#20110;&#21452;&#21521;Transformer&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#29983;&#25104;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25512;&#29702;&#20013;&#30340;&#35821;&#35328;&#21464;&#21270;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;Pororo-SV&#21644;Flintstones-SV&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;FID&#12289;&#23383;&#31526;...
&lt;/p&gt;
&lt;p&gt;
Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformers with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training, for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various evaluation metrics including FID, char
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CPEM&#30340;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#26469;&#25913;&#36827;&#34892;&#20026;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;&#23548;&#33322;&#21644;&#29289;&#20307;&#20132;&#20114;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07241</link><description>&lt;p&gt;
&#20855;&#26377;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#35268;&#21010;&#29992;&#20110;&#25351;&#23548;&#34892;&#20026;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents. (arXiv:2308.07241v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CPEM&#30340;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#26469;&#25913;&#36827;&#34892;&#20026;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;&#23548;&#33322;&#21644;&#29289;&#20307;&#20132;&#20114;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#25104;&#23478;&#21153;&#20219;&#21153;&#65288;&#20363;&#22914;&#8220;&#25343;&#19968;&#26479;&#27700;&#8221;&#65289;&#38656;&#35201;&#36890;&#36807;&#20445;&#25345;&#23545;&#31354;&#38388;&#23545;&#35937;&#30340;&#31354;&#38388;&#24067;&#23616;&#21644;&#20808;&#21069;&#34892;&#21160;&#30340;&#32467;&#26524;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;&#36880;&#27493;&#30340;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#34892;&#20026;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#27169;&#22411;&#26041;&#38754;&#32463;&#24120;&#20986;&#38169;&#65292;&#22240;&#20026;&#32570;&#20047;&#36825;&#31181;&#30693;&#35782;&#65292;&#32780;&#20381;&#36182;&#20110;&#19981;&#23436;&#32654;&#30340;&#23398;&#20064;&#30340;&#27169;&#20223;&#26234;&#33021;&#20307;&#25110;&#32773;&#27809;&#26377;&#20851;&#20110;&#20808;&#21069;&#34892;&#21160;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#30693;&#35782;&#30340;&#31639;&#27861;&#35268;&#21010;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CPEM&#65288;&#19978;&#19979;&#25991;&#24863;&#30693;&#35268;&#21010;&#22120;&#21644;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#65289;&#65292;&#23558;&#20808;&#21069;&#34892;&#21160;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#29615;&#22659;&#20013;&#29289;&#20307;&#30340;&#31354;&#38388;&#24067;&#23616;&#21644;&#29366;&#24577;&#65288;&#20363;&#22914;&#29289;&#20307;&#26159;&#21542;&#34987;&#31227;&#21160;&#65289;&#32467;&#21512;&#21040;&#24863;&#30693;&#27169;&#22411;&#20013;&#65292;&#20197;&#25913;&#36827;&#35270;&#35273;&#23548;&#33322;&#21644;&#29289;&#20307;&#20132;&#20114;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;CPEM&#22312;&#21508;&#31181;&#24230;&#37327;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#25104;&#21151;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accomplishing household tasks such as 'bringing a cup of water' requires planning step-by-step actions by maintaining knowledge about the spatial arrangement of objects and the consequences of previous actions. Perception models of the current embodied AI agents, however, often make mistakes due to a lack of such knowledge but rely on imperfect learning of imitating agents or an algorithmic planner without knowledge about the changed environment by the previous actions. To address the issue, we propose CPEM (Context-aware Planner and Environment-aware Memory) to incorporate the contextual information of previous actions for planning and maintaining spatial arrangement of objects with their states (e.g., if an object has been moved or not) in an environment to the perception model for improving both visual navigation and object interaction. We observe that CPEM achieves state-of-the-art task success performance in various metrics using a challenging interactive instruction following ben
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20856;&#22411;&#26680;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;&#65292;&#35299;&#20915;&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#34920;&#31034;&#20998;&#21106;&#21644;&#23884;&#20837;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20998;&#21106;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#26680;&#20197;&#21450;&#20856;&#22411;&#23398;&#20064;&#21644;&#21069;&#26223;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04952</link><description>&lt;p&gt;
&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#20856;&#22411;&#26680;&#23398;&#20064;&#19982;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation. (arXiv:2308.04952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20856;&#22411;&#26680;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;&#65292;&#35299;&#20915;&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#34920;&#31034;&#20998;&#21106;&#21644;&#23884;&#20837;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20998;&#21106;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#26680;&#20197;&#21450;&#20856;&#22411;&#23398;&#20064;&#21644;&#21069;&#26223;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#65288;GFSS&#65289;&#23558;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#65288;FSS&#65289;&#25193;&#23637;&#21040;&#35780;&#20272;&#36807;&#31243;&#20013;&#21516;&#26102;&#20998;&#21106;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#24050;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#39069;&#22806;&#30340;&#20998;&#25903;&#25110;&#20856;&#22411;&#32858;&#21512;&#26469;&#28040;&#38500;FSS&#30340;&#32422;&#26463;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#34920;&#31034;&#20998;&#21106;&#21644;&#23884;&#20837;&#20559;&#35265;&#65292;&#20005;&#37325;&#24433;&#21709;GFSS&#30340;&#24615;&#33021;&#65292;&#23578;&#26410;&#32508;&#21512;&#32771;&#34385;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20856;&#22411;&#26680;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#26680;&#26469;&#23545;&#27599;&#20010;&#31867;&#21035;&#36827;&#34892;&#20998;&#21106;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20856;&#22411;&#23398;&#20064;&#19982;&#22522;&#31867;&#26680;&#30340;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#36825;&#19982;&#23569;&#26679;&#26412;&#26032;&#31867;&#21035;&#30340;&#21407;&#22411;&#30693;&#35782;&#32858;&#21512;&#30456;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#19982;&#26465;&#20214;&#20559;&#24046;&#22522;&#20110;&#25512;&#29702;&#30340;&#21069;&#26223;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22359;&#65292;&#29992;&#20110;&#25191;&#34892;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot Semantic Segmentation (FSS) to simultaneously segment unseen classes and seen classes during evaluation. Previous works leverage additional branch or prototypical aggregation to eliminate the constrained setting of FSS. However, representation division and embedding prejudice, which heavily results in poor performance of GFSS, have not been synthetical considered. We address the aforementioned problems by jointing the prototypical kernel learning and open-set foreground perception. Specifically, a group of learnable kernels is proposed to perform segmentation with each kernel in charge of a stuff class. Then, we explore to merge the prototypical learning to the update of base-class kernels, which is consistent with the prototype knowledge aggregation of few-shot novel classes. In addition, a foreground contextual perception module cooperating with conditional bias based inference is adopted to perform class-agnostic as 
&lt;/p&gt;</description></item><item><title>Temporal DINO&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#35270;&#39057;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#21644;&#19968;&#20010;&#25945;&#24072;&#27169;&#22411;&#65292;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#36807;&#21435;&#24103;&#26469;&#23398;&#20064;&#26410;&#26469;&#24103;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#22686;&#24378;&#21160;&#20316;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#34892;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#22312;ROAD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.04589</link><description>&lt;p&gt;
Temporal DINO:&#19968;&#31181;&#22686;&#24378;&#21160;&#20316;&#39044;&#27979;&#30340;&#33258;&#30417;&#30563;&#35270;&#39057;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction. (arXiv:2308.04589v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04589
&lt;/p&gt;
&lt;p&gt;
Temporal DINO&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#35270;&#39057;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#21644;&#19968;&#20010;&#25945;&#24072;&#27169;&#22411;&#65292;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#36807;&#21435;&#24103;&#26469;&#23398;&#20064;&#26410;&#26469;&#24103;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#22686;&#24378;&#21160;&#20316;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#34892;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#22312;ROAD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#39044;&#27979;&#30340;&#26032;&#20852;&#39046;&#22495;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#27963;&#21160;&#20998;&#26512;&#21644;&#20154;&#26426;&#20132;&#20114;&#12290;&#23613;&#31649;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35270;&#39057;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39640;&#32500;&#24230;&#12289;&#22797;&#26434;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#21160;&#20316;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#30417;&#30563;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;DINO&#65288;&#26080;&#26631;&#31614;&#30340;&#33258;&#33976;&#39311;&#65289;&#21551;&#21457;&#30340;&#22686;&#24378;&#21160;&#20316;&#39044;&#27979;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#35270;&#39057;&#31574;&#30053;&#65292;&#31216;&#20026;Temporal-DINO&#12290;&#35813;&#31574;&#30053;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#23398;&#29983;&#8221;&#22788;&#29702;&#36807;&#21435;&#30340;&#24103;&#65292;&#19968;&#20010;&#8220;&#25945;&#24072;&#8221;&#21516;&#26102;&#22788;&#29702;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#24103;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25945;&#24072;&#36890;&#36807;&#20165;&#35266;&#23519;&#36807;&#21435;&#30340;&#24103;&#26469;&#25351;&#23548;&#23398;&#29983;&#23398;&#20064;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#12290;&#35813;&#31574;&#30053;&#22312;ROAD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#34892;&#21160;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emerging field of action prediction plays a vital role in various computer vision applications such as autonomous driving, activity analysis and human-computer interaction. Despite significant advancements, accurately predicting future actions remains a challenging problem due to high dimensionality, complex dynamics and uncertainties inherent in video data. Traditional supervised approaches require large amounts of labelled data, which is expensive and time-consuming to obtain. This paper introduces a novel self-supervised video strategy for enhancing action prediction inspired by DINO (self-distillation with no labels). The Temporal-DINO approach employs two models; a 'student' processing past frames; and a 'teacher' processing both past and future frames, enabling a broader temporal context. During training, the teacher guides the student to learn future context by only observing past frames. The strategy is evaluated on ROAD dataset for the action prediction downstream task usi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20026;&#20160;&#20040;&#23578;&#26410;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#65292;&#24182;&#25351;&#20986;&#20102;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21644;&#36164;&#37329;&#25512;&#24191;&#19981;&#36275;&#26159;&#21046;&#32422;AGI&#21457;&#23637;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#23454;&#29616;&#20154;&#31867;&#36866;&#24212;&#33021;&#21147;&#21644;&#33258;&#20027;&#23398;&#20064;&#25152;&#38656;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03598</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#25105;&#20204;&#23578;&#26410;&#25317;&#26377;AGI
&lt;/p&gt;
&lt;p&gt;
Why We Don't Have AGI Yet. (arXiv:2308.03598v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20026;&#20160;&#20040;&#23578;&#26410;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#65292;&#24182;&#25351;&#20986;&#20102;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21644;&#36164;&#37329;&#25512;&#24191;&#19981;&#36275;&#26159;&#21046;&#32422;AGI&#21457;&#23637;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#23454;&#29616;&#20154;&#31867;&#36866;&#24212;&#33021;&#21147;&#21644;&#33258;&#20027;&#23398;&#20064;&#25152;&#38656;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2002&#24180;&#37325;&#26032;&#38416;&#36848;&#20102;AI&#30340;&#21407;&#22987;&#24895;&#26223;&#65292;&#31216;&#20043;&#20026;&#8220;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#8221;&#25110;AGI&#12290;&#36825;&#19968;&#24895;&#26223;&#26159;&#26500;&#24314;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#35299;&#20915;&#38382;&#39064;&#30340;&#8220;&#24605;&#32771;&#26426;&#22120;&#8221;&#35745;&#31639;&#26426;&#31995;&#32479;&#12290;&#36825;&#19982;&#20960;&#21313;&#24180;&#26469;&#20960;&#20046;&#25152;&#26377;&#20154;&#22312;&#35813;&#39046;&#22495;&#23454;&#36341;&#30340;&#8220;&#29421;&#20041;AI&#8221;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#34429;&#28982;&#26377;&#20960;&#20010;&#22823;&#35268;&#27169;&#30340;&#39033;&#30446;&#21517;&#20041;&#19978;&#22312;&#33268;&#21147;&#20110;AGI&#30340;&#30740;&#21457;&#65288;&#23588;&#20854;&#26159;DeepMind&#65289;&#65292;&#20294;&#22312;&#32431;&#31929;&#19987;&#27880;&#30340;AGI&#21457;&#23637;&#39046;&#22495;&#65292;&#36164;&#37329;&#21644;&#25512;&#24191;&#24182;&#19981;&#20805;&#36275;&#12290;&#36825;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#30495;&#27491;&#30340;AGI&#21487;&#20197;&#20026;&#20154;&#31867;&#24102;&#26469;&#24040;&#22823;&#30340;&#20215;&#20540;&#12290;&#38500;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#32570;&#20047;&#21162;&#21147;&#20043;&#22806;&#65292;&#36824;&#23384;&#22312;&#20111;&#27424;&#30340;&#29702;&#35770;&#21644;&#26041;&#27861;&#19978;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#24378;&#35843;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#26080;&#27861;&#23454;&#29616;AGI&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#36866;&#24212;&#33021;&#21147;&#21644;&#33258;&#20027;&#23398;&#20064;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#31038;&#20250;&#25216;&#26415;&#21457;&#23637;&#26041;&#38754;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The original vision of AI was re-articulated in 2002 via the term 'Artificial General Intelligence' or AGI. This vision is to build 'Thinking Machines' computer systems that can learn, reason, and solve problems similar to the way humans do. This is in stark contrast to the 'Narrow AI' approach practiced by almost everyone in the field over the many decades. While several large-scale efforts have nominally been working on AGI (most notably DeepMind), the field of pure focused AGI development has not been well funded or promoted. This is surprising given the fantastic value that true AGI can bestow on humanity. In addition to the dearth of effort in this field, there are also several theoretical and methodical missteps that are hampering progress. We highlight why purely statistical approaches are unlikely to lead to AGI, and identify several crucial cognitive abilities required to achieve human-like adaptability and autonomous learning. We conclude with a survey of socio-technical fa
&lt;/p&gt;</description></item><item><title>PaniniQA&#26159;&#19968;&#20010;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#24739;&#32773;&#29702;&#35299;&#20182;&#20204;&#30340;&#20986;&#38498;&#25351;&#23548;&#65292;&#25552;&#20379;&#21450;&#26102;&#30340;&#21453;&#39304;&#20197;&#32416;&#27491;&#24739;&#32773;&#30340;&#35823;&#35299;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#20114;&#21160;&#25552;&#39640;&#24739;&#32773;&#23545;&#21307;&#30103;&#25351;&#23548;&#30340;&#25484;&#25569;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.03253</link><description>&lt;p&gt;
PaniniQA: &#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#31572;&#25552;&#21319;&#24739;&#32773;&#25945;&#32946;
&lt;/p&gt;
&lt;p&gt;
PaniniQA: Enhancing Patient Education Through Interactive Question Answering. (arXiv:2308.03253v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03253
&lt;/p&gt;
&lt;p&gt;
PaniniQA&#26159;&#19968;&#20010;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#24739;&#32773;&#29702;&#35299;&#20182;&#20204;&#30340;&#20986;&#38498;&#25351;&#23548;&#65292;&#25552;&#20379;&#21450;&#26102;&#30340;&#21453;&#39304;&#20197;&#32416;&#27491;&#24739;&#32773;&#30340;&#35823;&#35299;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#20114;&#21160;&#25552;&#39640;&#24739;&#32773;&#23545;&#21307;&#30103;&#25351;&#23548;&#30340;&#25484;&#25569;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#38376;&#25143;&#20801;&#35768;&#20986;&#38498;&#24739;&#32773;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#35775;&#38382;&#20182;&#20204;&#30340;&#20010;&#24615;&#21270;&#20986;&#38498;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24739;&#32773;&#24456;&#38590;&#29702;&#35299;&#25110;&#35760;&#20303;&#20182;&#20204;&#30340;&#20986;&#38498;&#25351;&#23548;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PaniniQA&#65292;&#19968;&#20010;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#24739;&#32773;&#29702;&#35299;&#20182;&#20204;&#30340;&#20986;&#38498;&#25351;&#23548;&#12290;PaniniQA&#39318;&#20808;&#20174;&#24739;&#32773;&#30340;&#20986;&#38498;&#25351;&#23548;&#20013;&#35782;&#21035;&#37325;&#35201;&#30340;&#20020;&#24202;&#20869;&#23481;&#65292;&#28982;&#21518;&#25552;&#20986;&#24739;&#32773;&#29305;&#23450;&#30340;&#25945;&#32946;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;PaniniQA&#36824;&#37197;&#22791;&#20102;&#31572;&#26696;&#39564;&#35777;&#21151;&#33021;&#65292;&#21487;&#20197;&#21450;&#26102;&#21453;&#39304;&#26469;&#32416;&#27491;&#24739;&#32773;&#30340;&#35823;&#35299;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PaniniQA&#33021;&#22815;&#36890;&#36807;&#26377;&#25928;&#30340;&#20114;&#21160;&#25552;&#39640;&#24739;&#32773;&#23545;&#21307;&#30103;&#25351;&#23548;&#30340;&#25484;&#25569;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patient portal allows discharged patients to access their personalized discharge instructions in electronic health records (EHRs). However, many patients have difficulty understanding or memorizing their discharge instructions. In this paper, we present PaniniQA, a patient-centric interactive question answering system designed to help patients understand their discharge instructions. PaniniQA first identifies important clinical content from patients' discharge instructions and then formulates patient-specific educational questions. In addition, PaniniQA is also equipped with answer verification functionality to provide timely feedback to correct patients' misunderstandings. Our comprehensive automatic and human evaluation results demonstrate our PaniniQA is capable of improving patients' mastery of their medical instructions through effective interactions
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2308.03202</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Source-free Domain Adaptive Human Pose Estimation. (arXiv:2308.03202v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#24191;&#27867;&#24212;&#29992;&#20110;&#36816;&#21160;&#20998;&#26512;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#34394;&#25311;&#29616;&#23454;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#24040;&#22823;&#24320;&#38144;&#23545;&#23039;&#21183;&#20272;&#35745;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23039;&#21183;&#20272;&#35745;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;(DA)&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;HPE&#30340;DA&#26041;&#27861;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#24573;&#30053;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21517;&#20026;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;HPE&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;HPE&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;&#26694;&#26550;&#65306;&#28304;&#27169;&#22411;&#12289;&#20013;&#38388;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#65292;&#20174;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#35282;&#24230;&#25506;&#32034;&#35813;&#20219;&#21153;&#12290;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26041;&#27861;&#35299;&#20915;&#20102;&#29289;&#27969;&#20013;&#20197;&#23454;&#38469;&#20026;&#23548;&#21521;&#30340;&#35013;&#31665;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#35299;&#20915;&#19981;&#21516;&#32500;&#24230;&#38382;&#39064;&#21644;&#20855;&#26377;&#19981;&#21516;&#35201;&#27714;&#30340;&#31665;&#23376;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02787</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26041;&#27861;&#35299;&#20915;&#20197;&#29289;&#27969;&#20026;&#23548;&#21521;&#30340;&#35013;&#31665;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Logistic-Oriented Bin Packing Problems Through a Hybrid Quantum-Classical Approach. (arXiv:2308.02787v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26041;&#27861;&#35299;&#20915;&#20102;&#29289;&#27969;&#20013;&#20197;&#23454;&#38469;&#20026;&#23548;&#21521;&#30340;&#35013;&#31665;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#35299;&#20915;&#19981;&#21516;&#32500;&#24230;&#38382;&#39064;&#21644;&#20855;&#26377;&#19981;&#21516;&#35201;&#27714;&#30340;&#31665;&#23376;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35013;&#31665;&#38382;&#39064;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24037;&#19994;&#24212;&#29992;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#20107;&#23454;&#19978;&#65292;&#23558;&#29289;&#21697;&#39640;&#25928;&#22320;&#35013;&#31665;&#26159;&#35768;&#22810;&#29289;&#27969;&#20844;&#21496;&#38754;&#20020;&#30340;&#26368;&#33392;&#24040;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#20063;&#26159;&#38477;&#20302;&#23384;&#20648;&#25104;&#26412;&#25110;&#25913;&#21892;&#36710;&#36742;&#31354;&#38388;&#20998;&#37197;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#25105;&#20204;&#20808;&#21069;&#21457;&#34920;&#30340;&#37327;&#23376;-&#32463;&#20856;&#26694;&#26550;Q4RealBPP&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#35299;&#20915;&#20197;&#23454;&#38469;&#20026;&#23548;&#21521;&#30340;&#35013;&#31665;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20197;&#19979;&#29305;&#28857;&#65306;i&#65289;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#31665;&#23376;&#65292;ii&#65289;&#25193;&#23637;&#26694;&#26550;&#20197;&#35299;&#20915;&#19977;&#32500;&#12289;&#20108;&#32500;&#21644;&#19968;&#32500;&#38382;&#39064;&#65292;iii&#65289;&#29289;&#21697;&#19982;&#31665;&#23376;&#20851;&#32852;&#30340;&#35201;&#27714;&#65292;iv&#65289;&#20132;&#20184;&#20248;&#20808;&#32423;&#12290;&#26412;&#25991;&#23545;&#25152;&#26377;&#36825;&#20123;&#29305;&#28857;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#21450;Q4RealBPP&#35299;&#20915;&#20197;&#23454;&#38469;&#20026;&#23548;&#21521;&#30340;&#35013;&#31665;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bin Packing Problem is a classic problem with wide industrial applicability. In fact, the efficient packing of items into bins is one of the toughest challenges in many logistic corporations and is a critical issue for reducing storage costs or improving vehicle space allocation. In this work, we resort to our previously published quantum-classical framework known as Q4RealBPP, and elaborate on the solving of real-world oriented instances of the Bin Packing Problem. With this purpose, this paper gravitates on the following characteristics: i) the existence of heterogeneous bins, ii) the extension of the framework to solve not only three-dimensional, but also one- and two-dimensional instances of the problem, iii) requirements for item-bin associations, and iv) delivery priorities. All these features have been tested in this paper, as well as the ability of Q4RealBPP to solve real-world oriented instances.
&lt;/p&gt;</description></item><item><title>NeRFs&#26159;&#35270;&#22270;&#21512;&#25104;&#21644;&#30456;&#20851;&#38382;&#39064;&#20013;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26597;&#35810;&#33719;&#21462;&#20307;&#31215;&#21442;&#25968;&#26469;&#25551;&#36848;&#36830;&#32493;&#20307;&#31215;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.02751</link><description>&lt;p&gt;
NeRFs: &#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
NeRFs: The Search for the Best 3D Representation. (arXiv:2308.02751v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02751
&lt;/p&gt;
&lt;p&gt;
NeRFs&#26159;&#35270;&#22270;&#21512;&#25104;&#21644;&#30456;&#20851;&#38382;&#39064;&#20013;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26597;&#35810;&#33719;&#21462;&#20307;&#31215;&#21442;&#25968;&#26469;&#25551;&#36848;&#36830;&#32493;&#20307;&#31215;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#24050;&#25104;&#20026;&#35270;&#22270;&#21512;&#25104;&#25110;&#22522;&#20110;&#22270;&#20687;&#28210;&#26579;&#31561;&#38382;&#39064;&#30340;&#39318;&#36873;&#34920;&#31034;&#26041;&#27861;&#65292;&#20063;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;NeRFs&#36890;&#36807;&#26597;&#35810;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#35270;&#22270;&#30456;&#20851;&#36752;&#23556;&#21644;&#20307;&#31215;&#23494;&#24230;&#31561;&#20307;&#31215;&#21442;&#25968;&#65292;&#23558;&#22330;&#26223;&#34920;&#31034;&#20026;&#36830;&#32493;&#30340;&#20307;&#31215;&#12290;&#35813;&#34920;&#31034;&#26041;&#27861;&#24050;&#24191;&#27867;&#24212;&#29992;&#65292;&#27599;&#24180;&#26377;&#25968;&#21315;&#31687;&#35770;&#25991;&#22312;&#20854;&#22522;&#30784;&#19978;&#25193;&#23637;&#25110;&#30456;&#20851;&#30740;&#31350;&#65292;&#22810;&#20301;&#20316;&#32773;&#21644;&#32593;&#31449;&#25552;&#20379;&#27010;&#36848;&#21644;&#35843;&#30740;&#65292;&#24182;&#26377;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#21019;&#19994;&#20844;&#21496;&#12290;&#26412;&#25991;&#31616;&#35201;&#22238;&#39038;&#20102;NeRFs&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;&#38271;&#36798;&#19977;&#21313;&#24180;&#30340;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#26041;&#27861;&#20197;&#21450;&#26368;&#32456;&#24341;&#20986;NeRFs&#35770;&#25991;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe n
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35299;&#20915;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00946</link><description>&lt;p&gt;
&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Teaching Smaller Language Models To Generalise To Unseen Compositional Questions. (arXiv:2308.00946v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00946
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35299;&#20915;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#19968;&#20010;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#22238;&#31572;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#20986;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#26368;&#22810;93&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#22521;&#20859;&#22810;&#26679;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#20010;&#23494;&#38598;&#30340;&#26816;&#32034;&#31995;&#32479;&#65292;&#26088;&#22312;&#26816;&#32034;&#19968;&#32452;&#35777;&#25454;&#24615;&#30340;&#27573;&#33853;&#29255;&#27573;&#12290;&#22312;&#38382;&#31572;&#26041;&#38754;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#20040;&#36890;&#36807;&#38024;&#23545;&#38750;&#24120;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#23454;&#29616;&#38646;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#26377;&#26102;&#32467;&#21512;&#20449;&#24687;&#26816;&#32034;&#36827;&#34892;&#12290;&#25105;&#20204;&#20851;&#27880;&#36739;&#23569;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#21363;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#23545;&#20110;&#19981;&#23384;&#22312;&#36275;&#22815;&#20449;&#24687;&#26469;&#22238;&#31572;&#29305;&#23450;&#38382;&#39064;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#26816;&#32034;&#26102;&#65292;&#33021;&#21542;&#23454;&#29616;&#38646;&#26679;&#26412;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#20026;&#22810;&#26679;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65288;StrategyQA&#65292;CommonsenseQA&#65292;IIRC&#65292;DROP&#65292;Musique&#21644;ARC-DA&#65289;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We equip a smaller Language Model to generalise to answering challenging compositional questions that have not been seen in training. To do so we propose a combination of multitask supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities, and a dense retrieval system that aims to retrieve a set of evidential paragraph fragments. Recent progress in question-answering has been achieved either through prompting methods against very large pretrained Language Models in zero or few-shot fashion, or by fine-tuning smaller models, sometimes in conjunction with information retrieval. We focus on the less explored question of the extent to which zero-shot generalisation can be enabled in smaller models with retrieval against a corpus within which sufficient information to answer a particular question may not exist. We establish strong baselines in this setting for diverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and ARC-DA), and show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00158</link><description>&lt;p&gt;
&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#30340;&#23436;&#32654;&#36136;&#37327;&#27573;&#33853;&#65306;&#26159;&#21542;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#32534;&#36753;&#36317;&#31163;&#27169;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#65288;TQE&#65289;&#26159;&#23558;&#36755;&#20986;&#32763;&#35793;&#37096;&#32626;&#21040;&#20351;&#29992;&#20013;&#20043;&#21069;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290; TQE&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#30340;&#36136;&#37327;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#21442;&#32771;&#32763;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20026;TQE&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;Fine-Tune&#12290;&#25105;&#20204;&#20197;ChatGPT&#20026;&#20363;&#65292;&#23558;TQE&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;&#33521;&#24847;&#21644;&#33521;&#24503;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;ChatGPT&#30340;API Fine-Tuned&#21487;&#20197;&#22312;&#39044;&#27979;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#21363;&#26159;&#21542;&#38656;&#35201;&#32534;&#36753;&#32763;&#35793;&#65292;&#20294;&#32943;&#23450;&#26377;&#25913;&#36827;&#20934;&#30830;&#24615;&#30340;&#31354;&#38388;&#12290;&#33521;&#24847;&#21452;&#35821;&#25688;&#35201;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.00121</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#65306;AI&#20316;&#20026;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23433;&#20840;&#27979;&#35797;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28183;&#36879;&#27979;&#35797;&#26159;&#19968;&#39033;&#38656;&#35201;&#39640;&#27700;&#24179;&#19987;&#19994;&#30693;&#35782;&#30340;&#27963;&#21160;&#65292;&#24182;&#28041;&#21450;&#35768;&#22810;&#25163;&#21160;&#27979;&#35797;&#21644;&#20998;&#26512;&#27493;&#39588;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29992;&#20363;&#65306;&#29992;&#20110;&#23433;&#20840;&#27979;&#35797;&#20219;&#21153;&#30340;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#22312;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#20013;&#36827;&#34892;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#38381;&#29615;&#21453;&#39304;&#65292;&#23558;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20302;&#32423;&#25805;&#20316;&#19982;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#65288;&#36890;&#36807;SSH&#36830;&#25509;&#65289;&#30456;&#36830;&#65292;&#24182;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#34394;&#25311;&#26426;&#29366;&#24577;&#20197;&#23547;&#25214;&#28431;&#27934;&#65292;&#24182;&#25552;&#20379;&#20855;&#20307;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#36884;&#24452;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
&lt;/p&gt;</description></item><item><title>&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#29992;&#26469;&#35299;&#20915;&#20266;&#26631;&#31614;&#22122;&#22768;&#21644;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;TMR&#38454;&#27573;&#36890;&#36807;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#65292;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#23398;&#21040;&#30340;&#27169;&#24335;&#65307;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.13755</link><description>&lt;p&gt;
TMR-RD: &#29992;&#20110;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;&#21644;&#34920;&#31034;&#20998;&#27495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection. (arXiv:2307.13755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13755
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#29992;&#26469;&#35299;&#20915;&#20266;&#26631;&#31614;&#22122;&#22768;&#21644;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;TMR&#38454;&#27573;&#36890;&#36807;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#65292;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#23398;&#21040;&#30340;&#27169;&#24335;&#65307;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;(SSOD)&#21487;&#20197;&#23558;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#29616;&#26377;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#26159;&#26368;&#36817;&#30340;SSOD&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#20266;&#26631;&#31614;&#22122;&#22768;/&#35823;&#23548;&#12289;&#32463;&#20856;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;(EMA)&#31574;&#30053;&#21644;&#21518;&#26399;&#35757;&#32451;&#20013;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#32463;&#20856;EMA&#30340;&#23616;&#38480;&#24615;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;TMR&#38454;&#27573;&#20248;&#21270;&#20102;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#65292;&#20197;&#31934;&#21270;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#36825;&#20123;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#32423;&#36830;&#22238;&#24402;&#26469;&#29983;&#25104;... (&#25688;&#35201;&#26410;&#23436;&#25972;&#25552;&#20379;)
&lt;/p&gt;
&lt;p&gt;
Semi-supervised object detection (SSOD) can incorporate limited labeled data and large amounts of unlabeled data to improve the performance and generalization of existing object detectors. Despite many advances, recent SSOD methods are still challenged by noisy/misleading pseudo-labels, classical exponential moving average (EMA) strategy, and the consensus of Teacher-Student models in the latter stages of training. This paper proposes a novel training-based model refinement (TMR) stage and a simple yet effective representation disagreement (RD) strategy to address the limitations of classical EMA and the consensus problem. The TMR stage of Teacher-Student models optimizes the lightweight scaling operation to refine the model's weights and prevent overfitting or forgetting learned patterns from unlabeled data. Meanwhile, the RD strategy helps keep these models diverged to encourage the student model to explore complementary representations. In addition, we use cascade regression to gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#39764;&#26041;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;PDDL&#35821;&#35328;&#65292;&#20351;&#20854;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#21644;&#26131;&#35835;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepCubeA&#21487;&#20197;&#35299;&#20915;&#25152;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#39764;&#26041;&#38382;&#39064;&#65292;&#20294;&#21482;&#26377;18&#65285;&#26159;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13552</link><description>&lt;p&gt;
&#29992;&#39046;&#22495;&#26080;&#20851;&#35268;&#21010;&#22120;&#21644;&#26631;&#20934;&#34920;&#31034;&#35299;&#20915;&#39764;&#26041;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Solving the Rubik's Cube with Domain-Independent Planners Using Standard Representations. (arXiv:2307.13552v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#39764;&#26041;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;PDDL&#35821;&#35328;&#65292;&#20351;&#20854;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#21644;&#26131;&#35835;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepCubeA&#21487;&#20197;&#35299;&#20915;&#25152;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#39764;&#26041;&#38382;&#39064;&#65292;&#20294;&#21482;&#26377;18&#65285;&#26159;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39764;&#26041;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#65292;&#24050;&#32463;&#28608;&#21457;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#39640;&#25928;&#30340;&#26367;&#20195;&#34920;&#31034;&#21644;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#39764;&#26041;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;&#27969;&#34892;&#30340;PDDL&#35821;&#35328;&#65292;&#20351;&#24471;&#39046;&#22495;&#23545;PDDL&#35268;&#21010;&#22120;&#12289;&#31454;&#36187;&#21644;&#30693;&#35782;&#24037;&#31243;&#24037;&#20855;&#26356;&#21152;&#21487;&#35775;&#38382;&#21644;&#26131;&#35835;&#12290;&#28982;&#21518;&#25105;&#20204;&#27604;&#36739;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19968;&#20010;&#21487;&#27604;&#36739;&#30340;&#23454;&#39564;&#20013;&#65292;DeepCubeA&#21487;&#20197;&#35299;&#20915;&#25152;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#65292;&#23613;&#31649;&#21482;&#26377;18&#65285;&#26159;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rubik's Cube (RC) is a well-known and computationally challenging puzzle that has motivated AI researchers to explore efficient alternative representations and problem-solving methods. The ideal situation for planning here is that a problem be solved optimally and efficiently represented in a standard notation using a general-purpose solver and heuristics. The fastest solver today for RC is DeepCubeA with a custom representation, and another approach is with Scorpion planner with State-Action-Space+ (SAS+) representation. In this paper, we present the first RC representation in the popular PDDL language so that the domain becomes more accessible to PDDL planners, competitions, and knowledge engineering tools, and is more human-readable. We then bridge across existing approaches and compare performance. We find that in one comparable experiment, DeepCubeA solves all problems with varying complexities, albeit only 18\% are optimal plans. For the same problem set, Scorpion with SAS+ repre
&lt;/p&gt;</description></item><item><title>RLCD&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#33976;&#39311;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#12290;&#22312;&#22810;&#20010;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;RLCD&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.12950</link><description>&lt;p&gt;
RLCD: &#22522;&#20110;&#23545;&#27604;&#33976;&#39311;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment. (arXiv:2307.12950v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12950
&lt;/p&gt;
&lt;p&gt;
RLCD&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#33976;&#39311;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#12290;&#22312;&#22810;&#20010;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;RLCD&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Reinforcement Learning from Contrast Distillation (RLCD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#38656;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21363;&#21487;&#20351;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#30340;&#23545;&#40784;&#12290;RLCD&#20351;&#29992;&#27169;&#25311;&#30340;&#20559;&#22909;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#23545;&#21253;&#21547;&#20102;&#39640;&#36136;&#37327;&#21644;&#20302;&#36136;&#37327;&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#20351;&#29992;&#23545;&#27604;&#30340;&#27491;&#36127;&#25552;&#31034;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#22522;&#30784;&#30340;&#26080;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;RLCD&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#23545;&#40784;&#20219;&#21153;&#65288;&#26080;&#23475;&#24615;&#12289;&#26377;&#29992;&#24615;&#21644;&#25925;&#20107;&#22823;&#32434;&#29983;&#25104;&#65289;&#20197;&#21450;7B&#21644;30B&#27169;&#22411;&#35268;&#27169;&#30340;&#20559;&#22909;&#25968;&#25454;&#27169;&#25311;&#19978;&#65292;&#37117;&#20248;&#20110;RLAIF (Bai&#31561;&#20154;&#65292;2022b)&#21644;&#19978;&#19979;&#25991;&#33976;&#39311; (Huang&#31561;&#20154;&#65292;2022) &#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Reinforcement Learning from Contrast Distillation (RLCD), a method for aligning language models to follow natural language principles without using human feedback. RLCD trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7B and 30B model scales for preference data simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hindsight-DICE&#31639;&#27861;&#65292;&#21033;&#29992;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#25913;&#21892;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11897</link><description>&lt;p&gt;
Hindsight-DICE&#65306;&#31283;&#23450;&#20449;&#29992;&#20998;&#37197;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning. (arXiv:2307.11897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hindsight-DICE&#31639;&#27861;&#65292;&#21033;&#29992;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#25913;&#21892;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#29615;&#22659;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#35780;&#20272;&#21453;&#39304;&#26469;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#34892;&#20026;&#30340;&#38271;&#26102;&#38388;&#36712;&#36857;&#20165;&#20197;&#19968;&#20010;&#32456;&#27490;&#20449;&#21495;&#26631;&#35760;&#65292;&#23548;&#33268;&#35266;&#23519;&#21040;&#38750;&#24179;&#20961;&#22870;&#21169;&#21644;&#35302;&#21457;&#27492;&#31867;&#21453;&#39304;&#30340;&#20010;&#20307;&#27493;&#39588;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#26102;&#38388;&#24310;&#36831;&#12290;&#35299;&#20915;&#36825;&#31181;&#20449;&#29992;&#20998;&#37197;&#25361;&#25112;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#26469;&#26174;&#33879;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#22788;&#29702;&#12290;&#34429;&#28982;&#20351;&#29992;&#25152;&#35859;&#30340;&#20107;&#21518;&#31574;&#30053;&#20026;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#36820;&#22238;&#36820;&#22238;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26426;&#21046;&#65292;&#20294;&#26159;&#31616;&#21333;&#22320;&#24212;&#29992;&#37325;&#35201;&#25277;&#26679;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#25110;&#36807;&#24230;&#28382;&#21518;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, engendering a significant temporal delay between the observation of non-trivial reward and the individual steps of behavior culpable for eliciting such feedback. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning and, in this work, we capitalize on existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the handling of credit assignment with policy-gradient methods. While the use of so-called hindsight policies offers a principled mechanism for reweighting on-policy data by saliency to the observed trajectory return, naively applying importance sampling results in unstable or excessively lagged learning.
&lt;/p&gt;</description></item><item><title>Ethosight&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#32852;&#24230;&#35745;&#31639;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#32454;&#24494;&#34892;&#20026;&#21644;&#22330;&#26223;&#32454;&#33410;&#30340;&#20934;&#30830;&#24863;&#30693;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#39044;&#20808;&#23384;&#22312;&#31526;&#21495;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.10577</link><description>&lt;p&gt;
Ethosight: &#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23884;&#20837;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#32852;&#24230;&#24230;&#37327;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#36827;&#34892;&#32454;&#33268;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning. (arXiv:2307.10577v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10577
&lt;/p&gt;
&lt;p&gt;
Ethosight&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#32852;&#24230;&#35745;&#31639;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#32454;&#24494;&#34892;&#20026;&#21644;&#22330;&#26223;&#32454;&#33410;&#30340;&#20934;&#30830;&#24863;&#30693;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#39044;&#20808;&#23384;&#22312;&#31526;&#21495;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21162;&#21147;&#26469;&#36827;&#34892;&#25968;&#25454;&#33719;&#21462;&#21644;&#39564;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;&#26816;&#27979;&#32454;&#24494;&#30340;&#34892;&#20026;&#32454;&#33410;&#25110;&#20107;&#20214;&#26102;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21306;&#20998;&#24120;&#35268;&#34892;&#20026;&#21644;&#28508;&#22312;&#39118;&#38505;&#30340;&#22256;&#38590;&#65292;&#22914;&#21306;&#20998;&#24120;&#35268;&#36141;&#29289;&#21644;&#28508;&#22312;&#25170;&#31363;&#65292;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Ethosight&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#12290;Ethosight&#28040;&#38500;&#20102;&#23545;&#39044;&#20808;&#23384;&#22312;&#30340;&#31526;&#21495;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#20174;&#29992;&#25143;&#38656;&#27714;&#21644;&#24863;&#20852;&#36259;&#30340;&#35821;&#20041;&#30693;&#35782;&#20986;&#21457;&#36827;&#34892;&#33258;&#20027;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#26631;&#31614;&#20851;&#32852;&#24230;&#35745;&#31639;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#24490;&#29615;&#65292;Ethosight&#25512;&#26029;&#22330;&#26223;&#32454;&#33410;&#24182;&#36845;&#20195;&#22320;&#20248;&#21270;&#26631;&#31614;&#38598;&#12290;&#25512;&#29702;&#26426;&#21046;&#21487;&#20197;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;GPT4&#12289;&#31526;&#21495;&#25512;&#29702;&#22120;&#22914;OpenNARS&#25110;&#28151;&#21512;&#31995;&#32479;&#12290;Ethosight&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;ImageBind&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional computer vision models often require extensive manual effort for data acquisition and validation, particularly when detecting subtle behavioral nuances or events. The difficulty in distinguishing routine behaviors from potential risks in real-world applications, like differentiating routine shopping from potential shoplifting, further complicates the process.  We present Ethosight, a novel zero-shot computer vision algorithm. Ethosight eradicates the need for pre-existing symbolic knowledge, initiating from a clean slate based on user requirements and semantic knowledge of interest. Using localized label affinity calculations and a reasoning-guided iterative learning loop, Ethosight infers scene details and iteratively refines the label set. Reasoning mechanisms can be derived from large language models like GPT4, symbolic reasoners like OpenNARS, or hybrid systems.  Ethosight further capitalizes on the capabilities of a pre-trained multi-modal model, ImageBind, generating 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.07887</link><description>&lt;p&gt;
&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#65306;&#19968;&#20010;&#31614;&#21517;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#26512;&#25195;&#25551;&#25991;&#26723;&#26102;&#65292;&#25163;&#20889;&#25991;&#26412;&#21487;&#33021;&#35206;&#30422;&#25171;&#21360;&#25991;&#26412;&#12290;&#36825;&#22312;&#25991;&#26723;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21644;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#36896;&#25104;&#22256;&#38590;&#65292;&#24182;&#19988;&#36827;&#32780;&#24433;&#21709;&#21040;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#20165;&#20851;&#27880;&#25163;&#20889;&#25991;&#26412;&#30340;&#20108;&#20998;&#31867;&#65292;&#35201;&#20040;&#36827;&#34892;&#19977;&#31867;&#25991;&#26723;&#30340;&#20998;&#21106;&#65292;&#21363;&#25163;&#20889;&#12289;&#25171;&#21360;&#21644;&#32972;&#26223;&#20687;&#32032;&#30340;&#35782;&#21035;&#12290;&#36825;&#23548;&#33268;&#25163;&#20889;&#21644;&#25171;&#21360;&#37325;&#21472;&#30340;&#20687;&#32032;&#21482;&#34987;&#20998;&#37197;&#21040;&#19968;&#20010;&#31867;&#21035;&#20013;&#65292;&#22240;&#27492;&#22312;&#21478;&#19968;&#20010;&#31867;&#21035;&#20013;&#19981;&#34987;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#30446;&#26631;&#26159;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#25552;&#39640;&#37325;&#21472;&#37096;&#20998;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#33258;&#30495;&#23454;&#30340;&#27861;&#24459;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.07870</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#21270;&#35282;&#24230;&#30340;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07870
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#35748;&#20026;LLMs&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#21472;&#21152;&#12290;LLMs&#34920;&#29616;&#20986;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22522;&#20110;&#20135;&#29983;&#30340;&#35282;&#24230;&#32780;&#25913;&#21464;&#65288;&#19982;&#20154;&#31867;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#36890;&#24120;&#20855;&#26377;&#26356;&#19968;&#33268;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35282;&#24230;&#21487;&#25511;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#12289;VSM&#12289;IPIP&#65289;&#26469;&#30740;&#31350;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#35282;&#24230;&#32780;&#25913;&#21464;&#12290;&#36890;&#36807;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#31034;&#20013;&#65288;&#38544;&#24335;&#25110;&#26174;&#24335;&#65289;&#26263;&#31034;&#20102;&#26576;&#20123;&#20215;&#20540;&#35266;&#26102;&#65292;LLMs&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#26263;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SINC&#30340;&#33258;&#20027;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#36991;&#20813;&#20102;&#27169;&#26495;&#25935;&#24863;&#24615;&#21644;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07742</link><description>&lt;p&gt;
SINC: &#33258;&#20027;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SINC: Self-Supervised In-Context Learning for Vision-Language Tasks. (arXiv:2307.07742v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07742
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SINC&#30340;&#33258;&#20027;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#36991;&#20813;&#20102;&#27169;&#26495;&#25935;&#24863;&#24615;&#21644;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;Transformers&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24341;&#20154;&#20837;&#32988;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;&#30340;&#28436;&#31034;&#20013;&#65292;&#36805;&#36895;&#26500;&#24314;&#26032;&#30340;&#39044;&#27979;&#22120;&#65292;&#32780;&#26080;&#38656;&#26799;&#24230;&#26356;&#26032;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#22312;&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#20013;&#20419;&#36827;&#20102;&#36825;&#31181;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#20449;&#24687;&#34701;&#20837;&#21040;&#24050;&#32463;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32487;&#25215;&#20102;&#35821;&#35328;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#22914;&#27169;&#26495;&#25935;&#24863;&#24615;&#21644;&#20135;&#29983;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#25552;&#39640;&#20102;&#35745;&#31639;&#38656;&#27714;&#65292;&#20351;&#24471;&#23398;&#20064;&#21644;&#25805;&#20316;&#36825;&#20123;&#27169;&#22411;&#36164;&#28304;&#23494;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22914;&#20309;&#22312;&#19981;&#38480;&#21046;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#35753;&#36890;&#29992;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;&#8221;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#32780;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#33258;&#20027;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;SINC&#65289;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#25552;&#31034;&#20026;&#22522;&#30784;&#36827;&#34892;&#23398;&#20064;&#65292;&#36825;&#20123;&#25552;&#31034;&#21253;&#25324;&#37327;&#36523;&#23450;&#21046;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pre-trained Transformers exhibit an intriguing capacity for in-context learning. Without gradient updates, these models can rapidly construct new predictors from demonstrations presented in the inputs. Recent works promote this ability in the vision-language domain by incorporating visual information into large language models that can already make in-context predictions. However, these methods could inherit issues in the language domain, such as template sensitivity and hallucination. Also, the scale of these language models raises a significant demand for computations, making learning and operating these models resource-intensive. To this end, we raise a question: ``How can we enable in-context learning for general models without being constrained on large language models?". To answer it, we propose a succinct and general framework, Self-supervised IN-Context learning (SINC), that introduces a meta-model to learn on self-supervised prompts consisting of tailored demonstrations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#30340;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#25552;&#20379;&#23398;&#20064;&#21644;&#29702;&#35299;&#25163;&#26415;&#35270;&#39057;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.05182</link><description>&lt;p&gt;
Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery&#65288;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#30340;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#65289;
&lt;/p&gt;
&lt;p&gt;
Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.05182v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#30340;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#25552;&#20379;&#23398;&#20064;&#21644;&#29702;&#35299;&#25163;&#26415;&#35270;&#39057;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#22312;&#23398;&#20064;&#25163;&#26415;&#26102;&#36890;&#24120;&#20381;&#36182;&#20110;&#39640;&#32423;&#22806;&#31185;&#21307;&#29983;&#21644;&#19987;&#23478;&#22238;&#31572;&#20182;&#20204;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19987;&#23478;&#20204;&#32463;&#24120;&#24537;&#20110;&#20020;&#24202;&#21644;&#23398;&#26415;&#24037;&#20316;&#65292;&#27809;&#26377;&#22810;&#23569;&#26102;&#38388;&#25552;&#20379;&#25351;&#23548;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22806;&#31185;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#21482;&#33021;&#25552;&#20379;&#31616;&#21333;&#31572;&#26696;&#65292;&#32780;&#27809;&#26377;&#31572;&#26696;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#65292;&#35270;&#35273;&#35821;&#35328;&#23884;&#20837;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#23569;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#22806;&#31185;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#31995;&#32479;&#23545;&#20110;&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#20174;&#24405;&#21046;&#30340;&#25163;&#26415;&#35270;&#39057;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22806;&#31185;&#22330;&#26223;&#30340;&#31471;&#21040;&#31471;Transformer&#19982;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#65288;CAT-ViL&#65289;&#30340;VQLA&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#36890;&#36807;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#12290;CAT-ViL&#23884;&#20837;&#27169;&#22359;&#30340;&#35774;&#35745;&#26088;&#22312;&#34701;&#21512;&#26469;&#33258;&#35270;&#35273;&#21644;&#25991;&#26412;&#26469;&#28304;&#30340;&#24322;&#26500;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical students and junior surgeons often rely on senior surgeons and specialists to answer their questions when learning surgery. However, experts are often busy with clinical and academic work, and have little time to give guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question Answering (VQA) systems can only provide simple answers without the location of the answers. In addition, vision-language (ViL) embedding is still a less explored research in these kinds of tasks. Therefore, a surgical Visual Question Localized-Answering (VQLA) system would be helpful for medical students and junior surgeons to learn and understand from recorded surgical videos. We propose an end-to-end Transformer with Co-Attention gaTed Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not require feature extraction through detection models. The CAT-ViL embedding module is designed to fuse heterogeneous features from visual and textual sources. The fused embedding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#35745;&#31639;&#21463;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#19968;&#22871;&#24037;&#20855;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.04345</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#35745;&#31639;&#21463;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning as Computationally Constrained Reinforcement Learning. (arXiv:2307.04345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#35745;&#31639;&#21463;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#19968;&#22871;&#24037;&#20855;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#33021;&#22815;&#22312;&#28459;&#38271;&#30340;&#29983;&#21629;&#21608;&#26399;&#20869;&#39640;&#25928;&#31215;&#32047;&#30693;&#35782;&#24182;&#21457;&#23637;&#36234;&#26469;&#36234;&#22797;&#26434;&#25216;&#33021;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#12290;&#36830;&#32493;&#23398;&#20064;&#36825;&#19968;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#27010;&#24565;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#19968;&#22871;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#30340;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21033;&#29992;&#35780;&#35770;&#26469;&#24110;&#21161;&#23398;&#29983;&#32593;&#32476;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#25552;&#21462;&#23398;&#29983;&#32593;&#32476;&#22312;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#23398;&#29983;&#32593;&#32476;&#20013;&#26087;&#30693;&#35782;&#30340;&#20248;&#21270;&#21644;&#21033;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00811</link><description>&lt;p&gt;
&#35780;&#35770;&#24110;&#21161;&#26356;&#22909;&#22320;&#23398;&#20064;&#65306;&#22522;&#20110;&#26102;&#38388;&#30340;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Review helps learn better: Temporal Supervised Knowledge Distillation. (arXiv:2307.00811v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#30340;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21033;&#29992;&#35780;&#35770;&#26469;&#24110;&#21161;&#23398;&#29983;&#32593;&#32476;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#25552;&#21462;&#23398;&#29983;&#32593;&#32476;&#22312;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#23398;&#29983;&#32593;&#32476;&#20013;&#26087;&#30693;&#35782;&#30340;&#20248;&#21270;&#21644;&#21033;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#30693;&#35782;&#26102;&#65292;&#35780;&#35770;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26576;&#20010;&#26102;&#38388;&#28857;&#33719;&#21462;&#30340;&#30693;&#35782;&#21487;&#33021;&#22312;&#20043;&#21069;&#30340;&#32463;&#39564;&#24110;&#21161;&#19979;&#24471;&#21040;&#26497;&#22823;&#30340;&#21551;&#21457;&#12290;&#22240;&#27492;&#65292;&#30693;&#35782;&#22686;&#38271;&#36807;&#31243;&#24212;&#35813;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#20851;&#32852;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#29305;&#24449;&#22270;&#30340;&#28436;&#21270;&#36981;&#24490;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;&#12290;&#36866;&#24403;&#30340;&#26102;&#38388;&#30417;&#30563;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#32593;&#32476;&#35757;&#32451;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#38388;&#30340;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#65288;TSKD&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;Conv-LSTM&#65289;&#25552;&#21462;&#23398;&#29983;&#32593;&#32476;&#22312;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;&#26102;&#31354;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21160;&#24577;&#30446;&#26631;&#35757;&#32451;&#23398;&#29983;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#30340;&#25945;&#24072;&#32593;&#32476;&#29305;&#24449;&#12290;&#36825;&#20010;&#36807;&#31243;&#23454;&#29616;&#20102;&#23398;&#29983;&#32593;&#32476;&#20013;&#26087;&#30693;&#35782;&#30340;&#20248;&#21270;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36741;&#21161;&#24403;&#21069;&#30340;&#23398;&#20064;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this observation, we propose Temporal Supervised Knowledge Distillation (TSKD). Specifically, we extract the spatiotemporal features in the different training phases of student by convolutional Long Short-term memory network (Conv-LSTM). Then, we train the student net through a dynamic target, rather than static teacher network features. This process realizes the refinement of old knowledge in student network, and utilizes it to assist current learning. Extensive experiments verify the effectiveness and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21307;&#23398;&#22270;&#20687;&#20013;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#27604;&#36739;&#19981;&#21516;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#20351;&#29992;&#27491;&#24120;&#22270;&#20687;&#36827;&#34892;&#39564;&#35777;&#36807;&#31243;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.00750</link><description>&lt;p&gt;
&#19981;&#30693;&#36947;&#24322;&#24120;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#65292;&#21307;&#23398;&#22270;&#20687;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Feasibility of Universal Anomaly Detection without Knowing the Abnormality in Medical Images. (arXiv:2307.00750v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21307;&#23398;&#22270;&#20687;&#20013;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#27604;&#36739;&#19981;&#21516;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#20351;&#29992;&#27491;&#24120;&#22270;&#20687;&#36827;&#34892;&#39564;&#35777;&#36807;&#31243;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27491;&#24120;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26469;&#35782;&#21035;&#24322;&#24120;&#22270;&#20687;&#24418;&#24577;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26159;&#20026;&#29305;&#23450;&#30340;&#8220;&#24050;&#30693;&#8221;&#24322;&#24120;&#65288;&#20363;&#22914;&#65292;&#33041;&#32959;&#30244;&#12289;&#39592;&#25240;&#12289;&#32454;&#32990;&#31867;&#22411;&#65289;&#36827;&#34892;&#20248;&#21270;&#30340;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21482;&#20351;&#29992;&#27491;&#24120;&#22270;&#20687;&#65292;&#24322;&#24120;&#22270;&#20687;&#36890;&#24120;&#20063;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#20351;&#29992;&#65288;&#20363;&#22914;&#65292;&#26102;&#26399;&#36873;&#25321;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#24847;&#22806;&#22320;&#27844;&#28431;&#8220;&#26410;&#30693;&#8221;&#24322;&#24120;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#65288;1&#65289;&#27604;&#36739;&#22235;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#65288;2&#65289;&#30740;&#31350;&#22312;&#39564;&#35777;&#38454;&#27573;&#22914;&#20309;&#20844;&#27491;&#22320;&#36873;&#25321;&#20165;&#20351;&#29992;&#27491;&#24120;&#22270;&#20687;&#26469;&#36873;&#25321;&#26368;&#20339;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#26102; inevitable but often neglected &#38382;&#39064;&#65292;&#20197;&#21450; &#65288;3&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#36827;&#34892;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many anomaly detection approaches, especially deep learning methods, have been recently developed to identify abnormal image morphology by only employing normal images during training. Unfortunately, many prior anomaly detection methods were optimized for a specific "known" abnormality (e.g., brain tumor, bone fraction, cell types). Moreover, even though only the normal images were used in the training process, the abnormal images were often employed during the validation process (e.g., epoch selection, hyper-parameter tuning), which might leak the supposed ``unknown" abnormality unintentionally. In this study, we investigated these two essential aspects regarding universal anomaly detection in medical images by (1) comparing various anomaly detection methods across four medical datasets, (2) investigating the inevitable but often neglected issues on how to unbiasedly select the optimal anomaly detection model during the validation phase using only normal images, and (3) proposing a si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16699</link><description>&lt;p&gt;
&#24555;&#36895;-INR: &#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#25928;&#29575;&#39640;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation. (arXiv:2306.16699v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INR)&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#30340;&#24418;&#29366;&#25110;&#23545;&#35937;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#23450;&#20041;&#23427;&#20204;&#30340;&#20960;&#20309;&#24418;&#29366;&#25110;&#34920;&#38754;&#32467;&#26500;&#12290;&#30456;&#21453;&#65292;INR&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#36830;&#32493;&#20989;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20316;INR&#36827;&#34892;&#22270;&#20687;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;JPEG&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;INR&#22312;&#22270;&#20687;&#21387;&#32553;&#20043;&#22806;&#36824;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Rapid-INR&#65292;&#19968;&#31181;&#21033;&#29992;INR&#23545;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#21644;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#21152;&#36895;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GPU&#19978;&#30452;&#25509;&#20197;INR&#26684;&#24335;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#20174;INR&#21040;RGB&#26684;&#24335;&#30340;&#35299;&#30721;&#36807;&#31243;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21387;&#32553;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representation (INR) is an innovative approach for representing complex shapes or objects without explicitly defining their geometry or surface structure. Instead, INR represents objects as continuous functions. Previous research has demonstrated the effectiveness of using neural networks as INR for image compression, showcasing comparable performance to traditional methods such as JPEG. However, INR holds potential for various applications beyond image compression. This paper introduces Rapid-INR, a novel approach that utilizes INR for encoding and compressing images, thereby accelerating neural network training in computer vision tasks. Our methodology involves storing the whole dataset directly in INR format on a GPU, mitigating the significant data communication overhead between the CPU and GPU during training. Additionally, the decoding process from INR to RGB format is highly parallelized and executed on-the-fly. To further enhance compression, we propose iterativ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;Epistemic Neural Recommendation (ENR)&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#30340;Thompson&#25277;&#26679;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#28857;&#20987;&#29575;&#21644;&#29992;&#25143;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2306.14834</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;Bandit&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Neural Contextual Bandit for Recommender Systems. (arXiv:2306.14834v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;Epistemic Neural Recommendation (ENR)&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#30340;Thompson&#25277;&#26679;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#28857;&#20987;&#29575;&#21644;&#29992;&#25143;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#31995;&#32479;&#24212;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#26377;&#25928;&#21644;&#25506;&#32034;&#24615;&#20114;&#21160;&#25552;&#20379;&#21019;&#26032;&#21644;&#30456;&#20851;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#25512;&#33616;&#31995;&#32479;&#20013;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#20165;&#21033;&#29992;&#24050;&#35782;&#21035;&#30340;&#29992;&#25143;&#20852;&#36259;&#65292;&#23545;&#20110;&#26377;&#25928;&#21457;&#29616;&#26410;&#30693;&#29992;&#25143;&#20559;&#22909;&#23384;&#22312;&#19981;&#36275;&#12290;&#23613;&#31649;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#22312;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#22312;&#32447;&#25506;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20182;&#20204;&#23545;&#35745;&#31639;&#30340;&#35201;&#27714;&#36739;&#39640;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26679;&#26412;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;Epistemic Neural Recommendation (ENR)&#65292;&#23427;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#19978;&#23454;&#29616;Thompson&#25277;&#26679;&#12290;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;ENR&#26174;&#33879;&#25552;&#39640;&#20102;&#28857;&#20987;&#29575;&#21644;&#29992;&#25143;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality recommender systems ought to deliver both innovative and relevant content through effective and exploratory interactions with users. Yet, supervised learning-based neural networks, which form the backbone of many existing recommender systems, only leverage recognized user interests, falling short when it comes to efficiently uncovering unknown user preferences. While there has been some progress with neural contextual bandit algorithms towards enabling online exploration through neural networks, their onerous computational demands hinder widespread adoption in real-world recommender systems. In this work, we propose a scalable sample-efficient neural contextual bandit algorithm for recommender systems. To do this, we design an epistemic neural network architecture, Epistemic Neural Recommendation (ENR), that enables Thompson sampling at a large scale. In two distinct large-scale experiments with real-world tasks, ENR significantly boosts click-through rates and user rating
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#27861;&#65292;&#22312;&#26059;&#36716;&#26426;&#26800;&#25925;&#38556;&#35786;&#26029;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#20851;&#38190;&#36827;&#23637;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#20248;&#23376;&#24102;&#30340;&#29305;&#24449;&#31995;&#25968;&#30697;&#38453;&#65292;&#36827;&#34892;&#36866;&#24212;&#24615;&#20449;&#21495;&#37325;&#26500;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.05281</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#30340;&#26059;&#36716;&#26426;&#26800;&#25925;&#38556;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fault Identification of Rotating Machinery Based on Dynamic Feature Reconstruction Signal Graph. (arXiv:2306.05281v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#27861;&#65292;&#22312;&#26059;&#36716;&#26426;&#26800;&#25925;&#38556;&#35786;&#26029;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#20851;&#38190;&#36827;&#23637;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#20248;&#23376;&#24102;&#30340;&#29305;&#24449;&#31995;&#25968;&#30697;&#38453;&#65292;&#36827;&#34892;&#36866;&#24212;&#24615;&#20449;&#21495;&#37325;&#26500;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22312;&#26059;&#36716;&#26426;&#26800;&#24378;&#22122;&#22768;&#19979;&#35782;&#21035;&#25925;&#38556;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#27861;&#65292;&#23427;&#22312;&#25152;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#25925;&#38556;&#35786;&#26029;&#27169;&#22411;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#21033;&#29992;&#23567;&#27874;&#21253;&#20998;&#35299;&#65288;WPD&#65289;&#23558;&#21407;&#22987;&#26426;&#26800;&#20449;&#21495;&#20998;&#35299;&#20026;&#21253;&#25324;&#31995;&#25968;&#30697;&#38453;&#22312;&#20869;&#30340;&#22810;&#20010;&#23376;&#24102;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#26368;&#21021;&#23450;&#20041;&#30340;&#20004;&#20010;&#35201;&#32032;MDD&#21644;DDD&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#33021;&#37327;&#33539;&#25968;&#30340;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65288;DFSL&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#33021;&#37327;&#33539;&#25968;&#20998;&#24067;&#30340;&#24046;&#24322;&#21160;&#24577;&#22320;&#36873;&#25321;WPD&#30340;&#29305;&#24449;&#31995;&#25968;&#30697;&#38453;&#65292;&#20351;&#27599;&#20010;&#23376;&#20449;&#21495;&#33021;&#22815;&#36827;&#34892;&#36866;&#24212;&#24615;&#20449;&#21495;&#37325;&#26500;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;&#26368;&#20248;&#29305;&#24449;&#23376;&#24102;&#30340;&#31995;&#25968;&#30697;&#38453;&#36827;&#34892;&#37325;&#26500;&#21644;&#37325;&#26032;&#32452;&#21512;&#65292;&#24471;&#21040;&#29305;&#24449;&#20449;&#21495;&#22270;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;2D-&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;2D-CNN&#65289;&#20174;&#29305;&#24449;&#20449;&#21495;&#22270;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the performance in identifying the faults under strong noise for rotating machinery, this paper presents a dynamic feature reconstruction signal graph method, which plays the key role of the proposed end-to-end fault diagnosis model. Specifically, the original mechanical signal is first decomposed by wavelet packet decomposition (WPD) to obtain multiple subbands including coefficient matrix. Then, with originally defined two feature extraction factors MDD and DDD, a dynamic feature selection method based on L2 energy norm (DFSL) is proposed, which can dynamically select the feature coefficient matrix of WPD based on the difference in the distribution of norm energy, enabling each sub-signal to take adaptive signal reconstruction. Next the coefficient matrices of the optimal feature sub-bands are reconstructed and reorganized to obtain the feature signal graphs. Finally, deep features are extracted from the feature signal graphs by 2D-Convolutional neural network (2D-CNN). Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#24335;SemCom&#23454;&#29616;&#30340;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#30340;&#36890;&#20449;&#36164;&#28304;&#21387;&#21147;&#65292;&#24182;&#38416;&#36848;&#20102;&#35813;&#26694;&#26550;&#20013;SemCom&#27169;&#22359;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#21487;&#34892;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03528</link><description>&lt;p&gt;
&#22312;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#20013;&#36827;&#34892;&#35821;&#20041;&#36890;&#20449;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses for Semantic Communication in Vehicular Metaverses. (arXiv:2306.03528v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#24335;SemCom&#23454;&#29616;&#30340;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#30340;&#36890;&#20449;&#36164;&#28304;&#21387;&#21147;&#65292;&#24182;&#38416;&#36848;&#20102;&#35813;&#26694;&#26550;&#20013;SemCom&#27169;&#22359;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#21487;&#34892;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#65292;&#20248;&#21270;&#20174;&#36710;&#19978;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#21644;&#26381;&#21153;&#36136;&#37327;&#26159;&#26368;&#32456;&#30446;&#26631;&#20043;&#19968;&#12290;&#35821;&#20041;&#36890;&#20449;&#65288;SemCom&#65289;&#34987;&#24341;&#20837;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#33539;&#20363;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#30340;&#36890;&#20449;&#36164;&#28304;&#21387;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#35813;&#30446;&#26631;&#12290;SemCom&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#36229;&#39640;&#25928;&#30340;&#36710;&#36733;&#36890;&#20449;&#65292;&#29978;&#33267;&#22312;&#36710;&#36742;&#20043;&#38388;&#30340;&#25968;&#25454;&#27969;&#37327;&#39134;&#36895;&#22686;&#38271;&#26102;&#20173;&#33021;&#22914;&#27492;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#24335;SemCom&#23454;&#29616;&#30340;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#26694;&#26550;&#65292;&#21253;&#25324;&#20840;&#23616;&#34394;&#25311;&#29616;&#23454;&#12289;&#26412;&#22320;&#34394;&#25311;&#29616;&#23454;&#12289;SemCom&#27169;&#22359;&#21644;&#36164;&#28304;&#27744;&#12290;&#20174;&#20998;&#24067;&#35282;&#24230;&#32771;&#34385;&#29992;&#25143;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#30340;&#28508;&#22312;&#23433;&#20840;&#28431;&#27934;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#35813;&#26694;&#26550;&#30340;SemCom&#27169;&#22359;&#30340;&#29305;&#23450;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For vehicular metaverses, one of the ultimate user-centric goals is to optimize the immersive experience and Quality of Service (QoS) for users on board. Semantic Communication (SemCom) has been introduced as a revolutionary paradigm that significantly eases communication resource pressure for vehicular metaverse applications to achieve this goal. SemCom enables high-quality and ultra-efficient vehicular communication, even with explosively increasing data traffic among vehicles. In this article, we propose a hierarchical SemCom-enabled vehicular metaverses framework consisting of the global metaverse, local metaverses, SemCom module, and resource pool. The global and local metaverses are brand-new concepts from the metaverse's distribution standpoint. Considering the QoS of users, this article explores the potential security vulnerabilities of the proposed framework. To that purpose, this study highlights a specific security risk to the framework's SemCom module and offers a viable de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01792</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#31995;&#24863;&#30693;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#26159;&#22522;&#20110;&#20854;&#36807;&#21435;&#34892;&#20026;&#23398;&#20064;&#23558;&#29992;&#25143;&#34920;&#31034;&#20026;&#20302;&#32500;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23427;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#24314;&#27169;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#20026;&#21333;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#21363;&#19982;&#22810;&#31181;&#20219;&#21153;&#30456;&#20851;&#30340;&#26356;&#24191;&#20041;&#29992;&#25143;&#34920;&#31034;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38656;&#27714;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#20197;&#21450;&#20026;&#25345;&#32493;&#28155;&#21152;&#30340;&#20219;&#21153;&#25552;&#20379;&#26377;&#38480;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#19981;&#21463;&#20219;&#21153;&#25968;&#37327;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#26085;&#24535;&#35299;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#23454;&#29616;&#26377;&#21069;&#36884;&#30340;&#26085;&#24535;&#35299;&#26512;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;few-shot&#25552;&#31034;&#19979;&#12290;</title><link>http://arxiv.org/abs/2306.01590</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#36827;&#34892;&#26085;&#24535;&#35299;&#26512;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of Log Parsing with ChatGPT. (arXiv:2306.01590v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#26085;&#24535;&#35299;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#23454;&#29616;&#26377;&#21069;&#36884;&#30340;&#26085;&#24535;&#35299;&#26512;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;few-shot&#25552;&#31034;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#26085;&#24535;&#22312;&#30830;&#20445;&#22823;&#35268;&#27169;&#36719;&#20214;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#26041;&#38754;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#26159;&#36816;&#34892;&#26102;&#20449;&#24687;&#30340;&#21807;&#19968;&#26469;&#28304;&#12290;&#26085;&#24535;&#35299;&#26512;&#23558;&#21407;&#22987;&#26085;&#24535;&#28040;&#24687;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#26159;&#21521;&#19979;&#28216;&#26085;&#24535;&#20998;&#26512;&#30340;&#37325;&#35201;&#21021;&#22987;&#27493;&#39588;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;ChatGPT&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#33258;&#21160;&#21270;&#26085;&#24535;&#35299;&#26512;&#26041;&#38754;&#30340;&#24615;&#33021;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#35780;&#20272;&#20102;ChatGPT&#36827;&#34892;&#26085;&#24535;&#35299;&#26512;&#30340;&#33021;&#21147;&#12290; &#65288;1&#65289;ChatGPT&#33021;&#21542;&#26377;&#25928;&#22320;&#35299;&#26512;&#26085;&#24535;&#65311;&#65288;2&#65289;ChatGPT&#22312;&#19981;&#21516;&#25552;&#31034;&#26041;&#27861;&#19979;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#23454;&#29616;&#26377;&#21069;&#36884;&#30340;&#26085;&#24535;&#35299;&#26512;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;few-shot&#25552;&#31034;&#19979;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20960;&#20010;&#22522;&#20110;ChatGPT&#30340;&#26085;&#24535;&#35299;&#26512;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, ChatGPT, the current cutting-edge large language model (LLM), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate ChatGPT's ability to undertake log parsing by addressing two research questions. (1) Can ChatGPT effectively parse logs? (2) How does ChatGPT perform with different prompting methods? Our results show that ChatGPT can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19190</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36870;&#36817;&#20284;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#26469;&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#12290;&#36825;&#26159;&#36817;&#20284;&#29702;&#35770;&#20013;&#30340;&#19968;&#31181;&#31216;&#20026;Bernstein&#22411;&#32467;&#26524;&#30340;&#32467;&#26524;&#65292;&#23427;&#22312;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20551;&#35774;&#31354;&#38388;&#26377;&#25928;&#36924;&#36817;&#30340;&#26465;&#20214;&#19979;&#25512;&#23548;&#20986;&#30446;&#26631;&#20989;&#25968;&#30340;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#21487;&#20197;&#34987;&#20855;&#26377;hardtanh/tanh&#28608;&#27963;&#20989;&#25968;&#30340;RNNs&#31283;&#23450;&#36924;&#36817;&#30340;&#26102;&#20505;&#65292;&#24517;&#39035;&#20855;&#26377;&#19968;&#20010;&#25351;&#25968;&#34928;&#20943;&#30340;&#35760;&#24518;&#32467;&#26500;--&#36825;&#20010;&#27010;&#24565;&#21487;&#20197;&#34987;&#26126;&#30830;&#23450;&#20041;&#12290;&#36825;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#37327;&#21270;&#20102;RNN&#26550;&#26500;&#22312;&#23398;&#20064;&#20855;&#26377;&#38271;&#26399;&#35760;&#24518;&#30340;&#24207;&#21015;&#20851;&#31995;&#26102;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments.
&lt;/p&gt;</description></item><item><title>EgoHumans&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#25105;&#20013;&#24515;&#22810;&#20154;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#36827;&#33258;&#25105;&#20013;&#24515;&#30340;&#20154;&#31867;&#19977;&#32500;&#23039;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20154;&#31867;&#26816;&#27979;&#12289;&#36319;&#36394;&#12289;2D/3D&#23039;&#24577;&#20272;&#35745;&#21644;&#32593;&#26684;&#24674;&#22797;&#31561;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26080;&#26680;&#32534;&#25490;&#30340;&#22810;&#20154;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.16487</link><description>&lt;p&gt;
EgoHumans:&#19968;&#31181;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#19977;&#32500;&#22810;&#20154;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EgoHumans: An Egocentric 3D Multi-Human Benchmark. (arXiv:2305.16487v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16487
&lt;/p&gt;
&lt;p&gt;
EgoHumans&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#25105;&#20013;&#24515;&#22810;&#20154;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#36827;&#33258;&#25105;&#20013;&#24515;&#30340;&#20154;&#31867;&#19977;&#32500;&#23039;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20154;&#31867;&#26816;&#27979;&#12289;&#36319;&#36394;&#12289;2D/3D&#23039;&#24577;&#20272;&#35745;&#21644;&#32593;&#26684;&#24674;&#22797;&#31561;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26080;&#26680;&#32534;&#25490;&#30340;&#22810;&#20154;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EgoHumans&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#22810;&#20154;&#35270;&#39057;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#36827;&#33258;&#25105;&#20013;&#24515;&#30340;&#20154;&#31867;&#19977;&#32500;&#23039;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#29616;&#26377;&#30340;&#33258;&#25105;&#20013;&#24515;&#22522;&#20934;&#25968;&#25454;&#38598;&#20165;&#25429;&#25417;&#21333;&#20010;&#20027;&#20307;&#25110;&#20165;&#38480;&#20110;&#23460;&#20869;&#22330;&#26223;&#65292;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#25429;&#33719;&#35774;&#23450;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#25105;&#20013;&#24515;&#22810;&#20154;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#27880;&#37322;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#20363;&#22914;&#20154;&#31867;&#26816;&#27979;&#12289;&#36319;&#36394;&#12289;2D/3D&#23039;&#24577;&#20272;&#35745;&#21644;&#32593;&#26684;&#24674;&#22797;&#31561;&#12290;&#25105;&#20204;&#21033;&#29992;&#24102;&#25668;&#20687;&#22836;&#30340;&#26222;&#36890;&#30524;&#38236;&#36827;&#34892;&#35270;&#35282;&#25429;&#25417;&#65292;&#24182;&#33021;&#22815;&#25429;&#25417;&#35832;&#22914;&#36386;&#36275;&#29699;&#12289;&#20987;&#21073;&#12289;&#25490;&#29699;&#31561;&#21160;&#24577;&#27963;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#35774;&#32622;&#22312;&#20005;&#37325;&#25110;&#23436;&#20840;&#36974;&#25377;&#19979;&#20173;&#33021;&#29983;&#25104;&#20934;&#30830;&#30340;3D&#22522;&#20934;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;125k&#20010;&#33258;&#25105;&#20013;&#24515;&#22270;&#20687;&#65292;&#36328;&#36234;&#22810;&#31181;&#22330;&#26223;&#65292;&#29305;&#21035;&#20851;&#27880;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26080;&#26680;&#32534;&#25490;&#30340;&#22810;&#20154;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EgoHumans, a new multi-view multi-human video benchmark to advance the state-of-the-art of egocentric human 3D pose estimation and tracking. Existing egocentric benchmarks either capture single subject or indoor-only scenarios, which limit the generalization of computer vision algorithms for real-world applications. We propose a novel 3D capture setup to construct a comprehensive egocentric multi-human benchmark in the wild with annotations to support diverse tasks such as human detection, tracking, 2D/3D pose estimation, and mesh recovery. We leverage consumer-grade wearable camera-equipped glasses for the egocentric view, which enables us to capture dynamic activities like playing soccer, fencing, volleyball, etc. Furthermore, our multi-view setup generates accurate 3D ground truth even under severe or complete occlusion. The dataset consists of more than 125k egocentric images, spanning diverse scenes with a particular focus on challenging and unchoreographed multi-human 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#65292;&#23454;&#29616;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#19977;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11284</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#27169;&#22411;&#30340;&#23433;&#20840;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Federated learning for secure development of AI models for Parkinson's disease detection using speech from different languages. (arXiv:2305.11284v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#65292;&#23454;&#29616;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#19977;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#26159;&#19968;&#31181;&#24433;&#21709;&#20154;&#31867;&#35828;&#35805;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#24085;&#37329;&#26862;&#30149;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20005;&#26684;&#30340;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#38459;&#30861;&#20102;&#26426;&#26500;&#38388;&#20849;&#20139;&#25968;&#25454;&#12290;&#26412;&#25991;&#22312;&#19981;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#31561;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a neurological disorder impacting a person's speech. Among automatic PD assessment methods, deep learning models have gained particular interest. Recently, the community has explored cross-pathology and cross-language models which can improve diagnostic accuracy even further. However, strict patient data privacy regulations largely prevent institutions from sharing patient speech data with each other. In this paper, we employ federated learning (FL) for PD detection using speech signals from 3 real-world language corpora of German, Spanish, and Czech, each from a separate institution. Our results indicate that the FL model outperforms all the local models in terms of diagnostic accuracy, while not performing very differently from the model based on centrally combined training sets, with the advantage of not requiring any data sharing among collaborators. This will simplify inter-institutional collaborations, resulting in enhancement of patient outcomes.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#8220;&#36830;&#25509;&#35884;&#35823;&#8221;&#20316;&#20026;&#20844;&#24179;&#24615;&#38382;&#39064;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36991;&#20813;&#31867;&#20284;&#24773;&#20917;&#22312;&#26410;&#26469;&#20013;&#22797;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09535</link><description>&lt;p&gt;
&#12298;&#29747;&#36798;&#65292;&#20986;&#20102;&#20160;&#20040;&#38382;&#39064;&#65311;&#12299;&#8220;&#36830;&#25509;&#35884;&#35823;&#8221;&#20316;&#20026;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
What's the Problem, Linda? The Conjunction Fallacy as a Fairness Problem. (arXiv:2305.09535v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09535
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#8220;&#36830;&#25509;&#35884;&#35823;&#8221;&#20316;&#20026;&#20844;&#24179;&#24615;&#38382;&#39064;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36991;&#20813;&#31867;&#20284;&#24773;&#20917;&#22312;&#26410;&#26469;&#20013;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#27491;&#22312;&#19987;&#27880;&#20110;&#21019;&#24314;&#23613;&#21487;&#33021;&#25509;&#36817;&#20154;&#31867;&#26234;&#33021;&#30340;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#12290;&#36825;&#19968;&#21162;&#21147;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#24515;&#29702;&#23398;&#31561;&#35748;&#30693;&#39046;&#22495;&#12290; Daniel Kahneman&#21644;&#24050;&#25925;&#30340;Amos Tversky&#22312;&#26377;&#20559;&#35265;&#30340;&#20154;&#31867;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#23545;&#36830;&#25509;&#35884;&#35823;&#30340;&#30740;&#31350;&#65292;&#22240;&#27492;&#36827;&#34892;&#20102;&#31532;&#20108;&#27425;&#22797;&#20852;&#12290; &#22312;&#36830;&#25509;&#35884;&#35823;&#19979;&#65292;&#20915;&#31574;&#21046;&#23450;&#32773;&#20250;&#36829;&#21453;&#22522;&#26412;&#27010;&#29575;&#27861;&#21017;&#65292;&#35748;&#20026;&#36830;&#35789;&#27604;&#20854;&#20013;&#19968;&#20010;&#37096;&#20998;&#26356;&#26377;&#21487;&#33021;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#19982;&#29747;&#36798;&#38382;&#39064;&#26368;&#20026;&#33879;&#21517;&#30340;&#23454;&#39564;&#65292;&#23427;&#24050;&#34987;&#35777;&#26126;&#26159;&#32463;&#24471;&#36215;&#26102;&#38388;&#32771;&#39564;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#36328;&#23398;&#31185;&#30340;&#21162;&#21147;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#25105;&#20204;&#25285;&#24515;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#24573;&#30053;&#20102;&#29747;&#36798;&#38382;&#39064;&#25152;&#25429;&#25417;&#21040;&#30340;&#39537;&#21160;&#21147;&#65306;&#29747;&#36798;&#24517;&#39035;&#34987;&#21051;&#26495;&#22320;&#25551;&#36848;&#20026;&#19968;&#20010;&#22899;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29747;&#36798;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;AI&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#36830;&#25509;&#35884;&#35823;&#26159;&#20559;&#35265;&#25968;&#25454;&#38598;&#22914;&#20309;&#23548;&#33268;&#20559;&#35265;&#32467;&#26524;&#30340;&#26126;&#26174;&#20363;&#23376;&#65292;&#20174;&#32780;&#24310;&#32493;&#21644;&#25918;&#22823;&#29616;&#26377;&#30340;&#31995;&#32479;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20379;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#20351;&#29992;&#65292;&#20197;&#36991;&#20813;&#31867;&#20284;&#24773;&#20917;&#22312;&#26410;&#26469;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Artificial Intelligence (AI) is focusing on creating automated decision-making (ADM) systems that operate as close as possible to human-like intelligence. This effort has pushed AI researchers into exploring cognitive fields like psychology. The work of Daniel Kahneman and the late Amos Tversky on biased human decision-making, including the study of the conjunction fallacy, has experienced a second revival because of this. Under the conjunction fallacy a human decision-maker will go against basic probability laws and rank as more likely a conjunction over one of its parts. It has been proven overtime through a set of experiments with the Linda Problem being the most famous one. Although this interdisciplinary effort is welcomed, we fear that AI researchers ignore the driving force behind the conjunction fallacy as captured by the Linda Problem: the fact that Linda must be stereotypically described as a woman. In this paper we revisit the Linda Problem and formulate it as a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06741</link><description>&lt;p&gt;
IVP-VAE: &#21033;&#29992;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#23545;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;ODE&#21644;&#31070;&#32463;&#27969;&#37327;&#65289;&#22312;&#20998;&#26512;&#30005;&#23376;&#30149;&#21382;&#20013;&#24120;&#35265;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#22522;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#36890;&#36807;&#21021;&#20540;&#38382;&#39064;&#65288;IVP&#65289;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#22788;&#29702;&#12290; &#39034;&#24207;&#27714;&#35299;IVP&#20351;&#24471;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#19981;&#22815;&#39640;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#20351;&#29992;&#36830;&#32493;&#36807;&#31243;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#29366;&#24577;&#28436;&#21464;&#21487;&#20197;&#36890;&#36807;IVP&#30452;&#25509;&#36817;&#20284;&#12290; &#36825;&#28040;&#38500;&#20102;&#36882;&#24402;&#35745;&#31639;&#30340;&#38656;&#35201;&#65292;&#24182;&#20801;&#35768;&#22810;&#20010;&#29366;&#24577;&#24182;&#34892;&#28436;&#21464;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#20854;&#21487;&#36870;&#24615;&#30340;IVP&#27714;&#35299;&#22120;&#34701;&#21512;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#36825;&#23548;&#33268;&#21442;&#25968;&#26356;&#23569;&#65292;&#25910;&#25947;&#26356;&#24555;&#12290; &#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#33719;&#24471;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#30340;&#21516;&#26102;&#65292;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03515</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#24230;&#30340;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#29983;&#38271;&#31639;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#19978;&#23616;&#37096;&#26368;&#23567;&#21270;&#19981;&#32431;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#36138;&#24515;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#30452;&#36890;&#31639;&#23376;&#22312;&#23494;&#38598;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#19978;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.00948</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20998;&#26512;LLM&#30340;&#29702;&#35770;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs. (arXiv:2305.00948v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#26368;&#36817;&#24050;&#32463;&#25552;&#39640;&#21040;&#20102;&#33021;&#22815;&#22312;&#35768;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#30340;&#24418;&#24335;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#20803;&#35821;&#35328;&#33021;&#21147;&#20998;&#26512;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;LLMs&#20027;&#35201;&#26159;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#30340;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65307;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#25913;&#36827;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;&#35821;&#35328;&#23398;&#20013;&#30340;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#27880;&#20110;&#24418;&#24335;&#35821;&#35328;&#23398;&#30340;&#19977;&#20010;&#23376;&#39046;&#22495;&#65306;&#21477;&#27861;&#12289;&#38899;&#38901;&#23398;&#21644;&#35821;&#20041;&#23398;&#65292;&#25506;&#31350;&#20102;GPT-4&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20803;&#35821;&#35328;&#20998;&#26512;&#30340;&#30740;&#31350;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#23454;&#39564;&#35774;&#35745;&#65292;&#25552;&#20379;&#20102;&#19968;&#33324;&#25351;&#23548;&#26041;&#38024;&#65292;&#35752;&#35770;&#20102;&#38480;&#21046;&#65292;&#24182;&#20026;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#36825;&#20010;&#30740;&#31350;&#36824;&#26377;&#21161;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#21644;&#29702;&#35770;&#27169;&#22411;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of large language models (LLMs) has recently improved to the point where the models can perform well on many language tasks. We show here that for the first time, the models can also generate coherent and valid formal analyses of linguistic data and illustrate the vast potential of large language models for analyses of their metalinguistic abilities. LLMs are primarily trained on language data in the form of text; analyzing and evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. In this paper, we probe into GPT-4's metalinguistic capabilities by focusing on three subfields of formal linguistics: syntax, phonology, and semantics. We outline a research program for metalinguistic analyses of large language models, propose experimental designs, provide general guidelines, discuss limitations, and offer future directions for this line of research. This line of inquiry als
&lt;/p&gt;</description></item><item><title>SelfDocSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#24067;&#23616;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.00795</link><description>&lt;p&gt;
SelfDocSeg: &#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation. (arXiv:2305.00795v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00795
&lt;/p&gt;
&lt;p&gt;
SelfDocSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#24067;&#23616;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#28085;&#30422;&#20102;&#20174;&#25991;&#26412;&#25366;&#25496;&#12289;&#35782;&#21035;&#21040;&#22522;&#20110;&#22270;&#24418;&#30340;&#34920;&#31034;&#12289;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#31561;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#24573;&#30053;&#20102;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#36825;&#19968;&#20851;&#38190;&#20107;&#23454;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#25991;&#26412;&#26631;&#31614;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#20013;&#29983;&#25104;&#20266;&#24067;&#23616;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#30495;&#23454;&#26631;&#31614;&#25110;&#20854;&#23548;&#20986;&#29289;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#20013;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging from text mining, and recognition to graph-based representation, visual feature extraction, etc. However, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22825;&#27668;&#26465;&#20214;&#22797;&#26434;&#20849;&#29616;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#26631;&#31614;&#35782;&#21035;&#27169;&#22411;MASK-CNN-Transformer&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#24182;&#21033;&#29992;MASK&#26426;&#21046;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14857</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#22810;&#26631;&#31614;&#22825;&#27668;&#35782;&#21035;&#30340;MASK-CNN-Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MASK-CNN-Transformer For Real-Time Multi-Label Weather Recognition. (arXiv:2304.14857v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22825;&#27668;&#26465;&#20214;&#22797;&#26434;&#20849;&#29616;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#26631;&#31614;&#35782;&#21035;&#27169;&#22411;MASK-CNN-Transformer&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#24182;&#21033;&#29992;MASK&#26426;&#21046;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#35782;&#21035;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#29983;&#27963;&#24212;&#29992;&#65292;&#21253;&#25324;&#20132;&#36890;&#23433;&#20840;&#12289;&#29615;&#22659;&#21644;&#27668;&#35937;&#26041;&#38754;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#30456;&#20851;&#24037;&#20316;&#30001;&#20110;&#22797;&#26434;&#30340;&#20849;&#29616;&#20381;&#36182;&#20851;&#31995;&#32780;&#26080;&#27861;&#20840;&#38754;&#25551;&#36848;&#22825;&#27668;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#30340;&#26032;&#22411;&#22810;&#26631;&#31614;&#22825;&#27668;&#35782;&#21035;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21517;&#20026;MASK-CNN-Transformer (MASK-CT)&#65292;&#22522;&#20110;Transformer&#12289;&#21367;&#31215;&#36807;&#31243;&#21644;MASK&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather recognition is an essential support for many practical life applications, including traffic safety, environment, and meteorology. However, many existing related works cannot comprehensively describe weather conditions due to their complex co-occurrence dependencies. This paper proposes a novel multi-label weather recognition model considering these dependencies. The proposed model called MASK-Convolutional Neural Network-Transformer (MASK-CT) is based on the Transformer, the convolutional process, and the MASK mechanism. The model employs multiple convolutional layers to extract features from weather images and a Transformer encoder to calculate the probability of each weather condition based on the extracted features. To improve the generalization ability of MASK-CT, a MASK mechanism is used during the training phase. The effect of the MASK mechanism is explored and discussed. The Mask mechanism randomly withholds some information from one-pair training instances (one image an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.09991</link><description>&lt;p&gt;
&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#23457;&#35745;LLM&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#37096;&#32626;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#21644;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;&#20998;&#31867;&#36824;&#26159;&#29983;&#25104;&#65292;&#37117;&#34920;&#29616;&#20986;&#26377;&#20559;&#24046;&#21644;&#19981;&#36127;&#36131;&#20219;&#30340;&#34892;&#20026;&#65292;&#23545;&#20154;&#31867;&#36896;&#25104;&#20102;&#35268;&#27169;&#24615;&#30340;&#20260;&#23475;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#23457;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23457;&#35745;&#24037;&#20855;&#21033;&#29992;&#20154;&#21644;&#25110;AI&#26469;&#21457;&#29616;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#30340;&#25991;&#29486;&#65292;&#24182;&#37319;&#35775;&#20102;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#19987;&#23478;&#65292;&#20197;&#22686;&#24378;&#23457;&#35745;&#24037;&#20855;&#8220;AdaTest&#8221;&#65288;Ribeiro&#21644;Lundberg&#65292;2022&#65289;&#65292;&#35813;&#24037;&#20855;&#30001;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#12290;&#36890;&#36807;&#35774;&#35745;&#36807;&#31243;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24863;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#22312;&#21327;&#20316;&#23457;&#35745;&#20013;&#21033;&#29992;&#20154;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#22686;&#24378;&#24037;&#20855;AdaTest ++&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#20351;&#21442;&#19982;&#32773;&#36827;&#34892;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants audit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#35270;&#35273;Transformer&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26725;&#22359;&#37325;&#26500;&#30340;&#28151;&#21512;&#35270;&#35273;Transformer&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20854;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12557</link><description>&lt;p&gt;
Q-HyViT: &#24102;&#26725;&#22359;&#37325;&#26500;&#30340;&#28151;&#21512;&#35270;&#35273;Transformer&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction. (arXiv:2303.12557v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#35270;&#35273;Transformer&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26725;&#22359;&#37325;&#26500;&#30340;&#28151;&#21512;&#35270;&#35273;Transformer&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20854;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;Transformer &#65288;ViT&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#20195;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292; ViT &#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#28151;&#21512;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#32467;&#21512;&#21367;&#31215;&#21644;&#21464;&#21387;&#22120;&#23618;&#65292;&#24182;&#20248;&#21270;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#20351;&#32447;&#24615;&#22797;&#26434;&#24230;&#36798;&#21040;&#26368;&#22823;&#12290;&#27492;&#22806;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#34987;&#25552;&#20986;&#20316;&#20026;&#32531;&#35299;&#35745;&#31639;&#35201;&#27714;&#30340;&#19968;&#31181;&#25163;&#27573;&#12290;&#23558;&#37327;&#21270;&#25216;&#26415;&#21644;&#39640;&#25928;&#30340;&#28151;&#21512;&#21464;&#21387;&#22120;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#23545;&#20110;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#21152;&#36895;&#35270;&#35273;transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#27809;&#26377;&#30740;&#31350;&#23558;&#37327;&#21270;&#24212;&#29992;&#20110;&#39640;&#25928;&#30340;&#28151;&#21512;&#21464;&#21387;&#22120;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#23558;&#29616;&#26377;&#30340;ViT PTQ&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#39640;&#25928;&#30340;&#28151;&#21512;transformer&#26550;&#26500;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#31934;&#24230;&#19979;&#38477;&#65292;&#30001;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;Q-HyViT&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, vision transformers (ViT) have replaced convolutional neural network models in numerous tasks, including classification, detection, and segmentation. However, the high computational requirements of ViTs hinder their widespread implementation. To address this issue, researchers have proposed efficient hybrid transformer architectures that combine convolutional and transformer layers and optimize attention computation for linear complexity. Additionally, post-training quantization has been proposed as a means of mitigating computational demands. Combining quantization techniques and efficient hybrid transformer structures is crucial to maximize the acceleration of vision transformers on mobile devices. However, no prior investigation has applied quantization to efficient hybrid transformers. In this paper, at first, we discover that the straightforward manner to apply the existing PTQ methods for ViT to efficient hybrid transformers results in a drastic accuracy drop due to the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeTO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#20013;&#25429;&#25417;&#22266;&#20307;&#36879;&#26126;&#29289;&#20307;&#30340;3D&#20960;&#20309;&#20307;&#12290;&#36890;&#36807;&#37319;&#29992;&#38544;&#24335;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#20316;&#20026;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36974;&#25377;&#24863;&#30693;&#30340;&#25240;&#23556;&#36861;&#36394;&#36890;&#36807;&#20307;&#28210;&#26579;&#26469;&#20248;&#21270;SDF&#22330;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11219</link><description>&lt;p&gt;
NeTO: &#36879;&#26126;&#29289;&#20307;&#30340;&#31070;&#32463;&#37325;&#24314;&#19982;&#33258;&#36974;&#25377;&#24863;&#30693;&#25240;&#23556;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing. (arXiv:2303.11219v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11219
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeTO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#20013;&#25429;&#25417;&#22266;&#20307;&#36879;&#26126;&#29289;&#20307;&#30340;3D&#20960;&#20309;&#20307;&#12290;&#36890;&#36807;&#37319;&#29992;&#38544;&#24335;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#20316;&#20026;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36974;&#25377;&#24863;&#30693;&#30340;&#25240;&#23556;&#36861;&#36394;&#36890;&#36807;&#20307;&#28210;&#26579;&#26469;&#20248;&#21270;SDF&#22330;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;NeTO&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#20013;&#25429;&#25417;&#22266;&#20307;&#36879;&#26126;&#29289;&#20307;&#30340;3D&#20960;&#20309;&#20307;&#12290;&#36879;&#26126;&#29289;&#20307;&#30340;&#37325;&#24314;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#19981;&#36866;&#21512;&#36890;&#29992;&#30340;&#37325;&#24314;&#25216;&#26415;&#65292;&#22240;&#20026;&#20854;&#22256;&#25200;&#20110;&#38236;&#38754;&#20809;&#20256;&#36755;&#29616;&#35937;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25240;&#23556;&#36861;&#36394;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#20248;&#21270;&#19981;&#31283;&#23450;&#21644;&#32454;&#33410;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#37319;&#29992;&#30340;&#26174;&#24335;&#34920;&#38754;&#34920;&#31034;&#38590;&#20197;&#20248;&#21270;&#65292;&#19988;&#24573;&#30053;&#20102;&#33258;&#36974;&#25377;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#38544;&#24335;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#20316;&#20026;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36974;&#25377;&#24863;&#30693;&#30340;&#25240;&#23556;&#36861;&#36394;&#36890;&#36807;&#20307;&#28210;&#26579;&#26469;&#20248;&#21270;SDF&#22330;&#12290;&#38544;&#24335;&#34920;&#31034;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#22270;&#20687;&#38598;&#21512;&#19979;&#37325;&#24314;&#20986;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method, called NeTO, for capturing 3D geometry of solid transparent objects from 2D images via volume rendering. Reconstructing transparent objects is a very challenging task, which is ill-suited for general-purpose reconstruction techniques due to the specular light transport phenomena. Although existing refraction-tracing based methods, designed specially for this task, achieve impressive results, they still suffer from unstable optimization and loss of fine details, since the explicit surface representation they adopted is difficult to be optimized, and the self-occlusion problem is ignored for refraction-tracing. In this paper, we propose to leverage implicit Signed Distance Function (SDF) as surface representation, and optimize the SDF field via volume rendering with a self-occlusion aware refractive ray tracing. The implicit representation enables our method to be capable of reconstructing high-quality reconstruction even with a limited set of images, and the s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21457;&#29616;&#22823;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#21644;&#21508;&#34892;&#21508;&#19994;&#65292;&#39044;&#31034;&#30528;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.10130</link><description>&lt;p&gt;
GPT&#26159;GPT&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#21171;&#21160;&#21147;&#24066;&#22330;&#24433;&#21709;&#30340;&#26089;&#26399;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. (arXiv:2303.10130v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21457;&#29616;&#22823;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#21644;&#21508;&#34892;&#21508;&#19994;&#65292;&#39044;&#31034;&#30528;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#20351;&#29992;&#26032;&#30340;&#26631;&#20934;&#65292;&#25105;&#20204;&#35780;&#20272;&#32844;&#19994;&#19982;GPT&#33021;&#21147;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;GPT-4&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#33267;&#23569;&#26377;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#21463;&#21040;GPT&#24341;&#20837;&#30340;&#24433;&#21709;&#65292;&#32780;&#32422;19%&#30340;&#24037;&#20154;&#21487;&#33021;&#20250;&#30475;&#21040;&#33267;&#23569;50%&#30340;&#20219;&#21153;&#21463;&#21040;&#24433;&#21709;&#12290;&#24433;&#21709;&#33539;&#22260;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#65292;&#39640;&#25910;&#20837;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#30340;&#39118;&#38505;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24433;&#21709;&#24182;&#19981;&#23616;&#38480;&#20110;&#26368;&#36817;&#29983;&#20135;&#29575;&#22686;&#38271;&#36739;&#39640;&#30340;&#34892;&#19994;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#20855;&#26377;&#36890;&#29992;&#25216;&#26415;&#65288;GPT&#65289;&#30340;&#29305;&#24615;&#65292;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.08983</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#22686;&#24378;&#65306;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08983
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#23545;&#29992;&#25143;&#27809;&#26377;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#25454;&#38598;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#31574;&#30053;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;CNN&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30340;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#24102;&#26377;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33976;&#39311;&#30740;&#31350;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;ImageDataNet+&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#20197;&#21450;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;CIFAR-100+&#65292;Flowers-102+&#21644;Food-101+&#12290;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#20934;&#30830;&#12289;&#26356;&#26377;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#21106;&#21644;&#26816;&#27979;&#65289;&#20855;&#26377;&#24456;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;&#22312;ImageDataNet+&#19978;&#27979;&#37327;&#30340;Expected Calibration Error&#65288;ECE&#65289;&#20063;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the Ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;XAI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#25351;&#26631;&#37492;&#21035;in-distribution&#21644;out of-distribution&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#30340;OoD&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;&#35780;&#20272;&#35757;&#32451;&#21644;&#25805;&#20316;&#26465;&#20214;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.01860</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;OoD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rule-based Out-Of-Distribution Detection. (arXiv:2303.01860v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;XAI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#25351;&#26631;&#37492;&#21035;in-distribution&#21644;out of-distribution&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#30340;OoD&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;&#35780;&#20272;&#35757;&#32451;&#21644;&#25805;&#20316;&#26465;&#20214;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;OoD&#26159;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#20851;&#38190;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#37319;&#29992;XAI&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#30340;&#25351;&#26631;&#37492;&#21035;&#20986;in-distribution&#21644;out of-distribution&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20998;&#24067;&#20551;&#35774;&#12290;&#22312;&#22797;&#26434;&#24212;&#29992;&#22330;&#26223;&#19979;&#65288;&#22914;&#39044;&#27979;&#24615;&#32500;&#25252;&#12289;&#36710;&#38431;&#31649;&#21046;&#12289;&#32593;&#32476;&#23433;&#20840;&#36861;&#36394;&#31561;&#65289;&#30340;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;&#35780;&#20272;&#35757;&#32451;&#21644;&#25805;&#20316;&#26465;&#20214;&#30340;&#25509;&#36817;&#31243;&#24230;&#26041;&#38754;&#30340;&#20248;&#33391;&#24615;&#33021;&#12290;&#32467;&#26524;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#30340;github&#19978;&#33719;&#24471;: https://github.com/giacomo97cnr/Rule-based-ODD&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution detection is one of the most critical issue in the deployment of machine learning. The data analyst must assure that data in operation should be compliant with the training phase as well as understand if the environment has changed in a way that autonomous decisions would not be safe anymore. The method of the paper is based on eXplainable Artificial Intelligence (XAI); it takes into account different metrics to identify any resemblance between in-distribution and out of, as seen by the XAI model. The approach is non-parametric and distributional assumption free. The validation over complex scenarios (predictive maintenance, vehicle platooning, covert channels in cybersecurity) corroborates both precision in detection and evaluation of training-operation conditions proximity. Results are available via open source and open data at the following link: https://github.com/giacomo97cnr/Rule-based-ODD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20146;&#21644;&#21147;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;logit&#25439;&#22833;&#21644;&#29305;&#24449;&#20146;&#21644;&#21147;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10899</link><description>&lt;p&gt;
&#29305;&#24449;&#20146;&#21644;&#21147;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Feature Affinity Assisted Knowledge Distillation and Quantization of Deep Neural Networks on Label-Free Data. (arXiv:2302.10899v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20146;&#21644;&#21147;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;logit&#25439;&#22833;&#21644;&#29305;&#24449;&#20146;&#21644;&#21147;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20146;&#21644;&#21147;&#65288;FA&#65289;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#12290;DNN&#30340;&#20013;&#38388;&#29305;&#24449;&#22270;&#19978;&#30340;FA&#25439;&#22833;&#36215;&#21040;&#20102;&#23558;&#20013;&#38388;&#27493;&#39588;&#30340;&#35299;&#20915;&#26041;&#26696;&#25945;&#32473;&#23398;&#29983;&#30340;&#20316;&#29992;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#20256;&#32479;&#30340;KD&#20013;&#20316;&#29992;&#20110;&#32593;&#32476;&#36755;&#20986;&#32423;&#21035;&#30340;logits&#25439;&#22833;&#12290;&#23558;logit&#25439;&#22833;&#21644;FA&#25439;&#22833;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#21270;&#30340;&#23398;&#29983;&#32593;&#32476;&#24471;&#21040;&#30340;&#30417;&#30563;&#27604;&#26469;&#33258;&#26631;&#35760;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#30340;&#30417;&#30563;&#26356;&#24378;&#12290;&#25152;&#24471;&#21040;&#30340;FAQD&#33021;&#22815;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#21387;&#32553;&#27169;&#22411;&#65292;&#36825;&#24102;&#26469;&#20102;&#21363;&#26102;&#30340;&#23454;&#38469;&#25928;&#30410;&#65292;&#22240;&#20026;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#26159;&#38543;&#26102;&#21487;&#29992;&#30340;&#65292;&#32780;&#26080;&#26631;&#31614;&#25968;&#25454;&#21448;&#26159;&#20016;&#23500;&#30340;&#12290;&#30456;&#21453;&#65292;&#25968;&#25454;&#26631;&#35760;&#36890;&#24120;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#29305;&#24449;&#20146;&#21644;&#21147;&#65288;FFA&#65289;&#25439;&#22833;&#65292;&#23427;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20934;&#30830;&#36817;&#20284;FA&#25439;&#22833;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a feature affinity (FA) assisted knowledge distillation (KD) method to improve quantization-aware training of deep neural networks (DNN). The FA loss on intermediate feature maps of DNNs plays the role of teaching middle steps of a solution to a student instead of only giving final answers in the conventional KD where the loss acts on the network logits at the output level. Combining logit loss and FA loss, we found that the quantized student network receives stronger supervision than from the labeled ground-truth data. The resulting FAQD is capable of compressing model on label-free data, which brings immediate practical benefits as pre-trained teacher models are readily available and unlabeled data are abundant. In contrast, data labeling is often laborious and expensive. Finally, we propose a fast feature affinity (FFA) loss that accurately approximates FA loss with a lower order of computational complexity, which helps speed up training for high resolution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2302.09582</link><description>&lt;p&gt;
&#35821;&#35328;&#29305;&#23450;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#23545;&#24773;&#32490;&#25512;&#26029;&#30340;&#22240;&#26524;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#32490;&#31185;&#23398;&#20013;&#65292;&#22914;&#20309;&#29702;&#35299;&#35821;&#35328;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#20173;&#28982;&#26159;&#19968;&#20010;&#20105;&#35758;&#30340;&#35805;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#35843;&#26597;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#20351;&#29992;&#25552;&#31034;&#25216;&#26415;&#65292;&#21457;&#29616;&#20102;14&#20010;&#24773;&#32490;&#27010;&#24565;&#30340;&#23646;&#24615;&#30001;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#32676;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#23646;&#24615;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#19982;&#38543;&#26426;&#25805;&#32437;&#30456;&#27604;&#65292;&#22823;&#22810;&#25968;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#34920;&#29616;&#20986;&#29616;&#20102;&#19979;&#38477;&#12290;&#23646;&#24615;&#29305;&#23450;&#30340;&#34920;&#29616;&#19979;&#38477;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#30340;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#24378;&#35843;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;&#65292;&#36890;&#36807;&#29289;&#29702;&#35770;&#35777;&#25512;&#23548;&#20986;&#19968;&#31867;&#26032;&#22411;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09580</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator. (arXiv:2302.09580v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;&#65292;&#36890;&#36807;&#29289;&#29702;&#35770;&#35777;&#25512;&#23548;&#20986;&#19968;&#31867;&#26032;&#22411;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#12290;&#21327;&#26041;&#24046;&#26680;&#26159;&#39640;&#26031;&#36807;&#31243;&#30340;&#20027;&#35201;&#24341;&#25806;&#65292;&#21253;&#21547;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#26102;&#31354;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#65292;&#21512;&#36866;&#30340;&#26680;&#24212;&#35813;&#24314;&#27169;&#32852;&#21512;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#21487;&#20998;&#31163;&#30340;&#26102;&#31354;&#21327;&#26041;&#24046;&#26680;&#25552;&#20379;&#20102;&#31616;&#21333;&#21644;&#35745;&#31639;&#25928;&#29575;&#36739;&#39640;&#30340;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38750;&#21487;&#20998;&#31163;&#26680;&#21253;&#21547;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#30456;&#20851;&#24615;&#30340;&#26102;&#31354;&#20132;&#20114;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#20855;&#26377;&#26174;&#24335;&#34920;&#36798;&#24335;&#30340;&#38750;&#21487;&#20998;&#31163;&#26680;&#26159;&#22522;&#20110;&#25968;&#23398;&#32771;&#34385;&#65288;&#21487;&#20801;&#35768;&#26465;&#20214;&#65289;&#32780;&#38750;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#35770;&#35777;&#30340;&#28151;&#21512;&#35889;&#26041;&#27861;&#26469;&#29983;&#25104;&#21327;&#26041;&#24046;&#26680;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#25512;&#23548;&#20102;&#19968;&#31867;&#26032;&#22411;&#30340;&#29289;&#29702;&#21160;&#26426;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#23427;&#20204;&#30340;&#26681;&#28304;&#26469;&#33258;&#38543;&#26426;&#32447;&#24615;...
&lt;/p&gt;
&lt;p&gt;
Gaussian processes provide a flexible, non-parametric framework for the approximation of functions in high-dimensional spaces. The covariance kernel is the main engine of Gaussian processes, incorporating correlations that underpin the predictive distribution. For applications with spatiotemporal datasets, suitable kernels should model joint spatial and temporal dependence. Separable space-time covariance kernels offer simplicity and computational efficiency. However, non-separable kernels include space-time interactions that better capture observed correlations. Most non-separable kernels that admit explicit expressions are based on mathematical considerations (admissibility conditions) rather than first-principles derivations. We present a hybrid spectral approach for generating covariance kernels which is based on physical arguments. We use this approach to derive a new class of physically motivated, non-separable covariance kernels which have their roots in the stochastic, linear, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#36845;&#20195;&#32454;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#20013;&#37325;&#24314;&#30456;&#20114;&#20316;&#29992;&#30340;&#21452;&#25163;&#12290;&#36890;&#36807;&#23450;&#20041;2D&#35270;&#35273;&#29305;&#24449;&#31354;&#38388;&#21644;3D&#20851;&#33410;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#25163;&#37096;&#37325;&#24314;&#30340;&#20687;&#32032;&#23545;&#40784;&#65292;&#24182;&#39640;&#25928;&#22320;&#24314;&#27169;&#25163;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.02410</link><description>&lt;p&gt;
&#21333;&#20010;RGB&#22270;&#20687;&#20013;&#30456;&#20114;&#20316;&#29992;&#30340;&#21452;&#25163;&#37325;&#24314;&#30340;&#20998;&#31163;&#36845;&#20195;&#32454;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image. (arXiv:2302.02410v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#36845;&#20195;&#32454;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#20013;&#37325;&#24314;&#30456;&#20114;&#20316;&#29992;&#30340;&#21452;&#25163;&#12290;&#36890;&#36807;&#23450;&#20041;2D&#35270;&#35273;&#29305;&#24449;&#31354;&#38388;&#21644;3D&#20851;&#33410;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#25163;&#37096;&#37325;&#24314;&#30340;&#20687;&#32032;&#23545;&#40784;&#65292;&#24182;&#39640;&#25928;&#22320;&#24314;&#27169;&#25163;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#20013;&#37325;&#24314;&#30456;&#20114;&#20316;&#29992;&#30340;&#21452;&#25163;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#19968;&#26041;&#38754;&#65292;&#20004;&#21482;&#25163;&#20043;&#38388;&#20005;&#37325;&#30340;&#20114;&#30456;&#36974;&#25377;&#21644;&#31867;&#20284;&#30340;&#23616;&#37096;&#22806;&#35266;&#20351;&#24471;&#35270;&#35273;&#29305;&#24449;&#30340;&#25552;&#21462;&#21464;&#24471;&#22256;&#38590;&#65292;&#23548;&#33268;&#20272;&#35745;&#30340;&#25163;&#27169;&#22411;&#21644;&#22270;&#20687;&#30340;&#38169;&#20301;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30456;&#20114;&#20316;&#29992;&#30340;&#21452;&#25163;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#25163;&#23039;&#24577;&#30340;&#35299;&#31354;&#38388;&#65292;&#22686;&#21152;&#20102;&#32593;&#32476;&#23398;&#20064;&#30340;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31163;&#36845;&#20195;&#32454;&#21270;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#20687;&#32032;&#23545;&#40784;&#30340;&#25163;&#37096;&#37325;&#24314;&#65292;&#24182;&#39640;&#25928;&#22320;&#24314;&#27169;&#25163;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#21363;2D&#35270;&#35273;&#29305;&#24449;&#31354;&#38388;&#21644;3D&#20851;&#33410;&#29305;&#24449;&#31354;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35270;&#35273;&#29305;&#24449;&#22270;&#20013;&#33719;&#21462;&#20851;&#33410;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#21464;&#24418;&#22120;&#22312;3D&#20851;&#33410;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#25163;&#20869;&#37096;&#21644;&#25163;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing interacting hands from a single RGB image is a very challenging task. On the one hand, severe mutual occlusion and similar local appearance between two hands confuse the extraction of visual features, resulting in the misalignment of estimated hand meshes and the image. On the other hand, there are complex spatial relationship between interacting hands, which significantly increases the solution space of hand poses and increases the difficulty of network learning. In this paper, we propose a decoupled iterative refinement framework to achieve pixel-alignment hand reconstruction while efficiently modeling the spatial relationship between hands. Specifically, we define two feature spaces with different characteristics, namely 2D visual feature space and 3D joint feature space. First, we obtain joint-wise features from the visual feature map and utilize a graph convolution network and a transformer to perform intra- and inter-hand information interaction in the 3D joint fea
&lt;/p&gt;</description></item><item><title>NeSyFOLD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#33258;&#21160;&#26144;&#23556;&#31639;&#27861;&#26469;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#12290;</title><link>http://arxiv.org/abs/2301.12667</link><description>&lt;p&gt;
NeSyFOLD: &#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks. (arXiv:2301.12667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12667
&lt;/p&gt;
&lt;p&gt;
NeSyFOLD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#33258;&#21160;&#26144;&#23556;&#31639;&#27861;&#26469;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;NeSyFOLD&#65292;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#21019;&#24314;&#19968;&#20010;NeSyFOLD&#27169;&#22411;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;NeSyFOLD&#30340;&#23398;&#20064;&#27969;&#31243;&#22914;&#19979;&#65306;&#65288;i&#65289;&#25105;&#20204;&#39318;&#20808;&#22312;&#36755;&#20837;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;CNN&#65292;&#24182;&#25552;&#21462;&#26368;&#21518;&#19968;&#23618;&#26680;&#30340;&#28608;&#27963;&#20316;&#20026;&#20108;&#36827;&#21046;&#20540;&#65307;&#65288;ii&#65289;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#33021;&#22815;&#20998;&#31867;&#22270;&#20687;&#30340;&#36923;&#36753;&#31243;&#24207;&#8212;&#8212;&#34920;&#31034;&#20026;&#27599;&#20010;&#26680;&#23545;&#24212;&#30340;&#20108;&#36827;&#21046;&#28608;&#27963;&#21521;&#37327;&#65292;&#21516;&#26102;&#20135;&#29983;&#36923;&#36753;&#35299;&#37322;&#12290;&#30001;FOLD-SE-M&#31639;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#20855;&#26377;&#26680;&#32534;&#21495;&#20316;&#20026;&#35859;&#35789;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#27010;&#24565;&#12290;&#36825;&#20010;&#26144;&#23556;&#34987;&#29992;&#26469;&#23558;&#35268;&#21017;&#38598;&#20013;&#30340;&#35859;&#35789;&#21517;&#65288;&#26680;&#32534;&#21495;&#65289;&#26367;&#25442;&#20026;&#23545;&#24212;&#30340;&#35821;&#20041;&#27010;&#24565;&#26631;&#31614;&#12290;&#32467;&#26524;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#21487;&#20197;&#34987;&#20154;&#31867;&#30452;&#35266;&#22320;&#29702;&#35299;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;NeSyFOLD&#26694;&#26550;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#21644;&#35299;&#37322;&#24615;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel neurosymbolic framework called NeSyFOLD to extract logic rules from a CNN and create a NeSyFOLD model to classify images. NeSyFOLD's learning pipeline is as follows: (i) We first pre-train a CNN on the input image dataset and extract activations of the last layer kernels as binary values; (ii) Next, we use the FOLD-SE-M rule-based machine learning algorithm to generate a logic program that can classify an image -- represented as a vector of binary activations corresponding to each kernel -- while producing a logical explanation. The rules generated by the FOLD-SE-M algorithm have kernel numbers as predicates. We have devised a novel algorithm for automatically mapping the CNN kernels to semantic concepts in the images. This mapping is used to replace predicate names (kernel numbers) in the rule-set with corresponding semantic concept labels. The resulting rule-set is interpretable, and can be intuitively understood by humans. We compare our NeSyFOLD framework with th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#30340;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20165;&#20855;&#26377;&#20840;&#37327;&#21270;&#36523;&#20307;&#21464;&#37327;&#30340;&#35268;&#21017;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#22312;NOPI&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.07629</link><description>&lt;p&gt;
&#36890;&#36807;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#23454;&#29616;&#19968;&#33324;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalisation Through Negation and Predicate Invention. (arXiv:2301.07629v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07629
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#30340;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20165;&#20855;&#26377;&#20840;&#37327;&#21270;&#36523;&#20307;&#21464;&#37327;&#30340;&#35268;&#21017;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#22312;NOPI&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20174;&#23569;&#37327;&#30340;&#31034;&#20363;&#20013;&#36827;&#34892;&#27867;&#21270;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#12290;&#32467;&#21512;&#36825;&#20004;&#20010;&#29305;&#24615;&#21487;&#20197;&#20351;ILP&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#20165;&#20855;&#26377;&#20840;&#37327;&#21270;&#36523;&#20307;&#21464;&#37327;&#30340;&#35268;&#21017;&#26469;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24819;&#27861;&#23454;&#29616;&#22312;NOPI&#20013;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#35859;&#35789;&#21019;&#36896;&#30340;&#27491;&#24120;&#36923;&#36753;&#31243;&#24207;&#65292;&#21253;&#25324;&#20855;&#26377;&#20998;&#23618;&#21542;&#23450;&#30340;Datalog&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generalise from a small number of examples is a fundamental challenge in machine learning. To tackle this challenge, we introduce an inductive logic programming (ILP) approach that combines negation and predicate invention. Combining these two features allows an ILP system to generalise better by learning rules with universally quantified body-only variables. We implement our idea in NOPI, which can learn normal logic programs with predicate invention, including Datalog programs with stratified negation. Our experimental results on multiple domains show that our approach can improve predictive accuracies and learning times.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21355;&#26143;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#21457;&#29616;&#22303;&#22320;&#35206;&#30422;&#31867;&#21035;&#20043;&#38388;&#30340;&#26377;&#36259;&#20449;&#24687;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#40723;&#21169;&#26102;&#38388;&#36830;&#32493;&#30340;&#25200;&#21160;&#26469;&#24471;&#21040;&#26356;&#31232;&#30095;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.01520</link><description>&lt;p&gt;
&#25506;&#32034;&#21487;&#35299;&#37322;&#30340;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#65306;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy. (arXiv:2301.01520v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21355;&#26143;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#21457;&#29616;&#22303;&#22320;&#35206;&#30422;&#31867;&#21035;&#20043;&#38388;&#30340;&#26377;&#36259;&#20449;&#24687;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#40723;&#21169;&#26102;&#38388;&#36830;&#32493;&#30340;&#25200;&#21160;&#26469;&#24471;&#21040;&#26356;&#31232;&#30095;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#24037;&#20855;&#12290;&#22312;&#32473;&#23450;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#23547;&#25214;&#24182;&#21521;&#29992;&#25143;&#26174;&#31034;&#20915;&#31574;&#36793;&#30028;&#19978;&#31867;&#20284;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#20219;&#21153;&#30340;&#22810;&#31867;&#21035;&#35774;&#32622;&#20013;&#30340;&#21355;&#26143;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#30340;&#23545;&#25239;&#29983;&#25104;&#21453;&#20107;&#23454;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#22312;&#32473;&#23450;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24773;&#20917;&#19979;&#23545;&#30446;&#26631;&#31867;&#21035;&#27809;&#26377;&#20808;&#39564;&#20551;&#35774;&#12290;&#36825;&#31181;&#22266;&#26377;&#30340;&#28789;&#27963;&#24615;&#20801;&#35768;&#21457;&#29616;&#22303;&#22320;&#35206;&#30422;&#31867;&#21035;&#20043;&#38388;&#30340;&#26377;&#36259;&#20449;&#24687;&#20851;&#31995;&#12290;&#21478;&#19968;&#20010;&#29305;&#28857;&#26159;&#40723;&#21169;&#21453;&#20107;&#23454;&#19982;&#21407;&#22987;&#26679;&#26412;&#20043;&#38388;&#20165;&#22312;&#19968;&#20010;&#23567;&#32780;&#32039;&#20945;&#30340;&#26102;&#38388;&#27573;&#20869;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#20123;&#26102;&#38388;&#36830;&#32493;&#30340;&#25200;&#21160;&#20801;&#35768;&#24471;&#21040;&#26356;&#31232;&#30095;&#19988;&#22240;&#27492;&#26356;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24378;&#21046;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#21512;&#29702;&#24615;/&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are an emerging tool to enhance interpretability of deep learning models. Given a sample, these methods seek to find and display to the user similar samples across the decision boundary. In this paper, we propose a generative adversarial counterfactual approach for satellite image time series in a multi-class setting for the land cover classification task. One of the distinctive features of the proposed approach is the lack of prior assumption on the targeted class for a given counterfactual explanation. This inherent flexibility allows for the discovery of interesting information on the relationship between land cover classes. The other feature consists of encouraging the counterfactual to differ from the original sample only in a small and compact temporal segment. These time-contiguous perturbations allow for a much sparser and, thus, interpretable solution. Furthermore, plausibility/realism of the generated counterfactual explanations is enforced via the
&lt;/p&gt;</description></item><item><title>RECOMED&#26159;&#22522;&#20110;&#24739;&#32773;&#21644;&#33647;&#29289;&#29305;&#24449;&#35774;&#35745;&#30340;&#19968;&#31181;&#32508;&#21512;&#24615;&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;&#65292;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#23454;&#29616;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#24739;&#32773;&#26465;&#20214;&#21644;&#21382;&#21490;&#26469;&#36873;&#25321;&#21512;&#36866;&#33647;&#29289;&#65292;&#20197;&#21450;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.00280</link><description>&lt;p&gt;
RECOMED: &#19968;&#31181;&#32508;&#21512;&#24615;&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
RECOMED: A Comprehensive Pharmaceutical Recommendation System. (arXiv:2301.00280v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00280
&lt;/p&gt;
&lt;p&gt;
RECOMED&#26159;&#22522;&#20110;&#24739;&#32773;&#21644;&#33647;&#29289;&#29305;&#24449;&#35774;&#35745;&#30340;&#19968;&#31181;&#32508;&#21512;&#24615;&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;&#65292;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#23454;&#29616;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#24739;&#32773;&#26465;&#20214;&#21644;&#21382;&#21490;&#26469;&#36873;&#25321;&#21512;&#36866;&#33647;&#29289;&#65292;&#20197;&#21450;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20174;Drugs.com&#21644;Druglib.com&#25552;&#21462;&#30340;&#24739;&#32773;&#21644;&#33647;&#29289;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#32508;&#21512;&#24615;&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;&#23558;&#36825;&#20123;&#25968;&#25454;&#24211;&#30340;&#25968;&#25454;&#36827;&#34892;&#21512;&#24182;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#24739;&#32773;&#21644;&#33647;&#29289;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#23545;&#24739;&#32773;&#21644;&#33647;&#29289;&#36827;&#34892;&#20102;&#32858;&#31867;&#65292;&#28982;&#21518;&#26681;&#25454;&#24739;&#32773;&#25552;&#20379;&#30340;&#19981;&#21516;&#35780;&#32423;&#20197;&#21450;&#20174;&#24739;&#32773;&#21644;&#33647;&#29289;&#35268;&#26684;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#65292;&#24182;&#32771;&#34385;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#25512;&#33616;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#32452;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#32771;&#34385;&#24739;&#32773;&#30340;&#29366;&#20917;&#21644;&#21382;&#21490;&#26469;&#36873;&#25321;&#29305;&#23450;&#21512;&#36866;&#33647;&#29289;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#22312;&#39044;&#22788;&#29702;&#20013;&#65292;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21644;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#23545;&#31995;&#32479;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
A comprehensive pharmaceutical recommendation system was designed based on the patients and drugs features extracted from Drugs.com and Druglib.com. First, data from these databases were combined, and a dataset of patients and drug information was built. Secondly, the patients and drugs were clustered, and then the recommendation was performed using different ratings provided by patients, and importantly by the knowledge obtained from patients and drug specifications, and considering drug interactions. To the best of our knowledge, we are the first group to consider patients conditions and history in the proposed approach for selecting a specific medicine appropriate for that particular user. Our approach applies artificial intelligence (AI) models for the implementation. Sentiment analysis using natural language processing approaches is employed in pre-processing along with neural network-based methods and recommender system algorithms for modeling the system. In our work, patients co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;AraGPT-2&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;Euclidean&#12289;cosine&#12289;Jaccard&#21644;BLEU&#36317;&#31163;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2212.13939</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#25913;&#36827;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification. (arXiv:2212.13939v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;AraGPT-2&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;Euclidean&#12289;cosine&#12289;Jaccard&#21644;BLEU&#36317;&#31163;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#20805;&#36275;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20805;&#36275;&#24615;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#25506;&#32034;&#20102;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;DA&#36890;&#36807;&#23545;&#29616;&#26377;&#25968;&#25454;&#24212;&#29992;&#36716;&#25442;&#26469;&#29983;&#25104;&#26032;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#20174;&#32780;&#22686;&#21152;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#21464;&#21270;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#38463;&#25289;&#20271;&#35821;&#30340;DA&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#22914;&#37322;&#20041;&#25110;&#22522;&#20110;&#22122;&#22768;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38463;&#25289;&#20271;&#35821;DA&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#26368;&#36817;&#24378;&#22823;&#30340;&#24314;&#27169;&#25216;&#26415;AraGPT-2&#26469;&#36827;&#34892;&#22686;&#24378;&#36807;&#31243;&#12290;&#21033;&#29992;&#27431;&#27663;&#36317;&#31163;&#12289;&#20313;&#24358;&#36317;&#31163;&#12289;Jaccard&#36317;&#31163;&#21644;BLEU&#36317;&#31163;&#23545;&#29983;&#25104;&#30340;&#21477;&#23376;&#36827;&#34892;&#20102;&#19978;&#19979;&#25991;&#12289;&#35821;&#20041;&#12289;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#30340;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;AraBERT transformer&#23545;&#24773;&#24863;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of learning models heavily relies on the availability and adequacy of training data. To address the dataset adequacy issue, researchers have extensively explored data augmentation (DA) as a promising approach. DA generates new data instances through transformations applied to the available data, thereby increasing dataset size and variability. This approach has enhanced model performance and accuracy, particularly in addressing class imbalance problems in classification tasks. However, few studies have explored DA for the Arabic language, relying on traditional approaches such as paraphrasing or noising-based techniques. In this paper, we propose a new Arabic DA method that employs the recent powerful modeling technique, namely the AraGPT-2, for the augmentation process. The generated sentences are evaluated in terms of context, semantics, diversity, and novelty using the Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT transformer is used on sentime
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#20154;&#23398;&#20064;&#32773;&#22312;&#19982;&#38750;&#31283;&#24577;&#20154;&#31867;&#20849;&#21516;&#20114;&#21160;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#21644;&#25512;&#29702;&#20154;&#31867;&#31574;&#30053;&#21644;&#31574;&#30053;&#21160;&#24577;&#30340;&#39640;&#32423;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#24418;&#24335;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#19982;&#21160;&#24577;&#20154;&#31867;&#20849;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2212.09586</link><description>&lt;p&gt;
&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#20197;&#19982;&#20154;&#31867;&#20849;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learning Latent Representations to Co-Adapt to Humans. (arXiv:2212.09586v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09586
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#32773;&#22312;&#19982;&#38750;&#31283;&#24577;&#20154;&#31867;&#20849;&#21516;&#20114;&#21160;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#21644;&#25512;&#29702;&#20154;&#31867;&#31574;&#30053;&#21644;&#31574;&#30053;&#21160;&#24577;&#30340;&#39640;&#32423;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#24418;&#24335;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#19982;&#21160;&#24577;&#20154;&#31867;&#20849;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#20154;&#22312;&#23478;&#24237;&#12289;&#36947;&#36335;&#25110;&#24037;&#21378;&#20013;&#19982;&#20154;&#31867;&#20114;&#21160;&#26102;&#65292;&#20154;&#31867;&#30340;&#34892;&#20026;&#36890;&#24120;&#20250;&#22240;&#20026;&#26426;&#22120;&#20154;&#32780;&#25913;&#21464;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#38750;&#31283;&#24577;&#30340;&#20154;&#31867;&#26159;&#19968;&#20010;&#25361;&#25112;&#65306;&#26426;&#22120;&#20154;&#24050;&#32463;&#23398;&#20250;&#19982;&#21407;&#22987;&#20154;&#31867;&#21327;&#35843;&#19968;&#36215;&#25191;&#34892;&#30340;&#21160;&#20316;&#21487;&#33021;&#22312;&#20154;&#31867;&#36866;&#24212;&#26426;&#22120;&#20154;&#21518;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#24418;&#24335;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#65288;&#21363;&#33258;&#25105;&#20195;&#29702;&#65289;&#33021;&#22815;&#19982;&#21160;&#24577;&#20154;&#31867;&#65288;&#21363;&#20854;&#20182;&#20195;&#29702;&#65289;&#20849;&#21516;&#36866;&#24212;&#65292;&#21482;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#20302;&#32423;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#12290;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#65292;&#20154;&#31867;&#19981;&#20165;&#20250;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#20570;&#20986;&#21453;&#24212;&#65292;&#32780;&#19988;&#20154;&#31867;&#30340;&#21453;&#24212;&#26041;&#24335;&#38543;&#30528;&#26102;&#38388;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#26159;&#65292;&#26426;&#22120;&#20154;&#19981;&#38656;&#35201;&#24314;&#31435;&#20154;&#31867;&#30340;&#31934;&#30830;&#27169;&#22411;&#65292;&#32780;&#26159;&#21487;&#20197;&#23398;&#20064;&#21644;&#25512;&#29702;&#20154;&#31867;&#31574;&#30053;&#21644;&#31574;&#30053;&#21160;&#24577;&#30340;&#39640;&#32423;&#34920;&#31034;&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RILI&#65306;&#31283;&#20581;&#22320;&#24433;&#21709;&#28508;&#22312;&#24847;&#22270;&#12290;RILI&#39318;&#20808;&#23558;&#20302;&#32423;&#26426;&#22120;&#20154;&#35266;&#23519;&#23884;&#20837;&#21040;&#39640;&#32423;&#34920;&#31034;&#20013;...
&lt;/p&gt;
&lt;p&gt;
When robots interact with humans in homes, roads, or factories the human's behavior often changes in response to the robot. Non-stationary humans are challenging for robot learners: actions the robot has learned to coordinate with the original human may fail after the human adapts to the robot. In this paper we introduce an algorithmic formalism that enables robots (i.e., ego agents) to co-adapt alongside dynamic humans (i.e., other agents) using only the robot's low-level states, actions, and rewards. A core challenge is that humans not only react to the robot's behavior, but the way in which humans react inevitably changes both over time and between users. To deal with this challenge, our insight is that -- instead of building an exact model of the human -- robots can learn and reason over high-level representations of the human's policy and policy dynamics. Applying this insight we develop RILI: Robustly Influencing Latent Intent. RILI first embeds low-level robot observations into 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#24449;&#38477;&#32500;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20998;&#21106;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#26500;&#24314;&#22270;&#26102;&#21516;&#26102;&#20351;&#29992;&#23616;&#37096;&#29305;&#24449;&#21644;&#21407;&#22987;&#29305;&#24449;&#65292;&#20174;&#32780;&#33021;&#26356;&#22909;&#22320;&#36827;&#34892;&#32858;&#31867;&#21644;&#20998;&#31867;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2212.05853</link><description>&lt;p&gt;
DeepCut: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#20998;&#21106;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
DeepCut: Unsupervised Segmentation using Graph Neural Networks Clustering. (arXiv:2212.05853v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#24449;&#38477;&#32500;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20998;&#21106;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#26500;&#24314;&#22270;&#26102;&#21516;&#26102;&#20351;&#29992;&#23616;&#37096;&#29305;&#24449;&#21644;&#21407;&#22987;&#29305;&#24449;&#65292;&#20174;&#32780;&#33021;&#26356;&#22909;&#22320;&#36827;&#34892;&#32858;&#31867;&#21644;&#20998;&#31867;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#21106;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#20026;&#20102;&#35757;&#32451;&#30417;&#30563;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#32791;&#21147;&#30340;&#24037;&#20316;&#65292;&#22240;&#27492;&#28608;&#21169;&#20102;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#39044;&#35757;&#32451;&#32593;&#32476;&#20013;&#25552;&#21462;&#28145;&#24230;&#29305;&#24449;&#26469;&#26500;&#24314;&#22270;&#65292;&#28982;&#21518;&#20877;&#24212;&#29992;&#32463;&#20856;&#30340;&#32858;&#31867;&#26041;&#27861;&#22914;k-means&#21644;&#24402;&#19968;&#21270;&#21106;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#29305;&#24449;&#20013;&#30340;&#39640;&#32500;&#20449;&#24687;&#38477;&#32500;&#20026;&#19968;&#23545;&#19968;&#30340;&#26631;&#37327;&#20146;&#21644;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#29992;&#23427;&#26469;&#26367;&#20195;&#32463;&#20856;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#22312;&#20248;&#21270;&#30456;&#21516;&#30340;&#32858;&#31867;&#30446;&#26631;&#20989;&#25968;&#30340;&#21516;&#26102;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;GNN&#21516;&#26102;&#25509;&#21463;&#23616;&#37096;&#22270;&#20687;&#29305;&#24449;&#20043;&#38388;&#30340;&#19968;&#23545;&#19968;&#20146;&#21644;&#21147;&#21644;&#21407;&#22987;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#12290;&#21407;&#22987;&#29305;&#24449;&#19982;&#32858;&#31867;&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#36830;&#25509;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#38544;&#24335;&#22320;&#23545;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#32858;&#31867;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image segmentation is a fundamental task in computer vision. Data annotation for training supervised methods can be labor-intensive, motivating unsupervised methods. Current approaches often rely on extracting deep features from pre-trained networks to construct a graph, and classical clustering methods like k-means and normalized-cuts are then applied as a post-processing step. However, this approach reduces the high-dimensional information encoded in the features to pair-wise scalar affinities. To address this limitation, this study introduces a lightweight Graph Neural Network (GNN) to replace classical clustering methods while optimizing for the same clustering objective function. Unlike existing methods, our GNN takes both the pair-wise affinities between local image features and the raw features as input. This direct connection between the raw features and the clustering objective enables us to implicitly perform classification of the clusters between different graphs, resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#28041;&#21450;&#23545;&#35937;&#23646;&#24615;&#12289;&#20316;&#29992;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#20197;&#25903;&#25345; OCL&#65292;&#25552;&#20986;&#20102; Object Concept Reasoning Network (OCRN) &#20316;&#20026;&#22522;&#32447;&#65292;&#25552;&#21319;&#20102;&#23545;&#35937;&#35748;&#30693;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2212.02710</link><description>&lt;p&gt;
&#36229;&#36234;&#23545;&#35937;&#35782;&#21035;&#65306;&#38754;&#21521;&#23545;&#35937;&#27010;&#24565;&#23398;&#20064;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Beyond Object Recognition: A New Benchmark towards Object Concept Learning. (arXiv:2212.02710v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#28041;&#21450;&#23545;&#35937;&#23646;&#24615;&#12289;&#20316;&#29992;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#20197;&#25903;&#25345; OCL&#65292;&#25552;&#20986;&#20102; Object Concept Reasoning Network (OCRN) &#20316;&#20026;&#22522;&#32447;&#65292;&#25552;&#21319;&#20102;&#23545;&#35937;&#35748;&#30693;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23545;&#35937;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#20307;&#39564;&#30340;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#22312;&#23545;&#35937;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#21069;&#26426;&#22120;&#20173;&#28982;&#38590;&#20197;&#23398;&#20064;&#26356;&#39640;&#23618;&#27425;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#23545;&#35937;&#20855;&#26377;&#21738;&#20123;&#23646;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#23545;&#35937;&#20570;&#20160;&#20040;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#20197;&#25512;&#21160;&#23545;&#35937;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;&#23427;&#35201;&#27714;&#26426;&#22120;&#25512;&#29702;&#20986;&#23545;&#35937;&#30340;&#20316;&#29992;&#65292;&#24182;&#21516;&#26102;&#32473;&#20986;&#21407;&#22240;&#65306;&#26159;&#21738;&#20123;&#23646;&#24615;&#20351;&#24471;&#19968;&#20010;&#23545;&#35937;&#20855;&#26377;&#36825;&#20123;&#20316;&#29992;&#12290;&#20026;&#20102;&#25903;&#25345; OCL&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#65292;&#21253;&#25324;&#19977;&#20010;&#23618;&#27425;&#30340;&#23545;&#35937;&#27010;&#24565;&#65288;&#31867;&#21035;&#12289;&#23646;&#24615;&#12289;&#20316;&#29992;&#65289;&#65292;&#20197;&#21450;&#19977;&#20010;&#23618;&#27425;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512; OCL &#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#65306;Object Concept Reasoning Network (OCRN)&#12290;&#23427;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#21644;&#27010;&#24565;&#23454;&#20363;&#21270;&#26469;&#25512;&#26029;&#19977;&#20010;&#23618;&#27425;&#65292;&#36981;&#24490;&#23427;&#20204;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding objects is a central building block of artificial intelligence, especially for embodied AI. Even though object recognition excels with deep learning, current machines still struggle to learn higher-level knowledge, e.g., what attributes an object has, and what can we do with an object. In this work, we propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances. To support OCL, we build a densely annotated knowledge base including extensive labels for three levels of object concept (category, attribute, affordance), and the causal relations of three levels. By analyzing the causal structure of OCL, we present a baseline, Object Concept Reasoning Network (OCRN). It leverages causal intervention and concept instantiation to infer the three levels following their causal relations. In ex
&lt;/p&gt;</description></item><item><title>PhysDiff&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20154;&#20307;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#32422;&#26463;&#34701;&#20837;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#32780;&#36924;&#30495;&#30340;&#20154;&#20307;&#21160;&#20316;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#29289;&#29702;&#32570;&#38519;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.02500</link><description>&lt;p&gt;
PhysDiff: &#29289;&#29702;&#24341;&#23548;&#30340;&#20154;&#20307;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PhysDiff: Physics-Guided Human Motion Diffusion Model. (arXiv:2212.02500v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02500
&lt;/p&gt;
&lt;p&gt;
PhysDiff&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20154;&#20307;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#32422;&#26463;&#34701;&#20837;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#32780;&#36924;&#30495;&#30340;&#20154;&#20307;&#21160;&#20316;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#29289;&#29702;&#32570;&#38519;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#20026;&#29983;&#25104;&#22810;&#26679;&#32780;&#36924;&#30495;&#30340;&#20154;&#20307;&#21160;&#20316;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#24448;&#24448;&#24573;&#35270;&#20102;&#29289;&#29702;&#23450;&#24459;&#65292;&#24182;&#19988;&#32463;&#24120;&#29983;&#25104;&#20986;&#20855;&#26377;&#26126;&#26174;&#32570;&#38519;&#30340;&#19981;&#31526;&#21512;&#29289;&#29702;&#35268;&#24459;&#30340;&#21160;&#20316;&#65292;&#22914;&#28014;&#31354;&#12289;&#28369;&#33050;&#21644;&#36879;&#22320;&#12290;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#29983;&#25104;&#21160;&#20316;&#30340;&#36136;&#37327;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#30340;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;PhysDiff&#65289;&#65292;&#23427;&#23558;&#29289;&#29702;&#32422;&#26463;&#34701;&#20837;&#21040;&#25193;&#25955;&#36807;&#31243;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#27169;&#25311;&#22120;&#20013;&#36816;&#21160;&#27169;&#20223;&#30340;&#29289;&#29702;&#36816;&#21160;&#25237;&#24433;&#27169;&#22359;&#65292;&#29992;&#20110;&#23558;&#25193;&#25955;&#27493;&#39588;&#30340;&#21435;&#22122;&#21160;&#20316;&#25237;&#24433;&#20026;&#31526;&#21512;&#29289;&#29702;&#35268;&#24459;&#30340;&#21160;&#20316;&#12290;&#25237;&#24433;&#21518;&#30340;&#21160;&#20316;&#36827;&#19968;&#27493;&#29992;&#20110;&#19979;&#19968;&#20010;&#25193;&#25955;&#27493;&#39588;&#65292;&#20197;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#27169;&#22411;&#20013;&#20351;&#29992;&#29289;&#29702;&#23450;&#24459;&#20351;&#24471;&#21160;&#20316;&#36880;&#27493;&#36235;&#21521;&#20110;&#31526;&#21512;&#29289;&#29702;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models hold great promise for generating diverse and realistic human motions. However, existing motion diffusion models largely disregard the laws of physics in the diffusion process and often generate physically-implausible motions with pronounced artifacts such as floating, foot sliding, and ground penetration. This seriously impacts the quality of generated motions and limits their real-world application. To address this issue, we present a novel physics-guided motion diffusion model (PhysDiff), which incorporates physical constraints into the diffusion process. Specifically, we propose a physics-based motion projection module that uses motion imitation in a physics simulator to project the denoised motion of a diffusion step to a physically-plausible motion. The projected motion is further used in the next diffusion step to guide the denoising diffusion process. Intuitively, the use of physics in our model iteratively pulls the motion toward a physically-plausib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ELICIT&#65292;&#19968;&#31181;&#20174;&#21333;&#24352;&#22270;&#29255;&#23398;&#20064;&#20154;&#31867;&#29305;&#23450;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;3D&#20960;&#20309;&#20808;&#39564;&#21644;&#35270;&#35273;&#35821;&#20041;&#20808;&#39564;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#25968;&#25454;&#39640;&#25928;&#30340;&#36924;&#30495;&#21487;&#21160;3D&#20154;&#20307;&#30340;&#21019;&#24314;&#12290;</title><link>http://arxiv.org/abs/2212.02469</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#27169;&#22411;&#20808;&#39564;&#30340;&#19968;&#27425;&#24615;&#38544;&#24335;&#21160;&#30011;&#21270;&#22836;&#20687;&#21046;&#20316;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-shot Implicit Animatable Avatars with Model-based Priors. (arXiv:2212.02469v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ELICIT&#65292;&#19968;&#31181;&#20174;&#21333;&#24352;&#22270;&#29255;&#23398;&#20064;&#20154;&#31867;&#29305;&#23450;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;3D&#20960;&#20309;&#20808;&#39564;&#21644;&#35270;&#35273;&#35821;&#20041;&#20808;&#39564;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#25968;&#25454;&#39640;&#25928;&#30340;&#36924;&#30495;&#21487;&#21160;3D&#20154;&#20307;&#30340;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21019;&#24314;&#20154;&#31867;&#22836;&#20687;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#38656;&#35201;&#31264;&#23494;&#36755;&#20837;&#20449;&#21495;&#65288;&#22914;&#35270;&#39057;&#25110;&#22810;&#35270;&#35282;&#22270;&#20687;&#65289;&#65292;&#35201;&#20040;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#29305;&#23450;3D&#20154;&#20307;&#25968;&#25454;&#38598;&#20013;&#23398;&#21040;&#30340;&#20808;&#39564;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#31232;&#30095;&#35270;&#35282;&#36755;&#20837;&#36827;&#34892;&#37325;&#24314;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#22312;&#20165;&#26377;&#19968;&#24352;&#22270;&#20687;&#26102;&#26080;&#27861;&#23454;&#29616;&#36924;&#30495;&#37325;&#24314;&#12290;&#20026;&#20102;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#36924;&#30495;&#21487;&#21160;3D&#20154;&#20307;&#30340;&#21019;&#24314;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELICIT&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#19968;&#24352;&#22270;&#29255;&#23398;&#20064;&#20154;&#20307;&#29305;&#23450;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26032;&#26041;&#27861;&#12290;&#21463;&#21040;&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#20272;&#35745;&#36523;&#20307;&#20960;&#20309;&#24418;&#29366;&#24182;&#20174;&#19968;&#24352;&#22270;&#29255;&#20013;&#24819;&#35937;&#36896;&#22411;&#23436;&#25972;&#30340;&#34915;&#26588;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#22312;ELICIT&#20013;&#21033;&#29992;&#20102;&#20004;&#20010;&#20808;&#39564;&#65306;3D&#20960;&#20309;&#20808;&#39564;&#21644;&#35270;&#35273;&#35821;&#20041;&#20808;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ELICIT&#21033;&#29992;&#19968;&#20010;&#33945;&#30382;&#39030;&#28857;&#27169;&#26495;&#27169;&#22411;&#65288;&#21363;SMPL&#65289;&#30340;3D&#36523;&#20307;&#24418;&#29366;&#20960;&#20309;&#20808;&#39564;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;CLIP&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#20102;&#35270;&#35273;&#26381;&#35013;&#35821;&#20041;&#20808;&#39564;&#12290;&#36825;&#20004;&#20010;&#20808;&#39564;&#22343;&#29992;&#20110;&#20174;&#21333;&#20010;&#22270;&#20687;&#36827;&#34892;&#36924;&#30495;&#30340;&#21487;&#21160;3D&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing neural rendering methods for creating human avatars typically either require dense input signals such as video or multi-view images, or leverage a learned prior from large-scale specific 3D human datasets such that reconstruction can be performed with sparse-view inputs. Most of these methods fail to achieve realistic reconstruction when only a single image is available. To enable the data-efficient creation of realistic animatable 3D humans, we propose ELICIT, a novel method for learning human-specific neural radiance fields from a single image. Inspired by the fact that humans can effortlessly estimate the body geometry and imagine full-body clothing from a single image, we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior. Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned vertex-based template model (i.e., SMPL) and implements the visual clothing semantic prior with the CLIP-based pre-trained models. Both priors are used 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15046</link><description>&lt;p&gt;
&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#20010;&#19990;&#32426;&#37324;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#38632;&#27700;&#23545;&#20154;&#31867;&#29983;&#27963;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#38477;&#27700;&#39044;&#27979;&#27169;&#22411;&#21253;&#25324;&#23450;&#37327;&#38477;&#27700;&#39044;&#27979; (QPF) &#27169;&#22411;&#12289;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518; (ConvLSTM) &#27169;&#22411;&#20197;&#21450;&#26368;&#26032;&#30340; MetNet-2 &#31561;&#22810;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476; (PCT-CycleGAN) &#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#21463;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476; (CycleGAN) &#24378;&#22823;&#30340;&#22270;&#20687;&#36716;&#25442;&#24615;&#33021;&#21551;&#21457;&#12290;PCT-CycleGAN &#20351;&#29992;&#20004;&#20010;&#20855;&#26377;&#21521;&#21069;&#21644;&#21521;&#21518;&#26102;&#38388;&#21160;&#24577;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#29983;&#25104;&#26102;&#24207;&#24615;&#65292;&#27599;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#24222;&#22823;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#65292;&#20197;&#36924;&#36817;&#34920;&#31034;&#27599;&#20010;&#26041;&#21521;&#19978;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#20026;&#20102;&#21019;&#24314;&#37197;&#23545;&#20114;&#34917;&#24490;&#29615;&#20043;&#38388;&#30340;&#24378;&#20581;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PCT-CycleGAN &#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#27169;&#24335;&#30340;&#32467;&#26524;&#23548;&#21521;&#30340;&#35268;&#33539;&#36807;&#31243;&#30417;&#25511;&#31995;&#32479;&#65292;&#25512;&#33616;&#22312;&#36807;&#31243;&#25191;&#34892;&#26399;&#38388;&#24517;&#39035;&#20445;&#35777;&#30340;&#27963;&#21160;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.04880</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#27169;&#24335;&#30340;&#32467;&#26524;&#23548;&#21521;&#30340;&#35268;&#33539;&#36807;&#31243;&#30417;&#25511;
&lt;/p&gt;
&lt;p&gt;
Outcome-Oriented Prescriptive Process Monitoring Based on Temporal Logic Patterns. (arXiv:2211.04880v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04880
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#27169;&#24335;&#30340;&#32467;&#26524;&#23548;&#21521;&#30340;&#35268;&#33539;&#36807;&#31243;&#30417;&#25511;&#31995;&#32479;&#65292;&#25512;&#33616;&#22312;&#36807;&#31243;&#25191;&#34892;&#26399;&#38388;&#24517;&#39035;&#20445;&#35777;&#30340;&#27963;&#21160;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#33539;&#36807;&#31243;&#30417;&#25511;&#31995;&#32479;&#22312;&#19994;&#21153;&#36807;&#31243;&#25191;&#34892;&#26399;&#38388;&#24314;&#35758;&#24178;&#39044;&#25514;&#26045;&#65292;&#20197;&#38450;&#27490;&#20986;&#29616;&#36127;&#38754;&#32467;&#26524;&#12290;&#36825;&#31181;&#24178;&#39044;&#25514;&#26045;&#24517;&#39035;&#21487;&#38752;&#65292;&#21363;&#23427;&#20204;&#24517;&#39035;&#20445;&#35777;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#25110;&#24615;&#33021;&#65292;&#24182;&#19988;&#23427;&#20204;&#24517;&#39035;&#28789;&#27963;&#65292;&#21363;&#23427;&#20204;&#24517;&#39035;&#36991;&#20813;&#25512;&#32763;&#27491;&#24120;&#30340;&#27969;&#31243;&#25191;&#34892;&#25110;&#24378;&#21046;&#25191;&#34892;&#32473;&#23450;&#30340;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#35268;&#33539;&#36807;&#31243;&#30417;&#25511;&#35299;&#20915;&#26041;&#26696;&#65292;&#34429;&#28982;&#22312;&#25512;&#33616;&#21487;&#38752;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#29992;&#25143;&#25552;&#20379;&#20102;&#38750;&#24120;&#20855;&#20307;&#30340;&#65288;&#24207;&#21015;&#65289;&#27963;&#21160;&#30340;&#25512;&#33616;&#65292;&#32780;&#24182;&#19981;&#20851;&#24515;&#36825;&#20123;&#25512;&#33616;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26524;&#23548;&#21521;&#30340;&#35268;&#33539;&#36807;&#31243;&#30417;&#25511;&#31995;&#32479;&#65292;&#25512;&#33616;&#22312;&#36807;&#31243;&#25191;&#34892;&#26399;&#38388;&#24517;&#39035;&#20445;&#35777;&#30340;&#27963;&#21160;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescriptive Process Monitoring systems recommend, during the execution of a business process, interventions that, if followed, prevent a negative outcome of the process. Such interventions have to be reliable, that is, they have to guarantee the achievement of the desired outcome or performance, and they have to be flexible, that is, they have to avoid overturning the normal process execution or forcing the execution of a given activity. Most of the existing Prescriptive Process Monitoring solutions, however, while performing well in terms of recommendation reliability, provide the users with very specific (sequences of) activities that have to be executed without caring about the feasibility of these recommendations. In order to face this issue, we propose a new Outcome-Oriented Prescriptive Process Monitoring system recommending temporal relations between activities that have to be guaranteed during the process execution in order to achieve a desired outcome. This softens the mandat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02641</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65306;&#26469;&#33258;&#26102;&#39057;&#20998;&#26512;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#30340;&#20998;&#31867;&#26159;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#22522;&#30784;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#22791;&#21463;&#36861;&#25447;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#65292;MI-EEG&#20998;&#31867;&#22120;&#30340;&#36235;&#21183;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#36716;&#21464;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#25552;&#39640;&#12290; Tensor-CSPNet&#30340;&#20986;&#29616;&#26159;BCI&#30740;&#31350;&#20013;&#31532;&#19968;&#20010;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#65292;&#20854;&#24402;&#22240;&#20110;&#20449;&#21495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#30340;&#29305;&#24449;&#21270;&#12290;&#20174;&#26681;&#26412;&#19978;&#35762;&#65292;Tensor-CSPNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#12290;&#19982;&#21033;&#29992;EEG&#20449;&#21495;&#30340;&#19968;&#38454;&#32479;&#35745;&#37327;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#21033;&#29992;&#36825;&#20123;&#20108;&#38454;&#32479;&#35745;&#37327;&#20195;&#34920;&#20102;&#32463;&#20856;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#20123;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#21306;&#20998;&#20449;&#24687;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;MI-EEG&#20998;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;GDL&#20998;&#31867;&#22120;&#65292;
&lt;/p&gt;
&lt;p&gt;
The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FS-DETR&#30340;&#23569;&#26679;&#26412;&#26816;&#27979;&#21464;&#24418;&#22120;&#65292;&#23427;&#22522;&#20110;&#35270;&#35273;&#25552;&#31034;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#19988;&#26080;&#38656;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2210.04845</link><description>&lt;p&gt;
FS-DETR: &#24102;&#25552;&#31034;&#21644;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#23569;&#26679;&#26412;&#26816;&#27979;&#21464;&#24418;&#22120;
&lt;/p&gt;
&lt;p&gt;
FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training. (arXiv:2210.04845v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FS-DETR&#30340;&#23569;&#26679;&#26412;&#26816;&#27979;&#21464;&#24418;&#22120;&#65292;&#23427;&#22522;&#20110;&#35270;&#35273;&#25552;&#31034;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#19988;&#26080;&#38656;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#65288;FSOD&#65289;&#65292;&#21363;&#22312;&#32473;&#23450;&#23569;&#37327;&#26410;&#22312;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#27169;&#26495;&#65288;&#31034;&#20363;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#22312;&#19968;&#32452;&#22270;&#20687;&#20013;&#26816;&#27979;&#20986;&#35813;&#31867;&#21035;&#30340;&#25152;&#26377;&#23454;&#20363;&#12290;&#20174;&#23454;&#38469;&#35282;&#24230;&#32771;&#34385;&#65292;FSOD&#31995;&#32479;&#24517;&#39035;&#28385;&#36275;&#20197;&#19979;&#35201;&#27714;&#65306;&#65288;a&#65289;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#20219;&#20309;&#24494;&#35843;&#65292;&#21363;&#21487;&#20197;&#21407;&#26679;&#20351;&#29992;&#65307;&#65288;b&#65289;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#26032;&#31867;&#21035;&#23545;&#35937;&#65292;&#24182;&#25903;&#25345;&#27599;&#20010;&#31867;&#21035;&#30340;&#20219;&#24847;&#25968;&#37327;&#30340;&#31034;&#20363;&#65307;&#65288;c&#65289;&#33021;&#22815;&#36798;&#21040;&#19982;&#23553;&#38381;&#31995;&#32479;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#65288;a&#65289;-&#65288;c&#65289;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#20570;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#26816;&#27979;&#21464;&#24418;&#22120;&#65288;FS-DETR&#65289;&#65292;&#22522;&#20110;&#35270;&#35273;&#25552;&#31034;&#23454;&#29616;&#20102;&#23545;&#65288;a&#65289;&#21644;&#65288;b&#65289;&#30340;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22522;&#20110;DETR&#26694;&#26550;&#36827;&#34892;&#25193;&#23637;&#65292;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#24605;&#24819;&#65306;&#65288;1&#65289;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20351;&#29992;&#25552;&#20379;&#30340;&#26032;&#31867;&#21035;&#35270;&#35273;&#27169;&#26495;&#20316;&#20026;&#35270;&#35273;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is on Few-Shot Object Detection (FSOD), where given a few templates (examples) depicting a novel class (not seen during training), the goal is to detect all of its occurrences within a set of images. From a practical perspective, an FSOD system must fulfil the following desiderata: (a) it must be used as is, without requiring any fine-tuning at test time, (b) it must be able to process an arbitrary number of novel objects concurrently while supporting an arbitrary number of examples from each class and (c) it must achieve accuracy comparable to a closed system. Towards satisfying (a)-(c), in this work, we make the following contributions: We introduce, for the first time, a simple, yet powerful, few-shot detection transformer (FS-DETR) based on visual prompting that can address both desiderata (a) and (b). Our system builds upon the DETR framework, extending it based on two key ideas: (1) feed the provided visual templates of the novel classes as visual prompts during test t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.02390</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Prompt Learning for Image-Language Model Generalization. (arXiv:2210.02390v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#30340;&#22270;&#20687;-&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#39640;&#25928;&#30340;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#25552;&#31034;&#23398;&#20064;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25552;&#31034;&#23398;&#20064;&#23558;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#35270;&#20026;&#21487;&#35757;&#32451;&#30340;&#65292;&#21516;&#26102;&#20923;&#32467;&#20854;&#20313;&#37096;&#20998;&#65292;&#24182;&#20248;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#24050;&#30693;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#36825;&#24433;&#21709;&#20102;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#27491;&#21017;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#25552;&#31034;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#21046;&#23450;&#20026;&#21464;&#20998;&#25512;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20943;&#23569;&#23545;&#24050;&#35265;&#25552;&#31034;&#30340;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#25552;&#31034;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20197;&#27010;&#29575;&#30340;&#26041;&#24335;&#23545;&#36755;&#20837;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#20316;&#20026;&#20808;&#39564;&#20998;&#24067;&#65292;&#20351;&#25105;&#20204;&#30340;&#25552;&#35758;&#19982;&#22522;&#20110;&#22270;&#20687;&#26080;&#26465;&#20214;&#25110;&#26377;&#26465;&#20214;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#20860;&#23481;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#26412;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational image-language models have generated considerable interest due to their efficient adaptation to downstream tasks by prompt learning. Prompt learning treats part of the language model input as trainable while freezing the rest, and optimizes an Empirical Risk Minimization objective. However, Empirical Risk Minimization is known to suffer from distributional shifts which hurt generalizability to prompts unseen during training. By leveraging the regularization ability of Bayesian methods, we frame prompt learning from the Bayesian perspective and formulate it as a variational inference problem. Our approach regularizes the prompt space, reduces overfitting to the seen prompts and improves the prompt generalization on unseen prompts. Our framework is implemented by modeling the input prompt space in a probabilistic manner, as an a priori distribution which makes our proposal compatible with prompt learning approaches that are unconditional or conditional on the image. We demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14609</link><description>&lt;p&gt;
&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#33719;&#24471;&#20808;&#36827;&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#20915;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#23384;&#20648;&#21644;&#27169;&#22411;&#35757;&#32451;&#21464;&#24471;&#26114;&#36149;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#21512;&#25104;&#20445;&#30041;&#21407;&#22987;&#22823;&#22411;&#25968;&#25454;&#38598;&#22823;&#22810;&#25968;&#20449;&#24687;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#21305;&#37197;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#21442;&#25968;&#30340;&#32500;&#24230;&#36890;&#24120;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#21442;&#25968;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#38590;&#20197;&#21305;&#37197;&#65292;&#38477;&#20302;&#20102;&#33976;&#39311;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#20462;&#21098;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#21512;&#25104;&#26356;&#21152;&#31283;&#20581;&#30340;&#33976;&#39311;&#25968;&#25454;&#38598;&#24182;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
&lt;/p&gt;</description></item><item><title>FiBiNet++&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#29305;&#24449;&#20132;&#20114;&#23618;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;FiBiNet&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;12&#20493;&#33267;16&#20493;&#30340;&#21442;&#25968;&#20943;&#23567;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05016</link><description>&lt;p&gt;
FiBiNet++&#65306;&#36890;&#36807;&#20302;&#31209;&#29305;&#24449;&#20132;&#20114;&#23618;&#20943;&#23567;CTR&#39044;&#27979;&#27169;&#22411;&#30340;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;
FiBiNet++: Reducing Model Size by Low Rank Feature Interaction Layer for CTR Prediction. (arXiv:2209.05016v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05016
&lt;/p&gt;
&lt;p&gt;
FiBiNet++&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#29305;&#24449;&#20132;&#20114;&#23618;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;FiBiNet&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;12&#20493;&#33267;16&#20493;&#30340;&#21442;&#25968;&#20943;&#23567;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#20272;&#35745;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#28145;&#24230;&#27169;&#22411;&#12290;&#19968;&#20123;&#30740;&#31350;&#35777;&#26126;FiBiNet&#26159;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#24182;&#19988;&#22312;Avazu&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20102;&#25152;&#26377;&#20854;&#20182;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;FiBiNet&#30340;&#22823;&#22411;&#27169;&#22411;&#22823;&#23567;&#38480;&#21046;&#20102;&#23427;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FiBiNet++&#27169;&#22411;&#26469;&#37325;&#26032;&#35774;&#35745;FiBiNet&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25216;&#26415;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#20302;&#31209;&#23618;&#8221;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#20132;&#20114;&#65292;&#23427;&#20316;&#20026;&#23454;&#29616;&#27169;&#22411;&#20248;&#36234;&#21387;&#32553;&#27604;&#30340;&#20851;&#38190;&#39537;&#21160;&#22120;&#12290;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;FiBiNet++&#26377;&#25928;&#22320;&#23558;FiBiNet&#30340;&#38750;&#23884;&#20837;&#24335;&#27169;&#22411;&#21442;&#25968;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20943;&#23567;&#20102;12&#20493;&#33267;16&#20493;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;CTR&#26041;&#27861;&#65288;&#21253;&#25324;FiBiNet&#65289;&#30456;&#27604;&#65292;FiBiNet++&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) estimation has become one of the most fundamental tasks in many real-world applications and various deep models have been proposed. Some research has proved that FiBiNet is one of the best performance models and outperforms all other models on Avazu dataset. However, the large model size of FiBiNet hinders its wider application. In this paper, we propose a novel FiBiNet++ model to redesign FiBiNet's model structure, which greatly reduces model size while further improves its performance. One of the primary techniques involves our proposed "Low Rank Layer" focused on feature interaction, which serves as a crucial driver of achieving a superior compression ratio for models. Extensive experiments on three public datasets show that FiBiNet++ effectively reduces non-embedding model parameters of FiBiNet by 12x to 16x on three datasets. On the other hand, FiBiNet++ leads to significant performance improvements compared to state-of-the-art CTR methods, including FiBiN
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#28857;-BAX&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34394;&#25311;&#30446;&#26631;&#26469;&#39640;&#25928;&#35843;&#25972;&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#21457;&#23556;&#24230;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#20351;&#29992;&#20256;&#32479;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#36827;&#34892;&#32531;&#24930;&#32780;&#20302;&#25928;&#30340;&#22810;&#28857;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#24555;&#36895;&#23398;&#20064;&#27169;&#22411;&#35745;&#31639;&#21457;&#23556;&#24230;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;Linac&#30456;&#24178;&#20809;&#28304;(LCLS)&#21644;Facility for Adv&#20013;&#26368;&#23567;&#21270;&#21457;&#23556;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.04587</link><description>&lt;p&gt;
&#22810;&#28857;-BAX: &#19968;&#31181;&#36890;&#36807;&#34394;&#25311;&#30446;&#26631;&#39640;&#25928;&#35843;&#25972;&#31890;&#23376;&#21152;&#36895;&#22120;&#21457;&#23556;&#24230;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multipoint-BAX: A New Approach for Efficiently Tuning Particle Accelerator Emittance via Virtual Objectives. (arXiv:2209.04587v4 [physics.acc-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#28857;-BAX&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34394;&#25311;&#30446;&#26631;&#26469;&#39640;&#25928;&#35843;&#25972;&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#21457;&#23556;&#24230;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#20351;&#29992;&#20256;&#32479;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#36827;&#34892;&#32531;&#24930;&#32780;&#20302;&#25928;&#30340;&#22810;&#28857;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#24555;&#36895;&#23398;&#20064;&#27169;&#22411;&#35745;&#31639;&#21457;&#23556;&#24230;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;Linac&#30456;&#24178;&#20809;&#28304;(LCLS)&#21644;Facility for Adv&#20013;&#26368;&#23567;&#21270;&#21457;&#23556;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26463;&#21457;&#23556;&#24230;&#23545;&#20110;&#39640;&#20142;&#24230;&#21152;&#36895;&#22120;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20248;&#21270;&#36890;&#24120;&#20250;&#21463;&#21040;&#26102;&#38388;&#38480;&#21046;&#65292;&#22240;&#20026;&#21457;&#23556;&#24230;&#35745;&#31639;&#36890;&#24120;&#26159;&#36890;&#36807;&#22235;&#26497;&#25195;&#25551;&#23436;&#25104;&#30340;&#65292;&#32780;&#22235;&#26497;&#25195;&#25551;&#36890;&#24120;&#36739;&#24930;&#12290;&#36825;&#31181;&#35745;&#31639;&#26159;&#19968;&#31181;&#22810;&#28857;&#26597;&#35810;&#65292;&#21363;&#27599;&#20010;&#26597;&#35810;&#37117;&#38656;&#35201;&#22810;&#20010;&#36741;&#21161;&#27979;&#37327;&#12290;&#20256;&#32479;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#22788;&#29702;&#36825;&#26679;&#30340;&#30446;&#26631;&#26102;&#36895;&#24230;&#24930;&#19988;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#33719;&#21462;&#23436;&#25972;&#30340;&#27979;&#37327;&#24207;&#21015;&#65292;&#20294;&#27599;&#20010;&#26597;&#35810;&#20165;&#36820;&#22238;&#21457;&#23556;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36125;&#21494;&#26031;&#31639;&#27861;&#25191;&#34892;(BAX)&#24212;&#29992;&#20110;&#26597;&#35810;&#21644;&#24314;&#27169;&#21333;&#20010;&#26463;&#27969;&#23610;&#23544;&#27979;&#37327;&#12290;BAX&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#23398;&#20064;&#27169;&#22411;&#32780;&#19981;&#26159;&#30452;&#25509;&#20174;&#21152;&#36895;&#22120;&#20013;&#33719;&#21462;&#21457;&#23556;&#24230;&#25351;&#26631;&#26469;&#36991;&#20813;&#22312;&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#32531;&#24930;&#30340;&#22810;&#28857;&#26597;&#35810;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;BAX&#26469;&#26368;&#23567;&#21270;Linac&#30456;&#24178;&#20809;&#28304;(LCLS)&#21644;Facility for Adv&#30340;&#21457;&#23556;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although beam emittance is critical for the performance of high-brightness accelerators, optimization is often time limited as emittance calculations, commonly done via quadrupole scans, are typically slow. Such calculations are a type of $\textit{multi-point query}$, i.e. each query requires multiple secondary measurements. Traditional black-box optimizers such as Bayesian optimization are slow and inefficient when dealing with such objectives as they must acquire the full series of measurements, but return only the emittance, with each query. We propose applying Bayesian Algorithm Execution (BAX) to instead query and model individual beam-size measurements. BAX avoids the slow multi-point query on the accelerator by acquiring points through a $\textit{virtual objective}$, i.e. calculating the emittance objective from a fast learned model rather than directly from the accelerator. Here, we use BAX to minimize emittance at the Linac Coherent Light Source (LCLS) and the Facility for Adv
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#33829;&#38144;&#30740;&#31350;&#20013;&#30340;&#25968;&#25454;&#34701;&#21512;&#20026;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#24182;&#19988;&#38656;&#35201;&#20808;&#36827;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.00993</link><description>&lt;p&gt;
&#31070;&#32463;&#33829;&#38144;&#20013;&#30340;&#25968;&#25454;&#34701;&#21512;&#65306;&#29983;&#29289;&#20449;&#21495;&#12289;&#29983;&#21629;&#21608;&#26399;&#38454;&#27573;&#12289;&#24403;&#21069;&#36827;&#23637;&#12289;&#25968;&#25454;&#38598;&#12289;&#36235;&#21183;&#21644;&#25361;&#25112;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data Fusion in Neuromarketing: Multimodal Analysis of Biosignals, Lifecycle Stages, Current Advances, Datasets, Trends, and Challenges. (arXiv:2209.00993v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00993
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#33829;&#38144;&#30740;&#31350;&#20013;&#30340;&#25968;&#25454;&#34701;&#21512;&#20026;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#24182;&#19988;&#38656;&#35201;&#20808;&#36827;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#20844;&#21496;&#30340;&#39318;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#25552;&#39640;&#20135;&#21697;&#30340;&#36136;&#37327;&#21644;&#24191;&#21578;&#26041;&#24335;&#26469;&#22686;&#21152;&#21033;&#28070;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#31070;&#32463;&#33829;&#38144;&#26088;&#22312;&#25552;&#21319;&#20135;&#21697;&#30340;&#25512;&#24191;&#24182;&#22312;&#28508;&#22312;&#20080;&#23478;&#20013;&#20135;&#29983;&#26356;&#22823;&#30340;&#35748;&#21487;&#24230;&#12290;&#20256;&#32479;&#19978;&#65292;&#31070;&#32463;&#33829;&#38144;&#30740;&#31350;&#20381;&#36182;&#20110;&#21333;&#19968;&#29983;&#29289;&#20449;&#21495;&#26469;&#33719;&#24471;&#23545;&#21576;&#29616;&#30340;&#21050;&#28608;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26032;&#35774;&#22791;&#21644;&#25216;&#26415;&#36827;&#27493;&#65292;&#36817;&#26399;&#36235;&#21183;&#34920;&#26126;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#20986;&#29616;&#20102;&#22810;&#31181;&#29983;&#29289;&#20449;&#21495;&#30340;&#34701;&#21512;&#12290;&#19968;&#20010;&#20363;&#23376;&#26159;&#20351;&#29992;&#33041;&#30005;&#22270;&#26469;&#29702;&#35299;&#24191;&#21578;&#22312;&#31070;&#32463;&#27700;&#24179;&#19978;&#30340;&#24433;&#21709;&#65292;&#21033;&#29992;&#35270;&#35273;&#36319;&#36394;&#26469;&#35782;&#21035;&#23548;&#33268;&#36825;&#31181;&#24433;&#21709;&#30340;&#21050;&#28608;&#12290;&#36825;&#31181;&#26032;&#20852;&#27169;&#24335;&#20915;&#23450;&#20102;&#20026;&#23454;&#29616;&#29305;&#23450;&#31070;&#32463;&#33829;&#38144;&#30446;&#26631;&#32780;&#20351;&#29992;&#21738;&#20123;&#29983;&#29289;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#22810;&#28304;&#25968;&#25454;&#30340;&#34701;&#21512;&#38656;&#35201;&#20808;&#36827;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#20294;&#32570;&#20047;&#30456;&#20851;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary goal of any company is to increase its profits by improving both the quality of its products and how they are advertised. In this context, neuromarketing seeks to enhance the promotion of products and generate a greater acceptance on potential buyers. Traditionally, neuromarketing studies have relied on a single biosignal to obtain feedback from presented stimuli. However, thanks to new devices and technological advances studying this area of knowledge, recent trends indicate a shift towards the fusion of diverse biosignals. An example is the usage of electroencephalography for understanding the impact of an advertisement at the neural level and visual tracking to identify the stimuli that induce such impacts. This emerging pattern determines which biosignals to employ for achieving specific neuromarketing objectives. Furthermore, the fusion of data from multiple sources demands advanced processing methodologies. Despite these complexities, there is a lack of literature tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#29616;&#26377;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20197;&#25552;&#39640;&#25506;&#32034;&#25928;&#29575;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;&#35813;&#26041;&#27861;&#20250;&#23558;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20316;&#20026;&#24314;&#35758;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#22312;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;</title><link>http://arxiv.org/abs/2208.10533</link><description>&lt;p&gt;
&#19968;&#20123;&#30417;&#30563;&#26159;&#24517;&#39035;&#30340;&#65306;&#36890;&#36807;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#31070;&#35861;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics. (arXiv:2208.10533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#29616;&#26377;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20197;&#25552;&#39640;&#25506;&#32034;&#25928;&#29575;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;&#35813;&#26041;&#27861;&#20250;&#23558;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20316;&#20026;&#24314;&#35758;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#22312;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#22266;&#26377;&#38382;&#39064;&#26159;&#36890;&#36807;&#38543;&#26426;&#34892;&#21160;&#25506;&#32034;&#29615;&#22659;&#65292;&#20854;&#20013;&#24456;&#22823;&#19968;&#37096;&#20998;&#21487;&#33021;&#26159;&#26080;&#25928;&#30340;&#12290;&#30456;&#21453;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340;&#65288;&#20808;&#21069;&#23398;&#20064;&#30340;&#25110;&#30828;&#32534;&#30721;&#30340;&#65289;&#31070;&#35861;&#31574;&#30053;&#12289;&#31163;&#32447;&#25968;&#25454;&#25110;&#28436;&#31034;&#26469;&#25913;&#21892;&#25506;&#32034;&#12290;&#20294;&#22312;&#20351;&#29992;&#31070;&#35861;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#22823;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#22320;&#23558;&#31070;&#35861;&#32463;&#39564;&#34701;&#20837;&#21040;&#23398;&#20064;&#31574;&#30053;&#20013;&#21487;&#33021;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#65288;Critic Confidence Guided Exploration&#65292;CCGE&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#36825;&#26679;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;CCGE&#20197;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20026;&#24314;&#35758;&#65292;&#24182;&#23558;&#27492;&#20449;&#24687;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#24403;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;CCGE&#23545;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#19981;&#21152;&#21306;&#20998;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#23427;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
An inherent problem of reinforcement learning is performing exploration of an environment through random actions, of which a large portion can be unproductive. Instead, exploration can be improved by initializing the learning policy with an existing (previously learned or hard-coded) oracle policy, offline data, or demonstrations. In the case of using an oracle policy, it can be unclear how best to incorporate the oracle policy's experience into the learning policy in a way that maximizes learning sample efficiency. In this paper, we propose a method termed Critic Confidence Guided Exploration (CCGE) for incorporating such an oracle policy into standard actor-critic reinforcement learning algorithms. More specifically, CCGE takes in the oracle policy's actions as suggestions and incorporates this information into the learning scheme when uncertainty is high, while ignoring it when the uncertainty is low. CCGE is agnostic to methods of estimating uncertainty, and we show that it is equa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Topical&#65292;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#28304;&#20195;&#30721;&#20013;&#23398;&#20064;&#23384;&#20648;&#24211;&#32423;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12289;&#20195;&#30721;&#25512;&#33616;&#21644;&#20195;&#30721;&#33258;&#21160;&#26631;&#35760;&#31561;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09495</link><description>&lt;p&gt;
Topical: &#20351;&#29992;Attention&#20174;&#28304;&#20195;&#30721;&#20013;&#23398;&#20064;&#23384;&#20648;&#24211;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Topical: Learning Repository Embeddings from Source Code using Attention. (arXiv:2208.09495v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09495
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Topical&#65292;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#28304;&#20195;&#30721;&#20013;&#23398;&#20064;&#23384;&#20648;&#24211;&#32423;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12289;&#20195;&#30721;&#25512;&#33616;&#21644;&#20195;&#30721;&#33258;&#21160;&#26631;&#35760;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#30340;&#26426;&#22120;&#23398;&#20064;(MLOnCode)&#25215;&#35834;&#25913;&#21464;&#36719;&#20214;&#20132;&#20184;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#25366;&#25496;&#36719;&#20214;&#24037;&#20214;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#21644;&#20851;&#31995;&#65292;MLOnCode&#36890;&#36807;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12289;&#20195;&#30721;&#25512;&#33616;&#12289;&#20195;&#30721;&#33258;&#21160;&#26631;&#35760;&#21644;&#20854;&#20182;&#25968;&#25454;&#39537;&#21160;&#22686;&#24378;&#26469;&#22686;&#24378;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#26469;&#35828;&#65292;&#20195;&#30721;&#30340;&#33050;&#26412;&#32423;&#34920;&#31034;&#24050;&#32463;&#36275;&#22815;&#65292;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#21040;&#21508;&#31181;&#20381;&#36182;&#20851;&#31995;&#21644;&#23384;&#20648;&#24211;&#32467;&#26500;&#30340;&#23384;&#20648;&#24211;&#32423;&#34920;&#31034;&#26159;&#24517;&#35201;&#30340;&#65292;&#20363;&#22914;&#65292;&#20026;&#23384;&#20648;&#24211;&#33258;&#21160;&#26631;&#35760;&#20027;&#39064;&#25110;&#33258;&#21160;&#35760;&#24405;&#23384;&#20648;&#24211;&#20195;&#30721;&#31561;&#12290;&#29616;&#26377;&#30340;&#35745;&#31639;&#23384;&#20648;&#24211;&#32423;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(a) &#20381;&#36182;&#20110;&#20195;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26723;(&#20363;&#22914;README&#25991;&#20214;)&#65307;(b) &#36890;&#36807;&#20018;&#32852;&#25110;&#24179;&#22343;&#31561;&#26041;&#27861;&#23545;&#26041;&#27861;/&#33050;&#26412;&#32423;&#34920;&#31034;&#36827;&#34892;&#31616;&#21333;&#32858;&#21512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Topical&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20844;&#20849;&#23384;&#20648;&#24211;&#32423;&#23884;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on source code (MLOnCode) promises to transform how software is delivered. By mining the context and relationship between software artefacts, MLOnCode augments the software developers capabilities with code auto-generation, code recommendation, code auto-tagging and other data-driven enhancements. For many of these tasks a script level representation of code is sufficient, however, in many cases a repository level representation that takes into account various dependencies and repository structure is imperative, for example, auto-tagging repositories with topics or auto-documentation of repository code etc. Existing methods for computing repository level representations suffer from (a) reliance on natural language documentation of code (for example, README files) (b) naive aggregation of method/script-level representation, for example, by concatenation or averaging. This paper introduces Topical a deep neural network to generate repository level embeddings of publicly 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#21487;&#24494;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#26469;&#20805;&#20998;&#21033;&#29992;&#39640;&#32500;&#26799;&#24230;&#19979;&#38477;&#30340;&#25928;&#33021;&#65292;&#20197;&#23398;&#20064;&#36229;&#20986;&#29616;&#26377;&#31070;&#32463;&#31526;&#21495;ILP&#31995;&#32479;&#33021;&#21147;&#30340;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.06652</link><description>&lt;p&gt;
&#39640;&#32500;&#31354;&#38388;&#20013;&#21487;&#24494;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Differentiable Inductive Logic Programming in High-Dimensional Space. (arXiv:2208.06652v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#21487;&#24494;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#26469;&#20805;&#20998;&#21033;&#29992;&#39640;&#32500;&#26799;&#24230;&#19979;&#38477;&#30340;&#25928;&#33021;&#65292;&#20197;&#23398;&#20064;&#36229;&#20986;&#29616;&#26377;&#31070;&#32463;&#31526;&#21495;ILP&#31995;&#32479;&#33021;&#21147;&#30340;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31526;&#21495;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#21512;&#25104;&#22823;&#22411;&#36923;&#36753;&#31243;&#24207;&#36890;&#24120;&#38656;&#35201;&#20013;&#38388;&#23450;&#20041;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20869;&#28085;&#35859;&#35789;&#26434;&#20081;&#22320;&#21344;&#25454;&#20551;&#35774;&#31354;&#38388;&#36890;&#24120;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#26799;&#24230;&#19979;&#38477;&#25552;&#20379;&#20102;&#22312;&#36825;&#20123;&#39640;&#32500;&#31354;&#38388;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#31070;&#32463;&#31526;&#21495;ILP&#26041;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#25193;&#23637;{\delta}ILP&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#30340;&#24402;&#32435;&#21512;&#25104;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#39640;&#32500;&#26799;&#24230;&#19979;&#38477;&#30340;&#25928;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#21463;&#30410;&#20110;&#21487;&#24494;&#24402;&#32435;&#21512;&#25104;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#23398;&#20064;&#36229;&#20986;&#29616;&#26377;&#31070;&#32463;&#31526;&#21495;ILP&#31995;&#32479;&#33021;&#21147;&#30340;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#19981;&#25351;&#23450;&#35299;&#20915;&#26041;&#26696;&#30340;&#31934;&#30830;&#32467;&#26500;&#30340;&#35821;&#35328;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing large logic programs through symbolic Inductive Logic Programming (ILP) typically requires intermediate definitions. However, cluttering the hypothesis space with intensional predicates typically degrades performance. In contrast, gradient descent provides an efficient way to find solutions within such high- dimensional spaces. Neuro-symbolic ILP approaches have not fully exploited this so far. We propose extending the {\delta}ILP approach to inductive synthesis with large-scale predicate invention, thus allowing us to exploit the efficacy of high-dimensional gradient descent. We show that large-scale predicate invention benefits differentiable inductive synthesis through gradient descent and allows one to learn solutions for tasks beyond the capabilities of existing neuro-symbolic ILP systems. Furthermore, we achieve these results without specifying the precise structure of the solution within the language bias.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#20869;&#37096;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#24110;&#21161;&#26500;&#24314;&#26356;&#21487;&#20449;&#36182;&#30340;AI&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2207.13243</link><description>&lt;p&gt;
&#36208;&#21521;&#36879;&#26126;AI: &#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#30340;&#35299;&#37322;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#20869;&#37096;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#24110;&#21161;&#26500;&#24314;&#26356;&#21487;&#20449;&#36182;&#30340;AI&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#30340;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#35268;&#27169;&#21644;&#33021;&#21147;&#30340;&#22686;&#38271;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24456;&#38590;&#20998;&#26512;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#22312;&#19981;&#24443;&#24213;&#29702;&#35299;&#20854;&#24037;&#20316;&#21407;&#29702;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#20204;&#30340;&#25285;&#24551;&#12290;&#35299;&#37322;&#23427;&#20204;&#30340;&#26377;&#25928;&#24037;&#20855;&#23558;&#23545;&#26500;&#24314;&#26356;&#21487;&#20449;&#36182;&#30340;AI&#38750;&#24120;&#37325;&#35201;&#65292;&#36890;&#36807;&#24110;&#21161;&#35782;&#21035;&#38382;&#39064;&#12289;&#20462;&#22797;&#38169;&#35823;&#21644;&#22686;&#36827;&#22522;&#26412;&#29702;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;"&#20869;&#37096;"&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#23427;&#20204;&#19987;&#27880;&#20110;&#35299;&#37322;DNNs&#30340;&#20869;&#37096;&#32452;&#20214;&#65292;&#38750;&#24120;&#36866;&#21512;&#20110;&#24320;&#21457;&#26426;&#26800;&#29702;&#35299;&#12289;&#25351;&#23548;&#25163;&#21160;&#20462;&#25913;&#21644;&#36870;&#21521;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;DNN&#21487;&#35299;&#37322;&#24615;&#19978;&#65292;&#36805;&#36895;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#24471;&#23545;&#26041;&#27861;&#36827;&#34892;&#24443;&#24213;&#31995;&#32479;&#21270;&#30340;&#22256;&#38590;&#12290;&#22312;&#36825;&#31687;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;300&#22810;&#31687;&#20316;&#21697;&#65292;&#37325;&#28857;&#20851;&#27880;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#26041;&#27861;&#25353;&#32593;&#32476;&#30340;&#21738;&#20010;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions.  Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#37096;&#20998;&#38598;&#35206;&#30422;&#38382;&#39064;&#21644;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#22312;&#32473;&#23450;&#36755;&#20837;&#38598;&#21512;&#31995;&#32479;&#30340;&#26494;&#24347;&#26465;&#20214;&#19979;&#33021;&#22815;&#36755;&#20986;&#20855;&#26377;&#38750;&#24179;&#20961;&#36817;&#20284;&#20445;&#35777;&#30340;&#26174;&#24335;&#38598;&#35206;&#30422;&#12290;</title><link>http://arxiv.org/abs/2207.10240</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#37096;&#20998;&#38598;&#35206;&#30422;&#21450;&#20854;&#22312;&#35774;&#26045;&#36873;&#22336;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Partial Set Cover with Applications to Facility Location. (arXiv:2207.10240v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#37096;&#20998;&#38598;&#35206;&#30422;&#38382;&#39064;&#21644;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#22312;&#32473;&#23450;&#36755;&#20837;&#38598;&#21512;&#31995;&#32479;&#30340;&#26494;&#24347;&#26465;&#20214;&#19979;&#33021;&#22815;&#36755;&#20986;&#20855;&#26377;&#38750;&#24179;&#20961;&#36817;&#20284;&#20445;&#35777;&#30340;&#26174;&#24335;&#38598;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#65292;&#35266;&#23519;&#21040;&#38598;&#35206;&#30422;&#38382;&#39064;&#23384;&#22312;&#24378;&#22256;&#38590;&#32467;&#26524;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#25105;&#20204;&#36716;&#21521;&#37096;&#20998;&#38598;&#35206;&#30422;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#22256;&#38590;&#32467;&#26524;&#28040;&#22833;&#65292;&#20854;&#20013;&#25105;&#20204;&#21482;&#38656;&#35201;&#35206;&#30422;&#23431;&#23449;&#20013;&#30340;&#19968;&#37096;&#20998;&#20803;&#32032;$\rho$&#65288;$\rho \in (0,1)$&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36755;&#20837;&#38598;&#21512;&#31995;&#32479;&#30340;&#26494;&#24347;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20854;&#36755;&#20986;&#20855;&#26377;&#38750;&#24179;&#20961;&#36817;&#20284;&#20445;&#35777;&#30340;&#26174;&#24335;&#38598;&#35206;&#30422;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#26159;&#39318;&#20010;&#36755;&#20986;&#26174;&#24335;&#38598;&#35206;&#30422;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#37096;&#20998;&#38598;&#35206;&#30422;&#31639;&#27861;&#20316;&#20026;&#23376;&#31243;&#24335;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#65288;&#21452;&#30446;&#26631;&#65289;&#36817;&#20284;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#24191;&#20041;&#30340;$k$-&#20013;&#24515;/$k$-&#20379;&#24212;&#38382;&#39064;&#65292;&#21253;&#25324;&#31163;&#32676;&#28857;&#12290;&#19982;&#38598;&#35206;&#30422;&#38382;&#39064;&#19968;&#26679;&#65292;&#30446;&#21069;&#26080;&#27861;&#32473;&#20986;&#38750;&#24179;&#20961;&#30340;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It was observed in \citet{gupta2009differentially} that the Set Cover problem has strong impossibility results under differential privacy. In our work, we observe that these hardness results dissolve when we turn to the Partial Set Cover problem, where we only need to cover a $\rho$-fraction of the elements in the universe, for some $\rho\in(0,1)$. We show that this relaxation enables us to avoid the impossibility results: under loose conditions on the input set system, we give differentially private algorithms which output an explicit set cover with non-trivial approximation guarantees. In particular, this is the first differentially private algorithm which outputs an explicit set cover.  Using our algorithm for Partial Set Cover as a subroutine, we give a differentially private (bicriteria) approximation algorithm for a facility location problem which generalizes $k$-center/$k$-supplier with outliers. Like with the Set Cover problem, no algorithm has been able to give non-trivial gua
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35782;&#21035;&#20102;&#33258;&#20027;&#31995;&#32479;&#20449;&#20219;&#24230;&#35268;&#33539;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36328;&#36234;&#20102;&#19981;&#21516;&#39046;&#22495;&#30340;AS&#30340;&#24377;&#24615;&#12289;&#20449;&#20219;&#24230;&#12289;&#21151;&#33021;&#12289;&#21487;&#39564;&#35777;&#24615;&#12289;&#23433;&#20840;&#24615;&#20197;&#21450;&#27835;&#29702;&#21644;&#30417;&#31649;&#65292;&#24182;&#24378;&#35843;&#20102;&#19982;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#24605;&#32500;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2206.11421</link><description>&lt;p&gt;
&#20851;&#20110;&#20449;&#20219;&#24230;&#35268;&#33539;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Specifying for Trustworthiness. (arXiv:2206.11421v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35782;&#21035;&#20102;&#33258;&#20027;&#31995;&#32479;&#20449;&#20219;&#24230;&#35268;&#33539;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36328;&#36234;&#20102;&#19981;&#21516;&#39046;&#22495;&#30340;AS&#30340;&#24377;&#24615;&#12289;&#20449;&#20219;&#24230;&#12289;&#21151;&#33021;&#12289;&#21487;&#39564;&#35777;&#24615;&#12289;&#23433;&#20840;&#24615;&#20197;&#21450;&#27835;&#29702;&#21644;&#30417;&#31649;&#65292;&#24182;&#24378;&#35843;&#20102;&#19982;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#24605;&#32500;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#20027;&#31995;&#32479;&#65288;AS&#65289;&#36234;&#26469;&#36234;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#30830;&#20445;&#20854;&#20449;&#20219;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35777;&#26126;AS&#30340;&#20449;&#20219;&#24230;&#65292;&#25105;&#20204;&#39318;&#20808;&#38656;&#35201;&#25351;&#23450;AS&#34987;&#35748;&#20026;&#26159;&#21487;&#20449;&#30340;&#25152;&#38656;&#30340;&#26465;&#20214;&#12290;&#36825;&#31687;&#36335;&#32447;&#22270;&#35770;&#25991;&#22312;&#33521;&#22269;&#30740;&#31350;&#21644;&#21019;&#26032;&#65288;UKRI&#65289;&#21487;&#20449;&#20219;&#33258;&#20027;&#31995;&#32479;&#65288;TAS&#65289;&#35745;&#21010;&#30340;&#19968;&#37096;&#20998;&#20030;&#21150;&#30340;&#8220;&#25351;&#23450;&#20449;&#20219;&#24230;&#8221;&#30740;&#35752;&#20250;&#20013;&#65292;&#30830;&#23450;&#20102;&#22312;AS&#20013;&#20026;&#20449;&#20219;&#24230;&#25351;&#23450;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#36328;AS&#39046;&#22495;&#23637;&#26395;&#65292;&#32771;&#34385;AS&#30340;&#24377;&#24615;&#12289;&#20449;&#20219;&#24230;&#12289;&#21151;&#33021;&#12289;&#21487;&#39564;&#35777;&#24615;&#12289;&#23433;&#20840;&#24615;&#20197;&#21450;&#27835;&#29702;&#21644;&#30417;&#31649;&#65292;&#24182;&#30830;&#23450;&#20102;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#20851;&#38190;&#35268;&#33539;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#22312;AS&#20013;&#20026;&#20449;&#20219;&#24230;&#25351;&#23450;&#25152;&#28041;&#21450;&#30340;&#36328;&#39046;&#22495;&#30340;&#24605;&#32500;&#25361;&#25112;&#65292;&#24182;&#21078;&#26512;&#20102;AS&#38656;&#35201;&#36816;&#34892;&#30340;&#29615;&#22659;&#25152;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As autonomous systems (AS) increasingly become part of our daily lives, ensuring their trustworthiness is crucial. In order to demonstrate the trustworthiness of an AS, we first need to specify what is required for an AS to be considered trustworthy. This roadmap paper identifies key challenges for specifying for trustworthiness in AS, as identified during the "Specifying for Trustworthiness" workshop held as part of the UK Research and Innovation (UKRI) Trustworthy Autonomous Systems (TAS) programme. We look across a range of AS domains with consideration of the resilience, trust, functionality, verifiability, security, and governance and regulation of AS and identify some of the key specification challenges in these domains. We then highlight the intellectual challenges that are involved with specifying for trustworthiness in AS that cut across domains and are exacerbated by the inherent uncertainty involved with the environments in which AS need to operate.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;BONET&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#26679;&#26412;&#31574;&#30053;&#21512;&#25104;&#36712;&#36857;&#20197;&#24110;&#21161;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2206.10786</link><description>&lt;p&gt;
&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pretraining for Black-Box Optimization. (arXiv:2206.10786v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;BONET&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#26679;&#26412;&#31574;&#30053;&#21512;&#25104;&#36712;&#36857;&#20197;&#24110;&#21161;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#28041;&#21450;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#40657;&#30418;&#20248;&#21270; (BBO) &#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#24120;&#20551;&#35774;&#22312;&#32447;&#20989;&#25968;&#35780;&#20272;&#30340;&#39044;&#31639;&#24456;&#23567;&#65292;&#20294;&#24448;&#24448;&#21487;&#20197;&#35775;&#38382;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22266;&#23450;&#31163;&#32447;&#25968;&#25454;&#38598;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#36924;&#36817;&#20989;&#25968;&#25110;&#20854;&#21453;&#20989;&#25968;&#65292;&#20294;&#22312;&#31163;&#25968;&#25454;&#20998;&#24067;&#36739;&#36828;&#26102;&#19981;&#22815;&#31934;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BONET&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;&#12290;&#22312;BONET&#20013;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#23450;&#38271;&#36712;&#36857;&#35757;&#32451;&#19968;&#20010;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#65292;&#20351;&#29992;&#20174;&#20302;&#20445;&#30495;&#24230;&#26679;&#26412;&#21040;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#30340;&#21333;&#35843;&#36716;&#25442;&#30340;&#31616;&#21333;&#21551;&#21457;&#24335;&#26469;&#21512;&#25104;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#30340;&#36712;&#36857;&#12290;&#22312;Design-Bench&#19978;&#20351;&#29992;&#34987;&#22240;&#26524;&#25513;&#34109;&#30340;Transformer&#23454;&#20363;&#21270;BONET&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#24179;&#22343;&#25490;&#21517;&#19978;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. For such black-box optimization (BBO) problems, we typically assume a small budget for online function evaluations, but also often have access to a fixed, offline dataset for pretraining. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose BONET, a generative framework for pretraining a novel black-box optimizer using offline datasets. In BONET, we train an autoregressive model on fixed-length trajectories derived from an offline dataset. We design a sampling strategy to synthesize trajectories from offline data using a simple heuristic of rolling out monotonic transitions from low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using a causally masked Transformer and evaluate it on Design-Bench, where we rank the best on averag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#35745;&#25968;&#21644;&#37319;&#26679;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#38271;&#26399;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#27963;&#36291;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#21644;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#26041;&#38754;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.02654</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#22312;&#35745;&#25968;&#21644;&#37319;&#26679;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs with Applications. (arXiv:2205.02654v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#35745;&#25968;&#21644;&#37319;&#26679;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#38271;&#26399;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#27963;&#36291;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#21644;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#26041;&#38754;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#24418;&#22240;&#26524;&#20998;&#26512;&#20013;&#65292;&#20174;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#35745;&#25968;&#21644;&#37319;&#26679;&#26377;&#21521;&#26080;&#29615;&#22270;&#26159;&#22522;&#26412;&#20219;&#21153;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23436;&#25104;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#38271;&#26399;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26377;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#36825;&#20123;&#31361;&#30772;&#20351;&#24471;&#22312;&#27963;&#36291;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#21644;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#26041;&#38754;&#65292;&#23545;&#20110;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65292;&#21407;&#26412;&#35748;&#20026;&#19981;&#21487;&#34892;&#30340;&#31574;&#30053;&#23454;&#38469;&#21487;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counting and sampling directed acyclic graphs from a Markov equivalence class are fundamental tasks in graphical causal analysis. In this paper we show that these tasks can be performed in polynomial time, solving a long-standing open problem in this area. Our algorithms are effective and easily implementable. As we show in experiments, these breakthroughs make thought-to-be-infeasible strategies in active learning of causal structures and causal effect identification with regard to a Markov equivalence class practically applicable.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#38454;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#20013;&#29289;&#20307;&#20043;&#38388;&#20851;&#31995;&#24314;&#27169;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#30097;&#29289;&#20307;&#36716;&#25442;&#26426;&#21046;&#65288;SOT&#65289;&#65292;&#36890;&#36807;&#23545;&#21487;&#30097;&#29289;&#20307;&#36827;&#34892;&#36716;&#25442;&#21644;&#20851;&#31995;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.05186</link><description>&lt;p&gt;
&#21487;&#30097;&#29289;&#20307;&#30340;&#37325;&#35201;&#24615;&#65306;&#37325;&#26032;&#24605;&#32771;&#19968;&#38454;&#35270;&#35273;&#23450;&#20301;&#27169;&#22411;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Suspected Object Matters: Rethinking Model's Prediction for One-stage Visual Grounding. (arXiv:2203.05186v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#38454;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#20013;&#29289;&#20307;&#20043;&#38388;&#20851;&#31995;&#24314;&#27169;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#30097;&#29289;&#20307;&#36716;&#25442;&#26426;&#21046;&#65288;SOT&#65289;&#65292;&#36890;&#36807;&#23545;&#21487;&#30097;&#29289;&#20307;&#36827;&#34892;&#36716;&#25442;&#21644;&#20851;&#31995;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#19982;&#20004;&#38454;&#27573;&#23450;&#20301;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#20294;&#26174;&#33879;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#19968;&#38454;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#21463;&#21040;&#20102;&#39640;&#24230;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#38454;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#26469;&#35828;&#65292;&#29289;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#24314;&#27169;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#34429;&#28982;&#29289;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#24314;&#27169;&#24456;&#37325;&#35201;&#65292;&#20294;&#19981;&#19968;&#23450;&#38656;&#35201;&#22312;&#25152;&#26377;&#29289;&#20307;&#20043;&#38388;&#25191;&#34892;&#65292;&#22240;&#20026;&#21482;&#26377;&#20854;&#20013;&#19968;&#37096;&#20998;&#19982;&#25991;&#26412;&#26597;&#35810;&#30456;&#20851;&#65292;&#21487;&#33021;&#20250;&#22256;&#24785;&#27169;&#22411;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#29289;&#20307;&#20026;&#21487;&#30097;&#29289;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#38454;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#20013;&#25506;&#32034;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#65306;&#39318;&#20808;&#65292;&#27809;&#26377;&#29289;&#20307;&#25552;&#35758;&#20316;&#20026;&#36873;&#25321;&#21487;&#30097;&#29289;&#20307;&#21644;&#36827;&#34892;&#20851;&#31995;&#24314;&#27169;&#30340;&#22522;&#30784;&#12290;&#31532;&#20108;&#65292;&#21487;&#30097;&#29289;&#20307;&#27604;&#20854;&#20182;&#29289;&#20307;&#26356;&#23481;&#26131;&#22256;&#24785;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#20855;&#26377;&#31867;&#20284;&#30340;&#35821;&#20041;&#65292;&#19982;&#29305;&#23450;&#20851;&#31995;&#32416;&#32544;&#22312;&#19968;&#36215;&#31561;&#65292;&#20174;&#32780;&#26356;&#23481;&#26131;&#35823;&#23548;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#30097;&#29289;&#20307;&#36716;&#25442;&#26426;&#21046;&#65288;SOT&#65289;&#65292;&#23427;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, one-stage visual grounders attract high attention due to their comparable accuracy but significantly higher efficiency than two-stage grounders. However, inter-object relation modeling has not been well studied for one-stage grounders. Inter-object relationship modeling, though important, is not necessarily performed among all objects, as only part of them are related to the text query and may confuse the model. We call these objects suspected objects. However, exploring their relationships in the one-stage paradigm is non-trivial because: First, no object proposals are available as the basis on which to select suspected objects and perform relationship modeling. Second, suspected objects are more confusing than others, as they may share similar semantics, be entangled with certain relationships, etc, and thereby more easily mislead the model prediction. Toward this end, we propose a Suspected Object Transformation mechanism (SOT), which can be seamlessly integrated into exis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#35821;&#38899;&#23398;&#20449;&#24687;&#21644;&#23545;&#35937;&#20449;&#24687;&#65292;&#29992;&#20110;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#21457;&#29616;&#21333;&#35789;&#21644;&#38899;&#32032;&#65292;&#24182;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#27169;&#24577;&#30340;&#23545;&#35937;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#35789;&#21457;&#29616;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.06786</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21452;&#20851;&#33410;&#20998;&#26512;&#19982;&#20849;&#29616;&#32447;&#32034;&#30340;&#22810;&#27169;&#24577;&#21333;&#35789;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Multimodal Word Discovery based on Double Articulation Analysis with Co-occurrence cues. (arXiv:2201.06786v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#35821;&#38899;&#23398;&#20449;&#24687;&#21644;&#23545;&#35937;&#20449;&#24687;&#65292;&#29992;&#20110;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#21457;&#29616;&#21333;&#35789;&#21644;&#38899;&#32032;&#65292;&#24182;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#27169;&#24577;&#30340;&#23545;&#35937;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#35789;&#21457;&#29616;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23156;&#20799;&#22312;&#27809;&#26377;&#22826;&#22810;&#35821;&#35328;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22768;&#38899;&#20998;&#24067;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#20854;&#20182;&#24863;&#23448;&#21050;&#28608;&#30340;&#20849;&#29616;&#26469;&#33719;&#21462;&#20854;&#35821;&#35328;&#35789;&#27719;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#23398;&#20449;&#24687;&#21644;&#23545;&#35937;&#20449;&#24687;&#30340;&#20840;&#26032;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#35821;&#38899;&#21333;&#20803;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#33719;&#24471;&#21333;&#35789;&#21644;&#38899;&#32032;&#65292;&#24182;&#21516;&#26102;&#21033;&#29992;&#22522;&#20110;&#22810;&#31181;&#27169;&#24577;&#30340;&#23545;&#35937;&#20449;&#24687;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35302;&#35273;&#21644;&#21548;&#35273;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#21452;&#20851;&#33410;&#20998;&#26512;&#22120;&#65288;NPB-DAA&#65289;&#20174;&#35821;&#38899;&#23398;&#29305;&#24449;&#20013;&#21457;&#29616;&#38899;&#32032;&#21644;&#21333;&#35789;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#27169;&#24577;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;MLDA&#65289;&#23545;&#20174;&#23545;&#35937;&#20013;&#33719;&#21462;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#21333;&#35789;&#21457;&#29616;&#24615;&#33021;&#12290;&#34920;&#36798;&#20102;&#29305;&#24615;&#30340;&#21333;&#35789;&#21253;&#21547;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Human infants acquire their verbal lexicon with minimal prior knowledge of language based on the statistical properties of phonological distributions and the co-occurrence of other sensory stimuli. This study proposes a novel fully unsupervised learning method for discovering speech units using phonological information as a distributional cue and object information as a co-occurrence cue. The proposed method can acquire words and phonemes from speech signals using unsupervised learning and utilize object information based on multiple modalities-vision, tactile, and auditory-simultaneously. The proposed method is based on the nonparametric Bayesian double articulation analyzer (NPB-DAA) discovering phonemes and words from phonological features, and multimodal latent Dirichlet allocation (MLDA) categorizing multimodal information obtained from objects. In an experiment, the proposed method showed higher word discovery performance than baseline methods. Words that expressed the characteri
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;&#23433;&#20840;&#22343;&#34913;&#65292;&#35813;&#27010;&#24565;&#27169;&#25311;&#20102;&#23545;&#25163;&#26377;&#19968;&#23450;&#27010;&#29575;&#36827;&#34892;&#29702;&#24615;&#34892;&#20026;&#65292;&#26377;&#21097;&#20313;&#27010;&#29575;&#36827;&#34892;&#20219;&#24847;&#34892;&#20026;&#12290;&#30740;&#31350;&#35777;&#26126;&#22312;&#25152;&#26377;&#25112;&#30053;&#21338;&#24328;&#20013;&#37117;&#23384;&#22312;&#23433;&#20840;&#22343;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#23433;&#20840;&#22343;&#34913;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.04266</link><description>&lt;p&gt;
&#23433;&#20840;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Safe Equilibrium. (arXiv:2201.04266v10 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04266
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;&#23433;&#20840;&#22343;&#34913;&#65292;&#35813;&#27010;&#24565;&#27169;&#25311;&#20102;&#23545;&#25163;&#26377;&#19968;&#23450;&#27010;&#29575;&#36827;&#34892;&#29702;&#24615;&#34892;&#20026;&#65292;&#26377;&#21097;&#20313;&#27010;&#29575;&#36827;&#34892;&#20219;&#24847;&#34892;&#20026;&#12290;&#30740;&#31350;&#35777;&#26126;&#22312;&#25152;&#26377;&#25112;&#30053;&#21338;&#24328;&#20013;&#37117;&#23384;&#22312;&#23433;&#20840;&#22343;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#23433;&#20840;&#22343;&#34913;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#21338;&#24328;&#35770;&#35299;&#20915;&#27010;&#24565;&#32435;&#20160;&#22343;&#34913;&#20551;&#35774;&#25152;&#26377;&#29609;&#23478;&#37117;&#20250;&#29702;&#24615;&#34892;&#20026;&#12290;&#22914;&#26524;&#25105;&#20204;&#25353;&#29031;&#32435;&#20160;&#22343;&#34913;&#36827;&#34892;&#20915;&#31574;&#32780;&#23545;&#25163;&#21364;&#26159;&#38750;&#29702;&#24615;&#30340;&#65288;&#25110;&#32773;&#25353;&#29031;&#21478;&#22806;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;&#31574;&#30053;&#34892;&#21160;&#65289;&#65292;&#37027;&#20040;&#25105;&#20204;&#21487;&#33021;&#20250;&#24471;&#21040;&#38750;&#24120;&#20302;&#30340;&#22238;&#25253;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#22823;&#26368;&#23567;&#31574;&#30053;&#20551;&#35774;&#25152;&#26377;&#23545;&#25163;&#37117;&#22312;&#35797;&#22270;&#26368;&#23567;&#21270;&#25105;&#20204;&#30340;&#22238;&#25253;&#65288;&#21363;&#20351;&#36825;&#24182;&#19981;&#31526;&#21512;&#20182;&#20204;&#26368;&#20339;&#21033;&#30410;&#65289;&#65292;&#24182;&#30830;&#20445;&#20102;&#26368;&#24046;&#24773;&#20917;&#19979;&#30340;&#26368;&#22823;&#21487;&#33021;&#22238;&#25253;&#65292;&#20294;&#32467;&#26524;&#26159;&#36807;&#20110;&#20445;&#23432;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;&#31216;&#20026;&#23433;&#20840;&#22343;&#34913;&#65292;&#23427;&#23558;&#23545;&#25163;&#24314;&#27169;&#20026;&#20197;&#25351;&#23450;&#30340;&#27010;&#29575;&#36827;&#34892;&#29702;&#24615;&#34892;&#20026;&#65292;&#32780;&#29992;&#21097;&#20313;&#30340;&#27010;&#29575;&#36827;&#34892;&#20219;&#24847;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#24418;&#24335;&#30340;&#25112;&#30053;&#21338;&#24328;&#20013;&#37117;&#23384;&#22312;&#23433;&#20840;&#22343;&#34913;&#65288;&#23545;&#20110;&#25152;&#26377;&#21487;&#33021;&#30340;&#29702;&#24615;&#21442;&#25968;&#20540;&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#35745;&#31639;&#26159;PPAD-hard&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;2&#20154;&#21644;n&#20154;&#21338;&#24328;&#20013;&#35745;&#31639;&#23433;&#20840;&#22343;&#34913;&#30340;&#31934;&#30830;&#31639;&#27861;&#65292;&#20197;&#21450;&#21487;&#25193;&#23637;&#24615;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard game-theoretic solution concept, Nash equilibrium, assumes that all players behave rationally. If we follow a Nash equilibrium and opponents are irrational (or follow strategies from a different Nash equilibrium), then we may obtain an extremely low payoff. On the other hand, a maximin strategy assumes that all opposing agents are playing to minimize our payoff (even if it is not in their best interest), and ensures the maximal possible worst-case payoff, but results in exceedingly conservative play. We propose a new solution concept called safe equilibrium that models opponents as behaving rationally with a specified probability and behaving potentially arbitrarily with the remaining probability. We prove that a safe equilibrium exists in all strategic-form games (for all possible values of the rationality parameters), and prove that its computation is PPAD-hard. We present exact algorithms for computing a safe equilibrium in both 2 and $n$-player games, as well as scalab
&lt;/p&gt;</description></item><item><title>CausalAF&#26159;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#30340;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#20851;&#31995;&#20808;&#39564;&#21644;&#26032;&#39062;&#30340;&#22240;&#26524;&#25513;&#34109;&#25805;&#20316;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#20174;&#20165;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#25193;&#23637;&#21040;&#23398;&#20064;&#29983;&#25104;&#22330;&#26223;&#22914;&#20309;&#24341;&#36215;&#39118;&#38505;&#24773;&#20917;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2110.13939</link><description>&lt;p&gt;
CausalAF: &#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#30340;&#22240;&#26524;&#33258;&#22238;&#24402;&#27969;
&lt;/p&gt;
&lt;p&gt;
CausalAF: Causal Autoregressive Flow for Safety-Critical Driving Scenario Generation. (arXiv:2110.13939v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13939
&lt;/p&gt;
&lt;p&gt;
CausalAF&#26159;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#30340;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#20851;&#31995;&#20808;&#39564;&#21644;&#26032;&#39062;&#30340;&#22240;&#26524;&#25513;&#34109;&#25805;&#20316;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#20174;&#20165;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#25193;&#23637;&#21040;&#23398;&#20064;&#29983;&#25104;&#22330;&#26223;&#22914;&#20309;&#24341;&#36215;&#39118;&#38505;&#24773;&#20917;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20851;&#38190;&#23433;&#20840;&#22330;&#26223;&#26159;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#20294;&#26159;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#21644;&#29983;&#25104;&#26041;&#27861;&#30340;&#25928;&#29575;&#21463;&#38480;&#20110;&#20851;&#38190;&#23433;&#20840;&#22330;&#26223;&#30340;&#31232;&#32570;&#24615;&#21644;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#20165;&#20174;&#35266;&#27979;&#25968;&#25454;&#20272;&#35745;&#20998;&#24067;&#30340;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#20851;&#31995;&#20316;&#20026;&#20808;&#39564;&#34701;&#20837;&#21040;&#22330;&#26223;&#29983;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;&#33258;&#22238;&#24402;&#27969;&#65288;CausalAF&#65289;&#12290;CausalAF&#36890;&#36807;&#26032;&#39062;&#30340;&#22240;&#26524;&#25513;&#34109;&#25805;&#20316;&#65292;&#40723;&#21169;&#29983;&#25104;&#27169;&#22411;&#25581;&#31034;&#21644;&#36981;&#24490;&#29983;&#25104;&#23545;&#35937;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#38543;&#26426;&#37319;&#26679;&#12290;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#22330;&#26223;&#22914;&#20309;&#24341;&#36215;&#39118;&#38505;&#24773;&#20917;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#24615;&#65292;CausalAF&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating safety-critical scenarios, which are crucial yet difficult to collect, provides an effective way to evaluate the robustness of autonomous driving systems. However, the diversity of scenarios and efficiency of generation methods are heavily restricted by the rareness and structure of safety-critical scenarios. Therefore, existing generative models that only estimate distributions from observational data are not satisfying to solve this problem. In this paper, we integrate causality as a prior into the scenario generation and propose a flow-based generative framework, Causal Autoregressive Flow (CausalAF). CausalAF encourages the generative model to uncover and follow the causal relationship among generated objects via novel causal masking operations instead of searching the sample only from observational data. By learning the cause-and-effect mechanism of how the generated scenario causes risk situations rather than just learning correlations from data, CausalAF significantly
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#36817;&#24180;&#26469;&#24212;&#29992;&#20110;UAV&#36965;&#24863;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20998;&#31867;&#21644;&#22238;&#24402;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2101.10861</link><description>&lt;p&gt;
&#12298;UAV&#36965;&#24863;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Review on Deep Learning in UAV Remote Sensing. (arXiv:2101.10861v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.10861
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#36817;&#24180;&#26469;&#24212;&#29992;&#20110;UAV&#36965;&#24863;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20998;&#31867;&#21644;&#22238;&#24402;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20855;&#26377;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#24449;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#24182;&#22312;&#22270;&#20687;&#12289;&#26102;&#38388;&#24207;&#21015;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#31561;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#22312;&#36965;&#24863;&#39046;&#22495;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#26377;&#20851;DNN&#31639;&#27861;&#24212;&#29992;&#30340;&#35843;&#26597;&#21644;&#25991;&#29486;&#32508;&#36848;&#65292;&#35797;&#22270;&#24635;&#32467;&#20854;&#23376;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#22823;&#37327;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#24212;&#29992;&#22312;&#33322;&#31354;&#24863;&#30693;&#30740;&#31350;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#36827;&#34892;&#23558;&#8220;&#28145;&#24230;&#23398;&#20064;&#8221;&#21644;&#8220;UAV&#36965;&#24863;&#8221;&#20004;&#20010;&#20027;&#39064;&#32467;&#21512;&#36215;&#26469;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21160;&#26426;&#26159;&#23545;&#24212;&#29992;&#20110;&#22522;&#20110;UAV&#25104;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#22522;&#26412;&#21407;&#29702;&#36827;&#34892;&#32508;&#21512;&#35780;&#36848;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#25551;&#36848;&#29992;&#20110;&#26368;&#36817;UAV&#33719;&#21462;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#20849;&#31579;&#36873;&#20102;232&#31687;&#21457;&#34920;&#22312;&#22269;&#38469;&#31185;&#23398;&#26399;&#21002;&#36164;&#26009;&#20013;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) learn representation from data with an impressive capability, and brought important breakthroughs for processing images, time-series, natural language, audio, video, and many others. In the remote sensing field, surveys and literature revisions specifically involving DNNs algorithms' applications have been conducted in an attempt to summarize the amount of information produced in its subfields. Recently, Unmanned Aerial Vehicles (UAV) based applications have dominated aerial sensing research. However, a literature revision that combines both "deep learning" and "UAV remote sensing" thematics has not yet been conducted. The motivation for our work was to present a comprehensive review of the fundamentals of Deep Learning (DL) applied in UAV-based imagery. We focused mainly on describing classification and regression techniques used in recent applications with UAV-acquired data. For that, a total of 232 papers published in international scientific journal data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32929;&#31080;&#24066;&#22330;&#20013;&#36827;&#34892;&#27963;&#36291;&#30340;&#39640;&#39057;&#20132;&#26131;&#12290;&#36890;&#36807;&#35757;&#32451;DRL&#20195;&#29702;&#26469;&#20132;&#26131;&#32929;&#31080;&#65292;&#24182;&#20351;&#29992;Proximal Policy Optimization&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20165;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20215;&#26684;&#21464;&#21160;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20195;&#29702;&#33021;&#22815;&#21019;&#24314;&#23545;&#24213;&#23618;&#29615;&#22659;&#30340;&#21160;&#24577;&#34920;&#31034;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20598;&#23572;&#20986;&#29616;&#30340;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2101.07107</link><description>&lt;p&gt;
&#39640;&#39057;&#20132;&#26131;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Active High Frequency Trading. (arXiv:2101.07107v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.07107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32929;&#31080;&#24066;&#22330;&#20013;&#36827;&#34892;&#27963;&#36291;&#30340;&#39640;&#39057;&#20132;&#26131;&#12290;&#36890;&#36807;&#35757;&#32451;DRL&#20195;&#29702;&#26469;&#20132;&#26131;&#32929;&#31080;&#65292;&#24182;&#20351;&#29992;Proximal Policy Optimization&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20165;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20215;&#26684;&#21464;&#21160;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20195;&#29702;&#33021;&#22815;&#21019;&#24314;&#23545;&#24213;&#23618;&#29615;&#22659;&#30340;&#21160;&#24577;&#34920;&#31034;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20598;&#23572;&#20986;&#29616;&#30340;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32929;&#31080;&#24066;&#22330;&#20013;&#36827;&#34892;&#27963;&#36291;&#30340;&#39640;&#39057;&#20132;&#26131;&#12290;&#25105;&#20204;&#35757;&#32451;DRL&#20195;&#29702;&#20351;&#29992;Proximal Policy Optimization&#31639;&#27861;&#26469;&#20132;&#26131;&#19968;&#21333;&#20301;&#30340;&#33521;&#29305;&#23572;&#20844;&#21496;&#32929;&#31080;&#12290;&#35757;&#32451;&#26159;&#22312;&#36830;&#32493;&#19977;&#20010;&#26376;&#30340;&#39640;&#39057;&#38480;&#20215;&#22996;&#25176;&#31807;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#26368;&#21518;&#19968;&#20010;&#26376;&#26159;&#39564;&#35777;&#25968;&#25454;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#22122;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#20165;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20215;&#26684;&#21464;&#21160;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#32452;&#25104;&#21518;&#32773;&#12290;&#28982;&#21518;&#22312;&#25509;&#19979;&#26469;&#30340;&#19968;&#20010;&#26376;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#20351;&#29992;&#39034;&#24207;&#27169;&#22411;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#29366;&#24577;&#29305;&#24449;&#21270;&#26041;&#24335;&#65292;&#23427;&#20204;&#22312;&#22522;&#20110;LOB&#30340;&#20803;&#29305;&#24449;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#20998;&#26512;&#20195;&#29702;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#35748;&#20026;&#20195;&#29702;&#33021;&#22815;&#21019;&#24314;&#23545;&#24213;&#23618;&#29615;&#22659;&#30340;&#21160;&#24577;&#34920;&#31034;&#12290;&#23427;&#20204;&#33021;&#22815;&#35782;&#21035;&#20598;&#23572;&#20986;&#29616;&#30340;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the first end-to-end Deep Reinforcement Learning (DRL) based framework for active high frequency trading in the stock market. We train DRL agents to trade one unit of Intel Corporation stock by employing the Proximal Policy Optimization algorithm. The training is performed on three contiguous months of high frequency Limit Order Book data, of which the last month constitutes the validation data. In order to maximise the signal to noise ratio in the training data, we compose the latter by only selecting training samples with largest price changes. The test is then carried out on the following month of data. Hyperparameters are tuned using the Sequential Model Based Optimization technique. We consider three different state characterizations, which differ in their LOB-based meta-features. Analysing the agents' performances on test data, we argue that the agents are able to create a dynamic representation of the underlying environment. They identify occasional regularities pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#23402;&#29983;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#36827;&#34892;&#20302;&#26679;&#26412;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#21033;&#29992;&#39069;&#22806;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2012.04841</link><description>&lt;p&gt;
&#19968;&#31080;&#21542;&#20915;&#26435;&#65306;&#29992;&#20110;&#20302;&#26679;&#26412;&#38738;&#20809;&#30524;&#35786;&#26029;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Vote Veto: Semi-Supervised Learning for Low-Shot Glaucoma Diagnosis. (arXiv:2012.04841v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.04841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#23402;&#29983;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#36827;&#34892;&#20302;&#26679;&#26412;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#21033;&#29992;&#39069;&#22806;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#33258;&#21160;&#35786;&#26029;&#30524;&#24213;&#22270;&#20687;&#20013;&#30340;&#38738;&#20809;&#30524;&#65292;&#36825;&#20123;&#22270;&#20687;&#36890;&#24120;&#22312;&#30524;&#31185;&#26816;&#26597;&#20013;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;CNN&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#33391;&#22909;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#22312;&#35768;&#22810;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36275;&#22815;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#32597;&#35265;&#30142;&#30149;&#25110;&#19987;&#23478;&#26631;&#35760;&#25104;&#26412;&#39640;&#26114;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#26377;&#20004;&#20010;&#36129;&#29486;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23427;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#23402;&#29983;&#32593;&#32476;&#65292;&#24341;&#20837;&#19968;&#31181;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#19988;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#27861;&#65307;&#65288;2&#65289;&#23427;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#21033;&#29992;&#39069;&#22806;&#30340;&#26410;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#20219;&#21153;&#23402;&#29983;&#32593;&#32476;&#65288;MTSN&#65289;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#39592;&#24178;CNN&#65292;&#24182;&#19988;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#39592;&#24178;CNN&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20854;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#32463;&#36807;&#35757;&#32451;&#30340;&#39592;&#24178;CNN&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) are a promising technique for automated glaucoma diagnosis from images of the fundus, and these images are routinely acquired as part of an ophthalmic exam. Nevertheless, CNNs typically require a large amount of well-labeled data for training, which may not be available in many biomedical image classification applications, especially when diseases are rare and where labeling by experts is costly. This article makes two contributions to address this issue: (1) It extends the conventional Siamese network and introduces a training method for low-shot learning when labeled data are limited and imbalanced, and (2) it introduces a novel semi-supervised learning strategy that uses additional unlabeled training data to achieve greater accuracy. Our proposed multi-task Siamese network (MTSN) can employ any backbone CNN, and we demonstrate with four backbone CNNs that its accuracy with limited training data approaches the accuracy of backbone CNNs trained wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/1710.05468</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Generalization in Deep Learning. (arXiv:1710.05468v8 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1710.05468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#22238;&#24212;&#20102;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25552;&#20379;&#28145;&#24230;&#23398;&#20064;&#38750;&#34394;&#31354;&#27867;&#21270;&#20445;&#35777;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#29702;&#35770;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.
&lt;/p&gt;</description></item><item><title>DeepTransport&#26159;&#19968;&#20010;&#21033;&#29992;CNN&#21644;RNN&#23398;&#20064;&#20132;&#36890;&#32593;&#32476;&#25299;&#25169;&#20869;&#26102;&#31354;&#20132;&#36890;&#20449;&#24687;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#21608;&#22260;&#20301;&#32622;&#30340;&#20132;&#36890;&#24773;&#20917;&#26469;&#35299;&#20915;&#25928;&#26524;&#34928;&#20943;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23545;&#40784;&#26102;&#31354;&#20449;&#24687;&#12290;&#20182;&#20204;&#36824;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#20998;&#36776;&#29575;&#20026;5&#20998;&#38047;&#30340;&#30495;&#23454;&#19990;&#30028;&#22823;&#22411;&#20132;&#36890;&#29366;&#20917;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/1709.09585</link><description>&lt;p&gt;
DeepTransport: &#23398;&#20064;&#26102;&#31354;&#20381;&#36182;&#24615;&#36827;&#34892;&#20132;&#36890;&#24773;&#20917;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeepTransport: Learning Spatial-Temporal Dependency for Traffic Condition Forecasting. (arXiv:1709.09585v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1709.09585
&lt;/p&gt;
&lt;p&gt;
DeepTransport&#26159;&#19968;&#20010;&#21033;&#29992;CNN&#21644;RNN&#23398;&#20064;&#20132;&#36890;&#32593;&#32476;&#25299;&#25169;&#20869;&#26102;&#31354;&#20132;&#36890;&#20449;&#24687;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#21608;&#22260;&#20301;&#32622;&#30340;&#20132;&#36890;&#24773;&#20917;&#26469;&#35299;&#20915;&#25928;&#26524;&#34928;&#20943;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23545;&#40784;&#26102;&#31354;&#20449;&#24687;&#12290;&#20182;&#20204;&#36824;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#20998;&#36776;&#29575;&#20026;5&#20998;&#38047;&#30340;&#30495;&#23454;&#19990;&#30028;&#22823;&#22411;&#20132;&#36890;&#29366;&#20917;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20132;&#36890;&#29366;&#20917;&#26368;&#36817;&#34987;&#25506;&#32034;&#20316;&#20026;&#32531;&#35299;&#20132;&#36890;&#25317;&#22581;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#19968;&#20123;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#65292;&#22522;&#20110;&#30446;&#26631;&#20301;&#32622;&#21450;&#20854;&#30456;&#37051;&#21306;&#22495;&#30340;&#20132;&#36890;&#35266;&#23519;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#36947;&#36335;&#25299;&#25169;&#30340;&#25366;&#25496;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#26377;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#25928;&#26524;&#34928;&#20943;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#32771;&#34385;&#21608;&#22260;&#20301;&#32622;&#65288;&#27604;&#37051;&#25509;&#33539;&#22260;&#26356;&#24191;&#65289;&#30340;&#20132;&#36890;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;DeepTransport&#65292;&#20854;&#20013;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#33719;&#21462;&#20132;&#36890;&#32593;&#32476;&#25299;&#25169;&#20869;&#30340;&#26102;&#31354;&#20132;&#36890;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23545;&#40784;&#26102;&#31354;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22823;&#22411;&#20132;&#36890;&#29366;&#20917;&#25968;&#25454;&#38598;&#65292;&#20998;&#36776;&#29575;&#20026;5&#20998;&#38047;&#12290;&#25105;&#20204;&#23545;&#35813;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#25417;&#21040;&#20102;&#26102;&#31354;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting traffic conditions has been recently explored as a way to relieve traffic congestion. Several pioneering approaches have been proposed based on traffic observations of the target location as well as its adjacent regions, but they obtain somewhat limited accuracy due to a lack of mining road topology. To address the effect attenuation problem, we suggest taking into account the traffic of surrounding locations(wider than the adjacent range). We propose an end-to-end framework called DeepTransport, in which Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are utilized to obtain spatial-temporal traffic information within a transport network topology. In addition, an attention mechanism is introduced to align spatial and temporal information. Moreover, we constructed and released a real-world large traffic condition dataset with a 5-minute resolution. Our experiments on this dataset demonstrate our method captures the complex relationship in the temporal 
&lt;/p&gt;</description></item></channel></rss>