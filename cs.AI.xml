<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;MLLM&#22312;&#32431;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#23545;&#40784;&#33021;&#21147;&#65292;&#36825;&#24471;&#30410;&#20110;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#21644;&#20248;&#31168;&#30340;&#25351;&#23548;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.07120</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26412;&#35270;&#37326;&#65306;&#22810;&#27169;&#24577;&#35757;&#32451;&#25552;&#21319;&#20102;&#22312;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#36947;&#24503;&#26041;&#38754;&#30340;MLLM
&lt;/p&gt;
&lt;p&gt;
Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics. (arXiv:2309.07120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07120
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;MLLM&#22312;&#32431;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#23545;&#40784;&#33021;&#21147;&#65292;&#36825;&#24471;&#30410;&#20110;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#21644;&#20248;&#31168;&#30340;&#25351;&#23548;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#22791;&#29702;&#35299;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#29983;&#25104;&#25991;&#26412;&#21709;&#24212;&#30340;&#22686;&#24378;&#33021;&#21147;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;MLLM&#30340;&#32431;NLP&#33021;&#21147;&#24120;&#24120;&#20302;&#20272;&#24182;&#26410;&#32463;&#27979;&#35797;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;MLLM&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24615;&#8212;&#8212;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#65292;&#19968;&#31181;&#23558;LLM&#36716;&#25442;&#20026;MLLM&#30340;&#27969;&#34892;&#31574;&#30053;&#65292;&#20986;&#20046;&#24847;&#26009;&#22320;&#24110;&#21161;&#27169;&#22411;&#22312;&#32431;NLP&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;&#20363;&#22914;&#65292;&#32463;&#36807;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#30340;LLaMA2 7B&#27169;&#22411;&#22312;TruthfulQA-mc&#21644;&#20262;&#29702;&#36947;&#24503;&#22522;&#20934;&#19978;&#36229;&#36807;&#20102;&#32463;&#36807;&#36229;&#36807;&#19968;&#30334;&#19975;&#20154;&#24037;&#26631;&#27880;&#30340;LLaMA2-chat 7B&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#30340;&#23545;&#40784;&#21487;&#20197;&#24402;&#22240;&#20110;&#35270;&#35273;-&#25991;&#26412;&#25968;&#25454;&#22266;&#26377;&#30340;&#20248;&#31168;&#25351;&#23548;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models (MLLMs) are trained based on large language models (LLM), with an enhanced capability to comprehend multi-modal inputs and generate textual responses. While they excel in multi-modal tasks, the pure NLP abilities of MLLMs are often underestimated and left untested. In this study, we get out of the box and unveil an intriguing characteristic of MLLMs -- our preliminary results suggest that visual instruction tuning, a prevailing strategy for transitioning LLMs into MLLMs, unexpectedly and interestingly helps models attain both improved truthfulness and ethical alignment in the pure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model surpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one million human annotations, on TruthfulQA-mc and Ethics benchmarks. Further analysis reveals that the improved alignment can be attributed to the superior instruction quality inherent to visual-text data. In releasing our code at 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#36895;&#24230;&#24615;&#33021;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#30740;&#31350;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#20837;&#20998;&#31867;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;MARL&#31639;&#27861;&#20316;&#20026;&#30446;&#26631;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#20248;&#21270;&#22870;&#21169;&#30340;&#31639;&#27861;&#22312;&#35757;&#32451;&#26102;&#38388;&#36895;&#24230;&#24615;&#33021;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.07108</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36895;&#24230;&#24615;&#33021;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Characterizing Speed Performance of Multi-Agent Reinforcement Learning. (arXiv:2309.07108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#36895;&#24230;&#24615;&#33021;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#30740;&#31350;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#20837;&#20998;&#31867;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;MARL&#31639;&#27861;&#20316;&#20026;&#30446;&#26631;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#20248;&#21270;&#22870;&#21169;&#30340;&#31639;&#27861;&#22312;&#35757;&#32451;&#26102;&#38388;&#36895;&#24230;&#24615;&#33021;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#65288;&#22914;&#26234;&#33021;&#30005;&#32593;&#12289;&#30417;&#25511;&#31561;&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#26426;&#21046;&#26469;&#25913;&#36827;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20197;&#25552;&#39640;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20248;&#21270;&#36890;&#24120;&#20250;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#26041;&#38754;&#20135;&#29983;&#36739;&#22823;&#36127;&#25285;&#65292;&#20174;&#32780;&#23548;&#33268;&#31471;&#21040;&#31471;&#35757;&#32451;&#26102;&#38388;&#30340;&#36895;&#24230;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36895;&#24230;&#24615;&#33021;&#65288;&#21363;&#24310;&#36831;&#21463;&#38480;&#21534;&#21520;&#37327;&#65289;&#20316;&#20026;MARL&#23454;&#29616;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#21152;&#36895;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#20010;MARL&#31639;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#35757;&#32451;&#26041;&#26696;&#21644;&#65288;2&#65289;&#36890;&#20449;&#26041;&#27861;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;MARL&#31639;&#27861;&#8212;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;MADDPG&#65289;&#12289;&#38754;&#21521;&#30446;&#26631;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#19982;&#21512;&#20316;&#65288;ToM2C&#65289;&#21644;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;RL&#65288;NeurComm&#65289;&#8212;&#20316;&#20026;&#30446;&#26631;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) has achieved significant success in large-scale AI systems and big-data applications such as smart grids, surveillance, etc. Existing advancements in MARL algorithms focus on improving the rewards obtained by introducing various mechanisms for inter-agent cooperation. However, these optimizations are usually compute- and memory-intensive, thus leading to suboptimal speed performance in end-to-end training time. In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Llama-2&#21644;GPT-3&#22312;&#29983;&#25104;HPC&#26680;&#24515;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;Llama-2&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;GitHub Copilot&#29983;&#25104;&#30340;&#20195;&#30721;&#26356;&#21487;&#38752;&#65292;&#32780;Llama-2&#29983;&#25104;&#30340;&#20195;&#30721;&#26356;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2309.07103</link><description>&lt;p&gt;
&#27604;&#36739;Llama-2&#21644;GPT-3&#22312;HPC&#26680;&#24515;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Comparing Llama-2 and GPT-3 LLMs for HPC kernels generation. (arXiv:2309.07103v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Llama-2&#21644;GPT-3&#22312;&#29983;&#25104;HPC&#26680;&#24515;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;Llama-2&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;GitHub Copilot&#29983;&#25104;&#30340;&#20195;&#30721;&#26356;&#21487;&#38752;&#65292;&#32780;Llama-2&#29983;&#25104;&#30340;&#20195;&#30721;&#26356;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#24320;&#28304;Llama-2&#27169;&#22411;&#22312;&#19981;&#21516;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#21644;&#35821;&#35328;&#19978;&#29983;&#25104;&#33879;&#21517;&#39640;&#24615;&#33021;&#35745;&#31639;&#26680;&#24515;&#65288;&#22914;AXPY&#65292;GEMV&#65292;GEMM&#65289;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#20351;&#29992;&#22522;&#20110;OpenAI Codex&#30340;&#31616;&#21333;&#25552;&#31034;&#36890;&#36807;GitHub Copilot&#29983;&#25104;&#31867;&#20284;&#30340;&#26680;&#24515;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#31867;&#20284;&#30340;&#25351;&#26631;&#27604;&#36739;Llama-2&#21644;&#25105;&#20204;&#21407;&#22987;&#30340;GPT-3&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#12290;Llama-2&#20855;&#26377;&#31616;&#21270;&#30340;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26377;&#31454;&#20105;&#21147;&#29978;&#33267;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#36825;&#20123;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22240;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32487;&#32493;&#37325;&#26032;&#23450;&#20041;&#20154;&#26426;&#20132;&#20114;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Copilot&#29983;&#25104;&#30340;&#20195;&#30721;&#26356;&#21487;&#38752;&#20294;&#20248;&#21270;&#31243;&#24230;&#36739;&#20302;&#65292;&#32780;Llama-2&#29983;&#25104;&#30340;&#20195;&#30721;&#21017;&#26356;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate the use of the open-source Llama-2 model for generating well-known, high-performance computing kernels (e.g., AXPY, GEMV, GEMM) on different parallel programming models and languages (e.g., C++: OpenMP, OpenMP Offload, OpenACC, CUDA, HIP; Fortran: OpenMP, OpenMP Offload, OpenACC; Python: numpy, Numba, pyCUDA, cuPy; and Julia: Threads, CUDA.jl, AMDGPU.jl). We built upon our previous work that is based on the OpenAI Codex, which is a descendant of GPT-3, to generate similar kernels with simple prompts via GitHub Copilot. Our goal is to compare the accuracy of Llama-2 and our original GPT-3 baseline by using a similar metric. Llama-2 has a simplified model that shows competitive or even superior accuracy. We also report on the differences between these foundational large language models as generative AI continues to redefine human-computer interactions. Overall, Copilot generates codes that are more reliable but less optimized, whereas codes generated by Llama-2 are less relia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#26469;&#20943;&#36731;&#20840;&#23616;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07085</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#35774;&#22791;&#19978;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32676;&#20307;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Group Bias in Federated Learning for Heterogeneous Devices. (arXiv:2309.07085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#26469;&#20943;&#36731;&#20840;&#23616;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#27491;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#23853;&#38706;&#22836;&#35282;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36793;&#32536;&#37096;&#32626;&#26159;&#24322;&#26500;&#30340;&#65292;&#21363;&#23427;&#20204;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#29615;&#22659;&#22312;&#37096;&#32626;&#20013;&#21508;&#19981;&#30456;&#21516;&#12290;&#36825;&#31181;&#36793;&#32536;&#24322;&#26500;&#36829;&#21453;&#20102;&#26412;&#22320;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#29420;&#31435;&#19988;&#20998;&#24067;&#30456;&#21516; (IID) &#30340;&#29305;&#24615;&#65292;&#20135;&#29983;&#20102;&#26377;&#20559;&#35265;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21363;&#23545;&#29305;&#23450;&#31038;&#21306;&#25110;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#21644;&#27495;&#35270;&#12290;&#29616;&#26377;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#21482;&#20851;&#27880;&#38750;IID&#25968;&#25454;&#20013;&#30001;&#26631;&#31614;&#24322;&#26500;&#24341;&#36215;&#30340;&#20559;&#35265;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#30001;&#29305;&#24449;&#24322;&#26500;&#23548;&#33268;&#30340;&#39046;&#22495;&#21464;&#21270;&#65292;&#20063;&#27809;&#26377;&#35299;&#20915;&#20840;&#23616;&#32676;&#20307;&#20844;&#24179;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#19981;&#22686;&#21152;&#36164;&#28304;&#21033;&#29992;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#32676;&#20307;&#20559;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;&#24179;&#22343;&#26465;&#20214;&#27010;&#29575;&#26469;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients and produces biased global models i.e. models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity and do not address global group-fairness property.  Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy and without resource utilization overhead. Our main idea is to leverage average conditional probabilities to compute a cross-domain group \textit{importance we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#29616;&#20195;&#25968;&#23383;&#21462;&#35777;&#21644;&#20107;&#25925;&#21709;&#24212;&#20013;&#30340;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#30456;&#20851;&#25216;&#26415;&#30340;&#24212;&#29992;&#21450;&#20854;&#23545;&#21462;&#35777;&#35843;&#26597;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.07064</link><description>&lt;p&gt;
&#29616;&#20195;&#25968;&#23383;&#21462;&#35777;&#19982;&#20107;&#25925;&#21709;&#24212;&#20013;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#38754;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Analysis of the Role of Artificial Intelligence and Machine Learning in Modern Digital Forensics and Incident Response. (arXiv:2309.07064v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#29616;&#20195;&#25968;&#23383;&#21462;&#35777;&#21644;&#20107;&#25925;&#21709;&#24212;&#20013;&#30340;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#30456;&#20851;&#25216;&#26415;&#30340;&#24212;&#29992;&#21450;&#20854;&#23545;&#21462;&#35777;&#35843;&#26597;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#30340;&#25968;&#23383;&#21462;&#35777;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#25972;&#21512;&#20316;&#20026;&#19968;&#39033;&#21464;&#38761;&#24615;&#25216;&#26415;&#65292;&#26377;&#26395;&#25552;&#39640;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;AI&#21644;ML&#22312;&#25968;&#23383;&#21462;&#35777;&#20013;&#30340;&#24212;&#29992;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36827;&#34892;&#28145;&#20837;&#32780;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#36229;&#36234;&#31616;&#21333;&#30340;&#35843;&#30740;&#21644;&#22238;&#39038;&#12290;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#23494;&#20999;&#20851;&#27880;AI&#21644;ML&#25216;&#26415;&#22312;&#25968;&#23383;&#21462;&#35777;&#21644;&#20107;&#25925;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36328;&#39046;&#22495;&#30340;&#21069;&#27839;&#30740;&#31350;&#20513;&#35758;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#21644;&#24674;&#22797;&#12289;&#22797;&#26434;&#30340;&#32593;&#32476;&#29359;&#32618;&#26102;&#38388;&#32447;&#37325;&#24314;&#12289;&#24378;&#22823;&#30340;&#22823;&#25968;&#25454;&#20998;&#26512;&#12289;&#27169;&#24335;&#35782;&#21035;&#12289;&#20445;&#25252;&#35777;&#25454;&#38142;&#26465;&#21644;&#32452;&#32455;&#21709;&#24212;&#24615;&#31574;&#30053;&#31561;&#12290;&#36825;&#39033;&#21162;&#21147;&#28145;&#20837;&#25366;&#25496;&#20102;AI&#39537;&#21160;&#26041;&#27861;&#23545;&#25968;&#23383;&#21462;&#35777;&#30340;&#20851;&#38190;&#26041;&#38754;&#20135;&#29983;&#30340;&#24494;&#22937;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the dynamic landscape of digital forensics, the integration of Artificial Intelligence (AI) and Machine Learning (ML) stands as a transformative technology, poised to amplify the efficiency and precision of digital forensics investigations. However, the use of ML and AI in digital forensics is still in its nascent stages. As a result, this paper gives a thorough and in-depth analysis that goes beyond a simple survey and review. The goal is to look closely at how AI and ML techniques are used in digital forensics and incident response. This research explores cutting-edge research initiatives that cross domains such as data collection and recovery, the intricate reconstruction of cybercrime timelines, robust big data analysis, pattern recognition, safeguarding the chain of custody, and orchestrating responsive strategies to hacking incidents. This endeavour digs far beneath the surface to unearth the intricate ways AI-driven methodologies are shaping these crucial facets of digital fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20195;&#30721;&#20248;&#21270;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20197;7B&#21442;&#25968;&#30340;transformer&#27169;&#22411;&#20026;&#20363;&#65292;&#36890;&#36807;&#39044;&#27979;&#25351;&#20196;&#35745;&#25968;&#21644;&#29983;&#25104;&#20248;&#21270;&#20195;&#30721;&#31561;&#36741;&#21161;&#23398;&#20064;&#20219;&#21153;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;&#22823;&#37327;&#27979;&#35797;&#31243;&#24207;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#32534;&#35793;&#22120;&#30340;&#20248;&#21270;&#25928;&#26524;&#25552;&#39640;&#20102;3.0%&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#21916;&#30340;&#24378;&#22823;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07062</link><description>&lt;p&gt;
&#29992;&#20110;&#32534;&#35793;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Compiler Optimization. (arXiv:2309.07062v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20195;&#30721;&#20248;&#21270;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20197;7B&#21442;&#25968;&#30340;transformer&#27169;&#22411;&#20026;&#20363;&#65292;&#36890;&#36807;&#39044;&#27979;&#25351;&#20196;&#35745;&#25968;&#21644;&#29983;&#25104;&#20248;&#21270;&#20195;&#30721;&#31561;&#36741;&#21161;&#23398;&#20064;&#20219;&#21153;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;&#22823;&#37327;&#27979;&#35797;&#31243;&#24207;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#32534;&#35793;&#22120;&#30340;&#20248;&#21270;&#25928;&#26524;&#25552;&#39640;&#20102;3.0%&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#21916;&#30340;&#24378;&#22823;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20195;&#30721;&#20248;&#21270;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;7B&#21442;&#25968;&#30340;transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#20248;&#21270;LLVM&#27719;&#32534;&#30340;&#20195;&#30721;&#22823;&#23567;&#12290;&#35813;&#27169;&#22411;&#20197;&#26410;&#20248;&#21270;&#30340;&#27719;&#32534;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#19968;&#32452;&#26368;&#20339;&#20248;&#21270;&#31243;&#24207;&#30340;&#32534;&#35793;&#22120;&#36873;&#39033;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#20248;&#21270;&#21069;&#21518;&#30340;&#25351;&#20196;&#35745;&#25968;&#21644;&#20248;&#21270;&#21518;&#30340;&#20195;&#30721;&#26412;&#36523;&#12290;&#36825;&#20123;&#36741;&#21161;&#23398;&#20064;&#20219;&#21153;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#28145;&#24230;&#12290;&#25105;&#20204;&#22312;&#19968;&#22871;&#22823;&#22411;&#27979;&#35797;&#31243;&#24207;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#25351;&#20196;&#35745;&#25968;&#26041;&#38754;&#27604;&#32534;&#35793;&#22120;&#25552;&#39640;&#20102;3.0%&#65292;&#36229;&#36807;&#20102;&#38656;&#35201;&#25968;&#21315;&#27425;&#32534;&#35793;&#30340;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#26174;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#65292;91%&#30340;&#26102;&#38388;&#29983;&#25104;&#21487;&#32534;&#35793;&#30340;&#20195;&#30721;&#65292;&#24182;70%&#30340;&#26102;&#38388;&#33021;&#23436;&#32654;&#27169;&#25311;&#32534;&#35793;&#22120;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding.  We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#24067;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.07056</link><description>&lt;p&gt;
&#28145;&#24230;&#37327;&#23376;&#22270;&#20687;&#27169;&#25311;&#65306;&#35299;&#26512;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#23454;&#39564;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments. (arXiv:2309.07056v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#24067;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#20419;&#36827;&#26032;&#30340;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#20854;&#36923;&#36753;&#32972;&#21518;&#30340;&#19981;&#36879;&#26126;&#24615;&#32473;&#35299;&#37322;&#20854;&#21457;&#29616;&#30340;&#25361;&#25112;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#8220;inception&#8221;&#25110;&#8220;&#28145;&#24230;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#34987;&#21457;&#26126;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#26469;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#25925;&#20107;&#20174;&#23545;&#37327;&#23376;&#31995;&#32479;&#23646;&#24615;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24320;&#22987;&#12290;&#32463;&#36807;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#8220;&#21453;&#36716;&#8221;&#31070;&#32463;&#32593;&#32476;--&#23454;&#38469;&#19978;&#26159;&#35810;&#38382;&#23427;&#22914;&#20309;&#24819;&#35937;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#37327;&#23376;&#31995;&#32479;&#65292;&#20197;&#21450;&#22914;&#20309;&#36830;&#32493;&#20462;&#25913;&#37327;&#23376;&#31995;&#32479;&#20197;&#25913;&#21464;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#21021;&#22987;&#23646;&#24615;&#20998;&#24067;&#65292;&#25105;&#20204;&#21487;&#20197;&#27010;&#24565;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#36739;&#27973;&#23618;&#65292;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#31616;&#21333;&#30340;&#23646;&#24615;&#65292;&#32780;&#22312;&#36739;&#28145;&#23618;&#27425;&#19978;...&#65288;&#20869;&#23481;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Despite their promise to facilitate new scientific discoveries, the opaqueness of neural networks presents a challenge in interpreting the logic behind their findings. Here, we use a eXplainable-AI (XAI) technique called $inception$ or $deep$ $dreaming$, which has been invented in machine learning for computer vision. We use this techniques to explore what neural networks learn about quantum optics experiments. Our story begins by training a deep neural networks on the properties of quantum systems. Once trained, we "invert" the neural network -- effectively asking how it imagines a quantum system with a specific property, and how it would continuously modify the quantum system to change a property. We find that the network can shift the initial distribution of properties of the quantum system, and we can conceptualize the learned strategies of the neural network. Interestingly, we find that, in the first layers, the neural network identifies simple properties, while in the deeper ones
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28548;&#28165;&#20102;&#22312;&#38543;&#26426;&#32534;&#31243;&#20013;&#20351;&#29992;&#30340;&#23398;&#20064;&#27169;&#24335;Pearl&#21644;Jeffrey&#30340;&#26356;&#26032;&#26426;&#21046;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;Jeffrey&#30340;&#26356;&#26032;&#35268;&#21017;&#26159;&#36890;&#36807;&#21464;&#20998;&#25512;&#29702;&#24471;&#21040;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.07053</link><description>&lt;p&gt;
&#38543;&#26426;&#32534;&#31243;&#20013;&#30340;&#23398;&#20064;&#27169;&#24335;&#65306;Pearl&#21644;Jeffrey&#30340;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Pearl's and Jeffrey's Update as Modes of Learning in Probabilistic Programming. (arXiv:2309.07053v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28548;&#28165;&#20102;&#22312;&#38543;&#26426;&#32534;&#31243;&#20013;&#20351;&#29992;&#30340;&#23398;&#20064;&#27169;&#24335;Pearl&#21644;Jeffrey&#30340;&#26356;&#26032;&#26426;&#21046;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;Jeffrey&#30340;&#26356;&#26032;&#35268;&#21017;&#26159;&#36890;&#36807;&#21464;&#20998;&#25512;&#29702;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#26032;&#35777;&#25454;&#26356;&#26032;&#27010;&#29575;&#20998;&#24067;&#30340;&#27010;&#24565;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26680;&#24515;&#12290;Pearl&#21644;Jeffrey&#30340;&#35268;&#21017;&#26159;&#20004;&#31181;&#33258;&#28982;&#30340;&#26356;&#26032;&#26426;&#21046;&#65292;&#23427;&#20204;&#23548;&#33268;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#20294;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#20173;&#28982;&#31070;&#31192;&#12290;&#26412;&#25991;&#36890;&#36807;&#27010;&#29575;&#31243;&#24207;&#21644;&#37319;&#26679;&#35821;&#20041;&#30340;&#20998;&#21035;&#25551;&#36848;&#65292;&#20197;&#21450;&#20851;&#20110;Pearl&#21644;Jeffrey&#30340;&#19981;&#21516;&#20284;&#28982;&#24230;&#30340;&#27010;&#24565;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;Jeffrey&#30340;&#26356;&#26032;&#35268;&#21017;&#26159;&#36890;&#36807;&#21464;&#20998;&#25512;&#29702;&#24471;&#21040;&#30340;&#12290;&#20174;&#20998;&#31867;&#27010;&#29575;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#30456;&#24403;&#20110;&#23545;&#22810;&#37325;&#38598;&#21512;&#20989;&#23376;&#22312;&#20998;&#24067;&#21333;&#23376;&#33539;&#30068;&#20013;&#30340;&#34892;&#20026;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of updating a probability distribution in the light of new evidence lies at the heart of statistics and machine learning. Pearl's and Jeffrey's rule are two natural update mechanisms which lead to different outcomes, yet the similarities and differences remain mysterious. This paper clarifies their relationship in several ways: via separate descriptions of the two update mechanisms in terms of probabilistic programs and sampling semantics, and via different notions of likelihood (for Pearl and for Jeffrey). Moreover, it is shown that Jeffrey's update rule arises via variational inference. In terms of categorical probability theory, this amounts to an analysis of the situation in terms of the behaviour of the multiset functor, extended to the Kleisli category of the distribution monad.
&lt;/p&gt;</description></item><item><title>UnifiedGesture &#26159;&#19968;&#31181;&#35757;&#32451;&#22312;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#39592;&#26550;&#30340;&#25163;&#21183;&#25968;&#25454;&#38598;&#19978;&#30340;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#39537;&#21160;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#26469;&#32479;&#19968;&#25163;&#21183;&#34920;&#31034;&#24182;&#25429;&#25417;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07051</link><description>&lt;p&gt;
UnifiedGesture: &#22810;&#20010;&#39592;&#26550;&#30340;&#32479;&#19968;&#25163;&#21183;&#21512;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UnifiedGesture: A Unified Gesture Synthesis Model for Multiple Skeletons. (arXiv:2309.07051v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07051
&lt;/p&gt;
&lt;p&gt;
UnifiedGesture &#26159;&#19968;&#31181;&#35757;&#32451;&#22312;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#39592;&#26550;&#30340;&#25163;&#21183;&#25968;&#25454;&#38598;&#19978;&#30340;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#39537;&#21160;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#26469;&#32479;&#19968;&#25163;&#21183;&#34920;&#31034;&#24182;&#25429;&#25417;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20849;&#35821;&#25163;&#21183;&#29983;&#25104;&#22312;&#35745;&#31639;&#26426;&#21160;&#30011;&#20013;&#24341;&#36215;&#20102;&#24456;&#22823;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#23548;&#33268;&#20102;&#25968;&#25454;&#37327;&#30340;&#19981;&#36275;&#21644;&#22312;&#19981;&#21516;&#21160;&#20316;&#25429;&#25417;&#26631;&#20934;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35821;&#35328;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#24369;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UnifiedGesture&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#39537;&#21160;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#19981;&#21516;&#39592;&#26550;&#30340;&#22810;&#20010;&#25163;&#21183;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#37325;&#26032;&#23450;&#20301;&#32593;&#32476;&#65292;&#23398;&#20064;&#19981;&#21516;&#21160;&#20316;&#25429;&#25417;&#26631;&#20934;&#30340;&#28508;&#22312;&#21516;&#32986;&#22270;&#65292;&#32479;&#19968;&#21508;&#31181;&#25163;&#21183;&#30340;&#34920;&#31034;&#24182;&#25193;&#23637;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#25429;&#25417;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#36328;&#23616;&#37096;&#27880;&#24847;&#21147;&#21644;&#33258;&#27880;&#24847;&#21147;&#29983;&#25104;&#26356;&#22909;&#30340;&#19982;&#35821;&#38899;&#21305;&#37197;&#21644;&#26356;&#21152;&#36924;&#30495;&#30340;&#25163;&#21183;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#23545;&#40784;&#35821;&#38899;&#21644;&#25163;&#21183;&#24182;&#22686;&#21152;&#20004;&#32773;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#23618;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#21450;&#19968;&#20010;&#23039;&#21183;&#31283;&#23450;&#21270;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic co-speech gesture generation draws much attention in computer animation. Previous works designed network structures on individual datasets, which resulted in a lack of data volume and generalizability across different motion capture standards. In addition, it is a challenging task due to the weak correlation between speech and gestures. To address these problems, we present UnifiedGesture, a novel diffusion model-based speech-driven gesture synthesis approach, trained on multiple gesture datasets with different skeletons. Specifically, we first present a retargeting network to learn latent homeomorphic graphs for different motion capture standards, unifying the representations of various gestures while extending the dataset. We then capture the correlation between speech and gestures based on a diffusion model architecture using cross-local attention and self-attention to generate better speech-matched and realistic gestures. To further align speech and gesture and increa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#35299;&#20915;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26679;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2309.07038</link><description>&lt;p&gt;
&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning for Jumping Monopods. (arXiv:2309.07038v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#35299;&#20915;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26679;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#21363;&#20351;&#21333;&#33050;&#26426;&#22120;&#20154;&#33021;&#22815;&#36339;&#21040;&#20219;&#20309;&#26041;&#21521;&#65292;&#20854;&#33050;&#19979;&#30340;&#22320;&#24418;&#21487;&#33021;&#26159;&#19981;&#24179;&#30340;&#65292;&#25105;&#20204;&#35201;&#20351;&#23427;&#36798;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#36825;&#26159;&#19968;&#20010;&#26356;&#22823;&#31867;&#21035;&#38382;&#39064;&#30340;&#27169;&#26495;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#25216;&#26415;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#24378;&#21270;&#23398;&#20064; (RL) &#21487;&#33021;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23436;&#20840;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22312; RL &#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#24102;&#26469;&#20102;&#24191;&#27867;&#30340;&#22909;&#22788;&#65292;&#22914;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#25191;&#34892;&#36816;&#21160;&#30340;&#20302;&#32423;&#25511;&#21046;&#22120;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#22522;&#20110;&#20248;&#21270;&#21644;&#31471;&#21040;&#31471; RL &#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the complex control problem of making a monopod reach a target with a jump. The monopod can jump in any direction and the terrain underneath its foot can be uneven. This is a template of a much larger class of problems, which are extremely challenging and computationally expensive to solve using standard optimisation-based techniques. Reinforcement Learning (RL) could be an interesting alternative, but the application of an end-to-end approach in which the controller must learn everything from scratch, is impractical. The solution advocated in this paper is to guide the learning process within an RL framework by injecting physical knowledge. This expedient brings to widespread benefits, such as a drastic reduction of the learning time, and the ability to learn and compensate for possible errors in the low-level controller executing the motion. We demonstrate the advantage of our approach with respect to both optimization-based and end-to-end RL approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#65292;&#21457;&#29616;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07034</link><description>&lt;p&gt;
&#22914;&#20309;&#65288;&#19981;&#65289;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
How (Not) to Use Sociodemographic Information for Subjective NLP Tasks. (arXiv:2309.07034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#65292;&#21457;&#29616;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#37322;&#32773;&#30340;&#31038;&#20250;&#20154;&#21475;&#32972;&#26223;&#65288;&#21363;&#24615;&#21035;&#65292;&#24180;&#40836;&#65292;&#25945;&#32946;&#32972;&#26223;&#31561;&#20010;&#20307;&#32452;&#25104;&#65289;&#23545;&#20854;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#27604;&#22914;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12290;&#36890;&#24120;&#65292;&#24322;&#36136;&#30340;&#32972;&#26223;&#20250;&#23548;&#33268;&#39640;&#24230;&#20998;&#27495;&#12290;&#20026;&#20102;&#24314;&#27169;&#36825;&#31181;&#24046;&#24322;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#31181;&#25216;&#26415;&#23558;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#20855;&#26377;&#29305;&#23450;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#30340;&#20154;&#31867;&#21487;&#33021;&#32473;&#20986;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLP&#25991;&#29486;&#23545;&#36825;&#31181;&#25216;&#26415;&#30340;&#25928;&#26524;&#23384;&#22312;&#20998;&#27495; - &#23427;&#20173;&#28982;&#19981;&#28165;&#26970;&#23427;&#33021;&#22312;&#21738;&#20123;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#26377;&#24110;&#21161;&#65292;&#24182;&#19988;&#35780;&#20272;&#20165;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#21644;&#26368;&#20840;&#38754;&#30340;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19971;&#20010;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;&#27169;&#22411;&#23478;&#26063;&#20013;&#30340;&#20960;&#20010;&#25552;&#31034;&#24418;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotators' sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as hate speech detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique -- it remains unclear, for which tasks and scenarios it can help and evaluations are limited to specific tasks only. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. Concretely, we evaluate several prompt formulations across seven datasets and six instruction-tuned model families. We find that (1) while sociodemographic prompt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#31616;&#21382;&#35299;&#26512;&#38382;&#39064;&#20316;&#20026;&#20998;&#23618;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#21516;&#26102;&#35299;&#20915;&#34892;&#21644;&#26631;&#35760;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#30340;&#39640;&#36136;&#37327;&#31616;&#21382;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#65292;&#24182;&#25551;&#36848;&#20102;&#27169;&#22411;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.07015</link><description>&lt;p&gt;
&#31616;&#21382;&#35299;&#26512;&#20316;&#20026;&#20998;&#23618;&#24207;&#21015;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
R\'esum\'e Parsing as Hierarchical Sequence Labeling: An Empirical Study. (arXiv:2309.07015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31616;&#21382;&#35299;&#26512;&#38382;&#39064;&#20316;&#20026;&#20998;&#23618;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#21516;&#26102;&#35299;&#20915;&#34892;&#21644;&#26631;&#35760;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#30340;&#39640;&#36136;&#37327;&#31616;&#21382;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#65292;&#24182;&#25551;&#36848;&#20102;&#27169;&#22411;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31616;&#21382;&#20013;&#25552;&#21462;&#20449;&#24687;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#38382;&#39064;&#65292;&#21363;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#27573;&#65292;&#28982;&#21518;&#23545;&#27599;&#20010;&#27573;&#33853;&#36827;&#34892;&#21333;&#29420;&#22788;&#29702;&#20197;&#25552;&#21462;&#30446;&#26631;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#25972;&#20010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#32423;&#21035;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#21363;&#34892;&#21644;&#26631;&#35760;&#65292;&#24182;&#30740;&#31350;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#20013;&#25991;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#24503;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#29790;&#20856;&#35821;&#30340;&#39640;&#36136;&#37327;&#31616;&#21382;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#22522;&#20110;&#36825;&#20123;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#65292;&#24182;&#25551;&#36848;&#20102;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#36827;&#34892;&#27169;&#22411;&#37096;&#32626;&#26102;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting information from r\'esum\'es is typically formulated as a two-stage problem, where the document is first segmented into sections and then each section is processed individually to extract the target entities. Instead, we cast the whole problem as sequence labeling in two levels -- lines and tokens -- and study model architectures for solving both tasks simultaneously. We build high-quality r\'esum\'e parsing corpora in English, French, Chinese, Spanish, German, Portuguese, and Swedish. Based on these corpora, we present experimental results that demonstrate the effectiveness of the proposed models for the information extraction task, outperforming approaches introduced in previous work. We conduct an ablation study of the proposed architectures. We also analyze both model performance and resource efficiency, and describe the trade-offs for model deployment in the context of a production environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21160;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#26469;&#33258;&#25216;&#26415;&#20844;&#21496;&#30340;21&#19990;&#32426;ESG&#25253;&#21578;&#30340;&#20016;&#23500;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;ESG&#35266;&#28857;&#30340;&#28436;&#21270;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.07001</link><description>&lt;p&gt;
&#20225;&#19994;ESG&#25253;&#21578;&#30340;&#21160;&#24577;&#20998;&#26512;&#65306;&#19968;&#20010;&#28436;&#21270;&#36235;&#21183;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dynamic Analysis of Corporate ESG Reports: A Model of Evolutionary Trends. (arXiv:2309.07001v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21160;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#26469;&#33258;&#25216;&#26415;&#20844;&#21496;&#30340;21&#19990;&#32426;ESG&#25253;&#21578;&#30340;&#20016;&#23500;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;ESG&#35266;&#28857;&#30340;&#28436;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;(ESG)&#25253;&#21578;&#34987;&#20840;&#29699;&#20844;&#35748;&#20026;&#21487;&#25345;&#32493;&#20225;&#19994;&#21457;&#23637;&#30340;&#22522;&#30707;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#32472;&#21046;&#20840;&#29699;&#24066;&#22330;&#20013;&#20844;&#21496;ESG&#20027;&#39064;&#30340;&#21464;&#21270;&#26684;&#23616;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21160;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#21035;&#31867;&#21035;&#12289;&#22810;&#20010;&#31867;&#21035;&#21644;&#19982;&#29305;&#23450;&#21487;&#25345;&#32493;&#24615;&#25351;&#25968;&#20445;&#25345;&#19968;&#33268;&#30340;ESG&#25112;&#30053;&#31649;&#29702;&#12290;&#36825;&#20123;&#20998;&#26512;&#36807;&#31243;&#30340;&#36755;&#20986;&#26500;&#25104;&#20102;ESG&#25112;&#30053;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#23558;&#20998;&#26512;&#24615;&#20851;&#38190;&#35789;&#32435;&#20837;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26469;&#33258;&#25216;&#26415;&#20844;&#21496;&#30340;21&#19990;&#32426;ESG&#25253;&#21578;&#30340;&#20016;&#23500;&#25910;&#38598;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#38416;&#26126;&#20102;ESG&#35266;&#28857;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#39564;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#36817;&#24180;&#26469;ESG&#20027;&#39064;&#30340;&#24182;&#34892;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Environmental, social, and governance (ESG) reports are globally recognized as a keystone in sustainable enterprise development. This study aims to map the changing landscape of ESG topics within firms in the global market. A dynamic framework is developed to analyze ESG strategic management for individual classes, across multiple classes, and in alignment with a specific sustainability index. The output of these analytical processes forms the foundation of an ESG strategic model. Utilizing a rich collection of 21st-century ESG reports from technology companies, our experiment elucidates the changes in ESG perspectives by incorporating analytical keywords into the proposed framework. This work thus provides an empirical method that reveals the concurrent evolution of ESG topics over recent years.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASTERKEY&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#38024;&#23545;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#12290;&#19982;&#20197;&#24448;&#25915;&#20987;&#19981;&#21516;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#25915;&#20987;&#32773;&#23545;&#30446;&#26631;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#20102;&#35299;&#12290;&#36890;&#36807;&#23884;&#20837;&#35828;&#35805;&#20154;&#29305;&#24449;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#38598;&#25104;&#20449;&#36947;&#22833;&#30495;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#30772;&#22351;&#22810;&#20010;&#27969;&#34892;&#30340;SV&#27169;&#22411;&#65292;&#36798;&#21040;100&#65285;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.06981</link><description>&lt;p&gt;
MASTERKEY: &#23545;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#30340;&#23454;&#38469;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems. (arXiv:2309.06981v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06981
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASTERKEY&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#38024;&#23545;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#12290;&#19982;&#20197;&#24448;&#25915;&#20987;&#19981;&#21516;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#25915;&#20987;&#32773;&#23545;&#30446;&#26631;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#20102;&#35299;&#12290;&#36890;&#36807;&#23884;&#20837;&#35828;&#35805;&#20154;&#29305;&#24449;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#38598;&#25104;&#20449;&#36947;&#22833;&#30495;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#30772;&#22351;&#22810;&#20010;&#27969;&#34892;&#30340;SV&#27169;&#22411;&#65292;&#36798;&#21040;100&#65285;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;SV&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#31227;&#21160;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#30340;&#35821;&#38899;&#29305;&#24449;&#26469;&#35748;&#35777;&#21512;&#27861;&#29992;&#25143;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASTERKEY&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20197;&#30772;&#22351;SV&#27169;&#22411;&#12290;&#19982;&#20197;&#24448;&#30340;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#22312;&#25915;&#20987;&#32773;&#23545;&#30446;&#26631;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#20102;&#35299;&#30340;&#23454;&#38469;&#29615;&#22659;&#19979;&#36827;&#34892;&#25915;&#20987;&#12290;&#20026;&#20102;&#35774;&#35745;MASTERKEY&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#38024;&#23545;&#26410;&#35265;&#30446;&#26631;&#30340;&#20013;&#27602;&#25915;&#20987;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21518;&#38376;&#65292;&#21487;&#20197;&#25915;&#20987;&#20219;&#24847;&#30446;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#35828;&#35805;&#20154;&#30340;&#29305;&#24449;&#21644;&#35821;&#20041;&#20449;&#24687;&#23884;&#20837;&#21040;&#21518;&#38376;&#20013;&#65292;&#20351;&#20854;&#19981;&#21487;&#23519;&#35273;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#20449;&#36947;&#22833;&#30495;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#21518;&#38376;&#20013;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;6&#20010;&#27969;&#34892;&#30340;SV&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20013;&#27602;&#20102;&#20849;&#35745;53&#20010;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#35302;&#21457;&#22120;&#25915;&#20987;&#20102;16,430&#20010;&#27880;&#20876;&#35828;&#35805;&#20154;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;53&#20010;&#20013;&#27602;&#27169;&#22411;&#20013;&#27880;&#20876;&#30340;310&#20010;&#30446;&#26631;&#35828;&#35805;&#20154;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#36798;&#21040;100&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker Verification (SV) is widely deployed in mobile systems to authenticate legitimate users by using their voice traits. In this work, we propose a backdoor attack MASTERKEY, to compromise the SV models. Different from previous attacks, we focus on a real-world practical setting where the attacker possesses no knowledge of the intended victim. To design MASTERKEY, we investigate the limitation of existing poisoning attacks against unseen targets. Then, we optimize a universal backdoor that is capable of attacking arbitrary targets. Next, we embed the speaker's characteristics and semantics information into the backdoor, making it imperceptible. Finally, we estimate the channel distortion and integrate it into the backdoor. We validate our attack on 6 popular SV models. Specifically, we poison a total of 53 models and use our trigger to attack 16,430 enrolled speakers, composed of 310 target speakers enrolled in 53 poisoned models. Our attack achieves 100% attack success rate with a
&lt;/p&gt;</description></item><item><title>DNNShifter&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;&#35745;&#31639;DNN&#21098;&#26525;&#31995;&#32479;&#65292;&#36890;&#36807;&#24555;&#36895;&#25512;&#23548;&#20986;&#21512;&#36866;&#30340;&#27169;&#22411;&#21464;&#20307;&#26469;&#25552;&#20379;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#65292;&#36866;&#24212;&#31995;&#32479;&#21644;&#32593;&#32476;&#26465;&#20214;&#21464;&#21270;&#30340;&#24037;&#20316;&#36127;&#36733;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.06973</link><description>&lt;p&gt;
DNNShifter: &#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;&#35745;&#31639;DNN&#21098;&#26525;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DNNShifter: An Efficient DNN Pruning System for Edge Computing. (arXiv:2309.06973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06973
&lt;/p&gt;
&lt;p&gt;
DNNShifter&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;&#35745;&#31639;DNN&#21098;&#26525;&#31995;&#32479;&#65292;&#36890;&#36807;&#24555;&#36895;&#25512;&#23548;&#20986;&#21512;&#36866;&#30340;&#27169;&#22411;&#21464;&#20307;&#26469;&#25552;&#20379;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#65292;&#36866;&#24212;&#31995;&#32479;&#21644;&#32593;&#32476;&#26465;&#20214;&#21464;&#21270;&#30340;&#24037;&#20316;&#36127;&#36733;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#30784;&#12290;&#29983;&#20135;&#36136;&#37327;&#30340;DNN&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#25968;&#30334;&#19975;&#20010;DNN&#21442;&#25968;&#26469;&#23454;&#29616;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#65292;&#20294;&#36825;&#21344;&#29992;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#23545;&#20110;&#22312;&#32593;&#32476;&#30340;&#26497;&#31471;&#36793;&#32536;&#22788;&#24037;&#20316;&#30340;&#36164;&#28304;&#65288;&#22914;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#30340;&#31227;&#21160;&#21644;&#23884;&#20837;&#24335;&#35774;&#22791;&#65289;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#21098;&#26525;&#65292;&#20197;&#21019;&#24314;&#36731;&#37327;&#32423;&#12289;&#26356;&#36866;&#21512;&#36825;&#20123;&#35774;&#22791;&#30340;&#21464;&#20307;&#12290;&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#26080;&#27861;&#22312;&#19981;&#24341;&#20837;&#26174;&#33879;&#26102;&#38388;&#25104;&#26412;&#21644;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#19982;&#26410;&#21098;&#26525;&#27169;&#22411;&#30456;&#20284;&#30340;&#36136;&#37327;&#27169;&#22411;&#65292;&#25110;&#32773;&#21482;&#38480;&#20110;&#31163;&#32447;&#20351;&#29992;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#20445;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24555;&#36895;&#25512;&#23548;&#20986;&#36866;&#21512;&#30340;&#27169;&#22411;&#21464;&#20307;&#12290;&#27169;&#22411;&#21464;&#20307;&#21487;&#20197;&#22312;&#31995;&#32479;&#21644;&#32593;&#32476;&#26465;&#20214;&#21457;&#29983;&#21464;&#21270;&#20197;&#21305;&#37197;&#24037;&#20316;&#36127;&#36733;&#38656;&#27714;&#26102;&#24555;&#36895;&#20999;&#25442;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DNNShifter&#65292;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;DNN&#35757;&#32451;&#12289;&#31354;&#38388;&#21098;&#26525;&#21644;&#27169;&#22411;&#20999;&#25442;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) underpin many machine learning applications. Production quality DNN models achieve high inference accuracy by training millions of DNN parameters which has a significant resource footprint. This presents a challenge for resources operating at the extreme edge of the network, such as mobile and embedded devices that have limited computational and memory resources. To address this, models are pruned to create lightweight, more suitable variants for these devices. Existing pruning methods are unable to provide similar quality models compared to their unpruned counterparts without significant time costs and overheads or are limited to offline use cases. Our work rapidly derives suitable model variants while maintaining the accuracy of the original model. The model variants can be swapped quickly when system and network conditions change to match workload demand. This paper presents DNNShifter, an end-to-end DNN training, spatial pruning, and model switching syst
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#20013;&#24573;&#35270;&#30340;&#20851;&#38190;&#35201;&#32032; - &#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#23545;&#34917;&#25937;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26102;&#38388;&#25512;&#31227;&#21644;&#20010;&#20307;&#38388;&#31454;&#20105;&#30340;&#24773;&#20917;&#19979;&#65292;&#21021;&#22987;&#30340;&#34917;&#25937;&#24314;&#35758;&#21487;&#33021;&#21464;&#24471;&#19981;&#21487;&#38752;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;&#26102;&#38388;&#21464;&#21270;&#26469;&#30830;&#20445;&#34917;&#25937;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06969</link><description>&lt;p&gt;
&#35774;&#23450;&#27491;&#30830;&#30340;&#26399;&#26395;&#65306;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Setting the Right Expectations: Algorithmic Recourse Over Time. (arXiv:2309.06969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#20013;&#24573;&#35270;&#30340;&#20851;&#38190;&#35201;&#32032; - &#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#23545;&#34917;&#25937;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26102;&#38388;&#25512;&#31227;&#21644;&#20010;&#20307;&#38388;&#31454;&#20105;&#30340;&#24773;&#20917;&#19979;&#65292;&#21021;&#22987;&#30340;&#34917;&#25937;&#24314;&#35758;&#21487;&#33021;&#21464;&#24471;&#19981;&#21487;&#38752;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;&#26102;&#38388;&#21464;&#21270;&#26469;&#30830;&#20445;&#34917;&#25937;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#31995;&#32479;&#32463;&#24120;&#34987;&#29992;&#20110;&#21327;&#21161;&#39640;&#39118;&#38505;&#20915;&#31574;&#12290;&#37492;&#20110;&#27492;&#65292;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#65292;&#21363;&#20010;&#20307;&#24212;&#33021;&#22815;&#38024;&#23545;&#31639;&#27861;&#31995;&#32479;&#20135;&#29983;&#30340;&#19981;&#33391;&#32467;&#26524;&#37319;&#21462;&#34892;&#21160;&#65292;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#30340;&#22823;&#37096;&#20998;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#20026;&#21333;&#20010;&#20010;&#20307;&#25552;&#20379;&#34917;&#25937;&#65292;&#32780;&#24573;&#30053;&#20102;&#19968;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;&#24573;&#35270;&#36825;&#20123;&#23545;&#34917;&#25937;&#25514;&#26045;&#30340;&#24433;&#21709;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#30095;&#24573;&#65292;&#22240;&#20026;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#34917;&#25937;&#25514;&#26045;&#37117;&#21253;&#25324;&#20010;&#20307;&#39318;&#27425;&#20570;&#20986;&#19981;&#21033;&#23581;&#35797;&#65292;&#28982;&#21518;&#22312;&#20197;&#21518;&#30340;&#26576;&#20010;&#26102;&#38388;&#28857;&#25552;&#20379;&#19968;&#27425;&#25110;&#22810;&#27425;&#23581;&#35797;&#30340;&#26426;&#20250; - &#24403;&#26102;&#29615;&#22659;&#21487;&#33021;&#24050;&#32463;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#36825;&#21487;&#33021;&#20250;&#20135;&#29983;&#34394;&#20551;&#30340;&#26399;&#26395;&#65292;&#22240;&#20026;&#21021;&#22987;&#30340;&#34917;&#25937;&#24314;&#35758;&#38543;&#26102;&#38388;&#30340;&#25512;&#31227;&#21487;&#33021;&#21464;&#24471;&#19981;&#22826;&#21487;&#38752;&#65292;&#21407;&#22240;&#26159;&#27169;&#22411;&#28418;&#31227;&#21644;&#20010;&#20307;&#20043;&#38388;&#23545;&#26377;&#21033;&#32467;&#26524;&#30340;&#31454;&#20105;&#23548;&#33268;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26102;&#38388;&#21464;&#21270;&#30340;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic systems are often called upon to assist in high-stakes decision making. In light of this, algorithmic recourse, the principle wherein individuals should be able to take action against an undesirable outcome made by an algorithmic system, is receiving growing attention. The bulk of the literature on algorithmic recourse to-date focuses primarily on how to provide recourse to a single individual, overlooking a critical element: the effects of a continuously changing context. Disregarding these effects on recourse is a significant oversight, since, in almost all cases, recourse consists of an individual making a first, unfavorable attempt, and then being given an opportunity to make one or several attempts at a later date - when the context might have changed. This can create false expectations, as initial recourse recommendations may become less reliable over time due to model drift and competition for access to the favorable outcome between individuals.  In this work we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25968;&#25454;&#28165;&#29702;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25968;&#23383;&#30382;&#32932;&#31185;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#21487;&#20449;&#24230;&#12290;&#21327;&#35758;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#30382;&#32932;&#31185;&#21307;&#29983;&#30340;&#30830;&#35748;&#19979;&#65292;&#21024;&#38500;&#20102;&#26080;&#20851;&#26679;&#26412;&#21644;&#36817;&#20284;&#37325;&#22797;&#26679;&#26412;&#65292;&#20272;&#35745;&#20102;&#26631;&#31614;&#38169;&#35823;&#30340;&#30334;&#20998;&#27604;&#65292;&#24182;&#25552;&#20379;&#20102;&#20462;&#35746;&#21518;&#30340;&#25968;&#25454;&#38598;&#25991;&#20214;&#21015;&#34920;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#25968;&#23383;&#30382;&#32932;&#31185;&#20013;&#26356;&#21487;&#38752;&#30340;&#24615;&#33021;&#35780;&#20272;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2309.06961</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#38752;&#30340;&#30382;&#32932;&#31185;&#35780;&#20272;&#22522;&#20934;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Dermatology Evaluation Benchmarks. (arXiv:2309.06961v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25968;&#25454;&#28165;&#29702;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25968;&#23383;&#30382;&#32932;&#31185;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#21487;&#20449;&#24230;&#12290;&#21327;&#35758;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#30382;&#32932;&#31185;&#21307;&#29983;&#30340;&#30830;&#35748;&#19979;&#65292;&#21024;&#38500;&#20102;&#26080;&#20851;&#26679;&#26412;&#21644;&#36817;&#20284;&#37325;&#22797;&#26679;&#26412;&#65292;&#20272;&#35745;&#20102;&#26631;&#31614;&#38169;&#35823;&#30340;&#30334;&#20998;&#27604;&#65292;&#24182;&#25552;&#20379;&#20102;&#20462;&#35746;&#21518;&#30340;&#25968;&#25454;&#38598;&#25991;&#20214;&#21015;&#34920;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#25968;&#23383;&#30382;&#32932;&#31185;&#20013;&#26356;&#21487;&#38752;&#30340;&#24615;&#33021;&#35780;&#20272;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30382;&#32932;&#31185;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#26080;&#24847;&#38388;&#21253;&#21547;&#30340;&#19981;&#20934;&#30830;&#24615;&#38477;&#20302;&#20102;&#27169;&#22411;&#24615;&#33021;&#20272;&#35745;&#30340;&#20449;&#20219;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25968;&#25454;&#28165;&#29702;&#21327;&#35758;&#65292;&#29992;&#20110;&#35782;&#21035;&#20043;&#21069;&#30340;&#31574;&#21010;&#20013;&#36951;&#28431;&#30340;&#38382;&#39064;&#12290;&#35813;&#21327;&#35758;&#21033;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#28165;&#29702;&#31574;&#30053;&#65292;&#24182;&#22312;&#30452;&#35266;&#30340;&#20572;&#27490;&#20934;&#21017;&#30340;&#32456;&#27490;&#19979;&#36827;&#34892;&#30830;&#35748;&#36807;&#31243;&#12290;&#22522;&#20110;&#22810;&#20010;&#30382;&#32932;&#31185;&#21307;&#29983;&#30340;&#30830;&#35748;&#65292;&#25105;&#20204;&#21024;&#38500;&#20102;&#26080;&#20851;&#26679;&#26412;&#21644;&#36817;&#20284;&#37325;&#22797;&#26679;&#26412;&#65292;&#24182;&#20272;&#35745;&#20102;&#30001;&#22269;&#38469;&#30382;&#32932;&#25104;&#20687;&#21327;&#20316;&#32452;&#25512;&#24191;&#30340;&#20845;&#20010;&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#26631;&#31614;&#38169;&#35823;&#30340;&#30334;&#20998;&#27604;&#65292;&#20197;&#29992;&#20110;&#27169;&#22411;&#35780;&#20272;&#12290;&#38500;&#20102;&#26412;&#25991;&#65292;&#25105;&#20204;&#36824;&#20844;&#24067;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#20462;&#35746;&#25991;&#20214;&#21015;&#34920;&#65292;&#24212;&#35813;&#29992;&#20110;&#27169;&#22411;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25968;&#23383;&#30382;&#32932;&#31185;&#20013;&#26356;&#21487;&#38752;&#30340;&#24615;&#33021;&#35780;&#20272;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmark datasets for digital dermatology unwittingly contain inaccuracies that reduce trust in model performance estimates. We propose a resource-efficient data cleaning protocol to identify issues that escaped previous curation. The protocol leverages an existing algorithmic cleaning strategy and is followed by a confirmation process terminated by an intuitive stopping criterion. Based on confirmation by multiple dermatologists, we remove irrelevant samples and near duplicates and estimate the percentage of label errors in six dermatology image datasets for model evaluation promoted by the International Skin Imaging Collaboration. Along with this paper, we publish revised file lists for each dataset which should be used for model evaluation. Our work paves the way for more trustworthy performance assessment in digital dermatology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PhantomSound&#30340;&#40657;&#30418;&#12289;&#26597;&#35810;&#39640;&#25928;&#30340;&#38899;&#39057;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31186;&#32423;&#38899;&#32032;&#27880;&#20837;&#65292;&#33021;&#22815;&#23454;&#26102;&#25915;&#20987;&#19981;&#21516;&#35821;&#38899;&#21161;&#25163;&#30340;&#35821;&#38899;&#36716;&#25991;&#23383;API&#65292;&#24182;&#19988;&#25104;&#21151;&#32469;&#36807;&#20102;&#22810;&#31181;&#27963;&#20307;&#26816;&#27979;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.06960</link><description>&lt;p&gt;
PhantomSound: &#40657;&#30418;&#12289;&#26597;&#35810;&#39640;&#25928;&#30340;&#38899;&#39057;&#23545;&#25239;&#25915;&#20987;&#65306;&#36890;&#36807;&#20998;&#31186;&#32423;&#38899;&#32032;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
PhantomSound: Black-Box, Query-Efficient Audio Adversarial Attack via Split-Second Phoneme Injection. (arXiv:2309.06960v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PhantomSound&#30340;&#40657;&#30418;&#12289;&#26597;&#35810;&#39640;&#25928;&#30340;&#38899;&#39057;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31186;&#32423;&#38899;&#32032;&#27880;&#20837;&#65292;&#33021;&#22815;&#23454;&#26102;&#25915;&#20987;&#19981;&#21516;&#35821;&#38899;&#21161;&#25163;&#30340;&#35821;&#38899;&#36716;&#25991;&#23383;API&#65292;&#24182;&#19988;&#25104;&#21151;&#32469;&#36807;&#20102;&#22810;&#31181;&#27963;&#20307;&#26816;&#27979;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;PhantomSound&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#38899;&#21161;&#25163;&#30340;&#26597;&#35810;&#39640;&#25928;&#40657;&#30418;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#35821;&#38899;&#21161;&#25163;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#35201;&#20040;&#37319;&#29992;&#26367;&#25442;&#27169;&#22411;&#65292;&#35201;&#20040;&#21033;&#29992;&#20013;&#38388;&#27169;&#22411;&#36755;&#20986;&#26469;&#20272;&#35745;&#29992;&#20110;&#21046;&#20316;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#26597;&#35810;&#21644;&#20887;&#38271;&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;PhantomSound&#21033;&#29992;&#22522;&#20110;&#20915;&#31574;&#30340;&#25915;&#20987;&#26469;&#29983;&#25104;&#26377;&#25928;&#30340;&#23545;&#25239;&#24615;&#38899;&#39057;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26799;&#24230;&#20272;&#35745;&#26469;&#20943;&#23569;&#26597;&#35810;&#25968;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;4&#31181;&#19981;&#21516;&#30340;&#35821;&#38899;&#36716;&#25991;&#23383;API&#65292;&#22312;3&#31181;&#29616;&#23454;&#22330;&#26223;&#19979;&#36827;&#34892;&#25915;&#20987;&#65292;&#20197;&#23637;&#31034;&#23454;&#26102;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;PhantomSound&#22312;&#25915;&#20987;5&#31181;&#27969;&#34892;&#30340;&#21830;&#29992;&#35821;&#38899;&#21487;&#25511;&#35774;&#22791;&#26041;&#38754;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#36229;&#36807;95%&#30340;&#25104;&#21151;&#29575;&#32469;&#36807;3&#31181;&#27963;&#20307;&#26816;&#27979;&#26426;&#21046;&#12290;&#22522;&#20934;&#32467;&#26524;&#26174;&#31034;&#65292;PhantomSound&#21487;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose PhantomSound, a query-efficient black-box attack toward voice assistants. Existing black-box adversarial attacks on voice assistants either apply substitution models or leverage the intermediate model output to estimate the gradients for crafting adversarial audio samples. However, these attack approaches require a significant amount of queries with a lengthy training stage. PhantomSound leverages the decision-based attack to produce effective adversarial audios, and reduces the number of queries by optimizing the gradient estimation. In the experiments, we perform our attack against 4 different speech-to-text APIs under 3 real-world scenarios to demonstrate the real-time attack impact. The results show that PhantomSound is practical and robust in attacking 5 popular commercial voice controllable devices over the air, and is able to bypass 3 liveness detection mechanisms with &gt;95% success rate. The benchmark result shows that PhantomSound can generate adversar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNA&#30340;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#65292;&#37319;&#29992;&#38544;&#24335;&#31070;&#32463;&#22810;&#25551;&#36848;&#25216;&#26415;&#32534;&#30721;&#25968;&#25454;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;DNA&#23384;&#20648;&#20013;&#30340;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;MDC&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06956</link><description>&lt;p&gt;
&#22522;&#20110;DNA&#30340;&#25968;&#25454;&#23384;&#20648;&#30340;&#38544;&#24335;&#31070;&#32463;&#22810;&#25551;&#36848;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Multiple Description for DNA-based data storage. (arXiv:2309.06956v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNA&#30340;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#65292;&#37319;&#29992;&#38544;&#24335;&#31070;&#32463;&#22810;&#25551;&#36848;&#25216;&#26415;&#32534;&#30721;&#25968;&#25454;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;DNA&#23384;&#20648;&#20013;&#30340;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;MDC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNA&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#23384;&#20648;&#23494;&#24230;&#21644;&#38271;&#26399;&#31283;&#23450;&#24615;&#65292;&#22522;&#20110;&#20854;&#22266;&#26377;&#30340;&#29983;&#29289;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#31034;&#20986;&#20316;&#20026;&#25968;&#25454;&#23384;&#20648;&#35299;&#20915;&#26041;&#26696;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#31181;&#26032;&#22411;&#20171;&#36136;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22788;&#29702;&#23384;&#20648;&#21644;&#29983;&#29289;&#25805;&#20316;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#36825;&#20123;&#25361;&#25112;&#36827;&#19968;&#27493;&#21463;&#38480;&#20110;DNA&#24207;&#21015;&#30340;&#32467;&#26500;&#32422;&#26463;&#21644;&#25104;&#26412;&#32771;&#34385;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#39318;&#21019;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21387;&#32553;&#26041;&#26696;&#21644;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;DNA&#25968;&#25454;&#23384;&#20648;&#30340;&#23574;&#31471;&#22810;&#25551;&#36848;&#32534;&#30721;&#65288;MDC&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;MDC&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23558;&#25968;&#25454;&#32534;&#30721;&#25104;DNA&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#35774;&#35745;&#26469;&#26377;&#25928;&#22320;&#25269;&#25239;&#38169;&#35823;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26032;&#21387;&#32553;&#26041;&#26696;&#22312;DNA&#25968;&#25454;&#23384;&#20648;&#26041;&#38754;&#36229;&#36807;&#20102;&#32463;&#20856;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#20110;&#20381;&#36182;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20256;&#32479;MDC&#26041;&#27861;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNA exhibits remarkable potential as a data storage solution due to its impressive storage density and long-term stability, stemming from its inherent biomolecular structure. However, developing this novel medium comes with its own set of challenges, particularly in addressing errors arising from storage and biological manipulations. These challenges are further conditioned by the structural constraints of DNA sequences and cost considerations. In response to these limitations, we have pioneered a novel compression scheme and a cutting-edge Multiple Description Coding (MDC) technique utilizing neural networks for DNA data storage. Our MDC method introduces an innovative approach to encoding data into DNA, specifically designed to withstand errors effectively. Notably, our new compression scheme overperforms classic image compression methods for DNA-data storage. Furthermore, our approach exhibits superiority over conventional MDC methods reliant on auto-encoders. Its distinctive streng
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#20316;&#20026;&#26032;&#30340;&#32447;&#32034;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;DEFormer&#36824;&#21487;&#20197;&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06941</link><description>&lt;p&gt;
DEFormer: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#21644;&#26263;&#35270;&#35273;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision. (arXiv:2309.06941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#20316;&#20026;&#26032;&#30340;&#32447;&#32034;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;DEFormer&#36824;&#21487;&#20197;&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#22270;&#20687;&#30340;&#39068;&#33394;&#21644;&#32454;&#33410;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#39640;&#32423;&#35270;&#35273;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;RGB&#39046;&#22495;&#24456;&#38590;&#24674;&#22797;&#26263;&#21306;&#22495;&#30340;&#20002;&#22833;&#32454;&#33410;&#12290;&#26412;&#25991;&#23558;&#39057;&#29575;&#20316;&#20026;&#32593;&#32476;&#30340;&#26032;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#29992;&#20110;&#39057;&#29575;&#22686;&#24378;&#65292;&#21253;&#25324;DCT&#22788;&#29702;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#12290;CFE&#35745;&#31639;&#27599;&#20010;&#36890;&#36947;&#30340;&#26354;&#29575;&#20197;&#34920;&#31034;&#19981;&#21516;&#39057;&#29575;&#24102;&#30340;&#32454;&#33410;&#20016;&#23500;&#24230;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#39057;&#29575;&#29305;&#24449;&#21010;&#20998;&#20026;&#26356;&#20016;&#23500;&#32441;&#29702;&#30340;&#39057;&#29575;&#24102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;RGB&#39046;&#22495;&#21644;&#39057;&#29575;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;DEFormer&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;DEFormer&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of low-light image enhancement is to restore the color and details of the image and is of great significance for high-level visual tasks in autonomous driving. However, it is difficult to restore the lost details in the dark area by relying only on the RGB domain. In this paper we introduce frequency as a new clue into the network and propose a novel DCT-driven enhancement transformer (DEFormer). First, we propose a learnable frequency branch (LFB) for frequency enhancement contains DCT processing and curvature-based frequency enhancement (CFE). CFE calculates the curvature of each channel to represent the detail richness of different frequency bands, then we divides the frequency features, which focuses on frequency bands with richer textures. In addition, we propose a cross domain fusion (CDF) for reducing the differences between the RGB domain and the frequency domain. We also adopt DEFormer as a preprocessing in dark detection, DEFormer effectively improves the performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#23398;&#20064;&#21327;&#35758;&#30340;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#29615;&#22659;&#20132;&#20114;&#32972;&#26223;&#20013;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#38598;&#20013;&#21270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.06938</link><description>&lt;p&gt;
&#26080;&#38598;&#21512;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Collectionless Artificial Intelligence. (arXiv:2309.06938v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#23398;&#20064;&#21327;&#35758;&#30340;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#29615;&#22659;&#20132;&#20114;&#32972;&#26223;&#20013;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#38598;&#20013;&#21270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#20307;&#19978;&#65292;&#22788;&#29702;&#24222;&#22823;&#25968;&#25454;&#38598;&#34987;&#35748;&#20026;&#26159;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#22766;&#35266;&#32467;&#26524;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#38598;&#30340;&#38598;&#20013;&#21270;&#23384;&#22312;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#39118;&#38505;&#24847;&#35782;&#12290;&#26412;&#25991;&#25903;&#25345;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#21327;&#35758;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#30495;&#27491;&#20197;&#29615;&#22659;&#20132;&#20114;&#20026;&#20013;&#24515;&#30340;&#31867;&#20154;&#35748;&#30693;&#32972;&#26223;&#19979;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#23398;&#20064;&#21327;&#35758;&#38656;&#35201;&#36981;&#24490;&#26080;&#38598;&#21512;&#21407;&#21017;&#65292;&#21363;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#65292;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#30340;&#25968;&#25454;&#34987;&#29992;&#20110;&#26356;&#26032;&#24403;&#21069;&#29615;&#22659;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#19988;&#20195;&#29702;&#19981;&#33021;&#23545;&#26102;&#38388;&#27969;&#36827;&#34892;&#35760;&#24405;&#12290;&#22522;&#26412;&#19978;&#65292;&#19981;&#33021;&#23384;&#20648;&#26469;&#33258;&#20256;&#24863;&#22120;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
By and large, the professional handling of huge data collections is regarded as a fundamental ingredient of the progress of machine learning and of its spectacular results in related disciplines, with a growing agreement on risks connected to the centralization of such data collections. This paper sustains the position that the time has come for thinking of new learning protocols where machines conquer cognitive skills in a truly human-like context centered on environmental interactions. This comes with specific restrictions on the learning protocol according to the collectionless principle, which states that, at each time instant, data acquired from the environment is processed with the purpose of contributing to update the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Basically, there is neither permission to store the temporal information coming from the sensors, thus promoting the development of s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#29983;&#25104;&#30340;&#22238;&#39038;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#20266;&#26679;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.06917</link><description>&lt;p&gt;
&#20351;&#29992;&#29380;&#21033;&#20811;&#38647;&#29983;&#25104;&#22522;&#30784;&#30340;&#22238;&#39038;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning with Dirichlet Generative-based Rehearsal. (arXiv:2309.06917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#29983;&#25104;&#30340;&#22238;&#39038;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#20266;&#26679;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#25968;&#25454;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#65288;ToDs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#21644;&#32791;&#26102;&#38382;&#39064;&#32780;&#22256;&#25200;&#30528;&#22686;&#37327;&#23398;&#20064;&#12290;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#35797;&#22270;&#36890;&#36807;&#36991;&#20813;&#23494;&#38598;&#30340;&#39044;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#22522;&#20110;&#29983;&#25104;&#30340;&#22238;&#39038;CL&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#33021;&#20934;&#30830;&#21453;&#26144;&#24213;&#23618;&#20219;&#21153;&#29305;&#23450;&#20998;&#24067;&#30340;&#20266;&#26679;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29380;&#21033;&#20811;&#38647;&#36830;&#32493;&#23398;&#20064;&#65288;DCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29983;&#25104;&#30340;&#22238;&#39038;&#31574;&#30053;&#29992;&#20110;CL&#12290;&#19982;&#20256;&#32479;&#19978;&#22312;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#20013;&#20351;&#29992;&#30340;&#39640;&#26031;&#28508;&#21464;&#37327;&#19981;&#21516;&#65292;DCL&#21033;&#29992;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#26679;&#24615;&#26469;&#24314;&#27169;&#28508;&#21464;&#37327;&#20808;&#39564;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#39640;&#25928;&#22320;&#25429;&#25417;&#20808;&#21069;&#20219;&#21153;&#30340;&#21477;&#32423;&#29305;&#24449;&#65292;&#24182;&#26377;&#25928;&#22320;&#25351;&#23548;&#20266;&#26679;&#26412;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#36824;&#24341;&#20837;&#20102;Jensen-&#33487;&#24443;&#21033;&#25955;&#24230;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;DCL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in data-driven task-oriented dialogue systems (ToDs) struggle with incremental learning due to computational constraints and time-consuming issues. Continual Learning (CL) attempts to solve this by avoiding intensive pre-training, but it faces the problem of catastrophic forgetting (CF). While generative-based rehearsal CL methods have made significant strides, generating pseudo samples that accurately reflect the underlying task-specific distribution is still a challenge. In this paper, we present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal strategy for CL. Unlike the traditionally used Gaussian latent variable in the Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and versatility of the Dirichlet distribution to model the latent prior variable. This enables it to efficiently capture sentence-level features of previous tasks and effectively guide the generation of pseudo samples. In addition, we introduce Jensen-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06908</link><description>&lt;p&gt;
&#36208;&#21521;TopMost&#65306;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Towards the TopMost: A Topic Modeling System Toolkit. (arXiv:2309.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#34987;&#25552;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#22312;&#31070;&#32463;&#21464;&#20998;&#25512;&#26029;&#30340;&#25512;&#21160;&#19979;&#36817;&#26399;&#24471;&#21040;&#20102;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20027;&#39064;&#27169;&#22411;&#37319;&#29992;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24555;&#36895;&#21033;&#29992;&#21644;&#20844;&#24179;&#27604;&#36739;&#12290;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65288;TopMost&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;TopMost&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#30340;&#23436;&#25972;&#29983;&#21629;&#21608;&#26399;&#65292;&#33073;&#39062;&#32780;&#20986;&#12290;TopMost&#30340;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#21487;&#20197;&#24555;&#36895;&#21033;&#29992;&#65292;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#28789;&#27963;&#25193;&#23637;&#19981;&#21516;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25945;&#31243;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/bobxwu/topmost &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#31995;&#32479;&#32508;&#36848;&#20102;100&#22810;&#20010;OWL&#25512;&#29702;&#22120;/&#31995;&#32479;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;2023&#24180;&#26159;&#21542;&#20173;&#21487;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;95&#20010;&#29420;&#31435;OWL&#25512;&#29702;&#22120;&#21644;&#20351;&#29992;OWL&#25512;&#29702;&#22120;&#30340;&#31995;&#32479;&#30340;&#20840;&#38754;&#21015;&#34920;&#12290;</title><link>http://arxiv.org/abs/2309.06888</link><description>&lt;p&gt;
2023&#24180;&#20173;&#21487;&#20351;&#29992;&#30340;OWL&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
OWL Reasoners still useable in 2023. (arXiv:2309.06888v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#31995;&#32479;&#32508;&#36848;&#20102;100&#22810;&#20010;OWL&#25512;&#29702;&#22120;/&#31995;&#32479;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;2023&#24180;&#26159;&#21542;&#20173;&#21487;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;95&#20010;&#29420;&#31435;OWL&#25512;&#29702;&#22120;&#21644;&#20351;&#29992;OWL&#25512;&#29702;&#22120;&#30340;&#31995;&#32479;&#30340;&#20840;&#38754;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#21644;&#36719;&#20214;&#32508;&#36848;&#20013;&#65292;&#20998;&#26512;&#20102;&#36229;&#36807;100&#20010;OWL&#25512;&#29702;&#22120;/&#31995;&#32479;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#22312;2023&#24180;&#20173;&#28982;&#21487;&#29992;&#12290;&#36825;&#22312;&#36825;&#20010;&#33539;&#22260;&#20869;&#26159;&#39318;&#27425;&#23436;&#25104;&#30340;&#12290;OWL&#25512;&#29702;&#22120;&#22312;&#30693;&#35782;&#32452;&#32455;&#21644;&#31649;&#29702;&#20013;&#20173;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#26368;&#36817;&#30340;&#32508;&#21512;&#35843;&#26597;/&#30740;&#31350;&#24050;&#26377;&#36229;&#36807;8&#24180;&#30340;&#26102;&#38388;&#12290;&#35813;&#30740;&#31350;&#30340;&#32467;&#26524;&#26159;&#19968;&#20010;&#21253;&#21547;95&#20010;&#29420;&#31435;&#30340;OWL&#25512;&#29702;&#22120;&#21644;&#20351;&#29992;OWL&#25512;&#29702;&#22120;&#30340;&#31995;&#32479;&#30340;&#20840;&#38754;&#21015;&#34920;&#12290;&#38024;&#23545;&#27599;&#20010;&#39033;&#30446;&#65292;&#25910;&#38598;&#20102;&#39033;&#30446;&#39029;&#38754;&#12289;&#28304;&#20195;&#30721;&#24211;&#21644;&#30456;&#20851;&#25991;&#26723;&#30340;&#20449;&#24687;&#12290;&#21407;&#22987;&#30740;&#31350;&#25968;&#25454;&#22312;Github&#20179;&#24211;&#20013;&#25552;&#20379;&#65292;&#20379;&#20219;&#20309;&#20154;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a systematic literature and software review over 100 OWL reasoners/systems were analyzed to see if they would still be usable in 2023. This has never been done in this capacity. OWL reasoners still play an important role in knowledge organisation and management, but the last comprehensive surveys/studies are more than 8 years old. The result of this work is a comprehensive list of 95 standalone OWL reasoners and systems using an OWL reasoner. For each item, information on project pages, source code repositories and related documentation was gathered. The raw research data is provided in a Github repository for anyone to use.
&lt;/p&gt;</description></item><item><title>Gpachov&#22242;&#38431;&#22312;CLEF-2023 CheckThat&#65281;&#23454;&#39564;&#23460;&#20219;&#21153;2&#20013;&#26500;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#30340;&#22810;&#26041;&#27861;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24494;&#35843;&#21477;&#23376;&#23884;&#20837;&#32534;&#30721;&#27169;&#22411;&#12289;&#26679;&#26412;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#36716;&#25442;&#22120;&#31561;&#26041;&#27861;&#32467;&#21512;&#24471;&#21040;&#20102;0.77&#30340;&#23439;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#33521;&#35821;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#12290;</title><link>http://arxiv.org/abs/2309.06844</link><description>&lt;p&gt;
Gpachov&#22312;CheckThat&#65281;2023&#20013;&#65306;&#19968;&#31181;&#22810;&#26679;&#30340;&#22810;&#26041;&#27861;&#38598;&#25104;&#29992;&#20110;&#26032;&#38395;&#25991;&#31456;&#20027;&#35266;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Gpachov at CheckThat! 2023: A Diverse Multi-Approach Ensemble for Subjectivity Detection in News Articles. (arXiv:2309.06844v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06844
&lt;/p&gt;
&lt;p&gt;
Gpachov&#22242;&#38431;&#22312;CLEF-2023 CheckThat&#65281;&#23454;&#39564;&#23460;&#20219;&#21153;2&#20013;&#26500;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#30340;&#22810;&#26041;&#27861;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24494;&#35843;&#21477;&#23376;&#23884;&#20837;&#32534;&#30721;&#27169;&#22411;&#12289;&#26679;&#26412;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#36716;&#25442;&#22120;&#31561;&#26041;&#27861;&#32467;&#21512;&#24471;&#21040;&#20102;0.77&#30340;&#23439;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#33521;&#35821;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#30340;&#24191;&#27867;&#20351;&#29992;&#23548;&#33268;&#20102;&#20114;&#32852;&#32593;&#19978;&#30340;&#20027;&#35266;&#12289;&#35823;&#23548;&#29978;&#33267;&#34394;&#20551;&#20449;&#24687;&#30340;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#20027;&#35266;&#24615;&#26816;&#27979;&#22312;&#30830;&#20445;&#20449;&#24687;&#23458;&#35266;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Gpachov&#22242;&#38431;&#38024;&#23545;CLEF-2023 CheckThat&#65281;&#23454;&#39564;&#23460;&#20219;&#21153;2&#30340;&#20027;&#35266;&#24615;&#26816;&#27979;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25991;&#31456;&#25506;&#32034;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#31532;&#19968;&#20010;&#26041;&#21521;&#22522;&#20110;&#24494;&#35843;&#21477;&#23376;&#23884;&#20837;&#32534;&#30721;&#27169;&#22411;&#21644;&#38477;&#32500;&#12290;&#31532;&#20108;&#20010;&#26041;&#21521;&#25506;&#32034;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#12290;&#31532;&#19977;&#20010;&#26041;&#21521;&#35780;&#20272;&#20102;&#22312;&#32463;&#36807;&#20462;&#25913;&#30340;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#22810;&#35821;&#35328;&#36716;&#25442;&#22120;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#19977;&#31181;&#26041;&#27861;&#20197;&#31616;&#21333;&#30340;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#65292;&#32467;&#26524;&#22312;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;0.77&#30340;&#23439;F1&#65292;&#24182;&#22312;&#33521;&#35821;&#23376;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31532;&#20108;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wide-spread use of social networks has given rise to subjective, misleading, and even false information on the Internet. Thus, subjectivity detection can play an important role in ensuring the objectiveness and the quality of a piece of information. This paper presents the solution built by the Gpachov team for the CLEF-2023 CheckThat! lab Task~2 on subjectivity detection. Three different research directions are explored. The first one is based on fine-tuning a sentence embeddings encoder model and dimensionality reduction. The second one explores a sample-efficient few-shot learning model. The third one evaluates fine-tuning a multilingual transformer on an altered dataset, using data from multiple languages. Finally, the three approaches are combined in a simple majority voting ensemble, resulting in 0.77 macro F1 on the test set and achieving 2nd place on the English subtask.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23616;&#37096;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#22522;&#20110;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#21644;&#20108;&#27425;Lyapunov&#20989;&#25968;&#65292;&#24182;&#32467;&#21512;&#20102;&#21407;&#28857;&#38468;&#36817;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#32447;&#24615;&#32467;&#26500;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#20026;&#20934;&#30830;&#21644;&#26377;&#25928;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#23616;&#37096;&#25351;&#25968;&#31283;&#23450;&#24615;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#31946;Lyapunov&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06841</link><description>&lt;p&gt;
&#20851;&#20110;T-S&#27169;&#31946;&#31995;&#32479;&#22312;&#21407;&#28857;&#38468;&#36817;&#30340;&#23616;&#37096;&#20108;&#27425;&#31283;&#23450;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Local Quadratic Stability of T-S Fuzzy Systems in the Vicinity of the Origin. (arXiv:2309.06841v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23616;&#37096;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#22522;&#20110;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#21644;&#20108;&#27425;Lyapunov&#20989;&#25968;&#65292;&#24182;&#32467;&#21512;&#20102;&#21407;&#28857;&#38468;&#36817;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#32447;&#24615;&#32467;&#26500;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#20026;&#20934;&#30830;&#21644;&#26377;&#25928;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#23616;&#37096;&#25351;&#25968;&#31283;&#23450;&#24615;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#31946;Lyapunov&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24341;&#20837;&#26032;&#30340;&#23616;&#37096;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;Takagi-Sugeno&#65288;T-S&#65289;&#27169;&#31946;&#31995;&#32479;&#12290;&#36825;&#20123;&#31283;&#23450;&#24615;&#26465;&#20214;&#22522;&#20110;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#65288;LMIs&#65289;&#21644;&#20108;&#27425;Lyapunov&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#32467;&#21512;&#20102;&#21407;&#28857;&#22788;&#30340;&#38582;&#23646;&#20989;&#25968;&#20449;&#24687;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;&#21407;&#28857;&#38468;&#36817;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#32447;&#24615;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#19982;&#25991;&#29486;&#20013;&#20351;&#29992;&#27169;&#31946;Lyapunov&#20989;&#25968;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25552;&#20986;&#30340;&#26465;&#20214;&#35777;&#26126;&#20102;&#26356;&#23569;&#30340;&#20445;&#23432;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;T-S&#27169;&#31946;&#31995;&#32479;&#23616;&#37096;&#25351;&#25968;&#31283;&#23450;&#24615;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#27169;&#31946;Lyapunov&#26041;&#27861;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#28436;&#31034;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#31034;&#20363;&#65292;&#38416;&#26126;&#20102;&#26680;&#24515;&#27010;&#24565;&#65292;&#24182;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main goal of this paper is to introduce new local stability conditions for continuous-time Takagi-Sugeno (T-S) fuzzy systems. These stability conditions are based on linear matrix inequalities (LMIs) in combination with quadratic Lyapunov functions. Moreover, they integrate information on the membership functions at the origin and effectively leverage the linear structure of the underlying nonlinear system in the vicinity of the origin. As a result, the proposed conditions are proved to be less conservative compared to existing methods using fuzzy Lyapunov functions in the literature. Moreover, we establish that the proposed methods offer necessary and sufficient conditions for the local exponential stability of T-S fuzzy systems. The paper also includes discussions on the inherent limitations associated with fuzzy Lyapunov approaches. To demonstrate the theoretical results, we provide comprehensive examples that elucidate the core concepts and validate the efficacy of the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAMUS&#65292;&#19968;&#20010;&#19987;&#20026;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;CNN&#20998;&#25903;&#21644;&#36866;&#37197;&#22120;&#26469;&#25913;&#21892;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06824</link><description>&lt;p&gt;
SAMUS&#65306;&#20026;&#20020;&#24202;&#21451;&#22909;&#21644;&#27867;&#21270;&#24615;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#35843;&#25972;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SAMUS: Adapting Segment Anything Model for Clinically-Friendly and Generalizable Ultrasound Image Segmentation. (arXiv:2309.06824v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAMUS&#65292;&#19968;&#20010;&#19987;&#20026;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;CNN&#20998;&#25903;&#21644;&#36866;&#37197;&#22120;&#26469;&#25913;&#21892;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#21331;&#36234;&#30340;&#36890;&#29992;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;SAM&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#26102;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#20302;&#23545;&#27604;&#24230;&#12289;&#27169;&#31946;&#36793;&#30028;&#12289;&#22797;&#26434;&#24418;&#29366;&#21644;&#23567;&#23610;&#23544;&#23545;&#35937;&#30340;&#22270;&#20687;&#26102;&#65292;SAM&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#21644;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;SAMUS&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#19982;&#20197;&#21069;&#22522;&#20110;SAM&#30340;&#36890;&#29992;&#27169;&#22411;&#19981;&#21516;&#65292;SAMUS&#36861;&#27714;&#30340;&#19981;&#20165;&#26159;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#26377;&#26356;&#20302;&#30340;&#37096;&#32626;&#25104;&#26412;&#65292;&#20351;&#20854;&#26356;&#36866;&#21512;&#20020;&#24202;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;SAM&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#24182;&#34892;CNN&#20998;&#25903;&#65292;&#36890;&#36807;&#36328;&#20998;&#25903;&#27880;&#24847;&#21147;&#23558;&#23616;&#37096;&#29305;&#24449;&#27880;&#20837;ViT&#32534;&#30721;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#21518;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20301;&#32622;&#36866;&#37197;&#22120;&#21644;&#19968;&#20010;&#29305;&#24449;&#36866;&#37197;&#22120;&#26469;&#35843;&#25972;SAM&#30340;&#36755;
&lt;/p&gt;
&lt;p&gt;
Segment anything model (SAM), an eminent universal image segmentation model, has recently gathered considerable attention within the domain of medical image segmentation. Despite the remarkable performance of SAM on natural images, it grapples with significant performance degradation and limited generalization when confronted with medical images, particularly with those involving objects of low contrast, faint boundaries, intricate shapes, and diminutive sizes. In this paper, we propose SAMUS, a universal model tailored for ultrasound image segmentation. In contrast to previous SAM-based universal models, SAMUS pursues not only better generalization but also lower deployment cost, rendering it more suitable for clinical applications. Specifically, based on SAM, a parallel CNN branch is introduced to inject local features into the ViT encoder through cross-branch attention for better medical image segmentation. Then, a position adapter and a feature adapter are developed to adapt SAM fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20998;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#12290;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#39640;&#25928;&#39044;&#27979;&#30001;&#22810;&#20110;&#20004;&#20010;&#20851;&#31995;&#21644;&#26410;&#25351;&#23450;&#23454;&#20307;&#32452;&#25104;&#30340;&#21477;&#23376;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#22810;&#20010;&#21477;&#23376;&#30340;&#35821;&#22659;&#20013;&#35782;&#21035;&#35821;&#20041;&#20851;&#31995;&#12290;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20108;&#20803;&#20851;&#31995;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#38543;&#30528;&#20851;&#31995;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#20934;&#30830;&#29575;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06814</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Contextual Relation Extraction based on Deep Learning Models. (arXiv:2309.06814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20998;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#12290;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#39640;&#25928;&#39044;&#27979;&#30001;&#22810;&#20110;&#20004;&#20010;&#20851;&#31995;&#21644;&#26410;&#25351;&#23450;&#23454;&#20307;&#32452;&#25104;&#30340;&#21477;&#23376;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#22810;&#20010;&#21477;&#23376;&#30340;&#35821;&#22659;&#20013;&#35782;&#21035;&#35821;&#20041;&#20851;&#31995;&#12290;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20108;&#20803;&#20851;&#31995;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#38543;&#30528;&#20851;&#31995;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#20934;&#30830;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#20027;&#35201;&#29992;&#20110;&#20511;&#21161;&#26412;&#20307;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#22312;&#35821;&#20041;&#25628;&#32034;&#12289;&#26597;&#35810;&#22238;&#31572;&#21644;&#25991;&#26412;&#34164;&#28085;&#31561;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#20851;&#31995;&#25552;&#21462;&#35782;&#21035;&#21407;&#22987;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#21450;&#20854;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#29983;&#29289;&#21307;&#33647;&#34892;&#19994;&#20013;&#65292;&#39640;&#25928;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#31995;&#32479;&#23545;&#20110;&#21019;&#24314;&#39046;&#22495;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26080;&#27861;&#39640;&#25928;&#22320;&#20174;&#30001;&#22810;&#20110;&#20004;&#20010;&#20851;&#31995;&#21644;&#26410;&#25351;&#23450;&#23454;&#20307;&#32452;&#25104;&#30340;&#21477;&#23376;&#20013;&#39044;&#27979;&#22797;&#26434;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#22810;&#20010;&#21477;&#23376;&#30340;&#35821;&#22659;&#20013;&#35782;&#21035;&#20986;&#36866;&#24403;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#23613;&#31649;&#20851;&#31995;&#25552;&#21462;&#20013;&#20351;&#29992;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#21482;&#23545;&#20108;&#20803;&#20851;&#31995;&#65288;&#21363;&#22312;&#21477;&#23376;&#20013;&#23436;&#20840;&#21457;&#29983;&#22312;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#20250;&#38543;&#30528;&#20851;&#31995;&#30340;&#25968;&#37327;&#22686;&#21152;&#32780;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual Relation Extraction (CRE) is mainly used for constructing a knowledge graph with a help of ontology. It performs various tasks such as semantic search, query answering, and textual entailment. Relation extraction identifies the entities from raw texts and the relations among them. An efficient and accurate CRE system is essential for creating domain knowledge in the biomedical industry. Existing Machine Learning and Natural Language Processing (NLP) techniques are not suitable to predict complex relations from sentences that consist of more than two relations and unspecified entities efficiently. In this work, deep learning techniques have been used to identify the appropriate semantic relation based on the context from multiple sentences. Even though various machine learning models have been used for relation extraction, they provide better results only for binary relations, i.e., relations occurred exactly between the two entities in a sentence. Machine learning models are
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;SE(3)&#31561;&#21464;&#24615;&#26469;&#36827;&#34892;3D&#20960;&#20309;&#24418;&#29366;&#32452;&#35013;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#37096;&#20214;&#20851;&#32852;&#30340;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#32452;&#35013;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06810</link><description>&lt;p&gt;
&#21033;&#29992;SE(3)&#31561;&#21464;&#24615;&#23398;&#20064;3D&#20960;&#20309;&#24418;&#29366;&#32452;&#35013;
&lt;/p&gt;
&lt;p&gt;
Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly. (arXiv:2309.06810v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;SE(3)&#31561;&#21464;&#24615;&#26469;&#36827;&#34892;3D&#20960;&#20309;&#24418;&#29366;&#32452;&#35013;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#37096;&#20214;&#20851;&#32852;&#30340;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#32452;&#35013;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#32452;&#35013;&#26088;&#22312;&#23558;&#37096;&#20214;&#65288;&#25110;&#30862;&#29255;&#65289;&#37325;&#26032;&#32452;&#35013;&#25104;&#23436;&#25972;&#30340;&#29289;&#20307;&#65292;&#36825;&#26159;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24120;&#35265;&#20219;&#21153;&#12290;&#19982;&#35821;&#20041;&#37096;&#20214;&#32452;&#35013;&#65288;&#20363;&#22914;&#65292;&#23558;&#26885;&#23376;&#30340;&#35821;&#20041;&#37096;&#20214;&#22914;&#33151;&#32452;&#35013;&#25104;&#25972;&#20010;&#26885;&#23376;&#65289;&#19981;&#21516;&#65292;&#20960;&#20309;&#37096;&#20214;&#32452;&#35013;&#65288;&#20363;&#22914;&#65292;&#23558;&#30871;&#30862;&#29255;&#32452;&#35013;&#25104;&#23436;&#25972;&#30340;&#30871;&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#19968;&#39033;&#26032;&#20852;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#19981;&#20851;&#27880;&#37096;&#20214;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#26159;&#20851;&#27880;&#37096;&#20214;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#30001;&#20110;&#26029;&#35010;&#37096;&#20214;&#30340;&#20960;&#20309;&#21644;&#23039;&#24577;&#31354;&#38388;&#37117;&#24322;&#24120;&#24222;&#22823;&#65292;&#23545;&#37096;&#20214;&#34920;&#31034;&#36827;&#34892;&#24418;&#29366;&#23039;&#24577;&#35299;&#32544;&#26159;&#26377;&#30410;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#26469;&#36827;&#34892;&#24418;&#29366;&#23039;&#24577;&#35299;&#32544;&#12290;&#27492;&#22806;&#65292;&#20197;&#24448;&#30340;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#24037;&#20316;&#21482;&#32771;&#34385;&#21333;&#20010;&#23545;&#35937;&#30340;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#34920;&#31034;&#65292;&#32780;&#25105;&#20204;&#26356;&#36827;&#19968;&#27493;&#25552;&#20986;&#21033;&#29992;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#26469;&#32771;&#34385;&#22810;&#37096;&#20214;&#20851;&#32852;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Shape assembly aims to reassemble parts (or fragments) into a complete object, which is a common task in our daily life. Different from the semantic part assembly (e.g., assembling a chair's semantic parts like legs into a whole chair), geometric part assembly (e.g., assembling bowl fragments into a complete bowl) is an emerging task in computer vision and robotics. Instead of semantic information, this task focuses on geometric information of parts. As the both geometric and pose space of fractured parts are exceptionally large, shape pose disentanglement of part representations is beneficial to geometric shape assembly. In our paper, we propose to leverage SE(3) equivariance for such shape pose disentanglement. Moreover, while previous works in vision and robotics only consider SE(3) equivariance for the representations of single objects, we move a step forward and propose leveraging SE(3) equivariance for representations considering multi-part correlations, which further boosts the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#25439;&#22833;&#26469;&#25913;&#21892;&#22810;&#20013;&#24515;&#24687;&#20998;&#21106;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36991;&#20813;&#22240;&#22806;&#35266;&#12289;&#20202;&#22120;&#32423;&#21035;&#21644;&#37319;&#38598;&#36136;&#37327;&#30340;&#21464;&#24322;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#28508;&#22312;&#30340;&#25913;&#36827;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06807</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#25439;&#22833;&#29992;&#20110;&#25913;&#36827;&#24687;&#35782;&#21035;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Bayesian uncertainty-weighted loss for improved generalisability on polyp segmentation task. (arXiv:2309.06807v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#25439;&#22833;&#26469;&#25913;&#21892;&#22810;&#20013;&#24515;&#24687;&#20998;&#21106;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36991;&#20813;&#22240;&#22806;&#35266;&#12289;&#20202;&#22120;&#32423;&#21035;&#21644;&#37319;&#38598;&#36136;&#37327;&#30340;&#21464;&#24322;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#28508;&#22312;&#30340;&#25913;&#36827;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#24687;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#22312;&#22810;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#12290;&#30001;&#20110;&#19981;&#21516;&#20013;&#24515;&#24687;&#30340;&#22806;&#35266;&#21464;&#24322;&#12289;&#20869;&#31397;&#38236;&#20202;&#22120;&#31561;&#32423;&#30340;&#24046;&#24322;&#21644;&#37319;&#38598;&#36136;&#37327;&#30340;&#19981;&#21516;&#65292;&#23548;&#33268;&#36825;&#20123;&#26041;&#27861;&#22312;&#20869;&#37096;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#22806;&#37096;&#27979;&#35797;&#25110;&#20195;&#34920;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#23545;&#20020;&#24202;&#24212;&#29992;&#20855;&#26377;&#20005;&#37325;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#23545;&#20020;&#24202;&#24212;&#29992;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#38544;&#24335;&#20559;&#24046;&#20943;&#36731;&#26041;&#27861;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#40723;&#21169;&#27169;&#22411;&#19987;&#27880;&#20110;&#20195;&#34920;&#24615;&#26679;&#26412;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#20013;&#24515;&#21644;&#22270;&#20687;&#27169;&#24335;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20013;&#24515;&#24687;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;PolypGen&#65289;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While several previous studies have devised methods for segmentation of polyps, most of these methods are not rigorously assessed on multi-center datasets. Variability due to appearance of polyps from one center to another, difference in endoscopic instrument grades, and acquisition quality result in methods with good performance on in-distribution test data, and poor performance on out-of-distribution or underrepresented samples. Unfair models have serious implications and pose a critical challenge to clinical applications. We adapt an implicit bias mitigation method which leverages Bayesian epistemic uncertainties during training to encourage the model to focus on underrepresented sample regions. We demonstrate the potential of this approach to improve generalisability without sacrificing state-of-the-art performance on a challenging multi-center polyp segmentation dataset (PolypGen) with different centers and image modalities.
&lt;/p&gt;</description></item><item><title>FedDIP&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21160;&#24577;&#27169;&#22411;&#20462;&#21098;&#21644;&#22686;&#37327;&#27491;&#21017;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#20132;&#25442;&#21644;&#23454;&#29616;&#26497;&#31471;&#31232;&#30095;&#27169;&#22411;&#26469;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06805</link><description>&lt;p&gt;
FedDIP: &#37319;&#29992;&#26497;&#31471;&#21160;&#24577;&#20462;&#21098;&#21644;&#22686;&#37327;&#27491;&#21017;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization. (arXiv:2309.06805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06805
&lt;/p&gt;
&lt;p&gt;
FedDIP&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21160;&#24577;&#27169;&#22411;&#20462;&#21098;&#21644;&#22686;&#37327;&#27491;&#21017;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#20132;&#25442;&#21644;&#23454;&#29616;&#26497;&#31471;&#31232;&#30095;&#27169;&#22411;&#26469;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;DNN&#20855;&#26377;&#26497;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#22240;&#27492;&#22312;&#20998;&#24067;&#24335;&#33410;&#28857;&#20043;&#38388;&#20132;&#25442;&#36825;&#20123;&#21442;&#25968;&#21644;&#31649;&#29702;&#20869;&#23384;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;DNN&#21387;&#32553;&#26041;&#27861;&#65288;&#20363;&#22914;&#31232;&#30095;&#21270;&#12289;&#20462;&#21098;&#65289;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#24182;&#26410;&#20840;&#38754;&#32771;&#34385;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#27700;&#24179;&#30340;&#21516;&#26102;&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#21442;&#25968;&#20132;&#25442;&#30340;&#20943;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65288;&#31216;&#20026;FedDIP&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#65288;i&#65289;&#21160;&#24577;&#27169;&#22411;&#20462;&#21098;&#21644;&#35823;&#24046;&#21453;&#39304;&#26469;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#20132;&#25442;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22686;&#37327;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#23454;&#29616;&#8220;&#26497;&#31471;&#8221;&#31232;&#30095;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;FedDIP&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#21644;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has been successfully adopted for distributed training and inference of large-scale Deep Neural Networks (DNNs). However, DNNs are characterized by an extremely large number of parameters, thus, yielding significant challenges in exchanging these parameters among distributed nodes and managing the memory. Although recent DNN compression methods (e.g., sparsification, pruning) tackle such challenges, they do not holistically consider an adaptively controlled reduction of parameter exchange while maintaining high accuracy levels. We, therefore, contribute with a novel FL framework (coined FedDIP), which combines (i) dynamic model pruning with error feedback to eliminate redundant information exchange, which contributes to significant performance improvement, with (ii) incremental regularization that can achieve \textit{extreme} sparsity of models. We provide convergence analysis of FedDIP and report on a comprehensive performance and comparative assessment against
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#31614;&#21517;&#32593;&#32476;&#20013;&#30340;&#38450;&#24481;&#32852;&#30431;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#30340;&#32676;&#20307;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21916;&#22909;&#21644;&#21388;&#24694;&#20851;&#31995;&#65292;&#22312;&#32852;&#30431;&#24418;&#25104;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.06801</link><description>&lt;p&gt;
&#31614;&#21517;&#32593;&#32476;&#20013;&#30340;&#38450;&#24481;&#32852;&#30431;
&lt;/p&gt;
&lt;p&gt;
Defensive Alliances in Signed Networks. (arXiv:2309.06801v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#31614;&#21517;&#32593;&#32476;&#20013;&#30340;&#38450;&#24481;&#32852;&#30431;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#30340;&#32676;&#20307;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21916;&#22909;&#21644;&#21388;&#24694;&#20851;&#31995;&#65292;&#22312;&#32852;&#30431;&#24418;&#25104;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20998;&#26512;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#12290;&#26576;&#20123;&#30740;&#31350;&#26041;&#21521;&#28041;&#21450;&#23547;&#25214;&#33021;&#22815;&#20849;&#21516;&#21512;&#20316;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#30340;&#26234;&#33021;&#20307;&#32676;&#20307;&#12290;&#20026;&#27492;&#65292;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#19981;&#21516;&#27010;&#24565;&#30340;&#22270;&#19982;&#32593;&#32476;&#20013;&#30340;&#38598;&#32676;&#25110;&#31038;&#21306;&#12290;&#20854;&#20013;&#65292;&#38450;&#24481;&#32852;&#30431;&#26159;&#19968;&#31181;&#37327;&#21270;&#30340;&#32676;&#20307;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#32852;&#30431;&#30340;&#25152;&#26377;&#30740;&#31350;&#37117;&#24573;&#35270;&#20102;&#19968;&#20010;&#22312;&#24418;&#25104;&#32852;&#30431;&#20013;&#38750;&#24120;&#30452;&#35266;&#30340;&#26041;&#38754;&#65292;&#21363;&#20551;&#35774;&#26234;&#33021;&#20307;&#22312;&#24577;&#24230;&#26041;&#38754;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#26377;&#39044;&#35774;&#65292;&#20182;&#20204;&#21916;&#27426;&#21644;&#20182;&#20204;&#21916;&#27426;&#30340;&#26234;&#33021;&#20307;&#19968;&#36215;&#22312;&#26576;&#20010;&#32676;&#20307;&#65288;&#32852;&#30431;&#65289;&#20013;&#65292;&#22240;&#27492;&#24895;&#24847;&#30456;&#20114;&#24110;&#21161;&#23454;&#29616;&#20849;&#21516;&#30340;&#30446;&#26631;&#65292;&#21487;&#33021;&#20250;&#23545;&#19981;&#21916;&#27426;&#30340;&#32676;&#20307;&#22806;&#30340;&#26234;&#33021;&#20307;&#36827;&#34892;&#23545;&#25239;&#12290;&#31614;&#21517;&#32593;&#32476;&#22312;&#24515;&#29702;&#23398;&#25991;&#29486;&#20013;&#34987;&#24341;&#20837;&#20197;&#27169;&#25311;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21916;&#27426;&#21644;&#21388;&#24694;&#20851;&#31995;&#65292;&#36825;&#25193;&#23637;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of (social) networks and multi-agent systems is a central theme in Artificial Intelligence. Some line of research deals with finding groups of agents that could work together to achieve a certain goal. To this end, different notions of so-called clusters or communities have been introduced in the literature of graphs and networks. Among these, defensive alliance is a kind of quantitative group structure. However, all studies on the alliance so for have ignored one aspect that is central to the formation of alliances on a very intuitive level, assuming that the agents are preconditioned concerning their attitude towards other agents: they prefer to be in some group (alliance) together with the agents they like, so that they are happy to help each other towards their common aim, possibly then working against the agents outside of their group that they dislike. Signed networks were introduced in the psychology literature to model liking and disliking between agents, generaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06800</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#35768;&#22810;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#39044;&#27979;&#20301;&#32622;&#26377;&#23436;&#25972;&#25110;&#33267;&#23569;&#37096;&#20998;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#26080;&#21382;&#21490;&#35760;&#24405;&#30340;&#20301;&#32622;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#21644;&#23433;&#35013;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21487;&#33021;&#21463;&#38480;&#65292;&#36825;&#20351;&#24471;&#22823;&#22810;&#25968;&#24403;&#21069;&#27169;&#22411;&#19981;&#36866;&#29992;&#12290;&#34429;&#28982;&#23569;&#25968;&#25991;&#29486;&#23581;&#35797;&#22312;&#32570;&#22833;&#20301;&#32622;&#19978;&#25554;&#34917;&#20132;&#36890;&#29366;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19982;&#20256;&#24863;&#22120;&#20301;&#32622;&#21516;&#26102;&#35266;&#27979;&#30340;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#32570;&#20047;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#24471;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#20219;&#21153;&#25110;&#28041;&#21450;&#20915;&#31574;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21463;&#21040;&#20808;&#21069;&#30340;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
&lt;/p&gt;</description></item><item><title>&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#21019;&#26032;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.06799</link><description>&lt;p&gt;
&#24403;&#22320;&#29699;&#31185;&#23398;&#36935;&#35265;&#22522;&#30784;&#27169;&#22411;&#65306;&#36208;&#21521;&#36890;&#29992;&#22320;&#29699;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
When Geoscience Meets Foundation Models: Towards General Geoscience Artificial Intelligence System. (arXiv:2309.06799v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06799
&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#21019;&#26032;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20195;&#34920;&#20102;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#23427;&#20204;&#20174;&#30334;&#19975;&#20159;&#23383;&#33410;&#30340;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25581;&#31034;&#20986;&#27934;&#23519;&#21147;&#12290;&#28789;&#27963;&#30340;&#20219;&#21153;&#35268;&#33539;&#12289;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#20197;&#21450;&#22810;&#27169;&#24577;&#30340;&#30693;&#35782;&#34920;&#31034;&#20351;&#24471;&#32508;&#21512;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22320;&#29699;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#20801;&#35768;&#35299;&#20915;&#19982;&#22320;&#29699;&#31995;&#32479;&#30456;&#20114;&#20316;&#29992;&#30456;&#20851;&#30340;&#22810;&#31181;&#39044;&#27979;&#12289;&#27169;&#25311;&#21644;&#20915;&#31574;&#25361;&#25112;&#12290;&#39046;&#22495;&#19987;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#20043;&#38388;&#30340;&#21512;&#20316;&#25512;&#21160;&#20102;&#36825;&#20123;&#23453;&#36149;&#24037;&#20855;&#22312;&#29702;&#35299;&#25105;&#20204;&#22320;&#29699;&#30340;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#26041;&#38754;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#22686;&#24378;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;&#22320;&#29699;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geoscience foundation models represent a revolutionary approach in the field of Earth sciences by integrating massive cross-disciplinary data to simulate and understand the Earth systems dynamics. As a data-centric artificial intelligence (AI) paradigm, they uncover insights from petabytes of structured and unstructured data. Flexible task specification, diverse inputs and outputs and multi-modal knowledge representation enable comprehensive analysis infeasible with individual data sources. Critically, the scalability and generalizability of geoscience models allow for tackling diverse prediction, simulation, and decision challenges related to Earth systems interactions. Collaboration between domain experts and computer scientists leads to innovations in these invaluable tools for understanding the past, present, and future of our planet. However, challenges remain in validation and verification, scale, interpretability, knowledge representation, and social bias. Going forward, enhanci
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#24187;&#35273;&#30340;&#20998;&#31867;&#12289;&#29702;&#35770;&#20998;&#26512;&#12289;&#26816;&#27979;&#26041;&#27861;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#35774;&#24819;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.06794</link><description>&lt;p&gt;
&#35748;&#30693;&#24187;&#35273;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#29616;&#35937;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Cognitive Mirage: A Review of Hallucinations in Large Language Models. (arXiv:2309.06794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#24187;&#35273;&#30340;&#20998;&#31867;&#12289;&#29702;&#35770;&#20998;&#26512;&#12289;&#26816;&#27979;&#26041;&#27861;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#35774;&#24819;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#20196;&#20154;&#25285;&#24551;&#30340;&#29616;&#35937;&#65292;&#21363;&#24187;&#35273;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26368;&#36817;&#20851;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#24341;&#20154;&#27880;&#30446;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#24187;&#35273;&#30340;&#26032;&#20998;&#31867;&#20307;&#31995;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#29702;&#35770;&#24615;&#30340;&#27934;&#35265;&#12289;&#26816;&#27979;&#26041;&#27861;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20026;&#20986;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#25552;&#20379;&#20102;&#35814;&#32454;&#21644;&#23436;&#25972;&#30340;&#20998;&#31867;&#20307;&#31995;&#65307;&#65288;2&#65289;&#25105;&#20204;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;&#30340;&#26816;&#27979;&#21644;&#25913;&#36827;&#26041;&#27861;&#65307;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#21487;&#20197;&#21457;&#23637;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#30001;&#20110;&#24187;&#35273;&#21463;&#21040;&#20102;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#25105;&#20204;&#23558;&#32500;&#25252;&#19982;&#30456;&#20851;&#30740;&#31350;&#36827;&#23637;&#30340;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#27979;&#35797;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.06774</link><description>&lt;p&gt;
&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss. (arXiv:2309.06774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#27979;&#35797;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21270;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#30005;&#23376;&#24037;&#31243;&#12289;&#25968;&#23398;&#12289;&#21307;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#29289;&#29702;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#23545;&#20110;&#20026;&#20160;&#20040;&#21644;&#22914;&#20309;&#33719;&#24471;&#32463;&#39564;&#25104;&#21151;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#22522;&#26412;&#38590;&#20197;&#25226;&#25569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#26681;&#26412;&#38382;&#39064;&#24182;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#32972;&#21518;&#30340;&#22885;&#31192;&#65292;&#24050;&#32463;&#22312;&#24314;&#31435;&#32479;&#19968;&#29702;&#35770;&#30340;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#21019;&#26032;&#12290;&#36825;&#20123;&#21019;&#26032;&#21253;&#25324;&#20248;&#21270;&#12289;&#27867;&#21270;&#21644;&#36817;&#20284;&#31561;&#22522;&#30784;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#19968;&#20010;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#35299;&#20915;&#27169;&#24335;&#20998;&#31867;&#38382;&#39064;&#26102;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;&#20026;&#20102;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20811;&#26381;&#36825;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#27979;&#35797;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep learning (DL) has led to several breakthroughs in many disciplines as diverse as chemistry, computer science, electrical engineering, mathematics, medicine, neuroscience, and physics, a comprehensive understanding of why and how DL is empirically successful remains fundamentally elusive. To attack this fundamental problem and unravel the mysteries behind DL's empirical successes, significant innovations toward a unified theory of DL have been made. These innovations encompass nearly fundamental advances in optimization, generalization, and approximation. Despite these advances, however, no work to date has offered a way to quantify the testing performance of a DL-based algorithm employed to solve a pattern classification problem. To overcome this fundamental challenge in part, this paper exposes the fundamental testing performance limits of DL-based binary classifiers trained with hinge loss. For binary classifiers that are based on deep rectified linear unit (ReLU) feedf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.06726</link><description>&lt;p&gt;
&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;BART&#24494;&#35843;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Keyphrase Generation by BART Finetuning with Splitting and Shuffling. (arXiv:2309.06726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#26159;&#19968;&#39033;&#35782;&#21035;&#26368;&#20339;&#20195;&#34920;&#32473;&#23450;&#25991;&#26412;&#20027;&#39064;&#25110;&#20027;&#39064;&#30340;&#30701;&#35821;&#38598;&#30340;&#20219;&#21153;&#12290;&#20851;&#38190;&#30701;&#35821;&#20998;&#20026;&#20986;&#29616;&#21644;&#19981;&#22312;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#12290;&#26368;&#36817;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#19978;&#26174;&#31034;&#20986;&#20102;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25214;&#21040;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#30340;&#38590;&#24230;&#65292;&#24615;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#21033;&#29992;&#20102;&#20986;&#29616;&#21644;&#19981;&#20986;&#29616;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23545;&#20986;&#29616;&#21644;&#19981;&#20986;&#29616;&#20851;&#38190;&#30701;&#35821;&#20998;&#21035;&#36827;&#34892;&#20102;&#20004;&#20010;&#29420;&#31435;BART&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20851;&#38190;&#30701;&#35821;&#30340;&#37325;&#25490;&#21644;&#20505;&#36873;&#20851;&#38190;&#30701;&#35821;&#25490;&#24207;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23545;&#20110;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#65292;&#22312;&#20116;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#22312;F1@5&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase generation is a task of identifying a set of phrases that best repre-sent the main topics or themes of a given text. Keyphrases are dividend int pre-sent and absent keyphrases. Recent approaches utilizing sequence-to-sequence models show effectiveness on absent keyphrase generation. However, the per-formance is still limited due to the hardness of finding absent keyphrases. In this paper, we propose Keyphrase-Focused BART, which exploits the differ-ences between present and absent keyphrase generations, and performs fine-tuning of two separate BART models for present and absent keyphrases. We further show effective approaches of shuffling keyphrases and candidate keyphrase ranking. For absent keyphrases, our Keyphrase-Focused BART achieved new state-of-the-art score on F1@5 in two out of five keyphrase gen-eration benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#39057;&#35889;&#28151;&#21512;&#22120;&#65288;DSM&#65289;&#26159;&#19968;&#31181;&#20869;&#23481;&#33258;&#36866;&#24212;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#34920;&#31034;&#20196;&#29260;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#26399;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#23427;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#39057;&#35889;&#26435;&#37325;&#29983;&#25104;&#23618;&#20316;&#20026;&#39057;&#35889;&#24102;&#36873;&#25321;&#22120;&#65292;&#20197;&#24378;&#35843;&#20449;&#24687;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06721</link><description>&lt;p&gt;
&#21160;&#24577;&#39057;&#35889;&#28151;&#21512;&#22120;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spectrum Mixer for Visual Recognition. (arXiv:2309.06721v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06721
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#39057;&#35889;&#28151;&#21512;&#22120;&#65288;DSM&#65289;&#26159;&#19968;&#31181;&#20869;&#23481;&#33258;&#36866;&#24212;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#34920;&#31034;&#20196;&#29260;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#26399;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#23427;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#39057;&#35889;&#26435;&#37325;&#29983;&#25104;&#23618;&#20316;&#20026;&#39057;&#35889;&#24102;&#36873;&#25321;&#22120;&#65292;&#20197;&#24378;&#35843;&#20449;&#24687;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#35270;&#35273;&#20027;&#24178;&#22312;&#20960;&#20010;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;MLP&#30340;&#26041;&#27861;&#30452;&#25509;&#20351;&#29992;&#38745;&#24577;&#26435;&#37325;&#32858;&#21512;&#20196;&#20854;&#26080;&#27861;&#36866;&#24212;&#19981;&#21516;&#30340;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;MLP-Transformer&#22312;&#21019;&#24314;&#36828;&#31243;&#20381;&#36182;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25429;&#25417;&#20027;&#35201;&#20256;&#36755;&#23616;&#37096;&#20449;&#24687;&#30340;&#39640;&#39057;&#29575;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#20351;&#20854;&#26080;&#27861;&#24212;&#29992;&#20110;&#19979;&#28216;&#30340;&#31264;&#23494;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#35821;&#20041;&#20998;&#21106;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23481;&#33258;&#36866;&#24212;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32467;&#26500;&#65292;&#31216;&#20026;&#21160;&#24577;&#39057;&#35889;&#28151;&#21512;&#22120;&#65288;DSM&#65289;&#12290;DSM&#36890;&#36807;&#24212;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#22312;&#39057;&#22495;&#20013;&#34920;&#31034;&#20196;&#29260;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#20197;&#23545;&#25968;&#32447;&#24615;&#22797;&#26434;&#24230;&#23398;&#20064;&#38271;&#26399;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39057;&#35889;&#26435;&#37325;&#29983;&#25104;&#23618;&#20316;&#20026;&#39057;&#35889;&#24102;&#36873;&#25321;&#22120;&#65292;&#33021;&#22815;&#24378;&#35843;&#20449;&#24687;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, MLP-based vision backbones have achieved promising performance in several visual recognition tasks. However, the existing MLP-based methods directly aggregate tokens with static weights, leaving the adaptability to different images untouched. Moreover, Recent research demonstrates that MLP-Transformer is great at creating long-range dependencies but ineffective at catching high frequencies that primarily transmit local information, which prevents it from applying to the downstream dense prediction tasks, such as semantic segmentation. To address these challenges, we propose a content-adaptive yet computationally efficient structure, dubbed Dynamic Spectrum Mixer (DSM). The DSM represents token interactions in the frequency domain by employing the Discrete Cosine Transform, which can learn long-term spatial dependencies with log-linear complexity. Furthermore, a dynamic spectrum weight generation layer is proposed as the spectrum bands selector, which could emphasize the infor
&lt;/p&gt;</description></item><item><title>TrafficGPT&#26159;ChatGPT&#21644;&#20132;&#36890;&#22522;&#30784;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#36890;&#36807;&#25552;&#20379;&#26597;&#30475;&#12289;&#20998;&#26512;&#21644;&#20132;&#20114;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#20132;&#36890;&#38382;&#39064;&#21644;&#25552;&#20379;&#26377;&#35265;&#22320;&#24314;&#35758;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06719</link><description>&lt;p&gt;
TrafficGPT&#65306;&#26597;&#30475;&#12289;&#22788;&#29702;&#21644;&#19982;&#20132;&#36890;&#22522;&#30784;&#27169;&#22411;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models. (arXiv:2309.06719v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06719
&lt;/p&gt;
&lt;p&gt;
TrafficGPT&#26159;ChatGPT&#21644;&#20132;&#36890;&#22522;&#30784;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#36890;&#36807;&#25552;&#20379;&#26597;&#30475;&#12289;&#20998;&#26512;&#21644;&#20132;&#20114;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#20132;&#36890;&#38382;&#39064;&#21644;&#25552;&#20379;&#26377;&#35265;&#22320;&#24314;&#35758;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#21521;&#20844;&#20247;&#25512;&#24191;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#31216;&#22855;&#30340;&#24120;&#35782;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#32463;&#24120;&#25552;&#20379;&#26377;&#35265;&#22320;&#30340;&#25351;&#23548;&#12290;&#36825;&#20123;&#33021;&#21147;&#20026;&#23427;&#20204;&#22312;&#22478;&#24066;&#20132;&#36890;&#31649;&#29702;&#21644;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20132;&#36890;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22788;&#29702;&#25968;&#20540;&#25968;&#25454;&#21644;&#19982;&#27169;&#25311;&#20132;&#20114;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#20132;&#36890;&#30456;&#20851;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19987;&#38376;&#30340;&#20132;&#36890;&#22522;&#30784;&#27169;&#22411;&#23384;&#22312;&#65292;&#20294;&#36890;&#24120;&#21482;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#36755;&#20837;&#36755;&#20986;&#20132;&#20114;&#26377;&#38480;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22788;&#29702;&#22797;&#26434;&#30340;&#20132;&#36890;&#30456;&#20851;&#38382;&#39064;&#21644;&#25552;&#20379;&#26377;&#35265;&#22320;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrafficGPT&#65292;&#23427;&#26159;ChatGPT&#21644;&#20132;&#36890;&#22522;&#30784;&#27169;&#22411;&#30340;&#34701;&#21512;&#12290;&#36825;&#31181;&#25972;&#21512;&#20135;&#29983;&#20102;&#20197;&#19979;&#20851;&#38190;&#22686;&#24378;&#65306;1&#65289;&#36171;&#20104;ChatGPT&#26597;&#30475;&#12289;&#20998;&#26512;&#12289;&#22788;&#29702;&#25968;&#23383;&#25968;&#25454;&#21644;&#19982;&#20132;&#36890;&#27169;&#25311;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the promotion of chatgpt to the public, Large language models indeed showcase remarkable common sense, reasoning, and planning skills, frequently providing insightful guidance. These capabilities hold significant promise for their application in urban traffic management and control. However, LLMs struggle with addressing traffic issues, especially processing numerical data and interacting with simulations, limiting their potential in solving traffic-related challenges. In parallel, specialized traffic foundation models exist but are typically designed for specific tasks with limited input-output interactions. Combining these models with LLMs presents an opportunity to enhance their capacity for tackling complex traffic-related problems and providing insightful suggestions. To bridge this gap, we present TrafficGPT, a fusion of ChatGPT and traffic foundation models. This integration yields the following key enhancements: 1) empowering ChatGPT with the capacity to view, analyze, pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FedGH&#65292;&#36890;&#36807;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#65292;FedGH&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06692</link><description>&lt;p&gt;
&#35299;&#20915;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#30340;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization. (arXiv:2309.06692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FedGH&#65292;&#36890;&#36807;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#65292;FedGH&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#20174;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#21327;&#20316;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#21463;&#21040;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21644;&#35774;&#22791;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26381;&#21153;&#22120;&#31471;&#30340;&#26799;&#24230;&#20914;&#31361;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26799;&#24230;&#20914;&#31361;&#29616;&#35937;&#65292;&#24182;&#25581;&#31034;&#20102;&#26356;&#24378;&#30340;&#24322;&#26500;&#24615;&#20250;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#26799;&#24230;&#20914;&#31361;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedGH&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26469;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#12290;&#36825;&#31181;&#25216;&#26415;&#23558;&#19968;&#20010;&#26799;&#24230;&#21521;&#37327;&#25237;&#24433;&#21040;&#19982;&#20854;&#20182;&#20914;&#31361;&#23458;&#25143;&#31471;&#23545;&#20043;&#38388;&#30340;&#27491;&#20132;&#24179;&#38754;&#19978;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedGH&#22312;&#19981;&#21516;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#32447;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;FedGH&#22312;&#29305;&#23450;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26356;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving paradigm for collaboratively training a global model from decentralized clients. However, the performance of FL is hindered by non-independent and identically distributed (non-IID) data and device heterogeneity. In this work, we revisit this key challenge through the lens of gradient conflicts on the server side. Specifically, we first investigate the gradient conflict phenomenon among multiple clients and reveal that stronger heterogeneity leads to more severe gradient conflicts. To tackle this issue, we propose FedGH, a simple yet effective method that mitigates local drifts through Gradient Harmonization. This technique projects one gradient vector onto the orthogonal plane of the other within conflicting client pairs. Extensive experiments demonstrate that FedGH consistently enhances multiple state-of-the-art FL baselines across diverse benchmarks and non-IID scenarios. Notably, FedGH yields more significant improvements in scenarios 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#21270;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.06687</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#25105;&#25913;&#36827;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#21160;&#21270;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#24072;
&lt;/p&gt;
&lt;p&gt;
Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics. (arXiv:2309.06687v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#21270;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20247;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#35774;&#35745;&#39640;&#24615;&#33021;&#30340;&#22870;&#21169;&#20989;&#25968;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#36755;&#20837;&#12290;&#26368;&#36817;&#65292;&#24191;&#27867;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#38656;&#35201;&#28145;&#20837;&#24120;&#35782;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#22914;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#24847;&#35782;&#21040;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#19982;&#36825;&#31181;&#30693;&#35782;&#26412;&#36136;&#19978;&#26159;&#30456;&#20851;&#30340;&#65292;LLM&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#25552;&#20379;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#26694;&#26550;&#65292;&#20855;&#26377;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#12290;&#35813;&#26694;&#26550;&#20197;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20026;&#22522;&#30784;&#65292;&#30001;LLM&#21046;&#23450;&#19968;&#20010;&#21021;&#22987;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#32467;&#26524;&#21576;&#29616;&#32473;LLM&#20197;&#25351;&#23548;&#20854;&#33258;&#25105;&#25913;&#36827;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#22810;&#31181;&#36830;&#32493;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep Reinforcement Learning (DRL) has achieved notable success in numerous robotic applications, designing a high-performing reward function remains a challenging task that often requires substantial manual input. Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning. Recognizing that reward function design is also inherently linked to such knowledge, LLM offers a promising potential in this context. Motivated by this, we propose in this work a novel LLM framework with a self-refinement mechanism for automated reward function design. The framework commences with the LLM formulating an initial reward function based on natural language inputs. Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process. We examine the performance of our proposed framework through a variety of continuous robot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay (ALAP)&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#65292;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#65292;&#28040;&#38500;&#20102;&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#21644;&#23545;&#27604;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.06684</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attention Loss Adjusted Prioritized Experience Replay. (arXiv:2309.06684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay (ALAP)&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#65292;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#65292;&#28040;&#38500;&#20102;&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#21644;&#23545;&#27604;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;(Prioritized Experience Replay, PER)&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#26356;&#22810;&#30693;&#35782;&#37327;&#30340;&#32463;&#39564;&#26679;&#26412;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;PER&#20013;&#20351;&#29992;&#30340;&#38750;&#22343;&#21248;&#37319;&#26679;&#19981;&#21487;&#36991;&#20813;&#22320;&#20351;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#24102;&#26469;Q&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Attention Loss Adjusted Prioritized (ALAP) Experience Replay&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36866;&#24212;&#33021;&#22815;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#22240;PER&#24341;&#36215;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#23545;&#22522;&#20110;&#20540;&#20989;&#25968;&#12289;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#21644;&#22810;&#20027;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#23545;&#27604;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26694;&#26550;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prioritized Experience Replay (PER) is a technical means of deep reinforcement learning by selecting experience samples with more knowledge quantity to improve the training rate of neural network. However, the non-uniform sampling used in PER inevitably shifts the state-action space distribution and brings the estimation error of Q-value function. In this paper, an Attention Loss Adjusted Prioritized (ALAP) Experience Replay algorithm is proposed, which integrates the improved Self-Attention network with Double-Sampling mechanism to fit the hyperparameter that can regulate the importance sampling weights to eliminate the estimation error caused by PER. In order to verify the effectiveness and generality of the algorithm, the ALAP is tested with value-function based, policy-gradient based and multi-agent reinforcement learning algorithms in OPENAI gym, and comparison studies verify the advantage and efficiency of the proposed training framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#25300;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36866;&#24212;&#19981;&#21516;&#30340;&#37319;&#26679;&#35774;&#32622;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#27424;&#37319;&#26679;&#27169;&#24335;&#21644;&#37319;&#26679;&#29575;&#19979;&#25552;&#20379;&#20102;&#33391;&#22909;&#32780;&#31283;&#20581;&#30340;&#21152;&#36895;&#22270;&#20687;&#37325;&#24314;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06681</link><description>&lt;p&gt;
&#22522;&#20110;&#25554;&#25300;&#24335;&#21512;&#25104;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#27424;&#37319;&#26679;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
A plug-and-play synthetic data deep learning for undersampled magnetic resonance image reconstruction. (arXiv:2309.06681v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#25300;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36866;&#24212;&#19981;&#21516;&#30340;&#37319;&#26679;&#35774;&#32622;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#27424;&#37319;&#26679;&#27169;&#24335;&#21644;&#37319;&#26679;&#29575;&#19979;&#25552;&#20379;&#20102;&#33391;&#22909;&#32780;&#31283;&#20581;&#30340;&#21152;&#36895;&#22270;&#20687;&#37325;&#24314;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22312;&#29616;&#20195;&#21307;&#23398;&#35786;&#26029;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#25195;&#25551;&#26102;&#38388;&#36739;&#38271;&#12290;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#21435;&#28151;&#21472;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#30340;k&#31354;&#38388;&#27424;&#37319;&#26679;&#22330;&#26223;&#36827;&#34892;&#23450;&#21046;&#21270;&#12290;&#20294;&#26159;&#24403;&#37319;&#26679;&#35774;&#32622;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#37197;&#32622;&#19981;&#21516;&#30340;&#28145;&#24230;&#32593;&#32476;&#38750;&#24120;&#40635;&#28902;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#25300;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36866;&#24212;&#19981;&#21516;&#30340;&#37319;&#26679;&#35774;&#32622;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#21435;&#22122;&#32593;&#32476;&#23398;&#20064;&#22270;&#20687;&#21435;&#28151;&#21472;&#20808;&#39564;&#30693;&#35782;&#65292;&#28982;&#21518;&#23558;&#23398;&#20064;&#21040;&#30340;&#28145;&#24230;&#21435;&#22122;&#32593;&#32476;&#25554;&#20837;&#21040;&#36845;&#20195;&#31639;&#27861;&#20013;&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#12290;&#36890;&#36807;&#23545;&#20307;&#20869;&#25968;&#25454;&#30340;&#32467;&#26524;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#27424;&#37319;&#26679;&#27169;&#24335;&#21644;&#37319;&#26679;&#29575;&#19979;&#25552;&#20379;&#20102;&#33391;&#22909;&#32780;&#31283;&#20581;&#30340;&#21152;&#36895;&#22270;&#20687;&#37325;&#24314;&#24615;&#33021;&#65292;&#20174;&#35270;&#35273;&#21644;&#23450;&#37327;&#25351;&#26631;&#19978;&#22343;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic resonance imaging (MRI) plays an important role in modern medical diagnostic but suffers from prolonged scan time. Current deep learning methods for undersampled MRI reconstruction exhibit good performance in image de-aliasing which can be tailored to the specific kspace undersampling scenario. But it is very troublesome to configure different deep networks when the sampling setting changes. In this work, we propose a deep plug-and-play method for undersampled MRI reconstruction, which effectively adapts to different sampling settings. Specifically, the image de-aliasing prior is first learned by a deep denoiser trained to remove general white Gaussian noise from synthetic data. Then the learned deep denoiser is plugged into an iterative algorithm for image reconstruction. Results on in vivo data demonstrate that the proposed method provides nice and robust accelerated image reconstruction performance under different undersampling patterns and sampling rates, both visually and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#22836;&#37096;&#35299;&#21078;&#21442;&#32771;&#27169;&#22411;&#65288;SHARM&#65289;&#65292;&#29992;&#20110;&#21487;&#38752;&#22320;&#20998;&#21106;&#20154;&#22836;&#37096;&#35299;&#21078;&#32452;&#32455;&#65292;&#24182;&#38024;&#23545;&#38750;&#33041;&#32452;&#32455;&#30340;&#37325;&#35201;&#24615;&#22635;&#34917;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.06677</link><description>&lt;p&gt;
SHARM: &#20998;&#27573;&#22836;&#37096;&#35299;&#21078;&#21442;&#32771;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SHARM: Segmented Head Anatomical Reference Models. (arXiv:2309.06677v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#22836;&#37096;&#35299;&#21078;&#21442;&#32771;&#27169;&#22411;&#65288;SHARM&#65289;&#65292;&#29992;&#20110;&#21487;&#38752;&#22320;&#20998;&#21106;&#20154;&#22836;&#37096;&#35299;&#21078;&#32452;&#32455;&#65292;&#24182;&#38024;&#23545;&#38750;&#33041;&#32452;&#32455;&#30340;&#37325;&#35201;&#24615;&#22635;&#34917;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#20998;&#21106;&#20154;&#22836;&#37096;&#35299;&#21078;&#32452;&#32455;&#26159;&#33041;&#22270;&#32472;&#21046;&#12289;&#25163;&#26415;&#35745;&#21010;&#21644;&#30456;&#20851;&#35745;&#31639;&#27169;&#25311;&#30740;&#31350;&#31561;&#22810;&#31181;&#20020;&#24202;&#24212;&#29992;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#20998;&#21106;&#26159;&#22522;&#20110;&#36890;&#36807;&#23545;&#21307;&#23398;&#25104;&#20687;&#27169;&#24335;&#20013;&#30340;&#19981;&#21516;&#32452;&#32455;&#36827;&#34892;&#26631;&#35760;&#26469;&#35782;&#21035;&#19981;&#21516;&#35299;&#21078;&#32467;&#26500;&#12290;&#22823;&#33041;&#32467;&#26500;&#30340;&#20998;&#21106;&#22312;&#21307;&#23398;&#35270;&#35282;&#19979;&#24050;&#32463;&#26377;&#20102;&#19968;&#20123;&#26174;&#33879;&#30340;&#36129;&#29486;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#35299;&#21078;&#22797;&#26434;&#24615;&#21644;&#20351;&#29992;&#26631;&#20934;&#21307;&#23398;&#25104;&#20687;&#21327;&#35758;&#35266;&#23519;&#22256;&#38590;&#65292;&#38750;&#33041;&#32452;&#32455;&#30340;&#20852;&#36259;&#36739;&#23567;&#12290;&#32570;&#20047;&#25972;&#20010;&#22836;&#37096;&#20998;&#21106;&#26041;&#27861;&#21644;&#22823;&#22411;&#20154;&#31867;&#22836;&#37096;&#20998;&#21106;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#38480;&#21046;&#20102;&#21464;&#24322;&#24615;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#30005;&#33041;&#35780;&#20272;&#30005;&#33041;&#30340;&#33041;&#21050;&#28608;&#65288;&#31070;&#32463;&#35843;&#33410;&#65289;&#12289;&#20154;&#20307;&#23545;&#30005;&#30913;&#22330;&#30340;&#20445;&#25252;&#20197;&#21450;&#33041;&#30005;&#22270;&#26041;&#38754;&#65292;&#38750;&#33041;&#32452;&#32455;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#27573;&#22836;&#37096;&#35299;&#21078;&#21442;&#32771;&#27169;&#22411;&#65288;SHARM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable segmentation of anatomical tissues of human head is a major step in several clinical applications such as brain mapping, surgery planning and associated computational simulation studies. Segmentation is based on identifying different anatomical structures through labeling different tissues through medical imaging modalities. The segmentation of brain structures is commonly feasible with several remarkable contributions mainly for medical perspective; however, non-brain tissues are of less interest due to anatomical complexity and difficulties to be observed using standard medical imaging protocols. The lack of whole head segmentation methods and unavailability of large human head segmented datasets limiting the variability studies, especially in the computational evaluation of electrical brain stimulation (neuromodulation), human protection from electromagnetic field, and electroencephalography where non-brain tissues are of great importance.  To fill this gap, this study prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;(QDC)&#30340;&#27010;&#24565;&#65292;&#23427;&#26159;&#29616;&#26377;&#32463;&#20856;&#25968;&#25454;&#20013;&#24515;&#30340;&#37327;&#23376;&#29256;&#26412;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;(QRAM)&#21644;&#37327;&#23376;&#32593;&#32476;&#65292;QDC&#21487;&#20197;&#25552;&#20379;&#23458;&#25143;&#22312;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#31934;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#23545;&#20110;&#37327;&#23376;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20256;&#24863;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#30828;&#20214;&#23454;&#29616;&#21644;&#29305;&#23450;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#31185;&#23398;&#21644;&#21830;&#19994;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;QDC&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#34892;&#19994;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.06641</link><description>&lt;p&gt;
&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;: &#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Quantum Data Center: Perspectives. (arXiv:2309.06641v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;(QDC)&#30340;&#27010;&#24565;&#65292;&#23427;&#26159;&#29616;&#26377;&#32463;&#20856;&#25968;&#25454;&#20013;&#24515;&#30340;&#37327;&#23376;&#29256;&#26412;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;(QRAM)&#21644;&#37327;&#23376;&#32593;&#32476;&#65292;QDC&#21487;&#20197;&#25552;&#20379;&#23458;&#25143;&#22312;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#31934;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#23545;&#20110;&#37327;&#23376;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20256;&#24863;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#30828;&#20214;&#23454;&#29616;&#21644;&#29305;&#23450;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#31185;&#23398;&#21644;&#21830;&#19994;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;QDC&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#34892;&#19994;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#29256;&#26412;&#30340;&#25968;&#25454;&#20013;&#24515;&#21487;&#33021;&#22312;&#37327;&#23376;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;(QDC)&#65292;&#36825;&#26159;&#29616;&#26377;&#32463;&#20856;&#25968;&#25454;&#20013;&#24515;&#30340;&#37327;&#23376;&#29256;&#26412;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;(QRAM)&#21644;&#37327;&#23376;&#32593;&#32476;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#35748;&#20026;QDC&#23558;&#20026;&#23458;&#25143;&#25552;&#20379;&#22312;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#31934;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#22909;&#22788;&#65292;&#24182;&#23545;&#37327;&#23376;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20256;&#24863;&#26041;&#38754;&#20855;&#26377;&#24110;&#21161;&#12290;&#36890;&#36807;&#30828;&#20214;&#23454;&#29616;&#21644;&#21487;&#33021;&#30340;&#29305;&#23450;&#24212;&#29992;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#19968;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#30340;&#28508;&#22312;&#31185;&#23398;&#21644;&#21830;&#19994;&#26426;&#20250;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;QDC&#22312;&#21830;&#19994;&#21644;&#31185;&#23398;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#34892;&#19994;&#12290;
&lt;/p&gt;
&lt;p&gt;
A quantum version of data centers might be significant in the quantum era. In this paper, we introduce Quantum Data Center (QDC), a quantum version of existing classical data centers, with a specific emphasis on combining Quantum Random Access Memory (QRAM) and quantum networks. We argue that QDC will provide significant benefits to customers in terms of efficiency, security, and precision, and will be helpful for quantum computing, communication, and sensing. We investigate potential scientific and business opportunities along this novel research direction through hardware realization and possible specific applications. We show the possible impacts of QDCs in business and science, especially the machine learning and big data industries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#8212;&#8212;&#20851;&#31995;&#29942;&#39048;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35825;&#23548;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06629</link><description>&lt;p&gt;
&#20316;&#20026;&#26377;&#25928;&#25277;&#35937;&#30340;&#24402;&#32435;&#20559;&#22909;&#30340;&#20851;&#31995;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
The Relational Bottleneck as an Inductive Bias for Efficient Abstraction. (arXiv:2309.06629v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#8212;&#8212;&#20851;&#31995;&#29942;&#39048;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35825;&#23548;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#35299;&#37322;&#22914;&#20309;&#20174;&#26377;&#38480;&#32463;&#39564;&#20013;&#33719;&#21462;&#25277;&#35937;&#27010;&#24565;&#12290;&#36825;&#19968;&#21162;&#21147;&#24120;&#24120;&#34987;&#25551;&#36848;&#20026;&#32463;&#39564;&#20027;&#20041;&#21644;&#22825;&#36171;&#20027;&#20041;&#26041;&#27861;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#65292;&#26368;&#36817;&#20027;&#35201;&#20307;&#29616;&#22312;&#26377;&#20851;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#35748;&#30693;&#27169;&#22411;&#30340;&#20105;&#35770;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#31181;&#26368;&#36817;&#20852;&#36215;&#30340;&#24037;&#20316;&#32447;&#36335;&#65292;&#35813;&#32447;&#36335;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#31216;&#20043;&#20026;&#20851;&#31995;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#31181;&#26032;&#30340;&#35843;&#21644;&#26041;&#24335;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#31995;&#21015;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#26041;&#24335;&#19979;&#35825;&#23548;&#20986;&#25277;&#35937;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#20316;&#20026;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#30340;&#20505;&#36873;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central challenge for cognitive science is to explain how abstract concepts are acquired from limited experience. This effort has often been framed in terms of a dichotomy between empiricist and nativist approaches, most recently embodied by debates concerning deep neural networks and symbolic cognitive models. Here, we highlight a recently emerging line of work that suggests a novel reconciliation of these approaches, by exploiting an inductive bias that we term the relational bottleneck. We review a family of models that employ this approach to induce abstractions in a data-efficient manner, emphasizing their potential as candidate models for the acquisition of abstract concepts in the human mind and brain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#21368;&#36135;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;RGB-D&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#37319;&#29992;&#39640;&#23618;&#20915;&#31574;&#27169;&#22359;&#19982;&#32463;&#20856;&#36816;&#21160;&#25511;&#21046;&#30456;&#32467;&#21512;&#30340;&#23618;&#27425;&#21270;&#25511;&#21046;&#22120;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06621</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#21368;&#36135;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Approach for Robotic Unloading from Visual Observations. (arXiv:2309.06621v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#21368;&#36135;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;RGB-D&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#37319;&#29992;&#39640;&#23618;&#20915;&#31574;&#27169;&#22359;&#19982;&#32463;&#20856;&#36816;&#21160;&#25511;&#21046;&#30456;&#32467;&#21512;&#30340;&#23618;&#27425;&#21270;&#25511;&#21046;&#22120;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#20154;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#30340;&#21368;&#36135;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#38656;&#35201;&#20351;&#29992;RGB-D&#22270;&#20687;&#20316;&#20026;&#20027;&#35201;&#36755;&#20837;&#26469;&#33258;&#20027;&#22320;&#21368;&#19979;&#19968;&#22534;&#21253;&#35065;&#12290;&#23613;&#31649;&#30417;&#30563;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#26631;&#27880;&#25968;&#25454;&#65292;&#32780;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#33719;&#24471;&#36825;&#20123;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#25511;&#21046;&#22120;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#26469;&#23398;&#20064;&#21368;&#36135;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#25511;&#21046;&#22120;&#32467;&#26500;&#65292;&#23558;&#39640;&#23618;&#20915;&#31574;&#27169;&#22359;&#19982;&#32463;&#20856;&#36816;&#21160;&#25511;&#21046;&#30456;&#32467;&#21512;&#12290;&#39640;&#23618;&#27169;&#22359;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#20854;&#20013;&#34701;&#20837;&#20102;&#23433;&#20840;&#20559;&#35265;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#36866;&#21512;&#36825;&#20010;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20004;&#20010;&#20803;&#32032;&#22312;&#23454;&#29616;&#25913;&#36827;&#30340;&#23398;&#20064;&#24615;&#33021;&#26041;&#38754;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we focus on a robotic unloading problem from visual observations, where robots are required to autonomously unload stacks of parcels using RGB-D images as their primary input source. While supervised and imitation learning have accomplished good results in these types of tasks, they heavily rely on labeled data, which are challenging to obtain in realistic scenarios. Our study aims to develop a sample efficient controller framework that can learn unloading tasks without the need for labeled data during the learning process. To tackle this challenge, we propose a hierarchical controller structure that combines a high-level decision-making module with classical motion control. The high-level module is trained using Deep Reinforcement Learning (DRL), wherein we incorporate a safety bias mechanism and design a reward function tailored to this task. Our experiments demonstrate that both these elements play a crucial role in achieving improved learning performance. Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#21327;&#21516;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06604</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#19978;&#30340;&#28151;&#21512;&#31639;&#27861;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;: &#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#20195;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach. (arXiv:2309.06604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#21327;&#21516;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26159;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#20851;&#38190;&#30340;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#25968;&#37327;&#30340;&#22823;&#24133;&#22686;&#21152;&#12289;&#22810;&#26679;&#24615;&#21644;&#20998;&#24067;&#24615;&#65292;&#36825;&#20123;&#27493;&#39588;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#24403;&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#30340;&#35774;&#35745;&#26102;&#65292;&#20250;&#24102;&#26469;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#22810;&#20010;&#29420;&#29305;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21644;&#21327;&#21516;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#20854;&#26597;&#35810;&#32467;&#26500;&#26469;&#25903;&#25345;&#19978;&#36848;&#21151;&#33021;&#65292;&#32780;&#19981;&#38480;&#20110;&#29305;&#23450;&#30340;&#23398;&#20064;&#12289;&#36873;&#25321;&#21644;&#35843;&#25972;&#26426;&#21046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12289;&#24418;&#24335;&#39564;&#35777;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical st
&lt;/p&gt;</description></item><item><title>Rank2Tell&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06597</link><description>&lt;p&gt;
Rank2Tell: &#19968;&#20010;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning. (arXiv:2309.06597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06597
&lt;/p&gt;
&lt;p&gt;
Rank2Tell&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#29992;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21644;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#21487;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#31038;&#20250;&#23545;&#23427;&#20204;&#30340;&#25509;&#21463;&#31243;&#24230;&#65292;&#32780;&#23545;&#39569;&#36710;&#20154;&#26469;&#35828;&#65292;&#23427;&#20204;&#34987;&#35270;&#20026;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20010;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#29616;&#20195;&#33258;&#20027;&#31995;&#32479;&#36719;&#20214;&#20005;&#37325;&#20381;&#36182;&#20110;&#40657;&#30418;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;Rank2Tell&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#37325;&#35201;&#24615;&#32423;&#21035;&#25490;&#24207;&#21644;&#21407;&#22240;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#21508;&#31181;&#38381;&#21512;&#21644;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#21508;&#31181;&#35821;&#20041;&#12289;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20851;&#31995;&#23646;&#24615;&#30340;&#23494;&#38598;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#20351;&#20854;&#25104;&#20026;&#20174;&#20107;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#21644;&#30456;&#20851;&#39046;&#22495;&#30740;&#31350;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#34920;&#31034;&#21644;&#25512;&#29702;&#37325;&#35201;&#24615;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Further, we introduce a joint model for joint i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#12289;&#24615;&#33021;&#21644;&#35745;&#31639;&#36164;&#28304;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#26469;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#65292;&#20174;&#32780;&#21019;&#24314;&#26356;&#39640;&#25928;&#12289;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#20026;AI&#35821;&#35328;&#24314;&#27169;&#30340;&#21487;&#25345;&#32493;&#21644;&#21487;&#35775;&#38382;&#30340;&#26410;&#26469;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.06589</link><description>&lt;p&gt;
&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#38656;&#35201;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Generative Large Language Models need billions of parameters?. (arXiv:2309.06589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#12289;&#24615;&#33021;&#21644;&#35745;&#31639;&#36164;&#28304;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#26469;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#65292;&#20174;&#32780;&#21019;&#24314;&#26356;&#39640;&#25928;&#12289;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#20026;AI&#35821;&#35328;&#24314;&#27169;&#30340;&#21487;&#25345;&#32493;&#21644;&#21487;&#35775;&#38382;&#30340;&#26410;&#26469;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26032;&#31995;&#32479;&#21644;&#26041;&#27861;&#12290;&#23427;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#24615;&#33021;&#21644;&#35745;&#31639;&#36164;&#28304;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;&#20801;&#35768;&#27169;&#22411;&#30340;&#19981;&#21516;&#37096;&#20998;&#20849;&#20139;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#25152;&#38656;&#30340;&#29420;&#31435;&#21442;&#25968;&#24635;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#20102;&#27169;&#22411;&#26082;&#32039;&#20945;&#21448;&#19981;&#25439;&#22833;&#23398;&#20064;&#21644;&#34920;&#31034;&#22797;&#26434;&#35821;&#35328;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#20026;&#21019;&#24314;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#30340;LLMs&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#21644;&#24037;&#20855;&#65292;&#20026;AI&#35821;&#35328;&#24314;&#27169;&#30340;&#21487;&#25345;&#32493;&#21644;&#21487;&#35775;&#38382;&#30340;&#26410;&#26469;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents novel systems and methodologies for the development of efficient large language models (LLMs). It explores the trade-offs between model size, performance, and computational resources, with the aim of maximizing the efficiency of these AI systems. The research explores novel methods that allow different parts of the model to share parameters, reducing the total number of unique parameters required. This approach ensures that the model remains compact without sacrificing its ability to learn and represent complex language structures. This study provides valuable insights and tools for creating more efficient and effective LLMs, contributing to a more sustainable and accessible future for AI language modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#33487;&#40654;&#19990;&#35748;&#30693;&#35821;&#26009;&#24211;&#30340;&#35748;&#30693;&#29305;&#24449;&#19982;BERT&#27169;&#22411;&#38598;&#25104;&#65292;&#35777;&#26126;&#20102;&#33041;&#30005;&#22270;&#21644;&#30524;&#21160;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#30340;&#35789;-EEG&#35789;&#20856;&#12290;</title><link>http://arxiv.org/abs/2309.06580</link><description>&lt;p&gt;
&#20154;&#31867;&#33021;&#24110;&#21161;BERT&#33719;&#24471;&#8220;&#20449;&#24515;&#8221;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can humans help BERT gain "confidence"?. (arXiv:2309.06580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#33487;&#40654;&#19990;&#35748;&#30693;&#35821;&#26009;&#24211;&#30340;&#35748;&#30693;&#29305;&#24449;&#19982;BERT&#27169;&#22411;&#38598;&#25104;&#65292;&#35777;&#26126;&#20102;&#33041;&#30005;&#22270;&#21644;&#30524;&#21160;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#30340;&#35789;-EEG&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#20026;&#36328;&#23398;&#31185;&#30740;&#31350;&#24320;&#36767;&#20102;&#22810;&#31181;&#36884;&#24452;&#12290;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#28789;&#24863;&#26469;&#33258;&#22823;&#33041;&#31070;&#32463;&#20803;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#23558;&#36825;&#20004;&#20010;&#39046;&#22495;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#35748;&#30693;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#20284;&#20046;&#26159;&#38750;&#24120;&#23454;&#38469;&#30340;&#12290;&#36825;&#19981;&#20165;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25216;&#26415;&#65292;&#36824;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#23454;&#39564;&#65292;&#23558;&#33487;&#40654;&#19990;&#35748;&#30693;&#35821;&#26009;&#24211;&#65288;ZuCo&#65289;&#30340;&#35748;&#30693;&#29305;&#24449;&#19982;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;BERT&#38598;&#25104;&#12290;&#25105;&#23637;&#31034;&#20102;&#26469;&#33258;ZuCo&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21644;&#30524;&#21160;&#29305;&#24449;&#22914;&#20309;&#24110;&#21161;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#21033;&#29992;&#19968;&#20010;&#40065;&#26834;&#24615;&#26816;&#26597;&#27969;&#27700;&#32447;&#30830;&#35748;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#24182;&#29983;&#25104;&#20102;&#19968;&#20010;&#21333;&#35789;-EEG&#35789;&#20856;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#20219;&#20309;&#35748;&#30693;&#29305;&#24449;&#30340;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20998;&#26512;&#20102;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancements in artificial intelligence over the last decade have opened a multitude of avenues for interdisciplinary research. Since the idea of artificial intelligence was inspired by the working of neurons in the brain, it seems pretty practical to combine the two fields and take the help of cognitive data to train AI models. Not only it will help to get a deeper understanding of the technology, but of the brain as well. In this thesis, I conduct novel experiments to integrate cognitive features from the Zurich Cognitive Corpus (ZuCo) (Hollenstein et al., 2018) with a transformer-based encoder model called BERT. I show how EEG and eye-tracking features from ZuCo can help to increase the performance of the NLP model. I confirm the performance increase with the help of a robustness-checking pipeline and derive a word-EEG lexicon to use in benchmarking on an external dataset that does not have any cognitive features associated with it. Further, I analyze the internal working mechan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.06578</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36776;&#21035;&#31185;&#23398;&#20551;&#35774;&#30340;&#35777;&#25454;&#65311;&#31038;&#20250;&#31185;&#23398;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences. (arXiv:2309.06578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#30340;&#21046;&#23450;&#21644;&#27979;&#35797;&#26159;&#32463;&#39564;&#24615;&#30740;&#31350;&#30340;&#26680;&#24515;&#12290;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#20551;&#35774;&#26159;&#22522;&#20110;&#29616;&#26377;&#35777;&#25454;&#30340;&#26368;&#20339;&#29468;&#27979;&#65292;&#24182;&#19988;&#26159;&#22522;&#20110;&#30456;&#20851;&#25991;&#29486;&#30340;&#20840;&#38754;&#35270;&#22270;&#36827;&#34892;&#21551;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27599;&#24180;&#31185;&#23398;&#25991;&#31456;&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;&#20110;&#32473;&#23450;&#20551;&#35774;&#30456;&#20851;&#35777;&#25454;&#30340;&#25163;&#21160;&#27719;&#24635;&#21644;&#32508;&#21512;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#65292;&#33021;&#21542;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20849;&#20139;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#20013;&#20351;&#29992;&#31038;&#21306;&#39537;&#21160;&#30340;&#30740;&#31350;&#27880;&#37322;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#24615;&#33021;&#19982;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Sai90000/ScientificHypothesisEvidencing.git&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothesis formulation and testing are central to empirical research. A strong hypothesis is a best guess based on existing evidence and informed by a comprehensive view of relevant literature. However, with exponential increase in the number of scientific articles published annually, manual aggregation and synthesis of evidence related to a given hypothesis is a challenge. Our work explores the ability of current large language models (LLMs) to discern evidence in support or refute of specific hypotheses based on the text of scientific abstracts. We share a novel dataset for the task of scientific hypothesis evidencing using community-driven annotations of studies in the social sciences. We compare the performance of LLMs to several state-of-the-art benchmarks and highlight opportunities for future research in this area. The dataset is available at https://github.com/Sai90000/ScientificHypothesisEvidencing.git
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22278;&#24418;&#29305;&#24449;&#22270;&#36716;&#25442;&#22120;&#65288;CFG&#65289;&#65292;&#29992;&#20110;&#20002;&#22833;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#22270;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CFG&#22312;ogbl-citation2&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06574</link><description>&lt;p&gt;
Circle Feature Graphormer: &#33021;&#22815;&#21050;&#28608;&#22270;&#36716;&#25442;&#22120;&#30340;&#22278;&#24418;&#29305;&#24449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?. (arXiv:2309.06574v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22278;&#24418;&#29305;&#24449;&#22270;&#36716;&#25442;&#22120;&#65288;CFG&#65289;&#65292;&#29992;&#20110;&#20002;&#22833;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#22270;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CFG&#22312;ogbl-citation2&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#29992;&#20110;ogbl-citation2&#20013;&#20002;&#22833;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#30340;&#26412;&#22320;&#22270;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#23450;&#20041;&#20026;&#22278;&#24418;&#29305;&#24449;&#65292;&#20511;&#37492;&#20102;&#26379;&#21451;&#22280;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#36848;&#29305;&#24449;&#30340;&#35814;&#32454;&#35745;&#31639;&#20844;&#24335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#31532;&#19968;&#20010;&#22278;&#24418;&#29305;&#24449;&#23450;&#20041;&#20026;&#24120;&#35265;&#22270;&#20013;&#30340;&#25913;&#36827;&#25391;&#33633;&#29305;&#24449;&#65292;&#23427;&#26469;&#33258;&#20110;&#20108;&#20998;&#22270;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#31532;&#20108;&#20010;&#22278;&#24418;&#29305;&#24449;&#23450;&#20041;&#20026;&#26725;&#26753;&#65292;&#23427;&#34920;&#31034;&#19981;&#21516;&#26379;&#21451;&#22280;&#20013;&#20004;&#20010;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#19978;&#36848;&#29305;&#24449;&#20316;&#20026;&#20559;&#32622;&#26469;&#22686;&#24378;&#22270;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22270;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#22522;&#20110;SIEG&#32593;&#32476;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#22278;&#24418;&#29305;&#24449;&#30340;&#22270;&#36716;&#25442;&#22120;&#65288;CFG&#65289;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#21452;&#22612;&#32467;&#26500;&#26469;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#32467;&#26500;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CFG&#22312;ogbl-citation2&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce two local graph features for missing link prediction tasks on ogbl-citation2. We define the features as Circle Features, which are borrowed from the concept of circle of friends. We propose the detailed computing formulas for the above features. Firstly, we define the first circle feature as modified swing for common graph, which comes from bipartite graph. Secondly, we define the second circle feature as bridge, which indicates the importance of two nodes for different circle of friends. In addition, we firstly propose the above features as bias to enhance graph transformer neural network, such that graph self-attention mechanism can be improved. We implement a Circled Feature aware Graph transformer (CFG) model based on SIEG network, which utilizes a double tower structure to capture both global and local structure features. Experimental results show that CFG achieves the state-of-the-art performance on dataset ogbl-citation2.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#22312;&#24490;&#29615;&#20154;&#22312;&#24037;&#21378;&#31995;&#32479;&#20013;&#26080;&#32447;&#32593;&#32476;&#30340;&#26102;&#38388;&#21464;&#24322;&#24615;&#23545;&#27169;&#25311;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#32447;&#24615;&#26102;&#38388;&#19981;&#21464;&#27169;&#25311;&#65288;PLIS&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;2.1&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.06558</link><description>&lt;p&gt;
&#20154;&#22312;&#24490;&#29615;&#20154;&#22312;&#24037;&#21378;&#31995;&#32479;&#30340;&#39640;&#20445;&#30495;&#24555;&#36895;&#27169;&#25311;&#65288;HIL-HIP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
High Fidelity Fast Simulation of Human in the Loop Human in the Plant (HIL-HIP) Systems. (arXiv:2309.06558v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#22312;&#24490;&#29615;&#20154;&#22312;&#24037;&#21378;&#31995;&#32479;&#20013;&#26080;&#32447;&#32593;&#32476;&#30340;&#26102;&#38388;&#21464;&#24322;&#24615;&#23545;&#27169;&#25311;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#32447;&#24615;&#26102;&#38388;&#19981;&#21464;&#27169;&#25311;&#65288;PLIS&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;2.1&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#22312;&#24490;&#29615;&#65292;&#20154;&#22312;&#24037;&#21378;&#65288;HIL-HIP&#65289;&#29289;&#29702;&#31995;&#32479;&#20013;&#65292;&#26080;&#32447;&#31227;&#21160;&#32593;&#32476;&#30340;&#26102;&#38388;&#21464;&#24322;&#24615;&#24341;&#36215;&#20102;&#38750;&#32447;&#24615;&#27169;&#25311;&#65292;&#23548;&#33268;&#27169;&#25311;&#20943;&#36895;&#12290;&#36890;&#36807;&#22312;&#26102;&#38388;&#38388;&#38548;&#20869;&#25512;&#23548;&#19968;&#31995;&#21015;&#20998;&#27573;&#32447;&#24615;&#26102;&#38388;&#19981;&#21464;&#27169;&#25311;&#65288;PLIS&#65289;&#65292;&#26469;&#22788;&#29702;&#26102;&#38388;&#21464;&#24322;&#24615;&#65292;&#24182;&#22312;&#26102;&#38388;&#22495;&#20013;&#23558;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#12290;&#26412;&#25991;&#23545;&#26080;&#32447;&#32593;&#32476;&#25511;&#21046;&#30340;HIL-HIP&#31995;&#32479;&#20013;&#26102;&#38388;&#21464;&#21270;&#30340;&#32452;&#20214;&#36827;&#34892;&#20102;&#24418;&#24335;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#27169;&#25311;&#31934;&#24230;&#21644;&#21152;&#36895;&#24230;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#20026;&#20154;&#24037;&#33008;&#33146;&#26080;&#32447;&#32593;&#32476;&#31995;&#32479;&#24320;&#21457;&#20102;&#19968;&#20010;&#20934;&#30830;&#30340;&#27169;&#25311;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#34880;&#31958;&#65292;&#20855;&#26377;&#19982;&#24515;&#29702;&#24212;&#28608;&#21644;&#36827;&#39135;&#27169;&#24335;&#30456;&#20851;&#30340;&#26102;&#38388;&#21464;&#24322;&#23646;&#24615;&#12290;PLIS&#26041;&#27861;&#30456;&#27604;&#38750;&#32447;&#24615;&#31995;&#32479;&#27169;&#25311;&#23454;&#29616;&#20102;&#36229;&#36807;2.1&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-linearities in simulation arise from the time variance in wireless mobile networks when integrated with human in the loop, human in the plant (HIL-HIP) physical systems under dynamic contexts, leading to simulation slowdown. Time variance is handled by deriving a series of piece wise linear time invariant simulations (PLIS) in intervals, which are then concatenated in time domain. In this paper, we conduct a formal analysis of the impact of discretizing time-varying components in wireless network-controlled HIL-HIP systems on simulation accuracy and speedup, and evaluate trade-offs with reliable guarantees. We develop an accurate simulation framework for an artificial pancreas wireless network system that controls blood glucose in Type 1 Diabetes patients with time varying properties such as physiological changes associated with psychological stress and meal patterns. PLIS approach achieves accurate simulation with greater than 2.1 times speedup than a non-linear system simulation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#27809;&#26377;&#20154;&#20026;&#24433;&#21709;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#23398;&#25253;&#32440;&#26723;&#26696;&#20013;&#33719;&#21462;&#24182;&#26816;&#27979;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#30340;&#24773;&#24863;&#19982;&#21407;&#25991;&#26469;&#35745;&#31639;&#20559;&#35265;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20026;&#23458;&#35266;&#29702;&#35299;&#23398;&#29983;&#25253;&#32440;&#26469;&#28304;&#20013;&#30340;&#20559;&#35265;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06557</link><description>&lt;p&gt;
&#22312;&#22823;&#23398;&#23398;&#29983;&#25253;&#32440;&#20013;&#26080;&#30417;&#30563;&#26816;&#27979;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Bias Detection in College Student Newspapers. (arXiv:2309.06557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#27809;&#26377;&#20154;&#20026;&#24433;&#21709;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#23398;&#25253;&#32440;&#26723;&#26696;&#20013;&#33719;&#21462;&#24182;&#26816;&#27979;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#30340;&#24773;&#24863;&#19982;&#21407;&#25991;&#26469;&#35745;&#31639;&#20559;&#35265;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20026;&#23458;&#35266;&#29702;&#35299;&#23398;&#29983;&#25253;&#32440;&#26469;&#28304;&#20013;&#30340;&#20559;&#35265;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#27809;&#26377;&#20154;&#20026;&#24433;&#21709;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#23398;&#25253;&#32440;&#26723;&#26696;&#20013;&#33719;&#21462;&#24182;&#26816;&#27979;&#20559;&#35265;&#12290;&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;&#33258;&#21160;&#21270;&#24037;&#20855;&#26080;&#27861;&#33719;&#21462;&#25968;&#25454;&#30340;&#22797;&#26434;&#26723;&#26696;&#32593;&#31449;&#19978;&#33719;&#21462;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;23,154&#20010;&#26465;&#30446;&#30340;14&#20010;&#23398;&#29983;&#25253;&#32440;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#30340;&#24773;&#24863;&#19982;&#21407;&#25991;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#20851;&#38190;&#23383;&#26597;&#35810;&#26469;&#35745;&#31639;&#20559;&#35265;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#27604;&#37325;&#26500;&#20559;&#35265;&#26356;&#23569;&#27604;&#36739;&#65292;&#24182;&#19988;&#27604;&#29983;&#25104;&#20851;&#38190;&#23383;&#24773;&#32490;&#38656;&#35201;&#26356;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#25919;&#27835;&#24615;&#35789;&#27719;&#20197;&#21450;&#25511;&#21046;&#35789;&#19978;&#35745;&#31639;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#24471;&#20986;&#32467;&#35770;&#12290;&#35813;&#23436;&#25972;&#26041;&#27861;&#26377;&#21161;&#20110;&#22312;&#20551;&#35774;&#21644;&#20998;&#31867;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#32454;&#33268;&#20837;&#24494;&#30340;&#35265;&#35299;&#65292;&#20026;&#23458;&#35266;&#29702;&#35299;&#23398;&#29983;&#25253;&#32440;&#26469;&#28304;&#20013;&#30340;&#20559;&#35265;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a pipeline with minimal human influence for scraping and detecting bias on college newspaper archives. This paper introduces a framework for scraping complex archive sites that automated tools fail to grab data from, and subsequently generates a dataset of 14 student papers with 23,154 entries. This data can also then be queried by keyword to calculate bias by comparing the sentiment of a large language model summary to the original article. The advantages of this approach are that it is less comparative than reconstruction bias and requires less labelled data than generating keyword sentiment. Results are calculated on politically charged words as well as control words to show how conclusions can be drawn. The complete method facilitates the extraction of nuanced insights with minimal assumptions and categorizations, paving the way for a more objective understanding of bias within student newspaper sources.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#20013;&#38388;&#31232;&#30095;&#26684;&#24335;&#29983;&#25104;&#25991;&#26412;&#12290;&#36890;&#36807;&#25200;&#21160;&#26694;&#26550;&#20869;&#23481;&#65292;&#21253;&#25324;&#25299;&#25169;&#20998;&#26512;&#25366;&#25496;&#26032;&#30340;&#36229;&#36793;&#20197;&#21450;&#21253;&#21547;&#23618;&#27425;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#22797;&#26434;&#22810;&#20803;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;&#26679;&#24335;&#12289;&#24773;&#24863;&#12289;&#26684;&#24335;&#12289;&#26500;&#25104;&#21644;&#20107;&#23454;&#19978;&#26159;&#22810;&#26679;&#30340;&#12289;&#36830;&#36143;&#30340;&#21644;&#21464;&#21270;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.06550</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Synthetic Text Generation using Hypergraph Representations. (arXiv:2309.06550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#20013;&#38388;&#31232;&#30095;&#26684;&#24335;&#29983;&#25104;&#25991;&#26412;&#12290;&#36890;&#36807;&#25200;&#21160;&#26694;&#26550;&#20869;&#23481;&#65292;&#21253;&#25324;&#25299;&#25169;&#20998;&#26512;&#25366;&#25496;&#26032;&#30340;&#36229;&#36793;&#20197;&#21450;&#21253;&#21547;&#23618;&#27425;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#22797;&#26434;&#22810;&#20803;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;&#26679;&#24335;&#12289;&#24773;&#24863;&#12289;&#26684;&#24335;&#12289;&#26500;&#25104;&#21644;&#20107;&#23454;&#19978;&#26159;&#22810;&#26679;&#30340;&#12289;&#36830;&#36143;&#30340;&#21644;&#21464;&#21270;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25991;&#26723;&#30340;&#21512;&#25104;&#21464;&#20307;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#20013;&#38388;&#31232;&#30095;&#26684;&#24335;&#29983;&#25104;&#25991;&#26412;&#12290;&#36825;&#20123;&#26694;&#26550;&#20351;&#29992;&#36229;&#22270;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#20197;&#24688;&#24403;&#30340;&#26041;&#24335;&#25200;&#21160;&#26694;&#26550;&#20869;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#25299;&#25169;&#20998;&#26512;&#25366;&#25496;&#26032;&#30340;&#36229;&#36793;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#22797;&#26434;&#22810;&#20803;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;&#26679;&#24335;&#12289;&#24773;&#24863;&#12289;&#26684;&#24335;&#12289;&#26500;&#25104;&#21644;&#20107;&#23454;&#19978;&#26159;&#22810;&#26679;&#30340;&#12289;&#36830;&#36143;&#30340;&#21644;&#21464;&#21270;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating synthetic variants of a document is often posed as text-to-text transformation. We propose an alternate LLM based method that first decomposes a document into semantic frames and then generates text using this interim sparse format. The frames are modeled using a hypergraph, which allows perturbing the frame contents in a principled manner. Specifically, new hyperedges are mined through topological analysis and complex polyadic relationships including hierarchy and temporal dynamics are accommodated. We show that our solution generates documents that are diverse, coherent and vary in style, sentiment, format, composition and facts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20250;&#35805;&#25512;&#33616;&#30340;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;HierSRec&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#35774;&#32622;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#36755;&#20986;&#26469;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06533</link><description>&lt;p&gt;
&#38754;&#21521;&#20250;&#35805;&#25512;&#33616;&#30340;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Task Learning Framework for Session-based Recommendations. (arXiv:2309.06533v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20250;&#35805;&#25512;&#33616;&#30340;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;HierSRec&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#35774;&#32622;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#36755;&#20986;&#26469;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;SBRS&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#20294;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24050;&#32463;&#34987;SBRS&#37319;&#29992;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#12290;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;H-MTL&#65289;&#22312;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#35774;&#32622;&#20102;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#23558;&#36741;&#21161;&#20219;&#21153;&#30340;&#36755;&#20986;&#39304;&#36865;&#32473;&#20027;&#20219;&#21153;&#12290;&#19982;&#29616;&#26377;&#30340;MTL&#26694;&#26550;&#30456;&#27604;&#65292;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#20026;&#20027;&#20219;&#21153;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;H-MTL&#26694;&#26550;&#22312;SBRS&#20013;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HierSRec&#65292;&#23558;H-MTL&#26550;&#26500;&#32435;&#20837;SBRS&#20013;&#12290;HierSRec&#20351;&#29992;&#20803;&#25968;&#25454;&#24863;&#30693;Transformer&#23545;&#32473;&#23450;&#20250;&#35805;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20351;&#29992;&#20250;&#35805;&#32534;&#30721;&#36827;&#34892;&#19979;&#19968;&#31867;&#21035;&#39044;&#27979;&#65288;&#21363;&#36741;&#21161;&#20219;&#21153;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;HierSRec&#20351;&#29992;&#31867;&#21035;&#39044;&#27979;&#32467;&#26524;&#21644;&#20250;&#35805;&#32534;&#30721;&#36827;&#34892;&#19979;&#19968;&#20010;&#29289;&#21697;&#39044;&#27979;&#65288;&#21363;&#20027;&#20219;&#21153;&#65289;&#12290;&#20026;&#20102;&#21487;&#25193;&#23637;&#30340;&#25512;&#26029;&#65292;HierSRec&#21019;&#24314;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#20505;&#36873;&#29289;&#21697;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
While session-based recommender systems (SBRSs) have shown superior recommendation performance, multi-task learning (MTL) has been adopted by SBRSs to enhance their prediction accuracy and generalizability further. Hierarchical MTL (H-MTL) sets a hierarchical structure between prediction tasks and feeds outputs from auxiliary tasks to main tasks. This hierarchy leads to richer input features for main tasks and higher interpretability of predictions, compared to existing MTL frameworks. However, the H-MTL framework has not been investigated in SBRSs yet. In this paper, we propose HierSRec which incorporates the H-MTL architecture into SBRSs. HierSRec encodes a given session with a metadata-aware Transformer and performs next-category prediction (i.e., auxiliary task) with the session encoding. Next, HierSRec conducts next-item prediction (i.e., main task) with the category prediction result and session encoding. For scalable inference, HierSRec creates a compact set of candidate items (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06520</link><description>&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#30340;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayes' Risk Decoding for System Combination of Grammatical Error Correction Systems. (arXiv:2309.06520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#23558;&#21508;&#20010;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#32452;&#21512;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;&#21516;&#26102;&#65292;&#35299;&#30721;&#20934;&#21017;&#19982;&#35780;&#20272;&#20934;&#21017;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#21487;&#20197;&#29992;&#20110;&#20197;&#26356;&#22909;&#22320;&#19982;&#26368;&#32456;&#35780;&#20272;&#20934;&#21017;&#23545;&#40784;&#30340;&#26041;&#24335;&#32452;&#21512;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#31995;&#32479;&#20013;&#30340;MBR&#35299;&#30721;&#65292;&#35813;&#31995;&#32479;&#36890;&#24120;&#20197;&#32534;&#36753;&#27425;&#25968;&#21644;&#30456;&#20851;&#30340;F&#20998;&#25968;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#36825;&#31181;&#20934;&#21017;&#30452;&#25509;&#30456;&#20851;&#30340;&#26032;&#39062;MBR&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#25193;&#23637;&#20505;&#36873;&#21477;&#23376;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24403;&#21069;&#30340;&#26368;&#22823;&#25237;&#31080;&#32452;&#21512;&#26041;&#26696;&#65292;&#20197;&#21450;&#20010;&#20307;&#32534;&#36753;&#32423;&#21035;&#30340;&#36873;&#25321;&#12290;&#22312;&#19977;&#20010;&#27969;&#34892;&#30340;GEC&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;GEC&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;MBR&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#31361;&#20986;&#20102;MBR&#35299;&#30721;&#20013;&#19981;&#21516;&#22870;&#21169;&#25351;&#26631;&#30340;&#21464;&#21270;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
For sequence-to-sequence tasks it is challenging to combine individual system outputs. Further, there is also often a mismatch between the decoding criterion and the one used for assessment. Minimum Bayes' Risk (MBR) decoding can be used to combine system outputs in a manner that encourages better alignment with the final assessment criterion. This paper examines MBR decoding for Grammatical Error Correction (GEC) systems, where performance is usually evaluated in terms of edits and an associated F-score. Hence, we propose a novel MBR loss function directly linked to this form of criterion. Furthermore, an approach to expand the possible set of candidate sentences is described. This builds on a current max-voting combination scheme, as well as individual edit-level selection. Experiments on three popular GEC datasets and with state-of-the-art GEC systems demonstrate the efficacy of the proposed MBR approach. Additionally, the paper highlights how varying reward metrics within the MBR d
&lt;/p&gt;</description></item><item><title>AGIBench&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#12289;&#22810;&#27169;&#24577;&#12289;&#20154;&#24037;&#21442;&#32771;&#12289;&#33258;&#21160;&#35780;&#20998;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#26631;&#35760;&#38382;&#39064;&#30340;&#23646;&#24615;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#26234;&#33021;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06495</link><description>&lt;p&gt;
AGIBench: &#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#12289;&#22810;&#27169;&#24577;&#12289;&#20154;&#24037;&#21442;&#32771;&#12289;&#33258;&#21160;&#35780;&#20998;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models. (arXiv:2309.06495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06495
&lt;/p&gt;
&lt;p&gt;
AGIBench&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#12289;&#22810;&#27169;&#24577;&#12289;&#20154;&#24037;&#21442;&#32771;&#12289;&#33258;&#21160;&#35780;&#20998;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#26631;&#35760;&#38382;&#39064;&#30340;&#23646;&#24615;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#26234;&#33021;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#26234;&#33021;&#12290;&#22914;&#20309;&#35780;&#20272;LLM&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#26234;&#33021;&#31243;&#24230;&#26159;&#19968;&#20010;&#28909;&#28857;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#19982;&#19981;&#21516;&#30340;&#33021;&#21147;&#20998;&#25903;&#65288;&#22914;&#29702;&#35299;&#65289;&#21644;&#22823;&#35268;&#27169;&#30340;&#30693;&#35782;&#31867;&#21035;&#65288;&#22914;&#25968;&#23398;&#65289;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#31532;&#20108;&#65292;&#38382;&#39064;&#30340;&#36755;&#20837;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#21487;&#33021;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#31532;&#19977;&#65292;LLM&#30340;&#21709;&#24212;&#26684;&#24335;&#22810;&#26679;&#65292;&#22240;&#27492;&#23545;&#32467;&#26524;&#25552;&#21462;&#21644;&#35780;&#20272;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AGIBench--&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#22810;&#31890;&#24230;&#12289;&#22810;&#27169;&#24577;&#12289;&#20154;&#24037;&#21442;&#32771;&#21644;&#33258;&#21160;&#35780;&#20998;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#19982;&#28151;&#21512;&#38382;&#39064;&#38598;&#21512;&#19981;&#21516;&#65292;AGIBench&#19987;&#27880;&#20110;&#19977;&#20010;&#20856;&#22411;&#30340;&#33021;&#21147;&#20998;&#25903;&#65292;&#24182;&#37319;&#29992;&#22235;&#20803;&#32452;&lt;&#33021;&#21147;&#20998;&#25903;&#12289;&#30693;&#35782;&#12289;&#38590;&#24230;&#12289;&#27169;&#24577;&gt;&#26469;&#26631;&#35760;&#27599;&#20010;&#38382;&#39064;&#30340;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#25903;&#25345;&#22810;&#31890;&#24230;&#30340;&#22522;&#20934;&#21270;&#65292;&#20363;&#22914;&#27599;&#20010;&#38382;&#39064;&#12289;&#27599;&#20010;&#33021;&#21147;&#20998;&#25903;&#12289;&#27599;&#20010;&#30693;&#35782;&#31867;&#21035;&#30340;&#22522;&#20934;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT have revealed amazing intelligence. How to evaluate the question-solving abilities of LLMs and their degrees of intelligence is a hot-spot but challenging issue. First, the question-solving abilities are interlaced with different ability branches like understanding and massive knowledge categories like mathematics. Second, the inputs of questions are multimodal that may involve text and images. Third, the response format of LLMs is diverse and thus poses great challenges for result extraction and evaluation. In this paper, we propose AGIBench -- a multi-granularity, multimodal, human-referenced, and auto-scoring benchmarking methodology for LLMs. Instead of a collection of blended questions, AGIBench focuses on three typical ability branches and adopts a four-tuple &lt;ability branch, knowledge, difficulty, modal&gt; to label the attributes of each question. First, it supports multi-granularity benchmarking, e.g., per-question, per-ability branch, pe
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;</title><link>http://arxiv.org/abs/2309.05918</link><description>&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#65306;&#36208;&#21521;&#31526;&#21495;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#26412;&#20307;&#35770;&#22522;&#20110;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05918
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#22260;&#32469;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#23545;&#25104;&#21151;&#30340;&#29378;&#28909;&#26159;&#26377;&#20123;&#35823;&#23548;&#30340;&#65292;&#21407;&#22240;&#22914;&#19979;&#65306;&#65288;i&#65289;LLMs&#19981;&#33021;&#20381;&#36182;&#20110;&#20107;&#23454;&#20449;&#24687;&#65292;&#22240;&#20026;&#23545;&#20110;LLMs&#26469;&#35828;&#65292;&#25668;&#20837;&#30340;&#25152;&#26377;&#25991;&#26412;&#65288;&#20107;&#23454;&#25110;&#38750;&#20107;&#23454;&#65289;&#37117;&#26159;&#24179;&#31561;&#30340;&#65307;&#65288;ii&#65289;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#35821;&#35328;&#30340;&#20219;&#20309;&#8220;&#30693;&#35782;&#8221;&#37117;&#23558;&#27704;&#36828;&#22475;&#34255;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#26412;&#36523;&#26159;&#26377;&#24847;&#20041;&#30340;&#65307;&#20197;&#21450;&#65288;iii&#65289;LLMs&#22312;&#20960;&#31181;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#24120;&#24120;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#65288;&#22914;&#21517;&#35789;&#22797;&#21512;&#35789;&#12289;&#20849;&#35859;&#35789;&#12289;&#37327;&#35789;&#33539;&#22260;&#27169;&#31946;&#21644;&#24847;&#21521;&#24615;&#19978;&#19979;&#25991;&#65289;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30456;&#23545;&#25104;&#21151;&#19981;&#26159;&#31526;&#21495;&#19982;&#20122;&#31526;&#21495;&#20043;&#36777;&#30340;&#21453;&#26144;&#65292;&#32780;&#26159;&#22312;&#35268;&#27169;&#19978;&#24212;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#30340;&#25104;&#21151;&#31574;&#30053;&#30340;&#21453;&#26144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;&#24212;&#29992;&#20110;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;
&lt;/p&gt;
&lt;p&gt;
In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts. Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbol-ic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.05605</link><description>&lt;p&gt;
&#20869;&#23384;&#27880;&#20837;&#65306;&#22312;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#20013;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. (arXiv:2309.05605v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#38656;&#35201;&#20174;&#22810;&#20010;&#20449;&#24687;&#28304;&#20013;&#26816;&#32034;&#21644;&#32508;&#21512;&#20449;&#24687;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24448;&#24448;&#38590;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#36827;&#34892;&#23450;&#21521;&#20869;&#23384;&#27880;&#20837;&#26469;&#30830;&#23450;&#21644;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GPT-2&#27169;&#22411;&#22312;&#21333;&#36339;&#21644;&#22810;&#36339;&#25552;&#31034;&#19979;&#21508;&#23618;&#30340;&#28608;&#27963;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21521;&#20851;&#38190;LLM&#20301;&#32622;&#27880;&#20837;&#30456;&#20851;&#30340;&#25552;&#31034;&#29305;&#23450;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#35760;&#24518;&#8221;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;LLM&#33021;&#22815;&#25972;&#21512;&#39069;&#22806;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22810;&#36339;&#25552;&#31034;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#23558;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#23450;&#21521;&#30340;&#35760;&#24518;&#27880;&#20837;&#21040;&#20851;&#38190;&#27880;&#24847;&#21147;&#23618;&#20013;&#24448;&#24448;&#33021;&#22815;&#25552;&#39640;&#22810;&#36339;&#20219;&#21153;&#20013;&#25152;&#38656;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#25552;&#39640;&#20102;&#36798;&#21040;424%&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
&lt;/p&gt;</description></item><item><title>NExT-GPT&#26159;&#19968;&#20010;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25509;&#21463;&#21644;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2309.05519</link><description>&lt;p&gt;
NExT-GPT: &#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NExT-GPT: Any-to-Any Multimodal LLM. (arXiv:2309.05519v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05519
&lt;/p&gt;
&lt;p&gt;
NExT-GPT&#26159;&#19968;&#20010;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25509;&#21463;&#21644;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLM&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#25391;&#22859;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23384;&#22312;&#19968;&#20010;&#38480;&#21046;&#65292;&#21363;&#21482;&#33021;&#22312;&#36755;&#20837;&#31471;&#36827;&#34892;&#22810;&#27169;&#24577;&#29702;&#35299;&#65292;&#26080;&#27861;&#20197;&#22810;&#31181;&#27169;&#24335;&#29983;&#25104;&#20869;&#23481;&#12290;&#30001;&#20110;&#25105;&#20204;&#20154;&#31867;&#24635;&#26159;&#36890;&#36807;&#21508;&#31181;&#27169;&#24577;&#24863;&#30693;&#19990;&#30028;&#21644;&#19982;&#20154;&#20132;&#27969;&#65292;&#22240;&#27492;&#24320;&#21457;&#33021;&#22815;&#25509;&#21463;&#21644;&#20256;&#36882;&#20219;&#20309;&#27169;&#24577;&#20869;&#23481;&#30340;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;MM-LLM&#31995;&#32479;&#23545;&#20110;&#23454;&#29616;&#20154;&#32423;AI&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;NExT-GPT&#12290;&#25105;&#20204;&#36890;&#36807;&#36830;&#25509;&#19968;&#20010;&#21547;&#26377;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#30340;LLM&#65292;&#20351;&#24471;NExT-GPT&#33021;&#22815;&#20197;&#20219;&#24847;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#36827;&#34892;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#35757;&#32451;&#26377;&#32032;&#30340;&#39640;&#24615;&#33021;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;NExT-GPT&#20165;&#36890;&#36807;&#35843;&#25972;&#26576;&#20123;&#25237;&#24433;&#23618;&#30340;&#23569;&#37327;&#21442;&#25968;&#65288;1%&#65289;&#36827;&#34892;&#35843;&#20248;&#65292;&#36825;&#19981;&#20165;&#26377;&#21033;&#20110;&#20302;&#25104;&#26412;&#35757;&#32451;&#65292;&#36824;&#26377;&#21161;&#20110;&#26041;&#20415;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#32534;&#30721;&#22120;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#22330;&#26223;&#34920;&#31034;&#21644;&#32463;&#20856;&#30340;&#26629;&#26684;&#21270;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#20102;&#25551;&#36848;&#24615;&#30340;&#22330;&#26223;&#23884;&#20837;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.05282</link><description>&lt;p&gt;
&#21487;&#20197;&#36890;&#36807;&#30701;&#20449;&#20256;&#36755;&#21457;&#29983;&#30340;&#20107;&#24773;&#21527;&#65311;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#32534;&#30721;&#22120;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving. (arXiv:2309.05282v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#32534;&#30721;&#22120;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#22330;&#26223;&#34920;&#31034;&#21644;&#32463;&#20856;&#30340;&#26629;&#26684;&#21270;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#20102;&#25551;&#36848;&#24615;&#30340;&#22330;&#26223;&#23884;&#20837;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#65292;&#22330;&#26223;&#29702;&#35299;&#26159;&#39044;&#27979;&#21608;&#22260;&#20132;&#36890;&#21442;&#19982;&#32773;&#26410;&#26469;&#34892;&#20026;&#30340;&#31532;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#34920;&#31034;&#32473;&#23450;&#30340;&#22330;&#26223;&#24182;&#25552;&#21462;&#20854;&#29305;&#24449;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#36890;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#36827;&#34892;&#22788;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#34920;&#31034;&#19982;&#20256;&#32479;&#30340;&#26629;&#26684;&#21270;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#24471;&#21040;&#25551;&#36848;&#24615;&#30340;&#22330;&#26223;&#23884;&#20837;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#39044;&#27979;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#65292;&#25991;&#26412;&#21644;&#26629;&#26684;&#21270;&#22270;&#20687;&#30340;&#32852;&#21512;&#32534;&#30721;&#22120;&#32988;&#36807;&#21333;&#29420;&#30340;&#32534;&#30721;&#22120;&#65292;&#30830;&#35748;&#20102;&#20004;&#31181;&#34920;&#31034;&#20855;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving tasks, scene understanding is the first step towards predicting the future behavior of the surrounding traffic participants. Yet, how to represent a given scene and extract its features are still open research questions. In this study, we propose a novel text-based representation of traffic scenes and process it with a pre-trained language encoder.  First, we show that text-based representations, combined with classical rasterized image representations, lead to descriptive scene embeddings. Second, we benchmark our predictions on the nuScenes dataset and show significant improvements compared to baselines. Third, we show in an ablation study that a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#29289;&#29702;&#20449;&#24687;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#21644;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#26469;&#33719;&#21462;&#26368;&#36866;&#24403;&#30340;&#21453;&#23545;&#35282;&#39033;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2309.04434</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26368;&#20248;&#21453;&#23545;&#35282;&#37327;&#23376;&#35745;&#31639;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation. (arXiv:2309.04434v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#29289;&#29702;&#20449;&#24687;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#21644;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#26469;&#33719;&#21462;&#26368;&#36866;&#24403;&#30340;&#21453;&#23545;&#35282;&#39033;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#20248;&#21183;&#26469;&#35299;&#20915;&#30001;$N_{Q}$&#27604;&#29305;&#31995;&#32479;&#32452;&#25104;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#30340;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#31934;&#30830;&#22320;&#35299;&#20915;&#37327;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#30340;&#26102;&#38388;&#28436;&#21270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#24517;&#35201;&#30340;&#29289;&#29702;&#20449;&#24687;&#23884;&#20837;&#21040;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23545;&#25152;&#26377;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#26045;&#21152;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#65292;&#20445;&#35777;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26368;&#36866;&#24403;&#21453;&#23545;&#35282;&#39033;&#30340;&#33719;&#21462;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#36873;&#25321;&#26469;&#35299;&#20915;CD&#39537;&#21160;&#38382;&#39064;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel methodology that leverages the strength of Physics-Informed Neural Networks (PINNs) to address the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits. The primary objective is to utilize physics-inspired deep learning techniques to accurately solve the time evolution of the different physical observables within the quantum system. To accomplish this objective, we embed the necessary physical information into an underlying neural network to effectively tackle the problem. In particular, we impose the hermiticity condition on all physical observables and make use of the principle of least action, guaranteeing the acquisition of the most appropriate counterdiabatic terms based on the underlying physics. The proposed approach offers a dependable alternative to address the CD driving problem, free from the constraints typically encountered in previous methodologies relying on classical numerical approximations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04339</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20984;&#20248;&#21270;&#23454;&#29616;&#22312;&#32447;&#23376;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#19968;&#33324;&#24615;&#27169;&#24615;&#32422;&#26463;&#19979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#20248;&#21270;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#65292;&#21363;&#21152;&#26435;&#38408;&#20540;&#21183;&#20989;&#25968;&#65292;&#21487;&#20197;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;(OCO)&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20010;&#31867;&#21035;&#30340;&#20989;&#25968;&#21487;&#20197;&#36827;&#34892;&#20985;&#26494;&#24347;;&#22240;&#27492;&#65292;&#32467;&#21512;&#36866;&#24403;&#30340;&#33293;&#20837;&#26041;&#26696;&#65292;OCO&#31574;&#30053;&#21487;&#20197;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31616;&#21270;&#26041;&#24335;&#21487;&#20197;&#24212;&#29992;&#22312;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#21160;&#24577;&#36951;&#25022;&#12289;&#24378;&#30423;&#21644;&#20048;&#35266;&#23398;&#20064;&#31561;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03581</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#22312;&#22810;&#30446;&#26631;&#38382;&#39064;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#23545;&#20110;&#21457;&#25381;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#29992;&#25143;&#36890;&#24120;&#23545;&#22810;&#30446;&#26631;&#38382;&#39064;&#24863;&#20852;&#36259;&#65292;&#21363;&#20248;&#21270;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#27604;&#22914;&#20934;&#30830;&#24615;&#21644;&#33021;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32477;&#22823;&#22810;&#25968;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#36820;&#22238;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#36825;&#31181;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35780;&#20272;&#19968;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#28041;&#21450;&#35780;&#20272;&#24471;&#21040;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#26377;&#19968;&#20123;&#25351;&#26631;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#23646;&#24615;&#65288;&#22914;&#20307;&#31215;&#12289;&#19982;&#21442;&#32771;&#28857;&#30340;&#25509;&#36817;&#31243;&#24230;&#65289;&#26469;&#35780;&#20272;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#65288;&#20363;&#22914;&#36229;&#20307;&#31215;&#12289;R2&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#65292;&#36873;&#25321;&#23548;&#33268;&#26399;&#26395;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25351;&#26631;&#21487;&#33021;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#38024;&#23545;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.01538</link><description>&lt;p&gt;
ChatRule&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning. (arXiv:2309.01538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#35268;&#21017;&#23545;&#20110;&#21457;&#29616;&#20851;&#31995;&#20043;&#38388;&#30340;&#36923;&#36753;&#36830;&#25509;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#21162;&#21147;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#25366;&#25496;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#35268;&#21017;&#31354;&#38388;&#19978;&#25628;&#32034;&#35745;&#31639;&#23494;&#38598;&#19988;&#32570;&#20047;&#21487;&#20280;&#32553;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#20102;&#20851;&#31995;&#30340;&#35821;&#20041;&#65292;&#32780;&#36825;&#23545;&#20110;&#25581;&#31034;&#36923;&#36753;&#36830;&#25509;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21644;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#26032;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20197;&#22522;&#20110;LLM&#30340;&#35268;&#21017;&#29983;&#25104;&#22120;&#20026;&#21021;&#22987;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical rules are essential for uncovering the logical connections between relations, which could improve the reasoning performance and provide interpretable results on knowledge graphs (KGs). Although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from the computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs 
&lt;/p&gt;</description></item><item><title>&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;"&#23545;&#40784;&#22686;&#24378;&#22120;"&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#21487;&#36716;&#31227;&#24615;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;&#35843;&#25972;&#21442;&#25968;&#30340;&#26102;&#38388;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01516</link><description>&lt;p&gt;
&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#65306;&#20026;&#21487;&#25193;&#23637;&#30340;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#35843;&#25972;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval. (arXiv:2309.01516v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01516
&lt;/p&gt;
&lt;p&gt;
&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;"&#23545;&#40784;&#22686;&#24378;&#22120;"&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#21487;&#36716;&#31227;&#24615;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;&#35843;&#25972;&#21442;&#25968;&#30340;&#26102;&#38388;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#23558;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#35843;&#25972;&#21040;&#19987;&#38376;&#30340;&#20219;&#21153;&#19978;&#24050;&#25104;&#20026;&#19968;&#20010;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#36827;&#34892;&#23396;&#31435;&#12289;&#31351;&#20030;&#30340;&#37325;&#26032;&#35843;&#25972;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#32463;&#24120;&#24573;&#35270;&#27169;&#24577;&#23545;&#40784;&#65292;&#20165;&#20851;&#27880;&#26032;&#20219;&#21153;&#30340;&#30693;&#35782;&#25552;&#21462;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#21253;&#21547;&#20102;&#19968;&#20010;&#8220;&#23545;&#40784;&#22686;&#24378;&#22120;&#8221;&#65292;&#21487;&#20197;&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#24230;&#30340;&#21487;&#36716;&#31227;&#24615;&#32780;&#26080;&#38656;&#35843;&#25972;&#39044;&#35757;&#32451;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21521;LMMs&#28155;&#21152;&#20102;&#19981;&#21040;1.25%&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#20197;BEiT-3&#27169;&#22411;&#20026;&#20363;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#32553;&#30701;&#20102;&#39640;&#36798;57%&#30340;&#24494;&#35843;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15321</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#65292;&#32570;&#20047;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23545;&#37319;&#26679;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;&#39044;&#27979;&#35823;&#24046;&#24402;&#22240;&#20026;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#38500;&#20102;&#38416;&#26126;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;Epsilon Scaling&#65292;&#20197;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Epsilon Scaling&#36890;&#36807;&#32553;&#23567;&#32593;&#32476;&#36755;&#20986;&#65288;Epsilon&#65289;&#26126;&#30830;&#22320;&#23558;&#37319;&#26679;&#36712;&#36857;&#31227;&#36817;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#12290;&#22312;&#21508;&#31181;&#25193;&#25955;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24182;&#34892;&#38598;&#21512;&#20013;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20803;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#27714;&#35299;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#36229;&#36234;&#21333;&#20010;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.07347</link><description>&lt;p&gt;
&#38024;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#24182;&#34892;&#20803;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#30340;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
A Parallel Ensemble of Metaheuristic Solvers for the Traveling Salesman Problem. (arXiv:2308.07347v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07347
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24182;&#34892;&#38598;&#21512;&#20013;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20803;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#27714;&#35299;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#36229;&#36234;&#21333;&#20010;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#26159;&#25991;&#29486;&#20013;&#30740;&#31350;&#36739;&#22810;&#30340;NP-hard&#38382;&#39064;&#20043;&#19968;&#12290;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;TSP&#27714;&#35299;&#22120;&#26159;Lin-Kernighan-Helsgaun&#65288;LKH&#65289;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;Edge Assembly&#20132;&#21449;&#31639;&#27861;&#65288;EAX&#65289;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24102;&#26377;&#37325;&#21551;&#26426;&#21046;&#30340;EAX&#22312;&#24191;&#27867;&#30340;TSP&#23454;&#20363;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#30740;&#31350;&#20165;&#38480;&#20110;&#28041;&#21450;2000&#20010;&#22478;&#24066;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;2000&#21040;85900&#20010;&#22478;&#24066;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#22240;&#38382;&#39064;&#31867;&#22411;&#32780;&#24322;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27714;&#35299;&#22120;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#38598;&#21512;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#21333;&#29420;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#38598;&#21512;&#24335;&#35774;&#32622;&#26159;&#21033;&#29992;&#20016;&#23500;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#38500;&#20102;EAX&#21644;LKH&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;EAX&#21644;&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#65288;MGA&#65289;&#30340;&#22810;&#20010;&#29256;&#26412;&#30340;&#28151;&#21512;&#31639;&#27861;&#12290;MGA&#21644;EAX&#30340;&#28151;&#21512;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#35299;&#20915;&#19968;&#20123;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#28151;&#21512;&#29256;&#26412;&#30340;&#38598;&#21512;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The travelling salesman problem (TSP) is one of the well-studied NP-hard problems in the literature. The state-of-the art inexact TSP solvers are the Lin-Kernighan-Helsgaun (LKH) heuristic and Edge Assembly crossover (EAX). A recent study suggests that EAX with restart mechanisms perform well on a wide range of TSP instances. However, this study is limited to 2,000 city problems. We study for problems ranging from 2,000 to 85,900. We see that the performance of the solver varies with the type of the problem. However, combining these solvers in an ensemble setup, we are able to outperform the individual solver's performance. We see the ensemble setup as an efficient way to make use of the abundance of compute resources. In addition to EAX and LKH, we use several versions of the hybrid of EAX and Mixing Genetic Algorithm (MGA). A hybrid of MGA and EAX is known to solve some hard problems. We see that the ensemble of the hybrid version outperforms the state-of-the-art solvers on problems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22269;&#23478;&#31726;&#29699;&#21327;&#20250;&#30340;&#36827;&#25915;&#25112;&#26415;&#35268;&#21010;&#12290;&#36890;&#36807;&#24314;&#31435;&#27169;&#22411;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#21487;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#25112;&#26415;&#25191;&#34892;&#30340;&#20855;&#20307;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2308.06851</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22269;&#23478;&#31726;&#29699;&#21327;&#20250;&#30340;&#36827;&#25915;&#25112;&#26415;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning. (arXiv:2308.06851v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22269;&#23478;&#31726;&#29699;&#21327;&#20250;&#30340;&#36827;&#25915;&#25112;&#26415;&#35268;&#21010;&#12290;&#36890;&#36807;&#24314;&#31435;&#27169;&#22411;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#21487;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#25112;&#26415;&#25191;&#34892;&#30340;&#20855;&#20307;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;NBA&#21457;&#29983;&#30340;&#20998;&#26512;&#38761;&#21629;&#20013;&#65292;&#29305;&#23450;&#30340;&#25351;&#26631;&#21644;&#20844;&#24335;&#30340;&#21457;&#23637;&#20026;&#29699;&#38431;&#12289;&#25945;&#32451;&#21644;&#29699;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#30475;&#24453;&#27604;&#36187;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65292;&#25105;&#20204;&#22914;&#20309;&#39564;&#35777;&#36825;&#20123;&#25351;&#26631;&#21602;&#65311;&#19968;&#31181;&#26041;&#27861;&#21487;&#33021;&#26159;&#31616;&#21333;&#22320;&#20973;&#30524;&#29699;&#25512;&#27979;&#65288;&#23581;&#35797;&#35768;&#22810;&#19981;&#21516;&#30340;&#25112;&#26415;&#35745;&#21010;&#65289;&#21644;/&#25110;&#35797;&#38169;&#27861;-&#19968;&#31181;&#20272;&#35745;&#24615;&#30340;&#26114;&#36149;&#26041;&#27861;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#23581;&#35797;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#24050;&#26377;&#25351;&#26631;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#29420;&#29305;&#30340;&#29305;&#24449;&#26469;&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#65292;&#36890;&#36807;&#36873;&#25321;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#23581;&#35797;&#35780;&#20272;&#36825;&#20123;&#29305;&#24449;&#30340;&#32452;&#21512;&#25928;&#26524;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20998;&#26512;&#31616;&#21333;&#30340;&#25351;&#26631;&#35780;&#20272;&#12290;&#22914;&#26524;&#25105;&#20204;&#26377;&#19968;&#20010;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#30830;&#23450;&#25112;&#26415;&#25191;&#34892;&#30340;&#20855;&#20307;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#32479;&#35745;&#25351;&#26631;ORTG&#65288;Dean Oliver&#24320;&#21457;&#30340;&#36827;&#25915;&#35780;&#20998;&#65289;&#21457;&#29616;&#19982;&#19981;&#21516;&#30340;NBA&#27604;&#36187;&#31867;&#22411;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout the analytical revolution that has occurred in the NBA, the development of specific metrics and formulas has given teams, coaches, and players a new way to see the game. However - the question arises - how can we verify any metrics? One method would simply be eyeball approximation (trying out many different gameplans) and/or trial and error - an estimation-based and costly approach. Another approach is to try to model already existing metrics with a unique set of features using machine learning techniques. The key to this approach is that with these features that are selected, we can try to gauge the effectiveness of these features combined, rather than using individual analysis in simple metric evaluation. If we have an accurate model, it can particularly help us determine the specifics of gameplan execution. In this paper, the statistic ORTG (Offensive Rating, developed by Dean Oliver) was found to have a correlation with different NBA playtypes using both a linear regress
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;(SER)&#25552;&#20986;&#20102;MSAC&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#39062;&#30340;CNN-based SER&#27169;&#22411;&#21644;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;MSAC&#65292;&#23454;&#29616;&#20102;&#23545;&#24773;&#24863;&#30340;&#26356;&#31934;&#32454;&#25511;&#21046;&#21644;&#25429;&#25417;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;SER&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04025</link><description>&lt;p&gt;
MSAC&#65306;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition. (arXiv:2308.04025v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;(SER)&#25552;&#20986;&#20102;MSAC&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#39062;&#30340;CNN-based SER&#27169;&#22411;&#21644;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;MSAC&#65292;&#23454;&#29616;&#20102;&#23545;&#24773;&#24863;&#30340;&#26356;&#31934;&#32454;&#25511;&#21046;&#21644;&#25429;&#25417;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;SER&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#24773;&#24863;&#23646;&#24615;&#30340;&#22797;&#26434;&#24615;&#21644;&#27495;&#20041;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#19979;&#65292;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32780;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;SER&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21508;&#31181;&#35821;&#38899;&#23646;&#24615;&#30340;&#25968;&#25454;&#20998;&#24067;&#26469;&#24314;&#27169;&#35821;&#38899;&#24773;&#24863;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;CNN&#30340;SER&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#21152;&#24615;&#36793;&#30028;&#26368;&#22823;&#21270;&#36719;&#20214;&#26368;&#22823;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#25193;&#22823;&#20102;&#19981;&#21516;&#31867;&#21035;&#29305;&#24449;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;MSAC&#65292;&#20197;&#26126;&#30830;&#25511;&#21046;&#35821;&#38899;&#23646;&#24615;&#65292;&#20351;&#27169;&#22411;&#21463;&#24773;&#24863;&#26080;&#20851;&#23646;&#24615;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#24182;&#25429;&#25417;&#21040;&#26356;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#30456;&#20851;&#29305;&#24449;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#27979;&#35797;&#21644;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;SER&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress, speech emotion recognition (SER) remains challenging due to inherent complexity and ambiguity of the emotion attribute, particularly in wild world. Whereas current studies primarily focus on recognition and generalization capabilities, this work pioneers an exploration into the reliability of SER methods and investigates how to model the speech emotion from the aspect of data distribution across various speech attributes. Specifically, we first build a novel CNN-based SER model which adopts additive margin softmax loss to expand the distance between features of different classes, thereby enhancing their discrimination. Second, a novel multiple speech attribute control method MSAC is proposed to explicitly control speech attributes, enabling the model to be less affected by emotion-agnostic attributes and capture more fine-grained emotion-related features. Third, we make a first attempt to test and analyze the reliability of the proposed SER workflow using 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#34892;&#20026;&#32463;&#27982;&#23398;&#35282;&#24230;&#65292;&#23545;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;AI&#23545;&#40784;&#20013;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;AI&#23433;&#20840;&#38382;&#39064;&#19981;&#20165;&#28041;&#21450;&#35774;&#35745;&#32773;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#36824;&#28041;&#21450;&#21040;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#19982;&#25928;&#29992;&#20989;&#25968;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;</title><link>http://arxiv.org/abs/2307.11137</link><description>&lt;p&gt;
&#27169;&#22411;&#19982;&#38177;&#20154;&#20043;&#38388;&#8212;&#8212;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;AI&#23545;&#40784;&#20013;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#30340;&#34892;&#20026;&#32463;&#27982;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Of Models and Tin Men -- a behavioural economics study of principal-agent problems in AI alignment using large-language models. (arXiv:2307.11137v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#34892;&#20026;&#32463;&#27982;&#23398;&#35282;&#24230;&#65292;&#23545;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;AI&#23545;&#40784;&#20013;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;AI&#23433;&#20840;&#38382;&#39064;&#19981;&#20165;&#28041;&#21450;&#35774;&#35745;&#32773;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#36824;&#28041;&#21450;&#21040;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#19982;&#25928;&#29992;&#20989;&#25968;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#19968;&#20010;&#35774;&#35745;&#32773;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35774;&#35745;&#32773;&#35797;&#22270;&#30830;&#20445;&#20195;&#29702;&#30340;&#34892;&#20026;&#19982;&#20854;&#30446;&#30340;&#19968;&#33268;&#65292;&#24182;&#19988;&#39118;&#38505;&#20165;&#20165;&#26159;&#30001;&#20110;&#35774;&#35745;&#32773;&#24847;&#22270;&#20013;&#30340;&#25928;&#29992;&#20989;&#25968;&#19982;&#20195;&#29702;&#30340;&#20869;&#37096;&#25928;&#29992;&#20989;&#25968;&#20043;&#38388;&#30340;&#24847;&#22806;&#38169;&#20301;&#32780;&#23548;&#33268;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#20363;&#21270;&#30340;&#20195;&#29702;&#30340;&#20986;&#29616;&#65292;&#36825;&#31181;&#25551;&#36848;&#19981;&#33021;&#25429;&#25417;&#21040;AI&#23433;&#20840;&#30340;&#26680;&#24515;&#26041;&#38754;&#65292;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#20013;&#35774;&#35745;&#32773;&#19982;&#20195;&#29702;&#20043;&#38388;&#24182;&#27809;&#26377;&#19968;&#23545;&#19968;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#32780;&#19988;&#35768;&#22810;&#20195;&#29702;&#65292;&#26080;&#35770;&#26159;&#20154;&#24037;&#26234;&#33021;&#36824;&#26159;&#20154;&#31867;&#65292;&#37117;&#20855;&#26377;&#22810;&#26679;&#30340;&#20215;&#20540;&#35266;&#12290;&#22240;&#27492;&#65292;AI&#23433;&#20840;&#20855;&#26377;&#32463;&#27982;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI Alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. With the advent of agents instantiated with large-language models (LLMs), which are typically pre-trained, we argue this does not capture the essential aspects of AI safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. Therefore, there is an economic aspect to AI safety and the principal-agent problem is likely to arise. In a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, an
&lt;/p&gt;</description></item><item><title>&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#20581;&#24247;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20116;&#32423;&#21457;&#23637;&#36335;&#32447;&#22270;&#65292;&#28085;&#30422;&#20102;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#25968;&#25454;&#25910;&#38598;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20915;&#31574;&#31995;&#32479;&#31561;&#32452;&#25104;&#37096;&#20998;&#30340;&#24320;&#21457;&#65292;&#24182;&#24378;&#35843;&#20102;&#25903;&#25345;&#12289;&#23433;&#20840;&#12289;&#25104;&#26412;&#21644;&#20262;&#29702;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09225</link><description>&lt;p&gt;
&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;: &#19968;&#20010;&#24635;&#20307;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Human Body Digital Twin: A Master Plan. (arXiv:2307.09225v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09225
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#20581;&#24247;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20116;&#32423;&#21457;&#23637;&#36335;&#32447;&#22270;&#65292;&#28085;&#30422;&#20102;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#25968;&#25454;&#25910;&#38598;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20915;&#31574;&#31995;&#32479;&#31561;&#32452;&#25104;&#37096;&#20998;&#30340;&#24320;&#21457;&#65292;&#24182;&#24378;&#35843;&#20102;&#25903;&#25345;&#12289;&#23433;&#20840;&#12289;&#25104;&#26412;&#21644;&#20262;&#29702;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;&#20855;&#26377;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#21644;&#20581;&#24247;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#36127;&#36131;&#21644;&#26377;&#25928;&#30340;&#23454;&#26045;&#38656;&#35201;&#32771;&#34385;&#21508;&#31181;&#22240;&#32032;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#21069;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20116;&#32423;&#21457;&#23637;&#36335;&#32447;&#22270;&#12290;&#36335;&#32447;&#22270;&#28085;&#30422;&#20102;&#21508;&#31181;&#32452;&#25104;&#37096;&#20998;&#30340;&#21457;&#23637;&#65292;&#22914;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#25968;&#25454;&#25910;&#38598;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20915;&#31574;&#31995;&#32479;&#12290;&#26412;&#25991;&#36824;&#24378;&#35843;&#20102;&#24517;&#39035;&#35299;&#20915;&#30340;&#25903;&#25345;&#12289;&#23433;&#20840;&#12289;&#25104;&#26412;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;&#30340;&#36127;&#36131;&#21644;&#26377;&#25928;&#23454;&#26045;&#12290;&#25152;&#25552;&#20986;&#30340;&#36335;&#32447;&#22270;&#20026;&#25351;&#23548;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#26694;&#26550;&#65292;&#24182;&#20026;&#25506;&#32034;&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;&#30340;&#26410;&#26469;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#20419;&#36827;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#20013;&#30340;&#26032;&#23398;&#31185;&#30740;&#31350;&#21644;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human body DT has the potential to revolutionize healthcare and wellness, but its responsible and effective implementation requires consideration of various factors. This article presents a comprehensive overview of the current status and future prospects of the human body DT and proposes a five-level roadmap for its development. The roadmap covers the development of various components, such as wearable devices, data collection, data analysis, and decision-making systems. The article also highlights the necessary support, security, cost, and ethical considerations that must be addressed in order to ensure responsible and effective implementation of the human body DT. The proposed roadmap provides a framework for guiding future development and offers a unique perspective on the future of the human body DT, facilitating new interdisciplinary research and innovative solutions in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17100</link><description>&lt;p&gt;
RL4CO: &#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark. (arXiv:2306.17100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17100
&lt;/p&gt;
&lt;p&gt;
RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;RL4CO&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;RL4CO&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#24211;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#22914;&#27169;&#22359;&#21270;&#21644;&#37197;&#32622;&#31649;&#29702;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#29615;&#22659;&#21644;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65289;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24378;&#35843;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#21508;&#31181;&#20248;&#21270;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#38646;-shot&#27867;&#21270;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#20351;&#29992;&#36825;&#20123;&#26032;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#26102;&#33853;&#21518;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#26356;&#21152;&#24179;&#34913;&#22320;&#35780;&#20272;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;RL4CO&#33021;&#22815;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse optimization tasks. We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these new metrics, suggesting the necessity for a more balanced view of the performance of neural CO solvers. We hope RL4CO will 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;Gradient-Informed Discrete Emitter (ME-GIDE)&#30340;Map-Elites&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;&#31163;&#25955;&#31354;&#38388;&#30340;&#25628;&#32034;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#22312;&#31163;&#25955;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05138</link><description>&lt;p&gt;
&#31163;&#25955;&#31354;&#38388;&#30340;&#26799;&#24230;&#20449;&#24687;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gradient-Informed Quality Diversity for the Illumination of Discrete Spaces. (arXiv:2306.05138v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;Gradient-Informed Discrete Emitter (ME-GIDE)&#30340;Map-Elites&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;&#31163;&#25955;&#31354;&#38388;&#30340;&#25628;&#32034;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#22312;&#31163;&#25955;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#26088;&#22312;&#25628;&#32034;&#22823;&#37327;&#21508;&#24322;&#19988;&#24615;&#33021;&#20248;&#36234;&#30340;&#35299;&#38598;&#65292;&#32780;&#19981;&#26159;&#19968;&#32452;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#23613;&#31649;&#26089;&#26399;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#23558;&#30446;&#26631;&#21644;&#25551;&#36848;&#31526;&#20989;&#25968;&#35270;&#20026;&#40657;&#30418;&#20989;&#25968;&#65292;&#20294;&#24341;&#20837;&#20102;&#26032;&#24037;&#20855;&#20197;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#65292;&#21152;&#36895;&#25628;&#32034;&#24182;&#25552;&#39640;&#36825;&#20123;&#31639;&#27861;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#30340;&#24212;&#29992;&#28041;&#21450;&#31163;&#25955;&#31354;&#38388;&#65292;&#20363;&#22914;&#33647;&#29289;&#21457;&#29616;&#25110;&#22270;&#20687;&#29983;&#25104;&#12290;&#25506;&#32034;&#36825;&#20123;&#31354;&#38388;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#32452;&#21512;&#35268;&#27169;&#24456;&#22823;&#65292;&#24182;&#19988;&#19981;&#33021;&#20687;&#36830;&#32493;&#31354;&#38388;&#37027;&#26679;&#20351;&#29992;&#26799;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#26799;&#24230;&#20449;&#24687;&#30340;&#31163;&#25955;&#21457;&#23556;&#22120;&#30340; Map-Elites&#65288;ME-GIDE&#65289;&#25193;&#23637;&#20102;&#23545;&#31163;&#25955;&#25628;&#32034;&#31354;&#38388;&#30340; QD &#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#30446;&#26631;&#21644;&#25551;&#36848;&#31526;&#20989;&#25968;&#30456;&#23545;&#20110;&#20854;&#31163;&#25955;&#36755;&#20837;&#30340;&#26799;&#24230;&#20449;&#24687;&#26469;&#25552;&#20986;&#26799;&#24230;&#20449;&#24687;&#36873;&#25321;&#30340;&#35299;&#38598;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123;&#31163;&#25955;&#22522;&#20934;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102; ME-GIDE&#65292;&#35777;&#26126;&#23427;&#20248;&#20110;&#23558;&#25628;&#32034;&#31354;&#38388;&#35270;&#20026;&#40657;&#30418;&#30340;&#32463;&#20856;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality Diversity (QD) algorithms have been proposed to search for a large collection of both diverse and high-performing solutions instead of a single set of local optima. While early QD algorithms view the objective and descriptor functions as black-box functions, novel tools have been introduced to use gradient information to accelerate the search and improve overall performance of those algorithms over continuous input spaces. However a broad range of applications involve discrete spaces, such as drug discovery or image generation. Exploring those spaces is challenging as they are combinatorially large and gradients cannot be used in the same manner as in continuous spaces. We introduce map-elites with a Gradient-Informed Discrete Emitter (ME-GIDE), which extends QD optimisation with differentiable functions over discrete search spaces. ME-GIDE leverages the gradient information of the objective and descriptor functions with respect to its discrete inputs to propose gradient-inform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#36825;&#20123;&#24187;&#35273;&#12290;&#24182;&#19988;&#36890;&#36807;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#26597;&#35810;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21442;&#32771;&#25991;&#29486;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.18248</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30693;&#36947;&#33258;&#24049;&#22312;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Know When They're Hallucinating References?. (arXiv:2305.18248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#36825;&#20123;&#24187;&#35273;&#12290;&#24182;&#19988;&#36890;&#36807;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#26597;&#35810;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21442;&#32771;&#25991;&#29486;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#32780;&#38395;&#21517;&#12290;&#36825;&#20123;&#34394;&#26500;&#30340;&#25991;&#31456;&#21644;&#20070;&#21517;&#24341;&#36215;&#20102;&#21361;&#23475;&#65292;&#23545;&#23427;&#20204;&#30340;&#20351;&#29992;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#24182;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#21453;&#24377;&#12290;&#23613;&#31649;&#20854;&#20182;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#20063;&#24456;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#23558;&#24187;&#35273;&#21442;&#32771;&#25991;&#29486;&#25552;&#20986;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#24187;&#35273;&#30740;&#31350;&#30340;&#8220;&#26524;&#34631;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#29305;&#21035;&#23481;&#26131;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#27492;&#31867;&#24187;&#35273;&#65292;&#20174;&#32780;&#20415;&#20110;&#35780;&#20272;&#12290;&#20026;&#20102;&#24320;&#22987;&#21078;&#26512;&#24187;&#35273;&#35821;&#35328;&#27169;&#22411;&#21442;&#32771;&#25991;&#29486;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#26597;&#35810;&#26469;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#20511;&#21161;&#20219;&#20309;&#22806;&#37096;&#36164;&#28304;&#12290;&#25105;&#20204;&#23558;&#8220;&#30452;&#25509;&#8221;&#26597;&#35810;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#19982;&#8220;&#38388;&#25509;&#8221;&#26597;&#35810;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21518;&#32773;&#35810;&#38382;&#20102;&#38468;&#21152;&#30340;&#32454;&#33410;&#65292;&#22914;&#20316;&#21697;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art language models (LMs) are famous for "hallucinating" references. These fabricated article and book titles lead to harms, obstacles to their use, and public backlash. While other types of LM hallucinations are also important, we propose hallucinated references as the "drosophila" of research on hallucination in large language models (LLMs), as they are particularly easy to study. We show that simple search engine queries reliably identify such hallucinations, which facilitates evaluation. To begin to dissect the nature of hallucinated LM references, we attempt to classify them using black-box queries to the same LM, without consulting any external resources. Consistency checks done with "direct" queries about whether the generated reference title is real (inspired by Kadavath et al. 2022, Lin et al. 2022, Manakul et al. 2023) are compared to consistency checks with "indirect" queries which ask for ancillary details such as the authors of the work. These consistency chec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#24515;&#26234;&#29702;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#29256;&#26412;&#30340;ChatGPT&#22312;&#20960;&#20010;&#32463;&#20856;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT-4&#27604;&#38543;&#26426;&#31572;&#26696;&#32473;&#20986;&#20102;&#26356;&#22810;&#27491;&#30830;&#31572;&#26696;&#65292;&#23613;&#31649;&#36825;&#20123;&#31572;&#26696;&#24448;&#24448;&#22522;&#20110;&#38169;&#35823;&#30340;&#20551;&#35774;&#25110;&#26080;&#25928;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.14020</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#20855;&#26377;&#24515;&#26234;&#29702;&#35770;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does ChatGPT have Theory of Mind?. (arXiv:2305.14020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#24515;&#26234;&#29702;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#29256;&#26412;&#30340;ChatGPT&#22312;&#20960;&#20010;&#32463;&#20856;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT-4&#27604;&#38543;&#26426;&#31572;&#26696;&#32473;&#20986;&#20102;&#26356;&#22810;&#27491;&#30830;&#31572;&#26696;&#65292;&#23613;&#31649;&#36825;&#20123;&#31572;&#26696;&#24448;&#24448;&#22522;&#20110;&#38169;&#35823;&#30340;&#20551;&#35774;&#25110;&#26080;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#29702;&#35299;&#20154;&#31867;&#24605;&#32500;&#21644;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;&#36825;&#31181;&#33021;&#21147;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#35821;&#35328;&#20132;&#27969;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#36817;ChatGPT&#31995;&#21015;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20855;&#22791;ToM&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#29256;&#26412;&#30340;ChatGPT&#25552;&#20986;&#20102;&#20845;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#20154;&#31867;&#25512;&#29702;&#21644;&#20915;&#31574;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#22312;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#19979;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#34429;&#28982;&#20851;&#20110;ChatGPT-3&#30340;&#32467;&#26524;&#26377;&#20123;&#19981;&#30830;&#23450;&#65292;&#20294;ChatGPT-4&#34987;&#35777;&#26126;&#27604;&#39044;&#26399;&#26356;&#32463;&#24120;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#23613;&#31649;&#27491;&#30830;&#31572;&#26696;&#24448;&#24448;&#26159;&#22522;&#20110;&#38169;&#35823;&#30340;&#20551;&#35774;&#25110;&#26080;&#25928;&#30340;&#25512;&#29702;&#24471;&#20986;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM) is the ability to understand human thinking and decision-making, an ability that plays a crucial role in social interaction between people, including linguistic communication. This paper investigates to what extent recent Large Language Models in the ChatGPT tradition possess ToM. We posed six well-known problems that address biases in human reasoning and decision making to two versions of ChatGPT and we compared the results under a range of prompting strategies. While the results concerning ChatGPT-3 were somewhat inconclusive, ChatGPT-4 was shown to arrive at the correct answers more often than would be expected based on chance, although correct answers were often arrived at on the basis of false assumptions or invalid reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06695</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#29992;&#20110;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species. (arXiv:2305.06695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#65292;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#36890;&#24120;&#29992;&#20110;&#35782;&#21035;&#29289;&#31181;&#21644;&#20010;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#19978;&#22686;&#24378;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#31232;&#26377;&#31867;&#21035;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#38754;&#65292;&#35813;&#39046;&#22495;&#23578;&#26410;&#36827;&#34892;&#23581;&#35797;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#65292;&#26088;&#22312;&#38544;&#24335;&#32534;&#30721;&#36328;&#22495;&#20851;&#32852;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36825;&#31181;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#30452;&#25509;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65288;LTR&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20110;32&#20010;&#29289;&#31181;&#12289;&#36229;&#36807;30,000&#20010;&#28014;&#28216;&#26377;&#23380;&#34411;&#22771;&#30340;&#26174;&#24494;&#22270;&#20687;&#24182;&#19982;&#29420;&#31435;&#30340;&#36951;&#20256;&#25968;&#25454;&#26679;&#26412;&#19968;&#36215;&#20351;&#29992;&#26469;&#23454;&#39564;&#23460;&#23637;&#29616;&#20102;&#35813;&#27010;&#24565;&#30340;&#25928;&#21147;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23545;&#20174;&#19994;&#32773;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35270;&#35273;-&#36951;&#20256;&#23545;&#40784;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic inference spaces with the aim to implicitly encode cross-domain associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR) particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest speci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;TSformer-VO&#26041;&#27861;&#65292;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#24182;&#36890;&#36807;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#20272;&#35745;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06121</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65306;&#19968;&#31181;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transformer-based model for monocular visual odometry: a video understanding approach. (arXiv:2305.06121v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;TSformer-VO&#26041;&#27861;&#65292;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#24182;&#36890;&#36807;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#20272;&#35745;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#36710;&#36742;&#20013;&#65292;&#32473;&#23450;&#21333;&#20010;&#25668;&#20687;&#26426;&#22270;&#20687;&#20272;&#35745;&#25668;&#20687;&#26426;&#23039;&#21183;&#26159;&#19968;&#39033;&#20256;&#32479;&#20219;&#21153;&#12290;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#22330;&#26223;&#36827;&#34892;&#24037;&#31243;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;&#32463;&#36807;&#36866;&#24403;&#35757;&#32451;&#21644;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#12290;Transformer&#26550;&#26500;&#24050;&#32479;&#27835;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26368;&#21069;&#27839;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#12290;&#26412;&#25991;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#20272;&#35745;6-DoF&#25668;&#20687;&#26426;&#30340;&#23039;&#21183;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;TSformer-VO&#27169;&#22411;&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#20272;&#35745;&#36816;&#21160;&#65292;&#19982;&#20960;&#20309;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the camera pose given images of a single camera is a traditional task in mobile robots and autonomous vehicles. This problem is called monocular visual odometry and it often relies on geometric approaches that require engineering effort for a specific scenario. Deep learning methods have shown to be generalizable after proper training and a considerable amount of available data. Transformer-based architectures have dominated the state-of-the-art in natural language processing and computer vision tasks, such as image and video understanding. In this work, we deal with the monocular visual odometry as a video understanding task to estimate the 6-DoF camera's pose. We contribute by presenting the TSformer-VO model based on spatio-temporal self-attention mechanisms to extract features from clips and estimate the motions in an end-to-end manner. Our approach achieved competitive state-of-the-art performance compared with geometry-based and deep learning-based methods on the KITTI
&lt;/p&gt;</description></item><item><title>LaCViT&#26159;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#30340;&#21508;&#21521;&#31561;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#39640;&#20854;&#34920;&#31034;&#31354;&#38388;&#31561;&#24615;&#30340;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20116;&#20010;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18013</link><description>&lt;p&gt;
LaCViT&#65306;&#19968;&#31181;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550;&#65292;&#25552;&#39640;&#35270;&#35273;Transformer&#30340;&#34920;&#31034;&#31354;&#38388;&#30340;&#31561;&#24615;
&lt;/p&gt;
&lt;p&gt;
LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers. (arXiv:2303.18013v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18013
&lt;/p&gt;
&lt;p&gt;
LaCViT&#26159;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#30340;&#21508;&#21521;&#31561;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#39640;&#20854;&#34920;&#31034;&#31354;&#38388;&#31561;&#24615;&#30340;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20116;&#20010;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273; Transformer &#24050;&#32463;&#22312;&#22788;&#29702;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#27169;&#25311;&#38271;&#26102;&#38388;&#30340;&#29305;&#24449;&#20381;&#36182;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#21508;&#31181;&#33258;&#30417;&#30563;&#20449;&#21495;&#65288;&#20363;&#22914;&#65292;&#36974;&#34109;&#38543;&#26426;&#22359;&#65289;&#65292;&#35270;&#35273; Transformer &#22312; ImageNet-1k &#21644; CIFAR-10 &#31561;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#36890;&#29992;&#22823;&#35268;&#27169;&#22270;&#20687;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21482;&#33021;&#20135;&#29983;&#21508;&#21521;&#24322;&#24615;&#34920;&#31034;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550; LaCViT&#65292;&#23427;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#30340;&#31561;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LaCViT&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers have been incredibly effective when tackling computer vision tasks due to their ability to model long feature dependencies. By using large-scale training data and various self-supervised signals (e.g., masked random patches), vision transformers provide state-of-the-art performance on several benchmarking datasets, such as ImageNet-1k and CIFAR-10. However, these vision transformers pretrained over general large-scale image corpora could only produce an anisotropic representation space, limiting their generalizability and transferability to the target downstream tasks. In this paper, we propose a simple and effective Label-aware Contrastive Training framework LaCViT, which improves the isotropy of the pretrained representation space for vision transformers, thereby enabling more effective transfer learning amongst a wide range of image classification tasks. Through experimentation over five standard image classification datasets, we demonstrate that LaCViT-trained m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#21644; Transformer &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26426;&#21046;&#20197;&#23454;&#29616;&#36136;&#37327;&#19968;&#33268;&#30340;&#29983;&#25104;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2303.16207</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#21464;&#24418;&#22120;&#65306;&#22522;&#20110;&#20915;&#31574;Transformer&#29983;&#25104;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
The Quality-Diversity Transformer: Generating Behavior-Conditioned Trajectories with Decision Transformers. (arXiv:2303.16207v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#21644; Transformer &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26426;&#21046;&#20197;&#23454;&#29616;&#36136;&#37327;&#19968;&#33268;&#30340;&#29983;&#25104;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#36827;&#21270;&#35745;&#31639;&#30340;&#32972;&#26223;&#19979;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#36890;&#36807;&#20381;&#36182;&#34892;&#20026;&#31354;&#38388;&#30340;&#23450;&#20041;&#26469;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#21644;&#39640;&#25928;&#30340;&#31574;&#30053;&#38598;&#21512;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#65292;&#20250;&#26377;&#20004;&#20010;&#38382;&#39064;&#20986;&#29616;&#12290;&#31532;&#19968;&#65292;&#31574;&#30053;&#21487;&#33021;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#21363;&#22312;&#30053;&#24494;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#20010; episodes &#24448;&#24448;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#34892;&#20026;&#32467;&#26524;&#12290;&#31532;&#20108;&#65292;&#30001;&#20110;&#31574;&#30053;&#38598;&#30340;&#31163;&#25955;&#24615;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#21464;&#21270;&#26159;&#19981;&#36830;&#32493;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22522;&#20110;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;&#29983;&#25104;&#65292;&#20854;&#22522;&#20110;&#20004;&#20010;&#26426;&#21046;&#65306;&#39318;&#20808;&#26159; MAP-Elites Low-Spread (ME-LS)&#65292;&#23427;&#38480;&#21046;&#20102;&#36873;&#25321;&#37027;&#20123;&#22312;&#34892;&#20026;&#31354;&#38388;&#19978;&#26368;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#27425;&#26159;&#36136;&#37327;&#22810;&#26679;&#24615;&#21464;&#24418;&#22120; (QDT)&#65292;&#23427;&#26159;&#22522;&#20110; Transformer &#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of neuroevolution, Quality-Diversity algorithms have proven effective in generating repertoires of diverse and efficient policies by relying on the definition of a behavior space. A natural goal induced by the creation of such a repertoire is trying to achieve behaviors on demand, which can be done by running the corresponding policy from the repertoire. However, in uncertain environments, two problems arise. First, policies can lack robustness and repeatability, meaning that multiple episodes under slightly different conditions often result in very different behaviors. Second, due to the discrete nature of the repertoire, solutions vary discontinuously. Here we present a new approach to achieve behavior-conditioned trajectory generation based on two mechanisms: First, MAP-Elites Low-Spread (ME-LS), which constrains the selection of solutions to those that are the most consistent in the behavior space. Second, the Quality-Diversity Transformer (QDT), a Transformer-based 
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#29992;&#20316;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20316;&#32773;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16203</link><description>&lt;p&gt;
&#24744;&#30340;&#25193;&#25955;&#27169;&#22411;&#26263;&#20013;&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Your Diffusion Model is Secretly a Zero-Shot Classifier. (arXiv:2303.16203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16203
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#29992;&#20316;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20316;&#32773;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20026;&#22823;&#37327;&#25552;&#31034;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#29992;&#20363;&#21040;&#30446;&#21069;&#20026;&#27490;&#37117;&#21482;&#20851;&#27880;&#25277;&#26679;&#65292;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#36824;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#30340;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;Stable Diffusion&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#25191;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20248;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25105;&#20204;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#24615;&#30340;&#23545;&#27604;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21576;&#29616;&#20102;&#23450;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#23427;&#23398;&#20064;&#20102;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. We also find that our diffusion-based approach has stronger multimodal relational reasoning abilities than competing contrastive approaches. Finally, we eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07267</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#23398;&#20013;&#30340;&#8220;&#27491;&#30830;&#31572;&#26696;&#8221;
&lt;/p&gt;
&lt;p&gt;
"Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#12290;&#36825;&#31181;AI&#31995;&#32479;&#30340;&#19968;&#20010;&#25552;&#20986;&#30340;&#24212;&#29992;&#26159;&#25903;&#25345;&#31038;&#20250;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#30446;&#21069;&#23436;&#32654;&#30340;&#23454;&#39564;&#25511;&#21046;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#22823;&#35268;&#27169;&#12289;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65288;&#20439;&#31216;GPT3.5&#65289;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#39033;&#30740;&#31350;&#30340;&#35843;&#26597;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;GPT3.5&#30340;&#40664;&#35748;&#35774;&#32622;&#20013;&#25910;&#38598;&#20102;&#21709;&#24212;&#12290;&#22312;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#30340;&#20843;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;GPT&#26679;&#26412;&#22797;&#21046;&#20102;&#21407;&#22987;&#32467;&#26524;&#30340;37.5%&#20197;&#21450;Many Labs 2&#32467;&#26524;&#30340;37.5%&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#26080;&#27861;&#20687;&#39044;&#20808;&#27880;&#20876;&#30340;&#35745;&#21010;&#37027;&#26679;&#20998;&#26512;&#21097;&#19979;&#30340;&#20845;&#39033;&#30740;&#31350;&#12290;&#36825;&#26159;&#22240;&#20026;&#23545;&#20110;&#36825;&#20845;&#39033;&#30740;&#31350;&#20013;&#30340;&#27599;&#19968;&#39033;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65288;&#26080;&#35770;&#26159;&#22240;&#21464;&#37327;&#36824;&#26159;&#26465;&#20214;&#21464;&#37327;&#65289;&#65306;&#19968;&#20010;&#26410;&#30693;&#30340;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
&lt;/p&gt;</description></item><item><title>DWRSeg&#37325;&#26032;&#24605;&#32771;&#20102;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20013;&#33719;&#21462;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#21333;&#27493;&#26041;&#27861;&#20998;&#35299;&#20026;&#21306;&#22495;&#27531;&#20313;&#21270;&#21644;&#35821;&#20041;&#27531;&#20313;&#21270;&#20004;&#20010;&#27493;&#39588;&#65292;&#31616;&#21270;&#20102;&#22810;&#36895;&#29575;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#30340;&#35282;&#33394;&#65292;&#24182;&#25552;&#39640;&#20102;&#29305;&#24449;&#25552;&#21462;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.01173</link><description>&lt;p&gt;
DWRSeg:&#37325;&#26032;&#24605;&#32771;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20013;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#39640;&#25928;&#33719;&#21462;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
DWRSeg: Rethinking Efficient Acquisition of Multi-scale Contextual Information for Real-time Semantic Segmentation. (arXiv:2212.01173v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01173
&lt;/p&gt;
&lt;p&gt;
DWRSeg&#37325;&#26032;&#24605;&#32771;&#20102;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20013;&#33719;&#21462;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#21333;&#27493;&#26041;&#27861;&#20998;&#35299;&#20026;&#21306;&#22495;&#27531;&#20313;&#21270;&#21644;&#35821;&#20041;&#27531;&#20313;&#21270;&#20004;&#20010;&#27493;&#39588;&#65292;&#31616;&#21270;&#20102;&#22810;&#36895;&#29575;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#30340;&#35282;&#33394;&#65292;&#24182;&#25552;&#39640;&#20102;&#29305;&#24449;&#25552;&#21462;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35768;&#22810;&#26041;&#27861;&#30452;&#25509;&#37319;&#29992;&#22810;&#36895;&#29575;&#30340;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#26469;&#21516;&#26102;&#20174;&#19968;&#20010;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#20013;&#25429;&#33719;&#22810;&#23610;&#24230;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#30340;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#35745;&#21487;&#33021;&#30001;&#20110;&#19981;&#21512;&#29702;&#30340;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#32780;&#23548;&#33268;&#38590;&#20197;&#35775;&#38382;&#22810;&#23610;&#24230;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#38477;&#20302;&#33719;&#21462;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#38590;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#30340;&#21333;&#27493;&#26041;&#27861;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#21306;&#22495;&#27531;&#20313;&#21270;&#21644;&#35821;&#20041;&#27531;&#20313;&#21270;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#22810;&#36895;&#29575;&#30340;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#22312;&#29305;&#24449;&#25552;&#21462;&#26041;&#38754;&#25198;&#28436;&#20102;&#26356;&#31616;&#21333;&#30340;&#35282;&#33394;&#65306;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#26681;&#25454;&#31532;&#19968;&#27493;&#25552;&#20379;&#30340;&#27599;&#20010;&#31616;&#27905;&#30340;&#21306;&#22495;&#24418;&#24335;&#29305;&#24449;&#26144;&#23556;&#65292;&#23545;&#27599;&#20010;&#26399;&#26395;&#30340;&#24863;&#21463;&#37326;&#36827;&#34892;&#31616;&#21333;&#30340;&#22522;&#20110;&#35821;&#20041;&#30340;&#24418;&#24577;&#28388;&#27874;&#65292;&#20197;&#25552;&#39640;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many current works directly adopt multi-rate depth-wise dilated convolutions to capture multi-scale contextual information simultaneously from one input feature map, thus improving the feature extraction efficiency for real-time semantic segmentation. However, this design may lead to difficult access to multi-scale contextual information because of the unreasonable structure and hyperparameters. To lower the difficulty of drawing multi-scale contextual information, we propose a highly efficient multi-scale feature extraction method, which decomposes the original single-step method into two steps, Region Residualization-Semantic Residualization.In this method, the multi-rate depth-wise dilated convolutions take a simpler role in feature extraction: performing simple semantic-based morphological filtering with one desired receptive field in the second step based on each concise feature map of region form provided by the first step, to improve their efficiency. Moreover, the dilation rate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20154;&#26426;&#20849;&#29983;&#33402;&#26415;&#21019;&#20316;&#30340;&#21457;&#23637;&#36335;&#24452;&#65292;&#20174;&#27169;&#20223;&#20154;&#31867;&#33402;&#26415;&#23478;&#36880;&#28176;&#21457;&#23637;&#20026;&#20855;&#22791;&#29420;&#31435;&#21019;&#20316;&#33021;&#21147;&#30340;&#26426;&#22120;&#33402;&#26415;&#23478;&#12290;&#22312;&#36825;&#26465;&#36335;&#24452;&#19978;&#65292;&#38656;&#35201;&#26426;&#22120;&#29702;&#35299;&#20154;&#31867;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#27785;&#28024;&#24335;&#29615;&#22659;&#21644;&#20803;&#23431;&#23449;&#30340;&#21457;&#23637;&#23454;&#29616;&#21452;&#21521;&#27807;&#36890;&#65292;&#20197;&#23454;&#29616;&#20849;&#29983;&#33402;&#26415;&#21019;&#20316;&#12290;</title><link>http://arxiv.org/abs/2209.02388</link><description>&lt;p&gt;
&#26410;&#26469;&#20849;&#29983;&#21019;&#36896;&#30340;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Pathway to Future Symbiotic Creativity. (arXiv:2209.02388v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20154;&#26426;&#20849;&#29983;&#33402;&#26415;&#21019;&#20316;&#30340;&#21457;&#23637;&#36335;&#24452;&#65292;&#20174;&#27169;&#20223;&#20154;&#31867;&#33402;&#26415;&#23478;&#36880;&#28176;&#21457;&#23637;&#20026;&#20855;&#22791;&#29420;&#31435;&#21019;&#20316;&#33021;&#21147;&#30340;&#26426;&#22120;&#33402;&#26415;&#23478;&#12290;&#22312;&#36825;&#26465;&#36335;&#24452;&#19978;&#65292;&#38656;&#35201;&#26426;&#22120;&#29702;&#35299;&#20154;&#31867;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#27785;&#28024;&#24335;&#29615;&#22659;&#21644;&#20803;&#23431;&#23449;&#30340;&#21457;&#23637;&#23454;&#29616;&#21452;&#21521;&#27807;&#36890;&#65292;&#20197;&#23454;&#29616;&#20849;&#29983;&#33402;&#26415;&#21019;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25552;&#20986;&#20102;&#25105;&#20204;&#23545;&#20154;&#26426;&#20849;&#29983;&#33402;&#26415;&#21019;&#20316;&#21457;&#23637;&#36335;&#24452;&#30340;&#20840;&#38754;&#35266;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#36896;&#31995;&#32479;&#30340;&#20998;&#31867;&#65292;&#20854;&#20013;&#21253;&#25324;5&#20010;&#23618;&#32423;&#65292;&#23637;&#31034;&#20102;&#21019;&#36896;&#21147;&#20174;&#27169;&#20223;&#20154;&#31867;&#33402;&#26415;&#23478;&#65288;&#22270;&#28789;&#33402;&#26415;&#23478;&#65289;&#36880;&#28176;&#21457;&#23637;&#20026;&#20855;&#22791;&#29420;&#31435;&#21019;&#20316;&#33021;&#21147;&#30340;&#26426;&#22120;&#33402;&#26415;&#23478;&#30340;&#36335;&#24452;&#12290;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#22270;&#28789;&#33402;&#26415;&#23478;&#30340;&#23616;&#38480;&#24615;&#65292;&#28982;&#21518;&#37325;&#28857;&#20851;&#27880;&#20102;&#39030;&#23618;&#30340;&#20004;&#32423;&#31995;&#32479;&#65292;&#26426;&#22120;&#33402;&#26415;&#23478;&#65292;&#24378;&#35843;&#20102;&#26426;&#22120;&#19982;&#20154;&#31867;&#22312;&#33402;&#26415;&#21019;&#20316;&#20013;&#30340;&#20132;&#27969;&#12290;&#22312;&#33402;&#26415;&#21019;&#20316;&#20013;&#65292;&#26426;&#22120;&#38656;&#35201;&#29702;&#35299;&#20154;&#31867;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#21253;&#25324;&#27442;&#26395;&#12289;&#27427;&#36175;&#21644;&#24773;&#24863;&#65292;&#32780;&#20154;&#31867;&#20063;&#38656;&#35201;&#29702;&#35299;&#26426;&#22120;&#30340;&#21019;&#36896;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#27785;&#28024;&#24335;&#29615;&#22659;&#30340;&#24555;&#36895;&#21457;&#23637;&#20197;&#21450;&#36827;&#19968;&#27493;&#28436;&#21464;&#25104;&#20803;&#23431;&#23449;&#30340;&#26032;&#27010;&#24565;&#65292;&#20026;&#33402;&#26415;&#23478;&#19982;&#33402;&#26415;&#34920;&#29616;&#29615;&#22659;&#20043;&#38388;&#30340;&#21452;&#21521;&#27807;&#36890;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#28789;&#27963;&#24615;&#65292;&#23454;&#29616;&#20102;&#20849;&#29983;&#33402;&#26415;&#21019;&#20316;&#12290;&#36890;&#36807;&#30740;&#31350;&#26368;&#26032;&#30340;...
&lt;/p&gt;
&lt;p&gt;
This report presents a comprehensive view of our vision on the development path of the human-machine symbiotic art creation. We propose a classification of the creative system with a hierarchy of 5 classes, showing the pathway of creativity evolving from a mimic-human artist (Turing Artists) to a Machine artist in its own right. We begin with an overview of the limitations of the Turing Artists then focus on the top two-level systems, Machine Artists, emphasizing machine-human communication in art creation. In art creation, it is necessary for machines to understand humans' mental states, including desires, appreciation, and emotions, humans also need to understand machines' creative capabilities and limitations. The rapid development of immersive environment and further evolution into the new concept of metaverse enable symbiotic art creation through unprecedented flexibility of bi-directional communication between artists and art manifestation environments. By examining the latest se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35270;&#35282;RGBD&#35270;&#35273;&#21487;&#31649;&#29702;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;37&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;47210&#20010;&#22270;&#20687;&#65292;&#24182;&#19988;&#27599;&#20010;&#22270;&#20687;&#37117;&#24102;&#26377;15&#20010;&#35270;&#35273;&#21487;&#31649;&#29702;&#31867;&#21035;&#30340;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2203.14092</link><description>&lt;p&gt;
&#21521;&#35270;&#35273;&#21487;&#31649;&#29702;&#23398;&#20064;&#36808;&#36827;&#65306;&#19968;&#20010;&#29992;&#20110;&#21487;&#31649;&#29702;&#20998;&#21106;&#21644;&#35782;&#21035;&#30340;&#22522;&#20934;&#12290; (arXiv:2203.14092v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Towards Visual Affordance Learning: A Benchmark for Affordance Segmentation and Recognition. (arXiv:2203.14092v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35270;&#35282;RGBD&#35270;&#35273;&#21487;&#31649;&#29702;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;37&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;47210&#20010;&#22270;&#20687;&#65292;&#24182;&#19988;&#27599;&#20010;&#22270;&#20687;&#37117;&#24102;&#26377;15&#20010;&#35270;&#35273;&#21487;&#31649;&#29702;&#31867;&#21035;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#35782;&#21035;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#20219;&#21153;&#65292;&#23545;&#35937;&#30340;&#29289;&#29702;&#21644;&#32441;&#29702;&#23646;&#24615;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#35768;&#22810;&#25968;&#25454;&#38598;&#65292;&#22914;&#22823;&#35268;&#27169;&#30340;ImageNet&#65292;&#24050;&#32463;&#34987;&#25552;&#20986;&#29992;&#20110;&#20351;&#29992;&#25968;&#25454;&#39269;&#39295;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#21644;&#25163;&#24037;&#29305;&#24449;&#25552;&#21462;&#12290;&#20026;&#20102;&#26234;&#33021;&#22320;&#19982;&#23545;&#35937;&#36827;&#34892;&#20132;&#20114;&#65292;&#26426;&#22120;&#20154;&#21644;&#26234;&#33021;&#26426;&#22120;&#38656;&#35201;&#38500;&#20102;&#20256;&#32479;&#30340;&#29289;&#29702;/&#32441;&#29702;&#23646;&#24615;&#20043;&#22806;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#21450;&#29702;&#35299;/&#23398;&#20064;&#34987;&#31216;&#20026;&#35270;&#35273;&#21487;&#31649;&#29702;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#29992;&#20110;&#21487;&#31649;&#29702;&#30340;&#35782;&#21035;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#35270;&#35273;&#21487;&#31649;&#29702;&#29702;&#35299;&#21644;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35270;&#35282;RGBD&#35270;&#35273;&#21487;&#31649;&#29702;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;37&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;47210&#20010;RGBD&#22270;&#20687;&#30340;&#22522;&#20934;&#65292;&#27599;&#20010;&#22270;&#20687;&#37117;&#24102;&#26377;15&#20010;&#35270;&#35273;&#21487;&#31649;&#29702;&#31867;&#21035;&#30340;&#27880;&#37322;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#20010;&#20063;&#26159;&#26368;&#22823;&#30340;&#22810;&#35270;&#35282;RGBD&#35270;&#35273;&#21487;&#31649;&#29702;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The physical and textural attributes of objects have been widely studied for recognition, detection and segmentation tasks in computer vision.~A number of datasets, such as large scale ImageNet, have been proposed for feature learning using data hungry deep neural networks and for hand-crafted feature extraction. To intelligently interact with objects, robots and intelligent machines need the ability to infer beyond the traditional physical/textural attributes, and understand/learn visual cues, called visual affordances, for affordance recognition, detection and segmentation. To date there is no publicly available large dataset for visual affordance understanding and learning. In this paper, we introduce a large scale multi-view RGBD visual affordance learning dataset, a benchmark of 47210 RGBD images from 37 object categories, annotated with 15 visual affordance categories. To the best of our knowledge, this is the first ever and the largest multi-view RGBD visual affordance learning 
&lt;/p&gt;</description></item></channel></rss>