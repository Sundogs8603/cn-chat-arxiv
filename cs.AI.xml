<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#22987;&#32456;&#20934;&#30830;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13981</link><description>&lt;p&gt;
&#20445;&#25345;&#30693;&#35782;&#19981;&#21464;&#24615;&#65306;&#37325;&#26032;&#24605;&#32771;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction. (arXiv:2305.13981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#22987;&#32456;&#20934;&#30830;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#26159;&#30830;&#20445;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#32780;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#37117;&#19987;&#27880;&#20110;&#39564;&#35777;&#37197;&#23545;&#21305;&#37197;&#30340;&#27491;&#30830;&#24615;&#65292;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#40065;&#26834;&#24615;&#27979;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#27169;&#25311;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#21516;&#19968;&#30693;&#35782;&#21547;&#20041;&#30340;&#21477;&#27861;&#21644;&#34920;&#36798;&#20998;&#24067;&#20250;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#35774;&#35745;&#21644;&#27880;&#37322;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#20854;&#20013;&#27599;&#20010;&#31034;&#20363;&#37117;&#26159;&#19968;&#20010;&#30693;&#35782;&#19981;&#21464;&#30340;&#22242;&#20307;&#65292;&#30001;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#20294;&#32467;&#26500;&#19981;&#21516;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#38416;&#36848;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#24403;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#22987;&#32456;&#20934;&#30830;&#26102;&#65292;&#34987;&#21028;&#23450;&#20026;&#40065;&#26834;&#24615;&#24378;&#12290;&#25105;&#20204;&#23545;&#36807;&#21435;&#21313;&#24180;&#20013;&#21457;&#34920;&#30340;&#20960;&#31181;&#20856;&#22411;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial measurement of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;REGARD&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#21644;&#32426;&#24459;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#21644;&#23454;&#29616;&#33258;&#36866;&#24212;&#33258;&#20027;&#35745;&#31639;&#31995;&#32479;&#19979;&#33258;&#21160;&#32593;&#32476;&#38450;&#24481;&#20195;&#29702;&#30340;&#34892;&#21160;&#35268;&#21017;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#25915;&#20987;&#21709;&#24212;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.13967</link><description>&lt;p&gt;
REGARD&#65306;&#33258;&#36866;&#24212;&#33258;&#20027;&#35745;&#31639;&#31995;&#32479;&#26694;&#26550;&#19979;&#30340;&#33258;&#21160;&#21270;&#32593;&#32476;&#38450;&#24481;&#25351;&#23548;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
REGARD: Rules of EngaGement for Automated cybeR Defense to aid in Intrusion Response. (arXiv:2305.13967v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;REGARD&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#21644;&#32426;&#24459;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#21644;&#23454;&#29616;&#33258;&#36866;&#24212;&#33258;&#20027;&#35745;&#31639;&#31995;&#32479;&#19979;&#33258;&#21160;&#32593;&#32476;&#38450;&#24481;&#20195;&#29702;&#30340;&#34892;&#21160;&#35268;&#21017;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#25915;&#20987;&#21709;&#24212;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;REGARD&#8221;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#31995;&#32479;&#21270;&#21644;&#32426;&#24459;&#24615;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#23454;&#29616;&#33258;&#36866;&#24212;&#33258;&#20027;&#35745;&#31639;&#31995;&#32479;&#19979;&#33258;&#21160;&#32593;&#32476;&#38450;&#24481;&#20195;&#29702;&#30340;&#34892;&#21160;&#35268;&#21017;&#65292;&#37325;&#28857;&#20851;&#27880;&#25915;&#20987;&#21709;&#24212;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Intelligent Cyberdefense Agents (AICAs) that are part Intrusion Detection Systems (IDS) and part Intrusion Response Systems (IRS) are being designed to protect against sophisticated and automated cyber-attacks. An AICA based on the ideas of Self-Adaptive Autonomic Computing Systems (SA-ACS) can be considered as a managing system that protects a managed system like a personal computer, web application, critical infrastructure, etc. An AICA, specifically the IRS components, can compute a wide range of potential responses to meet its security goals and objectives, such as taking actions to prevent the attack from completing, restoring the system to comply with the organizational security policy, containing or confining an attack, attack eradication, deploying forensics measures to enable future attack analysis, counterattack, and so on. To restrict its activities in order to minimize collateral/organizational damage, such an automated system must have set Rules of Engagement (Ro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPNet&#30340;&#32593;&#32476;&#65292;&#21033;&#29992;&#22522;&#20110;CLIP&#30340;&#20851;&#27880;&#20957;&#32858;&#22120;&#21644;&#27010;&#29575;&#26144;&#23556;&#25351;&#23548;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#30340;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.13962</link><description>&lt;p&gt;
CPNet&#65306;&#21033;&#29992;&#22522;&#20110;CLIP&#30340;&#20851;&#27880;&#20957;&#32858;&#22120;&#21644;&#27010;&#29575;&#26144;&#23556;&#25351;&#23548;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#30340;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CPNet: Exploiting CLIP-based Attention Condenser and Probability Map Guidance for High-fidelity Talking Face Generation. (arXiv:2305.13962v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPNet&#30340;&#32593;&#32476;&#65292;&#21033;&#29992;&#22522;&#20110;CLIP&#30340;&#20851;&#27880;&#20957;&#32858;&#22120;&#21644;&#27010;&#29575;&#26144;&#23556;&#25351;&#23548;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#30340;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33392;&#24040;&#30340;&#25361;&#25112;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#20363;&#22914;&#30005;&#24433;&#21160;&#30011;&#21644;&#34394;&#25311;&#20027;&#25773;&#65292;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#24341;&#36215;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30028;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#24050;&#32463;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#21162;&#21147;&#26469;&#25552;&#39640;&#29983;&#25104;&#30340;&#35828;&#35805;&#20154;&#33080;&#35270;&#39057;&#30340;&#20445;&#30495;&#24230;&#21644;&#21475;&#22411;&#21516;&#27493;&#36136;&#37327;&#65292;&#20294;&#26159;&#21512;&#25104;&#36136;&#37327;&#21644;&#25928;&#29575;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#22256;&#22659;&#65292;&#26412;&#25991;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;CLIP&#30340;&#20851;&#27880;&#21644;&#27010;&#29575;&#22320;&#22270;&#24341;&#23548;&#32593;&#32476;&#65288;CPNet&#65289;&#26469;&#25512;&#26029;&#39640;&#20445;&#30495;&#24230;&#30340;&#35828;&#35805;&#20154;&#33080;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, talking face generation has drawn ever-increasing attention from the research community in computer vision due to its arduous challenges and widespread application scenarios, e.g. movie animation and virtual anchor. Although persevering efforts have been undertaken to enhance the fidelity and lip-sync quality of generated talking face videos, there is still large room for further improvements of synthesis quality and efficiency. Actually, these attempts somewhat ignore the explorations of fine-granularity feature extraction/integration and the consistency between probability distributions of landmarks, thereby recurring the issues of local details blurring and degraded fidelity. To mitigate these dilemmas, in this paper, a novel CLIP-based Attention and Probability Map Guided Network (CPNet) is delicately designed for inferring high-fidelity talking face videos. Specifically, considering the demands of fine-grained feature recalibration, a clip-based attention condenser is ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CP&#20998;&#35299;&#31639;&#27861;DL-CPALS&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#26377;&#21033;&#30340;&#21021;&#22987;&#21270;&#20540;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#20449;&#36947;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.13947</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#27861;&#29992;&#20110;&#24352;&#37327;CP&#20998;&#35299;&#21450;&#20854;&#22312;&#22823;&#35268;&#27169;MIMO&#20449;&#36947;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep-Learning-Aided Alternating Least Squares for Tensor CP Decomposition and Its Application to Massive MIMO Channel Estimation. (arXiv:2305.13947v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CP&#20998;&#35299;&#31639;&#27861;DL-CPALS&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#26377;&#21033;&#30340;&#21021;&#22987;&#21270;&#20540;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#20449;&#36947;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CANDECOMP/PARAFAC (CP)&#20998;&#35299;&#26159;&#22312;&#22810;&#22495;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31995;&#32479;&#20013;&#34920;&#36848;&#25509;&#25910;&#21040;&#30340;&#24352;&#37327;&#20449;&#21495;&#30340;&#26368;&#24120;&#29992;&#27169;&#22411;&#65292;&#22240;&#20026;&#25509;&#25910;&#26426;&#36890;&#24120;&#20250;&#23558;&#26469;&#33258;&#19981;&#21516;&#20256;&#36755;&#36335;&#24452;&#25110;&#29992;&#25143;&#30340;&#32452;&#20214;&#30456;&#21152;&#12290;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#21644;&#20302;&#24310;&#36831;&#30340;&#20449;&#36947;&#20272;&#35745;&#65292;&#38656;&#35201;&#22909;&#30340;&#12289;&#24555;&#36895;&#30340;CP&#20998;&#35299;&#31639;&#27861;&#12290;CP&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;CPALS&#65289;&#26159;&#35745;&#31639;CP&#20998;&#35299;&#30340;&#20027;&#21147;&#31639;&#27861;&#12290;&#20294;&#26159;&#65292;&#23427;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#21021;&#22987;&#21270;&#65292;&#33391;&#22909;&#30340;&#36215;&#22987;&#20540;&#21487;&#20197;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#19982;CPALS&#35299;&#32806;&#65292;&#24182;&#19981;&#19968;&#23450;&#26377;&#21033;&#20110;&#35299;&#20915;CP&#20998;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#31639;&#27861;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CPALS&#65288;DL-CPALS&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29983;&#25104;&#26377;&#21033;&#30340;&#21021;&#22987;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;DL-CPALS&#23558;DNN&#21644;CPALS&#38598;&#25104;&#21040;&#27169;&#22411;&#22522;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
CANDECOMP/PARAFAC (CP) decomposition is the mostly used model to formulate the received tensor signal in a multi-domain massive multiple-input multiple-output (MIMO) system, as the receiver generally sums the components from different paths or users. To achieve accurate and low-latency channel estimation, good and fast CP decomposition algorithms are desired. The CP alternating least squares (CPALS) is the workhorse algorithm for calculating the CP decomposition. However, its performance depends on the initializations, and good starting values can lead to more efficient solutions. Existing initialization strategies are decoupled from the CPALS and are not necessarily favorable for solving the CP decomposition. To enhance the algorithm's speed and accuracy, this paper proposes a deep-learning-aided CPALS (DL-CPALS) method that uses a deep neural network (DNN) to generate favorable initializations. The proposed DL-CPALS integrates the DNN and CPALS to a model-based deep learning paradigm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#28151;&#21512;&#27169;&#22411;&#31561;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.13941</link><description>&lt;p&gt;
&#25163;&#35821;&#35782;&#21035;&#25216;&#26415;&#21644;&#31639;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Techniques and Algorithms for Recognising Sign Language. (arXiv:2305.13941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#28151;&#21512;&#27169;&#22411;&#31561;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#26159;&#19968;&#31181;&#35270;&#35273;&#35821;&#35328;&#65292;&#22686;&#24378;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#24182;&#19988;&#32463;&#24120;&#20316;&#20026;&#20808;&#22825;&#24615;&#21548;&#21147;&#20007;&#22833;&#32773;&#20027;&#35201;&#30340;&#20132;&#27969;&#26041;&#24335;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20351;&#29992;&#25163;&#35821;&#30340;&#20808;&#22825;&#24615;&#21548;&#21147;&#20007;&#22833;&#32773;&#24182;&#19981;&#22810;&#65292;&#20182;&#20204;&#32463;&#24120;&#38754;&#20020;&#31038;&#20132;&#23396;&#31435;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21019;&#24314;&#20154;&#26426;&#30028;&#38754;&#31995;&#32479;&#65292;&#20026;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#25552;&#20379;&#31038;&#20132;&#24179;&#21488;&#12290;&#24066;&#22330;&#19978;&#22823;&#22810;&#25968;&#21830;&#29992;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#26159;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#65292;&#20215;&#26684;&#26114;&#36149;&#65292;&#20351;&#29992;&#36215;&#26469;&#20063;&#24456;&#22256;&#38590;&#12290;&#23613;&#31649;&#36843;&#20999;&#38656;&#35201;&#22522;&#20110;&#35270;&#35273;&#30340;&#31995;&#32479;&#65292;&#20294;&#39318;&#20808;&#24517;&#39035;&#20811;&#26381;&#20960;&#20010;&#25361;&#25112;&#12290;&#26089;&#26399;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#25216;&#26415;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#21253;&#21547;&#26102;&#38388;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#29616;&#22312;&#27491;&#22312;&#24212;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23558;&#25163;&#37096;&#21644;&#25163;&#35821;&#21160;&#20316;&#36716;&#21270;&#20026;&#21475;&#35821;&#25110;&#20070;&#38754;&#35821;&#35328;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language is a visual language that enhances communication between people and is frequently used as the primary form of communication by people with hearing loss. Even so, not many people with hearing loss use sign language, and they frequently experience social isolation. Therefore, it is necessary to create human-computer interface systems that can offer hearing-impaired people a social platform. Most commercial sign language translation systems now on the market are sensor-based, pricey, and challenging to use. Although vision-based systems are desperately needed, they must first overcome several challenges. Earlier continuous sign language recognition techniques used hidden Markov models, which have a limited ability to include temporal information. To get over these restrictions, several machine learning approaches are being applied to transform hand and sign language motions into spoken or written language. In this study, we compare various deep learning techniques for recogn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#20174;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35282;&#24230;&#35762;&#36848;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#19981;&#20844;&#27491;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#26694;&#26550;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#20844;&#24179;&#24615;&#25351;&#26631;&#20197;&#31526;&#21512;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.13938</link><description>&lt;p&gt;
&#36890;&#36807;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35270;&#35282;&#35762;&#36848;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#65306;&#25110;&#35859;&#27861;&#24459;&#38750;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is not a Decision Tree. (arXiv:2305.13938v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#20174;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35282;&#24230;&#35762;&#36848;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#19981;&#20844;&#27491;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#26694;&#26550;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#20844;&#24179;&#24615;&#25351;&#26631;&#20197;&#31526;&#21512;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#32654;&#24863;&#21040;&#19981;&#20844;&#24179;&#21644;&#27495;&#35270;&#30340;&#38382;&#39064;&#26368;&#36817;&#24341;&#36215;&#20102;&#27861;&#24459;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#32773;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#20197;&#21450;&#27861;&#24459;&#19978;&#30340;&#27495;&#35270;&#21644;&#24179;&#31561;&#27010;&#24565;&#20043;&#38388;&#30340;&#37325;&#21472;&#31243;&#24230;&#36890;&#24120;&#19981;&#28165;&#26970;&#65292;&#23548;&#33268;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#27861;&#24459;&#20043;&#38388;&#30340;&#35823;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#19982;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#27010;&#24565;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#21512;&#20197;&#21450;&#23427;&#20204;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#27431;&#30431;&#26696;&#20363;&#27861;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#31639;&#27861;&#19981;&#20844;&#24179;&#30340;&#20856;&#22411;&#20363;&#23376;&#65292;&#25214;&#20986;&#19982;&#27431;&#30431;&#26696;&#20363;&#27861;&#30340;&#31867;&#27604;&#20043;&#22788;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#21644;AI&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#31526;&#21512;&#27431;&#30431;&#30340;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22269;&#23478;&#23433;&#20840;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#25112;&#30053;&#37325;&#35201;&#24615;&#65292;&#20174;&#20891;&#20107;&#35282;&#24230;&#23457;&#35270;&#20102;&#32654;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#24182;&#24378;&#35843;&#20102;&#38656;&#35201;&#20445;&#25252;&#36825;&#20123;&#25216;&#26415;&#20813;&#21463;&#25932;&#26041;&#20837;&#20405;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13927</link><description>&lt;p&gt;
&#36890;&#36807;LLM&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#20248;&#21270;&#22269;&#23478;&#23433;&#20840;&#25112;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimizing National Security Strategies through LLM-Driven Artificial Intelligence Integration. (arXiv:2305.13927v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22269;&#23478;&#23433;&#20840;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#25112;&#30053;&#37325;&#35201;&#24615;&#65292;&#20174;&#20891;&#20107;&#35282;&#24230;&#23457;&#35270;&#20102;&#32654;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#24182;&#24378;&#35843;&#20102;&#38656;&#35201;&#20445;&#25252;&#36825;&#20123;&#25216;&#26415;&#20813;&#21463;&#25932;&#26041;&#20837;&#20405;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#23427;&#20204;&#22312;&#22269;&#23478;&#23433;&#20840;&#20013;&#30340;&#25112;&#30053;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#20891;&#20107;&#20013;&#29420;&#29305;&#30340;AI&#24212;&#29992;&#65292;&#24378;&#35843;&#25104;&#21151;&#30340;&#25112;&#30053;&#35201;&#27714;&#65292;&#24182;&#26088;&#22312;&#37325;&#26032;&#28857;&#29123;AI&#22312;&#22269;&#23478;&#23433;&#20840;&#20013;&#30340;&#35282;&#33394;&#30340;&#28608;&#24773;&#12290;&#25105;&#20204;&#23558;&#20174;&#20891;&#20107;&#35282;&#24230;&#23457;&#35270;&#32654;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#35752;&#35770;&#20445;&#25252;&#36825;&#20123;&#25216;&#26415;&#20813;&#21463;&#25932;&#26041;&#20837;&#20405;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20854;&#38598;&#25104;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#24378;&#35843;&#20154;&#24037;&#26234;&#33021;&#23545;&#22269;&#23478;&#23433;&#20840;&#30340;&#25112;&#30053;&#24847;&#20041;&#20197;&#21450;&#19968;&#31995;&#21015;&#20891;&#20107;&#39046;&#34966;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#30340;&#25112;&#30053;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence and machine learning continue to advance, we must understand their strategic importance in national security. This paper focuses on unique AI applications in the military, emphasizes strategic imperatives for success, and aims to rekindle excitement about AI's role in national security. We will examine the United States progress in AI and ML from a military standpoint, discuss the importance of securing these technologies from adversaries, and explore the challenges and risks associated with their integration. Finally, we will highlight the strategic significance of AI to national security and a set of strategic imperatives for military leaders and policymakers
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;SymGen&#30001;&#20449;&#24687;&#25552;&#31034;&#21644;&#22522;&#20110;&#21327;&#35758;&#30340;&#39564;&#35777;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#27880;&#37322;&#26114;&#36149;&#30340;&#31526;&#21495;&#35821;&#35328;&#25968;&#25454;&#12290;&#30456;&#23545;&#20110;LLMs&#65292;&#20351;&#29992;1%&#22823;&#23567;&#30340;&#20219;&#21153;&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#22823;&#24133;&#21066;&#20943;&#20102;&#25512;&#29702;&#21644;&#37096;&#32626;&#25104;&#26412;&#12290;&#20351;&#29992;SymGen&#29983;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13917</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21495;&#35821;&#35328;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Data for Symbolic Language with Large Language Models. (arXiv:2305.13917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13917
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;SymGen&#30001;&#20449;&#24687;&#25552;&#31034;&#21644;&#22522;&#20110;&#21327;&#35758;&#30340;&#39564;&#35777;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#27880;&#37322;&#26114;&#36149;&#30340;&#31526;&#21495;&#35821;&#35328;&#25968;&#25454;&#12290;&#30456;&#23545;&#20110;LLMs&#65292;&#20351;&#29992;1%&#22823;&#23567;&#30340;&#20219;&#21153;&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#22823;&#24133;&#21066;&#20943;&#20102;&#25512;&#29702;&#21644;&#37096;&#32626;&#25104;&#26412;&#12290;&#20351;&#29992;SymGen&#29983;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#23558;LLMs&#36716;&#25442;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#32780;&#19981;&#26159;&#20219;&#21153;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#21478;&#19968;&#20010;&#21487;&#36127;&#25285;&#30340;&#20219;&#21153;&#27169;&#22411;&#20197;&#23454;&#29616;&#39640;&#25928;&#37096;&#32626;&#21644;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20027;&#35201;&#34987;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#19988;&#23578;&#26410;&#25506;&#32034;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#36755;&#20986;&#65288;&#20363;&#22914;&#35821;&#20041;&#35299;&#26512;&#21644;&#20195;&#30721;&#29983;&#25104;&#65289;&#30340;&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SymGen&#65292;&#21033;&#29992;LLMs&#29983;&#25104;&#21508;&#31181;&#27880;&#37322;&#26114;&#36149;&#30340;&#31526;&#21495;&#35821;&#35328;&#25968;&#25454;&#12290;SymGen&#30001;&#20449;&#24687;&#25552;&#31034;&#21644;&#22522;&#20110;&#21327;&#35758;&#30340;&#39564;&#35777;&#22120;&#32452;&#25104;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#23545;&#20845;&#20010;&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#19982;LLMs&#30456;&#27604;&#65292;&#25105;&#20204;&#35777;&#26126;1\%&#22823;&#23567;&#30340;&#20219;&#21153;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#25512;&#29702;&#21644;&#37096;&#32626;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;SymGen&#29983;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1\%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#37197;&#23545;&#20256;&#36882;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;CycleGAN&#27169;&#22411;&#23558;&#20013;&#27874;&#32418;&#22806;&#65288;MWIR&#65289;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#65288;VIS&#65289;&#22495;&#22270;&#20687;&#65288;&#25110;&#21487;&#35265;&#22270;&#20687;&#21040;MWIR&#22495;&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#30446;&#26631;&#35782;&#21035;&#22330;&#26223;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13886</link><description>&lt;p&gt;
&#28145;&#24230;&#20256;&#36882;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Deep Transductive Transfer Learning for Automatic Target Recognition. (arXiv:2305.13886v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13886
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#37197;&#23545;&#20256;&#36882;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;CycleGAN&#27169;&#22411;&#23558;&#20013;&#27874;&#32418;&#22806;&#65288;MWIR&#65289;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#65288;VIS&#65289;&#22495;&#22270;&#20687;&#65288;&#25110;&#21487;&#35265;&#22270;&#20687;&#21040;MWIR&#22495;&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#30446;&#26631;&#35782;&#21035;&#22330;&#26223;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#36890;&#24120;&#23384;&#22312;&#19968;&#20010;&#39046;&#22495;&#65288;&#32418;&#22806;&#28304;&#39046;&#22495;&#65289;&#20013;&#26377;&#26631;&#35760;&#30340;&#22270;&#20687;&#65292;&#20294;&#22312;&#20854;&#20182;&#30446;&#26631;&#39046;&#22495;&#65288;&#22914;&#21487;&#35265;&#12289;SAR&#12289;LIDAR&#65289;&#20013;&#27809;&#26377;&#27880;&#37322;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#27880;&#37322;&#36825;&#20123;&#22270;&#20687;&#23545;&#20110;&#22522;&#20110;&#28304;&#39046;&#22495;&#26631;&#35760;&#22270;&#20687;&#26500;&#24314;&#30446;&#26631;&#39046;&#22495;&#30340;&#24378;&#22823;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#36882;&#36801;&#31227;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#28304;&#22495;&#20013;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#32593;&#32476;&#26469;&#20351;&#32593;&#32476;&#36866;&#24212;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#37197;&#23545;&#20256;&#36882;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;CycleGAN&#27169;&#22411;&#21644;&#28304;&#22495;&#20013;&#35757;&#32451;&#33391;&#22909;&#30340;ATR&#20998;&#31867;&#22120;&#26469;&#26500;&#24314;&#30446;&#26631;&#22495;&#30340;ATR&#20998;&#31867;&#22120;&#65292;&#32780;&#19981;&#38656;&#35201;&#30446;&#26631;&#22495;&#20013;&#30340;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;CycleGAN&#27169;&#22411;&#23558;&#20013;&#27874;&#32418;&#22806;&#65288;MWIR&#65289;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#65288;VIS&#65289;&#22495;&#22270;&#20687;&#65288;&#25110;&#21487;&#35265;&#22270;&#20687;&#21040;MWIR&#22495;&#65289;&#12290;&#20026;&#20102;&#35757;&#32451;&#20256;&#36882;&#36801;&#31227;&#23398;&#20064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#20301;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#30446;&#26631;&#35782;&#21035;&#22330;&#26223;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major obstacles in designing an automatic target recognition (ATR) algorithm, is that there are often labeled images in one domain (i.e., infrared source domain) but no annotated images in the other target domains (i.e., visible, SAR, LIDAR). Therefore, automatically annotating these images is essential to build a robust classifier in the target domain based on the labeled images of the source domain. Transductive transfer learning is an effective way to adapt a network to a new target domain by utilizing a pretrained ATR network in the source domain. We propose an unpaired transductive transfer learning framework where a CycleGAN model and a well-trained ATR classifier in the source domain are used to construct an ATR classifier in the target domain without having any labeled data in the target domain. We employ a CycleGAN model to transfer the mid-wave infrared (MWIR) images to visible (VIS) domain images (or visible to MWIR domain). To train the transductive CycleGAN, we 
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#36719;&#20214;&#28431;&#27934;&#20462;&#22797;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MiDas&#30340;&#22810;&#31890;&#24230;&#26816;&#27979;&#22120;&#65292;&#23427;&#21033;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#20195;&#30721;&#26356;&#25913;&#26500;&#24314;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#28431;&#27934;&#20462;&#22797;&#25552;&#20132;&#12290;</title><link>http://arxiv.org/abs/2305.13884</link><description>&lt;p&gt;
&#28431;&#27934;&#20462;&#22797;&#30340;&#22810;&#31890;&#24230;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multi-Granularity Detector for Vulnerability Fixes. (arXiv:2305.13884v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13884
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36719;&#20214;&#28431;&#27934;&#20462;&#22797;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MiDas&#30340;&#22810;&#31890;&#24230;&#26816;&#27979;&#22120;&#65292;&#23427;&#21033;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#20195;&#30721;&#26356;&#25913;&#26500;&#24314;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#28431;&#27934;&#20462;&#22797;&#25552;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#24320;&#28304;&#36719;&#20214;&#30340;&#26085;&#30410;&#20381;&#36182;&#65292;&#29992;&#25143;&#26292;&#38706;&#20110;&#31532;&#19977;&#26041;&#24211;&#28431;&#27934;&#20043;&#20013;&#12290;&#20026;&#20102;&#25552;&#37266;&#29992;&#25143;&#27492;&#31867;&#28431;&#27934;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#36719;&#20214;&#32452;&#25104;&#20998;&#26512;&#65288;SCA&#65289;&#24037;&#20855;&#12290;SCA&#38656;&#35201;&#30830;&#23450;&#28431;&#27934;&#20462;&#22797;&#25552;&#20132;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#20102;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#36825;&#26679;&#30340;&#28431;&#27934;&#20462;&#22797;&#25552;&#20132;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#37492;&#23450;&#36825;&#26679;&#30340;&#25552;&#20132;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21482;&#26377;&#26497;&#23569;&#25968;&#25552;&#20132;&#26159;&#28431;&#27934;&#20462;&#22797;&#12290;&#27492;&#22806;&#65292;&#20195;&#30721;&#26356;&#25913;&#21487;&#33021;&#26159;&#22024;&#26434;&#19988;&#38590;&#20197;&#20998;&#26512;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22122;&#22768;&#21487;&#20197;&#20986;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#30340;&#32454;&#33410;&#20013;&#65292;&#20351;&#24471;&#20934;&#30830;&#26816;&#27979;&#28431;&#27934;&#20462;&#22797;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#24182;&#25552;&#39640;&#20197;&#21069;&#24037;&#20316;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MiDas&#65288;&#28431;&#27934;&#20462;&#22797;&#30340;&#22810;&#31890;&#24230;&#26816;&#27979;&#22120;&#65289;&#12290;MiDas&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#23427;&#20026;&#27599;&#20010;&#20195;&#30721;&#26356;&#25913;&#31890;&#24230;&#32423;&#21035;&#26500;&#24314;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#25552;&#20132;&#32423;&#21035;&#12289;&#25991;&#20214;&#32423;&#21035;&#12289;&#30862;&#29255;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing reliance on Open Source Software, users are exposed to third-party library vulnerabilities. Software Composition Analysis (SCA) tools have been created to alert users of such vulnerabilities. SCA requires the identification of vulnerability-fixing commits. Prior works have proposed methods that can automatically identify such vulnerability-fixing commits. However, identifying such commits is highly challenging, as only a very small minority of commits are vulnerability fixing. Moreover, code changes can be noisy and difficult to analyze. We observe that noise can occur at different levels of detail, making it challenging to detect vulnerability fixes accurately.  To address these challenges and boost the effectiveness of prior works, we propose MiDas (Multi-Granularity Detector for Vulnerability Fixes). Unique from prior works, Midas constructs different neural networks for each level of code change granularity, corresponding to commit-level, file-level, hunk-level,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#30450;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#24191;&#20041;&#26399;&#26395;&#26497;&#22823;&#21270;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#20869;&#36827;&#34892;&#22270;&#20687;&#24674;&#22797;&#65292;&#21253;&#25324;&#20840;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#12290;&#25152;&#25552;&#20986;&#30340;SREMN&#26041;&#27861;&#22312;&#29616;&#26377;&#24037;&#20316;&#20013;&#34920;&#29616;&#21331;&#36234;&#65292;&#24182;&#20855;&#26377;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13880</link><description>&lt;p&gt;
&#30450;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#24191;&#20041;&#26399;&#26395;&#26497;&#22823;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Generalized Expectation Maximization Framework for Blind Image Super Resolution. (arXiv:2305.13880v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#30450;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#24191;&#20041;&#26399;&#26395;&#26497;&#22823;&#21270;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#20869;&#36827;&#34892;&#22270;&#20687;&#24674;&#22797;&#65292;&#21253;&#25324;&#20840;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#12290;&#25152;&#25552;&#20986;&#30340;SREMN&#26041;&#27861;&#22312;&#29616;&#26377;&#24037;&#20316;&#20013;&#34920;&#29616;&#21331;&#36234;&#65292;&#24182;&#20855;&#26377;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#30450;&#21333;&#24352;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SISR&#65289;&#26041;&#27861;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22270;&#20687;&#21644;&#23427;&#20204;&#30340;&#20302;&#20998;&#36776;&#29575;&#65288;LR&#65289;&#23545;&#24212;&#29289;&#38388;&#30340;&#23398;&#20064;&#26144;&#23556;&#36827;&#34892;&#24674;&#22797;&#65292;&#36825;&#20123;&#23545;&#24212;&#29289;&#29992;&#20219;&#24847;&#27169;&#31946;&#26680;&#38477;&#32423;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#38656;&#35201;&#19968;&#20010;&#29420;&#31435;&#30340;&#27493;&#39588;&#26469;&#20272;&#35745;&#27169;&#31946;&#26680;&#65292;&#23548;&#33268;&#27493;&#39588;&#20043;&#38388;&#30340;&#35823;&#24046;&#32047;&#31215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#30450;SISR&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#20869;&#36827;&#34892;&#22270;&#20687;&#24674;&#22797;&#65292;&#21253;&#25324;&#20840;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;SREMN&#65292;&#23558;&#23398;&#20064;&#25216;&#26415;&#25972;&#21512;&#21040;&#24191;&#20041;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20013;&#65292;&#24182;&#20174;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#20013;&#25512;&#26029;HR&#22270;&#20687;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#24037;&#20316;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based methods for blind single image super resolution (SISR) conduct the restoration by a learned mapping between high-resolution (HR) images and their low-resolution (LR) counterparts degraded with arbitrary blur kernels. However, these methods mostly require an independent step to estimate the blur kernel, leading to error accumulation between steps. We propose an end-to-end learning framework for the blind SISR problem, which enables image restoration within a unified Bayesian framework with either full- or semi-supervision. The proposed method, namely SREMN, integrates learning techniques into the generalized expectation-maximization (GEM) algorithm and infers HR images from the maximum likelihood estimation (MLE). Extensive experiments show the superiority of the proposed method with comparison to existing work and novelty in semi-supervised learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#38271;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#33258;&#28982;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#25913;&#36827;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25968;&#25454;&#38598;&#30001; GPT 3.5 &#29983;&#25104;&#65292;&#25688;&#35201;&#21253;&#25324;&#26469;&#33258; Project Gutenberg &#30340; 1500 &#26412;&#20070;&#20013;&#27599;&#20010;&#22330;&#26223;&#30340;&#24635;&#32467;&#65292;&#20197;&#21450;&#37197;&#22871;&#30340;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13877</link><description>&lt;p&gt;
Narrative XL: &#19968;&#20010;&#29992;&#20110;&#38271;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Narrative XL: A Large-scale Dataset For Long-Term Memory Models. (arXiv:2305.13877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#38271;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#33258;&#28982;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#25913;&#36827;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25968;&#25454;&#38598;&#30001; GPT 3.5 &#29983;&#25104;&#65292;&#25688;&#35201;&#21253;&#25324;&#26469;&#33258; Project Gutenberg &#30340; 1500 &#26412;&#20070;&#20013;&#27599;&#20010;&#22330;&#26223;&#30340;&#24635;&#32467;&#65292;&#20197;&#21450;&#37197;&#22871;&#30340;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#20219;&#20309;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#35201;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#19981;&#20165;&#38656;&#35201;&#23545;&#20856;&#22411;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#25110;&#35757;&#32451;&#31243;&#24207;&#36827;&#34892;&#26356;&#25913;&#65292;&#36824;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#36825;&#20123;&#26032;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#36164;&#28304;&#32570;&#23569;&#19968;&#20123;&#20851;&#38190;&#23646;&#24615;&#65292;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#35268;&#27169;&#30340;&#33258;&#28982;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#35780;&#20272;&#65289;&#38271;&#26399;&#35760;&#24518;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#30701;&#26399;&#35760;&#24518;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#21019;&#24314;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20351;&#29992; GPT 3.5&#65292;&#25105;&#20204;&#24635;&#32467;&#20102; Project Gutenberg &#20013; 1500 &#26412;&#25163;&#24037;&#31579;&#36873;&#30340;&#20070;&#31821;&#20013;&#30340;&#27599;&#20010;&#22330;&#26223;&#65292;&#27599;&#26412;&#20070;&#24471;&#21040;&#22823;&#32422; 150 &#20010;&#22330;&#26223;&#32423;&#21035;&#30340;&#25688;&#35201;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20123;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#65292;&#21253;&#25324;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#22330;&#26223;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
Despite their tremendous successes, most large language models do not have any long-term memory mechanisms, which restricts their applications. Overcoming this limitation would not only require changes to the typical transformer architectures or training procedures, but also a dataset on which these new models could be trained and evaluated. We argue that existing resources lack a few key properties, and that at present, there are no naturalistic datasets of sufficient scale to train (and not only evaluate) long-term memory language models. We then present our solution that capitalizes on the advances in short-term memory language models to create such a dataset. Using GPT 3.5, we summarized each scene in 1500 hand-curated books from Project Gutenberg, which resulted in approximately 150 scene-level summaries per book. We then created a number of reading comprehension questions based on these summaries, including three types of multiple-choice scene recognition questions, as well as fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#36136;&#24615;&#32858;&#31867;&#30340;&#20844;&#24179;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#32452;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#19988;&#33021;&#22815;&#25269;&#25239;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.13875</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#36136;&#24615;&#32858;&#31867;&#30340;&#20844;&#24179;&#36807;&#37319;&#26679;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Fair Oversampling Technique using Heterogeneous Clusters. (arXiv:2305.13875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#36136;&#24615;&#32858;&#31867;&#30340;&#20844;&#24179;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#32452;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#19988;&#33021;&#22815;&#25269;&#25239;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#32452;&#19981;&#24179;&#34913;&#34987;&#35748;&#20026;&#26159;&#22952;&#30861;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#26435;&#34913;&#30340;&#20004;&#20010;&#21407;&#22240;&#12290;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#25552;&#20986;&#20844;&#24179;&#36807;&#37319;&#26679;&#25216;&#26415;&#26469;&#20849;&#21516;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#32452;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#36136;&#24615;&#32676;&#38598;&#25968;&#25454;&#30340;&#20844;&#24179;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#20855;&#26377;&#28151;&#21512;&#31867;&#29305;&#24449;&#25110;&#28151;&#21512;&#32452;&#29305;&#24449;&#65292;&#20351;&#20998;&#31867;&#22120;&#33021;&#22815;&#25269;&#25239;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25554;&#20540;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance and group (e.g., race, gender, and age) imbalance are acknowledged as two reasons in data that hinder the trade-off between fairness and utility of machine learning classifiers. Existing techniques have jointly addressed issues regarding class imbalance and group imbalance by proposing fair over-sampling techniques. Unlike the common oversampling techniques, which only address class imbalance, fair oversampling techniques significantly improve the abovementioned trade-off, as they can also address group imbalance. However, if the size of the original clusters is too small, these techniques may cause classifier overfitting. To address this problem, we herein develop a fair oversampling technique using data from heterogeneous clusters. The proposed technique generates synthetic data that have class-mix features or group-mix features to make classifiers robust to overfitting. Moreover, we develop an interpolation method that can enhance the validity of generated synthetic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#25216;&#26415;&#23454;&#29616;&#39640;&#32423;&#26465;&#20214;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#36125;&#21494;&#26031;&#22270;&#20687;&#32763;&#35793;&#32593;&#32476;&#65288;VBITN&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#12289;&#35821;&#20041;&#32534;&#36753;&#21644;&#28151;&#21512;&#22495;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.13872</link><description>&lt;p&gt;
&#20855;&#26377;&#39046;&#22495;&#30456;&#20851;&#21464;&#37327;&#30340;&#39640;&#32423;&#22270;&#20687;&#29983;&#25104;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Variational Bayesian Framework for Advanced Image Generation with Domain-Related Variables. (arXiv:2305.13872v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#25216;&#26415;&#23454;&#29616;&#39640;&#32423;&#26465;&#20214;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#36125;&#21494;&#26031;&#22270;&#20687;&#32763;&#35793;&#32593;&#32476;&#65288;VBITN&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#12289;&#35821;&#20041;&#32534;&#36753;&#21644;&#28151;&#21512;&#22495;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGM&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#26465;&#20214;&#34920;&#24449;&#20026;&#25968;&#25454;&#20998;&#24067;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#36890;&#29992;&#29983;&#25104;&#24314;&#27169;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#35299;&#20915;&#39640;&#32423;&#26465;&#20214;&#29983;&#25104;&#38382;&#39064;&#65292;&#32780;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#22810;&#31181;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#22270;&#20687;&#32534;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#23398;&#20064;&#36807;&#31243;&#20013;&#28508;&#22312;&#21464;&#37327;&#30340;&#25512;&#26029;&#38454;&#27573;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#20998;&#36125;&#21494;&#26031;&#22270;&#20687;&#32763;&#35793;&#32593;&#32476;&#65288;VBITN&#65289;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#22270;&#20687;&#32763;&#35793;&#21644;&#32534;&#36753;&#20219;&#21153;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35821;&#20041;&#32534;&#36753;&#21644;&#28151;&#21512;&#22495;&#32763;&#35793;&#30340;&#21019;&#26032;&#39640;&#32423;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models (DGMs) and their conditional counterparts provide a powerful ability for general-purpose generative modeling of data distributions. However, it remains challenging for existing methods to address advanced conditional generative problems without annotations, which can enable multiple applications like image-to-image translation and image editing. We present a unified Bayesian framework for such problems, which introduces an inference stage on latent variables within the learning process. In particular, we propose a variational Bayesian image translation network (VBITN) that enables multiple image translation and editing tasks. Comprehensive experiments show the effectiveness of our method on unsupervised image-to-image translation, and demonstrate the novel advanced capabilities for semantic editing and mixed domain translation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312; CAFe II &#21644; LPI &#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23558;&#26657;&#27491;&#26102;&#38388;&#32553;&#30701;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.13869</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#36229;&#23548;&#32447;&#24615;&#21152;&#36895;&#22120;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator. (arXiv:2305.13869v1 [physics.acc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312; CAFe II &#21644; LPI &#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23558;&#26657;&#27491;&#26102;&#38388;&#32553;&#30701;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#23548;&#32447;&#24615;&#21152;&#36895;&#22120;&#26159;&#29616;&#20195;&#31185;&#23398;&#30740;&#31350;&#30340;&#39640;&#24230;&#28789;&#27963;&#30340;&#35774;&#26045;&#65292;&#38656;&#35201;&#27599;&#21608;&#37325;&#26032;&#37197;&#32622;&#21644;&#35843;&#25972;&#12290;&#22240;&#27492;&#65292;&#26368;&#23567;&#21270;&#35774;&#32622;&#26102;&#38388;&#23545;&#20110;&#25552;&#20379;&#20805;&#36275;&#30340;&#23454;&#39564;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#36719; actor-critic(TBSAC)&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#20110;&#30495;&#27491;&#30340;&#21152;&#36895;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#25511;&#21046;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20998;&#21035;&#22312;&#20013;&#22269;&#36229;&#37325;&#20803;&#32032;&#21152;&#36895;&#22120;&#35774;&#26045;(CAFe II)&#21644;&#19968;&#20010;&#36731;&#36136;&#31890;&#23376;&#27880;&#20837;&#22120;(LPI)&#20013;&#25191;&#34892;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20856;&#22411;&#26463;&#27969;&#25511;&#21046;&#20219;&#21153;&#12290;&#22312;CAFe II&#30340;&#19977;&#20010;&#20302;&#28201;&#27169;&#22359;&#20013;&#20998;&#21035;&#25191;&#34892;&#20102;&#36712;&#36947;&#26657;&#27491;&#20219;&#21153;&#65292;&#35843;&#35856;&#25152;&#38656;&#26102;&#38388;&#24050;&#32463;&#20943;&#23569;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#65292;&#26657;&#27491;&#21518;&#30340;RMS&#20540;&#37117;&#23567;&#20110;1&#27627;&#31859;&#12290;&#21478;&#19968;&#20010;&#20256;&#36755;&#25928;&#29575;&#20248;&#21270;&#20219;&#21153;&#22312;CAFe II&#30340;&#21152;&#36895;&#22120;&#27573;LPI&#20013;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#30772;&#35299;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#20917;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#65292;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13860</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study. (arXiv:2305.13860v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#30772;&#35299;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#20917;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#65292;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#19982;&#20869;&#23481;&#32422;&#26463;&#21644;&#28508;&#22312;&#28389;&#29992;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#31350;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#21487;&#20197;&#29992;&#22810;&#23569;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31867;&#22411;&#30772;&#35299;LLMs&#65292;&#65288;2&#65289;&#30772;&#35299;&#25552;&#31034;&#22312;&#35268;&#36991;LLM&#38480;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#65288;3&#65289;ChatGPT&#23545;&#36825;&#20123;&#30772;&#35299;&#25552;&#31034;&#30340;&#38887;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#27169;&#22411;&#26469;&#20998;&#26512;&#29616;&#26377;&#25552;&#31034;&#30340;&#20998;&#24067;&#65292;&#35782;&#21035;&#20986;&#21313;&#20010;&#19981;&#21516;&#27169;&#24335;&#21644;&#19977;&#20010;&#30772;&#35299;&#25552;&#31034;&#31867;&#21035;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;3,120&#20010;&#31105;&#27490;&#24773;&#26223;&#19979;&#30340;&#29425;&#20013;&#38382;&#39064;&#25968;&#25454;&#38598;&#35780;&#20272;ChatGPT 3.5&#21644;4.0&#29256;&#26412;&#30340;&#30772;&#35299;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#23545;&#30772;&#35299;&#25552;&#31034;&#30340;&#25269;&#25239;&#21147;&#65292;&#21457;&#29616;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#26223;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;LLMs&#23545;&#24847;&#22806;&#28389;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in j
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21019;&#24314;&#26631;&#20934;&#36895;&#24230;&#25856;&#29228;&#35757;&#32451;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#25913;&#36827;&#36895;&#24230;&#25856;&#29228;&#35757;&#32451;&#21644;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13858</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#29983;&#25104;&#26631;&#20934;&#30340;&#36895;&#24230;&#25856;&#29228;&#35757;&#32451;&#35270;&#39057;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Producing a Standard Dataset of Speed Climbing Training Videos Using Deep Learning Techniques. (arXiv:2305.13858v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21019;&#24314;&#26631;&#20934;&#36895;&#24230;&#25856;&#29228;&#35757;&#32451;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#25913;&#36827;&#36895;&#24230;&#25856;&#29228;&#35757;&#32451;&#21644;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#25668;&#20687;&#22836;&#35760;&#24405;&#36895;&#24230;&#25856;&#29228;&#35757;&#32451;&#35838;&#31243;&#21644;&#27880;&#37322;&#35270;&#39057;&#30456;&#20851;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#36523;&#20307;&#23039;&#21183;&#12289;&#25163;&#21644;&#33050;&#30340;&#20301;&#32622;&#21644;&#26102;&#38388;&#31561;&#20449;&#24687;&#12290;&#28982;&#21518;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#27880;&#37322;&#25968;&#25454;&#65292;&#21019;&#24314;&#20102;&#26631;&#20934;&#30340;&#36895;&#24230;&#25856;&#29228;&#35757;&#32451;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#22312;&#25913;&#36827;&#36895;&#24230;&#25856;&#29228;&#35757;&#32451;&#21644;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#23547;&#25214;&#25913;&#36827;&#30340;&#39046;&#22495;&#12289;&#21019;&#24314;&#20010;&#24615;&#21270;&#30340;&#35757;&#32451;&#35745;&#21010;&#20197;&#21450;&#20998;&#26512;&#19981;&#21516;&#35757;&#32451;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#36824;&#23558;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#32463;&#39564;&#30740;&#31350;&#24212;&#29992;&#20110;&#27743;&#35199;&#25856;&#29228;&#38431;&#30340;&#22521;&#35757;&#36807;&#31243;&#20013;&#65292;&#20197;&#27979;&#35797;&#36825;&#20123;&#21457;&#29616;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#36825;&#39033;&#30740;&#31350;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This dissertation presents a methodology for recording speed climbing training sessions with multiple cameras and annotating the videos with relevant data, including body position, hand and foot placement, and timing. The annotated data is then analyzed using deep learning techniques to create a standard dataset of speed climbing training videos. The results demonstrate the potential of the new dataset for improving speed climbing training and research, including identifying areas for improvement, creating personalized training plans, and analyzing the effects of different training methods.The findings will also be applied to the training process of the Jiangxi climbing team through further empirical research to test the findings and further explore the feasibility of this study.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13850</link><description>&lt;p&gt;
&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#25552;&#21462;&#65288;VRE&#65289;&#26088;&#22312;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#23454;&#20307;&#29305;&#24449;&#21333;&#29420;&#39044;&#27979;&#27599;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#24573;&#30053;&#20102;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#21363;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32570;&#20047;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#21487;&#33021;&#20351;&#27169;&#22411;&#38590;&#20197;&#23398;&#20064;&#38271;&#31243;&#20851;&#31995;&#65292;&#24182;&#23481;&#26131;&#20135;&#29983;&#20914;&#31361;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GOSE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#25429;&#33719;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32473;&#23450;&#25991;&#26723;&#30340;&#25195;&#25551;&#22270;&#20687;&#65292;GOSE&#39318;&#20808;&#23545;&#23454;&#20307;&#23545;&#29983;&#25104;&#21021;&#27493;&#30340;&#20851;&#31995;&#39044;&#27979;&#12290;&#31532;&#20108;&#65292;&#22312;&#20808;&#21069;&#36845;&#20195;&#30340;&#39044;&#27979;&#32467;&#26524;&#22522;&#30784;&#19978;&#65292;GOSE&#21033;&#29992;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#36827;&#19968;&#27493;&#25972;&#21512;&#23454;&#20307;&#34920;&#31034;&#12290;&#36825;&#31181;&#8220;&#29983;&#25104;-&#25429;&#33719;-&#25972;&#21512;&#8221;&#27169;&#24335;&#34987;&#22810;&#27425;&#25191;&#34892;&#65292;&#20197;&#20415;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#33021;&#22815;&#34987;&#24456;&#22909;&#22320;&#25429;&#33719;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual relation extraction (VRE) aims to extract relations between entities from visuallyrich documents. Existing methods usually predict relations for each entity pair independently based on entity features but ignore the global structure information, i.e., dependencies between entity pairs. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledgeguided relation Extraction (GOSE) framework, which captures dependencies between entity pairs in an iterative manner. Given a scanned image of the document, GOSE firstly generates preliminary relation predictions on entity pairs. Secondly, it mines global structure knowledge based on prediction results of the previous iteration and further incorporates global structure knowledge into entity representations. This "generate-capture-incorporate" schema is performed multiple times so that entit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.13840</link><description>&lt;p&gt;
Control-A-Video: &#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models. (arXiv:2305.13840v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#65288;T2V&#65289;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;Video-ControlNet&#12290;&#35813;&#27169;&#22411;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;&#26377;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#21487;&#35757;&#32451;&#30340;&#26102;&#38388;&#23618;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#36328;&#24103;&#24314;&#27169;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#24103;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#22312;&#33258;&#22238;&#24402;&#26041;&#24335;&#19979;&#29983;&#25104;&#36716;&#25442;&#33258;&#22270;&#20687;&#39046;&#22495;&#20197;&#21450;&#20219;&#24847;&#38271;&#24230;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;Video-ControlNet&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#20174;&#36755;&#20837;&#35270;&#39057;&#20013;&#24341;&#20837;&#36816;&#21160;&#20808;&#39564;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#65292;Video-ControlNet&#21487;&#20197;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#29983;&#25104;&#20855;&#26377;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#20248;&#36136;&#19968;&#33268;&#35270;&#39057;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a controllable text-to-video (T2V) diffusion model, named Video-ControlNet, that generates videos conditioned on a sequence of control signals, such as edge or depth maps. Video-ControlNet is built on a pre-trained conditional text-to-image (T2I) diffusion model by incorporating a spatial-temporal self-attention mechanism and trainable temporal layers for efficient cross-frame modeling. A first-frame conditioning strategy is proposed to facilitate the model to generate videos transferred from the image domain as well as arbitrary-length videos in an auto-regressive manner. Moreover, Video-ControlNet employs a novel residual-based noise initialization strategy to introduce motion prior from an input video, producing more coherent videos. With the proposed architecture and strategies, Video-ControlNet can achieve resource-efficient convergence and generate superior quality and consistent videos with fine-grained control. Extensive experiments demonstrate its success i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;RCPOM&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#29289;&#26009;&#25644;&#36816;&#20013;&#30340;&#33258;&#21160;&#24341;&#23548;&#36710;&#35843;&#24230;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;Lagrangian&#26494;&#24347;&#21644;&#26080;&#25928;&#21160;&#20316;&#23631;&#34109;&#65292;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#21160;&#24577;&#20107;&#20214;&#21644;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13824</link><description>&lt;p&gt;
&#21160;&#24577;&#29289;&#26009;&#25644;&#36816;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning for Dynamic Material Handling. (arXiv:2305.13824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;RCPOM&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#29289;&#26009;&#25644;&#36816;&#20013;&#30340;&#33258;&#21160;&#24341;&#23548;&#36710;&#35843;&#24230;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;Lagrangian&#26494;&#24347;&#21644;&#26080;&#25928;&#21160;&#20316;&#23631;&#34109;&#65292;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#21160;&#24577;&#20107;&#20214;&#21644;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#26009;&#25644;&#36816;&#26159;&#26580;&#24615;&#21046;&#36896;&#31995;&#32479;&#30340;&#26680;&#24515;&#37096;&#20998;&#20043;&#19968;&#65292;&#28041;&#21450;&#33258;&#21160;&#21270;&#36710;&#36742;&#22312;&#24037;&#20316;&#31449;&#20043;&#38388;&#30340;&#29289;&#26009;&#23384;&#20648;&#21644;&#25644;&#36816;&#12290;&#29289;&#26009;&#25644;&#36816;&#30340;&#25913;&#21892;&#21487;&#20197;&#20419;&#36827;&#21046;&#36896;&#31995;&#32479;&#30340;&#25972;&#20307;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#20219;&#21153;&#23433;&#25490;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#21457;&#29983;&#30340;&#21160;&#24577;&#20107;&#20214;&#23545;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;&#21160;&#24577;&#29289;&#26009;&#25644;&#36816;&#20013;&#30340;&#33258;&#21160;&#24341;&#23548;&#36710;&#36827;&#34892;&#35843;&#24230;&#12290;&#22312;&#19968;&#20123;&#30495;&#23454;&#22330;&#26223;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#23558;&#26410;&#30693;&#30340;&#26032;&#20219;&#21153;&#21644;&#24847;&#22806;&#30340;&#36710;&#36742;&#25925;&#38556;&#35270;&#20026;&#25105;&#20204;&#38382;&#39064;&#20013;&#30340;&#21160;&#24577;&#20107;&#20214;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#32771;&#34385;&#21040;&#36831;&#21040;&#21644;&#21487;&#29992;&#36710;&#36742;&#20316;&#20026;&#32047;&#31215;&#32422;&#26463;&#21644;&#30636;&#26102;&#32422;&#26463;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;RCPOM&#65292;&#23427;&#32467;&#21512;&#20102;Lagrangian&#26494;&#24347;&#21644;&#26080;&#25928;&#21160;&#20316;&#23631;&#34109;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#22788;&#29702;&#21160;&#24577;&#20107;&#20214;&#21644;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the core parts of flexible manufacturing systems, material handling involves storage and transportation of materials between workstations with automated vehicles. The improvement in material handling can impulse the overall efficiency of the manufacturing system. However, the occurrence of dynamic events during the optimisation of task arrangements poses a challenge that requires adaptability and effectiveness. In this paper, we aim at the scheduling of automated guided vehicles for dynamic material handling. Motivated by some real-world scenarios, unknown new tasks and unexpected vehicle breakdowns are regarded as dynamic events in our problem. We formulate the problem as a constrained Markov decision process which takes into account tardiness and available vehicles as cumulative and instantaneous constraints, respectively. An adaptive constrained reinforcement learning algorithm that combines Lagrangian relaxation and invalid action masking, named RCPOM, is proposed to addr
&lt;/p&gt;</description></item><item><title>XRoute&#29615;&#22659;&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#36335;&#30001;&#29615;&#22659;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#31471;&#21040;&#31471;&#36335;&#30001;&#26694;&#26550;&#20013;&#36873;&#25321;&#21644;&#36335;&#30001;&#32593;&#32476;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#25903;&#25345;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#22810;&#23454;&#20363;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.13823</link><description>&lt;p&gt;
XRoute&#29615;&#22659;&#65306;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#36335;&#30001;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
XRoute Environment: A Novel Reinforcement Learning Environment for Routing. (arXiv:2305.13823v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13823
&lt;/p&gt;
&lt;p&gt;
XRoute&#29615;&#22659;&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#36335;&#30001;&#29615;&#22659;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#31471;&#21040;&#31471;&#36335;&#30001;&#26694;&#26550;&#20013;&#36873;&#25321;&#21644;&#36335;&#30001;&#32593;&#32476;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#25903;&#25345;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#22810;&#23454;&#20363;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#30001;&#26159;&#29616;&#20195;&#35774;&#35745;&#33258;&#21160;&#21270;&#27969;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#32791;&#26102;&#30340;&#38454;&#27573;&#65292;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#24040;&#22823;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#25913;&#21892;&#36335;&#30001;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30740;&#31350;&#20013;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#30340;&#36335;&#30001;&#38382;&#39064;&#35268;&#27169;&#22826;&#23567;&#65292;&#26080;&#27861;&#22312;&#21830;&#19994;EDA&#24037;&#20855;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;XRoute&#29615;&#22659;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#20854;&#20013;&#20195;&#29702;&#34987;&#35757;&#32451;&#22312;&#20808;&#36827;&#30340;&#31471;&#21040;&#31471;&#36335;&#30001;&#26694;&#26550;&#20013;&#36873;&#25321;&#21644;&#36335;&#30001;&#32593;&#32476;&#12290;&#36825;&#20010;&#29615;&#22659;&#21487;&#20197;&#24555;&#36895;&#23433;&#20840;&#19988;&#21487;&#37325;&#22797;&#22320;&#27979;&#35797;&#26032;&#31639;&#27861;&#21644;&#24819;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#26131;&#20110;&#20351;&#29992;&#65292;&#23450;&#21046;&#21644;&#28155;&#21152;&#20854;&#20182;&#22330;&#26223;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#23485;&#26494;&#30340;&#24320;&#28304;&#35768;&#21487;&#19979;&#25552;&#20379;&#25903;&#25345;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#22810;&#23454;&#20363;&#23454;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#23398;&#20064;&#20219;&#21153;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#33455;&#29255;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Routing is a crucial and time-consuming stage in modern design automation flow for advanced technology nodes. Great progress in the field of reinforcement learning makes it possible to use those approaches to improve the routing quality and efficiency. However, the scale of the routing problems solved by reinforcement learning-based methods in recent studies is too small for these methods to be used in commercial EDA tools. We introduce the XRoute Environment, a new reinforcement learning environment where agents are trained to select and route nets in an advanced, end-to-end routing framework. Novel algorithms and ideas can be quickly tested in a safe and reproducible manner in it. The resulting environment is challenging, easy to use, customize and add additional scenarios, and it is available under a permissive open-source license. In addition, it provides support for distributed deployment and multi-instance experiments. We propose two tasks for learning and build a full-chip test 
&lt;/p&gt;</description></item><item><title>GenSpectrum&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;GPT-4&#20316;&#20026;LLM&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#22797;&#26434;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#20026;&#20844;&#20849;&#21355;&#29983;&#26426;&#26500;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#19968;&#33324;&#22823;&#20247;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#30452;&#35266;&#21644;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#19982;COVID-19&#30456;&#20851;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.13821</link><description>&lt;p&gt;
GenSpectrum&#32842;&#22825;&#26426;&#22120;&#20154;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
GenSpectrum Chat: Data Exploration in Public Health Using Large Language Models. (arXiv:2305.13821v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13821
&lt;/p&gt;
&lt;p&gt;
GenSpectrum&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;GPT-4&#20316;&#20026;LLM&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#22797;&#26434;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#20026;&#20844;&#20849;&#21355;&#29983;&#26426;&#26500;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#19968;&#33324;&#22823;&#20247;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#30452;&#35266;&#21644;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#19982;COVID-19&#30456;&#20851;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;&#65306;COVID-19&#22823;&#27969;&#34892;&#20984;&#26174;&#20102;&#20351;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#21644;&#31185;&#23398;&#35265;&#35299;&#26131;&#20110;&#20844;&#20849;&#21355;&#29983;&#26426;&#26500;&#65292;&#19968;&#33324;&#22823;&#20247;&#21644;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#25506;&#32034;&#30340;&#37325;&#35201;&#24615;&#12290;&#20998;&#20139;&#25968;&#25454;&#21644;&#35265;&#35299;&#30340;&#20808;&#36827;&#26041;&#27861;&#21253;&#25324;&#23450;&#26399;&#26356;&#26032;&#25253;&#21578;&#21644;Web&#20202;&#34920;&#26495;&#12290;&#20294;&#23427;&#20204;&#38754;&#20020;&#25968;&#25454;&#25506;&#32034;&#31616;&#21333;&#24615;&#21644;&#28789;&#27963;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#21033;&#29992;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#65292;&#21487;&#20197;&#20811;&#26381;&#36825;&#31181;&#26435;&#34913;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#8220;GenSpectrum Chat&#8221;&#65288;https://cov-spectrum.org/chat&#65289;&#65292;&#23427;&#20351;&#29992;GPT-4&#20316;&#20026;&#22522;&#30784;LLM&#26469;&#25506;&#32034;SARS-CoV-2&#22522;&#22240;&#32452;&#27979;&#24207;&#25968;&#25454;&#12290;&#22312;&#26469;&#33258;&#30495;&#23454;&#29992;&#25143;&#30340;500&#20010;&#36755;&#20837;&#20013;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#20026;453&#20010;&#25552;&#31034;&#25552;&#20379;&#20102;&#27491;&#30830;&#31572;&#26696;&#65307; 13&#20010;&#25552;&#31034;&#25552;&#20379;&#20102;&#38169;&#35823;&#31572;&#26696;&#65292;&#23613;&#31649;34&#20010;&#25552;&#31034;&#30340;&#38382;&#39064;&#22312;&#33539;&#22260;&#20869;&#65292;&#20294;&#23427;&#27809;&#26377;&#25552;&#20379;&#31572;&#26696;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#26469;&#33258;10&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#36755;&#20837;&#65292;&#23613;&#31649;GPT-4&#27809;&#26377;&#36827;&#34892;&#22810;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23427;&#20173;&#20026;&#26576;&#20123;&#26597;&#35810;&#25552;&#20379;&#20102;&#27491;&#30830;&#31572;&#26696;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#22797;&#26434;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#20026;&#20844;&#20849;&#21355;&#29983;&#26426;&#26500;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#19968;&#33324;&#22823;&#20247;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#30452;&#35266;&#21644;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#19982;COVID-19&#30456;&#20851;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: The COVID-19 pandemic highlighted the importance of making epidemiological data and scientific insights easily accessible and explorable for public health agencies, the general public, and researchers. State-of-the-art approaches for sharing data and insights included regularly updated reports and web dashboards. However, they face a trade-off between the simplicity and flexibility of data exploration. With the capabilities of recent large language models (LLMs) such as GPT-4, this trade-off can be overcome.  Results: We developed the chatbot "GenSpectrum Chat" (https://cov-spectrum.org/chat) which uses GPT-4 as the underlying large language model (LLM) to explore SARS-CoV-2 genomic sequencing data. Out of 500 inputs from real-world users, the chatbot provided a correct answer for 453 prompts; an incorrect answer for 13 prompts, and no answer although the question was within scope for 34 prompts. We also tested the chatbot with inputs from 10 different languages, and desp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;N&#21040;&#19968;&#30340;&#34920;&#31034;&#21305;&#37197;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;NORM&#65292;&#36890;&#36807;&#19968;&#31181;&#29305;&#24449;&#21464;&#25442;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#33021;&#20445;&#30041;&#25945;&#24072;&#32593;&#32476;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#20351;&#24471;&#23398;&#29983;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#36924;&#36817;&#25945;&#24072;&#32593;&#32476;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13803</link><description>&lt;p&gt;
&#22522;&#20110;N&#21040;&#19968;&#30340;&#34920;&#31034;&#21305;&#37197;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
NORM: Knowledge Distillation via N-to-One Representation Matching. (arXiv:2305.13803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;N&#21040;&#19968;&#30340;&#34920;&#31034;&#21305;&#37197;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;NORM&#65292;&#36890;&#36807;&#19968;&#31181;&#29305;&#24449;&#21464;&#25442;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#33021;&#20445;&#30041;&#25945;&#24072;&#32593;&#32476;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#20351;&#24471;&#23398;&#29983;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#36924;&#36817;&#25945;&#24072;&#32593;&#32476;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#39044;&#36873;&#30340;&#24072;&#29983;&#23618;&#23545;&#20043;&#38388;&#30340;&#19968;&#23545;&#19968;&#34920;&#31034;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;N&#21040;&#19968;&#34920;&#31034;&#65288;NORM&#65289;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#30001;&#20004;&#20010;&#32447;&#24615;&#23618;&#32452;&#25104;&#30340;&#31616;&#21333;&#29305;&#24449;&#21464;&#25442;&#65288;FT&#65289;&#27169;&#22359;&#12290;&#20026;&#20102;&#20445;&#30041;&#30001;&#25945;&#24072;&#32593;&#32476;&#23398;&#20064;&#30340;&#23436;&#25972;&#20449;&#24687;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#30340;FT&#27169;&#22359;&#20165;&#25554;&#20837;&#22312;&#23398;&#29983;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#20010;&#21367;&#31215;&#23618;&#20043;&#21518;&#12290;&#31532;&#19968;&#23618;&#32447;&#24615;&#23618;&#23558;&#23398;&#29983;&#34920;&#31034;&#25237;&#23556;&#21040;&#19968;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#35813;&#29305;&#24449;&#31354;&#38388;&#30340;&#29305;&#24449;&#36890;&#36947;&#25968;&#26159;&#26368;&#21518;&#19968;&#20010;&#21367;&#31215;&#23618;&#20013;&#25945;&#24072;&#34920;&#31034;&#30340;N&#20493;&#65292;&#31532;&#20108;&#20010;&#32447;&#24615;&#23618;&#23558;&#25193;&#23637;&#30340;&#36755;&#20986;&#25910;&#32553;&#22238;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#12290;&#36890;&#36807;&#23558;&#25193;&#23637;&#30340;&#23398;&#29983;&#34920;&#31034;&#39034;&#24207;&#20998;&#25104;N&#20010;&#19981;&#37325;&#21472;&#30340;&#29305;&#24449;&#27573;&#65292;&#27599;&#20010;&#27573;&#20855;&#26377;&#19982;&#25945;&#24072;&#30340;&#30456;&#21516;&#25968;&#37327;&#30340;&#29305;&#24449;&#36890;&#36947;&#65292;&#23427;&#20204;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#24378;&#21046;&#36817;&#20284;&#20110;&#25945;&#24072;&#30340;&#34920;&#31034;.
&lt;/p&gt;
&lt;p&gt;
Existing feature distillation methods commonly adopt the One-to-one Representation Matching between any pre-selected teacher-student layer pair. In this paper, we present N-to-One Representation (NORM), a new two-stage knowledge distillation method, which relies on a simple Feature Transform (FT) module consisting of two linear layers. In view of preserving the intact information learnt by the teacher network, during training, our FT module is merely inserted after the last convolutional layer of the student network. The first linear layer projects the student representation to a feature space having N times feature channels than the teacher representation from the last convolutional layer, and the second linear layer contracts the expanded output back to the original feature space. By sequentially splitting the expanded student representation into N non-overlapping feature segments having the same number of feature channels as the teacher's, they can be readily forced to approximate t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24067;&#26391;&#26725;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#33410;&#30465;&#37319;&#26679;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13796</link><description>&lt;p&gt;
SE-Bridge: &#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24067;&#26391;&#26725;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SE-Bridge: Speech Enhancement with Consistent Brownian Bridge. (arXiv:2305.13796v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13796
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24067;&#26391;&#26725;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#33410;&#30465;&#37319;&#26679;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SE-Bridge&#30340;&#26032;&#22411;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#12290;&#22312;&#26368;&#36817;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#20043;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#23454;&#29616;&#35821;&#38899;&#22686;&#24378;&#12290;&#27599;&#20010;SDE&#23545;&#24212;&#19968;&#20010;&#27010;&#29575;&#27969;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;PF-ODE&#65289;&#65292;PF-ODE&#35299;&#30340;&#36712;&#36857;&#30001;&#19981;&#21516;&#26102;&#21051;&#30340;&#35821;&#38899;&#29366;&#24577;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30830;&#20445;&#21516;&#19968;PF-ODE&#36712;&#36857;&#19978;&#30340;&#20219;&#20309;&#35821;&#38899;&#29366;&#24577;&#23545;&#24212;&#20110;&#21516;&#19968;&#21021;&#22987;&#29366;&#24577;&#12290;&#36890;&#36807;&#38598;&#25104;&#24067;&#26391;&#26725;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#21487;&#25026;&#24615;&#30340;&#35821;&#38899;&#26679;&#26412;&#65292;&#26080;&#38656;&#25932;&#23545;&#35757;&#32451;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23558;&#19968;&#33268;&#24615;&#27169;&#22411;&#24212;&#29992;&#20110;SE&#20219;&#21153;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#22522;&#32447;&#65292;&#33410;&#30465;&#20102;15&#20493;&#30340;&#37319;&#26679;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;SE-Bridge&#22312;SE&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#30475;&#25252;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SE-Bridge, a novel method for speech enhancement (SE). After recently applying the diffusion models to speech enhancement, we can achieve speech enhancement by solving a stochastic differential equation (SDE). Each SDE corresponds to a probabilistic flow ordinary differential equation (PF-ODE), and the trajectory of the PF-ODE solution consists of the speech states at different moments. Our approach is based on consistency model that ensure any speech states on the same PF-ODE trajectory, correspond to the same initial state. By integrating the Brownian Bridge process, the model is able to generate high-intelligibility speech samples without adversarial training. This is the first attempt that applies the consistency models to SE task, achieving state-of-the-art results in several metrics while saving 15 x the time required for sampling compared to the diffusion-based baseline. Our experiments on multiple datasets demonstrate the effectiveness of SE-Bridge in SE. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#26041;&#27861;&#19982;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;QD-RL&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#19979;&#35757;&#32451;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.13795</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36817;&#31471;&#31574;&#30053;&#26799;&#24230;&#26641;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning. (arXiv:2305.13795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#26041;&#27861;&#19982;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;QD-RL&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#19979;&#35757;&#32451;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22521;&#35757;&#36890;&#24120;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#26159;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#24378;&#21270;&#23398;&#20064;(QD-RL)&#26159;&#19968;&#31867;&#26032;&#20852;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#21644;RL&#30340;&#35265;&#35299;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#19968;&#31995;&#21015;&#20851;&#20110;&#34892;&#20026;&#23884;&#20837;&#30340;&#39640;&#24615;&#33021;&#21644;&#34892;&#20026;&#22810;&#26679;&#24615;&#30340;&#31574;&#30053;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;QD-RL&#26041;&#27861;&#36804;&#20170;&#20026;&#27490;&#21033;&#29992;&#20102;&#26679;&#26412;&#26377;&#25928;&#30340;&#31163;&#31574;&#30053;RL&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#30340;&#36827;&#27493;&#24050;&#32463;&#25171;&#24320;&#20102;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#31639;&#27861;&#30340;&#22823;&#38376;&#65292;&#32780;&#23558;&#29616;&#26377;&#30340;&#31163;&#31574;&#30053;QD-RL&#26041;&#27861;&#25193;&#23637;&#21040;&#36825;&#20123;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#37319;&#29992;&#20102;&#33021;&#22815;&#21033;&#29992;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#31561;&#31574;&#30053;&#26041;&#27861;&#19982;QD&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QD-RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generally capable agents that perform well in unseen dynamic environments is a long-term goal of robot learning. Quality Diversity Reinforcement Learning (QD-RL) is an emerging class of reinforcement learning (RL) algorithms that blend insights from Quality Diversity (QD) and RL to produce a collection of high performing and behaviorally diverse policies with respect to a behavioral embedding. Existing QD-RL approaches have thus far taken advantage of sample-efficient off-policy RL algorithms. However, recent advances in high-throughput, massively parallelized robotic simulators have opened the door for algorithms that can take advantage of such parallelism, and it is unclear how to scale existing off-policy QD-RL methods to these new data-rich regimes. In this work, we take the first steps to combine on-policy RL methods, specifically Proximal Policy Optimization (PPO), that can leverage massive parallelism, with QD, and propose a new QD-RL method with these high-throughput s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#25512;&#26029;&#33021;&#21147;&#26377;&#38480;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;NLU&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#24615;&#36136;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.13788</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#25512;&#29702;&#21644;&#20135;&#29983;&#20998;&#27495;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer and Disagree Like Humans?. (arXiv:2305.13788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#25512;&#26029;&#33021;&#21147;&#26377;&#38480;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;NLU&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#24615;&#36136;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#24191;&#27867;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25104;&#32489;&#12290;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#65292;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#37319;&#26679;&#26631;&#35760;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#12290;&#20294;&#26159;&#65292;LLM&#24456;&#38590;&#19982;&#20154;&#31867;&#30340;&#20998;&#27495;&#20998;&#24067;&#39640;&#24230;&#23545;&#40784;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#12290;&#26412;&#25991;&#20351;&#29992; Monte Carlo Reconstruction&#65288;MCR&#65289;&#21644; Log Probability Reconstruction&#65288;LPR&#65289;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#35780;&#20272;&#20102;LLM&#20998;&#24067;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#35299;&#20915;NLI&#20219;&#21153;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#65292;&#21516;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#20998;&#27495;&#20998;&#24067;&#65292;&#36825;&#23545;&#20854;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#33021;&#21147;&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#30340;&#29305;&#24615;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown stellar achievements in solving a broad range of tasks. When generating text, it is common to sample tokens from these models: whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of Natural Language Inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques: Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution, raising concerns about their natural language understanding (NLU) ability and their representativeness of human users.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#24863;&#30693;&#27979;&#35797;&#8221;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27979;&#35797;&#28085;&#30422;&#20102;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#31561;&#25512;&#29702;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13786</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#12298;&#24863;&#30693;&#27979;&#35797;&#65306;&#22810;&#27169;&#24577;&#35270;&#39057;&#27169;&#22411;&#30340;&#35786;&#26029;&#22522;&#20934;&#12299;
&lt;/p&gt;
&lt;p&gt;
Perception Test: A Diagnostic Benchmark for Multimodal Video Models. (arXiv:2305.13786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#24863;&#30693;&#27979;&#35797;&#8221;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27979;&#35797;&#28085;&#30422;&#20102;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#31561;&#25512;&#29702;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#8212;&#8212;&#24863;&#30693;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#20363;&#22914; Flamingo&#12289;BEiT-3 &#25110; GPT-4&#65289;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#25216;&#33021;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20391;&#37325;&#20110;&#35745;&#31639;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#12289;&#26816;&#27979;&#25110;&#36319;&#36394;&#65289;&#19981;&#21516;&#65292;&#24863;&#30693;&#27979;&#35797;&#20391;&#37325;&#20110;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#36328;&#36234;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25512;&#29702;&#31867;&#22411;&#65288;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#65289;&#65292;&#20197;&#25552;&#20379;&#20840;&#38754;&#32780;&#39640;&#25928;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#36890;&#36807;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#25110;&#26377;&#38480;&#24494;&#35843;&#19979;&#25361;&#36873;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#20123;&#30446;&#30340;&#65292;&#24863;&#30693;&#27979;&#35797;&#20171;&#32461;&#20102;11.6k&#31181;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#65292;&#24179;&#22343;&#38271;&#24230;&#20026;23&#31186;&#65292;&#26088;&#22312;&#23637;&#31034;&#24863;&#30693;&#19978;&#26377;&#36259;&#30340;&#24773;&#22659;&#65292;&#30001;&#20840;&#29699;&#32422;100&#21517;&#21442;&#19982;&#32773;&#25293;&#25668;&#12290;&#36825;&#20123;&#35270;&#39057;&#23494;&#38598;&#22320;&#24102;&#26377;&#20845;&#31181;&#26631;&#31614;&#65288;&#22810;&#39033;&#36873;&#25321;&#21644;&#22522;&#20110;&#35270;&#39057;&#38382;&#39064;&#22238;&#31572;&#65292;&#23545;&#35937;a&#65289;
&lt;/p&gt;
&lt;p&gt;
We propose a novel multimodal video benchmark - the Perception Test - to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, BEiT-3, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the Perception Test introduces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;2.5D&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#36317;&#31163;&#21644;&#33021;&#32791;&#36798;&#21040;&#33391;&#22909;&#26435;&#34913;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2305.13783</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22320;&#38754;&#36710;&#36742;&#22312;&#36234;&#37326;&#22320;&#24418;&#29615;&#22659;&#19979;&#30340;&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-based Multi-objective Path Planning on the Off-road Terrain Environment for Ground Vehicles. (arXiv:2305.13783v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;2.5D&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#36317;&#31163;&#21644;&#33021;&#32791;&#36798;&#21040;&#33391;&#22909;&#26435;&#34913;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#19978;&#22369;&#21644;&#19979;&#22369;&#20043;&#38388;&#30340;&#33021;&#32791;&#25928;&#29575;&#24046;&#24322;&#24040;&#22823;&#65292;&#22312;&#22797;&#26434;&#30340;&#36234;&#37326;&#22320;&#24418;&#29615;&#22659;&#65288;2.5D&#22320;&#22270;&#65289;&#19978;&#65292;&#26368;&#30701;&#36335;&#24452;&#19981;&#19968;&#23450;&#26159;&#33021;&#32791;&#26368;&#23569;&#30340;&#36335;&#24452;&#12290;&#23545;&#20110;&#20219;&#20309;&#33021;&#28304;&#25935;&#24863;&#30340;&#36710;&#36742;&#26469;&#35828;&#65292;&#23454;&#29616;&#36317;&#31163;&#21644;&#33021;&#32791;&#22312;2.5D&#36335;&#24452;&#35268;&#21010;&#19978;&#33391;&#22909;&#30340;&#26435;&#34913;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;2.5D&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65288;DMOP&#65289;&#12290;DMOP&#21487;&#20197;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#39640;&#25928;&#22320;&#25214;&#21040;&#25152;&#38656;&#36335;&#24452;&#65306;(1)&#23558;&#39640;&#20998;&#36776;&#29575;&#30340;2.5D&#22320;&#22270;&#36716;&#25442;&#20026;&#23567;&#23610;&#23544;&#22320;&#22270;&#12290;(2)&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#22312;&#23567;&#23610;&#23544;&#22320;&#22270;&#19978;&#25214;&#21040;&#25152;&#38656;&#36335;&#24452;&#12290;(3)&#20351;&#29992;&#36335;&#24452;&#22686;&#24378;&#26041;&#27861;&#23558;&#35745;&#21010;&#36335;&#24452;&#26500;&#24314;&#21040;&#21407;&#22987;&#39640;&#20998;&#36776;&#29575;&#22320;&#22270;&#19978;&#12290;&#27492;&#22806;&#65292;&#36824;&#24212;&#29992;&#20102;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#21644;&#22870;&#21169;&#22609;&#36896;&#29702;&#35770;&#26469;&#35757;&#32451;DQN&#12290;&#22870;&#21169;&#20989;&#25968;&#32467;&#21512;&#20102;&#22320;&#24418;&#12289;&#36317;&#31163;&#21644;&#36793;&#30028;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the energy-consumption efficiency between up-slope and down-slope is hugely different, a path with the shortest length on a complex off-road terrain environment (2.5D map) is not always the path with the least energy consumption. For any energy-sensitive vehicles, realizing a good trade-off between distance and energy consumption on 2.5D path planning is significantly meaningful. In this paper, a deep reinforcement learning-based 2.5D multi-objective path planning method (DMOP) is proposed. The DMOP can efficiently find the desired path with three steps: (1) Transform the high-resolution 2.5D map into a small-size map. (2) Use a trained deep Q network (DQN) to find the desired path on the small-size map. (3) Build the planned path to the original high-resolution map using a path enhanced method. In addition, the imitation learning method and reward shaping theory are applied to train the DQN. The reward function is constructed with the information of terrain, distance, border. S
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24847;&#22270;&#26465;&#20214;&#19979;&#30340;&#21453;&#35805;&#35821;&#29983;&#25104;&#26041;&#27861;QUARC&#65292;&#22522;&#20110;IntentCONAN&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#21521;&#37327;&#37327;&#21270;&#34920;&#31034;&#21644;PerFuMe&#34701;&#21512;&#27169;&#22359;&#23454;&#29616;&#29305;&#23450;&#24847;&#22270;&#30340;&#21453;&#35805;&#35821;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.13776</link><description>&lt;p&gt;
&#25345;&#32493;&#34701;&#21512;&#24847;&#22270;&#20998;&#24067;&#23398;&#20064;&#30340;&#21453;&#35805;&#35821;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Counterspeeches up my sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation. (arXiv:2305.13776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13776
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24847;&#22270;&#26465;&#20214;&#19979;&#30340;&#21453;&#35805;&#35821;&#29983;&#25104;&#26041;&#27861;QUARC&#65292;&#22522;&#20110;IntentCONAN&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#21521;&#37327;&#37327;&#21270;&#34920;&#31034;&#21644;PerFuMe&#34701;&#21512;&#27169;&#22359;&#23454;&#29616;&#29305;&#23450;&#24847;&#22270;&#30340;&#21453;&#35805;&#35821;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#35805;&#35821;&#24050;&#34987;&#35777;&#26126;&#26159;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27599;&#31181;&#22330;&#26223;&#65292;&#20855;&#26377;&#29305;&#23450;&#24847;&#22270;&#30340;&#21453;&#35805;&#35821;&#21487;&#33021;&#24182;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24847;&#22270;&#26465;&#20214;&#19979;&#30340;&#21453;&#35805;&#35821;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;IntentCONAN&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;6831&#20010;&#21453;&#35805;&#35821;&#65292;&#20998;&#20026;&#20116;&#31181;&#24847;&#22270;&#65306;&#20449;&#24687;&#12289;&#35892;&#36131;&#12289;&#38382;&#39064;&#12289;&#31215;&#26497;&#21644;&#24189;&#40664;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;QUARC&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26377;&#20004;&#20010;&#38454;&#27573;&#65292;&#29992;&#20110;&#24847;&#22270;&#26465;&#20214;&#19979;&#30340;&#21453;&#35805;&#35821;&#29983;&#25104;&#12290;QUARC&#21033;&#29992;&#23398;&#20064;&#27599;&#31181;&#24847;&#22270;&#31867;&#21035;&#30340;&#21521;&#37327;&#37327;&#21270;&#34920;&#31034;&#65292;&#20197;&#21450;PerFuMe&#65292;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#29305;&#23450;&#24847;&#22270;&#30340;&#20449;&#24687;&#30340;&#26032;&#22411;&#34701;&#21512;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterspeech has been demonstrated to be an efficacious approach for combating hate speech. While various conventional and controlled approaches have been studied in recent years to generate counterspeech, a counterspeech with a certain intent may not be sufficient in every scenario. Due to the complex and multifaceted nature of hate speech, utilizing multiple forms of counter-narratives with varying intents may be advantageous in different circumstances. In this paper, we explore intent-conditioned counterspeech generation. At first, we develop IntentCONAN, a diversified intent-specific counterspeech dataset with 6831 counterspeeches conditioned on five intents, i.e., informative, denouncing, question, positive, and humour. Subsequently, we propose QUARC, a two-stage framework for intent-conditioned counterspeech generation. QUARC leverages vector-quantized representations learned for each intent category along with PerFuMe, a novel fusion module to incorporate intent-specific inform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#26356;&#22909;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13775</link><description>&lt;p&gt;
&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Concept-aware Training Improves In-context Learning Ability of Language Models. (arXiv:2305.13775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#26356;&#22909;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#22810;&#20010;Transformer&#31995;&#21015;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#25152;&#35859;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;(ICL)&#65292;&#34920;&#29616;&#20026;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20219;&#21153;&#36827;&#34892;&#35843;&#33410;&#26469;&#25913;&#21464;&#33258;&#36523;&#30340;&#21151;&#33021;&#12290;&#20043;&#21069;&#30340;&#19968;&#20123;&#30740;&#31350;&#35748;&#20026;&#65292;ICL&#30340;&#20986;&#29616;&#26159;&#30001;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#25110;&#22810;&#20219;&#21153;&#35757;&#32451;&#35268;&#27169;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#19968;&#20123;&#29702;&#35770;&#30740;&#31350;&#35748;&#20026;&#65292;ICL&#30340;&#20986;&#29616;&#26159;&#30001;&#20855;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#23646;&#24615;&#24341;&#36215;&#30340;&#65292;&#24182;&#22312;&#23567;&#35268;&#27169;&#30340;&#20223;&#30495;&#29615;&#22659;&#20013;&#21019;&#24314;&#20102;&#21151;&#33021;&#24615;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#12290;&#20511;&#37492;&#25968;&#25454;&#23646;&#24615;&#39537;&#21160;ICL&#30340;&#26368;&#26032;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#24314;&#35757;&#32451;&#22330;&#26223;&#65292;&#20351;&#24471;&#27169;&#22411;&#25429;&#25417;&#21040;&#31867;&#27604;&#24605;&#32500;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451; (CoAT) &#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#65292;&#20351;&#29992;CoAT&#35757;&#32451;&#24471;&#21040;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent language models (LMs) of Transformers family exhibit so-called in-context learning (ICL) ability, manifested in the LMs' ability to modulate their function by a task described in a natural language input. Previous work curating these models assumes that ICL emerges from vast over-parametrization or the scale of multi-task training. However, a complementary branch of recent theoretical work attributes ICL emergence to specific properties of training data and creates functional in-context learners in small-scale, synthetic settings.  Inspired by recent findings on data properties driving the emergence of ICL, we propose a method to create LMs able to better utilize the in-context information, by constructing training scenarios where it is beneficial for the LM to capture the analogical reasoning concepts. We measure that data sampling of Concept-aware Training (CoAT) consistently improves models' reasoning ability. As a result, the in-context learners trained with CoAT on onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#27493;&#24577;&#20998;&#26512;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#39592;&#26550;&#20026;&#22522;&#30784;&#30340;&#27493;&#24577;&#36776;&#35782;&#26041;&#27861;&#12290;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24037;&#20855;&#12289;&#26041;&#27861;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.13765</link><description>&lt;p&gt;
&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#29992;&#20110;&#27493;&#24577;&#36776;&#35782;&#65306;&#25968;&#25454;&#38598;&#19982;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Human Body Pose Estimation for Gait Identification: A Comprehensive Survey of Datasets and Models. (arXiv:2305.13765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#27493;&#24577;&#20998;&#26512;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#39592;&#26550;&#20026;&#22522;&#30784;&#30340;&#27493;&#24577;&#36776;&#35782;&#26041;&#27861;&#12290;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24037;&#20855;&#12289;&#26041;&#27861;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#36776;&#35782;&#26159;&#19968;&#20010;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#23433;&#20840;&#39046;&#22495;&#12290;&#27493;&#24577;&#35782;&#21035;&#26159;&#26368;&#26041;&#20415;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#21487;&#20197;&#22312;&#36828;&#36317;&#31163;&#35782;&#21035;&#20010;&#20154;&#32780;&#26080;&#38656;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#34429;&#28982;&#39592;&#26550;&#20026;&#22522;&#30784;&#30340;&#20154;&#29289;&#35782;&#21035;&#27491;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#32780;&#19988;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20294;&#29616;&#26377;&#30340;&#32508;&#36848;&#30740;&#31350;&#32570;&#20047;&#23545;&#39592;&#26550;&#20026;&#22522;&#30784;&#30340;&#27493;&#24577;&#36776;&#35782;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#27493;&#24577;&#20998;&#26512;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#32508;&#36848;&#65292;&#38416;&#36848;&#20102;&#39592;&#26550;&#20026;&#22522;&#30784;&#30340;&#27493;&#24577;&#36776;&#35782;&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#28085;&#30422;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24037;&#20855;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#21508;&#31181;&#31867;&#22411;&#65292;&#20197;&#21450;&#30456;&#20851;&#25361;&#25112;&#12289;&#38480;&#21046;&#21644;&#24212;&#29992;&#39046;&#22495;&#12290;&#38024;&#23545;&#27599;&#20010;&#26041;&#38754;&#37117;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Person identification is a problem that has received substantial attention, particularly in security domains. Gait recognition is one of the most convenient approaches enabling person identification at a distance without the need of high-quality images. There are several review studies addressing person identification such as the utilization of facial images, silhouette images, and wearable sensor. Despite skeleton-based person identification gaining popularity while overcoming the challenges of traditional approaches, existing survey studies lack the comprehensive review of skeleton-based approaches to gait identification. We present a detailed review of the human pose estimation and gait analysis that make the skeleton-based approaches possible. The study covers various types of related datasets, tools, methodologies, and evaluation metrics with associated challenges, limitations, and application domains. Detailed comparisons are presented for each of these aspects with recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#39537;&#21160;&#30340;&#36828;&#31243;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31687;&#31456;&#35757;&#32451;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#31687;&#31456;&#20998;&#26512;&#24615;&#33021;&#65292;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13755</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#39537;&#21160;&#30340;&#36828;&#31243;&#30417;&#30563;&#26694;&#26550;&#23454;&#29616;&#23439;&#35266;&#23618;&#38754;&#30340;&#31687;&#31456;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Topic-driven Distant Supervision Framework for Macro-level Discourse Parsing. (arXiv:2305.13755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#39537;&#21160;&#30340;&#36828;&#31243;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31687;&#31456;&#35757;&#32451;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#31687;&#31456;&#20998;&#26512;&#24615;&#33021;&#65292;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31687;&#31456;&#20998;&#26512;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#30446;&#30340;&#26159;&#20998;&#26512;&#25991;&#26412;&#30340;&#20869;&#37096;&#20462;&#36766;&#32467;&#26500;&#12290;&#23613;&#31649;&#31070;&#32463;&#27169;&#22411;&#23384;&#22312;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#29992;&#20110;&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26469;&#20811;&#26381;&#27492;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#20363;&#22914;&#24773;&#24863;&#26497;&#24615;&#12289;&#27880;&#24847;&#21147;&#30697;&#38453;&#21644;&#20998;&#21106;&#27010;&#29575;&#65289;&#30340;&#32467;&#26524;&#26469;&#35299;&#26512;&#31687;&#31456;&#26641;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#39046;&#22495;&#20869;&#22806;&#20219;&#21153;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#25928;&#26524;&#36739;&#24046;&#24182;&#19988;&#19981;&#33021;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#20869;&#25968;&#25454;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#26694;&#26550;&#65292;&#23558;&#20027;&#39064;&#32467;&#26500;&#21644;&#20462;&#36766;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#21033;&#29992;&#36215;&#26469;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#21644;&#24072;&#29983;&#27169;&#22411;&#65292;&#29992;&#20110;&#37325;&#22797;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#20869;&#31687;&#31456;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#26694;&#26550;&#22312;&#35201;&#27714;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discourse parsing, the task of analyzing the internal rhetorical structure of texts, is a challenging problem in natural language processing. Despite the recent advances in neural models, the lack of large-scale, high-quality corpora for training remains a major obstacle. Recent studies have attempted to overcome this limitation by using distant supervision, which utilizes results from other NLP tasks (e.g., sentiment polarity, attention matrix, and segmentation probability) to parse discourse trees. However, these methods do not take into account the differences between in-domain and out-of-domain tasks, resulting in lower performance and inability to leverage the high-quality in-domain data for further improvement. To address these issues, we propose a distant supervision framework that leverages the relations between topic structure and rhetorical structure. Specifically, we propose two distantly supervised methods, based on transfer learning and the teacher-student model, that narr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#22256;&#38590;&#30446;&#26631;&#23398;&#20064;&#38382;&#39064;&#30340;L-SA&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#20027;&#21160;&#26597;&#35810;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;L-SA&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.13741</link><description>&lt;p&gt;
L-SA&#65306;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#22256;&#38590;&#30446;&#26631;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
L-SA: Learning Under-Explored Targets in Multi-Target Reinforcement Learning. (arXiv:2305.13741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13741
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#22256;&#38590;&#30446;&#26631;&#23398;&#20064;&#38382;&#39064;&#30340;L-SA&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#20027;&#21160;&#26597;&#35810;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;L-SA&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22810;&#20010;&#30446;&#26631;&#36827;&#34892;&#20132;&#20114;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#22810;&#30446;&#26631;&#20219;&#21153;&#12290;&#24403;&#24212;&#29992;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#36825;&#26679;&#30340;&#20219;&#21153;&#26102;&#65292;&#26576;&#20123;&#38590;&#20197;&#35775;&#38382;&#25110;&#20132;&#20114;&#30340;&#30446;&#26631;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#24573;&#35270;-&#36825;&#31181;&#22256;&#22659;&#31216;&#20026;&#25506;&#32034;&#22256;&#38590;&#30446;&#26631;&#38382;&#39064;&#65288;UTP&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#20027;&#21160;&#26597;&#35810;&#30340;&#23398;&#20064;&#26694;&#26550;L-SA&#65288;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#20027;&#21160;&#26597;&#35810;&#36827;&#34892;&#23398;&#20064;&#65289;&#12290;&#22312;L-SA&#26694;&#26550;&#20013;&#65292;&#33258;&#36866;&#24212;&#37319;&#26679;&#21160;&#24577;&#22320;&#20174;&#26368;&#39640;&#25104;&#21151;&#29575;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#20351;&#24471;&#23398;&#20064;&#20174;&#23481;&#26131;&#21040;&#22256;&#38590;&#30340;&#30446;&#26631;&#65292;&#20027;&#21160;&#26597;&#35810;&#21017;&#20419;&#20351;&#20195;&#29702;&#19982;&#38656;&#35201;&#26356;&#22810;&#32463;&#39564;&#25110;&#25506;&#32034;&#30340;&#25506;&#32034;&#22256;&#38590;&#30446;&#26631;&#26356;&#39057;&#32321;&#22320;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;L-SA&#26694;&#26550;&#25552;&#39640;&#20102;&#22810;&#20010;UTP&#22810;&#30446;&#26631;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#12290;&#21478;&#22806;&#65292;&#39044;&#35745;&#35813;&#25552;&#20986;&#30340;L-SA&#26694;&#26550;&#33021;&#22815;&#24212;&#29992;&#21040;&#20854;&#20182;&#28041;&#21450;&#22810;&#20010;&#23384;&#22312;UTP&#30340;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tasks that involve interaction with various targets are called multi-target tasks. When applying general reinforcement learning approaches for such tasks, certain targets that are difficult to access or interact with may be neglected throughout the course of training - a predicament we call Under-explored Target Problem (UTP). To address this problem, we propose L-SA (Learning by adaptive Sampling and Active querying) framework that includes adaptive sampling and active querying. In the L-SA framework, adaptive sampling dynamically samples targets with the highest increase of success rates at a high proportion, resulting in curricular learning from easy to hard targets. Active querying prompts the agent to interact more frequently with under-explored targets that need more experience or exploration. Our experimental results on visual navigation tasks show that the L-SA framework improves sample efficiency as well as success rates on various multi-target tasks with UTP. Also, it is expe
&lt;/p&gt;</description></item><item><title>i-Code Studio&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#12289;&#28789;&#27963;&#21644;&#21487;&#32452;&#21512;&#30340;&#35774;&#32622;&#65292;&#21487;&#20197;&#20351;&#24320;&#21457;&#20154;&#21592;&#24555;&#36895;&#36731;&#26494;&#22320;&#32452;&#21512;&#26368;&#20808;&#36827;&#30340;&#26381;&#21153;&#21644;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13738</link><description>&lt;p&gt;
i-Code Studio&#65306;&#19968;&#31181;&#21487;&#37197;&#32622;&#21644;&#21487;&#32452;&#21512;&#30340;&#32508;&#21512;AI&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
i-Code Studio: A Configurable and Composable Framework for Integrative AI. (arXiv:2305.13738v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13738
&lt;/p&gt;
&lt;p&gt;
i-Code Studio&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#12289;&#28789;&#27963;&#21644;&#21487;&#32452;&#21512;&#30340;&#35774;&#32622;&#65292;&#21487;&#20197;&#20351;&#24320;&#21457;&#20154;&#21592;&#24555;&#36895;&#36731;&#26494;&#22320;&#32452;&#21512;&#26368;&#20808;&#36827;&#30340;&#26381;&#21153;&#21644;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#38656;&#35201;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21487;&#20197;&#28085;&#30422;&#19981;&#21516;&#30340;&#27169;&#24577;&#21644;&#21151;&#33021;&#12290;&#32508;&#21512;AI&#26159;&#23454;&#29616;AGI&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#27169;&#22411;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#31181;&#28789;&#27963;&#21644;&#21487;&#32452;&#21512;&#30340;&#24179;&#21488;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#32452;&#21512;&#21644;&#21327;&#35843;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;i-Code Studio&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#37197;&#32622;&#21644;&#21487;&#32452;&#21512;&#30340;&#32508;&#21512;AI&#26694;&#26550;&#12290; i-Code Studio&#20197;&#26080;fine-tuning&#26041;&#24335;&#21327;&#35843;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290; i-Code Studio&#19981;&#20165;&#25552;&#20379;&#31616;&#21333;&#30340;&#27169;&#22411;&#32452;&#21512;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#12289;&#28789;&#27963;&#21644;&#21487;&#32452;&#21512;&#30340;&#35774;&#32622;&#65292;&#20351;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#36731;&#26494;&#22320;&#32452;&#21512;&#26368;&#20808;&#36827;&#30340;&#26381;&#21153;&#21644;&#25216;&#26415;&#65292;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#29305;&#23450;&#35201;&#27714;&#12290;i-Code Studio&#22312;&#22810;&#31181;&#38646;-shot&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence (AGI) requires comprehensive understanding and generation capabilities for a variety of tasks spanning different modalities and functionalities. Integrative AI is one important direction to approach AGI, through combining multiple models to tackle complex multimodal tasks. However, there is a lack of a flexible and composable platform to facilitate efficient and effective model composition and coordination. In this paper, we propose the i-Code Studio, a configurable and composable framework for Integrative AI. The i-Code Studio orchestrates multiple pre-trained models in a finetuning-free fashion to conduct complex multimodal tasks. Instead of simple model composition, the i-Code Studio provides an integrative, flexible, and composable setting for developers to quickly and easily compose cutting-edge services and technologies tailored to their specific requirements. The i-Code Studio achieves impressive results on a variety of zero-shot multimodal tasks,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13735</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models through Synthetic Feedback. (arXiv:2305.13735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#30340;LLMs&#25511;&#21046;&#65292;&#20363;&#22914;&#20351;&#23427;&#20204;&#25353;&#29031;&#29305;&#23450;&#30340;&#25351;&#20196;&#25805;&#20316;&#32780;&#19981;&#20250;&#20135;&#29983;&#26377;&#23475;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#31034;&#33539;&#21644;&#21453;&#39304;&#12290;&#26368;&#36817;&#65292;&#24320;&#28304;&#27169;&#22411;&#35797;&#22270;&#36890;&#36807;&#25552;&#28860;&#26469;&#33258;&#24050;&#23545;&#40784;&#30340;LLMs&#65288;&#22914;InstructGPT&#25110;ChatGPT&#65289;&#30340;&#25968;&#25454;&#26469;&#22797;&#21046;&#23545;&#40784;&#23398;&#20064;&#36807;&#31243;&#12290;&#34429;&#28982;&#36825;&#20010;&#36807;&#31243;&#20943;&#23569;&#20102;&#20154;&#21147;&#25104;&#26412;&#65292;&#20294;&#26159;&#26500;&#24314;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#25945;&#24072;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#23398;&#20064;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#31867;&#21171;&#21160;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#23567;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890;LLMs&#30340;&#21709;&#24212;&#36827;&#34892;&#21512;&#25104;&#21453;&#39304;&#30340;&#22870;&#21169;&#24314;&#27169;(RM)&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;RM&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs, e.g., making them follow given instructions while keeping them less toxic. However, it requires a significant amount of human demonstrations and feedback. Recently, open-sourced models have attempted to replicate the alignment learning process by distilling data from already aligned LLMs like InstructGPT or ChatGPT. While this process reduces human efforts, constructing these datasets has a heavy dependency on the teacher models. In this work, we propose a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM for simulating high-quality demonstrations to train a supervised policy and for further optimizing the model with reinforcement learning. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#32422;&#26463;&#25552;&#31034;&#29983;&#25104;&#65288;Co-Prompt&#65289;&#65292;&#36890;&#36807;&#20272;&#31639;&#26368;&#20339;&#25490;&#24207;&#26469;&#24341;&#23548; PLM &#29983;&#25104;&#30340;&#25991;&#26412;&#26397;&#21521;&#26368;&#20248;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-Prompt &#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#37325;&#25490;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13729</link><description>&lt;p&gt;
&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#22312;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker. (arXiv:2305.13729v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#32422;&#26463;&#25552;&#31034;&#29983;&#25104;&#65288;Co-Prompt&#65289;&#65292;&#36890;&#36807;&#20272;&#31639;&#26368;&#20339;&#25490;&#24207;&#26469;&#24341;&#23548; PLM &#29983;&#25104;&#30340;&#25991;&#26412;&#26397;&#21521;&#26368;&#20248;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-Prompt &#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#37325;&#25490;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#25490;&#22120;&#26159;&#22312;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#35780;&#20998;&#19979;&#23545;&#26816;&#32034;&#30340;&#25991;&#26723;&#36827;&#34892;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#20851;&#27880;&#12290;&#19982;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#19981;&#21516;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#20855;&#26377;&#20248;&#24322;&#32467;&#26524;&#30340;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#12290;&#34429;&#28982; LLM &#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25552;&#31034;&#35821;&#65292;&#20294;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#25552;&#31034;&#35821;&#30340;&#24433;&#21709;&#21644;&#20248;&#21270;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#38500;&#20102;&#24378;&#35843;&#20248;&#21270;&#23545;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#30340;&#24433;&#21709;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#32422;&#26463;&#25552;&#31034;&#29983;&#25104;&#65288;Co-Prompt&#65289;&#65292;&#36890;&#36807;&#20272;&#31639;&#26368;&#20339;&#25490;&#24207;&#26469;&#24341;&#23548; PLM &#29983;&#25104;&#30340;&#25991;&#26412;&#26397;&#21521;&#26368;&#20248;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-Prompt &#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#37325;&#25490;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;Co-Prompt &#29983;&#25104;&#30340;&#25552;&#31034;&#26356;&#26131;&#20110;&#20154;&#31867;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans aga
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#35758;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#37325;&#26500;&#20026;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#36890;&#36807;&#32467;&#21512;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13721</link><description>&lt;p&gt;
&#22522;&#20110;&#31034;&#20363;&#24341;&#23548;&#38382;&#31572;&#30340;&#25345;&#32493;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Continual Dialogue State Tracking via Example-Guided Question Answering. (arXiv:2305.13721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#35758;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#37325;&#26500;&#20026;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#36890;&#36807;&#32467;&#21512;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#20197;&#36866;&#24212;&#26032;&#26381;&#21153;&#65292;&#20294;&#26159;&#31616;&#21333;&#22320;&#20351;&#29992;&#26032;&#26381;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#38477;&#20302;&#20808;&#21069;&#23398;&#20064;&#30340;&#26381;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#20854;&#37325;&#26500;&#20026;&#19968;&#32452;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#20174;&#32780;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#29305;&#23450;&#26381;&#21153;&#30340;&#35760;&#24518;&#36127;&#25285;&#65292;&#24182;&#25945;&#20250;&#27169;&#22411;&#23558;&#25152;&#32473;&#38382;&#39064;&#21644;&#31034;&#20363;&#29992;&#20110;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#24517;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20010;&#21482;&#26377;6000&#19975;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20174;&#26816;&#32034;&#22120;&#33719;&#21462;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#33719;&#24471;&#24040;&#22823;&#30340;&#25552;&#21319;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems are frequently updated to accommodate new services, but naively updating them by continually training with data for new services in diminishing performance on previously learnt services. Motivated by the insight that dialogue state tracking (DST), a crucial component of dialogue systems that estimates the user's goal as a conversation proceeds, is a simple natural language understanding task, we propose reformulating it as a bundle of granular example-guided question answering tasks to minimize the task shift between services and thus benefit continual learning. Our approach alleviates service-specific memorization and teaches a model to contextualize the given question and example to extract the necessary information from the conversation. We find that a model with just 60M parameters can achieve a significant boost by learning to learn from in-context examples retrieved by a retriever trained to identify turns with similar dialogue state changes. Combining our method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#30693;&#35782;&#30340;&#29702;&#35299;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#37327;&#21270;&#20102;&#27169;&#22411;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13712</link><description>&lt;p&gt;
&#30693;&#35782;&#30340;&#30693;&#35782;&#65306;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;-&#24050;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models. (arXiv:2305.13712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#30693;&#35782;&#30340;&#29702;&#35299;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#37327;&#21270;&#20102;&#27169;&#22411;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#33258;&#36523;&#30693;&#35782;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#32531;&#35299;&#34394;&#26500;&#29616;&#35937;&#12290;&#25105;&#20204;&#19987;&#38376;&#20851;&#27880;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#36825;&#31181;&#38382;&#39064;&#30001;&#20110;&#32570;&#20047;&#30830;&#23450;&#30340;&#31572;&#26696;&#32780;&#20855;&#26377;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#24050;&#30693;-&#26410;&#30693;&#38382;&#39064;&#65288;KUQ&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#26469;&#38416;&#26126;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;LLM&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#38382;&#39064;&#20197;&#21450;&#30456;&#24212;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#24320;&#25918;&#24335;QA&#29615;&#22659;&#20013;&#35780;&#20272;LLM&#30340;&#31572;&#26696;&#36136;&#37327;&#12290;&#20026;&#20102;&#37327;&#21270;&#31572;&#26696;&#20013;&#34920;&#36798;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#27169;&#22411;&#22312;&#34920;&#36798;&#24050;&#30693;vs&#26410;&#30693;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their own knowledge and measuring their uncertainty. We argue this is an important feature for mitigating hallucinations. Specifically, we focus on addressing \textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty. Subsequently, we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly. Moreover, we evaluate the quality of their answers in an Open-Ended QA setting. To quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.
&lt;/p&gt;</description></item><item><title>LLM-Eval&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#30340;&#22810;&#32500;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#20854;&#22312;&#19968;&#20010;&#27169;&#22411;&#35843;&#29992;&#20013;&#28085;&#30422;&#20102;&#22810;&#20010;&#23545;&#35805;&#36136;&#37327;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#26159;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#22810;&#21151;&#33021;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.13711</link><description>&lt;p&gt;
LLM-Eval&#65306;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20013;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#22810;&#32500;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models. (arXiv:2305.13711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13711
&lt;/p&gt;
&lt;p&gt;
LLM-Eval&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#30340;&#22810;&#32500;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#20854;&#22312;&#19968;&#20010;&#27169;&#22411;&#35843;&#29992;&#20013;&#28085;&#30422;&#20102;&#22810;&#20010;&#23545;&#35805;&#36136;&#37327;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#26159;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#22810;&#21151;&#33021;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Eval&#65292;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#30340;&#32479;&#19968;&#22810;&#32500;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#24120;&#24120;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#12289;&#22522;&#26412;&#20107;&#23454;&#22238;&#22797;&#25110;&#22810;&#20010;LLM&#25552;&#31034;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#38656;&#35201;&#20184;&#20986;&#26114;&#36149;&#30340;&#20195;&#20215;&#24182;&#28040;&#32791;&#22823;&#37327;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21333;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#35780;&#20272;&#27169;&#24335;&#65292;&#22312;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20013;&#28085;&#30422;&#20102;&#23545;&#35805;&#36136;&#37327;&#30340;&#22810;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24191;&#27867;&#35780;&#20272;&#20102;LLM-Eval&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35780;&#20272;&#26041;&#27861;&#32780;&#35328;&#20855;&#26377;&#30340;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#24378;&#35843;&#20102;&#20026;&#33719;&#24471;&#20934;&#30830;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;LLM&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#12290;LLM-Eval&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#19988;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#30340;&#19968;&#33268;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;6G&#26102;&#20195;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#38382;&#39064;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13706</link><description>&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#30340;&#20256;&#36755;&#35843;&#24230;&#65306;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware Transmission Scheduling: a Monotonicity-driven Deep Reinforcement Learning Approach. (arXiv:2305.13706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13706
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;6G&#26102;&#20195;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#38382;&#39064;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;6G&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#65292;&#38656;&#35201;&#35821;&#20041;&#20256;&#36755;&#26469;&#36830;&#25509;&#20998;&#24067;&#24335;&#35774;&#22791;&#65292;&#20197;&#20445;&#35777;&#24212;&#29992;&#23618;&#24615;&#33021;&#65292;&#19981;&#20165;&#20165;&#26159;&#38598;&#20013;&#20110;&#36890;&#20449;&#23618;&#24615;&#33021;&#12290;&#35821;&#20041;&#22312;&#36825;&#37324;&#26159;&#20449;&#24687;&#20256;&#36755;&#26377;&#29992;&#24615;&#30340;&#34913;&#37327;&#12290;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#24120;&#24120;&#28041;&#21450;&#24222;&#22823;&#30340;&#20915;&#31574;&#31354;&#38388;&#65292;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#33719;&#24471;&#26368;&#20248;&#31574;&#30053;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#26368;&#20248;&#35821;&#20041;&#24863;&#30693;&#35843;&#24230;&#31574;&#30053;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#26681;&#25454;&#29702;&#35770;&#25351;&#23548;&#21407;&#21017;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
For cyber-physical systems in the 6G era, semantic communications connecting distributed devices for dynamic control and remote state estimation are required to guarantee application-level performance, not merely focus on communication-centric performance. Semantics here is a measure of the usefulness of information transmissions. Semantic-aware transmission scheduling of a large system often involves a large decision-making space, and the optimal policy cannot be obtained by existing algorithms effectively. In this paper, we first investigate the fundamental properties of the optimal semantic-aware scheduling policy and then develop advanced deep reinforcement learning (DRL) algorithms by leveraging the theoretical guidelines. Our numerical results show that the proposed algorithms can substantially reduce training time and enhance training performance compared to benchmark algorithms.
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22270;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#27491;&#22312;&#24555;&#36895;&#25104;&#20026;&#20027;&#35201;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#21644;&#21028;&#21035;&#24335;&#35757;&#32451;&#30340;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13689</link><description>&lt;p&gt;
&#35748;&#35782;&#20320;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#22270;&#20687;&#29983;&#25104;&#21644;&#21028;&#21035;&#24335;&#35757;&#32451;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training. (arXiv:2305.13689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13689
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22270;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#27491;&#22312;&#24555;&#36895;&#25104;&#20026;&#20027;&#35201;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#21644;&#21028;&#21035;&#24335;&#35757;&#32451;&#30340;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#36807;&#21435;&#65292;&#30417;&#30563;&#24335;&#23398;&#20064;&#22312;&#22270;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#25104;&#21151;&#65292;&#20294;&#36817;&#24180;&#26469;&#65292;&#20854;&#25913;&#36827;&#30340;&#24133;&#24230;&#24050;&#32463;&#26174;&#33879;&#20943;&#23567;&#65292;&#36825;&#34920;&#26126;&#19968;&#20010;&#29942;&#39048;&#24050;&#32463;&#20986;&#29616;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#36825;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#20135;&#29983;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#21551;&#21457;&#33258;NLP&#39046;&#22495;&#21462;&#24471;&#30340;&#20248;&#24322;&#32467;&#26524;&#65292;&#22522;&#20110;&#32858;&#31867;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#33976;&#39311;&#21644;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#20998;&#31867;&#24335;SSL&#31561;&#33258;&#30417;&#30563;&#26041;&#27861;&#36805;&#36895;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#19981;&#20037;&#20043;&#21518;&#65292;&#22522;&#20110;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#30340;&#29983;&#25104;&#24335;SSL&#26694;&#26550;&#22312;&#24357;&#34917;&#21644;&#36229;&#36234;&#20102;&#21028;&#21035;&#24335;SSL&#25152;&#21462;&#24471;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#22270;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#21644;&#21028;&#21035;&#24335;&#35757;&#32451;&#30340;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although supervised learning has been highly successful in improving the state-of-the-art in the domain of image-based computer vision in the past, the margin of improvement has diminished significantly in recent years, indicating that a plateau is in sight. Meanwhile, the use of self-supervised learning (SSL) for the purpose of natural language processing (NLP) has seen tremendous successes during the past couple of years, with this new learning paradigm yielding powerful language models. Inspired by the excellent results obtained in the field of NLP, self-supervised methods that rely on clustering, contrastive learning, distillation, and information-maximization, which all fall under the banner of discriminative SSL, have experienced a swift uptake in the area of computer vision. Shortly afterwards, generative SSL frameworks that are mostly based on masked image modeling, complemented and surpassed the results obtained with discriminative SSL. Consequently, within a span of three yea
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13681</link><description>&lt;p&gt;
GUARD: &#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13681
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#38169;&#30340;&#24615;&#36136;&#65292;&#23558;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#29616;&#23454;&#24212;&#29992;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#20154;&#26426;&#20132;&#20114;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#65289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26368;&#36817;&#65292;&#23433;&#20840;RL&#65288;&#21363;&#32422;&#26463;RL&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#36805;&#36895;&#20986;&#29616;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#25506;&#32034;&#29615;&#22659;&#12290;&#30001;&#20110;&#31639;&#27861;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#27604;&#36739;&#29616;&#26377;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GUARD&#65292;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;GUARD&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#20855;&#26377;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#12290;&#20854;&#27425;&#65292;GUARD&#20840;&#38754;&#28085;&#30422;&#20102;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#33258;&#21253;&#21547;&#30340;&#23454;&#29616;&#12290;&#31532;&#19977;&#65292;GUARD&#22312;&#20219;&#21153;&#21644;&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29366;&#24577;&#19979;&#29616;&#26377;&#26041;&#27861;&#22312;GUARD&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13669</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. (arXiv:2305.13669v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#36817;&#26399;&#36827;&#23637;&#26174;&#33879;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#35823;&#23548;&#24615;&#21644;&#19981;&#25903;&#25345;&#30340;&#22238;&#31572;&#12290;&#19968;&#31181;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20174;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#21644;&#25972;&#21512;&#25903;&#25345;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#38382;&#39064;&#36890;&#24120;&#19982;&#23384;&#20648;&#30340;&#30693;&#35782;&#19981;&#22826;&#23545;&#40784;&#65292;&#22240;&#20026;&#20182;&#20204;&#22312;&#25552;&#38382;&#21069;&#19981;&#30693;&#36947;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#19981;&#23545;&#40784;&#21487;&#33021;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#23450;&#20301;&#21644;&#21033;&#29992;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#36843;&#20351;&#20854;&#36890;&#36807;&#24573;&#30053;&#25110;&#35206;&#30422;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#32780;&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MixAlign&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#20197;&#33719;&#24471;&#24182;&#25972;&#21512;&#20851;&#20110;&#29992;&#25143;&#38382;&#39064;&#19982;&#23384;&#20648;&#20449;&#24687;&#30456;&#20851;&#24615;&#30340;&#28548;&#28165;&#20449;&#24687;&#12290; MixAlign &#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#20154;&#24037;&#29992;&#25143;&#28548;&#28165;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable recent advances in language models, they still struggle with the hallucination problem and can generate misleading and unsupported responses. A common approach to mitigate the hallucination issue is retrieving and incorporating supporting evidence from a knowledge base. However, user questions usually do not align well with the stored knowledge, as they are unaware of the information available before asking questions. This misalignment can limit the language model's ability to locate and utilize the knowledge, potentially forcing it to hallucinate by ignoring or overriding the retrieved evidence. To address this issue, we introduce MixAlign, a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic question-knowledge alignment and, if necessary, further enhances this alignment through human user clari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#21452;&#32858;&#28966;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#36807;&#24230;&#33258;&#20449;&#21644;&#20302;&#33258;&#20449;&#20043;&#38388;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13665</link><description>&lt;p&gt;
&#21452;&#32858;&#28966;&#25439;&#22833;&#29992;&#20110;&#32622;&#20449;&#24230;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Dual Focal Loss for Calibration. (arXiv:2305.13665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#21452;&#32858;&#28966;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#36807;&#24230;&#33258;&#20449;&#21644;&#20302;&#33258;&#20449;&#20043;&#38388;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20013;&#65292;&#38656;&#35201;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#30340;&#32593;&#32476;&#65292;&#20854;&#32622;&#20449;&#24230;&#20998;&#25968;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#23454;&#38469;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#36890;&#24120;&#25552;&#20379;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#23548;&#33268;&#26657;&#20934;&#19981;&#20339;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#36890;&#36807;&#28966;&#28857;&#25439;&#22833;&#26469;&#38477;&#20302;&#36807;&#24230;&#33258;&#20449;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#33021;&#23548;&#33268;&#20302;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#24050;&#32463;&#25506;&#32034;&#20102;&#28966;&#28857;&#25439;&#22833;&#30340;&#19981;&#21516;&#21464;&#20307;&#65292;&#20294;&#24456;&#38590;&#25214;&#21040;&#36807;&#24230;&#33258;&#20449;&#21644;&#20302;&#33258;&#20449;&#20043;&#38388;&#30340;&#24179;&#34913;&#28857;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#19987;&#27880;&#20110;&#21452;&#37325;&#36923;&#36753;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#22522;&#20110;&#23454;&#38469;&#24773;&#20917;&#30340;logit&#65292;&#32780;&#19988;&#36824;&#32771;&#34385;&#22312;&#22522;&#20110;&#23454;&#38469;&#24773;&#20917;&#20043;&#21518;&#35780;&#32423;&#26368;&#39640;&#30340;logit&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#36825;&#20004;&#20010;logit&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21452;&#32858;&#28966;&#25439;&#22833;&#21487;&#20197;&#22312;&#36807;&#24230;&#33258;&#20449;&#21644;&#20302;&#33258;&#20449;&#20043;&#38388;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#35777;&#25454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of deep neural networks in real-world applications require well-calibrated networks with confidence scores that accurately reflect the actual probability. However, it has been found that these networks often provide over-confident predictions, which leads to poor calibration. Recent efforts have sought to address this issue by focal loss to reduce over-confidence, but this approach can also lead to under-confident predictions. While different variants of focal loss have been explored, it is difficult to find a balance between over-confidence and under-confidence. In our work, we propose a new loss function by focusing on dual logits. Our method not only considers the ground truth logit, but also take into account the highest logit ranked after the ground truth logit. By maximizing the gap between these two logits, our proposed dual focal loss can achieve a better balance between over-confidence and under-confidence. We provide theoretical evidence to support our approach and de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#35823;&#29992;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;</title><link>http://arxiv.org/abs/2305.13661</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On the Risk of Misinformation Pollution with Large Language Models. (arXiv:2305.13661v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#35823;&#29992;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#22312;&#35823;&#29992;&#65292;&#25506;&#35752;&#20102;&#20854;&#29983;&#25104;&#21487;&#20449;&#24182;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#24182;&#23545;&#20449;&#24687;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#23545;&#26080;&#24847;&#21644;&#25925;&#24847;&#30340;&#28508;&#22312;&#35823;&#29992;&#22330;&#26223;&#36827;&#34892;&#27169;&#25311;&#65292;&#20197;&#35780;&#20272;LLM&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#20449;&#24687;&#19981;&#23454;&#30340;&#31243;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;ODQA&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#20026;&#20102;&#20943;&#36731;&#30001;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#20449;&#24687;&#24102;&#26469;&#30340;&#21361;&#23475;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;&#34429;&#28982;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#38450;&#24481;&#24615;&#31574;&#30053;&#26377;&#24076;&#26395;&#20135;&#29983;&#26126;&#26174;&#25928;&#26524;&#65292;&#20294;&#36824;&#38656;&#35201;&#20570;&#22823;&#37327;&#24037;&#20316;&#26469;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#36890;&#36807;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;MBO&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#31283;&#20581;&#22320;&#23547;&#25214;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.13650</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#22343;&#34913;&#25968;&#25454;&#30340;&#40065;&#26834;&#22522;&#20110;&#27169;&#22411;&#30340;&#35774;&#35745;&#30340;&#23646;&#24615;&#24341;&#23548;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Property-Guided Generative Modelling for Robust Model-Based Design with Imbalanced Data. (arXiv:2305.13650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#36890;&#36807;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;MBO&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#31283;&#20581;&#22320;&#23547;&#25214;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#25506;&#32034;&#20855;&#26377;&#26497;&#24230;&#31232;&#30095;&#30340;&#26377;&#24847;&#20041;&#21306;&#22495;&#30340;&#39640;&#32500;&#34507;&#30333;&#36136;&#24207;&#21015;&#31354;&#38388;&#12290;&#36825;&#23548;&#33268;&#20102;&#27169;&#22411;&#20248;&#21270;&#65288;MBO&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#20351;&#29992;&#30001;&#24207;&#21015;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#24341;&#23548;&#30340;&#26377;&#25928;&#25628;&#32034;&#27169;&#22411;&#26469;&#36741;&#21161;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#19981;&#24179;&#34913;&#24615;&#20351;&#24471;&#29616;&#26377;&#30340;MBO&#26041;&#27861;&#24456;&#38590;&#25110;&#26681;&#26412;&#26080;&#27861;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#20854;&#28508;&#22312;&#31354;&#38388;&#30001;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#25353;&#29031;&#36825;&#20123;&#23646;&#24615;&#20540;&#20248;&#20808;&#32771;&#34385;&#26679;&#26412;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#21644;&#21322;&#21512;&#25104;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MBO&#19982;PGVAE&#31283;&#20581;&#22320;&#21457;&#29616;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#65292;&#23613;&#31649;&#25968;&#25454;&#38598;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36830;&#32493;&#35774;&#35745;&#31354;&#38388;&#30340;&#26222;&#36866;&#24615;&#21450;&#20854;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of designing protein sequences with desired properties is challenging, as it requires to explore a high-dimensional protein sequence space with extremely sparse meaningful regions. This has led to the development of model-based optimization (MBO) techniques that aid in the design, by using effective search models guided by the properties over the sequence space. However, the intrinsic imbalanced nature of experimentally derived datasets causes existing MBO approaches to struggle or outright fail. We propose a property-guided variational auto-encoder (PGVAE) whose latent space is explicitly structured by the property values such that samples are prioritized according to these properties. Through extensive benchmarking on real and semi-synthetic protein datasets, we demonstrate that MBO with PGVAE robustly finds sequences with improved properties despite significant dataset imbalances. We further showcase the generality of our approach to continuous design spaces, and its rob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;kNN&#39044;&#27979;&#30340;&#32479;&#35745;&#20449;&#24687;&#26469;&#25913;&#21892;fine-tuning&#38454;&#27573;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#34920;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#25972;&#21512;kNN&#32479;&#35745;&#20449;&#24687;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;BLEU&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.13648</link><description>&lt;p&gt;
&#26080;&#21442;&#25968;&#65292;&#26368;&#36817;&#37051;&#36741;&#21161;&#24494;&#35843;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Non-parametric, Nearest-neighbor-assisted Fine-tuning for Neural Machine Translation. (arXiv:2305.13648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;kNN&#39044;&#27979;&#30340;&#32479;&#35745;&#20449;&#24687;&#26469;&#25913;&#21892;fine-tuning&#38454;&#27573;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#34920;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#25972;&#21512;kNN&#32479;&#35745;&#20449;&#24687;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#37051;&#31639;&#27861;&#24050;&#32463;&#34987;&#29992;&#20110;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#21644;&#26426;&#22120;&#32763;&#35793;&#35299;&#30721;&#22120;&#31561;&#29983;&#25104;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#38750;&#21442;&#25968;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;kNN&#39044;&#27979;&#30340;&#32479;&#35745;&#20449;&#24687;&#26469;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;fine-tuning&#38454;&#27573;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#22914;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#36827;&#34892;&#28176;&#21464;&#32553;&#25918;&#12289;&#20351;&#29992;kNN&#30340;&#30495;&#23454;&#27010;&#29575;&#20197;&#21450;&#24378;&#21270;&#23398;&#20064;&#31561;&#26041;&#27861;&#26469;&#25972;&#21512;kNN&#32479;&#35745;&#20449;&#24687;&#12290;&#23545;&#20110;&#22235;&#20010;&#26631;&#20934;&#39046;&#22495;&#30340;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#19982;&#32463;&#20856;&#30340;&#24494;&#35843;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19977;&#31181;&#26041;&#27861;&#30340;&#19968;&#33268;&#25913;&#36827;&#65292;&#23545;&#20110;&#24503;&#33521;&#21644;&#33521;&#24503;&#32763;&#35793;&#65292;BLEU&#20998;&#21035;&#25552;&#39640;&#20102;1.45&#21644;1.28&#20998;&#12290;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#32763;&#35793;&#35821;&#27861;&#20851;&#31995;&#25110;&#20989;&#25968;&#35789;&#26102;&#65292;&#26377;&#30528;&#29305;&#21035;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-parametric, k-nearest-neighbor algorithms have recently made inroads to assist generative models such as language models and machine translation decoders. We explore whether such non-parametric models can improve machine translation models at the fine-tuning stage by incorporating statistics from the kNN predictions to inform the gradient updates for a baseline translation model. There are multiple methods which could be used to incorporate kNN statistics and we investigate gradient scaling by a gating mechanism, the kNN's ground truth probability, and reinforcement learning. For four standard in-domain machine translation datasets, compared with classic fine-tuning, we report consistent improvements of all of the three methods by as much as 1.45 BLEU and 1.28 BLEU for German-English and English-German translations respectively. Through qualitative analysis, we found particular improvements when it comes to translating grammatical relations or function words, which results in incre
&lt;/p&gt;</description></item><item><title>SMAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#26500;&#20449;&#24687;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#22522;&#20110;&#22330;&#26223;&#30340;&#26368;&#20248;&#27169;&#22411;&#20998;&#37197;&#38382;&#39064;&#65292;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20934;&#30830;&#12289;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.13634</link><description>&lt;p&gt;
SMAP&#65306;&#19968;&#31181;&#38754;&#21521;&#22330;&#26223;&#30340;&#26368;&#20248;&#27169;&#22411;&#20998;&#37197;&#30340;&#26032;&#22411;&#24322;&#26500;&#20449;&#24687;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
SMAP: A Novel Heterogeneous Information Framework for Scenario-based Optimal Model Assignment. (arXiv:2305.13634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13634
&lt;/p&gt;
&lt;p&gt;
SMAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#26500;&#20449;&#24687;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#22522;&#20110;&#22330;&#26223;&#30340;&#26368;&#20248;&#27169;&#22411;&#20998;&#37197;&#38382;&#39064;&#65292;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20934;&#30830;&#12289;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#25968;&#25454;&#24212;&#29992;&#26085;&#30410;&#25104;&#29087;&#65292;&#23548;&#33268;&#21516;&#19968;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#20013;&#38024;&#23545;&#30456;&#21516;&#30446;&#26631;&#30340;&#27169;&#22411;&#25968;&#37327;&#19981;&#26029;&#22686;&#22810;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#27169;&#22411;&#29305;&#24449;&#21644;&#29305;&#23450;&#35201;&#27714;&#21644;&#32422;&#26463;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20559;&#37325;&#20110;&#22522;&#20110;&#20247;&#21253;&#30340;&#24037;&#20154;-&#20219;&#21153;&#20998;&#37197;&#65292;&#24573;&#30053;&#20102;&#22330;&#26223;-&#25968;&#25454;&#38598;-&#27169;&#22411;&#20998;&#37197;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#31216;&#20026;&#22522;&#20110;&#22330;&#26223;&#30340;&#26368;&#20248;&#27169;&#22411;&#20998;&#37197;&#65288;SOMA&#65289;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#22330;&#26223;&#21644;&#27169;&#22411;&#32852;&#21512;&#24863;&#30693;&#65288;SMAP&#65289;&#30340;&#26032;&#26694;&#26550;&#12290; SMAP&#26159;&#19968;&#31181;&#24322;&#26500;&#20449;&#24687;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#21508;&#31181;&#31867;&#22411;&#30340;&#20449;&#24687;&#20197;&#26234;&#33021;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#24182;&#20026;&#29305;&#23450;&#22330;&#26223;&#20998;&#37197;&#26368;&#20339;&#27169;&#22411;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#24471;&#20998;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#23558;SMAP&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMAP&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing maturity of big data applications has led to a proliferation of models targeting the same objectives within the same scenarios and datasets. However, selecting the most suitable model that considers model's features while taking specific requirements and constraints into account still poses a significant challenge. Existing methods have focused on worker-task assignments based on crowdsourcing, they neglect the scenario-dataset-model assignment problem. To address this challenge, a new problem named the Scenario-based Optimal Model Assignment (SOMA) problem is introduced and a novel framework entitled Scenario and Model Associative percepts (SMAP) is developed. SMAP is a heterogeneous information framework that can integrate various types of information to intelligently select a suitable dataset and allocate the optimal model for a specific scenario. To comprehensively evaluate models, a new score function that utilizes multi-head attention mechanisms is proposed. Moreov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13632</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#23545;&#20110;&#25277;&#35937;&#25688;&#35201;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#34429;&#28982;&#33258;&#21160;&#20135;&#29983;&#30340;&#25688;&#35201;&#21487;&#33021;&#27969;&#30021;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#23545;&#21407;&#22987;&#25991;&#26723;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#65292;&#22914;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#33521;&#35821;&#65292;&#22240;&#27492;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#29978;&#33267;&#34913;&#37327;&#36825;&#31181;&#29616;&#35937;&#30340;&#31243;&#24230;&#20063;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#33521;&#35821;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#32467;&#26524;&#20013;&#20511;&#37492;&#32763;&#35793;&#22522;&#30784;&#30693;&#35782;&#20026;&#38750;&#33521;&#35821;&#25688;&#35201;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#28982;&#21518;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#24187;&#35273;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#25439;&#22833;&#20056;&#20197;&#20854;&#24544;&#23454;&#24615;&#24471;&#20998;&#12290;&#36890;&#36807;&#22810;&#31181;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;mFACT&#26159;&#26368;&#36866;&#21512;&#26816;&#27979;&#24187;&#35273;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#25552;&#20986;&#30340;&#21152;&#26435;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed l
&lt;/p&gt;</description></item><item><title>Instruct-Align&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#26694;&#26550;&#65292;&#20351;&#24471;&#25945;&#23398;&#35843;&#25972;&#30340;LLMs&#33021;&#22815;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#19988;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2305.13627</link><description>&lt;p&gt;
Instruct-Align&#65306;&#36890;&#36807;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#23558;&#26032;&#35821;&#35328;&#25945;&#32473;LLM
&lt;/p&gt;
&lt;p&gt;
Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction. (arXiv:2305.13627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13627
&lt;/p&gt;
&lt;p&gt;
Instruct-Align&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#26694;&#26550;&#65292;&#20351;&#24471;&#25945;&#23398;&#35843;&#25972;&#30340;LLMs&#33021;&#22815;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#19988;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#23398;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#21644;&#22810;&#31181;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#19981;&#21516;&#35821;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23569;&#25968;&#35821;&#35328;&#25110;&#32773;&#26159;&#26410;&#30693;&#35821;&#35328;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#21457;&#29616;&#65292;&#31616;&#21333;&#22320;&#23558;&#26032;&#35821;&#35328;&#36866;&#24212;&#21040;&#32463;&#36807;&#25945;&#23398;&#35843;&#25972;&#30340;LLM&#20013;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20174;&#32780;&#23548;&#33268;&#36825;&#20123;LLM&#22833;&#21435;&#22810;&#20219;&#21153;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;Instruct-Align&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#65292;&#20351;&#24471;&#32463;&#36807;&#25945;&#23398;&#35843;&#25972;&#30340;LLM&#33021;&#22815;&#23398;&#20064;&#21040;&#30475;&#19981;&#35265;&#30340;&#21644;&#20043;&#21069;&#23398;&#20064;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#22312;BLOOMZ-560M&#25968;&#25454;&#38598;&#19978;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;Instruct-Align&#33021;&#22815;&#22312;&#20165;&#20351;&#29992;&#26377;&#38480;&#37327;&#30340;&#24179;&#34892;&#35821;&#26009;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#24182;&#19988;&#36890;&#36807;&#25345;&#32493;&#30340;&#25945;&#23398;&#35843;&#25972;&#65292;&#38450;&#27490;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) have shown remarkable generalization capability over multiple tasks in multiple languages. Nevertheless, their generalization towards different languages varies especially to underrepresented languages or even to unseen languages. Prior works on adapting new languages to LLMs find that naively adapting new languages to instruction-tuned LLMs will result in catastrophic forgetting, which in turn causes the loss of multitasking ability in these LLMs. To tackle this, we propose the Instruct-Align a.k.a (IA)$^1$ framework, which enables instruction-tuned LLMs to learn cross-lingual alignment between unseen and previously learned languages via alignment-based cross-lingual instruction-tuning. Our preliminary result on BLOOMZ-560M shows that (IA)$^1$ is able to learn a new language effectively with only a limited amount of parallel data and at the same time prevent catastrophic forgetting by applying continual instruction-tuning through experien
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Semantic Fusion&#8221;&#30340;&#36890;&#29992;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#20004;&#20010;&#25110;&#22810;&#20010;&#19981;&#21516;&#27169;&#24577;&#30340;&#20869;&#23481;&#23457;&#26680;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#39564;&#35777;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#22823;&#35268;&#27169;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#39564;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13623</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#34701;&#21512;&#39564;&#35777;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
Validating Multimedia Content Moderation Software via Semantic Fusion. (arXiv:2305.13623v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Semantic Fusion&#8221;&#30340;&#36890;&#29992;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#20004;&#20010;&#25110;&#22810;&#20010;&#19981;&#21516;&#27169;&#24577;&#30340;&#20869;&#23481;&#23457;&#26680;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#39564;&#35777;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#22823;&#35268;&#27169;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#39564;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#22914;Facebook&#21644;TikTok&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24050;&#32463;&#25913;&#21464;&#20102;&#20154;&#31867;&#31038;&#20250;&#30340;&#20132;&#27969;&#21644;&#20869;&#23481;&#21457;&#24067;&#26041;&#24335;&#12290;&#22312;&#36825;&#20123;&#24179;&#21488;&#19978;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#24067;&#32467;&#21512;&#25991;&#26412;&#65292;&#38899;&#39057;&#65292;&#22270;&#20687;&#21644;&#35270;&#39057;&#20256;&#36882;&#20449;&#24687;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22810;&#23186;&#20307;&#20869;&#23481;&#21457;&#24067;&#35774;&#26045;&#26085;&#30410;&#34987;&#21033;&#29992;&#26469;&#20256;&#25773;&#26377;&#23475;&#20869;&#23481;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#65292;&#24694;&#24847;&#24191;&#21578;&#21644;&#33394;&#24773;&#20869;&#23481;&#12290;&#20026;&#27492;&#65292;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#24050;&#32463;&#24191;&#27867;&#37096;&#32626;&#22312;&#36825;&#20123;&#24179;&#21488;&#19978;&#65292;&#20197;&#26816;&#27979;&#21644;&#23631;&#34109;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20869;&#23481;&#23457;&#26680;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#36328;&#22810;&#31181;&#27169;&#24335;&#29702;&#35299;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#29616;&#26377;&#30340;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#23548;&#33268;&#26497;&#20026;&#36127;&#38754;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;Semantic Fusion&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#32780;&#26377;&#25928;&#30340;&#39564;&#35777;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#34701;&#21512;&#20004;&#20010;&#25110;&#26356;&#22810;&#22522;&#20110;&#19981;&#21516;&#27169;&#24577;&#65288;&#22914;&#25991;&#26412;&#65292;&#38899;&#39057;&#65292;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#30340;&#29616;&#26377;&#20869;&#23481;&#23457;&#26680;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#20114;&#34917;&#24615;&#21644;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#39564;&#35777;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Semantic Fusion&#65292;&#24182;&#26174;&#31034;&#23427;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#39564;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media platforms, such as Facebook and TikTok, has revolutionized communication and content publication in human society. Users on these platforms can publish multimedia content that delivers information via the combination of text, audio, images, and video. Meanwhile, the multimedia content release facility has been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisements, and pornography. To this end, content moderation software has been widely deployed on these platforms to detect and blocks toxic content. However, due to the complexity of content moderation models and the difficulty of understanding information across multiple modalities, existing content moderation software can fail to detect toxic content, which often leads to extremely negative impacts.  We introduce Semantic Fusion, a general, effective methodology for validating multimedia content moderation software. Our key idea is to fuse two or more ex
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13617</link><description>&lt;p&gt;
SPEECH: &#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20013;&#24515;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#28041;&#21450;&#39044;&#27979;&#20107;&#20214;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24773;&#20917;&#19979;&#65292;&#20107;&#20214;&#32467;&#26500;&#37117;&#20855;&#26377;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979; (SPEECH)&#12290; SPEECH &#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#24314;&#27169;&#26469;&#27169;&#25311;&#20107;&#20214;&#32467;&#26500;&#32452;&#20214;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#22312;&#20004;&#20010;&#32479;&#19968;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#21344;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13592</link><description>&lt;p&gt;
&#21033;&#29992;&#65288;&#27169;&#31946;&#27979;&#35797;&#65289;&#27979;&#35797;&#29992;&#20363;&#26469;&#29702;&#35299;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Understanding Programs by Exploiting (Fuzzing) Test Cases. (arXiv:2305.13592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#30340;&#35821;&#20041;&#29702;&#35299;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#36890;&#36807;&#23558;&#32534;&#31243;&#35821;&#35328;&#35270;&#20026;&#21478;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#22312;&#31243;&#24207;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;LLM&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#31243;&#24207;&#27605;&#31455;&#19982;&#25991;&#26412;&#26377;&#26412;&#36136;&#30340;&#21306;&#21035;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#20005;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#31243;&#24207;&#21450;&#20854;&#22522;&#26412;&#21333;&#20803;&#65288;&#21363;&#20989;&#25968;&#21644;&#23376;&#31243;&#24207;&#65289;&#26088;&#22312;&#23637;&#31034;&#21508;&#31181;&#34892;&#20026;&#21644;/&#25110;&#25552;&#20379;&#21487;&#33021;&#30340;&#36755;&#20986;&#65292;&#32473;&#23450;&#19981;&#21516;&#30340;&#36755;&#20837;&#12290;&#36755;&#20837;&#21644;&#21487;&#33021;&#30340;&#36755;&#20986;/&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#34920;&#31034;&#20989;&#25968;/&#23376;&#31243;&#24207;&#65292;&#24182;&#27010;&#36848;&#20102;&#25972;&#20010;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#31181;&#20851;&#31995;&#32435;&#20837;&#23398;&#20064;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#31243;&#24207;&#30340;&#26356;&#28145;&#20837;&#35821;&#20041;&#29702;&#35299;&#12290;&#20026;&#20102;&#33719;&#24471;&#36275;&#22815;&#20195;&#34920;&#24615;&#30340;&#36755;&#20837;&#20197;&#35302;&#21457;&#22823;&#37327;&#25191;&#34892;&#65292;&#21487;&#20197;&#20351;&#29992;&#27169;&#31946;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#33021;&#21516;&#26102;&#31363;&#21462;&#22810;&#20986;&#21475;&#32593;&#32476;&#27169;&#22411;&#20989;&#25968;&#21644;&#36755;&#20986;&#31574;&#30053;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21464;&#28857;&#26816;&#27979;&#21644;&#24615;&#33021;&#25439;&#22833;&#12289;&#31574;&#30053;&#25439;&#22833;&#25351;&#23548;&#26367;&#20195;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20986;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13584</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#20986;&#21475;&#32593;&#32476;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Model Stealing Attack against Multi-Exit Networks. (arXiv:2305.13584v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#33021;&#21516;&#26102;&#31363;&#21462;&#22810;&#20986;&#21475;&#32593;&#32476;&#27169;&#22411;&#20989;&#25968;&#21644;&#36755;&#20986;&#31574;&#30053;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21464;&#28857;&#26816;&#27979;&#21644;&#24615;&#33021;&#25439;&#22833;&#12289;&#31574;&#30053;&#25439;&#22833;&#25351;&#23548;&#26367;&#20195;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20986;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20855;&#26377;&#21333;&#20010;&#20986;&#21475;&#30340;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#22810;&#20986;&#21475;&#32593;&#32476;&#20855;&#26377;&#22810;&#20010;&#20986;&#21475;&#65292;&#36825;&#20123;&#20986;&#21475;&#20801;&#35768;&#20174;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#26089;&#26399;&#36755;&#20986;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#31867;&#20284;&#35782;&#21035;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#24403;&#20351;&#29992;&#20256;&#32479;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#26041;&#27861;&#23581;&#35797;&#31363;&#21462;&#36825;&#20123;&#26377;&#20215;&#20540;&#30340;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#26041;&#27861;&#21482;&#33021;&#31363;&#21462;&#27169;&#22411;&#30340;&#20998;&#31867;&#20989;&#25968;&#65292;&#32780;&#19981;&#33021;&#25429;&#25417;&#20854;&#36755;&#20986;&#31574;&#30053;&#12290;&#36825;&#23548;&#33268;&#31363;&#21462;&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#26174;&#33879;&#38477;&#20302;&#65292;&#22833;&#21435;&#22810;&#20986;&#21475;&#32593;&#32476;&#30340;&#20248;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31363;&#21462;&#27169;&#22411;&#25915;&#20987;&#65292;&#21487;&#20197;&#25552;&#21462;&#27169;&#22411;&#20989;&#25968;&#21644;&#36755;&#20986;&#31574;&#30053;&#12290;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#21464;&#28857;&#26816;&#27979;&#26469;&#20998;&#26512;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#24615;&#33021;&#25439;&#22833;&#21644;&#31574;&#30053;&#25439;&#22833;&#26469;&#25351;&#23548;&#26367;&#20195;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36755;&#20986;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#20351;&#26367;&#20195;&#27169;&#22411;&#36824;&#21407;&#31363;&#21462;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to traditional neural networks with a single exit, a multi-exit network has multiple exits that allow for early output from intermediate layers of the model, thus bringing significant improvement in computational efficiency while maintaining similar recognition accuracy. When attempting to steal such valuable models using traditional model stealing attacks, we found that conventional methods can only steal the model's classification function while failing to capture its output strategy. This results in a significant decrease in computational efficiency for the stolen substitute model, thereby losing the advantages of multi-exit networks.In this paper, we propose the first model stealing attack to extract both the model function and output strategy. We employ bayesian changepoint detection to analyze the target model's output strategy and use performance loss and strategy loss to guide the training of the substitute model. Furthermore, we designed a novel output strategy search
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#37325;&#22609;&#37096;&#20998;&#36890;&#36947;&#20026;&#25209;&#22788;&#29702;&#32500;&#24230;&#24182;&#23558;&#36890;&#36947;&#20998;&#32452;&#65292;&#20197;&#22686;&#21152;&#31354;&#38388;&#35821;&#20041;&#20998;&#24067;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#20132;&#21449;&#32500;&#24230;&#20132;&#20114;&#32858;&#21512;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#30340;&#36755;&#20986;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;EMA&#21487;&#20197;&#39640;&#25928;&#22320;&#20248;&#21270;&#24615;&#33021;&#65292;&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.13563</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#39640;&#25928;&#20132;&#21449;&#31354;&#38388;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-Scale Attention Module with Cross-Spatial Learning. (arXiv:2305.13563v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#37325;&#22609;&#37096;&#20998;&#36890;&#36947;&#20026;&#25209;&#22788;&#29702;&#32500;&#24230;&#24182;&#23558;&#36890;&#36947;&#20998;&#32452;&#65292;&#20197;&#22686;&#21152;&#31354;&#38388;&#35821;&#20041;&#20998;&#24067;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#20132;&#21449;&#32500;&#24230;&#20132;&#20114;&#32858;&#21512;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#30340;&#36755;&#20986;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;EMA&#21487;&#20197;&#39640;&#25928;&#22320;&#20248;&#21270;&#24615;&#33021;&#65292;&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#65288;EMA&#65289;&#27169;&#22359;&#65292;&#26088;&#22312;&#20445;&#30041;&#27599;&#20010;&#36890;&#36947;&#30340;&#20449;&#24687;&#21644;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#35813;&#27169;&#22359;&#23558;&#37096;&#20998;&#36890;&#36947;&#37325;&#22609;&#20026;&#25209;&#22788;&#29702;&#32500;&#24230;&#65292;&#24182;&#23558;&#36890;&#36947;&#20998;&#32452;&#25104;&#22810;&#20010;&#23376;&#29305;&#24449;&#65292;&#20174;&#32780;&#20351;&#31354;&#38388;&#35821;&#20041;&#29305;&#24449;&#22312;&#27599;&#20010;&#29305;&#24449;&#32452;&#20013;&#20998;&#24067;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22359;&#36890;&#36807;&#20132;&#21449;&#32500;&#24230;&#20132;&#20114;&#36827;&#19968;&#27493;&#32858;&#21512;&#20102;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#30340;&#36755;&#20986;&#29305;&#24449;&#65292;&#20197;&#25429;&#25417;&#20687;&#32032;&#32423;&#21035;&#30340;&#25104;&#23545;&#20851;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EMA&#21487;&#20197;&#22312;&#22810;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remarkable effectiveness of the channel or spatial attention mechanisms for producing more discernible feature representation are illustrated in various computer vision tasks. However, modeling the cross-channel relationships with channel dimensionality reduction may bring side effect in extracting deep visual representations. In this paper, a novel efficient multi-scale attention (EMA) module is proposed. Focusing on retaining the information on per channel and decreasing the computational overhead, we reshape the partly channels into the batch dimensions and group the channel dimensions into multiple sub-features which make the spatial semantic features well-distributed inside each feature group. Specifically, apart from encoding the global information to re-calibrate the channel-wise weight in each parallel branch, the output features of the two parallel branches are further aggregated by a cross-dimension interaction for capturing pixel-level pairwise relationship. We conduct exten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;&#8212;&#8212;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65292;&#20854;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#21644;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#25351;&#25968;&#26063;&#65292;&#20855;&#26377;&#38381;&#24615;&#26465;&#20214;&#25512;&#26029;&#21644;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.13552</link><description>&lt;p&gt;
&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65306;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;
&lt;/p&gt;
&lt;p&gt;
Squared Neural Families: A New Class of Tractable Density Models. (arXiv:2305.13552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13552
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;&#8212;&#8212;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65292;&#20854;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#21644;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#25351;&#25968;&#26063;&#65292;&#20855;&#26377;&#38381;&#24615;&#26465;&#20214;&#25512;&#26029;&#21644;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20998;&#24067;&#30340;&#28789;&#27963;&#27169;&#22411;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#20998;&#24067;&#31867;&#21035;&#65292;&#31216;&#20026;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65288;SNEFY&#65289;&#65292;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#24182;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;&#31867;&#20284;&#20110;&#26080;&#31351;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#24191;&#27867;&#32852;&#31995;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#24773;&#20917;&#19979;&#65292;SNEFY&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#65292;&#22240;&#27492;&#26159;&#28789;&#27963;&#19988;&#23436;&#20840;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#12290;SNEFY&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#30340;&#25351;&#25968;&#26063;&#65292;&#23545;&#20110;&#26465;&#20214;&#25512;&#26029;&#20855;&#26377;&#38381;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23494;&#24230;&#20272;&#35745;&#21644;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#20219;&#21153;&#20013;&#23637;&#31034;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13551</link><description>&lt;p&gt;
EntRED: &#29992;&#26356;&#23569;&#30340;&#25463;&#24452;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EntRED: Benchmarking Relation Extraction with Fewer Shortcuts. (arXiv:2305.13551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21517;&#31216;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#36215;&#30528;&#26377;&#25928;&#30340;&#20316;&#29992;&#65292;&#24182;&#24120;&#24120;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#38598;&#20013;&#30340;&#23454;&#20307;&#21517;&#31216;&#26174;&#33879;&#24433;&#21709;&#20102;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26631;&#20934;&#30340;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#22823;&#37327;&#38169;&#35823;&#30340;&#23454;&#20307;&#27880;&#37322;&#65292;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19982;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EntRED&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36739;&#23569;&#25463;&#24452;&#21644;&#26356;&#39640;&#23454;&#20307;&#22810;&#26679;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#26500;&#24314;EntRED&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#65288;CI&#65289;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26367;&#25442;&#31649;&#36947;&#65306;ERIC&#12290;ERIC&#23545;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#32422;&#26463;&#26367;&#25442;&#65292;&#20197;&#20943;&#23569;&#20174;&#23454;&#20307;&#20559;&#24046;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;ERIC&#22312;&#20004;&#20010;&#26041;&#38754;&#24212;&#29992;CI&#65306;1&#65289;&#38024;&#23545;&#38656;&#35201;&#23454;&#20307;&#26367;&#25442;&#30340;&#23454;&#20363;&#65292;2&#65289;&#30830;&#23450;&#20505;&#36873;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity names play an effective role in relation extraction (RE) and often influence model performance. As a result, the entity names in the benchmarks' test sets significantly influence the evaluation of RE models. In this work, we find that the standard RE benchmarks' datasets have a large portion of incorrect entity annotations, low entity name diversity, and are prone to have shortcuts from entity names to ground-truth relations. These issues make the standard benchmarks far from reflecting the real-world scenarios. Hence, in this work, we present EntRED, a challenging RE benchmark with reduced shortcuts and higher diversity of entities. To build EntRED, we propose an end-to-end entity replacement pipeline based on causal inference (CI): ERIC. ERIC performs type-constrained replacements on entities to reduce the shortcuts from entity bias to ground-truth relations. ERIC applies CI in two aspects: 1) targeting the instances that need entity replacements, and 2) determining the candid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#21151;&#33021;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25805;&#20316;&#20854;&#26435;&#37325;&#31354;&#38388;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36755;&#20837;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23450;&#20041;&#32622;&#25442;&#31561;&#21464;&#30340;&#26435;&#37325;&#31354;&#38388;&#23618;&#12290;&#22312;&#22788;&#29702;&#21069;&#39304;MLPs&#21644;CNNs&#30340;&#26435;&#37325;&#30340;&#23454;&#39564;&#20013;&#65292;NFTs&#30340;&#24615;&#33021;&#19982;&#25110;&#20248;&#20110;&#20808;&#21069;&#30340;&#26435;&#37325;&#31354;&#38388;&#26041;&#27861;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#32622;&#25442;&#19981;&#21464;&#28508;&#21464;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13546</link><description>&lt;p&gt;
&#31070;&#32463;&#21151;&#33021;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural Functional Transformers. (arXiv:2305.13546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#21151;&#33021;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25805;&#20316;&#20854;&#26435;&#37325;&#31354;&#38388;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36755;&#20837;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23450;&#20041;&#32622;&#25442;&#31561;&#21464;&#30340;&#26435;&#37325;&#31354;&#38388;&#23618;&#12290;&#22312;&#22788;&#29702;&#21069;&#39304;MLPs&#21644;CNNs&#30340;&#26435;&#37325;&#30340;&#23454;&#39564;&#20013;&#65292;NFTs&#30340;&#24615;&#33021;&#19982;&#25110;&#20248;&#20110;&#20808;&#21069;&#30340;&#26435;&#37325;&#31354;&#38388;&#26041;&#27861;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#32622;&#25442;&#19981;&#21464;&#28508;&#21464;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#25968;&#25454;&#30340;&#38544;&#24335;&#34920;&#31034;&#26041;&#24335;&#30340;&#25104;&#21151;&#65292;&#25512;&#21160;&#20102;&#23545;&#31070;&#32463;&#21151;&#33021;&#30340;&#22686;&#38271;&#20852;&#36259;&#65306;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25805;&#20316;&#20854;&#26435;&#37325;&#31354;&#38388;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36755;&#20837;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#26435;&#37325;&#31354;&#38388;&#23545;&#35937;&#30340;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#24615;&#30340;&#31070;&#32463;&#21151;&#33021;&#20307;&#31995;&#32467;&#26500;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23450;&#20041;&#19968;&#31181;&#26032;&#30340;&#32622;&#25442;&#31561;&#21464;&#30340;&#26435;&#37325;&#31354;&#38388;&#23618;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#28145;&#24230;&#31561;&#21464;&#27169;&#22411;&#65292;&#31216;&#20026;&#31070;&#32463;&#21151;&#33021;&#36716;&#25442;&#22120;(NFTs)&#12290;NFTs&#23562;&#37325;&#26435;&#37325;&#31354;&#38388;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#27880;&#24847;&#21147;&#30340;&#20248;&#21183;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#22312;&#22788;&#29702;&#21069;&#39304;MLPs&#21644;CNNs&#30340;&#26435;&#37325;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;NFTs&#30340;&#24615;&#33021;&#19982;&#25110;&#20248;&#20110;&#20808;&#21069;&#30340;&#26435;&#37325;&#31354;&#38388;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;NFTs&#24320;&#21457;&#20102;Inr2Array&#65292;&#19968;&#31181;&#35745;&#31639;&#32622;&#25442;&#19981;&#21464;&#28508;&#21464;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of neural networks as implicit representation of data has driven growing interest in neural functionals: models that can process other neural networks as input by operating directly over their weight spaces. Nevertheless, constructing expressive and efficient neural functional architectures that can handle high-dimensional weight-space objects remains challenging. This paper uses the attention mechanism to define a novel set of permutation equivariant weight-space layers and composes them into deep equivariant models called neural functional Transformers (NFTs). NFTs respect weight-space permutation symmetries while incorporating the advantages of attention, which have exhibited remarkable success across multiple domains. In experiments processing the weights of feedforward MLPs and CNNs, we find that NFTs match or exceed the performance of prior weight-space methods. We also leverage NFTs to develop Inr2Array, a novel method for computing permutation invariant laten
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ConvBoost&#65292;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#19977;&#23618;&#32467;&#26500;&#27169;&#22411;&#21644;&#22686;&#24378;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#20256;&#24863;&#22120;&#27963;&#21160;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#32531;&#35299;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13541</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20256;&#24863;&#22120;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;ConvBoost&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
ConvBoost: Boosting ConvNets for Sensor-based Activity Recognition. (arXiv:2305.13541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ConvBoost&#65292;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#19977;&#23618;&#32467;&#26500;&#27169;&#22411;&#21644;&#22686;&#24378;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#20256;&#24863;&#22120;&#27963;&#21160;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#32531;&#35299;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#26222;&#21450;&#21644;&#21487;&#31359;&#25140;&#35745;&#31639;&#30340;&#26680;&#24515;&#30740;&#31350;&#20027;&#39064;&#20043;&#19968;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20998;&#26512;&#26041;&#27861;&#30340;&#27969;&#34892;&#65292;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#25552;&#21462;&#39640;&#32423;&#29305;&#24449;&#21644;&#36827;&#34892;&#20998;&#31867;&#12290;&#20294;&#30001;&#20110;&#20856;&#22411;HAR&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#29992;&#30340;&#26631;&#35760;&#26679;&#26412;&#25968;&#25454;&#25968;&#37327;&#24448;&#24448;&#38750;&#24120;&#23569;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#36825;&#20351;&#24471;DL-Based HAR&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvBoost-&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#32593;&#32476;&#30340;HAR&#30340;&#26032;&#22411;&#12289;&#19977;&#23618;&#12289;&#32467;&#26500;&#21270;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#22686;&#24378;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#29983;&#25104;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21892;HAR&#65292;&#26088;&#22312;&#32531;&#35299;&#39046;&#22495;&#20869;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is one of the core research themes in ubiquitous and wearable computing. With the shift to deep learning (DL) based analysis approaches, it has become possible to extract high-level features and perform classification in an end-to-end manner. Despite their promising overall capabilities, DL-based HAR may suffer from overfitting due to the notoriously small, often inadequate, amounts of labeled sample data that are available for typical HAR applications. In response to such challenges, we propose ConvBoost -- a novel, three-layer, structured model architecture and boosting framework for convolutional network based HAR. Our framework generates additional training data from three different perspectives for improved HAR, aiming to alleviate the shortness of labeled training data in the field. Specifically, with the introduction of three conceptual layers--Sampling Layer, Data Augmentation Layer, and Resilient Layer -- we develop three "boosters" -R-Frame,
&lt;/p&gt;</description></item><item><title>StyloMetrix&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#21644;&#21477;&#27861;&#30340;&#25991;&#26412;&#25366;&#25496;&#24037;&#20855;&#65292;&#21487;&#20998;&#26512;&#20044;&#20811;&#20848;&#35821;&#30340;&#35821;&#27861;&#12289;&#25991;&#20307;&#21644;&#21477;&#27861;&#27169;&#24335;&#65292;&#36866;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.13530</link><description>&lt;p&gt;
&#38754;&#21521;&#20044;&#20811;&#20848;&#35821;&#30340;&#22522;&#20110;&#35821;&#27861;&#21644;&#21477;&#27861;&#30340;&#35821;&#26009;&#24211;&#20998;&#26512;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
The Grammar and Syntax Based Corpus Analysis Tool For The Ukrainian Language. (arXiv:2305.13530v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13530
&lt;/p&gt;
&lt;p&gt;
StyloMetrix&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#21644;&#21477;&#27861;&#30340;&#25991;&#26412;&#25366;&#25496;&#24037;&#20855;&#65292;&#21487;&#20998;&#26512;&#20044;&#20811;&#20848;&#35821;&#30340;&#35821;&#27861;&#12289;&#25991;&#20307;&#21644;&#21477;&#27861;&#27169;&#24335;&#65292;&#36866;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25991;&#26412;&#25366;&#25496;&#24037;&#20855;StyloMetrix&#65292;&#26368;&#21021;&#29992;&#20110;&#27874;&#20848;&#35821;&#65292;&#21518;&#26469;&#25193;&#23637;&#21040;&#33521;&#35821;&#21644;&#20044;&#20811;&#20848;&#35821;&#12290;&#23427;&#22522;&#20110;&#35745;&#31639;&#35821;&#35328;&#23398;&#23478;&#21644;&#25991;&#23398;&#30740;&#31350;&#20154;&#21592;&#25163;&#24037;&#21046;&#20316;&#30340;&#21508;&#31181;&#25351;&#26631;&#65292;&#20998;&#26512;&#35821;&#27861;&#12289;&#25991;&#20307;&#21644;&#21477;&#27861;&#27169;&#24335;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;StyloMetrix&#31649;&#36947;&#65292;&#24182;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#19968;&#20123;&#23454;&#39564;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#25105;&#20204;&#36719;&#20214;&#21253;&#30340;&#20027;&#35201;&#38480;&#21046;&#21644;&#25351;&#26631;&#30340;&#35780;&#20272;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides an overview of a text mining tool the StyloMetrix developed initially for the Polish language and further extended for English and recently for Ukrainian. The StyloMetrix is built upon various metrics crafted manually by computational linguists and researchers from literary studies to analyze grammatical, stylistic, and syntactic patterns. The idea of constructing the statistical evaluation of syntactic and grammar features is straightforward and familiar for the languages like English, Spanish, German, and others; it is yet to be developed for low-resource languages like Ukrainian. We describe the StyloMetrix pipeline and provide some experiments with this tool for the text classification task. We also describe our package's main limitations and the metrics' evaluation procedure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.13525</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#36890;&#20449;&#30340;&#24322;&#27493;&#24352;&#37327;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Communication-minimizing Asynchronous Tensor Parallelism. (arXiv:2305.13525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#25193;&#22823;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#35774;&#35745;&#33021;&#22815;&#22312;&#22810;GPU&#38598;&#32676;&#19978;&#39640;&#25928;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#30340;&#24182;&#34892;&#31639;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#19977;&#32500;&#65288;3D&#65289;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22823;&#22411;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24182;&#34892;&#35757;&#32451;&#20013;&#30001;&#36890;&#20449;&#24341;&#36215;&#30340;&#31354;&#38386;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20998;&#24067;&#26041;&#24335;&#65292;&#28040;&#38500;&#20102;&#20026;&#28385;&#36275;&#21508;&#23618;&#25968;&#25454;&#20381;&#36182;&#32780;&#38656;&#35201;&#30340;&#36890;&#20449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35757;&#32451;&#36807;&#31243;&#36229;&#20998;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36890;&#20449;&#19982;&#35745;&#31639;&#30340;&#37325;&#21472;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#31354;&#38386;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#27169;&#22411;&#65292;&#24110;&#21161;&#29992;&#25143;&#20026;&#32473;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#36890;&#20449;&#26368;&#20248;&#30340;&#21487;&#29992;&#30828;&#20214;&#36164;&#28304;&#20998;&#35299;&#12290; &#23545;&#20110;256 A100 GPU&#19978;&#30340;28B&#21442;&#25968;CNN&#65292;&#22312;&#26412;&#25991;&#30340; Tensor3D &#26041;&#27861;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604; GPU &#31354;&#38386;&#26102;&#38388;&#20063;&#38477;&#20302;&#20102;&#32422;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
As state-of-the-art neural networks scale to billions of parameters, designing parallel algorithms that can train these networks efficiently on multi-GPU clusters has become critical. This paper presents Tensor3D, a novel three-dimensional (3D) approach to parallelize tensor computations, that strives to minimize the idle time incurred due to communication in parallel training of large multi-billion parameter models. First, we introduce an intelligent distribution of neural network parameters across GPUs that eliminates communication required for satisfying data dependencies of individual layers. Then, we propose a novel overdecomposition of the parallel training process, using which we achieve significant overlap of communication with computation, thereby reducing GPU idle time. Finally, we present a communication model, which helps users identify communication optimal decompositions of available hardware resources for a given neural network. For a 28B parameter CNN on 256 A100 GPUs, 
&lt;/p&gt;</description></item><item><title>Tied-Augment&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#34920;&#31034;&#30456;&#20284;&#24615;&#25552;&#39640;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24456;&#22810;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#12290;</title><link>http://arxiv.org/abs/2305.13520</link><description>&lt;p&gt;
Tied-Augment&#65306;&#25511;&#21046;&#34920;&#31034;&#30456;&#20284;&#24615;&#20197;&#25552;&#39640;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Tied-Augment: Controlling Representation Similarity Improves Data Augmentation. (arXiv:2305.13520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13520
&lt;/p&gt;
&lt;p&gt;
Tied-Augment&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#34920;&#31034;&#30456;&#20284;&#24615;&#25552;&#39640;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24456;&#22810;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#25104;&#20026;&#21322;&#30417;&#30563;&#12289;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#35757;&#32451;&#20013;&#26368;&#20808;&#36827;&#27169;&#22411;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tied-Augment&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#19968;&#20010;&#31616;&#21333;&#30340;&#39033;&#28155;&#21152;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#26469;&#25511;&#21046;&#25197;&#26354;&#19979;&#30340;&#34920;&#31034;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#25552;&#39640;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#12290;Tied-Augment&#21487;&#20197;&#25913;&#21892;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#65292;&#20248;&#21270;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;&#20363;&#22914;RandAugment&#65292;mixup&#21644;SAM&#65289;&#12290;&#20363;&#22914;&#65292;Tied-RandAugment&#21487;&#20197;&#20248;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Data augmentation methods have played an important role in the recent advance of deep learning models, and have become an indispensable component of state-of-the-art models in semi-supervised, self-supervised, and supervised training for vision. Despite incurring no additional latency at test time, data augmentation often requires more epochs of training to be effective. For example, even the simple flips-and-crops augmentation requires training for more than 5 epochs to improve performance, whereas RandAugment requires more than 90 epochs. We propose a general framework called Tied-Augment, which improves the efficacy of data augmentation in a wide range of applications by adding a simple term to the loss that can control the similarity of representations under distortions. Tied-Augment can improve state-of-the-art methods from data augmentation (e.g. RandAugment, mixup), optimization (e.g. SAM), and semi-supervised learning (e.g. FixMatch). For example, Tied-RandAugment can outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#22823;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36798;&#21040;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#36817;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#23545;ASR&#38169;&#35823;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.13512</link><description>&lt;p&gt;
&#33021;ChatGPT&#26816;&#27979;&#20986;&#24847;&#22270;&#21527;&#65311;&#35780;&#20272;&#29992;&#20110;&#21475;&#35821;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding. (arXiv:2305.13512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#22823;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36798;&#21040;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#36817;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#23545;ASR&#38169;&#35823;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#20307;&#29616;&#22312;&#36890;&#36807;&#25552;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#38646;-shot&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#23545;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;ChatGPT&#21644;OPT&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#26368;&#22823;&#27169;&#22411;&#29305;&#26377;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#21363;&#22312;&#32473;&#23450;Oracle&#36716;&#24405;&#30340;&#21508;&#31181;&#35821;&#35328;&#19978;&#65292;&#20854;&#21487;&#20197;&#25509;&#36817;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36866;&#21512;&#21333;&#20010;GPU&#30340;&#36739;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#36828;&#36828;&#33853;&#21518;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#38169;&#35823;&#26696;&#20363;&#36890;&#24120;&#26469;&#33258;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26041;&#26696;&#65307;ChatGPT&#30340;&#21709;&#24212;&#20173;&#28982;&#26159;&#21512;&#29702;&#30340;&#12290;&#20294;&#26159;&#25105;&#20204;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19988;&#23545;ASR&#38169;&#35823;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#34920;&#26126;&#20102;&#23558;&#36825;&#20123;&#25991;&#26412;&#27169;&#22411;&#24212;&#29992;&#20110;&#21475;&#35821;&#29702;&#35299;&#30340;&#20005;&#23803;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large pretrained language models have demonstrated strong language understanding capabilities. This is particularly reflected in their zero-shot and in-context learning abilities on downstream tasks through prompting. To assess their impact on spoken language understanding (SLU), we evaluate several such models like ChatGPT and OPT of different sizes on multiple benchmarks. We verify the emergent ability unique to the largest models as they can reach intent classification accuracy close to that of supervised models with zero or few shots on various languages given oracle transcripts. By contrast, the results for smaller models fitting a single GPU fall far behind. We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#25340;&#36148;&#25340;&#36148;&#65288;collage pasting&#65289;&#65292;&#29992;&#20110;&#22686;&#21152;&#30446;&#26631;&#23494;&#24230;&#65292;&#25552;&#39640;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#35777;&#26126;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13509</link><description>&lt;p&gt;
ColMix -- &#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#21487;&#25552;&#39640;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
ColMix -- A Simple Data Augmentation Framework to Improve Object Detector Performance and Robustness in Aerial Images. (arXiv:2305.13509v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#25340;&#36148;&#25340;&#36148;&#65288;collage pasting&#65289;&#65292;&#29992;&#20110;&#22686;&#21152;&#30446;&#26631;&#23494;&#24230;&#65292;&#25552;&#39640;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#35777;&#26126;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#26816;&#27979;&#25991;&#29486;&#26159;&#22312;MS COCO&#31561;&#25968;&#25454;&#38598;&#19978;&#21457;&#23637;&#36825;&#31181;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26816;&#27979;&#22120;&#24050;&#34987;&#35777;&#26126;&#22312;&#36965;&#24863;&#24212;&#29992;&#20013;&#20063;&#24456;&#26377;&#25928;&#12290;&#32780;&#36825;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#22914;&#26631;&#27880;&#23545;&#35937;&#25968;&#37327;&#23569;&#21644;&#20302;&#30446;&#26631;&#23494;&#24230;&#65292;&#38459;&#30861;&#20102;&#24635;&#20307;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#25340;&#36148;&#25340;&#36148;&#65288;collage pasting&#65289;&#65292;&#29992;&#20110;&#22686;&#21152;&#30446;&#26631;&#23494;&#24230;&#65292;&#26080;&#38656;&#20998;&#21106;&#25513;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#27979;&#22120;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#31867;&#20284;&#30340;&#26041;&#27861;&#65288;&#22914;&#39532;&#36187;&#20811;&#22686;&#24378;&#65289;&#30456;&#27604;&#65292;&#25340;&#36148;&#25340;&#36148;&#65288;collage pasting&#65289;&#21487;&#20197;&#25552;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#65292;&#24182;&#23454;&#29616;&#26356;&#22823;&#30340;&#30446;&#26631;&#23494;&#24230;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#25340;&#36148;&#25340;&#36148;&#65288;collage pasting&#65289;&#23481;&#26131;&#21463;&#21040;&#26576;&#20123;&#20998;&#24067;&#22806;&#31227;&#20301;&#65288;&#22914;&#22270;&#20687;&#25439;&#22351;&#65289;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In the last decade, Convolutional Neural Network (CNN) and transformer based object detectors have achieved high performance on a large variety of datasets. Though the majority of detection literature has developed this capability on datasets such as MS COCO, these detectors have still proven effective for remote sensing applications. Challenges in this particular domain, such as small numbers of annotated objects and low object density, hinder overall performance. In this work, we present a novel augmentation method, called collage pasting, for increasing the object density without a need for segmentation masks, thereby improving the detector performance. We demonstrate that collage pasting improves precision and recall beyond related methods, such as mosaic augmentation, and enables greater control of object density. However, we find that collage pasting is vulnerable to certain out-of-distribution shifts, such as image corruptions. To address this, we introduce two simple approaches
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.13507</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65306;&#19968;&#20221;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#20449;&#24687;&#65292;&#21363;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#65292;&#36890;&#24120;&#20197;&#22810;&#31181;&#24418;&#24335;&#20256;&#36798;&#65292;&#20363;&#22914;&#24102;&#26377;&#26631;&#39064;&#30340;&#22270;&#20687;&#12290; &#23427;&#34987;&#20154;&#20204;&#35270;&#20026;&#26356;&#21487;&#20449;&#65292;&#27604;&#20854;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#23545;&#24212;&#29289;&#25193;&#25955;&#36895;&#24230;&#26356;&#24555;&#65292;&#33539;&#22260;&#26356;&#24191;&#12290; &#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#28041;&#21450;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65288;AFC&#65289;&#65292;&#20294;&#20197;&#24448;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#35823;&#23548;&#26041;&#38754;&#12290; &#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#22810;&#27169;&#24577;&#35823;&#23548;&#29420;&#29305;&#23376;&#20219;&#21153;&#22312;&#20869;&#30340;AFC&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19978;&#35752;&#35770;&#20102;&#19981;&#21516;&#31038;&#21306;&#25152;&#21457;&#23637;&#30340;&#30456;&#20851;&#26415;&#35821;&#12290; &#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23384;&#22312;&#30340;&#22235;&#31181;&#27169;&#24577;&#65306;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#12290; &#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20934;&#21644;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation, i.e. factually incorrect information, is often conveyed in multiple modalities, e.g. an image accompanied by a caption. It is perceived as more credible by humans, and spreads faster and wider than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on textual misinformation. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terminological developed in different communities in the context of our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LaDI-VTON&#65292;&#19968;&#31181;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#25991;&#26412;&#21453;&#28436;&#32452;&#20214;&#22686;&#24378;&#34394;&#25311;&#35797;&#31359;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#36136;&#37327;&#36824;&#21407;&#21830;&#24215;&#26381;&#35013;&#30340;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2305.13501</link><description>&lt;p&gt;
LaDI-VTON: &#28508;&#22312;&#25193;&#25955;&#25991;&#26412;&#21453;&#28436;&#22686;&#24378;&#34394;&#25311;&#35797;&#31359;
&lt;/p&gt;
&lt;p&gt;
LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On. (arXiv:2305.13501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LaDI-VTON&#65292;&#19968;&#31181;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#25991;&#26412;&#21453;&#28436;&#32452;&#20214;&#22686;&#24378;&#34394;&#25311;&#35797;&#31359;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#36136;&#37327;&#36824;&#21407;&#21830;&#24215;&#26381;&#35013;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#21644;&#20803;&#23431;&#23449;&#36825;&#20004;&#20010;&#19981;&#26029;&#21457;&#23637;&#21464;&#21270;&#30340;&#39046;&#22495;&#32487;&#32493;&#23547;&#27714;&#21019;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#28040;&#36153;&#32773;&#20307;&#39564;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#32593;&#32476;&#33021;&#22815;&#21019;&#36896;&#20986;&#38750;&#24120;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#34394;&#25311;&#35797;&#31359;&#65292;&#23601;&#26159;&#29983;&#25104;&#19968;&#20010;&#30446;&#26631;&#27169;&#22411;&#31359;&#30528;&#21830;&#24215;&#20013;&#30340;&#26576;&#20214;&#26381;&#35013;&#30340;&#26032;&#22270;&#29255;&#65292;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LaDI-VTON&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#25991;&#26412;&#21453;&#28436;&#22686;&#24378;&#30340;&#34394;&#25311;&#35797;&#31359;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20381;&#36182;&#20110;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#19968;&#20010;&#26032;&#30340;&#38468;&#21152;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36339;&#36807;&#36830;&#25509;&#26469;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#65292;&#20445;&#25345;&#27169;&#22411;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20445;&#25345;&#21830;&#24215;&#26381;&#35013;&#30340;&#36136;&#22320;&#21644;&#32454;&#33410;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#21453;&#28436;&#32452;&#20214;&#65292;&#21487;&#20197;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;&#28508;&#22312;&#25193;&#25955;&#31354;&#38388;&#65292;&#23454;&#29616;&#26381;&#35013;&#30340;&#39640;&#36136;&#37327;&#32454;&#33410;&#36824;&#21407;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly evolving fields of e-commerce and metaverse continue to seek innovative approaches to enhance the consumer experience. At the same time, recent advancements in the development of diffusion models have enabled generative networks to create remarkably realistic images. In this context, image-based virtual try-on, which consists in generating a novel image of a target model wearing a given in-shop garment, has yet to capitalize on the potential of these powerful generative solutions. This work introduces LaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for the Virtual Try-ON task. The proposed architecture relies on a latent diffusion model extended with a novel additional autoencoder module that exploits learnable skip connections to enhance the generation process preserving the model's characteristics. To effectively maintain the texture and details of the in-shop garment, we propose a textual inversion component that can map the visual features of the 
&lt;/p&gt;</description></item><item><title>Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13484</link><description>&lt;p&gt;
Flover&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13484
&lt;/p&gt;
&lt;p&gt;
Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#27169;&#22411;&#25512;&#26029;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#24182;&#34987;&#37096;&#32626;&#22312;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24773;&#20917;&#19979;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#30001;&#20110;&#22312;&#20247;&#22810;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#19978;&#37319;&#29992;&#20102;&#19968;&#31181;&#26102;&#38388;&#20381;&#36182;&#32467;&#26500;&#65292;&#20854;&#20013;&#24403;&#21069;token&#30340;&#27010;&#29575;&#20998;&#24067;&#21463;&#21040;&#21069;&#38754;token&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26412;&#36136;&#19978;&#30340;&#24207;&#21015;&#29305;&#24615;&#36981;&#24490;&#39532;&#23572;&#21487;&#22827;&#38142;&#20551;&#35774;&#65292;&#32570;&#20047;&#26102;&#38388;&#24182;&#34892;&#24615;&#65292;&#22240;&#27492;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#65292;&#25512;&#26029;&#35831;&#27714;&#36981;&#24490;&#27850;&#26494;&#26102;&#38388;&#20998;&#24067;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#21709;&#24212;&#38271;&#24230;&#65292;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#32570;&#22833;&#26356;&#21152;&#26126;&#26174;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#21160;&#24577;&#25209;&#22788;&#29702;&#21644;&#24182;&#21457;&#27169;&#22411;&#23454;&#20363;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#24320;&#38144;&#21644;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37038;&#20214;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;\dataset&#65292;&#27604;&#36739;&#20102;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#23384;&#22312;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#38590;&#28857;&#65292;&#26410;&#26469;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.13469</link><description>&lt;p&gt;
MAILEX: &#37038;&#20214;&#20107;&#20214;&#19982;&#21442;&#25968;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
MAILEX: Email Event and Argument Extraction. (arXiv:2305.13469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37038;&#20214;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;\dataset&#65292;&#27604;&#36739;&#20102;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#23384;&#22312;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#38590;&#28857;&#65292;&#26410;&#26469;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598; \dataset&#65292;&#29992;&#20110;&#20174;&#37038;&#20214;&#20018;&#20013;&#25191;&#34892;&#20107;&#20214;&#25277;&#21462;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;&#37038;&#20214;&#39046;&#22495;&#20013;&#30340; 10 &#31181;&#20107;&#20214;&#31867;&#22411;&#21644; 76 &#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#25968;&#25454;&#38598;&#21253;&#25324;&#32422; 4K &#23553;&#26631;&#35760;&#26377;&#32422; 9K &#20010;&#20107;&#20214;&#23454;&#20363;&#30340;&#37038;&#20214;&#12290;&#20026;&#20102;&#20102;&#35299;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;&#65292;&#21363;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#65288;&#21253;&#25324;&#20960;&#29575; GPT-3.5&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37038;&#20214;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#36828;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#22240;&#20026;&#23384;&#22312;&#35832;&#22810;&#38590;&#28857;&#65292;&#20363;&#22914;&#25552;&#21462;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#25552;&#21462;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#24314;&#27169;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#20013;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present the first dataset, \dataset, for performing event extraction from conversational email threads. To this end, we first proposed a new taxonomy covering 10 event types and 76 arguments in the email domain. Our final dataset includes $\sim$4K emails annotated with $\sim$9K event instances. To understand the task challenges, we conducted a series of experiments comparing two commonly-seen lines of approaches for event extraction, i.e., sequence labeling and generative end-to-end extraction (including few-shot GPT-3.5). Our results showed that the task of email event extraction is far from being addressed, due to challenges lying in, e.g., extracting non-continuous, shared trigger spans, extracting non-named entity arguments, and modeling the email conversational history. Our work thus suggests more investigations in this domain-specific event extraction task in the future.\footnote{The source code and dataset can be obtained from \url{https://github.com/salokr/Emai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#36951;&#20256;&#31639;&#27861;&#22312;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;&#32463;&#36807;&#39564;&#35777;&#30340;NSGA-II&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.13459</link><description>&lt;p&gt;
&#36951;&#20256;&#31639;&#27861;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The First Proven Performance Guarantees for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) on a Combinatorial Optimization Problem. (arXiv:2305.13459v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#36951;&#20256;&#31639;&#27861;&#22312;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;&#32463;&#36807;&#39564;&#35777;&#30340;NSGA-II&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#26159;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#31361;&#20986;&#31639;&#27861;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#35813;&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#24050;&#34987;&#35777;&#26126;&#65292;&#20294;&#20165;&#36866;&#29992;&#20110;&#21512;&#25104;&#22522;&#20934;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;NP&#23436;&#20840;&#30340;&#21452;&#30446;&#26368;&#23567;&#29983;&#25104;&#26641;&#38382;&#39064;&#25552;&#20379;&#20102;&#39318;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;NSGA-II&#24615;&#33021;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;NSGA-II&#20351;&#29992;&#31181;&#32676;&#22823;&#23567;$N \ge 4((n-1) w_\max + 1)$&#65292;&#33021;&#22815;&#22312;&#26395;&#22312;$O(m^2 n w_\max \log(n w_\max))$&#27425;&#36845;&#20195;&#20013;&#65292;&#35745;&#31639;&#20986;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25152;&#26377;&#26497;&#31471;&#28857;&#65292;&#20854;&#20013;$n$&#26159;&#39030;&#28857;&#25968;&#65292;$m$&#26159;&#36793;&#25968;&#65292;$w_\max$&#26159;&#38382;&#39064;&#23454;&#20363;&#20013;&#30340;&#26368;&#22823;&#36793;&#26435;&#12290;&#35813;&#32467;&#26524;&#36890;&#36807;&#25968;&#23398;&#25163;&#27573;&#35777;&#23454;&#20102;NSGA-II&#30340;&#33391;&#22909;&#25928;&#26524;&#24182;&#19988;&#34920;&#26126;&#21487;&#20197;&#23558;&#27492;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Non-dominated Sorting Genetic Algorithm-II (NSGA-II) is one of the most prominent algorithms to solve multi-objective optimization problems. Recently, the first mathematical runtime guarantees have been obtained for this algorithm, however only for synthetic benchmark problems.  In this work, we give the first proven performance guarantees for a classic optimization problem, the NP-complete bi-objective minimum spanning tree problem. More specifically, we show that the NSGA-II with population size $N \ge 4((n-1) w_{\max} + 1)$ computes all extremal points of the Pareto front in an expected number of $O(m^2 n w_{\max} \log(n w_{\max}))$ iterations, where $n$ is the number of vertices, $m$ the number of edges, and $w_{\max}$ is the maximum edge weight in the problem instance. This result confirms, via mathematical means, the good performance of the NSGA-II observed empirically. It also shows that mathematical analyses of this algorithm are not only possible for synthetic benchmark pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13453</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#21487;&#25512;&#24191;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning based Generalizable Indoor Localization Model using Channel State Information. (arXiv:2305.13453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23460;&#20869;&#23450;&#20301;&#22240;&#20854;&#22312;&#26234;&#33021;&#23478;&#23621;&#12289;&#24037;&#19994;&#33258;&#21160;&#21270;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20381;&#36182;&#20854;&#26080;&#32447;&#35774;&#22791;&#36827;&#34892;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#26080;&#32447;&#21442;&#25968;&#65288;&#22914;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#21644;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#31034;&#22120;&#65288;RSSI&#65289;&#65289;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20934;&#30830;&#20272;&#35745;&#26080;&#32447;&#35774;&#22791;&#30340;&#20301;&#32622;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#36731;&#26494;&#37096;&#32626;&#21040;&#26032;&#29615;&#22659;&#25110;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#23450;&#20301;&#27169;&#22411;&#26469;&#35299;&#20915;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20803;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#22810;&#20803;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indoor localization has gained significant attention in recent years due to its various applications in smart homes, industrial automation, and healthcare, especially since more people rely on their wireless devices for location-based services. Deep learning-based solutions have shown promising results in accurately estimating the position of wireless devices in indoor environments using wireless parameters such as Channel State Information (CSI) and Received Signal Strength Indicator (RSSI). However, despite the success of deep learning-based approaches in achieving high localization accuracy, these models suffer from a lack of generalizability and can not be readily-deployed to new environments or operate in dynamic environments without retraining. In this paper, we propose meta-learning-based localization models to address the lack of generalizability that persists in conventionally trained DL-based localization models. Furthermore, since meta-learning algorithms require diverse dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.13452</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#24314;&#27169;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Measuring and Modeling Physical Intrinsic Motivation. (arXiv:2305.13452v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#26377;&#39537;&#21160;&#21147;&#30340;&#20114;&#21160;&#24615;&#20195;&#29702;&#65292;&#20182;&#20204;&#36861;&#27714;&#26377;&#36259;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;&#24773;&#22659;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24418;&#24335;&#21270;&#30340;&#29289;&#29702;&#20869;&#22312;&#21160;&#26426;&#24418;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#20154;&#31867;&#23545;&#22810;&#31181;&#29289;&#29702;&#24773;&#22659;&#30340;&#35780;&#20998;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#20381;&#36182;&#20110;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#30340;&#27169;&#22411;&#21040;&#20381;&#36182;&#20110;&#21069;&#21521;&#29289;&#29702;&#39044;&#27979;&#30340;&#27169;&#22411;&#30340;&#21508;&#31181;&#20869;&#22312;&#21160;&#26426;&#20551;&#35774;&#26469;&#24314;&#27169;&#20154;&#31867;&#30340;&#36259;&#21619;&#21453;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#31867;&#21453;&#24212;&#30340;&#21333;&#19968;&#26368;&#20339;&#39044;&#27979;&#22120;&#26159;&#38024;&#23545;&#29289;&#29702;&#39044;&#27979;&#25439;&#22833;&#25512;&#23548;&#20986;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#19981;&#33021;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#25512;&#24191;&#20182;&#20204;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#19982;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#20542;&#21521;&#20110;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are interactive agents driven to seek out situations with interesting physical dynamics. Here we formalize the functional form of physical intrinsic motivation. We first collect ratings of how interesting humans find a variety of physics scenarios. We then model human interestingness responses by implementing various hypotheses of intrinsic motivation including models that rely on simple scene features to models that depend on forward physics prediction. We find that the single best predictor of human responses is adversarial reward, a model derived from physical prediction loss. We also find that simple scene feature models do not generalize their prediction of human responses across all scenarios. Finally, linearly combining the adversarial model with the number of collisions in a scene leads to the greatest improvement in predictivity of human responses, suggesting humans are driven towards scenarios that result in high information gain and physical activity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#22521;&#35757;&#36807;&#31243;&#24182;&#23545;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.13426</link><description>&lt;p&gt;
&#23545;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Model Performance in Medical Datasets Over Time. (arXiv:2305.13426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#22521;&#35757;&#36807;&#31243;&#24182;&#23545;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24517;&#39035;&#38754;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25552;&#20986;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20197;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#26681;&#25454;&#22312;&#25972;&#20010;&#30740;&#31350;&#26102;&#38388;&#27573;&#38543;&#26426;&#25277;&#21462;&#30340;&#24739;&#32773;&#26469;&#25286;&#20998;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;&#21463;&#21040;&#21453;&#21521;&#27979;&#35797;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;EMDOT&#27169;&#25311;&#23454;&#36341;&#32773;&#21487;&#33021;&#33021;&#22815;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#25191;&#34892;&#30340;&#28508;&#22312;&#22521;&#35757;&#36807;&#31243;&#65292;&#24182;&#22312;&#25152;&#26377;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#35780;&#20272;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#12290;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#21307;&#30103;&#25968;&#25454;&#28304;&#65288;&#34920;&#26684;&#21644;&#25104;&#20687;&#65289;&#19978;&#35780;&#20272;&#32447;&#24615;&#21644;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#25152;&#26377;&#21382;&#21490;&#25968;&#25454;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#29702;&#24819;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20351;&#29992;&#26368;&#36817;&#25968;&#25454;&#30340;&#31383;&#21475;&#21487;&#33021;&#26159;&#26377;&#21033;&#30340;&#12290;&#22312;&#27169;&#22411;&#31361;&#28982;&#21463;&#21040;&#24433;&#21709;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#22312;&#30456;&#23545;&#36739;&#36817;&#30340;&#25968;&#25454;&#31383;&#21475;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models deployed in healthcare systems must face data drawn from continually evolving environments. However, researchers proposing such models typically evaluate them in a time-agnostic manner, splitting datasets according to patients sampled randomly throughout the entire study time period. This work proposes the Evaluation on Medical Datasets Over Time (EMDOT) framework, which evaluates the performance of a model class across time. Inspired by the concept of backtesting, EMDOT simulates possible training procedures that practitioners might have been able to execute at each point in time and evaluates the resulting models on all future time points. Evaluating both linear and more complex models on six distinct medical data sources (tabular and imaging), we show how depending on the dataset, using all historical data may be ideal in many cases, whereas using a window of the most recent data could be advantageous in others. In datasets where models suffer from sudde
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;EINCASM&#65292;&#19968;&#20010;&#30740;&#31350;&#36719;&#27877;&#33740;&#31867;&#26377;&#26426;&#20307;&#26234;&#33021;&#30340;&#21407;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#32467;&#21512;&#34394;&#25311;&#27969;&#20307;&#36816;&#36755;&#33829;&#20859;&#29289;&#36136;&#21644;&#21270;&#23398;&#20449;&#21495;&#65292;&#36890;&#36807;&#27979;&#35797;&#26234;&#33021;&#30340;&#26041;&#24335;&#30740;&#31350;&#26377;&#26426;&#20307;&#30340;&#26234;&#33021;&#34892;&#20026;&#65292;&#22312;&#26410;&#26469;&#21487;&#20197;&#36827;&#19968;&#27493;&#28145;&#20837;&#30740;&#31350;&#36825;&#20123;&#20998;&#24067;&#24335;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#26234;&#33021;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.13425</link><description>&lt;p&gt;
EINCASM: &#36719;&#27877;&#33740;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#20013;&#30340;&#26032;&#20852;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
EINCASM: Emergent Intelligence in Neural Cellular Automaton Slime Molds. (arXiv:2305.13425v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;EINCASM&#65292;&#19968;&#20010;&#30740;&#31350;&#36719;&#27877;&#33740;&#31867;&#26377;&#26426;&#20307;&#26234;&#33021;&#30340;&#21407;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#32467;&#21512;&#34394;&#25311;&#27969;&#20307;&#36816;&#36755;&#33829;&#20859;&#29289;&#36136;&#21644;&#21270;&#23398;&#20449;&#21495;&#65292;&#36890;&#36807;&#27979;&#35797;&#26234;&#33021;&#30340;&#26041;&#24335;&#30740;&#31350;&#26377;&#26426;&#20307;&#30340;&#26234;&#33021;&#34892;&#20026;&#65292;&#22312;&#26410;&#26469;&#21487;&#20197;&#36827;&#19968;&#27493;&#28145;&#20837;&#30740;&#31350;&#36825;&#20123;&#20998;&#24067;&#24335;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#26234;&#33021;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EINCASM&#65292;&#19968;&#20010;&#37319;&#29992;&#26032;&#39062;&#26694;&#26550;&#30740;&#31350;&#31867;&#20284;&#20110;&#36719;&#27877;&#33740;&#30340;&#26377;&#26426;&#20307;&#20013;&#26032;&#20852;&#26234;&#33021;&#30340;&#21407;&#22411;&#31995;&#32479;&#12290;EINCASM&#36890;&#36807;NEAT&#36827;&#21270;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#65292;&#20197;&#26368;&#22823;&#21270;&#21463;&#33829;&#20859;&#21644;&#33021;&#37327;&#25104;&#26412;&#25152;&#38480;&#21046;&#30340;&#32454;&#32990;&#29983;&#38271;&#12290;&#36825;&#20123;&#26377;&#26426;&#20307;&#21033;&#29992;&#34394;&#25311;&#27969;&#20307;&#26469;&#36816;&#36755;&#33829;&#20859;&#29289;&#36136;&#21644;&#21270;&#23398;&#20449;&#21495;&#65292;&#20197;&#21327;&#35843;&#22312;&#22797;&#26434;&#12289;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#30340;&#29983;&#38271;&#21644;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#30740;&#31350;&#35868;&#39064;&#12289;&#29289;&#29702;&#12289;&#36890;&#20449;&#12289;&#31454;&#20105;&#21644;&#21160;&#24577;&#24320;&#25918;&#24335;&#29615;&#22659;&#22914;&#20309;&#20419;&#36827;&#26234;&#33021;&#34892;&#20026;&#30340;&#20986;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#31181;&#26377;&#26426;&#20307;&#26234;&#33021;&#24615;&#30340;&#21021;&#27493;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#26356;&#24378;&#22823;&#30340;&#31995;&#32479;&#20351;&#29992;EINCASM&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20998;&#24067;&#24335;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents EINCASM, a prototype system employing a novel framework for studying emergent intelligence in organisms resembling slime molds. EINCASM evolves neural cellular automata with NEAT to maximize cell growth constrained by nutrient and energy costs. These organisms capitalize physically simulated fluid to transport nutrients and chemical-like signals to orchestrate growth and adaptation to complex, changing environments. Our framework builds the foundation for studying how the presence of puzzles, physics, communication, competition and dynamic open-ended environments contribute to the emergence of intelligent behavior. We propose preliminary tests for intelligence in such organisms and suggest future work for more powerful systems employing EINCASM to better understand intelligence in distributed dynamical systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#21457;&#23637;&#24615;&#22909;&#22855;&#24515;&#20026;&#22522;&#30784;&#30340;&#20869;&#22312;&#21160;&#26426;&#22914;&#20309;&#20419;&#36827;&#20195;&#29702;&#20154;&#36827;&#34892;&#25506;&#31350;&#65292;&#24182;&#21457;&#29616;&#20195;&#34920;&#26032;&#22855;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#26368;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#22810;&#26679;&#30340;&#20307;&#39564;&#65292;&#24182;&#28608;&#27963;&#20102;&#29615;&#22659;&#20013;&#30340;&#24212;&#21464;&#12290;</title><link>http://arxiv.org/abs/2305.13396</link><description>&lt;p&gt;
&#34394;&#25311;&#20195;&#29702;&#20154;&#20013;&#30340;&#21457;&#23637;&#22909;&#22855;&#24515;&#21644;&#31038;&#20132;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Developmental Curiosity and Social Interaction in Virtual Agents. (arXiv:2305.13396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#21457;&#23637;&#24615;&#22909;&#22855;&#24515;&#20026;&#22522;&#30784;&#30340;&#20869;&#22312;&#21160;&#26426;&#22914;&#20309;&#20419;&#36827;&#20195;&#29702;&#20154;&#36827;&#34892;&#25506;&#31350;&#65292;&#24182;&#21457;&#29616;&#20195;&#34920;&#26032;&#22855;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#26368;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#22810;&#26679;&#30340;&#20307;&#39564;&#65292;&#24182;&#28608;&#27963;&#20102;&#29615;&#22659;&#20013;&#30340;&#24212;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23156;&#20799;&#20250;&#26377;&#24847;&#35782;&#22320;&#25506;&#31350;&#22797;&#26434;&#30340;&#29289;&#29702;&#21644;&#31038;&#20250;&#29615;&#22659;&#12290;&#20026;&#20102;&#25506;&#23547;&#20869;&#22312;&#21160;&#26426;&#22914;&#20309;&#24110;&#21161;&#32452;&#32455;&#25506;&#31350;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#34394;&#25311;&#23156;&#20799;&#20195;&#29702;&#20154;&#24182;&#23558;&#20854;&#32622;&#20110;&#21463;&#21551;&#21457;&#30340;3D&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#22806;&#37096;&#22870;&#21169;&#12290;&#35813;&#29615;&#22659;&#26377;&#19968;&#20010;&#34394;&#25311;&#30340;&#29031;&#30475;&#20195;&#29702;&#20154;&#65292;&#33021;&#22815;&#19982;&#23156;&#20799;&#20195;&#29702;&#20132;&#20114;&#24182;&#20197;&#31867;&#20284;&#28216;&#25103;&#30340;&#26041;&#24335;&#20114;&#21160;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#31867;&#20284;&#20110;&#25512;&#21160;&#20154;&#31867;&#25506;&#32034;&#30340;&#21160;&#26426;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65306;&#24778;&#22855;&#65292;&#19981;&#30830;&#23450;&#24615;&#65292;&#26032;&#22855;&#24615;&#21644;&#23398;&#20064;&#36827;&#24230;&#12290;&#36825;&#20123;&#36890;&#29992;&#30340;&#22870;&#21169;&#20989;&#25968;&#24341;&#23548;&#23156;&#20799;&#20195;&#29702;&#20154;&#25506;&#32034;&#20854;&#29615;&#22659;&#65292;&#24182;&#21457;&#29616;&#20869;&#23884;&#22312;&#29031;&#30475;&#20195;&#29702;&#20013;&#30340;&#24212;&#21464;&#12290;&#20195;&#34920;&#26032;&#22855;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#26368;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#22810;&#26679;&#30340;&#20307;&#39564;&#65292;&#24182;&#28608;&#27963;&#20102;&#29615;&#22659;&#20013;&#30340;&#24212;&#21464;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#23384;&#22312;&#19968;&#20010;&#20851;&#27880;&#19988;&#21453;&#24212;&#36805;&#36895;&#30340;&#29031;&#30475;&#20195;&#29702;&#20154;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infants explore their complex physical and social environment in an organized way. To gain insight into what intrinsic motivations may help structure this exploration, we create a virtual infant agent and place it in a developmentally-inspired 3D environment with no external rewards. The environment has a virtual caregiver agent with the capability to interact contingently with the infant agent in ways that resemble play. We test intrinsic reward functions that are similar to motivations that have been proposed to drive exploration in humans: surprise, uncertainty, novelty, and learning progress. These generic reward functions lead the infant agent to explore its environment and discover the contingencies that are embedded into the caregiver agent. The reward functions that are proxies for novelty and uncertainty are the most successful in generating diverse experiences and activating the environment contingencies. We also find that learning a world model in the presence of an attentiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22823;&#37327;&#24320;&#28304;&#20195;&#30721;&#20013;&#25351;&#20196;&#20108;&#20803;&#32452;&#30340;&#39057;&#29575;&#20998;&#24067;&#65292;&#35777;&#26126;&#21487;&#20197;&#21033;&#29992;&#25351;&#20196;&#20108;&#20803;&#32452;&#38480;&#21046;&#25351;&#20196;&#36873;&#25321;&#20174;&#32780;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#24402;&#32435;&#32534;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.13347</link><description>&lt;p&gt;
&#22522;&#20110;&#25351;&#20196;&#20108;&#20803;&#32452;&#30340;&#24402;&#32435;&#31243;&#24207;&#25628;&#32034;&#31354;&#38388;&#20877;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Further Decimating the Inductive Programming Search Space with Instruction Digrams. (arXiv:2305.13347v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22823;&#37327;&#24320;&#28304;&#20195;&#30721;&#20013;&#25351;&#20196;&#20108;&#20803;&#32452;&#30340;&#39057;&#29575;&#20998;&#24067;&#65292;&#35777;&#26126;&#21487;&#20197;&#21033;&#29992;&#25351;&#20196;&#20108;&#20803;&#32452;&#38480;&#21046;&#25351;&#20196;&#36873;&#25321;&#20174;&#32780;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#24402;&#32435;&#32534;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;&#20154;&#31867;&#21407;&#21019;&#30340;&#20195;&#30721;&#20013;&#23548;&#20986;&#30340;&#37325;&#21472;&#25351;&#20196;&#23376;&#38598;&#33021;&#22815;&#26497;&#22823;&#22320;&#32553;&#23567;&#24402;&#32435;&#32534;&#31243;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#36890;&#24120;&#32553;&#23567;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#26412;&#25991;&#23558;&#25351;&#20196;&#23376;&#38598;&#26041;&#27861;&#25193;&#23637;&#21040;&#32771;&#34385;&#25351;&#20196;-&#25351;&#20196;&#24212;&#29992;&#65288;&#25110;&#25351;&#20196;&#20108;&#20803;&#32452;&#65289;&#20316;&#20026;&#24402;&#32435;&#32534;&#31243;&#30340;&#21478;&#19968;&#31181;&#25628;&#32034;&#21551;&#21457;&#24335;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#37327;&#24320;&#28304;&#20195;&#30721;&#20013;&#25351;&#20196;&#20108;&#20803;&#32452;&#30340;&#39057;&#29575;&#20998;&#24067;&#65292;&#34920;&#26126;&#25351;&#20196;&#20108;&#20803;&#32452;&#20998;&#24067;&#26497;&#24230;&#20559;&#26012;&#65292;&#36229;&#36807;93%&#30340;&#25351;&#20196;&#20108;&#20803;&#32452;&#22312;&#20195;&#30721;&#26679;&#26412;&#20013;&#24182;&#26410;&#20986;&#29616;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25351;&#20196;&#20108;&#20803;&#32452;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#38480;&#21046;&#25351;&#20196;&#36873;&#25321;&#65292;&#36827;&#19968;&#27493;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#30340;&#35268;&#27169;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20943;&#23567;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#22823;&#22823;&#22686;&#21152;&#20102;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#24402;&#32435;&#32534;&#31243;&#25216;&#26415;&#29983;&#25104;&#31243;&#24207;&#30340;&#35268;&#27169;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#32467;&#26524;&#21644;&#30456;&#20851;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overlapping instruction subsets derived from human originated code have previously been shown to dramatically shrink the inductive programming search space, often by many orders of magnitude. Here we extend the instruction subset approach to consider direct instruction-instruction applications (or instruction digrams) as an additional search heuristic for inductive programming. In this study we analyse the frequency distribution of instruction digrams in a large sample of open source code. This indicates that the instruction digram distribution is highly skewed with over 93% of possible instruction digrams not represnted in the code sample. We demonstrate that instruction digrams can be used to constrain instruction selection during search, further reducing size of the the search space, in some cases by several orders of magnitude. This significantly increases the size of programs that can be generated using search based inductive programming techniques. We discuss the results and prov
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#23398;&#21033;&#29992;&#31185;&#23398;&#26041;&#27861;&#22238;&#31572;&#33258;&#28982;&#29616;&#35937;&#65292;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20022;&#29289;&#29702;&#23450;&#24459;&#21644;&#26041;&#31243;&#24335;&#26159;&#20854;&#22522;&#30784;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21457;&#23637;&#65292;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#21644;&#26041;&#31243;&#24335;&#36880;&#28176;&#25104;&#20026;&#30740;&#31350;&#30340;&#26680;&#24515;&#65292;&#20294;&#20173;&#38754;&#20020;&#22810;&#39033;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.13341</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#21644;&#26041;&#31243;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Causal Relations and Equations from Data. (arXiv:2305.13341v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13341
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#21033;&#29992;&#31185;&#23398;&#26041;&#27861;&#22238;&#31572;&#33258;&#28982;&#29616;&#35937;&#65292;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20022;&#29289;&#29702;&#23450;&#24459;&#21644;&#26041;&#31243;&#24335;&#26159;&#20854;&#22522;&#30784;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21457;&#23637;&#65292;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#21644;&#26041;&#31243;&#24335;&#36880;&#28176;&#25104;&#20026;&#30740;&#31350;&#30340;&#26680;&#24515;&#65292;&#20294;&#20173;&#38754;&#20020;&#22810;&#39033;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#26159;&#19968;&#38376;&#21033;&#29992;&#31185;&#23398;&#26041;&#27861;&#22238;&#31572;&#33258;&#28982;&#29616;&#35937;&#20135;&#29983;&#21407;&#22240;&#24182;&#24314;&#31435;&#21487;&#39564;&#35777;&#27169;&#22411;&#26469;&#35299;&#37322;&#36825;&#20123;&#29616;&#35937;&#30340;&#23398;&#31185;&#12290;&#20960;&#20010;&#19990;&#32426;&#20197;&#26469;&#65292;&#21457;&#29616;&#19981;&#21464;&#12289;&#24378;&#20581;&#21644;&#22240;&#26524;&#35299;&#37322;&#19990;&#30028;&#30340;&#26041;&#31243;&#12289;&#27861;&#21017;&#21644;&#21407;&#21017;&#19968;&#30452;&#26159;&#29289;&#29702;&#23398;&#20013;&#30340;&#22522;&#30784;&#12290;&#36825;&#20123;&#21457;&#29616;&#28304;&#20110;&#23545;&#19990;&#30028;&#30340;&#35266;&#23519;&#65292;&#24182;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#22312;&#25152;&#30740;&#31350;&#30340;&#31995;&#32479;&#20013;&#36827;&#34892;&#24178;&#39044;&#30740;&#31350;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#30340;&#20986;&#29616;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#22240;&#26524;&#21644;&#26041;&#31243;&#24335;&#21457;&#29616;&#39046;&#22495;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#32479;&#35745;&#23398;&#12289;&#21746;&#23398;&#20197;&#21450;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#25152;&#26377;&#36825;&#20123;&#39046;&#22495;&#37117;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#21487;&#29992;&#20110;&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12289;&#29289;&#29702;&#23450;&#24459;&#21644;&#26041;&#31243;&#24335;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29289;&#29702;&#23398;&#24191;&#27867;&#39046;&#22495;&#20013;&#30340;&#22240;&#26524;&#21644;&#26041;&#31243;&#24335;&#21457;&#29616;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#30456;&#20851;&#20316;&#21697;&#65292;&#24182;&#27010;&#36848;&#20102;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics is a field of science that has traditionally used the scientific method to answer questions about why natural phenomena occur and to make testable models that explain the phenomena. Discovering equations, laws and principles that are invariant, robust and causal explanations of the world has been fundamental in physical sciences throughout the centuries. Discoveries emerge from observing the world and, when possible, performing interventional studies in the system under study. With the advent of big data and the use of data-driven methods, causal and equation discovery fields have grown and made progress in computer science, physics, statistics, philosophy, and many applied fields. All these domains are intertwined and can be used to discover causal relations, physical laws, and equations from observational data. This paper reviews the concepts, methods, and relevant works on causal and equation discovery in the broad field of Physics and outlines the most important challenges 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13338</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#22240;&#38598;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Gene Set Summarization using Large Language Models. (arXiv:2305.13338v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#35299;&#37322;&#20174;&#39640;&#36890;&#37327;&#23454;&#39564;&#21644;&#35745;&#31639;&#20998;&#26512;&#20013;&#33719;&#24471;&#30340;&#22522;&#22240;&#21015;&#34920;&#12290;&#36825;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#23500;&#38598;&#20998;&#26512;&#26469;&#23436;&#25104;&#30340;&#65292;&#35813;&#20998;&#26512;&#27979;&#37327;&#19982;&#22522;&#22240;&#25110;&#20854;&#23646;&#24615;&#30456;&#20851;&#30340;&#29983;&#29289;&#21151;&#33021;&#26415;&#35821;&#30340;&#36807;&#24230;&#25110;&#27424;&#34920;&#31034;&#31243;&#24230;&#65292;&#22522;&#20110;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#65288;&#20363;&#22914;Gene Ontology&#65288;GO&#65289;&#65289;&#20013;&#30340;&#32534;&#35793;&#26029;&#35328;&#12290;&#35299;&#37322;&#22522;&#22240;&#21015;&#34920;&#20063;&#21487;&#20197;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#25991;&#26412;&#27010;&#25324;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#65292;&#21487;&#33021;&#30452;&#25509;&#21033;&#29992;&#31185;&#23398;&#25991;&#26412;&#24182;&#36991;&#20813;&#20381;&#36182;KB&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;SPINDOCTOR&#65288;&#31283;&#23450;&#30340;&#25552;&#31034;&#25554;&#20540;&#30340;&#21463;&#25511;&#26415;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#27169;&#26495;&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#25191;&#34892;&#22522;&#22240;&#38598;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#26631;&#20934;&#23500;&#38598;&#20998;&#26512;&#30340;&#34917;&#20805;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#22240;&#21151;&#33021;&#20449;&#24687;&#26469;&#28304;&#65306;&#65288;1&#65289;&#20174;&#37492;&#23450;&#30340;&#26412;&#20307;KB&#27880;&#37322;&#20013;&#33719;&#24471;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#65288;2&#65289;&#20174;&#25991;&#26412;&#25366;&#25496;&#20013;&#25512;&#26029;&#30340;&#26412;&#20307;&#26415;&#35821;&#65292;&#20197;&#21450;&#65288;3&#65289;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;1813&#20010;&#22522;&#22240;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SPINDOCTOR&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;GPT&#27169;&#22411;&#26174;&#33879;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#30340;&#22522;&#22240;&#21151;&#33021;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular biologists frequently interpret gene lists derived from high-throughput experiments and computational analysis. This is typically done as a statistical enrichment analysis that measures the over- or under-representation of biological function terms associated with genes or their properties, based on curated assertions from a knowledge base (KB) such as the Gene Ontology (GO). Interpreting gene lists can also be framed as a textual summarization task, enabling the use of Large Language Models (LLMs), potentially utilizing scientific texts directly and avoiding reliance on a KB.  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language Descriptions of Controlled Terms for Ontology Reporting), a method that uses GPT models to perform gene set function summarization as a complement to standard enrichment analysis. This method can use different sources of gene functional information: (1) structured text derived from curated ontological KB annotations, (2) ontol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;2D&#22270;&#24418;&#24182;&#20511;&#21161;&#24179;&#34913;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;EMPNN&#65289;&#39044;&#27979;&#20998;&#23376;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#39044;&#27979;&#20934;&#30830;&#30340;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#65292;&#20248;&#20110;RDKit&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13315</link><description>&lt;p&gt;
&#20351;&#29992;2D&#22270;&#24418;&#36827;&#34892;&#19977;&#32500;&#20998;&#23376;&#20960;&#20309;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
3D Molecular Geometry Analysis with 2D Graphs. (arXiv:2305.13315v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;2D&#22270;&#24418;&#24182;&#20511;&#21161;&#24179;&#34913;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;EMPNN&#65289;&#39044;&#27979;&#20998;&#23376;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#39044;&#27979;&#20934;&#30830;&#30340;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#65292;&#20248;&#20110;RDKit&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#23545;&#20110;&#35768;&#22810;&#20998;&#23376;&#20998;&#26512;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#20195;&#37327;&#23376;&#21147;&#23398;&#26041;&#27861;&#21487;&#20197;&#35745;&#31639;&#20934;&#30830;&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;&#30446;&#21069;&#65292;&#32570;&#20047;&#20174;2D&#22270;&#24418;&#35745;&#31639;&#22522;&#24577;&#19977;&#32500;&#20998;&#23376;&#20960;&#20309;&#24418;&#29366;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#20998;&#23376;&#22270;&#24418;&#39044;&#27979;&#19977;&#32500;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24179;&#34913;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;EMPNN&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#20174;&#20998;&#23376;&#22270;&#24418;&#20013;&#25429;&#25417;&#22522;&#24577;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#20102;&#25552;&#20379;&#19968;&#20010;&#19977;&#32500;&#20998;&#23376;&#20960;&#20309;&#20998;&#26512;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20998;&#23376;&#20960;&#20309;&#25968;&#25454;&#38598;&#12289;&#25968;&#25454;&#20998;&#21106;&#21644;&#35780;&#20272;&#21327;&#35758;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EMPNN&#21487;&#20197;&#27604;RDKit&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#39044;&#27979;&#26356;&#20934;&#30830;&#30340;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#12290;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ground-state 3D geometries of molecules are essential for many molecular analysis tasks. Modern quantum mechanical methods can compute accurate 3D geometries but are computationally prohibitive. Currently, an efficient alternative to computing ground-state 3D molecular geometries from 2D graphs is lacking. Here, we propose a novel deep learning framework to predict 3D geometries from molecular graphs. To this end, we develop an equilibrium message passing neural network (EMPNN) to better capture ground-state geometries from molecular graphs. To provide a testbed for 3D molecular geometry analysis, we develop a benchmark that includes a large-scale molecular geometry dataset, data splits, and evaluation protocols. Experimental results show that EMPNN can efficiently predict more accurate ground-state 3D geometries than RDKit and other deep learning methods. Results also show that the proposed framework outperforms self-supervised learning methods on property prediction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13301</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#28789;&#27963;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#30340;&#36817;&#20284;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#24182;&#19981;&#20851;&#27880;&#20284;&#28982;&#65292;&#32780;&#26159;&#20851;&#27880;&#20154;&#31867;&#24863;&#30693;&#30340;&#22270;&#20687;&#36136;&#37327;&#25110;&#33647;&#29289;&#25928;&#21147;&#31561;&#19979;&#28216;&#30446;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#27492;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#21435;&#22122;&#35270;&#20026;&#22810;&#27493;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#19968;&#31867;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#26367;&#20195;&#30340;&#22870;&#21169;&#21152;&#26435;&#20284;&#28982;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;DDPO&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;DDPO&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#21453;&#39304;&#30340;&#25552;&#31034;-&#22270;&#20687;&#23545;&#40784;&#26041;&#24335;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;SparseFit&#65292;&#19968;&#31181;&#23569;&#26679;&#26412;&#21050;&#28608;&#30340;&#31232;&#30095;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21487;&#29992;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.13235</link><description>&lt;p&gt;
SPARSEFIT&#65306;&#23569;&#26679;&#26412;&#21050;&#28608;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations. (arXiv:2305.13235v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;SparseFit&#65292;&#19968;&#31181;&#23569;&#26679;&#26412;&#21050;&#28608;&#30340;&#31232;&#30095;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21487;&#29992;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#30340;&#20915;&#31574;&#23545;&#20110;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#37096;&#32626;&#26102;&#30340;&#21487;&#20449;&#24230;&#24456;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35777;&#26126;&#27169;&#22411;&#30340;&#39044;&#27979;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20316;&#20026;&#30495;&#23454;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26082;&#26114;&#36149;&#21448;&#21487;&#33021;&#23545;&#20110;&#26576;&#20123;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#20351;&#27169;&#22411;&#22312;&#21482;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21487;&#29992;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#21050;&#28608;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#20351;&#24471;&#24494;&#35843;&#21313;&#20998;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SparseFit&#65292;&#19968;&#31181;&#31232;&#30095;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#31574;&#30053;&#65292;&#21033;&#29992;&#31163;&#25955;&#21050;&#28608;&#26469;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;T5&#27169;&#22411;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;SparseFit&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining the decisions of neural models is crucial for ensuring their trustworthiness at deployment time. Using Natural Language Explanations (NLEs) to justify a model's predictions has recently gained increasing interest. However, this approach usually demands large datasets of human-written NLEs for the ground-truth answers, which are expensive and potentially infeasible for some applications. For models to generate high-quality NLEs when only a few NLEs are available, the fine-tuning of Pre-trained Language Models (PLMs) in conjunction with prompt-based learning recently emerged. However, PLMs typically have billions of parameters, making fine-tuning expensive. We propose SparseFit, a sparse few-shot fine-tuning strategy that leverages discrete prompts to jointly generate predictions and NLEs. We experiment with SparseFit on the T5 model and four datasets and compare it against state-of-the-art parameter-efficient fine-tuning techniques. We perform automatic and human evaluations 
&lt;/p&gt;</description></item><item><title>GPT-SW3&#26159;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#26412;&#22320;&#21270;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20854;&#24320;&#21457;&#36807;&#31243;&#65292;&#21487;&#20316;&#20026;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#38754;&#21521;&#36739;&#23567;&#35821;&#35328;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#21335;&#21644;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2305.12987</link><description>&lt;p&gt;
GPT-SW3&#65306;&#19968;&#31181;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GPT-SW3: An Autoregressive Language Model for the Nordic Languages. (arXiv:2305.12987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12987
&lt;/p&gt;
&lt;p&gt;
GPT-SW3&#26159;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#26412;&#22320;&#21270;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20854;&#24320;&#21457;&#36807;&#31243;&#65292;&#21487;&#20316;&#20026;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#38754;&#21521;&#36739;&#23567;&#35821;&#35328;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#21335;&#21644;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#24320;&#21457;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#26412;&#22320;&#21270;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-SW3&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#24320;&#21457;&#36807;&#31243;&#30340;&#25152;&#26377;&#37096;&#20998;&#65292;&#20174;&#25968;&#25454;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#35757;&#32451;&#37197;&#32622;&#21644;&#25351;&#20196;&#24494;&#35843;&#65292;&#21040;&#35780;&#20272;&#21644;&#21457;&#24067;&#31574;&#30053;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#20316;&#20026;&#25351;&#21335;&#21644;&#21442;&#32771;&#65292;&#24110;&#21161;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#38754;&#21521;&#36739;&#23567;&#35821;&#35328;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper details the process of developing the first native large generative language model for the Nordic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation and considerations for release strategies. We hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20026;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.12715</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65306;&#23398;&#20064;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations. (arXiv:2305.12715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20026;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;ILL&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#65292;&#23558;&#31934;&#30830;&#26631;&#31614;&#35270;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#19982;&#20197;&#21069;&#35797;&#22270;&#20174;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#20013;&#25512;&#26029;&#27491;&#30830;&#26631;&#31614;&#30340;&#22810;&#21151;&#33021;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;ILL&#26694;&#26550;&#32771;&#34385;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#24378;&#21152;&#30340;&#25152;&#26377;&#21487;&#33021;&#26631;&#31614;&#65292;&#20801;&#35768;&#23545;&#20219;&#20309;&#19981;&#31934;&#30830;&#26631;&#31614;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ILL&#21487;&#20197;&#26080;&#32541;&#22320;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#20197;&#21450;&#36825;&#20123;&#37197;&#32622;&#30340;&#28151;&#21512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#19981;&#31934;&#30830;&#26631;&#31614;&#30340;&#25216;&#26415;&#65292;&#26631;&#24535;&#30528;&#31532;&#19968;&#20010;&#32479;&#19968;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the imprecise label learning (ILL) framework, a unified approach to handle various imprecise label configurations, which are commonplace challenges in machine learning tasks. ILL leverages an expectation-maximization (EM) algorithm for the maximum likelihood estimation (MLE) of the imprecise label information, treating the precise labels as latent variables. Compared to previous versatile methods attempting to infer correct labels from the imprecise label information, our ILL framework considers all possible labeling imposed by the imprecise label information, allowing a unified solution to deal with any imprecise labels. With comprehensive experimental results, we demonstrate that ILL can seamlessly adapt to various situations, including partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings. Notably, our simple method surpasses the existing techniques for handling imprecise labels, marking the first unified 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#32771;&#32771;&#35797;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;GAOKAO-Benchmark&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;ChatGPT&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#22312;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.12474</link><description>&lt;p&gt;
&#22312;&#39640;&#32771;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of Large Language Models on GAOKAO Benchmark. (arXiv:2305.12474v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#32771;&#32771;&#35797;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;GAOKAO-Benchmark&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;ChatGPT&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#22312;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#23427;&#20204;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#20219;&#21153;&#20013;&#30340;&#21151;&#25928;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GAOKAO-Benchmark&#65288;GAOKAO-Bench&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#30452;&#35266;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#20351;&#29992;&#20013;&#22269;&#39640;&#32771;&#32771;&#35797;&#30340;&#39064;&#30446;&#20316;&#20026;&#27979;&#35797;&#26679;&#26412;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#23613;&#21487;&#33021;&#22320;&#20351;&#35780;&#20272;&#32467;&#26524;&#19982;&#20154;&#31867;&#19968;&#33268;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;-shot&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#20026;&#20027;&#35266;&#21644;&#23458;&#35266;&#31867;&#22411;&#26469;&#20998;&#26512;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#35780;&#20998;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#27169;&#22411;&#22312;GAOKAO-Benchmark&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT&#27169;&#22411;&#22312;&#35299;&#20915;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#23457;&#26597;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#20154;&#31867;&#35780;&#20272;&#12290;&#24635;&#20043;&#65292;&#26412;&#30740;&#31350;&#20026;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#35780;&#20272;GAOKAO&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated remarkable performance across various natural language processing tasks; however, their efficacy in more challenging and domain-specific tasks remains less explored. This paper introduces the GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions from the Chinese Gaokao examination as test samples for evaluating large language models.In order to align the evaluation results with humans as much as possible, we designed a method based on zero-shot prompts to analyze the accuracy and scoring rate of the model by dividing the questions into subjective and objective types. We evaluated the ChatGPT model on GAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels in tackling objective questions, while also shedding light on its shortcomings and areas for improvement. To further scrutinize the model's responses, we incorporate human evaluations.In conclusion, this research contributes a robust evaluation ben
&lt;/p&gt;</description></item><item><title>PhotoMat&#26159;&#31532;&#19968;&#20010;&#20165;&#22522;&#20110;&#30495;&#23454;&#29289;&#26009;&#29031;&#29255;&#36827;&#34892;&#35757;&#32451;&#30340;&#26448;&#36136;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25968;&#23383;&#26448;&#26009;&#65292;&#36229;&#36807;&#20854;&#20182;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;/&#25110;&#26448;&#36136;&#22320;&#22270;&#30417;&#30563;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12296</link><description>&lt;p&gt;
PhotoMat&#65306;&#20174;&#21333;&#38378;&#20809;&#29031;&#29255;&#20013;&#23398;&#20064;&#30340;&#26448;&#36136;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
PhotoMat: A Material Generator Learned from Single Flash Photos. (arXiv:2305.12296v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12296
&lt;/p&gt;
&lt;p&gt;
PhotoMat&#26159;&#31532;&#19968;&#20010;&#20165;&#22522;&#20110;&#30495;&#23454;&#29289;&#26009;&#29031;&#29255;&#36827;&#34892;&#35757;&#32451;&#30340;&#26448;&#36136;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25968;&#23383;&#26448;&#26009;&#65292;&#36229;&#36807;&#20854;&#20182;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;/&#25110;&#26448;&#36136;&#22320;&#22270;&#30417;&#30563;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#25968;&#23383;&#26448;&#36136;&#30340;&#21046;&#20316;&#23545;&#20110;3D&#28210;&#26579;&#30340;&#36924;&#30495;&#38750;&#24120;&#20851;&#38190;&#12290;&#20197;&#21069;&#30340;&#26448;&#36136;&#29983;&#25104;&#27169;&#22411;&#37117;&#26159;&#20165;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#31181;&#25968;&#25454;&#26377;&#29616;&#23454;&#19990;&#30028;&#20013;&#26448;&#36136;&#30340;&#35270;&#35273;&#24046;&#36317;&#65292;&#32780;&#19988;&#36824;&#24456;&#38590;&#33719;&#24471;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PhotoMat&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20165;&#22522;&#20110;&#25163;&#26426;&#38378;&#20809;&#28783;&#25293;&#25668;&#30340;&#30495;&#23454;&#29289;&#26009;&#29031;&#29255;&#36827;&#34892;&#35757;&#32451;&#30340;&#26448;&#36136;&#29983;&#25104;&#22120;&#12290;&#30001;&#20110;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26080;&#27861;&#33719;&#24471;&#21333;&#29420;&#26448;&#36136;&#22320;&#22270;&#30340;&#30417;&#30563;&#65292;&#22240;&#27492;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#26448;&#36136;&#34920;&#31034;&#29983;&#25104;&#22120;&#65292;&#23427;&#20351;&#29992;&#23398;&#20064;&#30340;&#37325;&#29031;&#26126;&#27169;&#22359;&#28210;&#26579;&#25104;&#20219;&#24847;&#20809;&#29031;&#19979;&#30340;RGB&#22270;&#20687;&#65292;&#19982;&#30495;&#23454;&#29031;&#29255;&#20351;&#29992;&#37492;&#21035;&#22120;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#26448;&#36136;&#22320;&#22270;&#20272;&#35745;&#22120;&#26469;&#20174;&#31070;&#32463;&#26448;&#36136;&#34920;&#31034;&#20013;&#35299;&#30721;&#26448;&#26009;&#30340;&#21453;&#23556;&#23646;&#24615;&#12290;&#25105;&#20204;&#29992;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;PhotoMat&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#25163;&#25345;&#25163;&#26426;&#30456;&#26426;&#22312;&#38378;&#20809;&#28783;&#29031;&#26126;&#19979;&#25429;&#33719;&#30340;12,000&#24352;&#29289;&#26009;&#29031;&#29255;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#26448;&#26009;&#19982;&#30495;&#23454;&#29031;&#29255;&#30456;&#27604;&#20855;&#26377;&#39640;&#24230;&#30340;&#35270;&#35273;&#20445;&#30495;&#24230;&#65292;&#24182;&#19988;&#20248;&#20110;&#20381;&#36182;&#21512;&#25104;&#25968;&#25454;&#21644;/&#25110;&#26448;&#36136;&#22320;&#22270;&#30417;&#30563;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#26448;&#26009;&#21019;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#29031;&#29255;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Authoring high-quality digital materials is key to realism in 3D rendering. Previous generative models for materials have been trained exclusively on synthetic data; such data is limited in availability and has a visual gap to real materials. We circumvent this limitation by proposing PhotoMat: the first material generator trained exclusively on real photos of material samples captured using a cell phone camera with flash. Supervision on individual material maps is not available in this setting. Instead, we train a generator for a neural material representation that is rendered with a learned relighting module to create arbitrarily lit RGB images; these are compared against real photos using a discriminator. We then train a material maps estimator to decode material reflectance properties from the neural material representation. We train PhotoMat with a new dataset of 12,000 material photos captured with handheld phone cameras under flash lighting. We demonstrate that our generated mat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#22312;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#29992;&#25143;&#20449;&#20219;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#36235;&#21183;&#65292;&#21628;&#21505;&#28548;&#28165;&#27010;&#24565;&#20197;&#36991;&#20813;&#21487;&#33021;&#30340;&#20449;&#20219;&#24046;&#36317;&#21644;&#35823;&#35299;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.11876</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#29992;&#25143;&#20449;&#20219;&#35805;&#35821;&#30340;&#25361;&#25112;&#19982;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Challenges and Trends in User Trust Discourse in AI. (arXiv:2305.11876v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#22312;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#29992;&#25143;&#20449;&#20219;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#36235;&#21183;&#65292;&#21628;&#21505;&#28548;&#28165;&#27010;&#24565;&#20197;&#36991;&#20813;&#21487;&#33021;&#30340;&#20449;&#20219;&#24046;&#36317;&#21644;&#35823;&#35299;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1990&#24180;&#30340;&#20114;&#32852;&#32593;&#38761;&#21629;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20449;&#24687;&#38761;&#21629;&#25913;&#21464;&#20102;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#19990;&#30028;&#12290;&#29616;&#22312;&#65292;&#26366;&#32463;&#34987;&#35270;&#20026;&#31185;&#24187;&#24819;&#27861;&#65288;&#21363;&#26426;&#22120;&#32479;&#27835;&#19990;&#30028;&#65289;&#30340;&#20107;&#24773;&#34987;&#35748;&#20026;&#26159;&#21487;&#33021;&#30340;&#12290;&#36825;&#22330;&#38761;&#21629;&#20063;&#24341;&#21457;&#20102;&#23545;&#26032;&#30340;&#30417;&#31649;&#23454;&#36341;&#30340;&#38656;&#27714;&#65292;&#20854;&#20013;&#29992;&#25143;&#20449;&#20219;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35805;&#35821;&#21457;&#25381;&#20102;&#26680;&#24515;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#28548;&#28165;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#29992;&#25143;&#20449;&#20219;&#30340;&#19968;&#20123;&#35823;&#35299;&#65292;&#21453;&#23545;&#20542;&#21521;&#20110;&#35774;&#35745;&#23481;&#26131;&#24341;&#36215;&#20449;&#20219;&#30772;&#35010;&#30340;&#20132;&#20114;&#30340;&#36235;&#21183;&#65292;&#21253;&#25324;&#30495;&#23454;&#21644;&#24863;&#30693;&#19978;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#20204;&#23545;&#20110;&#29992;&#25143;&#20449;&#20219;&#29702;&#35299;&#19981;&#28165;&#26224;&#65292;&#24182;&#24050;&#32463;&#24433;&#21709;&#21040;&#35745;&#31639;&#26426;&#31185;&#23398;&#65292;&#23588;&#20854;&#26159;&#22312;&#27979;&#37327;&#29992;&#25143;&#20449;&#20219;&#29305;&#24449;&#26041;&#38754;&#12290;&#30740;&#31350;&#21628;&#21505;&#28548;&#28165;&#36825;&#20123;&#27010;&#24565;&#65292;&#20197;&#36991;&#20813;&#22312;&#20154;&#24037;&#26234;&#33021;&#37319;&#29992;&#19982;&#36866;&#29992;&#20013;&#20986;&#29616;&#28508;&#22312;&#30340;&#20449;&#20219;&#24046;&#36317;&#21644;&#35823;&#35299;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet revolution in 1990, followed by the data-driven and information revolution, has transformed the world as we know it. Nowadays, what seam to be 10 to 20 years ago, a science fiction idea (i.e., machines dominating the world) is seen as possible. This revolution also brought a need for new regulatory practices where user trust and artificial Intelligence (AI) discourse has a central role. This work aims to clarify some misconceptions about user trust in AI discourse and fight the tendency to design vulnerable interactions that lead to further breaches of trust, both real and perceived. Findings illustrate the lack of clarity in understanding user trust and its effects on computer science, especially in measuring user trust characteristics. It argues for clarifying those notions to avoid possible trust gaps and misinterpretations in AI adoption and appropriation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#21407;&#29702;&#30340;&#26126;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11831</link><description>&lt;p&gt;
&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30340;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regularization of Soft Actor-Critic Algorithms with Automatic Temperature Adjustment. (arXiv:2305.11831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#21407;&#29702;&#30340;&#26126;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#65288;SAC&#65289;&#31639;&#27861;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;&#31574;&#30053;&#35780;&#20272;&#12289;&#31574;&#30053;&#25913;&#36827;&#21644;&#28201;&#24230;&#35843;&#25972;&#36827;&#34892;&#37325;&#26032;&#23450;&#20041;&#21644;&#20462;&#25913;&#65292;&#20197;&#26356;&#21152;&#26126;&#30830;&#22320;&#38416;&#36848;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive analysis to regularize the Soft Actor-Critic (SAC) algorithm with automatic temperature adjustment. The the policy evaluation, the policy improvement and the temperature adjustment are reformulated, addressing certain modification and enhancing the clarity of the original theory in a more explicit manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#33021;&#21147;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#19981;&#21516;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11662</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#35780;&#20272;&#20219;&#21153;&#29702;&#35299;&#65306;ChatGPT&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating task understanding through multilingual consistency: A ChatGPT case study. (arXiv:2305.11662v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#33021;&#21147;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#19981;&#21516;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21151;&#33021;&#30340;&#24778;&#20154;&#25552;&#21319;&#65292;&#21019;&#24314;&#26410;&#26469;&#21487;&#25345;&#32493;&#30340;&#35780;&#20272;&#38598;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#29702;&#35299;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;LLM&#30340;&#33539;&#20363;&#65292;&#35813;&#33539;&#20363;&#21033;&#29992;&#20102;&#27491;&#30830;&#30340;&#19990;&#30028;&#29702;&#35299;&#24212;&#35813;&#22312;&#30456;&#21516;&#21547;&#20041;&#30340;&#19981;&#21516;&#65288;&#24343;&#38647;&#26684;&#65289;&#24847;&#20041;&#19978;&#20445;&#25345;&#19968;&#33268;&#30340;&#24605;&#24819;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#26159;&#36890;&#36807;&#27491;&#30830;&#24615;&#26469;&#34913;&#37327;&#29702;&#35299;&#65292;&#32780;&#26159;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#22810;&#20010;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#21270;&#19968;&#20010;&#27979;&#35797;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#24847;&#20041;&#26159;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#22240;&#27492;&#23558;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#24182;&#21516;&#26102;&#35299;&#20915;&#22810;&#35821;&#35328;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#25105;&#20204;&#20197;&#26368;&#26032;&#29256;&#26412;&#30340;ChatGPT&#20026;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#35937;&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#35780;&#20272;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the staggering pace with which the capabilities of large language models (LLMs) are increasing, creating future-proof evaluation sets to assess their understanding becomes more and more challenging. In this paper, we propose a novel paradigm for evaluating LLMs which leverages the idea that correct world understanding should be consistent across different (Fregean) senses of the same meaning. Accordingly, we measure understanding not in terms of correctness but by evaluating consistency across multiple senses that are generated by the model itself. We showcase our approach by instantiating a test where the different senses are different languages, hence using multilingual self-consistency as a litmus test for the model's understanding and simultaneously addressing the important topic of multilingualism. Taking one of the latest versions of ChatGPT as our object of study, we evaluate multilingual consistency for two different tasks across three different languages. We show that its m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Instruct2Act&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#22810;&#27169;&#24577;&#25351;&#20196;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#23558;&#22797;&#26434;&#30340;&#39640;&#32423;&#25351;&#20196;&#36716;&#25442;&#20026;&#31934;&#30830;&#30340;&#31574;&#30053;&#20195;&#30721;&#65292;&#35813;&#26041;&#27861;&#21487;&#35843;&#25972;&#21644;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#25351;&#20196;&#27169;&#24577;&#21644;&#36755;&#20837;&#31867;&#22411;&#65292;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#20219;&#21153;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.11176</link><description>&lt;p&gt;
Instruct2Act&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#22810;&#27169;&#24577;&#25351;&#20196;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model. (arXiv:2305.11176v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Instruct2Act&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#22810;&#27169;&#24577;&#25351;&#20196;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#23558;&#22797;&#26434;&#30340;&#39640;&#32423;&#25351;&#20196;&#36716;&#25442;&#20026;&#31934;&#30830;&#30340;&#31574;&#30053;&#20195;&#30721;&#65292;&#35813;&#26041;&#27861;&#21487;&#35843;&#25972;&#21644;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#25351;&#20196;&#27169;&#24577;&#21644;&#36755;&#20837;&#31867;&#22411;&#65292;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#20219;&#21153;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12289;&#20840;&#26223;&#20998;&#21106;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Instruct2Act&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#22810;&#27169;&#24577;&#25351;&#20196;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;Instruct2Act&#21033;&#29992;LLM&#27169;&#22411;&#29983;&#25104;Python&#31243;&#24207;&#65292;&#26500;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#20840;&#38754;&#24863;&#30693;&#12289;&#35268;&#21010;&#21644;&#21160;&#20316;&#24490;&#29615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#23558;&#22797;&#26434;&#30340;&#39640;&#32423;&#25351;&#20196;&#36716;&#25442;&#20026;&#31934;&#30830;&#30340;&#31574;&#30053;&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35843;&#25972;&#21644;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#25351;&#20196;&#27169;&#24577;&#21644;&#36755;&#20837;&#31867;&#22411;&#65292;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#20219;&#21153;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#29702;&#20013;&#35299;&#20915;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#24182;&#25311;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35843;&#25972;&#35299;&#20915;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#35777;&#26126;&#26356;&#22823;&#30340;&#36793;&#30028;&#21644;&#21457;&#29616;&#26356;&#22810;&#21453;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11087</link><description>&lt;p&gt;
&#33258;&#21160;&#25512;&#29702;&#39046;&#22495;&#30456;&#20851;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lightweight Online Learning for Sets of Related Problems in Automated Reasoning. (arXiv:2305.11087v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#29702;&#20013;&#35299;&#20915;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#24182;&#25311;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35843;&#25972;&#35299;&#20915;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#35777;&#26126;&#26356;&#22823;&#30340;&#36793;&#30028;&#21644;&#21457;&#29616;&#26356;&#22810;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Self-Driven Strategy Learning (sdsl)&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#33258;&#21160;&#25512;&#29702;&#20013;&#38656;&#35201;&#35299;&#20915;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;sdsl&#20250;&#22312;&#35299;&#20915;&#26089;&#26399;&#38382;&#39064;&#26102;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#26469;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#23427;&#21033;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#25968;&#25454;&#26469;&#35843;&#25972;&#21518;&#32493;&#38382;&#39064;&#30340;&#35299;&#20915;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#32447;&#25311;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#25277;&#35937;&#30340;&#36716;&#25442;&#35268;&#21017;&#12290;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;sdsl&#35745;&#31639;&#23454;&#20363;&#65292;&#20854;&#20013;&#20351;&#29992;&#26465;&#20214;&#37319;&#26679;&#26469;&#29983;&#25104;&#25968;&#25454;&#65292;&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Kissat&#27714;&#35299;&#22120;&#19978;&#23454;&#29616;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;Kissat+sdsl&#22312;&#26368;&#26032;&#30340;&#30828;&#20214;&#27169;&#22411;&#26816;&#26597;&#31454;&#36187;&#20013;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26377;&#38480;&#27169;&#22411;&#26816;&#26597;&#26041;&#27861;&#35777;&#26126;&#20102;&#26356;&#22823;&#30340;&#36793;&#30028;&#21644;&#21457;&#29616;&#20102;&#26356;&#22810;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Self-Driven Strategy Learning (sdsl), a lightweight online learning methodology for automated reasoning tasks that involve solving a set of related problems. sdsl automatically gathers information, in form of a dataset, while solving earlier problems. It utilizes the learned data to adjust the solving strategy for later problems by fitting a machine learning model to the obtained data on the fly. We formally define the approach as a set of abstract transition rules. We describe a concrete instance of the sdsl calculus which uses conditional sampling for generating data and random forests as the underlying machine learning model. We implement the approach on top of the Kissat solver and show that the combination of Kissat+sdsl certifies larger bounds and finds more counter-examples than other state-of-the-art bounded model checking approaches on benchmarks obtained from the latest Hardware Model Checking Competition.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#65288;PEFT&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#21487;&#33021;&#24212;&#29992;&#12290;&#36890;&#36807;&#36229;&#36807;600&#20010;&#25511;&#21046;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PEFT&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;PEFT&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.08252</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#34987;&#24573;&#35270;&#30340;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed Opportunity. (arXiv:2305.08252v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#65288;PEFT&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#21487;&#33021;&#24212;&#29992;&#12290;&#36890;&#36807;&#36229;&#36807;600&#20010;&#25511;&#21046;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PEFT&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;PEFT&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#65288;PEFT&#65289;&#22312;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#12290;PEFT&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#30693;&#35782;&#36716;&#31227;&#30340;&#26377;&#20215;&#20540;&#26041;&#27861;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35270;&#35273;&#12289;&#35821;&#38899;&#20197;&#21450;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#20363;&#22914;&#35270;&#35273;&#35821;&#35328;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#23427;&#30340;&#24212;&#29992;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#34987;&#21033;&#29992;&#65292;&#35843;&#26597;&#21644;&#27604;&#36739;&#35780;&#20272;&#21508;&#31181;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#20854;&#31867;&#21035;&#20013;&#31532;&#19968;&#20010;&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#65292;&#35780;&#20272;&#20102;16&#31181;&#21367;&#31215;&#21644;&#22522;&#20110;&#36716;&#25442;&#22120;&#32593;&#32476;&#30340;PEFT&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20845;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#22823;&#23567;&#65292;&#27169;&#24577;&#21644;&#22797;&#26434;&#24615;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#36229;&#36807;600&#20010;&#25511;&#21046;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#36884;&#24452;&#65292;&#24182;&#24378;&#35843;&#20102;PEFT&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#21628;&#21505;&#22312;&#26410;&#26469;&#30740;&#31350;&#20013;&#21152;&#20197;&#24191;&#27867;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive evaluation of Parameter-Efficient Fine-Tuning (PEFT) techniques for diverse medical image analysis tasks. PEFT is increasingly exploited as a valuable approach for knowledge transfer from pre-trained models in natural language processing, vision, speech, and cross-modal tasks, such as vision-language and text-to-image generation. However, its application in medical image analysis remains relatively unexplored. As foundation models are increasingly exploited in the medical domain, it is crucial to investigate and comparatively assess various strategies for knowledge transfer that can bolster a range of downstream tasks. Our study, the first of its kind (to the best of our knowledge), evaluates 16 distinct PEFT methodologies proposed for convolutional and transformer-based networks, focusing on image classification and text-to-image generation tasks across six medical datasets ranging in size, modality, and complexity. Through a battery of more than 600 control
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06626</link><description>&lt;p&gt;
&#24403;&#22810;&#25968;&#20154;&#26159;&#38169;&#35823;&#30340;&#65306;&#21033;&#29992;&#26631;&#27880;&#32773;&#19981;&#19968;&#33268;&#24615;&#36827;&#34892;&#20027;&#35266;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#25237;&#31080;&#26469;&#30830;&#23450;&#26631;&#31614;&#65292;&#20294;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#65292;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#21453;&#26144;&#20986;&#32676;&#20307;&#35266;&#28857;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#19968;&#20010;&#35821;&#21477;&#26159;&#21542;&#20882;&#29359;&#20102;&#23427;&#25152;&#38024;&#23545;&#30340;&#20154;&#32676;&#65292;&#32780;&#36825;&#21487;&#33021;&#21482;&#21344;&#26631;&#27880;&#32773;&#27744;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#25991;&#26412;&#19978;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30340;&#39044;&#27979;&#30446;&#26631;&#32676;&#20307;&#26469;&#27169;&#25311;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#25552;&#39640;&#20102;22&#65285;&#22312;&#39044;&#27979;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;33&#65285;&#22312;&#39044;&#27979;&#26631;&#27880;&#32773;&#20043;&#38388;&#26041;&#24046;&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19979;&#28216;&#29992;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#20854;&#22312;&#32447;&#24847;&#35265;&#26469;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
&lt;/p&gt;</description></item><item><title>&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36807;&#20110;&#37325;&#35270;&#20070;&#38754;&#35821;&#35328;&#65292;&#24212;&#35813;&#23558;&#21475;&#35821;&#20316;&#20026;&#20027;&#35201;&#20132;&#27969;&#26041;&#24335;&#32435;&#20837;&#32771;&#34385;&#65292;&#30495;&#27491;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#36229;&#36234;&#25991;&#26412;&#65292;&#19982;&#20854;&#20182;&#35821;&#35328;&#31185;&#23398;&#26356;&#22909;&#22320;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#12289;&#26356;&#20687;&#20154;&#31867;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.04572</link><description>&lt;p&gt;
&#35753;&#33258;&#28982;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Putting Natural in Natural Language Processing. (arXiv:2305.04572v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04572
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36807;&#20110;&#37325;&#35270;&#20070;&#38754;&#35821;&#35328;&#65292;&#24212;&#35813;&#23558;&#21475;&#35821;&#20316;&#20026;&#20027;&#35201;&#20132;&#27969;&#26041;&#24335;&#32435;&#20837;&#32771;&#34385;&#65292;&#30495;&#27491;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#36229;&#36234;&#25991;&#26412;&#65292;&#19982;&#20854;&#20182;&#35821;&#35328;&#31185;&#23398;&#26356;&#22909;&#22320;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#12289;&#26356;&#20687;&#20154;&#31867;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#39318;&#20808;&#26159;&#21475;&#35821;&#65292;&#20854;&#27425;&#25165;&#26159;&#20070;&#20889;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#26159;&#35821;&#35328;&#30340;&#19968;&#31181;&#38750;&#24120;&#26041;&#20415;&#21644;&#26377;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#29616;&#20195;&#25991;&#26126;&#24050;&#23558;&#20854;&#26222;&#21450;&#12290;&#22240;&#27492;&#65292;NLP&#39046;&#22495;&#20027;&#35201;&#20851;&#27880;&#22788;&#29702;&#20070;&#38754;&#35821;&#35328;&#65292;&#24456;&#23569;&#20851;&#27880;&#21475;&#35821;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21475;&#35821;&#22788;&#29702;&#21017;&#20027;&#35201;&#38598;&#20013;&#20110;&#29420;&#31435;&#30340;&#35821;&#38899;&#22788;&#29702;&#31038;&#21306;&#65292;&#22312;&#23558;&#35821;&#38899;&#36716;&#24405;&#20026;&#25991;&#26412;&#26041;&#38754;&#19968;&#30452;&#26497;&#20026;&#21344;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#35821;&#38899;&#22788;&#29702;&#21644;&#20027;&#27969;NLP&#26041;&#27861;&#20043;&#38388;&#30340;&#26377;&#21033;&#36235;&#21516;&#12290;&#26377;&#20154;&#35748;&#20026;&#65292;&#29616;&#22312;&#26159;&#23558;&#36825;&#20004;&#20010;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#65292;&#35748;&#30495;&#23545;&#24453;&#21475;&#35821;&#20316;&#20026;&#20154;&#31867;&#20027;&#35201;&#20132;&#27969;&#26041;&#24335;&#30340;&#26102;&#20505;&#20102;&#12290;&#30495;&#27491;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#24102;&#26469;&#19982;&#20854;&#20182;&#35821;&#35328;&#31185;&#23398;&#26356;&#22909;&#30340;&#25972;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#12289;&#26356;&#20687;&#20154;&#31867;&#30340;&#31995;&#32479;&#65292;&#20174;&#32780;&#21487;&#20197;&#36229;&#36234;&#25991;&#26412;&#27169;&#24335;&#36827;&#34892;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processing community which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to a fortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of these two fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processing could lead to better integration with the rest of language science and could lead to systems which are more data-efficient and more human-like, and which can communicate beyond the textual moda
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#65292;&#24182;&#35201;&#27714;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#12289;&#20351;&#29992;&#20844;&#35748;&#25968;&#25454;&#38598;&#20197;&#21450;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.04532</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#36235;&#21183;&#65306;&#19968;&#20010;&#33539;&#22260;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Latest Trends in Artificial Intelligence Technology: A Scoping Review. (arXiv:2305.04532v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#65292;&#24182;&#35201;&#27714;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#12289;&#20351;&#29992;&#20844;&#35748;&#25968;&#25454;&#38598;&#20197;&#21450;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#12290;&#26234;&#33021;&#25163;&#26426;&#12289;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12289;&#25628;&#32034;&#24341;&#25806;&#21644;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#31561;&#24212;&#29992;&#31243;&#24207;&#37117;&#21033;&#29992;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25353;&#29031; PRISMA &#26694;&#26550;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#12290;&#30446;&#26631;&#26159;&#23547;&#25214;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30740;&#31350;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#20174;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36873;&#21462;&#20102;&#19977;&#20010;&#30693;&#21517;&#26399;&#21002;&#65306;&#12298;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#26434;&#24535;&#12299;&#12289;&#12298;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#26434;&#24535;&#12299;&#21644;&#12298;&#26426;&#22120;&#23398;&#20064;&#12299;&#65292;&#24182;&#35266;&#23519;&#20102;2022&#24180;&#21457;&#34920;&#30340;&#25991;&#31456;&#12290;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#19968;&#23450;&#30340;&#36164;&#26684;&#35201;&#27714;&#65306;&#25216;&#26415;&#24517;&#39035;&#38024;&#23545;&#21487;&#27604;&#36739;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#65292;&#24517;&#39035;&#20351;&#29992;&#20844;&#35748;&#25110;&#20854;&#20182;&#20805;&#20998;&#35777;&#26126;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24212;&#29992;&#65292;&#24182;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is more ubiquitous in multiple domains. Smartphones, social media platforms, search engines, and autonomous vehicles are just a few examples of applications that utilize artificial intelligence technologies to enhance their performance. This study carries out a scoping review of the current state-of-the-art artificial intelligence technologies following the PRISMA framework. The goal was to find the most advanced technologies used in different domains of artificial intelligence technology research. Three recognized journals were used from artificial intelligence and machine learning domain: Journal of Artificial Intelligence Research, Journal of Machine Learning Research, and Machine Learning, and articles published in 2022 were observed. Certain qualifications were laid for the technological solutions: the technology must be tested against comparable solutions, commonly approved or otherwise well justified datasets must be used while applying, and results must 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#21512;&#25104;&#26694;&#26550;AADiff&#65292;&#23427;&#20351;&#29992;&#38899;&#39057;&#20449;&#21495;&#25511;&#21046;&#26102;&#38388;&#21160;&#24577;&#65292;&#36890;&#36807;&#38899;&#39057;&#23545;&#40784;&#29983;&#25104;&#35270;&#39057;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#36890;&#36807;&#38899;&#39057;&#21306;&#22495;&#32534;&#36753;&#21644;&#20449;&#21495;&#24179;&#28369;&#65292;&#22312;&#26102;&#38388;&#28789;&#27963;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#21487;&#29992;&#20110;&#20869;&#23481;&#21019;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.04001</link><description>&lt;p&gt;
AADiff: &#22522;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#30340;&#38899;&#39057;&#23545;&#40784;&#35270;&#39057;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion. (arXiv:2305.04001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#21512;&#25104;&#26694;&#26550;AADiff&#65292;&#23427;&#20351;&#29992;&#38899;&#39057;&#20449;&#21495;&#25511;&#21046;&#26102;&#38388;&#21160;&#24577;&#65292;&#36890;&#36807;&#38899;&#39057;&#23545;&#40784;&#29983;&#25104;&#35270;&#39057;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#36890;&#36807;&#38899;&#39057;&#21306;&#22495;&#32534;&#36753;&#21644;&#20449;&#21495;&#24179;&#28369;&#65292;&#22312;&#26102;&#38388;&#28789;&#27963;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#21487;&#29992;&#20110;&#20869;&#23481;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#22312;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#21512;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;T2V&#27169;&#22411;&#20165;&#20351;&#29992;&#25991;&#26412;&#20316;&#20026;&#24341;&#23548;&#65292;&#23427;&#20204;&#24448;&#24448;&#22312;&#24314;&#27169;&#35814;&#32454;&#30340;&#26102;&#38388;&#21160;&#24577;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;T2V&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21478;&#22806;&#20351;&#29992;&#38899;&#39057;&#20449;&#21495;&#26469;&#25511;&#21046;&#26102;&#38388;&#21160;&#24577;&#65292;&#20351;&#24471;&#19968;&#20010;&#29616;&#25104;&#30340;T2I&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#38899;&#39057;&#23545;&#40784;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#22522;&#20110;&#38899;&#39057;&#30340;&#21306;&#22495;&#32534;&#36753;&#21644;&#20449;&#21495;&#24179;&#28369;&#26469;&#22312;&#35270;&#39057;&#21512;&#25104;&#30340;&#20004;&#20010;&#30456;&#20114;&#30683;&#30462;&#30340;&#24895;&#26395;&#65292;&#21363;&#26102;&#38388;&#28789;&#27963;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#26469;&#32463;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20869;&#23481;&#21019;&#24314;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in diffusion models have showcased promising results in the text-to-video (T2V) synthesis task. However, as these T2V models solely employ text as the guidance, they tend to struggle in modeling detailed temporal dynamics. In this paper, we introduce a novel T2V framework that additionally employ audio signals to control the temporal dynamics, empowering an off-the-shelf T2I diffusion to generate audio-aligned videos. We propose audio-based regional editing and signal smoothing to strike a good balance between the two contradicting desiderata of video synthesis, i.e., temporal flexibility and coherence. We empirically demonstrate the effectiveness of our method through experiments, and further present practical applications for contents creation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Vera&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.03695</link><description>&lt;p&gt;
Vera&#65306;&#19968;&#20010;&#29992;&#20110;&#36890;&#29992;&#24120;&#35782;&#35821;&#21477;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Vera&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#20986;&#29616;&#33618;&#35884;&#21644;&#24847;&#22806;&#30340;&#24120;&#35782;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#39038;&#24615;&#39564;&#35777;&#26041;&#27861;&#65292;&#21453;&#24605;LM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;Vera&#65292;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;19&#20010;QA&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#21019;&#24314;&#30340;&#32422;700&#19975;&#26465;&#24120;&#35782;&#35821;&#21477;&#20197;&#21450;&#19977;&#20010;&#35757;&#32451;&#30446;&#26631;&#30340;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;Vera&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#21508;&#31181;&#24120;&#35782;&#39046;&#22495;&#20013;&#30340;&#27491;&#30830;&#21644;&#38169;&#35823;&#35821;&#21477;&#12290;&#24403;&#24212;&#29992;&#20110;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21487;&#37325;&#29992;&#20110;&#24120;&#35782;&#39564;&#35777;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#23427;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;Vera&#22312;&#36807;&#28388;LM&#29983;&#25104;&#30340;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#22686;&#24378;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures. We consider a retrospective verification approach that reflects on the correctness of LM outputs, and introduce Vera, a general-purpose model that estimates the plausibility of declarative statements based on commonsense knowledge. Trained on ~7M commonsense statements created from 19 QA datasets and two large-scale knowledge bases, and with a combination of three training objectives, Vera is a versatile model that effectively separates correct from incorrect statements across diverse commonsense domains. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering LM-generated commonsense knowledge an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.02783</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25216;&#26415;&#20219;&#21153;&#20013;&#33258;&#21160;&#29983;&#25104;YAML&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22312;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#30340;&#21463;&#30410;&#26368;&#22823;&#65292;&#32780;&#38024;&#23545;IT&#33258;&#21160;&#21270;&#31561;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;Ansible-YAML&#30340;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#21253;&#21547;Ansible-YAML&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;&#29992;&#20110;&#25429;&#25417;&#27492;&#39046;&#22495;&#29305;&#24449;&#30340;YAML&#21644;Ansible&#24615;&#33021;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Ansible Wisdom&#21487;&#20197;&#31934;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#21487;&#19982;&#29616;&#26377;&#25216;&#26415;&#30340;&#29366;&#24577;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.13007</link><description>&lt;p&gt;
&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65306;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;&#38382;&#39064;&#35299;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#36339;&#38382;&#39064;&#35299;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#36890;&#24120;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#24605;&#32771;&#27493;&#39588;&#65288;CoT&#65289;&#65292;&#28982;&#21518;&#25165;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#24120;&#26469;&#35828;&#65292;&#22810;&#20010;&#38142;&#26465;&#34987;&#25277;&#26679;&#24182;&#36890;&#36807;&#26368;&#32456;&#31572;&#26696;&#30340;&#25237;&#31080;&#26426;&#21046;&#36827;&#34892;&#32858;&#21512;&#65292;&#20294;&#20013;&#38388;&#27493;&#39588;&#26412;&#36523;&#34987;&#20002;&#24323;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#32771;&#34385;&#38142;&#20043;&#38388;&#30340;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#19981;&#25552;&#20379;&#39044;&#27979;&#31572;&#26696;&#30340;&#32479;&#19968;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340; Multi-Chain Reasoning (MCR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36229;&#36234;&#22810;&#20010;&#24605;&#32771;&#38142;&#65292;&#32780;&#19981;&#26159;&#32858;&#21512;&#22238;&#31572;&#12290;MCR&#26816;&#26597;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#39044;&#27979;&#31572;&#26696;&#26102;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;MCR&#22312;7&#20010;&#22810;&#36339;QA&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;MCR&#30340;&#35299;&#37322;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.11090</link><description>&lt;p&gt;
&#22312;ChatGPT&#26102;&#20195;&#36808;&#21521;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#30340;&#21442;&#32771;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25512;&#20986;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#23558;&#25104;&#20026;&#26410;&#26469;&#22823;&#22810;&#25968;AI&#31995;&#32479;&#30340;&#22522;&#30784;&#26500;&#24314;&#22359;&#30340;&#36235;&#21183;&#27491;&#22312;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#32435;&#20837;AI&#31995;&#32479;&#24341;&#21457;&#20102;&#23545;&#36127;&#36131;&#20219;AI&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#40657;&#21283;&#23376;&#24615;&#36136;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#36229;&#32423;&#26234;&#33021;&#24341;&#36215;&#30340;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#22686;&#38271;&#33021;&#21147;&#26368;&#32456;&#21487;&#33021;&#20250;&#21534;&#22124;AI&#31995;&#32479;&#30340;&#20854;&#20182;&#32452;&#20214;&#65292;&#24341;&#20837;&#26550;&#26500;&#35774;&#35745;&#20013;&#30340;&#36816;&#21160;&#36793;&#30028;&#21644;&#25509;&#21475;&#28436;&#21464;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#26412;&#25991;&#39318;&#20808;&#21576;&#29616;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#22312;&#26550;&#26500;&#28436;&#36827;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#20174;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#36830;&#25509;&#22120;"&#21040;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21333;&#29255;&#26426;&#26680;"&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26550;&#26500;&#65292;&#21253;&#25324;&#20116;&#20010;&#31867;&#21035;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#20026;&#35774;&#35745;&#36127;&#36131;&#20219;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT, Bard, and other large language model (LLM)-based chatbots has drawn huge attention on foundations models worldwide. There is a growing trend that foundation models will serve as the fundamental building blocks for most of the future AI systems. However, incorporating foundation models in AI systems raises significant concerns about responsible AI due to their black box nature and rapidly advancing super-intelligence. Additionally, the foundation model's growing capabilities can eventually absorb the other components of AI systems, introducing the moving boundary and interface evolution challenges in architecture design. To address these challenges, this paper proposes a pattern-oriented responsible-AI-by-design reference architecture for designing foundation model-based AI systems. Specially, the paper first presents an architecture evolution of AI systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;</title><link>http://arxiv.org/abs/2304.10712</link><description>&lt;p&gt;
&#29289;&#29702;&#19990;&#30028;&#20013;&#24858;&#24324;&#28909;&#32418;&#22806;&#25506;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fooling Thermal Infrared Detectors in Physical World. (arXiv:2304.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#22312;&#34892;&#20154;&#26816;&#27979;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#26041;&#38754;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#33021;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#20102;&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#23433;&#20840;&#24615;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20351;&#29992;&#29289;&#29702;&#24178;&#25200;&#65292;&#22914;&#23567;&#28783;&#27873;&#21644;&#28909;&#8220;QR&#20195;&#30721;&#8221;&#26469;&#25915;&#20987;&#32418;&#22806;&#25104;&#20687;&#25506;&#27979;&#22120;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#34987;&#23519;&#35273;&#65292;&#32570;&#20047;&#38544;&#31192;&#24615;&#12290;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#28909;&#21644;&#20919;&#22359;&#26469;&#27450;&#39575;&#32418;&#22806;&#25104;&#20687;&#25506;&#27979;&#22120;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#20174;&#22810;&#20010;&#35282;&#24230;&#25191;&#34892;&#25915;&#20987;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#25105;&#20204;&#26681;&#25454;&#20854;&#26377;&#25928;&#24615;&#12289;&#38544;&#31192;&#24615;&#21644;&#31283;&#20581;&#24615;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AdvIB&#21487;&#20197;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22312;&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#20013;&#25552;&#39640;&#23433;&#20840;&#25514;&#26045;&#30340;&#24517;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infrared imaging systems have a vast array of potential applications in pedestrian detection and autonomous driving, and their safety performance is of great concern. However, few studies have explored the safety of infrared imaging systems in real-world settings. Previous research has used physical perturbations such as small bulbs and thermal "QR codes" to attack infrared imaging detectors, but such methods are highly visible and lack stealthiness. Other researchers have used hot and cold blocks to deceive infrared imaging detectors, but this method is limited in its ability to execute attacks from various angles. To address these shortcomings, we propose a novel physical attack called adversarial infrared blocks (AdvIB). By optimizing the physical parameters of the adversarial infrared blocks, this method can execute a stealthy black-box attack on thermal imaging system from various angles. We evaluate the proposed method based on its effectiveness, stealthiness, and robustness. Our
&lt;/p&gt;</description></item><item><title>NeRFVS&#26159;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#37325;&#24314;&#30340;&#8220;&#20840;&#23616;&#20449;&#24687;&#8221;&#65292;&#21253;&#25324;&#20266;&#28145;&#24230;&#22270;&#21644;&#35270;&#35282;&#35206;&#30422;&#20449;&#24687;&#65292;&#22522;&#20110;&#20960;&#20309;&#25903;&#25745;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#23460;&#20869;&#33258;&#30001;&#23548;&#33322;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#19988;&#21487;&#20943;&#23569;&#21487;&#35265;&#30340;&#20266;&#24433;&#12290;</title><link>http://arxiv.org/abs/2304.06287</link><description>&lt;p&gt;
NeRFVS:&#22522;&#20110;&#20960;&#20309;&#25903;&#25745;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#23454;&#29616;&#33258;&#30001;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds. (arXiv:2304.06287v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06287
&lt;/p&gt;
&lt;p&gt;
NeRFVS&#26159;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#37325;&#24314;&#30340;&#8220;&#20840;&#23616;&#20449;&#24687;&#8221;&#65292;&#21253;&#25324;&#20266;&#28145;&#24230;&#22270;&#21644;&#35270;&#35282;&#35206;&#30422;&#20449;&#24687;&#65292;&#22522;&#20110;&#20960;&#20309;&#25903;&#25745;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#23460;&#20869;&#33258;&#30001;&#23548;&#33322;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#19988;&#21487;&#20943;&#23569;&#21487;&#35265;&#30340;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeRFVS&#30340;&#26032;&#22411;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#23460;&#20869;&#33258;&#30001;&#23548;&#33322;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#37325;&#24314;&#30340;&#8220;&#20840;&#23616;&#20449;&#24687;&#8221;&#65292;&#21253;&#25324;&#20266;&#28145;&#24230;&#22270;&#21644;&#35270;&#35282;&#35206;&#30422;&#20449;&#24687;&#65292;&#20174;&#32780;&#25351;&#23548;3D&#23460;&#20869;&#22330;&#26223;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#37327;&#25351;&#26631;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NeRFVS, a novel neural radiance fields (NeRF) based method to enable free navigation in a room. NeRF achieves impressive performance in rendering images for novel views similar to the input views while suffering for novel views that are significantly different from the training views. To address this issue, we utilize the holistic priors, including pseudo depth maps and view coverage information, from neural reconstruction to guide the learning of implicit neural representations of 3D indoor scenes. Concretely, an off-the-shelf neural reconstruction method is leveraged to generate a geometry scaffold. Then, two loss functions based on the holistic priors are proposed to improve the learning of NeRF: 1) A robust depth loss that can tolerate the error of the pseudo depth map to guide the geometry learning of NeRF; 2) A variance loss to regularize the variance of implicit neural representations to reduce the geometry and color ambiguity in the learning procedure. These two loss
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00354</link><description>&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Context Distribution Shift in Task Representation Learning for Offline Meta RL. (arXiv:2304.00354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;OMRL&#65289;&#26088;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#20419;&#36827;&#26032;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;RL&#37319;&#29992;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#25512;&#26029;&#20219;&#21153;&#34920;&#31034;&#26469;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#28982;&#21518;&#26681;&#25454;&#25512;&#26029;&#20986;&#30340;&#20219;&#21153;&#34920;&#31034;&#35843;&#25972;&#34892;&#21160;&#31574;&#30053;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;OMRL&#65292;&#29305;&#21035;&#26159;OMRL&#20013;&#30340;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#21487;&#33021;&#20250;&#36973;&#21463;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#20351;&#29992;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#24378;&#20581;&#30340;&#20219;&#21153;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#12290;&#22522;&#20110;&#19981;&#21516;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#21033;&#29992;&#23548;&#33268;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#32047;&#31215;&#22238;&#25253;&#27604;&#22522;&#20934;&#26041;&#27861;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline meta reinforcement learning (OMRL) aims to learn transferrable knowledge from offline datasets to facilitate the learning process for new target tasks. Context-based RL employs a context encoder to rapidly adapt the agent to new tasks by inferring about the task representation, and then adjusting the acting policy based on the inferred task representation. Here we consider context-based OMRL, in particular, the issue of task representation learning for OMRL. We empirically demonstrate that the context encoder trained on offline datasets could suffer from distribution shift between the contexts used for training and testing. To tackle this issue, we propose a hard sampling based strategy for learning a robust task context encoder. Experimental results, based on distinct continuous control tasks, demonstrate that the utilization of our technique results in more robust task representations and better testing performance in terms of accumulated returns, compared with baseline metho
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2303.16281</link><description>&lt;p&gt;
&#22823;&#35937;&#30340;&#36879;&#35270;&#38236;&#65306;&#35843;&#26597;&#35895;&#27468;&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#30340;&#35821;&#35328;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16281
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35895;&#27468;&#25628;&#32034;&#8220;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#20449;&#24687;&#65292;&#20197;&#20415;&#20320;&#21487;&#20197;&#24418;&#25104;&#33258;&#24049;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#8221;&#30340;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35895;&#27468;&#21450;&#20854;&#26368;&#31361;&#20986;&#30340;&#25628;&#32034;&#32467;&#26524; - &#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#65292;&#20165;&#21453;&#26144;&#19982;&#8220;&#20315;&#25945;&#8221;&#12289;&#8220;&#33258;&#30001;&#20027;&#20041;&#8221;&#12289;&#8220;&#27542;&#27665;&#21270;&#8221;&#12289;&#8220;&#20234;&#26391;&#8221;&#21644;&#8220;&#32654;&#22269;&#8221;&#31561;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#21516;&#25628;&#32034;&#20013;&#65292;&#23427;&#20204;&#20197;&#19981;&#21516;&#31243;&#24230;&#21576;&#29616;&#19981;&#21516;&#30340;&#20449;&#24687;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#65289;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#22797;&#26434;&#20027;&#39064;&#30340;&#20840;&#29699;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#25628;&#32034;&#20351;&#25105;&#20204;&#25104;&#20026;&#35866;&#35821;&#20013;&#30340;&#30450;&#20154;&#65292;&#20165;&#35302;&#25720;&#23567;&#35937;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#19981;&#30693;&#36947;&#20854;&#20182;&#25991;&#21270;&#30340;&#35270;&#35282;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#29992;&#20110;&#25628;&#32034;&#30340;&#35821;&#35328;&#26368;&#32456;&#25104;&#20026;&#20419;&#36827;&#26412;&#26063;&#20013;&#24515;&#20027;&#20041;&#35266;&#28857;&#30340;&#25991;&#21270;&#36807;&#28388;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#26681;&#25454;&#33258;&#24049;&#30340;&#25991;&#21270;&#35780;&#20272;&#20854;&#20182;&#20154;&#25110;&#24605;&#24819;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;ChatGPT&#20013;&#28145;&#28145;&#23884;&#20837;&#20102;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#20004;&#36339;&#36890;&#20449;(AC2C)&#21327;&#35758;&#65292;&#29992;&#20110;&#21327;&#21516;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#21327;&#35758;&#22312;&#21021;&#22987;&#30340;&#23616;&#37096;&#36890;&#20449;&#21518;&#37319;&#29992;&#33258;&#36866;&#24212;&#30340;&#20004;&#36339;&#36890;&#20449;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#38271;&#36317;&#31163;&#20449;&#24687;&#20132;&#27969;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12515</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#20004;&#36339;&#36890;&#20449;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AC2C: Adaptively Controlled Two-Hop Communication for Multi-Agent Reinforcement Learning. (arXiv:2302.12515v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#20004;&#36339;&#36890;&#20449;(AC2C)&#21327;&#35758;&#65292;&#29992;&#20110;&#21327;&#21516;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#21327;&#35758;&#22312;&#21021;&#22987;&#30340;&#23616;&#37096;&#36890;&#20449;&#21518;&#37319;&#29992;&#33258;&#36866;&#24212;&#30340;&#20004;&#36339;&#36890;&#20449;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#38271;&#36317;&#31163;&#20449;&#24687;&#20132;&#27969;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23398;&#20064;&#21327;&#21516;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#31574;&#30053;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#26234;&#33021;&#20307;&#20043;&#38388;&#26377;&#23436;&#20840;&#36830;&#25509;&#30340;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#65292;&#23548;&#33268;&#39640;&#26114;&#30340;&#36890;&#20449;&#25104;&#26412;&#24182;&#19988;&#19981;&#20999;&#23454;&#38469;&#12290;&#36817;&#26399;&#30340;&#19968;&#20123;&#24037;&#20316;&#24050;&#32463;&#21457;&#23637;&#20986;&#20102;&#33258;&#36866;&#24212;&#36890;&#20449;&#31574;&#30053;&#20197;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#33719;&#21462;&#36229;&#20986;&#36890;&#20449;&#33539;&#22260;&#20043;&#22806;&#30340;&#26234;&#33021;&#20307;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#27599;&#20010;&#26234;&#33021;&#20307;&#20855;&#26377;&#26377;&#38480;&#30340;&#36890;&#20449;&#33539;&#22260;&#20197;&#21450;&#36890;&#20449;&#25299;&#25169;&#23454;&#26102;&#21160;&#24577;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23454;&#29616;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#20004;&#36339;&#36890;&#20449;(AC2C)&#12290;&#22312;&#21021;&#22987;&#30340;&#23616;&#37096;&#36890;&#20449;&#20043;&#21518;&#65292;AC2C&#37319;&#29992;&#33258;&#36866;&#24212;&#30340;&#20004;&#36339;&#36890;&#20449;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#38271;&#36317;&#31163;&#20449;&#24687;&#20132;&#27969;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#36825;&#26159;&#20165;&#36890;&#36807;&#23616;&#37096;&#36890;&#20449;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;AC2C&#22312;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#36890;&#20449;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning communication strategies in cooperative multi-agent reinforcement learning (MARL) has recently attracted intensive attention. Early studies typically assumed a fully-connected communication topology among agents, which induces high communication costs and may not be feasible. Some recent works have developed adaptive communication strategies to reduce communication overhead, but these methods cannot effectively obtain valuable information from agents that are beyond the communication range. In this paper, we consider a realistic communication model where each agent has a limited communication range, and the communication topology dynamically changes. To facilitate effective agent communication, we propose a novel communication protocol called Adaptively Controlled Two-Hop Communication (AC2C). After an initial local communication round, AC2C employs an adaptive two-hop communication strategy to enable long-range information exchange among agents to boost performance, which is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.10724</link><description>&lt;p&gt;
ChatGPT&#65306;&#24212;&#20184;&#21315;&#20107;&#30340;&#19975;&#33021;&#22411; AI&#65292;&#20294;&#26080;&#25152;&#19987;&#31934;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#25512;&#20986;&#20102;&#32842;&#22825;&#29983;&#25104;&#39044;&#35757;&#32451; Transformer&#65288;ChatGPT&#65289;&#65292;&#38761;&#26032;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797; ChatGPT &#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38750;&#33258;&#21160;&#21270;&#65292;&#24182;&#19988;&#35268;&#27169;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#26816;&#39564;&#20102; ChatGPT &#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20219;&#21153;&#29978;&#33267;&#23545;&#20154;&#31867;&#32780;&#35328;&#37117;&#26159;&#20027;&#35266;&#30340;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#12289;&#25915;&#20987;&#24615;&#21644;&#31435;&#22330;&#26816;&#27979;&#12290;&#21478;&#19968;&#20123;&#20219;&#21153;&#21017;&#38656;&#35201;&#26356;&#23458;&#35266;&#30340;&#25512;&#29702;&#65292;&#22914;&#35789;&#20041;&#28040;&#27495;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#23545; GPT-4 &#27169;&#22411;&#22312;&#20116;&#20010;&#36873;&#23450;&#30340; NLP &#20219;&#21153;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#33258;&#21160;&#21270;&#20102; ChatGPT &#21644; GPT-4 &#30340;&#24341;&#23548;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#36229;&#36807; 49k &#20010;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;SOTA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978; ChatGPT &#30340;&#24615;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
&lt;/p&gt;</description></item><item><title>&#21453;&#21319;&#32423;&#25110;&#27010;&#25324;&#26159;&#24402;&#32435;&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#26159;&#23450;&#29702;&#35777;&#26126;&#30340;&#21452;&#37325;&#25805;&#20316;&#20043;&#19968;&#12290;&#35813;&#35843;&#26597;&#25253;&#21578;&#23545;&#21453;&#21319;&#32423;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#24402;&#32435;&#21644;&#24635;&#32467;&#12290;</title><link>http://arxiv.org/abs/2302.00277</link><description>&lt;p&gt;
&#21453;&#21319;&#32423;&#19982;&#27010;&#25324;&#65306;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Anti-unification and Generalization: A Survey. (arXiv:2302.00277v3 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00277
&lt;/p&gt;
&lt;p&gt;
&#21453;&#21319;&#32423;&#25110;&#27010;&#25324;&#26159;&#24402;&#32435;&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#26159;&#23450;&#29702;&#35777;&#26126;&#30340;&#21452;&#37325;&#25805;&#20316;&#20043;&#19968;&#12290;&#35813;&#35843;&#26597;&#25253;&#21578;&#23545;&#21453;&#21319;&#32423;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#24402;&#32435;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21319;&#32423;&#65288;AU&#65289;&#21448;&#31216;&#27010;&#25324;&#65292;&#26159;&#24402;&#32435;&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#26159;&#23450;&#29702;&#35777;&#26126;&#22522;&#30784;&#19978;&#30340;&#21452;&#37325;&#25805;&#20316;&#20043;&#19968;&#12290; AI&#21644;&#30456;&#20851;&#31038;&#21306;&#23545;AU&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20294;&#27809;&#26377;&#31995;&#32479;&#30740;&#31350;&#35813;&#27010;&#24565;&#65292;&#20063;&#27809;&#26377;&#29616;&#26377;&#24037;&#20316;&#30340;&#35843;&#26597;&#65292;&#35843;&#26597;&#24448;&#24448;&#20250;&#37319;&#29992;&#29305;&#23450;&#20110;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#24050;&#32463;&#34987;&#29616;&#26377;&#26041;&#27861;&#35206;&#30422;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#26377;&#20851;AU&#30740;&#31350;&#21450;&#20854;&#24212;&#29992;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#20197;&#21450;&#19968;&#31181;&#23558;&#29616;&#26377;&#21644;&#26410;&#26469;&#21457;&#23637;&#20998;&#31867;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anti-unification (AU), also known as generalization, is a fundamental operation used for inductive inference and is the dual operation to unification, an operation at the foundation of theorem proving. Interest in AU from the AI and related communities is growing, but without a systematic study of the concept, nor surveys of existing work, investigations7 often resort to developing application-specific methods that may be covered by existing approaches. We provide the first survey of AU research and its applications, together with a general framework for categorizing existing and future developments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12901</link><description>&lt;p&gt;
&#27169;&#25311;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#34701;&#20837;&#29616;&#26377;&#20132;&#36890;&#31995;&#32479;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Simulating the Integration of Urban Air Mobility into Existing Transportation Systems: A Survey. (arXiv:2301.12901v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys the current state of research on urban air mobility (UAM) in metropolitan-scale traffic using simulation techniques, identifying key challenges and opportunities for integrating UAM into urban transportation systems, including impacts on existing traffic patterns and congestion, safety analysis and risk assessment, potential economic and environmental benefits, and the development of shared infrastructure and routes for UAM and ground-based transportation. The potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas, are also discussed.
&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#22823;&#37117;&#24066;&#22320;&#21306;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#32531;&#35299;&#25317;&#22581;&#65292;&#25552;&#39640;&#21487;&#36798;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;UAM&#34701;&#20837;&#29616;&#26377;&#20132;&#36890;&#31995;&#32479;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#20854;&#23545;&#20132;&#36890;&#27969;&#37327;&#21644;&#23481;&#37327;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#20351;&#29992;&#27169;&#25311;&#25216;&#26415;&#35843;&#26597;&#20102;UAM&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Urban air mobility (UAM) has the potential to revolutionize transportation in metropolitan areas, providing a new mode of transportation that could alleviate congestion and improve accessibility. However, the integration of UAM into existing transportation systems is a complex task that requires a thorough understanding of its impact on traffic flow and capacity. In this paper, we conduct a survey to investigate the current state of research on UAM in metropolitan-scale traffic using simulation techniques. We identify key challenges and opportunities for the integration of UAM into urban transportation systems, including impacts on existing traffic patterns and congestion; safety analysis and risk assessment; potential economic and environmental benefits; and the development of shared infrastructure and routes for UAM and ground-based transportation. We also discuss the potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas. Our survey 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#22312;&#25289;&#36817;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#40511;&#27807;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26426;&#22120;&#29983;&#25104;&#20316;&#21697;&#30340;&#21407;&#21019;&#24615;&#21644;&#21487;&#35782;&#21035;&#24615;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2301.11722</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#33402;&#26415;&#23478;&#65306;&#25105;&#20204;&#27491;&#22312;&#25289;&#36817;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#40511;&#27807;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?. (arXiv:2301.11722v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#22312;&#25289;&#36817;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#40511;&#27807;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26426;&#22120;&#29983;&#25104;&#20316;&#21697;&#30340;&#21407;&#21019;&#24615;&#21644;&#21487;&#35782;&#21035;&#24615;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#37324;&#31243;&#30865;&#26159;&#24320;&#21457;&#33021;&#22815;&#29983;&#25104;&#21644;&#20154;&#31867;&#26080;&#24322;&#30340;&#32472;&#30011;&#20316;&#21697;&#30340;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992; Boutin &#31561;&#20154; 2022 &#24180;&#25552;&#20986;&#30340;&#8220;&#22810;&#26679;&#24615; vs. &#21487;&#35782;&#21035;&#24615;&#8221;&#35780;&#20998;&#26694;&#26550;&#65292;&#21457;&#29616;&#19968;&#21457;&#21363;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#30830;&#23454;&#24050;&#32463;&#24320;&#22987;&#25289;&#36817;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#28982;&#32780;&#65292;&#29992;&#26356;&#32454;&#31890;&#24230;&#30340;&#26679;&#26412;&#29420;&#21019;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#24378;&#21270;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#26377;&#21161;&#20110;&#25552;&#39640;&#23427;&#20204;&#30340;&#32472;&#30011;&#20154;&#24615;&#21270;&#31243;&#24230;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#25509;&#36817;&#20154;&#31867;&#32472;&#30011;&#20316;&#21697;&#30340;&#21407;&#21019;&#24615;&#21644;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#22312;&#32447;&#24515;&#29702;&#29289;&#29702;&#23454;&#39564;&#25910;&#38598;&#20154;&#31867;&#31867;&#21035;&#35786;&#26029;&#29305;&#24449;&#24182;&#23558;&#20854;&#19982;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#23548;&#20986;&#30340;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#20381;&#36182;&#20110;&#26356;&#23569;&#19988;&#26356;&#23616;&#37096;&#30340;&#29305;&#24449;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#25193;&#25955;&#27169;&#22411;&#22312;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#32472;&#30011;&#20316;&#21697;&#30340;&#36136;&#37327;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important milestone for AI is the development of algorithms that can produce drawings that are indistinguishable from those of humans. Here, we adapt the 'diversity vs. recognizability' scoring framework from Boutin et al, 2022 and find that one-shot diffusion models have indeed started to close the gap between humans and machines. However, using a finer-grained measure of the originality of individual samples, we show that strengthening the guidance of diffusion models helps improve the humanness of their drawings, but they still fall short of approximating the originality and recognizability of human drawings. Comparing human category diagnostic features, collected through an online psychophysics experiment, against those derived from diffusion models reveals that humans rely on fewer and more localized features. Overall, our study suggests that diffusion models have significantly helped improve the quality of machine-generated drawings; however, a gap between humans and machines 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#32467;&#26500;&#21270;&#29289;&#20307;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32534;&#30721;&#38190;&#30340;&#20540;&#30340;&#34920;&#31034;&#24182;&#33258;&#25105;&#20851;&#27880;&#36825;&#20123;&#38190;&#20197;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#26469;&#35299;&#20915;&#38271;&#23545;&#35937;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.01015</link><description>&lt;p&gt;
&#21322;&#32467;&#26500;&#21270;&#29289;&#20307;&#24207;&#21015;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Semi-Structured Object Sequence Encoders. (arXiv:2301.01015v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#32467;&#26500;&#21270;&#29289;&#20307;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32534;&#30721;&#38190;&#30340;&#20540;&#30340;&#34920;&#31034;&#24182;&#33258;&#25105;&#20851;&#27880;&#36825;&#20123;&#38190;&#20197;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#26469;&#35299;&#20915;&#38271;&#23545;&#35937;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24314;&#27169;&#21322;&#32467;&#26500;&#21270;&#23545;&#35937;&#24207;&#21015;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#20851;&#27880;&#24320;&#21457;&#36825;&#20123;&#24207;&#21015;&#30340;&#32467;&#26500;&#24863;&#30693;&#36755;&#20837;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#25968;&#25454;&#30340;&#20363;&#23376;&#21253;&#25324;&#29992;&#25143;&#22312;&#32593;&#31449;&#19978;&#30340;&#27963;&#21160;&#12289;&#26426;&#22120;&#26085;&#24535;&#31561;&#12290;&#30001;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#31181;&#25968;&#25454;&#32463;&#24120;&#34987;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#30340;&#38190;&#20540;&#23545;&#38598;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#26041;&#27861;&#65292;&#39318;&#20808;&#29420;&#31435;&#32771;&#34385;&#27599;&#20010;&#38190;&#24182;&#32534;&#30721;&#20854;&#20540;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#33258;&#25105;&#20851;&#27880;&#36825;&#20123;&#20855;&#26377;&#20540;&#24863;&#30693;&#30340;&#38190;&#34920;&#31034;&#20197;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#25805;&#20316;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#38271;&#30340;&#23545;&#35937;&#24207;&#21015;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#27169;&#22359;&#20043;&#38388;&#30340;&#26032;&#22411;&#20849;&#20139;&#27880;&#24847;&#21147;&#22836;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#20132;&#26367;&#35757;&#32451;&#20004;&#20010;&#27169;&#22359;&#65292;&#26576;&#20123;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#20849;&#20139;&#26435;&#37325;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;&#23427;&#22312;&#20960;&#20010;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we explore the task of modeling semi-structured object sequences; in particular, we focus our attention on the problem of developing a structure-aware input representation for such sequences. Examples of such data include user activity on websites, machine logs, and many others. This type of data is often represented as a sequence of sets of key-value pairs over time and can present modeling challenges due to an ever-increasing sequence length. We propose a two-part approach, which first considers each key independently and encodes a representation of its values over time; we then self-attend over these value-aware key representations to accomplish a downstream task. This allows us to operate on longer object sequences than existing methods. We introduce a novel shared-attention-head architecture between the two modules and present an innovative training schedule that interleaves the training of both modules with shared weights for some attention heads. Our experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09561</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#30340;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#26102;&#65292;&#23427;&#38750;&#24120;&#25935;&#24863;&#20110;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#35757;&#32451;&#39564;&#35777;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#20316;&#20026;&#26465;&#20214;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#34987;&#25513;&#30422;&#30340;&#21407;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#22522;&#20110;&#20934;&#30830;&#24615;&#35745;&#31639;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26102;&#25552;&#39640;&#22810;&#20010;&#31639;&#26415;&#21644;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;LLM&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22810;&#31181;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QEBVerif&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35823;&#24046;&#36793;&#30028;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#24352;&#37327;&#65292;&#20197;&#35299;&#20915;&#22312;&#37327;&#21270;&#21518;&#20851;&#38190;&#39564;&#35777;&#23646;&#24615;&#21464;&#24471;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.02781</link><description>&lt;p&gt;
QEBVerif&#65306;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#35823;&#24046;&#36793;&#30028;&#30340;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
QEBVerif: Quantization Error Bound Verification of Neural Networks. (arXiv:2212.02781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QEBVerif&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35823;&#24046;&#36793;&#30028;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#24352;&#37327;&#65292;&#20197;&#35299;&#20915;&#22312;&#37327;&#21270;&#21518;&#20851;&#38190;&#39564;&#35777;&#23646;&#24615;&#21464;&#24471;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32531;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23454;&#38469;&#38480;&#21046;&#65292;&#37327;&#21270;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;DNN&#30340;&#26435;&#37325;&#21644;/&#25110;&#28608;&#27963;&#24352;&#37327;&#37327;&#21270;&#20026;&#36739;&#20302;&#20301;&#23485;&#30340;&#23450;&#28857;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#65292;&#38477;&#20302;&#20102;&#23545;&#35745;&#31639;&#33021;&#21147;&#21644;&#23384;&#20648;&#31354;&#38388;&#30340;&#36164;&#28304;&#35201;&#27714;&#65292;&#23613;&#31649;&#24050;&#32463;&#32463;&#39564;&#35777;&#26126;&#23427;&#20250;&#24341;&#20837;&#36731;&#24494;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#65292;&#20294;&#26159;&#22312;&#37327;&#21270;&#21518;&#65292;DNN&#30340;&#20851;&#38190;&#39564;&#35777;&#23646;&#24615;&#21487;&#33021;&#21464;&#24471;&#26080;&#25928;&#12290;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#19987;&#27880;&#20110;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#25110;QNN&#65289;&#25110;&#37096;&#20998;&#37327;&#21270;&#30340;&#37327;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QEBVerif&#30340;&#37327;&#21270;&#35823;&#24046;&#36793;&#30028;&#39564;&#35777;&#26041;&#27861;&#65292;&#20854;&#20013;&#26435;&#37325;&#21644;&#28608;&#27963;&#24352;&#37327;&#37117;&#34987;&#37327;&#21270;&#20102;&#12290;QEBVerif&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65292;&#21363;&#19981;&#21516;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65288;DRA&#65289;&#21644;&#22522;&#20110;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#30340;&#39564;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To alleviate the practical constraints for deploying deep neural networks (DNNs) on edge devices, quantization is widely regarded as one promising technique. It reduces the resource requirements for computational power and storage space by quantizing the weights and/or activation tensors of a DNN into lower bit-width fixed-point numbers, resulting in quantized neural networks (QNNs). While it has been empirically shown to introduce minor accuracy loss, critical verified properties of a DNN might become invalid once quantized. Existing verification methods focus on either individual neural networks (DNNs or QNNs) or quantization error bound for partial quantization. In this work, we propose a quantization error bound verification method, named QEBVerif, where both weights and activation tensors are quantized. QEBVerif consists of two parts, i.e., a differential reachability analysis (DRA) and a mixed-integer linear programming (MILP) based verification method. DRA performs difference an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#65292;&#23545;&#20110;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.00679</link><description>&lt;p&gt;
&#26410;&#30693;&#21160;&#24577;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#25511;&#21046;&#22120;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Formal Controller Synthesis for Markov Jump Linear Systems with Uncertain Dynamics. (arXiv:2212.00679v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#65292;&#23545;&#20110;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#65292;&#23545;&#20110;&#25511;&#21046;&#22120;&#30340;&#33258;&#21160;&#21270;&#21512;&#25104;&#21487;&#20197;&#30830;&#20445;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#28151;&#21512;&#29305;&#24615;&#21644;&#38543;&#26426;&#25110;&#26410;&#30693;&#30340;&#34892;&#20026;&#20351;&#24471;&#21512;&#25104;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#36825;&#26159;&#19968;&#31867;&#31163;&#25955;&#26102;&#38047;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#12290;&#19968;&#20010;MJLS&#30001;&#19968;&#32452;&#26377;&#38480;&#30340;&#38543;&#26426;&#32447;&#24615;&#21160;&#24577;&#21644;&#36825;&#20123;&#21160;&#24577;&#20043;&#38388;&#30340;&#31163;&#25955;&#36339;&#21464;&#32452;&#25104;&#65292;&#36825;&#20123;&#36339;&#21464;&#30001;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#31649;&#29702;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20010;MDP&#30340;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#26377;&#38480;&#29366;&#24577;&#25277;&#35937;&#65292;&#25429;&#25417;&#20102;MJLS&#30340;&#31163;&#25955;&#65288;&#27169;&#24335;&#36339;&#36291;&#65289;&#21644;&#36830;&#32493;&#65288;&#38543;&#26426;&#32447;&#24615;&#65289;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25277;&#35937;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21306;&#38388;MDP&#65288;iMDP&#65289;&#65292;&#28982;&#21518;&#35745;&#31639;&#20102;&#29366;&#24577;&#36716;&#31227;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated synthesis of provably correct controllers for cyber-physical systems is crucial for deployment in safety-critical scenarios. However, hybrid features and stochastic or unknown behaviours make this problem challenging. We propose a method for synthesising controllers for Markov jump linear systems (MJLSs), a class of discrete-time models for cyber-physical systems, so that they certifiably satisfy probabilistic computation tree logic (PCTL) formulae. An MJLS consists of a finite set of stochastic linear dynamics and discrete jumps between these dynamics that are governed by a Markov decision process (MDP). We consider the cases where the transition probabilities of this MDP are either known up to an interval or completely unknown. Our approach is based on a finite-state abstraction that captures both the discrete (mode-jumping) and continuous (stochastic linear) behaviour of the MJLS. We formalise this abstraction as an interval MDP (iMDP) for which we compute intervals of tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26234;&#33021;&#32593;&#26684;&#29983;&#25104;&#65288;IMG&#65289;&#29616;&#29366;&#65292;&#27010;&#25324;&#20102;113&#31181;IMG&#26041;&#27861;&#30340;&#26680;&#24515;&#25216;&#26415;&#12289;&#24212;&#29992;&#33539;&#22260;&#12289;&#20195;&#29702;&#23398;&#20064;&#30446;&#26631;&#12289;&#25968;&#25454;&#31867;&#22411;&#12289;&#30446;&#26631;&#25361;&#25112;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.06009</link><description>&lt;p&gt;
&#26234;&#33021;&#32593;&#26684;&#29983;&#25104;&#30340;&#29616;&#29366;&#65306;&#32508;&#36848;&#19982;&#21069;&#26223;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
What's the Situation with Intelligent Mesh Generation: A Survey and Perspectives. (arXiv:2211.06009v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26234;&#33021;&#32593;&#26684;&#29983;&#25104;&#65288;IMG&#65289;&#29616;&#29366;&#65292;&#27010;&#25324;&#20102;113&#31181;IMG&#26041;&#27861;&#30340;&#26680;&#24515;&#25216;&#26415;&#12289;&#24212;&#29992;&#33539;&#22260;&#12289;&#20195;&#29702;&#23398;&#20064;&#30446;&#26631;&#12289;&#25968;&#25454;&#31867;&#22411;&#12289;&#30446;&#26631;&#25361;&#25112;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#32593;&#26684;&#29983;&#25104;&#65288;IMG&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#32593;&#26684;&#30340;&#25216;&#26415;&#65292;&#26159;&#19968;&#20010;&#30456;&#23545;&#36739;&#26032;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#30701;&#26242;&#30340;&#21382;&#21490;&#20013;&#65292;IMG&#26497;&#22823;&#22320;&#25299;&#23637;&#20102;&#32593;&#26684;&#29983;&#25104;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#21462;&#24471;&#20102;&#35768;&#22810;&#31361;&#30772;&#65292;&#24182;&#20026;&#32593;&#26684;&#29983;&#25104;&#21019;&#36896;&#20102;&#28508;&#22312;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;IMG&#30340;&#32508;&#36848;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#31995;&#32479;&#32508;&#36848;&#21644;&#25551;&#36848;&#24403;&#19979;IMG&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#25628;&#38598;&#20102;113&#31181;IMG&#26041;&#27861;&#65292;&#20174;&#22810;&#20010;&#35282;&#24230;&#65288;&#21253;&#25324;&#31639;&#27861;&#30340;&#26680;&#24515;&#25216;&#26415;&#21644;&#24212;&#29992;&#33539;&#22260;&#12289;&#20195;&#29702;&#23398;&#20064;&#30446;&#26631;&#12289;&#25968;&#25454;&#31867;&#22411;&#12289;&#30446;&#26631;&#25361;&#25112;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65289;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#22522;&#20110;&#20869;&#23481;&#25552;&#21462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#27861;&#65306;&#20851;&#38190;&#25216;&#26415;&#12289;&#36755;&#20986;&#32593;&#26684;&#21333;&#20803;&#20803;&#32032;&#12289;&#25152;&#38024;&#23545;&#30340;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#25991;&#29486;&#25910;&#38598;&#21644;&#20998;&#31867;&#25972;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent mesh generation (IMG) refers to a technique for generating mesh by machine learning, which is a relatively new and promising research field. Within its short lifespan, IMG has greatly expanded the generalizability and practicality of mesh generation techniques, achieved many breakthroughs and created potential possibilities for mesh generation. However, there is a lack of surveys that focus on IMG methods in recent works. In this paper, we are committed to a systematic and comprehensive survey that describes the contemporary IMG landscape. Focusing on 113 preliminary IMG methods, we conducted an in-depth analysis from multiple perspectives, including the core technique and application scope of the algorithm, agent learning goals, data types, targeting challenges, advantages, and limitations. With the aim of literature collection and classification based on content extraction, we propose three different taxonomies from three views: key techniques, output mesh unit elements, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#20256;&#24863;&#22120;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21160;&#24577;&#12289;&#31232;&#30095;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#26368;&#22823;&#21270;&#25910;&#38598;&#26377;&#20851;&#23454;&#20307;&#20449;&#24687;&#12290;&#37319;&#29992;&#28145;&#24230;&#20808;&#34892;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#23637;&#25511;&#21046;&#31354;&#38388;&#24182;&#20351;&#29992;&#28151;&#21512;&#21367;&#31215;&#36882;&#24402;&#31070;&#32463;&#23618;&#26469;&#30417;&#27979;&#22797;&#26434;&#30340;&#21160;&#24577;&#27963;&#21160;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2211.01527</link><description>&lt;p&gt;
&#21160;&#24577;&#12289;&#31232;&#30095;&#21644;&#37096;&#20998;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24687;&#25910;&#38598;&#30340;&#20256;&#24863;&#22120;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sensor Control for Information Gain in Dynamic, Sparse and Partially Observed Environments. (arXiv:2211.01527v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#20256;&#24863;&#22120;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21160;&#24577;&#12289;&#31232;&#30095;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#26368;&#22823;&#21270;&#25910;&#38598;&#26377;&#20851;&#23454;&#20307;&#20449;&#24687;&#12290;&#37319;&#29992;&#28145;&#24230;&#20808;&#34892;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#23637;&#25511;&#21046;&#31354;&#38388;&#24182;&#20351;&#29992;&#28151;&#21512;&#21367;&#31215;&#36882;&#24402;&#31070;&#32463;&#23618;&#26469;&#30417;&#27979;&#22797;&#26434;&#30340;&#21160;&#24577;&#27963;&#21160;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#20256;&#24863;&#22120;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#12289;&#21160;&#24577;&#21644;&#31232;&#30095;&#37319;&#26679;&#30340;&#29615;&#22659;&#19979;&#26368;&#22823;&#21270;&#26377;&#20851;&#35813;&#31354;&#38388;&#20013;&#23384;&#22312;&#30340;&#23454;&#20307;&#30340;&#20449;&#24687;&#25910;&#38598;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#38024;&#23545;&#26080;&#32447;&#30005;&#39057;&#35889;&#30417;&#27979;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#25628;&#32034;&#21644;&#36319;&#36394;&#29615;&#22659;&#20013;&#30340;&#26410;&#30693;&#21160;&#24577;&#20449;&#21495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#65288;1&#65289;&#25913;&#36827;&#25506;&#32034;&#31232;&#30095;&#12289;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#26032;&#22411;&#20449;&#24687;&#25910;&#30410;&#22870;&#21169;&#21644;&#65288;2&#65289;&#25193;&#23637;&#25511;&#21046;&#31354;&#38388;&#24182;&#20351;&#29992;&#28151;&#21512;&#21367;&#31215;-&#36882;&#24402;&#31070;&#32463;&#23618;&#26469;&#30417;&#27979;&#22797;&#26434;&#21160;&#24577;&#27963;&#21160;&#27169;&#24335;&#26469;&#25193;&#23637;&#20102;&#28145;&#24230;&#20808;&#34892;&#32593;&#32476;(DAN)&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20010;&#38382;&#39064;&#25193;&#23637;&#21040;&#37319;&#26679;&#39044;&#26399;&#30340;&#26080;&#32447;&#30005;&#39057;&#35889;/&#22330;&#26377;&#38480;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#21407;&#22987;RL&#31639;&#27861;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#29256;&#26412;&#65292;&#36890;&#36807;&#20174;&#38480;&#21046;&#20013;&#19981;&#26029;&#25552;&#39640;&#30340;&#27169;&#22411;&#26469;&#24494;&#35843;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approach for autonomous sensor control for information gathering under partially observable, dynamic and sparsely sampled environments that maximizes information about entities present in that space. We describe our approach for the task of Radio-Frequency (RF) spectrum monitoring, where the goal is to search for and track unknown, dynamic signals in the environment. To this end, we extend the Deep Anticipatory Network (DAN) Reinforcement Learning (RL) framework by (1) improving exploration in sparse, non-stationary environments using a novel information gain reward, and (2) scaling up the control space and enabling the monitoring of complex, dynamic activity patterns using hybrid convolutional-recurrent neural layers. We also extend this problem to situations in which sampling from the intended RF spectrum/field is limited and propose a model-based version of the original RL algorithm that fine-tunes the controller via a model that is iteratively improved from the limite
&lt;/p&gt;</description></item><item><title>SAM-RL&#20351;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#65292;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#24863;&#30693;&#24863;&#30693;&#30340;&#23398;&#20064;&#31649;&#36947;&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290; &#29992;&#20110;&#23436;&#25104;&#26426;&#22120;&#20154;&#32452;&#35013;&#65292;&#24037;&#20855;&#25805;&#20316;&#21644;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.15185</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#28210;&#26579;&#30340;&#24863;&#30693;&#24863;&#30693;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering. (arXiv:2210.15185v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15185
&lt;/p&gt;
&lt;p&gt;
SAM-RL&#20351;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#65292;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#24863;&#30693;&#24863;&#30693;&#30340;&#23398;&#20064;&#31649;&#36947;&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290; &#29992;&#20110;&#23436;&#25104;&#26426;&#22120;&#20154;&#32452;&#35013;&#65292;&#24037;&#20855;&#25805;&#20316;&#21644;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#20855;&#26377;&#27604;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#22914;&#20309;&#20174;&#21407;&#22987;&#24863;&#23448;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#65289;&#33258;&#21160;&#26377;&#25928;&#22320;&#24320;&#21457;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21644;&#20219;&#21153;&#65292;&#26159;&#38480;&#21046;MBRL&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SAM-RL&#30340;&#24863;&#30693;&#24863;&#30693;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#12290;&#21033;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;SAM-RL&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#36890;&#36807;&#24863;&#30693;&#24863;&#30693;&#23398;&#20064;&#31649;&#36947;&#65292;SAM-RL&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#26469;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#23454;&#38469;&#30340;&#19977;&#20010;&#25805;&#20316;&#20219;&#21153;&#65306;&#26426;&#22120;&#20154;&#35013;&#37197;&#65292;&#24037;&#20855;&#25805;&#32437;&#21644;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample-efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectivene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASDOT&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20219;&#20309;&#32473;&#23450;&#25110;&#27809;&#26377;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#20854;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2210.04325</link><description>&lt;p&gt;
ASDOT&#65306;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#38646;&#26679;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models. (arXiv:2210.04325v3 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04325
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASDOT&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20219;&#20309;&#32473;&#23450;&#25110;&#27809;&#26377;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#20854;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#22312;&#36755;&#20837;&#25968;&#25454;&#30340;&#39046;&#22495;&#65288;&#22914;&#37329;&#34701; vs &#36816;&#21160;&#65289;&#25110;&#26550;&#26500;&#65288;&#20363;&#22914;&#65292;&#19981;&#21516;&#30340;&#35859;&#35789;&#65289;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#30340;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#38656;&#35201;&#36275;&#22815;&#22810;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#21040;&#28040;&#38500;&#27495;&#20041;&#21644;&#25551;&#36848;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#38382;&#39064;&#24448;&#24448;&#38754;&#20020;&#30528;&#21508;&#31181;&#19981;&#36275;&#26679;&#26412;&#30340;&#38382;&#39064;&#65306;&#21487;&#33021;&#21482;&#26377;&#26497;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#25110;&#26681;&#26412;&#27809;&#26377;&#35757;&#32451;&#26679;&#26412;&#65292;&#25110;&#38656;&#35201;&#20381;&#36182;&#20110;&#19981;&#21516;&#39046;&#22495;&#25110;&#26550;&#26500;&#30340;&#26679;&#20363;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Any-Shot Data-to-Text (ASDOT)&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20219;&#20309;&#32473;&#23450;&#65288;&#25110;&#27809;&#26377;&#65289;&#26679;&#26412;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;ASDOT&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#25968;&#25454;&#28040;&#27495;&#21644;&#21477;&#23376;&#34701;&#21512;&#65292;&#36825;&#20004;&#20010;&#27493;&#39588;&#37117;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#35299;&#20915;&#12290;&#22312;&#25968;&#25454;&#28040;&#27495;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#31034;&#24335;GPT-3&#27169;&#22411;&#26469;&#29702;&#35299;&#36755;&#20837;&#25968;&#25454;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#27169;&#31946;&#19977;&#20803;&#32452;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#21487;&#29992;&#26679;&#26412;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#20197;&#29983;&#25104;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-to-text generation is challenging due to the great variety of the input data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse predicates). Recent end-to-end neural methods thus require substantial training examples to learn to disambiguate and describe the data. Yet, real-world data-to-text problems often suffer from various data-scarce issues: one may have access to only a handful of or no training examples, and/or have to rely on examples in a different domain or schema. To fill this gap, we propose Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse settings by making efficient use of any given (or no) examples. ASDOT consists of two steps, data disambiguation and sentence fusion, both of which are amenable to be solved with off-the-shelf pretrained language models (LMs) with optional finetuning. In the data disambiguation stage, we employ the prompted GPT-3 model to understand possibly ambiguous triples from the input data and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.01969</link><description>&lt;p&gt;
&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2210.01969v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#19968;&#33324;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#24674;&#22797;&#19987;&#23478;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#24230;&#22797;&#26434;&#30340;&#12289;&#38271;&#26102;&#31243;&#20219;&#21153;&#65292;&#24674;&#22797;&#21333;&#19968;&#25972;&#20307;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#65292;&#32780;&#19987;&#23478;&#31574;&#30053;&#36890;&#24120;&#21253;&#21547;&#23376;&#20219;&#21153;&#23618;&#27425;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#65288;HIL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36873;&#39033;&#26694;&#26550;&#20013;&#26174;&#24335;&#22320;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#27963;&#21160;&#32467;&#26500;&#26469;&#23398;&#20064;&#20998;&#23618;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;HIL&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#23376;&#20219;&#21153;&#32467;&#26500;&#19982;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#35201;&#20040;&#26080;&#27861;&#21516;&#26102;&#22312;&#20998;&#23618;&#26694;&#26550;&#20013;&#23398;&#20064;&#39640;&#32423;&#21035;&#21644;&#20302;&#32423;&#21035;&#31574;&#30053;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HIL&#31639;&#27861;&#8212;&#8212;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;H-AIRL&#65289;&#65292;&#23427;&#22312;&#26368;&#26032;&#30340;IL&#31639;&#27861;AIRL&#19978;&#25193;&#23637;&#20102;&#19968;&#27493;&#36873;&#39033;&#26694;&#26550;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;AIRL&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) has been proposed to recover the expert policy from demonstrations. However, it would be difficult to learn a single monolithic policy for highly-complex long-horizon tasks of which the expert policy usually contains subtask hierarchies. Therefore, Hierarchical Imitation Learning (HIL) has been developed to learn a hierarchical policy from expert demonstrations through explicitly modelling the activity structure in a task with the option framework. Existing HIL methods either overlook the causal relationship between the subtask structure and the learned policy, or fail to learn the high-level and low-level policy in the hierarchical framework in conjuncture, which leads to suboptimality. In this work, we propose a novel HIL algorithm -Hierarchical Adversarial Inverse Reinforcement Learning (H-AIRL), which extends a state-of-the-art (SOTA) IL algorithm -- AIRL, with the one-step option framework. Specifically, we redefine the AIRL objectives on the extended sta
&lt;/p&gt;</description></item><item><title>L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.14402</link><description>&lt;p&gt;
L2XGNN&#65306;&#23398;&#20064;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
L2XGNN: Learning to Explain Graph Neural Networks. (arXiv:2209.14402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14402
&lt;/p&gt;
&lt;p&gt;
L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#23398;&#20064;&#35299;&#37322;&#65288;L2X&#65289;&#33539;&#24335;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2XGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#20379;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;L2XGNN&#23398;&#20064;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#65292;&#36825;&#20123;&#23376;&#22270;&#20165;&#29992;&#20110;GNN&#30340;&#20449;&#24687;&#20256;&#36882;&#25805;&#20316;&#20013;&#12290;&#23545;&#27169;&#20307;&#26045;&#21152;&#36825;&#26679;&#30340;&#38480;&#21046;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#26131;&#35299;&#37322;&#21644;&#26356;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#22312;&#20960;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;L2XGNN&#23454;&#29616;&#20102;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#65292;&#21516;&#26102;&#30830;&#20445;&#20165;&#20351;&#29992;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;L2XGNN&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a framework for explainable GNNs which provides faithful explanations by design. L2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2XGNN is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2XGNN achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2XGNN is able to identify motifs responsible for the graph's properties it is intended to predict.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#35780;&#36848;&#20102;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#30340;&#21487;&#35299;&#37322;&#25216;&#26415;&#65292;&#30528;&#37325;&#20851;&#27880;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24182;&#26681;&#25454;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;GNN&#35299;&#37322;&#30340;&#24120;&#35265;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#25351;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2207.12599</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#36848;&#65306;&#20998;&#31867;&#21644;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics. (arXiv:2207.12599v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#35780;&#36848;&#20102;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#30340;&#21487;&#35299;&#37322;&#25216;&#26415;&#65292;&#30528;&#37325;&#20851;&#27880;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24182;&#26681;&#25454;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;GNN&#35299;&#37322;&#30340;&#24120;&#35265;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#25351;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22270;&#25968;&#25454;&#30340;&#39044;&#27979;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#25152;&#20570;&#30340;&#39044;&#27979;&#24448;&#24448;&#38590;&#20197;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#24037;&#20316;&#37117;&#33268;&#21147;&#20110;&#20174;GNNExplainer&#12289;XGNN&#21644;PGExplainer&#31561;&#26041;&#38754;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#26426;&#21046;&#12290;&#34429;&#28982;&#36825;&#20123;&#24037;&#20316;&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#35299;&#37322;GNN&#30340;&#26694;&#26550;&#65292;&#20294;&#23545;&#20110;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#35780;&#36848;&#23578;&#19981;&#21487;&#29992;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20026;GNN&#24320;&#21457;&#30340;&#21487;&#35299;&#37322;&#25216;&#26415;&#30340;&#20840;&#38754;&#35780;&#36848;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26681;&#25454;&#20351;&#29992;&#21487;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;GNN&#35299;&#37322;&#30340;&#24120;&#35265;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#25351;&#20986;&#20960;&#20010;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have demonstrated a significant boost in prediction performance on graph data. At the same time, the predictions made by these models are often hard to interpret. In that regard, many efforts have been made to explain the prediction mechanisms of these models from perspectives such as GNNExplainer, XGNN and PGExplainer. Although such works present systematic frameworks to interpret GNNs, a holistic review for explainable GNNs is unavailable. In this survey, we present a comprehensive review of explainability techniques developed for GNNs. We focus on explainable graph neural networks and categorize them based on the use of explainable methods. We further provide the common performance metrics for GNNs explanations and point out several future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#23558;&#32473;&#23450;&#30340;&#19968;&#32452;&#28216;&#25103;&#28151;&#21512;&#21040;&#25152;&#38656;&#32452;&#21512;&#20013;&#20197;&#29983;&#25104;&#21487;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25511;&#21046;&#27599;&#20010;&#28216;&#25103;&#22312;&#28151;&#21512;&#28216;&#25103;&#20013;&#30340;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2206.14203</link><description>&lt;p&gt;
&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Latent Combinational Game Design. (arXiv:2206.14203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#23558;&#32473;&#23450;&#30340;&#19968;&#32452;&#28216;&#25103;&#28151;&#21512;&#21040;&#25152;&#38656;&#32452;&#21512;&#20013;&#20197;&#29983;&#25104;&#21487;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25511;&#21046;&#27599;&#20010;&#28216;&#25103;&#22312;&#28151;&#21512;&#28216;&#25103;&#20013;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;&#8221;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#23558;&#32473;&#23450;&#30340;&#19968;&#32452;&#28216;&#25103;&#28151;&#21512;&#21040;&#25152;&#38656;&#32452;&#21512;&#20013;&#20197;&#29983;&#25104;&#21487;&#29609;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; (GMVAEs) &#23545; VAE &#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#30417;&#30563;&#24335;&#35757;&#32451;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#19968;&#20010;&#28216;&#25103;&#30340;&#27700;&#24179;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#28151;&#21512;&#28216;&#25103;&#23450;&#20041;&#20026;&#36825;&#20123;&#32452;&#20214;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#36825;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#26032;&#28216;&#25103;&#65292;&#24182;&#25511;&#21046;&#28151;&#21512;&#20013;&#27599;&#20010;&#28216;&#25103;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26377;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25193;&#23637;&#20197;&#21069;&#30340;&#28151;&#21512;&#24037;&#20316;&#65292;&#24182;&#19982; GMVAE &#36827;&#34892;&#27604;&#36739;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#28151;&#21512;&#26465;&#20214; GMVAE (CGMVAE) &#32467;&#26500;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#25972;&#20010;&#28151;&#21512;&#27700;&#24179;&#21644;&#24067;&#23616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#36848;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#25353;&#25351;&#23450;&#32452;&#21512;&#28151;&#21512;&#30340;&#21487;&#29609;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;&#24179;&#21488;&#28216;&#25103;&#21644;&#22320;&#19979;&#22478;&#31867;&#28216;&#25103;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present latent combinational game design -- an approach for generating playable games that blend a given set of games in a desired combination using deep generative latent variable models. We use Gaussian Mixture Variational Autoencoders (GMVAEs) which model the VAE latent space via a mixture of Gaussian components. Through supervised training, each component encodes levels from one game and lets us define blended games as linear combinations of these components. This enables generating new games that blend the input games and controlling the relative proportions of each game in the blend. We also extend prior blending work using conditional VAEs and compare against the GMVAE and additionally introduce a hybrid conditional GMVAE (CGMVAE) architecture which lets us generate whole blended levels and layouts. Results show that the above approaches can generate playable games that blend the input games in specified combinations. We use both platformers and dungeon-based games to demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdvZL&#30340;&#26032;&#22411;&#29289;&#29702;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#65292;&#21033;&#29992;&#25932;&#23545;&#21464;&#28966;&#38236;&#22836;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#22270;&#20687;&#36827;&#34892;&#25918;&#22823;&#21644;&#32553;&#23567;&#65292;&#20174;&#32780;&#27450;&#39575;DNNs&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#30446;&#26631;&#23545;&#35937;&#30340;&#29305;&#24449;&#12290;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#26159;&#21807;&#19968;&#19968;&#31181;&#19981;&#28155;&#21152;&#29289;&#29702;&#25932;&#23545;&#25200;&#21160;&#25915;&#20987;DNNs&#30340;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2206.12251</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26032;&#22411;&#29289;&#29702;&#25915;&#20987;&#65306;&#25932;&#23545;&#21464;&#28966;&#38236;&#22836;
&lt;/p&gt;
&lt;p&gt;
Adversarial Zoom Lens: A Novel Physical-World Attack to DNNs. (arXiv:2206.12251v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdvZL&#30340;&#26032;&#22411;&#29289;&#29702;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#65292;&#21033;&#29992;&#25932;&#23545;&#21464;&#28966;&#38236;&#22836;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#22270;&#20687;&#36827;&#34892;&#25918;&#22823;&#21644;&#32553;&#23567;&#65292;&#20174;&#32780;&#27450;&#39575;DNNs&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#30446;&#26631;&#23545;&#35937;&#30340;&#29305;&#24449;&#12290;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#26159;&#21807;&#19968;&#19968;&#31181;&#19981;&#28155;&#21152;&#29289;&#29702;&#25932;&#23545;&#25200;&#21160;&#25915;&#20987;DNNs&#30340;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#20204;&#30693;&#36947;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24456;&#33030;&#24369;&#65292;&#20294;&#36824;&#27809;&#26377;&#20154;&#30740;&#31350;&#36807;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#23545;&#22270;&#20687;&#36827;&#34892;&#25918;&#22823;&#25110;&#32553;&#23567;&#23545;DNNs&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Adversarial Zoom Lens&#65288;AdvZL&#65289;&#30340;&#26032;&#22411;&#29289;&#29702;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20351;&#29992;&#25932;&#23545;&#21464;&#28966;&#38236;&#22836;&#23545;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#22270;&#20687;&#36827;&#34892;&#25918;&#22823;&#21644;&#32553;&#23567;&#65292;&#20174;&#32780;&#27450;&#39575;DNNs&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#30446;&#26631;&#23545;&#35937;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#26159;&#36804;&#20170;&#20026;&#27490;&#21807;&#19968;&#19968;&#31181;&#19981;&#28155;&#21152;&#29289;&#29702;&#25932;&#23545;&#25200;&#21160;&#25915;&#20987;DNNs&#30340;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#12290;&#22312;&#25968;&#23383;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;AdvZL&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#39564;&#35777;&#31561;&#27604;&#20363;&#25918;&#22823;&#22270;&#20687;&#23545;DNNs&#30340;&#25932;&#23545;&#24615;&#12290;&#22312;&#29289;&#29702;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#29992;&#21464;&#28966;&#38236;&#22836;&#23545;&#30446;&#26631;&#23545;&#35937;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#29983;&#25104;&#25932;&#23545;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;AdvZL&#22312;&#25968;&#23383;&#29615;&#22659;&#21644;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#30340;&#25932;&#23545;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep neural networks (DNNs) are known to be fragile, no one has studied the effects of zooming-in and zooming-out of images in the physical world on DNNs performance. In this paper, we demonstrate a novel physical adversarial attack technique called Adversarial Zoom Lens (AdvZL), which uses a zoom lens to zoom in and out of pictures of the physical world, fooling DNNs without changing the characteristics of the target object. The proposed method is so far the only adversarial attack technique that does not add physical adversarial perturbation attack DNNs. In a digital environment, we construct a data set based on AdvZL to verify the antagonism of equal-scale enlarged images to DNNs. In the physical environment, we manipulate the zoom lens to zoom in and out of the target object, and generate adversarial samples. The experimental results demonstrate the effectiveness of AdvZL in both digital and physical environments. We further analyze the antagonism of the proposed data set 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#25239;&#24615;&#28608;&#20809;&#28857;&#65288;AdvLS&#65289;&#30340;&#20809;&#23398;&#29289;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#20248;&#21270;&#28608;&#20809;&#28857;&#30340;&#29289;&#29702;&#21442;&#25968;&#36827;&#34892;&#29289;&#29702;&#25915;&#20987;&#65292;&#23454;&#29616;&#20102;&#23545;DNNs&#30340;&#20581;&#22766;&#21644;&#38544;&#34109;&#30340;&#29289;&#29702;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2206.01034</link><description>&lt;p&gt;
&#23545;DNNs&#30340;&#20581;&#22766;&#21644;&#38544;&#34109;&#30340;&#29289;&#29702;&#25915;&#20987;: &#23545;&#25239;&#24615;&#28608;&#20809;&#28857;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs. (arXiv:2206.01034v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01034
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#25239;&#24615;&#28608;&#20809;&#28857;&#65288;AdvLS&#65289;&#30340;&#20809;&#23398;&#29289;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#20248;&#21270;&#28608;&#20809;&#28857;&#30340;&#29289;&#29702;&#21442;&#25968;&#36827;&#34892;&#29289;&#29702;&#25915;&#20987;&#65292;&#23454;&#29616;&#20102;&#23545;DNNs&#30340;&#20581;&#22766;&#21644;&#38544;&#34109;&#30340;&#29289;&#29702;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37117;&#23481;&#26131;&#21463;&#21040;&#24494;&#23567;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20351;&#29992;&#29031;&#26126;&#35774;&#22791;&#36827;&#34892;&#29289;&#29702;&#25915;&#20987;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22522;&#20110;&#20809;&#30340;&#29289;&#29702;&#25915;&#20987;&#20855;&#26377;&#20986;&#33394;&#30340;&#38544;&#34109;&#24615;&#65292;&#36825;&#32473;&#35768;&#22810;&#22522;&#20110;&#35270;&#35273;&#30340;&#24212;&#29992;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#65289;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#25239;&#24615;&#28608;&#20809;&#28857;&#65288;AdvLS&#65289;&#30340;&#20809;&#23398;&#29289;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#20248;&#21270;&#28608;&#20809;&#28857;&#30340;&#29289;&#29702;&#21442;&#25968;&#36827;&#34892;&#29289;&#29702;&#25915;&#20987;&#12290;&#23427;&#21033;&#29992;&#20302;&#25104;&#26412;&#30340;&#28608;&#20809;&#35774;&#22791;&#23454;&#29616;&#20102;&#24378;&#20581;&#21644;&#38544;&#34109;&#30340;&#29289;&#29702;&#25915;&#20987;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;AdvLS&#26159;&#31532;&#19968;&#20010;&#22312;&#30333;&#22825;&#36827;&#34892;&#29289;&#29702;&#25915;&#20987;&#30340;&#22522;&#20110;&#20809;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;AdvLS&#20855;&#26377;&#20986;&#33394;&#30340;&#20581;&#22766;&#24615;&#21644;&#38544;&#34109;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#23454;&#39564;&#25968;&#25454;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;AdvLS&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#20855;&#26377;&#20248;&#31168;&#30340;&#23545;&#25239;&#25915;&#20987;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing deep neural networks (DNNs) are easily disturbed by slight noise. However, there are few researches on physical attacks by deploying lighting equipment. The light-based physical attacks has excellent covertness, which brings great security risks to many vision-based applications (such as self-driving). Therefore, we propose a light-based physical attack, called adversarial laser spot (AdvLS), which optimizes the physical parameters of laser spots through genetic algorithm to perform physical attacks. It realizes robust and covert physical attack by using low-cost laser equipment. As far as we know, AdvLS is the first light-based physical attack that perform physical attacks in the daytime. A large number of experiments in the digital and physical environments show that AdvLS has excellent robustness and covertness. In addition, through in-depth analysis of the experimental data, we find that the adversarial perturbations generated by AdvLS have superior adversarial attack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;SVM&#30340;&#25351;&#25968;&#32423;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#27809;&#26377;&#20551;&#35774;&#30828;Tsybakov&#36793;&#38469;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;SVM&#30340;&#25351;&#25968;&#32423;&#25910;&#25947;&#36895;&#24230;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2205.10055</link><description>&lt;p&gt;
SVM&#25351;&#25968;&#32423;&#25910;&#25947;&#36895;&#24230;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Case of Exponential Convergence Rates for SVM. (arXiv:2205.10055v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;SVM&#30340;&#25351;&#25968;&#32423;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#27809;&#26377;&#20551;&#35774;&#30828;Tsybakov&#36793;&#38469;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;SVM&#30340;&#25351;&#25968;&#32423;&#25910;&#25947;&#36895;&#24230;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#36890;&#24120;&#26159;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#35838;&#31243;&#20013;&#25551;&#36848;&#30340;&#31532;&#19968;&#20010;&#38382;&#39064;&#12290;&#21382;&#21490;&#19978;&#65292;&#29926;&#26222;&#23612;&#20811;-&#20999;&#23572;&#27779;&#24180;&#31185;&#29702;&#35770;&#25552;&#20379;&#20102;&#20998;&#31867;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20445;&#35777;&#22522;&#20110;&#38590;&#20197;&#22788;&#29702;&#30340;&#31639;&#27861;&#65292;&#36825;&#23548;&#33268;&#20102;&#20998;&#31867;&#20013;&#20195;&#29702;&#26041;&#27861;&#30340;&#29702;&#35770;&#12290;&#20195;&#29702;&#26041;&#27861;&#25552;&#20379;&#30340;&#20445;&#35777;&#22522;&#20110;&#26657;&#20934;&#19981;&#31561;&#24335;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#26576;&#20123;&#36793;&#38469;&#26465;&#20214;&#19979;&#38750;&#24120;&#27425;&#20248;&#65292;&#19981;&#33021;&#25429;&#25417;&#21040;&#25351;&#25968;&#32423;&#25910;&#25947;&#29616;&#35937;&#12290;&#36825;&#20123;"&#36229;"&#24555;&#36895;&#29575;&#29616;&#22312;&#24050;&#32463;&#23545;&#20110;&#20809;&#28369;&#30340;&#20195;&#29702;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#23545;&#20110;&#19982;&#33879;&#21517;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#30456;&#20851;&#30340;&#38750;&#20809;&#28369;&#25439;&#22833;&#65288;&#22914;&#38128;&#38142;&#25439;&#22833;&#65289;&#65292;&#30011;&#38754;&#20173;&#28982;&#27169;&#31946;&#19981;&#28165;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26426;&#21046;&#26469;&#33719;&#24471;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#30740;&#31350;&#20854;&#29992;&#20110;SVM&#30340;&#24773;&#20917;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SVM&#21487;&#20197;&#23637;&#29616;&#20986;&#25351;&#25968;&#32423;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21363;&#20351;&#27809;&#26377;&#20551;&#35774;&#30828;Tsybakov&#36793;&#38469;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification is often the first problem described in introductory machine learning classes. Generalization guarantees of classification have historically been offered by Vapnik-Chervonenkis theory. Yet those guarantees are based on intractable algorithms, which has led to the theory of surrogate methods in classification. Guarantees offered by surrogate methods are based on calibration inequalities, which have been shown to be highly sub-optimal under some margin conditions, failing short to capture exponential convergence phenomena. Those "super" fast rates are becoming to be well understood for smooth surrogates, but the picture remains blurry for non-smooth losses such as the hinge loss, associated with the renowned support vector machines. In this paper, we present a simple mechanism to obtain fast convergence rates and we investigate its usage for SVM. In particular, we show that SVM can exhibit exponential convergence rates even without assuming the hard Tsybakov margin conditi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#24555;&#36895;&#20960;&#20309;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;PFGE&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36830;&#32493;&#30340;&#38543;&#26426;&#26435;&#37325;&#24179;&#22343;&#36807;&#31243;&#29983;&#25104;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#39640;&#24615;&#33021;DNN&#38598;&#21512;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#20869;&#23384;&#25928;&#29575;&#25552;&#39640;&#20102;5&#20493;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2202.06658</link><description>&lt;p&gt;
PFGE: &#31616;&#27905;&#24555;&#36895;&#20960;&#20309;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PFGE: Parsimonious Fast Geometric Ensembling of DNNs. (arXiv:2202.06658v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#24555;&#36895;&#20960;&#20309;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;PFGE&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36830;&#32493;&#30340;&#38543;&#26426;&#26435;&#37325;&#24179;&#22343;&#36807;&#31243;&#29983;&#25104;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#39640;&#24615;&#33021;DNN&#38598;&#21512;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#20869;&#23384;&#25928;&#29575;&#25552;&#39640;&#20102;5&#20493;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#38598;&#25104;&#38656;&#35201;&#39640;&#35745;&#31639;&#24320;&#38144;&#65292;&#22240;&#27492;&#38598;&#25104;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#22914;&#24555;&#36895;&#20960;&#20309;&#38598;&#25104;&#65288;FGE&#65289;&#21644;&#24555;&#29031;&#38598;&#25104;&#65292;&#36890;&#36807;&#22312;&#19982;&#21333;&#20010;&#27169;&#22411;&#30456;&#21516;&#30340;&#26102;&#38388;&#20869;&#35757;&#32451;&#27169;&#22411;&#38598;&#25104;&#26469;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19982;&#21333;&#19968;&#27169;&#22411;&#30340;&#22522;&#20110;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20123;&#25216;&#26415;&#20173;&#38656;&#35201;&#39069;&#22806;&#30340;&#20869;&#23384;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#25512;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31616;&#27905;FGE&#65288;PFGE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#30001;&#36830;&#32493;&#30340;&#38543;&#26426;&#26435;&#37325;&#24179;&#22343;&#36807;&#31243;&#29983;&#25104;&#30340;&#39640;&#24615;&#33021;DNN&#30340;&#36731;&#37327;&#32423;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29616;&#20195;DNN&#26550;&#26500;&#30340;CIFAR-{10,100}&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PFGE&#23454;&#29616;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#27604;5&#20493;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods are commonly used to enhance the generalization performance of machine learning models. However, they present a challenge in deep learning systems due to the high computational overhead required to train an ensemble of deep neural networks (DNNs). Recent advancements such as fast geometric ensembling (FGE) and snapshot ensembles have addressed this issue by training model ensembles in the same time as a single model. Nonetheless, these techniques still require additional memory for test-time inference compared to single-model-based methods. In this paper, we propose a new method called parsimonious FGE (PFGE), which employs a lightweight ensemble of higher-performing DNNs generated through successive stochastic weight averaging procedures. Our experimental results on CIFAR-{10,100} and ImageNet datasets across various modern DNN architectures demonstrate that PFGE achieves 5x memory efficiency compared to previous methods, without compromising on generalization perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#37096;&#29366;&#24577;&#30340;&#26102;&#38388;&#24179;&#28369;&#24615;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21644;&#34920;&#31034;&#65292;&#21457;&#29616;&#20351;&#29992;&#26102;&#38388;&#24179;&#28369;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#8220;&#24930;&#8221;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#27604;&#21069;&#39304;&#32593;&#32476;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#31867;&#65292;&#21516;&#26102;&#20855;&#26377;&#32447;&#24615;&#24490;&#29615;&#21644;&#22810;&#26102;&#38388;&#23610;&#24230;&#38376;&#25511;&#26426;&#21046;&#30340;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#31034;&#36755;&#20837;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#20855;&#26377;&#26356;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2012.06694</link><description>&lt;p&gt;
&#24930;&#31070;&#32463;&#21160;&#21147;&#23398;&#23545;&#22686;&#37327;&#23398;&#20064;&#30340;&#21518;&#26524;
&lt;/p&gt;
&lt;p&gt;
Consequences of Slow Neural Dynamics for Incremental Learning. (arXiv:2012.06694v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.06694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#37096;&#29366;&#24577;&#30340;&#26102;&#38388;&#24179;&#28369;&#24615;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21644;&#34920;&#31034;&#65292;&#21457;&#29616;&#20351;&#29992;&#26102;&#38388;&#24179;&#28369;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#8220;&#24930;&#8221;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#27604;&#21069;&#39304;&#32593;&#32476;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#31867;&#65292;&#21516;&#26102;&#20855;&#26377;&#32447;&#24615;&#24490;&#29615;&#21644;&#22810;&#26102;&#38388;&#23610;&#24230;&#38376;&#25511;&#26426;&#21046;&#30340;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#31034;&#36755;&#20837;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#20855;&#26377;&#26356;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#33041;&#20013;&#65292;&#20869;&#37096;&#29366;&#24577;&#36890;&#24120;&#20250;&#38543;&#26102;&#38388;&#30456;&#20114;&#20851;&#32852;&#65288;&#30001;&#20110;&#23616;&#37096;&#24490;&#29615;&#21644;&#20854;&#20182;&#20869;&#22312;&#30005;&#36335;&#29305;&#24615;&#65289;&#65292;&#24182;&#30001;&#31361;&#28982;&#36716;&#25442;&#26029;&#26029;&#32493;&#32493;&#22320;&#21576;&#29616;&#12290;&#20045;&#19968;&#30475;&#65292;&#20869;&#37096;&#29366;&#24577;&#30340;&#26102;&#38388;&#24179;&#28369;&#24615;&#20250;&#23545;&#23398;&#20064;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#65288;&#20363;&#22914;&#22270;&#20687;&#30340;&#31867;&#21035;&#26631;&#31614;&#65289;&#20135;&#29983;&#38382;&#39064;&#65292;&#22240;&#20026;&#36755;&#20837;&#30340;&#20869;&#37096;&#34920;&#31034;&#23558;&#21253;&#21547;&#24403;&#21069;&#36755;&#20837;&#21644;&#20808;&#21069;&#36755;&#20837;&#30340;&#28151;&#21512;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#33258;&#28982;&#25968;&#25454;&#65288;&#20363;&#22914;&#30005;&#24433;&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#36755;&#20837;&#20063;&#23384;&#22312;&#26102;&#38388;&#33258;&#30456;&#20851;&#24615;&#12290;&#24403;&#35757;&#32451;&#25968;&#25454;&#20063;&#26159;&#26102;&#38388;&#24179;&#28369;&#30340;&#26102;&#65292;&#20869;&#37096;&#29366;&#24577;&#30340;&#26102;&#38388;&#24179;&#28369;&#24615;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#30340;&#25928;&#29575;&#65311;&#23427;&#22914;&#20309;&#24433;&#21709;&#25152;&#23398;&#30340;&#34920;&#31034;&#31867;&#22411;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#20351;&#29992;&#26102;&#38388;&#24179;&#28369;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#8220;&#24930;&#8221;&#31070;&#32463;&#32593;&#32476;&#65288;&#37197;&#22791;&#32447;&#24615;&#24490;&#29615;&#21644;&#38376;&#25511;&#26426;&#21046;&#65289;&#30340;&#32593;&#32476;&#27604;&#21069;&#39304;&#32593;&#32476;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#32447;&#24615;&#24490;&#29615;&#21644;&#22810;&#26102;&#38388;&#23610;&#24230;&#38376;&#25511;&#26426;&#21046;&#30340;&#32593;&#32476;&#23398;&#20250;&#20102;&#34920;&#31034;&#36755;&#20837;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the human brain, internal states are often correlated over time (due to local recurrence and other intrinsic circuit properties), punctuated by abrupt transitions. At first glance, temporal smoothness of internal states presents a problem for learning input-output mappings (e.g. category labels for images), because the internal representation of the input will contain a mixture of current input and prior inputs. However, when training with naturalistic data (e.g. movies) there is also temporal autocorrelation in the input. How does the temporal "smoothness" of internal states affect the efficiency of learning when the training data are also temporally smooth? How does it affect the kinds of representations that are learned? We found that, when trained with temporally smooth data, "slow" neural networks (equipped with linear recurrence and gating mechanisms) learned to categorize more efficiently than feedforward networks. Furthermore, networks with linear recurrence and multi-timesc
&lt;/p&gt;</description></item></channel></rss>