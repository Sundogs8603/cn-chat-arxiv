<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.02877</link><description>&lt;p&gt;
FlightScope: &#21355;&#26143;&#22270;&#20687;&#20013;&#39134;&#34892;&#22120;&#26816;&#27979;&#31639;&#27861;&#30340;&#28145;&#24230;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#36965;&#24863;&#21355;&#26143;&#22270;&#20687;&#20013;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#29289;&#29702;&#23398;&#21644;&#29615;&#22659;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#22312;&#24120;&#35265;&#30340;&#22522;&#20110;&#22320;&#38754;&#25293;&#25668;&#30340;&#29031;&#29255;&#19978;&#23454;&#26045;&#21644;&#27979;&#35797;&#12290;&#26412;&#25991;&#23545;&#19968;&#22871;&#38024;&#23545;&#22312;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#36825;&#19968;&#20219;&#21153;&#23450;&#21046;&#30340;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#21033;&#29992;&#22823;&#22411;HRPlanesV2&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19982;GDIT&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;&#35813;&#30740;&#31350;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;YOLO&#29256;&#26412;5&#21644;8&#12289;Faster RCNN&#12289;CenterNet&#12289;RetinaNet&#12289;RTMDet&#21644;DETR&#65292;&#22343;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#12290;&#36825;&#39033;&#20840;&#38754;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;YOLOv5&#20316;&#20026;&#35782;&#21035;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#39134;&#26426;&#36825;&#19968;&#29305;&#23450;&#26696;&#20363;&#30340;&#21331;&#36234;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#30340;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 Announce Type: cross  Abstract: Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#30334;&#24230;&#25628;&#32034;&#24341;&#25806;&#21457;&#24067;&#30340;&#22823;&#35268;&#27169;&#25628;&#32034;&#25968;&#25454;&#38598;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#22312;&#23454;&#38469;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#19982;&#25490;&#21517;&#25439;&#22833;&#21644;&#26597;&#35810;-&#25991;&#26723;&#29305;&#24449;&#36873;&#25321;&#30456;&#27604;&#65292;ULTR&#25216;&#26415;&#24182;&#26410;&#24102;&#26469;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.02543</link><description>&lt;p&gt;
&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#36935;&#21040;&#29616;&#23454;&#65306;&#30334;&#24230;&#22823;&#35268;&#27169;&#25628;&#32034;&#25968;&#25454;&#38598;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Unbiased Learning to Rank Meets Reality: Lessons from Baidu's Large-Scale Search Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#30334;&#24230;&#25628;&#32034;&#24341;&#25806;&#21457;&#24067;&#30340;&#22823;&#35268;&#27169;&#25628;&#32034;&#25968;&#25454;&#38598;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#22312;&#23454;&#38469;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#19982;&#25490;&#21517;&#25439;&#22833;&#21644;&#26597;&#35810;-&#25991;&#26723;&#29305;&#24449;&#36873;&#25321;&#30456;&#27604;&#65292;ULTR&#25216;&#26415;&#24182;&#26410;&#24102;&#26469;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#65288;ULTR&#65289;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#28857;&#20987;&#25968;&#25454;&#30340;&#25104;&#29087;&#26694;&#26550;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#21463;&#25910;&#38598;&#25968;&#25454;&#30340;&#25490;&#21517;&#32773;&#30340;&#20559;&#35265;&#24433;&#21709;&#12290;&#34429;&#28982;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#35777;&#26126;&#24182;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#65292;&#20294;ULTR&#25216;&#26415;&#32570;&#20047;&#32463;&#39564;&#39564;&#35777;&#65292;&#23588;&#20854;&#26159;&#22312;&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#20013;&#12290;&#30334;&#24230;&#25628;&#32034;&#24341;&#25806;&#21457;&#24067;&#30340;WSDM Cup 2023&#25968;&#25454;&#38598;&#20026;&#35780;&#20272;&#20027;&#35201;ULTR&#25216;&#26415;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#25552;&#20379;&#20102;&#38590;&#24471;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22312;WSDM Cup 2023&#26399;&#38388;&#26377;&#22810;&#27425;&#25552;&#20132;&#65292;&#20197;&#21450;&#38543;&#21518;&#30340;NTCIR ULTRE-2&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#35266;&#23519;&#21040;&#30340;&#25913;&#36827;&#26159;&#21542;&#28304;&#33258;&#24212;&#29992;ULTR&#25110;&#20854;&#20182;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24182;&#25193;&#23637;&#20102;&#29616;&#26377;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#25552;&#21319;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#19982;&#25490;&#21517;&#25439;&#22833;&#21644;&#26597;&#35810;-&#25991;&#26723;&#29305;&#24449;&#36873;&#25321;&#24102;&#26469;&#30340;&#26126;&#26174;&#24046;&#24322;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02543v1 Announce Type: cross  Abstract: Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data. While theoretically justified and extensively tested in simulation, ULTR techniques lack empirical validation, especially on modern search engines. The dataset released for the WSDM Cup 2023, collected from Baidu's search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques. We revisit and extend the available experiments. We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features. Our experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#36890;&#36807;&#22312;&#26799;&#24230;&#26356;&#26032;&#20013;&#24212;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25200;&#21160;&#65292;&#20445;&#25252;&#26377;&#29992;&#21333;&#20803;&#20197;&#38450;&#36951;&#24536;&#65292;&#21516;&#26102;&#24674;&#22797;&#19981;&#22826;&#26377;&#29992;&#21333;&#20803;&#30340;&#21487;&#22609;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00781</link><description>&lt;p&gt;
&#22788;&#29702;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#36890;&#36807;&#22312;&#26799;&#24230;&#26356;&#26032;&#20013;&#24212;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25200;&#21160;&#65292;&#20445;&#25252;&#26377;&#29992;&#21333;&#20803;&#20197;&#38450;&#36951;&#24536;&#65292;&#21516;&#26102;&#24674;&#22797;&#19981;&#22826;&#26377;&#29992;&#21333;&#20803;&#30340;&#21487;&#22609;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#26082;&#36973;&#21463;&#26377;&#29992;&#21333;&#20803;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21448;&#22240;&#20725;&#21270;&#21644;&#26080;&#29992;&#21333;&#20803;&#23548;&#33268;&#21487;&#22609;&#24615;&#20002;&#22833;&#12290;&#34429;&#28982;&#35768;&#22810;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#33021;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#25345;&#32493;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;UPGD&#32467;&#21512;&#20102;&#26799;&#24230;&#26356;&#26032;&#21644;&#25200;&#21160;&#65292;&#23427;&#23545;&#26356;&#26377;&#29992;&#30340;&#21333;&#20803;&#24212;&#29992;&#36739;&#23567;&#30340;&#20462;&#25913;&#65292;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#36951;&#24536;&#65292;&#23545;&#19981;&#22826;&#26377;&#29992;&#30340;&#21333;&#20803;&#24212;&#29992;&#36739;&#22823;&#30340;&#20462;&#25913;&#65292;&#24674;&#22797;&#23427;&#20204;&#30340;&#21487;&#22609;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00781v1 Announce Type: cross  Abstract: Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00247</link><description>&lt;p&gt;
&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20419;&#36827;&#36807;&#31243;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#36807;&#31243;&#24037;&#19994;&#39046;&#22495;&#24212;&#29992;DRL&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#24341;&#20837;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#24314;&#35758;&#21644;&#23637;&#26395;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36801;&#31227;&#23398;&#20064;&#19982;DRL&#32467;&#21512;&#36215;&#26469;&#21152;&#24378;&#36807;&#31243;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00247v1 Announce Type: cross  Abstract: This paper provides insights into deep reinforcement learning (DRL) for process control from the perspective of transfer learning. We analyze the challenges of applying DRL in the field of process industries and the necessity of introducing transfer learning. Furthermore, recommendations and prospects are provided for future research directions on how transfer learning can be integrated with DRL to empower process control.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00231</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24418;&#29366;&#21464;&#24418;&#32593;&#32476;&#29992;&#20110;&#26080;&#20266;&#24433;&#20960;&#20309;&#37325;&#26500;&#39592;&#30406;&#33136;&#26894;MR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00231
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33136;&#26894;&#26894;&#38388;&#30424;&#36864;&#21464;&#65292;&#26159;&#33136;&#26894;&#38388;&#30424;&#28176;&#36827;&#24615;&#32467;&#26500;&#24615;&#30952;&#25439;&#65292;&#34987;&#35748;&#20026;&#22312;&#33136;&#37096;&#30140;&#30171;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20840;&#29699;&#20581;&#24247;&#20851;&#27880;&#28966;&#28857;&#12290;&#20174;MR&#22270;&#20687;&#20013;&#33258;&#21160;&#37325;&#24314;&#33136;&#26894;&#20960;&#20309;&#24418;&#29366;&#65292;&#23558;&#20351;&#21307;&#23398;&#21442;&#25968;&#30340;&#24555;&#36895;&#27979;&#37327;&#25104;&#20026;&#21487;&#33021;&#65292;&#20197;&#35780;&#20272;&#33136;&#26894;&#29366;&#24577;&#65292;&#20174;&#32780;&#30830;&#23450;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#25216;&#26415;&#36890;&#24120;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#20998;&#21106;&#25110;&#19981;&#36866;&#21512;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#30340;&#26080;&#32467;&#26500;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransDeformer&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#39640;&#31354;&#38388;&#20934;&#30830;&#24230;&#21644;&#24739;&#32773;&#38388;&#32593;&#26684;&#23545;&#24212;&#30340;&#26041;&#24335;&#37325;&#24314;&#33136;&#26894;&#36718;&#24275;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;TransDeformer&#30340;&#21464;&#31181;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26032;&#30340;&#27880;&#24847;&#21147;&#20844;&#24335;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#21644;&#26631;&#35760;&#21270;&#30340;&#36718;&#24275;&#29305;&#24449;&#38598;&#25104;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00231v1 Announce Type: cross  Abstract: Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict 
&lt;/p&gt;</description></item><item><title>NUMTEMP&#26159;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#39564;&#35777;&#22797;&#26434;&#30340;&#25968;&#23383;&#35770;&#28857;&#65292;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#39564;&#35777;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17169</link><description>&lt;p&gt;
NUMTEMP&#65306;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#24102;&#26377;&#32479;&#35745;&#21644;&#26102;&#38388;&#34920;&#36798;&#24335;&#30340;&#35770;&#28857;&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17169
&lt;/p&gt;
&lt;p&gt;
NUMTEMP&#26159;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#39564;&#35777;&#22797;&#26434;&#30340;&#25968;&#23383;&#35770;&#28857;&#65292;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#39564;&#35777;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26816;&#26597;&#22312;&#25968;&#23383;&#26102;&#20195;&#24212;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#38169;&#35823;&#20449;&#24687;&#26041;&#38754;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#29616;&#26377;&#31995;&#32479;&#20027;&#35201;&#19987;&#27880;&#20110;&#32500;&#22522;&#30334;&#31185;&#19978;&#30340;&#21512;&#25104;&#35770;&#28857;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#19990;&#30028;&#35770;&#28857;&#19978;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;Numtemp&#65292;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#22810;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#20851;&#27880;&#25968;&#23383;&#35770;&#28857;&#65292;&#21253;&#25324;&#26102;&#38388;&#12289;&#32479;&#35745;&#21644;&#22810;&#26679;&#21270;&#26041;&#38754;&#30340;&#32454;&#31890;&#24230;&#20803;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#19981;&#27844;&#38706;&#30340;&#35777;&#25454;&#25910;&#38598;&#12290;&#36825;&#35299;&#20915;&#20102;&#39564;&#35777;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#35770;&#28857;&#22797;&#26434;&#65292;&#24448;&#24448;&#32570;&#20047;&#31934;&#30830;&#20449;&#24687;&#65292;&#36825;&#26159;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#21512;&#25104;&#35770;&#28857;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35780;&#20272;&#24182;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22312;&#39564;&#35777;&#25968;&#23383;&#35770;&#28857;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22522;&#20110;&#35770;&#28857;&#20998;&#35299;&#30340;&#26041;&#27861;&#12289;&#22522;&#20110;&#25968;&#23383;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#22522;&#32447;&#23454;&#29616;&#20102;58.32&#30340;&#23439;F1&#20998;&#25968;&#12290;&#36825;&#35777;&#26126;&#20102;Numtemp&#30340;&#20851;&#38190;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17169v1 Announce Type: cross  Abstract: Automated fact checking has gained immense interest to tackle the growing misinformation in the digital era. Existing systems primarily focus on synthetic claims on Wikipedia, and noteworthy progress has also been made on real-world claims. In this work, we release Numtemp, a diverse, multi-domain dataset focused exclusively on numerical claims, encompassing temporal, statistical and diverse aspects with fine-grained metadata and an evidence collection without leakage. This addresses the challenge of verifying real-world numerical claims, which are complex and often lack precise information, not addressed by existing works that mainly focus on synthetic claims. We evaluate and quantify the limitations of existing solutions for the task of verifying numerical claims. We also evaluate claim decomposition based methods, numerical understanding based models and our best baselines achieves a macro-F1 of 58.32. This demonstrates that Numtemp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13890</link><description>&lt;p&gt;
&#20197;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#23545;&#27604;&#21058;&#21487;&#20197;&#23450;&#20301;&#32959;&#30244;&#24182;&#35266;&#23519;&#20854;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#36825;&#23545;&#20110;&#30284;&#30151;&#34920;&#24449;&#21644;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#21058;&#30340;&#20351;&#29992;&#19981;&#20165;&#19982;&#19981;&#33391;&#20581;&#24247;&#39118;&#38505;&#30456;&#20851;&#65292;&#32780;&#19988;&#23545;&#20110;&#24576;&#23381;&#24739;&#32773;&#12289;&#32958;&#21151;&#33021;&#38556;&#30861;&#24739;&#32773;&#25110;&#20854;&#20182;&#19981;&#33391;&#21453;&#24212;&#24739;&#32773;&#23384;&#22312;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#27604;&#21058;&#25668;&#21462;&#26159;&#30149;&#28790;&#24694;&#24615;&#12289;&#30284;&#30151;&#22797;&#21457;&#39118;&#38505;&#21644;&#27835;&#30103;&#21453;&#24212;&#30340;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22240;&#27492;&#20943;&#23569;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36827;&#34892;DCE-MRI&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#26102;&#38388;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#30340;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#21464;&#24322;&#24615;&#30340;Fr\'echet&#25918;&#23556;&#32452;&#23398;&#36317;&#31163;&#20316;&#20026;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13890v1 Announce Type: cross  Abstract: Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making. However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions. With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration. To this end, we propose a multi-conditional latent diffusion model capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences. To evaluate medical image synthesis, we additionally propose and validate the Fr\'echet radiomics distance as an image quality measure based on biomarker variability 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#20445;&#30041;&#36873;&#23450;&#23376;&#20018;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13799</link><description>&lt;p&gt;
&#36870;&#21521;&#35757;&#32451;&#20197;&#28040;&#38500;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Reverse Training to Nurse the Reversal Curse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#20445;&#30041;&#36873;&#23450;&#23376;&#20018;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#29616;&#35937;&#65306;&#24403;&#35757;&#32451;&#27169;&#22411;&#20197;"A&#20855;&#26377;&#29305;&#24449;B"&#20026;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#26080;&#27861;&#27867;&#21270;&#21040;"B&#26159;A&#30340;&#29305;&#24449;"&#65292;&#36825;&#34987;&#31216;&#20026;&#36870;&#36716;&#35781;&#21650;&#12290;&#21363;&#20351;&#22312;&#20351;&#29992;&#25968;&#19975;&#20159;&#20196;&#29260;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#30001;&#20110;&#40784;&#22827;&#23450;&#24459;&#30340;&#23384;&#22312;&#65292;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#24847;&#21619;&#30528;&#21363;&#20351;&#25105;&#20204;&#22312;&#25972;&#20010;&#20114;&#32852;&#32593;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#38382;&#39064;&#20173;&#28982;&#20250;&#20986;&#29616;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#22312;&#20854;&#20013;&#25152;&#26377;&#21333;&#35789;&#34987;&#20351;&#29992;&#20004;&#27425;&#65292;&#20174;&#32780;&#20351;&#21487;&#29992;&#20196;&#29260;&#25968;&#37327;&#21152;&#20493;&#12290;&#35813;LLM&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#39072;&#20498;&#35757;&#32451;&#23383;&#31526;&#20018;&#26469;&#39072;&#20498;&#35757;&#32451;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#30041;&#65288;&#21363;&#19981;&#39072;&#20498;&#65289;&#36873;&#23450;&#30340;&#23376;&#20018;&#65292;&#22914;&#23454;&#20307;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#21305;&#37197;&#30340;&#36870;&#21521;&#35757;&#32451;&#27169;&#22411;&#22312;&#26631;&#20934;&#20219;&#21153;&#19978;&#27604;&#26631;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#31168;&#65292;&#24182;&#19988;&#35745;&#31639;&#21305;&#37197;&#30340;&#36870;&#21521;&#35757;&#32451;&#27169;&#22411;&#22312;&#36870;&#36716;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36828;&#36828;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13799v1 Announce Type: new  Abstract: Large language models (LLMs) have a surprising failure: when trained on "A has a feature B", they do not generalize to "B is a feature of A", which is termed the Reversal Curse. Even when training with trillions of tokens this issue still appears due to Zipf's law - hence even if we train on the entire internet. This work proposes an alternative training scheme, called reverse training, whereby all words are used twice, doubling the amount of available tokens. The LLM is trained in both forward and reverse directions by reversing the training strings while preserving (i.e., not reversing) chosen substrings, such as entities. We show that data-matched reverse-trained models provide superior performance to standard models on standard tasks, and compute-matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11169</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Correcting misinformation on social media with a large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20449;&#24687;&#20250;&#30772;&#22351;&#20844;&#20247;&#23545;&#31185;&#23398;&#21644;&#27665;&#20027;&#30340;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#19981;&#20934;&#30830;&#20449;&#24687;&#20250;&#36805;&#36895;&#20256;&#25773;&#12290;&#19987;&#23478;&#21644;&#26222;&#36890;&#20154;&#36890;&#36807;&#25163;&#21160;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#20934;&#30830;&#20449;&#24687;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#32416;&#27491;&#35823;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#25285;&#24551;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31561;&#25216;&#26415;&#20351;&#35823;&#20449;&#24687;&#26356;&#23481;&#26131;&#29983;&#25104;&#12290;LLMs&#36824;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#65292;&#21487;&#20197;&#21152;&#36895;&#32416;&#27491;&#35823;&#20449;&#24687;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30001;&#20110;&#32570;&#20047;&#26368;&#26032;&#20449;&#24687;&#12289;&#20542;&#21521;&#20110;&#29983;&#25104;&#20284;&#26159;&#32780;&#38750;&#30340;&#20869;&#23481;&#21644;&#24341;&#29992;&#20197;&#21450;&#26080;&#27861;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#32780;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUSE&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#26368;&#26032;&#20449;&#24687;&#35775;&#38382;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;LLM&#12290;&#36890;&#36807;&#26816;&#32034;&#19978;&#19979;&#25991;&#35777;&#25454;&#21644;&#21453;&#39539;&#65292;MUSE&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#21487;&#20449;&#30340;&#35299;&#37322;&#21644;&#21442;&#32771;&#12290;&#23427;&#36824;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22312;&#28145;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#21093;&#21066;&#36793;&#32536;&#32676;&#20307;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#26426;&#21046;&#22914;&#38646;&#24037;&#32463;&#27982;&#21171;&#24037;&#30340;&#28389;&#29992;&#12289;&#26377;&#20559;&#35265;&#30340;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#21644;&#23545;&#36825;&#20123;&#31038;&#32676;&#26045;&#21152;&#30340;&#19981;&#25104;&#27604;&#20363;&#30340;&#24515;&#29702;&#20581;&#24247;&#36127;&#25285;&#65292;&#21152;&#21095;&#20102;&#29616;&#26377;&#30340;&#19981;&#24179;&#31561;&#12290;</title><link>https://arxiv.org/abs/2403.06332</link><description>&lt;p&gt;
&#21033;&#29992;&#21033;&#28070;&#31354;&#38388;&#65306;&#36164;&#26412;&#20027;&#20041;&#22914;&#20309;&#20197;&#21093;&#21066;&#23569;&#25968;&#32676;&#20307;&#20026;&#20195;&#20215;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Margin: How Capitalism Fuels AI at the Expense of Minoritized Groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06332
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#28145;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#21093;&#21066;&#36793;&#32536;&#32676;&#20307;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#26426;&#21046;&#22914;&#38646;&#24037;&#32463;&#27982;&#21171;&#24037;&#30340;&#28389;&#29992;&#12289;&#26377;&#20559;&#35265;&#30340;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#21644;&#23545;&#36825;&#20123;&#31038;&#32676;&#26045;&#21152;&#30340;&#19981;&#25104;&#27604;&#20363;&#30340;&#24515;&#29702;&#20581;&#24247;&#36127;&#25285;&#65292;&#21152;&#21095;&#20102;&#29616;&#26377;&#30340;&#19981;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#36164;&#26412;&#20027;&#20041;&#12289;&#31181;&#26063;&#21387;&#36843;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#20803;&#32032;&#22914;&#20309;&#20849;&#21516;&#21152;&#28145;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;&#36890;&#36807;&#36861;&#28335;&#21382;&#21490;&#19978;&#23545;&#36793;&#32536;&#31038;&#32676;&#30340;&#21093;&#21066;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19981;&#20165;&#21453;&#26144;&#20102;&#31038;&#20250;&#20559;&#35265;&#65292;&#32780;&#19988;&#21152;&#21095;&#20102;&#31181;&#26063;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06332v1 Announce Type: cross  Abstract: This article investigates the complex nexus of capitalism, racial oppression, and artificial intelligence (AI), revealing how these elements coalesce to deepen social inequities. By tracing the historical exploitation of marginalized communities through capitalist practices, the study demonstrates how AI technologies not only reflect but also amplify societal biases, particularly in exacerbating racial disparities. Through a focused analysis, the paper presents how AI's development and application exploit marginalized groups via mechanisms such as gig economy labor abuses, biased facial recognition technologies, and the disproportionate mental health burdens placed on these communities. These examples underscore the critical role of AI in reinforcing and intensifying existing inequalities. Concluding that unregulated AI significantly threatens to compound current oppressions, the article calls for a concerted effort towards responsible
&lt;/p&gt;</description></item><item><title>LitSim&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#20132;&#20114;&#24335;&#20223;&#30495;&#26041;&#27861;&#65292;&#37325;&#25773;&#26085;&#24535;&#20197;&#33719;&#24471;&#36924;&#30495;&#22330;&#26223;&#65292;&#24403;&#20986;&#29616;&#19981;&#30495;&#23454;&#20914;&#31361;&#26102;&#36827;&#34892;&#24178;&#39044;&#65292;&#20174;&#32780;&#25552;&#39640;&#20223;&#30495;&#36924;&#30495;&#24230;&#24182;&#36991;&#20813;&#30896;&#25758;</title><link>https://arxiv.org/abs/2403.04299</link><description>&lt;p&gt;
LitSim&#65306;&#38271;&#26399;&#20132;&#20114;&#24335;&#20132;&#36890;&#20223;&#30495;&#30340;&#20914;&#31361;&#24863;&#30693;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
LitSim: Conflict-aware Policy for Long-term Interactive Traffic Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04299
&lt;/p&gt;
&lt;p&gt;
LitSim&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#20132;&#20114;&#24335;&#20223;&#30495;&#26041;&#27861;&#65292;&#37325;&#25773;&#26085;&#24535;&#20197;&#33719;&#24471;&#36924;&#30495;&#22330;&#26223;&#65292;&#24403;&#20986;&#29616;&#19981;&#30495;&#23454;&#20914;&#31361;&#26102;&#36827;&#34892;&#24178;&#39044;&#65292;&#20174;&#32780;&#25552;&#39640;&#20223;&#30495;&#36924;&#30495;&#24230;&#24182;&#36991;&#20813;&#30896;&#25758;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#23545;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#19982;&#22312;&#36947;&#36335;&#27979;&#35797;&#30456;&#27604;&#65292;&#27169;&#25311;&#20855;&#26377;&#25928;&#29575;&#21644;&#25104;&#26412;&#20248;&#21183;&#12290;&#38656;&#35201;&#30495;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#65288;&#20363;&#22914;&#20132;&#20114;&#24335;&#21644;&#38271;&#26399;&#34892;&#20026;&#65289;&#26469;&#32553;&#23567;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#29616;&#26377;&#24037;&#20316;&#22312;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#26041;&#38754;&#23384;&#22312;&#20197;&#19979;&#32570;&#28857;&#65306;&#65288;1&#65289;&#26085;&#24535;&#37325;&#25773;&#25552;&#20379;&#20102;&#36924;&#30495;&#30340;&#22330;&#26223;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#21160;&#24577;&#20132;&#20114;&#32780;&#23548;&#33268;&#20102;&#19981;&#30495;&#23454;&#30340;&#30896;&#25758;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#40723;&#21169;&#20132;&#20114;&#65292;&#20294;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#24448;&#24448;&#20559;&#31163;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LitSim&#65292;&#19968;&#31181;&#38271;&#26399;&#20132;&#20114;&#24335;&#20223;&#30495;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#36924;&#30495;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#19981;&#30495;&#23454;&#30340;&#30896;&#25758;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#22823;&#22810;&#25968;&#24773;&#20917;&#37325;&#25773;&#26085;&#24535;&#65292;&#21482;&#26377;&#22312;LitSim&#39044;&#27979;&#21040;&#19981;&#30495;&#23454;&#20914;&#31361;&#26102;&#25165;&#36827;&#34892;&#24178;&#39044;&#12290;&#28982;&#21518;&#25105;&#20204;&#40723;&#21169;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#24182;&#35299;&#20915;&#20914;&#31361;&#65292;&#20174;&#32780;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04299v1 Announce Type: cross  Abstract: Simulation is pivotal in evaluating the performance of autonomous driving systems due to the advantages in efficiency and cost compared to on-road testing. Realistic multi-agent behavior~(e.g., interactive and long-term) is needed to narrow the gap between the simulation and the reality. The existing work has the following shortcomings in achieving this goal:~(1) log replay offers realistic scenarios but leads to unrealistic collisions due to lacking dynamic interactions, and~(2) model-based and learning-based solutions encourage interactions but often deviate from real-world data in long horizons. In this work, we propose LitSim, a long-term interactive simulation approach that maximizes realism while avoiding unrealistic collisions. Specifically, we replay the log for most scenarios and intervene only when LitSim predicts unrealistic conflicts. We then encourage interactions among the agents and resolve the conflicts, thereby reducin
&lt;/p&gt;</description></item><item><title>&#23545;&#21307;&#23398;LLMs&#36827;&#34892;&#20102;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23450;&#20041;&#21307;&#23398;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#65292;&#24320;&#21457;&#20102;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.03744</link><description>&lt;p&gt;
&#20026;&#21307;&#33647;&#39046;&#22495;&#25171;&#36896;&#23433;&#20840;&#21644;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Safe and Aligned Large Language Models for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03744
&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#23398;LLMs&#36827;&#34892;&#20102;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23450;&#20041;&#21307;&#23398;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#65292;&#24320;&#21457;&#20102;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#27491;&#22312;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#36827;&#27493;&#65292;&#21363;&#20351;&#26159;&#23427;&#20204;&#30340;&#24320;&#21457;&#32773;&#20063;&#23545;&#23427;&#20204;&#30340;&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#28145;&#24230;&#24863;&#21040;&#22256;&#24785;&#12290;&#23613;&#31649;&#24050;&#32463;&#37319;&#21462;&#20102;&#21021;&#27493;&#27493;&#39588;&#35780;&#20272;&#36890;&#29992;&#30693;&#35782;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#24369;&#28857;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23613;&#31649;&#22312;&#20010;&#20154;&#20581;&#24247;&#21644;&#23433;&#20840;&#12289;&#20844;&#20849;&#20581;&#24247;&#21644;&#23433;&#20840;&#20197;&#21450;&#20154;&#26435;&#26041;&#38754;&#23384;&#22312;&#39118;&#38505;&#65292;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#23578;&#26410;&#24471;&#21040;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#21307;&#23398;LLMs&#30340;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21307;&#23398;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#30340;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;LLM&#30340;&#21307;&#23398;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#36890;&#29992;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26356;&#24191;&#27867;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03744v1 Announce Type: new  Abstract: The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale appr
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;Brilla AI&#22312;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#32570;&#20047;&#21512;&#26684;&#25945;&#24072;&#30340;&#38750;&#27954;&#25552;&#20379;&#20102;&#23398;&#20064;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.01699</link><description>&lt;p&gt;
Brilla AI: &#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#30340;&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;
&lt;/p&gt;
&lt;p&gt;
Brilla AI: AI Contestant for the National Science and Maths Quiz
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01699
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;Brilla AI&#22312;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#32570;&#20047;&#21512;&#26684;&#25945;&#24072;&#30340;&#38750;&#27954;&#25552;&#20379;&#20102;&#23398;&#20064;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#22823;&#38470;&#32570;&#20047;&#36275;&#22815;&#30340;&#21512;&#26684;&#25945;&#24072;&#65292;&#36825;&#38459;&#30861;&#20102;&#25552;&#20379;&#36275;&#22815;&#30340;&#23398;&#20064;&#25903;&#25345;&#12290;&#20154;&#24037;&#26234;&#33021;&#26377;&#21487;&#33021;&#22686;&#24378;&#26377;&#38480;&#25968;&#37327;&#25945;&#24072;&#30340;&#21162;&#21147;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#23398;&#20064;&#25104;&#26524;&#12290;&#26412;&#25991;&#25551;&#36848;&#24182;&#35780;&#20272;&#20102;NSMQ AI Grand Challenge&#30340;&#39318;&#35201;&#25104;&#26524;&#65292;&#35813;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#29616;&#23454;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27492;&#31867;&#20154;&#24037;&#26234;&#33021;&#65306;&#8220;&#24314;&#31435;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65292;&#21442;&#21152;&#21152;&#32435;&#30340;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#65288;NSMQ&#65289;&#65292;&#24182;&#33719;&#32988;&#8212;&#8212;&#22312;&#27604;&#36187;&#30340;&#25152;&#26377;&#36718;&#27425;&#21644;&#38454;&#27573;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20248;&#31168;&#30340;&#21442;&#36187;&#32773;&#8221;&#12290;NSMQ&#26159;&#21152;&#32435;&#30340;&#39640;&#20013;&#23398;&#29983;&#27599;&#24180;&#20030;&#34892;&#30340;&#29616;&#22330;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#65292;3&#38431;2&#21517;&#23398;&#29983;&#36890;&#36807;&#22238;&#31572;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#12289;&#29289;&#29702;&#21644;&#25968;&#23398;&#38382;&#39064;&#22312;5&#36718;&#27604;&#36187;&#20013;&#31454;&#20105;&#65292;&#36880;&#28176;&#26187;&#32423;&#33267;&#26368;&#32456;&#20896;&#20891;&#30340;&#38431;&#20237;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;Brilla AI&#65292;&#19968;&#20010;&#21442;&#21152;NSMQ&#31454;&#36187;&#30340;&#20154;&#24037;&#26234;&#33021;&#36873;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01699v1 Announce Type: cross  Abstract: The African continent lacks enough qualified teachers which hampers the provision of adequate learning support. An AI could potentially augment the efforts of the limited number of teachers, leading to better learning outcomes. Towards that end, this work describes and evaluates the first key output for the NSMQ AI Grand Challenge, which proposes a robust, real-world benchmark for such an AI: "Build an AI to compete live in Ghana's National Science and Maths Quiz (NSMQ) competition and win - performing better than the best contestants in all rounds and stages of the competition". The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. In this work, we built Brilla AI, an AI contestant that we de
&lt;/p&gt;</description></item><item><title>FusionVision&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#23558;YOLO&#21644;&#24555;&#36895;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#30340;&#25216;&#26415;&#25972;&#21512;&#21040;RGB-D&#30456;&#26426;&#22788;&#29702;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;3D&#29289;&#20307;&#30340;&#40065;&#26834;&#20998;&#21106;</title><link>https://arxiv.org/abs/2403.00175</link><description>&lt;p&gt;
FusionVision&#65306;&#20351;&#29992;YOLO&#21644;&#24555;&#36895;&#20998;&#21106;&#20219;&#24847;&#29289;&#20307;&#30340;&#32508;&#21512;&#26041;&#27861;&#36827;&#34892;&#20174;RGB-D&#30456;&#26426;&#37325;&#24314;&#21644;&#20998;&#21106;&#30340;3D&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
FusionVision: A comprehensive approach of 3D object reconstruction and segmentation from RGB-D cameras using YOLO and fast segment anything
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00175
&lt;/p&gt;
&lt;p&gt;
FusionVision&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#23558;YOLO&#21644;&#24555;&#36895;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#30340;&#25216;&#26415;&#25972;&#21512;&#21040;RGB-D&#30456;&#26426;&#22788;&#29702;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;3D&#29289;&#20307;&#30340;&#40065;&#26834;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#23558;&#20808;&#36827;&#25216;&#26415;&#25972;&#21512;&#21040;RGB-D&#30456;&#26426;&#36755;&#20837;&#22788;&#29702;&#20013;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#29615;&#22659;&#26465;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#29289;&#20307;&#22806;&#35266;&#30340;&#21464;&#21270;&#24102;&#26469;&#20102;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;FusionVision&#65292;&#19968;&#31181;&#20026;&#22312;RGB-D&#22270;&#20687;&#20013;&#40065;&#26834;&#22320;&#36827;&#34892;3D&#29289;&#20307;&#20998;&#21106;&#32780;&#35843;&#25972;&#30340;&#35814;&#23613;&#31649;&#36947;&#12290;&#20256;&#32479;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#22312;&#21516;&#26102;&#25429;&#25417;&#31934;&#30830;&#30340;&#29289;&#20307;&#36793;&#30028;&#24182;&#22312;&#28145;&#24230;&#22270;&#19978;&#23454;&#29616;&#39640;&#31934;&#24230;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#26159;&#20026;RGB&#25668;&#20687;&#26426;&#25552;&#20986;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;FusionVision&#37319;&#29992;&#20102;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#26816;&#27979;&#25216;&#26415;&#19982;&#20808;&#36827;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36825;&#20123;&#32452;&#20214;&#30340;&#25972;&#21512;&#20351;&#24471;&#33021;&#22815;&#23545;&#20174;&#24425;&#33394;RGB&#21644;&#28145;&#24230;D&#20449;&#36947;&#33719;&#24471;&#30340;&#20449;&#24687;&#36827;&#34892;&#20840;&#38754;&#32479;&#19968;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00175v1 Announce Type: cross  Abstract: In the realm of computer vision, the integration of advanced techniques into the processing of RGB-D camera inputs poses a significant challenge, given the inherent complexities arising from diverse environmental conditions and varying object appearances. Therefore, this paper introduces FusionVision, an exhaustive pipeline adapted for the robust 3D segmentation of objects in RGB-D imagery. Traditional computer vision systems face limitations in simultaneously capturing precise object boundaries and achieving high-precision object detection on depth map as they are mainly proposed for RGB cameras. To address this challenge, FusionVision adopts an integrated approach by merging state-of-the-art object detection techniques, with advanced instance segmentation methods. The integration of these components enables a holistic (unified analysis of information obtained from both color \textit{RGB} and depth \textit{D} channels) interpretation 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.10466</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#22120;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Zero-shot Dialogue State Tracker through Function Calling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#26085;&#30410;&#26222;&#36941;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#22312;&#19968;&#33324;&#24773;&#22659;&#20013;&#20855;&#26377;&#20808;&#36827;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#19981;&#20165;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#36824;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#20869;&#36827;&#34892;&#26377;&#25928;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#65288;DST&#65289;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#35299;&#20915;LLMs&#20013;&#30340;DST&#30340;&#26032;&#26041;&#27861;FnCTOD&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#20102;&#38646;-shot DST&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#24320;&#28304;&#25110;&#19987;&#26377;LLMs&#26102;&#37117;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20351;&#24471;&#21508;&#31181;7B&#25110;13B&#21442;&#25968;&#27169;&#22411;&#36229;&#36234;&#20102;&#20043;&#21069;&#30001;ChatGPT&#23454;&#29616;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#65288;SOTA&#65289;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#20987;&#36133;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10466v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#35299;&#26512;&#20102;&#27169;&#22411;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#32531;&#35299;&#20102;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;</title><link>https://arxiv.org/abs/2402.07712</link><description>&lt;p&gt;
&#27169;&#22411;&#23849;&#28291;&#35299;&#23494;&#65306;&#22238;&#24402;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Collapse Demystified: The Case of Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#35299;&#26512;&#20102;&#27169;&#22411;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#32531;&#35299;&#20102;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;"&#27169;&#22411;&#23849;&#28291;"&#29616;&#35937;&#25351;&#30340;&#26159;&#27169;&#22411;&#22312;&#36882;&#24402;&#22320;&#35757;&#32451;&#33258;&#36523;&#19978;&#19968;&#20195;&#21448;&#19968;&#20195;&#29983;&#25104;&#30340;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#38477;&#20302;&#65292;&#26368;&#32456;&#21464;&#24471;&#23436;&#20840;&#26080;&#29992;&#65292;&#21363;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#33719;&#24471;&#20102;&#32467;&#26524;&#65292;&#26174;&#31034;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#27169;&#22411;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20132;&#21449;&#28857;&#12290;&#22312;&#22810;&#39033;&#24335;&#34928;&#20943;&#30340;&#20809;&#35889;&#21644;&#28304;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20462;&#25913;&#21518;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#23637;&#31034;&#20102;&#20174;&#24555;&#36895;&#21040;&#32531;&#24930;&#36895;&#29575;&#30340;&#26032;&#20132;&#21449;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31616;&#21333;&#31574;&#30053;&#26469;&#32531;&#35299;&#27169;&#22411;&#23849;&#28291;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of large language models like ChatGPT, the phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06025</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#27010;&#29575;&#25512;&#29702;&#36827;&#34892;&#23454;&#39564;&#19982;&#20462;&#35746;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;&#35813;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#65292;&#21363;&#20351;&#35268;&#21017;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#23398;&#20064;&#32773;&#20063;&#20250;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#27169;&#31946;&#27010;&#29575;&#35268;&#21017;&#65292;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#26681;&#25454;&#36817;&#20284;&#36125;&#21494;&#26031;&#21407;&#21017;&#22312;&#27599;&#27425;&#23454;&#39564;&#21518;&#22312;&#32447;&#26356;&#26032;&#33258;&#24049;&#30340;&#20551;&#35774;&#12290;&#22312;&#21516;&#19968;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#26681;&#25454;&#20449;&#24687;&#35770;&#20934;&#21017;&#24314;&#31435;&#20102;&#23454;&#39564;&#35774;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#21407;&#21017;&#30340;&#32452;&#21512;&#8212;&#8212;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#8212;&#8212;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#21435;&#25481;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#32452;&#20214;&#37117;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
&lt;/p&gt;</description></item><item><title>RGI-Net&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#20256;&#32479;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#25151;&#38388;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2309.01513</link><description>&lt;p&gt;
RGI-Net&#65306;&#22312;&#27809;&#26377;&#19968;&#38454;&#22238;&#22768;&#30340;&#24773;&#20917;&#19979;&#20174;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#25512;&#26029;3D&#25151;&#38388;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01513
&lt;/p&gt;
&lt;p&gt;
RGI-Net&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#20256;&#32479;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#25151;&#38388;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25151;&#38388;&#20960;&#20309;&#26159;&#23454;&#29616;&#36924;&#30495;&#30340;3D&#38899;&#39057;&#28210;&#26579;&#30340;&#37325;&#35201;&#20808;&#39564;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#21040;&#36798;&#26102;&#38388;&#65288;TOA&#65289;&#25110;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;TDOA&#65289;&#20449;&#24687;&#21457;&#23637;&#20102;&#21508;&#31181;&#25151;&#38388;&#20960;&#20309;&#25512;&#26029;&#65288;RGI&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;RGI&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#20123;&#20551;&#35774;&#65292;&#22914;&#20984;&#25151;&#38388;&#24418;&#29366;&#12289;&#24050;&#30693;&#22681;&#22721;&#25968;&#37327;&#21644;&#19968;&#38454;&#21453;&#23556;&#30340;&#21487;&#35265;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;RGI-Net&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#19978;&#36848;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#25151;&#38388;&#20960;&#20309;&#12290;RGI-Net&#23398;&#20064;&#24182;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#65288;RIRs&#65289;&#20013;&#30340;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#24418;&#29366;&#20026;&#38750;&#20984;&#24418;&#25110;RIRs&#20013;&#32570;&#23569;&#19968;&#38454;&#21453;&#23556;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#25151;&#38388;&#24418;&#29366;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#20174;&#35013;&#26377;&#22278;&#24418;&#40614;&#20811;&#39118;&#30340;&#32039;&#20945;&#38899;&#39057;&#35774;&#22791;&#27979;&#37327;&#30340;RIRs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01513v2 Announce Type: replace-cross  Abstract: Room geometry is important prior information for implementing realistic 3D audio rendering. For this reason, various room geometry inference (RGI) methods have been developed by utilizing the time of arrival (TOA) or time difference of arrival (TDOA) information in room impulse responses. However, the conventional RGI technique poses several assumptions, such as convex room shapes, the number of walls known in priori, and the visibility of first-order reflections. In this work, we introduce the deep neural network (DNN), RGI-Net, which can estimate room geometries without the aforementioned assumptions. RGI-Net learns and exploits complex relationships between high-order reflections in room impulse responses (RIRs) and, thus, can estimate room shapes even when the shape is non-convex or first-order reflections are missing in the RIRs. The network takes RIRs measured from a compact audio device equipped with a circular microphon
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#21040;2047&#24180;&#20026;50%&#12290;</title><link>http://arxiv.org/abs/2401.02843</link><description>&lt;p&gt;
&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Thousands of AI Authors on the Future of AI. (arXiv:2401.02843v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02843
&lt;/p&gt;
&lt;p&gt;
&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#21040;2047&#24180;&#20026;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#35843;&#26597;&#20013;&#65292;2778&#21517;&#22312;&#39030;&#32423;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20250;&#35758;&#19978;&#21457;&#34920;&#36807;&#35770;&#25991;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;AI&#36827;&#23637;&#30340;&#36895;&#24230;&#12289;&#39640;&#32423;AI&#31995;&#32479;&#30340;&#24615;&#36136;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;&#24635;&#20307;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#33267;&#23569;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#19968;&#20010;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#21487;&#20197;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22914;&#26524;&#31185;&#23398;&#25345;&#32493;&#19981;&#21463;&#24178;&#25200;&#65292;2027&#24180;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#65292;&#21040;2047&#24180;&#20026;50%&#12290;&#21518;&#32773;&#30340;&#20272;&#35745;&#27604;&#25105;&#20204;&#19968;&#24180;&#21069;&#36827;&#34892;&#30340;&#31867;&#20284;&#35843;&#26597;[Grace et al., 2022]&#25552;&#21069;&#20102;13&#24180;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#20154;&#31867;&#32844;&#19994;&#23436;&#20840;&#21487;&#33258;&#21160;&#21270;&#30340;&#20960;&#29575;&#39044;&#35745;&#35201;&#21040;2037&#24180;&#36798;&#21040;10%&#65292;&#21040;2116&#24180;&#25165;&#36798;&#21040;50%&#65288;&#19982;2022&#24180;&#35843;&#26597;&#20013;&#30340;2164&#24180;&#30456;&#27604;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).  Most
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#36827;&#21270;&#26694;&#26550;&#65292;&#33258;&#21160;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#26631;&#24535;&#30528;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#26032;&#26102;&#20195;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02051</link><description>&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20987;&#36133;&#20154;&#31867;&#65306;&#39640;&#25928;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#35774;&#35745;&#30340;&#20363;&#23376;
&lt;/p&gt;
&lt;p&gt;
An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search. (arXiv:2401.02051v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02051
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#36827;&#21270;&#26694;&#26550;&#65292;&#33258;&#21160;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#26631;&#24535;&#30528;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#26032;&#26102;&#20195;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#65292;&#35774;&#35745;&#39640;&#25928;&#31639;&#27861;&#36890;&#24120;&#38750;&#24120;&#32321;&#29712;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#36827;&#21270;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;AEL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#12290;AEL&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#36827;&#21270;&#35745;&#31639;&#30340;&#33539;&#24335;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#33258;&#21160;&#35774;&#35745;&#12289;&#32452;&#21512;&#21644;&#20462;&#25913;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;AEL&#26469;&#35774;&#35745;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#65288;GLS&#65289;&#30340;&#24341;&#23548;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#33879;&#21517;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#12290;AEL&#33258;&#21160;&#28436;&#21270;&#20986;&#20248;&#31168;&#30340;GLS&#31639;&#27861;&#65292;&#22312;&#20004;&#22825;&#20869;&#23454;&#29616;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#21644;&#26080;&#38656;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;1,000&#20010;TSP20-TSP100&#23454;&#20363;&#21644;TSPLib&#23454;&#20363;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AEL&#35774;&#35745;&#30340;GLS&#31639;&#27861;&#22312;&#30456;&#21516;&#30340;&#36845;&#20195;&#39044;&#31639;&#19979;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#35774;&#35745;&#30340;GLS&#31639;&#27861;&#12290;&#22312;1,000&#27425;&#36845;&#20195;&#20013;&#65292;&#23427;&#22312;TSP20&#21644;TSP50&#19978;&#36798;&#21040;0%&#38388;&#38553;&#65292;&#22312;TSP100&#19978;&#36798;&#21040;0.032%&#38388;&#38553;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26631;&#24535;&#30528;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#26032;&#26102;&#20195;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often very tedious for human experts to design efficient algorithms. Recently, we have proposed a novel Algorithm Evolution using Large Language Model (AEL) framework for automatic algorithm design. AEL combines the power of a large language model and the paradigm of evolutionary computation to design, combine, and modify algorithms automatically. In this paper, we use AEL to design the guide algorithm for guided local search (GLS) to solve the well-known traveling salesman problem (TSP). AEL automatically evolves elite GLS algorithms in two days, with minimal human effort and no model training. Experimental results on 1,000 TSP20-TSP100 instances and TSPLib instances show that AEL-designed GLS outperforms state-of-the-art human-designed GLS with the same iteration budget. It achieves a 0% gap on TSP20 and TSP50 and a 0.032% gap on TSP100 in 1,000 iterations. Our findings mark the emergence of a new era in automatic algorithm design.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.11456</link><description>&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#36845;&#20195;&#20559;&#22909;&#23398;&#20064;&#65306;&#22312;KL&#32422;&#26463;&#19979;&#23558;&#29702;&#35770;&#19982;&#23454;&#36341;&#32852;&#31995;&#36215;&#26469;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#23545;&#40784;&#36807;&#31243;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#21363;&#21453;&#21521;KL&#27491;&#21017;&#21270;&#30340;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#29992;&#20110;RLHF&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#23545;&#36825;&#20010;&#20844;&#24335;&#30340;&#20005;&#26684;&#29702;&#35770;&#20998;&#26512;&#20173;&#28982;&#24456;&#24320;&#25918;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#22312;&#31163;&#32447;&#12289;&#22312;&#32447;&#21644;&#28151;&#21512;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#26397;&#30528;&#23454;&#38469;&#24212;&#29992;&#30340;&#26041;&#21521;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23545;&#20449;&#24687;&#29702;&#35770;&#31574;&#30053;&#25913;&#36827;&#39044;&#35328;&#30340;&#31283;&#20581;&#36817;&#20284;&#65292;&#33258;&#28982;&#22320;&#20135;&#29983;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;RLHF&#31639;&#27861;&#12290;&#36825;&#21253;&#25324;&#22312;&#32447;&#22330;&#26223;&#20013;&#30340;&#36845;&#20195;&#29256;&#26412;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#31639;&#27861;&#65292;&#20197;&#21450;&#31163;&#32447;&#24773;&#26223;&#19979;&#30340;&#22810;&#27493;&#25298;&#32477;&#25277;&#26679;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#23545;&#40784;&#23454;&#39564;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.19852</link><description>&lt;p&gt;
AI&#23545;&#40784;: &#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
AI Alignment: A Comprehensive Survey. (arXiv:2310.19852v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#26088;&#22312;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#12290;&#38543;&#30528;&#25317;&#26377;&#36229;&#20154;&#31867;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#38169;&#35823;&#23545;&#40784;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#28508;&#22312;&#22823;&#35268;&#27169;&#39118;&#38505;&#21464;&#24471;&#26126;&#26174;&#12290;&#25968;&#30334;&#21517;AI&#19987;&#23478;&#21644;&#20844;&#20247;&#20154;&#29289;&#37117;&#23545;AI&#39118;&#38505;&#34920;&#36798;&#20102;&#20851;&#27880;&#65292;&#35748;&#20026;&#20943;&#36731;AI&#24102;&#26469;&#30340;&#28781;&#32477;&#39118;&#38505;&#24212;&#35813;&#25104;&#20026;&#20840;&#29699;&#30340;&#20248;&#20808;&#20107;&#39033;&#65292;&#19982;&#22823;&#35268;&#27169;&#31038;&#20250;&#39118;&#38505;&#22914;&#22823;&#27969;&#34892;&#30149;&#21644;&#26680;&#25112;&#20105;&#24182;&#21015;&#12290;&#37492;&#20110;AI&#23545;&#40784;&#39046;&#22495;&#32570;&#20047;&#26368;&#26032;&#30340;&#31995;&#32479;&#35843;&#26597;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#23545;&#40784;&#30740;&#31350;&#30340;&#26680;&#24515;&#27010;&#24565;&#12289;&#26041;&#27861;&#35770;&#21644;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#30446;&#26631;&#21407;&#21017;&#20316;&#20026;AI&#23545;&#40784;&#30340;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#65288;RICE&#65289;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#24403;&#21069;&#23545;&#40784;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#12290;&#21069;&#32773;&#26088;&#22312;&#20351;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI alignment aims to build AI systems that are in accordance with human intentions and values. With the emergence of AI systems possessing superhuman capabilities, the potential large-scale risks associated with misaligned systems become apparent. Hundreds of AI experts and public figures have expressed their concerns about AI risks, arguing that mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. Motivated by the lack of an up-to-date systematic survey on AI alignment, in this paper, we delve into the core concepts, methodology, and practice of alignment research. To begin with, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). We outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned v
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#32858;&#31867;&#30340;&#27010;&#29575;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#65292;&#23454;&#29616;&#22312;D-Wave AQC&#19978;&#35782;&#21035;&#27169;&#31946;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.12153</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#30340;&#27010;&#29575;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing. (arXiv:2310.12153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#32858;&#31867;&#30340;&#27010;&#29575;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#65292;&#23454;&#29616;&#22312;D-Wave AQC&#19978;&#35782;&#21035;&#27169;&#31946;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#65288;AQC&#65289;&#26159;&#19968;&#31181;&#26377;&#26395;&#29992;&#20110;&#31163;&#25955;&#19988;&#36890;&#24120;&#20026;NP&#22256;&#38590;&#20248;&#21270;&#38382;&#39064;&#30340;&#37327;&#23376;&#35745;&#31639;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;AQC&#20801;&#35768;&#23454;&#29616;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#65292;&#36825;&#20419;&#20351;&#20102;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#24320;&#21457;&#37327;&#23376;&#34920;&#31034;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#38656;&#35201;&#20174;&#22122;&#22768;AQC&#36827;&#34892;&#22810;&#27425;&#27979;&#37327;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#20165;&#21033;&#29992;&#26368;&#20339;&#27979;&#37327;&#65292;&#20002;&#24323;&#20102;&#20854;&#20182;&#27979;&#37327;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36827;&#34892;&#27010;&#29575;&#24179;&#34913;k-means&#32858;&#31867;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#35745;&#31639;&#25104;&#26412;&#24456;&#20302;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#27169;&#31946;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#65292;&#25105;&#20204;&#22312;D-Wave AQC&#19978;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adiabatic quantum computing (AQC) is a promising quantum computing approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many machine learning and computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2310.01012</link><description>&lt;p&gt;
CCA&#23478;&#26063;&#30340;&#39640;&#25928;&#31639;&#27861;&#65306;&#26080;&#32422;&#26463;&#30446;&#26631;&#19982;&#26080;&#20559;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26041;&#27861;&#22312;&#22810;&#35270;&#35282;&#23398;&#20064;&#20013;&#20855;&#26377;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#27491;&#21017;&#21270;&#32447;&#24615;CCA&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#30340;&#25512;&#24191;&#65292;&#24182;&#19982;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65288;GEP&#65289;&#26694;&#26550;&#32479;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#24615;&#26041;&#27861;&#30340;&#20256;&#32479;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#28145;&#24230;CCA&#30340;&#25193;&#23637;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#35757;&#32451;&#36807;&#31243;&#32531;&#24930;&#19988;&#22797;&#26434;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#25551;&#36848;GEPs&#30340;&#39030;&#32423;&#23376;&#31354;&#38388;&#30340;&#26032;&#39062;&#26080;&#32422;&#26463;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#26159;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#24212;&#29992;&#20110;&#30456;&#24212;&#30340;CCA&#30446;&#26631;&#65292;&#20174;&#32780;&#33719;&#24471;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#25152;&#26377;&#26631;&#20934;CCA&#21644;&#28145;&#24230;CCA&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;&#36825;&#26679;&#30340;&#36895;&#24230;&#20351;&#25105;&#20204;&#33021;&#22815;&#39318;&#27425;&#36827;&#34892;&#22823;&#35268;&#27169;&#29983;&#29289;&#25968;&#25454;&#30340;PLS&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#8212;&#8212;&#20851;&#31995;&#29942;&#39048;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35825;&#23548;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06629</link><description>&lt;p&gt;
&#20316;&#20026;&#26377;&#25928;&#25277;&#35937;&#30340;&#24402;&#32435;&#20559;&#22909;&#30340;&#20851;&#31995;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
The Relational Bottleneck as an Inductive Bias for Efficient Abstraction. (arXiv:2309.06629v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#8212;&#8212;&#20851;&#31995;&#29942;&#39048;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35825;&#23548;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#35299;&#37322;&#22914;&#20309;&#20174;&#26377;&#38480;&#32463;&#39564;&#20013;&#33719;&#21462;&#25277;&#35937;&#27010;&#24565;&#12290;&#36825;&#19968;&#21162;&#21147;&#24120;&#24120;&#34987;&#25551;&#36848;&#20026;&#32463;&#39564;&#20027;&#20041;&#21644;&#22825;&#36171;&#20027;&#20041;&#26041;&#27861;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#65292;&#26368;&#36817;&#20027;&#35201;&#20307;&#29616;&#22312;&#26377;&#20851;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#35748;&#30693;&#27169;&#22411;&#30340;&#20105;&#35770;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#31181;&#26368;&#36817;&#20852;&#36215;&#30340;&#24037;&#20316;&#32447;&#36335;&#65292;&#35813;&#32447;&#36335;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#31216;&#20043;&#20026;&#20851;&#31995;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#31181;&#26032;&#30340;&#35843;&#21644;&#26041;&#24335;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#31995;&#21015;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#26041;&#24335;&#19979;&#35825;&#23548;&#20986;&#25277;&#35937;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#20316;&#20026;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#30340;&#20505;&#36873;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central challenge for cognitive science is to explain how abstract concepts are acquired from limited experience. This effort has often been framed in terms of a dichotomy between empiricist and nativist approaches, most recently embodied by debates concerning deep neural networks and symbolic cognitive models. Here, we highlight a recently emerging line of work that suggests a novel reconciliation of these approaches, by exploiting an inductive bias that we term the relational bottleneck. We review a family of models that employ this approach to induce abstractions in a data-efficient manner, emphasizing their potential as candidate models for the acquisition of abstract concepts in the human mind and brain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22238;&#24212;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23383;&#31526;&#20018;&#31867;&#27604;&#30340;&#21453;&#20363;&#26469;&#21453;&#39539;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;GPT-3&#26080;&#27861;&#35299;&#20915;&#26368;&#31616;&#21333;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#20026;&#20102;&#21152;&#24378;&#38646;&#28857;&#25512;&#29702;&#31561;&#20154;&#31867;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#38656;&#35201;&#21457;&#23637;&#20986;&#25490;&#38500;&#25968;&#25454;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16118</link><description>&lt;p&gt;
&#22238;&#24212;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Response: Emergent analogical reasoning in large language models. (arXiv:2308.16118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22238;&#24212;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23383;&#31526;&#20018;&#31867;&#27604;&#30340;&#21453;&#20363;&#26469;&#21453;&#39539;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;GPT-3&#26080;&#27861;&#35299;&#20915;&#26368;&#31616;&#21333;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#20026;&#20102;&#21152;&#24378;&#38646;&#28857;&#25512;&#29702;&#31561;&#20154;&#31867;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#38656;&#35201;&#21457;&#23637;&#20986;&#25490;&#38500;&#25968;&#25454;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#12298;&#33258;&#28982;&#20154;&#31867;&#34892;&#20026;&#12299;&#35770;&#25991;&#20013;&#65292;&#8220;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;&#8221;&#65288;Webb&#65292;Holyoak&#21644;Lu&#65292;2023&#65289;&#65292;&#20316;&#32773;&#20204;&#35748;&#20026;&#8220;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#33719;&#24471;&#20102;&#21457;&#29616;&#24191;&#27867;&#31867;&#27604;&#38382;&#39064;&#30340;&#38646;&#28857;&#35299;&#30340;&#32039;&#24613;&#33021;&#21147;&#8221;&#12290;&#22312;&#26412;&#22238;&#24212;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#23383;&#31526;&#20018;&#31867;&#27604;&#30340;&#21453;&#20363;&#12290;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#20013;&#65292;GPT-3&#29978;&#33267;&#26080;&#27861;&#35299;&#20915;&#21407;&#22987;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#26368;&#31616;&#21333;&#30340;&#21464;&#20307;&#38382;&#39064;&#12290;&#38646;&#28857;&#25512;&#29702;&#26159;&#19968;&#20010;&#38656;&#35201;&#38750;&#24120;&#20805;&#20998;&#35777;&#25454;&#25903;&#25345;&#30340;&#38750;&#20961;&#20027;&#24352;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27809;&#26377;&#30475;&#21040;&#36825;&#26679;&#30340;&#35777;&#25454;&#12290;&#20026;&#20102;&#21152;&#24378;&#20687;&#38646;&#28857;&#25512;&#29702;&#36825;&#26679;&#31867;&#20284;&#20154;&#31867;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#37325;&#35201;&#30340;&#26159;&#35813;&#39046;&#22495;&#24320;&#21457;&#20986;&#33021;&#22815;&#25490;&#38500;&#25968;&#25454;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In their recent Nature Human Behaviour paper, "Emergent analogical reasoning in large language models," (Webb, Holyoak, and Lu, 2023) the authors argue that "large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems." In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve even the easiest variants of the problems presented in the original paper. Zero-shot reasoning is an extraordinary claim that requires extraordinary evidence. We do not see that evidence in our experiments. To strengthen claims of humanlike reasoning such as zero-shot reasoning, it is important that the field develop approaches that rule out data memorization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#21453;&#39304;&#26469;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#20195;&#29702;&#33021;&#22815;&#20174;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.02151</link><description>&lt;p&gt;
Retroformer&#65306;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization. (arXiv:2308.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#21453;&#39304;&#26469;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#20195;&#29702;&#33021;&#22815;&#20174;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#26376;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26032;&#36235;&#21183;&#65292;&#21363;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#25104;&#33021;&#22815;&#33258;&#20027;&#23436;&#25104;&#30446;&#26631;&#23548;&#21521;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#35821;&#35328;&#20195;&#29702;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22238;&#31572;&#20154;&#31867;&#29992;&#25143;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#27809;&#26377;&#20351;&#29992;&#29615;&#22659;&#29305;&#23450;&#30340;&#22870;&#21169;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#19968;&#20123;&#20195;&#29702;&#36890;&#36807;&#21475;&#22836;&#21453;&#39304;&#23454;&#29616;&#20102;&#36845;&#20195;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20197;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#22870;&#21169;&#23398;&#20064;&#30456;&#20860;&#23481;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22238;&#39038;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#33258;&#21160;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#29615;&#22659;&#21453;&#39304;&#20013;&#20248;&#21270;&#20195;&#29702;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#26550;&#26500;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#30340;&#22870;&#21169;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#25311;&#31639;&#27861;&#65292;&#25104;&#21151;&#29983;&#25104;&#19982;&#23454;&#38469;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#30340;&#21402;&#20999;&#29255;CT&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23792;&#20540;&#20449;&#22122;&#27604;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#25311;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10182</link><description>&lt;p&gt;
&#36890;&#36807;&#30495;&#23454;&#21402;&#20999;&#29255;CT&#27169;&#25311;&#25913;&#36827;&#36229;&#20998;&#36776;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Enhancing Super-Resolution Networks through Realistic Thick-Slice CT Simulation. (arXiv:2307.10182v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#25311;&#31639;&#27861;&#65292;&#25104;&#21151;&#29983;&#25104;&#19982;&#23454;&#38469;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#30340;&#21402;&#20999;&#29255;CT&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23792;&#20540;&#20449;&#22122;&#27604;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#25311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#25311;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;AAPM-Mayo's 2016&#20302;&#21058;&#37327;CT&#22823;&#25361;&#25112;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#38469;&#22270;&#20687;&#23494;&#20999;&#30456;&#20284;&#30340;&#21402;&#20999;&#29255;CT&#22270;&#20687;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#20551;&#35774;&#25105;&#20204;&#30340;&#27169;&#25311;&#23558;&#20135;&#29983;&#19982;&#30495;&#23454;&#22270;&#20687;&#26356;&#19968;&#33268;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;PSNR&#21644;RMSE&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#26368;&#39640;PSNR&#20540;&#20026;D45&#21644;B30&#37325;&#24314;&#26680;&#20998;&#21035;&#20026;49.7369&#177;2.5223&#21644;48.5801&#177;7.3271&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#20197;0.0068&#177;0.0020&#21644;0.0108&#177;0.0099&#30340;RMSE&#20540;&#27880;&#20876;&#26368;&#20302;&#30340;&#35823;&#24046;&#65292;&#34920;&#26126;&#20854;&#20998;&#24067;&#26356;&#25509;&#36817;&#20110;&#30495;&#23454;&#30340;&#21402;&#20999;&#29255;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to develop and evaluate an innovative simulation algorithm for generating thick-slice CT images that closely resemble actual images in the AAPM-Mayo's 2016 Low Dose CT Grand Challenge dataset. The proposed method was evaluated using Peak Signal-to-Noise Ratio (PSNR) and Root Mean Square Error (RMSE) metrics, with the hypothesis that our simulation would produce images more congruent with their real counterparts. Our proposed method demonstrated substantial enhancements in terms of both PSNR and RMSE over other simulation methods. The highest PSNR values were obtained with the proposed method, yielding 49.7369 $\pm$ 2.5223 and 48.5801 $\pm$ 7.3271 for D45 and B30 reconstruction kernels, respectively. The proposed method also registered the lowest RMSE with values of 0.0068 $\pm$ 0.0020 and 0.0108 $\pm$ 0.0099 for D45 and B30, respectively, indicating a distribution more closely aligned with the authentic thick-slice image. Further validation of the proposed simulation al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20026;&#34920;&#26684;MDPs&#25512;&#23548;&#20986;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#30340;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.14063</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data. (arXiv:2306.14063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20026;&#34920;&#26684;MDPs&#25512;&#23548;&#20986;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#30340;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#31163;&#32447;RL&#26041;&#27861;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#29702;&#35770;&#20445;&#35777;&#26159;&#23454;&#29616;&#25968;&#25454;&#38656;&#27714;&#37327;&#36739;&#22823;&#30340;RL&#31639;&#27861;&#23454;&#38469;&#21487;&#34892;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#32467;&#26524;&#20381;&#36182;&#20110;&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#21363;&#21253;&#25324;&#19968;&#20010;&#30001;&#21333;&#19968;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;i.i.d.&#36712;&#36857;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#65292;&#21363;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#33258;&#36866;&#24212;&#25910;&#38598;&#30340;&#12290;&#25105;&#20204;&#20026;&#34920;&#26684;MDPs&#20013;&#30340;TMIS&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#20272;&#35745;&#22120;&#22312;&#36825;&#20010;&#24191;&#20041;&#35774;&#32622;&#20013;&#24320;&#21457;&#29702;&#35770;&#65292;&#25512;&#23548;&#20854;&#20272;&#35745;&#35823;&#24046;&#30340;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#36793;&#30028;&#12290;&#25105;&#20204;&#36824;&#22238;&#25910;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#27169;&#25311;&#65292;&#20197;&#32463;&#39564;&#20998;&#26512;&#36825;&#20123;&#20272;&#35745;&#22120;&#22312;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#27169;&#24335;&#19979;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing theoretical guarantees on the sample complexity of offline RL methods is an important step towards making data-hungry RL algorithms practically viable. Currently, most results hinge on unrealistic assumptions about the data distribution -- namely that it comprises a set of i.i.d. trajectories collected by a single logging policy. We consider a more general setting where the dataset may have been gathered adaptively. We develop theory for the TMIS Offline Policy Evaluation (OPE) estimator in this generalized setting for tabular MDPs, deriving high-probability, instance-dependent bounds on its estimation error. We also recover minimax-optimal offline learning in the adaptive setting. Finally, we conduct simulations to empirically analyze the behavior of these estimators under adaptive and non-adaptive regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2305.12081</link><description>&lt;p&gt;
AnyPredict: &#34920;&#26684;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#20027;&#35201;&#38382;&#39064;&#21253;&#25324; (1) &#32570;&#20047;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#24102;&#26377;&#26631;&#20934;&#26631;&#31614;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450; (2) &#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#25972;&#21512;&#34920;&#26684;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#27169;&#24335;&#34920;&#26684;&#20043;&#38388;&#30340;&#38556;&#30861;&#65292;&#24182;&#20351;&#29992;&#8220;&#23398;&#20064;&#65292;&#27880;&#37322;&#21644;&#23457;&#35745;&#8221;&#27969;&#31243;&#23558;&#39046;&#22495;&#22806;&#25968;&#25454;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#12290;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#20351;&#39044;&#35757;&#32451;&#30340; AnyPredict &#33021;&#22815;&#25903;&#25345;&#27599;&#20010;&#34920;&#26684;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
&lt;/p&gt;</description></item></channel></rss>