<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01879</link><description>&lt;p&gt;
VisualGPTScore: &#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#20998;&#25968;&#30340;&#35270;&#35273;&#35821;&#20041;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01879
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; VisualGPTScore &#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#26469;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#19978;&#36816;&#31639;&#12290;&#19982;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#30340;VLM&#21482;&#26159;&#26080;&#24847;&#20041;&#30340;&#21333;&#35789;&#34955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340; VisualGPTScore &#22312; ARO &#21644; Crepe &#31561;&#26368;&#36817;&#25552;&#20986;&#30340;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#39030;&#23574;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$ have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the ${\bf V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal generative}$ score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text) and the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#23558;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#36866;&#24212;&#20110;&#20855;&#26377;&#26377;&#38480;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Video Adapter&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;&#20989;&#25968;&#20316;&#20026;&#27010;&#29575;&#20808;&#39564;&#65292;&#26469;&#25351;&#23548;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#23567;&#22411;&#35270;&#39057;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01872</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#30340;&#27010;&#29575;&#33258;&#36866;&#24212;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Adaptation of Text-to-Video Models. (arXiv:2306.01872v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#23558;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#36866;&#24212;&#20110;&#20855;&#26377;&#26377;&#38480;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Video Adapter&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;&#20989;&#25968;&#20316;&#20026;&#27010;&#29575;&#20808;&#39564;&#65292;&#26469;&#25351;&#23548;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#23567;&#22411;&#35270;&#39057;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#20174;&#20219;&#24847;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#39640;&#20445;&#30495;&#35270;&#39057;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#20294;&#23558;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#20110;&#20855;&#26377;&#26377;&#38480;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#21160;&#30011;&#25110;&#26426;&#22120;&#20154;&#35270;&#39057;&#65292;&#20250;&#24102;&#26469;&#37325;&#22823;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#22240;&#20026;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#22823;&#27169;&#22411;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#12290;&#22312;&#33719;&#24471;&#28789;&#24863;&#20110;&#23567;&#22411;&#21487;&#20462;&#25913;&#32452;&#20214;&#65288;&#20363;&#22914;&#25552;&#31034;&#35821;&#12289;&#21069;&#32512;&#35843;&#25972;&#65289;&#22914;&#20309;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25191;&#34892;&#26032;&#20219;&#21153;&#32780;&#26080;&#38656;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#36866;&#24212;&#20110;&#21508;&#31181;&#19979;&#28216;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;&#22312;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Video Adapter&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;&#20989;&#25968;&#20316;&#20026;&#27010;&#29575;&#20808;&#39564;&#26469;&#25351;&#23548;&#20219;&#21153;&#29305;&#23450;&#23567;&#22411;&#35270;&#39057;&#27169;&#22411;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Video Adapter&#33021;&#22815;&#38598;&#25104;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#22810;&#31181;&#35270;&#39057;&#39046;&#22495;&#21644;&#20219;&#21153;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large text-to-video models trained on internet-scale data have demonstrated exceptional capabilities in generating high-fidelity videos from arbitrary textual descriptions. However, adapting these models to tasks with limited domain-specific data, such as animation or robotics videos, poses a significant computational challenge, since finetuning a pretrained large model can be prohibitively expensive. Inspired by how a small modifiable component (e.g., prompts, prefix-tuning) can adapt a large language model to perform new tasks without requiring access to the model weights, we investigate how to adapt a large pretrained text-to-video model to a variety of downstream domains and tasks without finetuning. In answering this question, we propose Video Adapter, which leverages the score function of a large pretrained video diffusion model as a probabilistic prior to guide the generation of a task-specific small video model. Our experiments show that Video Adapter is capable of incorporatin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37325;&#22797;&#25293;&#21334;&#35774;&#32622;&#30340;&#23545;&#20598;&#21453;&#39304;&#26426;&#21046;&#65292;&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26469;&#20174;&#31454;&#26631;&#32773;&#37027;&#37324;&#33719;&#21462;&#20449;&#24687;&#65292;&#36991;&#20813;&#20102;&#20043;&#21069;&#30340;&#23398;&#20064;&#20986;&#20215;&#38382;&#39064;&#65292;&#35813;&#26426;&#21046;&#34987;&#35777;&#26126;&#20026;&#28176;&#36817;&#35802;&#23454;&#12289;&#20010;&#20307;&#29702;&#24615;&#12289;&#31119;&#21033;&#21644;&#25910;&#30410;&#26368;&#22823;&#21270;&#30340;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#23450;&#21046;&#29983;&#20135;&#30340;&#21830;&#21697;&#25293;&#21334;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.01860</link><description>&lt;p&gt;
&#26080;&#31454;&#26631;&#65292;&#26080;&#36951;&#25022;&#65306;&#38024;&#23545;&#25968;&#23383;&#21830;&#21697;&#21644;&#25968;&#25454;&#25293;&#21334;&#30340;&#23545;&#20598;&#21453;&#39304;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
No Bidding, No Regret: Pairwise-Feedback Mechanisms for Digital Goods and Data Auctions. (arXiv:2306.01860v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37325;&#22797;&#25293;&#21334;&#35774;&#32622;&#30340;&#23545;&#20598;&#21453;&#39304;&#26426;&#21046;&#65292;&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26469;&#20174;&#31454;&#26631;&#32773;&#37027;&#37324;&#33719;&#21462;&#20449;&#24687;&#65292;&#36991;&#20813;&#20102;&#20043;&#21069;&#30340;&#23398;&#20064;&#20986;&#20215;&#38382;&#39064;&#65292;&#35813;&#26426;&#21046;&#34987;&#35777;&#26126;&#20026;&#28176;&#36817;&#35802;&#23454;&#12289;&#20010;&#20307;&#29702;&#24615;&#12289;&#31119;&#21033;&#21644;&#25910;&#30410;&#26368;&#22823;&#21270;&#30340;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#23450;&#21046;&#29983;&#20135;&#30340;&#21830;&#21697;&#25293;&#21334;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#21644; AI &#29983;&#25104;&#25968;&#23383;&#21830;&#21697;&#65288;&#22914;&#20010;&#24615;&#21270;&#20070;&#38754;&#20869;&#23481;&#21644;&#33402;&#26415;&#21697;&#65289;&#30340;&#38656;&#27714;&#22686;&#38271;&#65292;&#38656;&#35201;&#26377;&#25928;&#23450;&#20215;&#21644;&#21453;&#39304;&#26426;&#21046;&#26469;&#32771;&#34385;&#19981;&#30830;&#23450;&#30340;&#25928;&#29992;&#21644;&#26114;&#36149;&#30340;&#29983;&#20135;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#35774;&#35745;&#65292;&#36866;&#29992;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#37325;&#22797;&#25293;&#21334;&#35774;&#32622;&#65292;&#20854;&#20013;&#21806;&#20986;&#21830;&#21697;&#30340;&#25928;&#29992;&#22312;&#38144;&#21806;&#21518;&#25581;&#31034;&#12290;&#35813;&#26426;&#21046;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26469;&#20174;&#31454;&#26631;&#32773;&#37027;&#37324;&#33719;&#21462;&#20449;&#24687;&#65292;&#30456;&#23545;&#20110;&#25351;&#23450;&#25968;&#37327;&#20540;&#26469;&#35828;&#23545;&#20154;&#31867;&#26356;&#23481;&#26131;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#26426;&#21046;&#20351;&#29992; epsilon-greedy &#31574;&#30053;&#36873;&#25321;&#20998;&#37197;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#24050;&#20998;&#37197;&#21830;&#21697;&#30340;&#23454;&#29616;&#25928;&#29992;&#21644;&#20219;&#24847;&#20540;&#20043;&#38388;&#30340;&#25104;&#23545;&#27604;&#36739;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#23398;&#20064;&#20986;&#20215;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26426;&#21046;&#26159;&#28176;&#36817;&#35802;&#23454;&#12289;&#20010;&#20307;&#29702;&#24615;&#12289;&#31119;&#21033;&#21644;&#25910;&#30410;&#26368;&#22823;&#21270;&#30340;&#12290;&#35813;&#26426;&#21046;&#36866;&#29992;&#33539;&#22260;&#24191;&#27867;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#23450;&#21046;&#29983;&#20135;&#12289;&#27809;&#26377;&#22266;&#23450;&#20215;&#26684;&#30340;&#21830;&#21697;&#25110;&#26381;&#21153;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing demand for data and AI-generated digital goods, such as personalized written content and artwork, necessitates effective pricing and feedback mechanisms that account for uncertain utility and costly production. Motivated by these developments, this study presents a novel mechanism design addressing a general repeated-auction setting where the utility derived from a sold good is revealed post-sale. The mechanism's novelty lies in using pairwise comparisons for eliciting information from the bidder, arguably easier for humans than assigning a numerical value. Our mechanism chooses allocations using an epsilon-greedy strategy and relies on pairwise comparisons between realized utility from allocated goods and an arbitrary value, avoiding the learning-to-bid problem explored in previous work. We prove this mechanism to be asymptotically truthful, individually rational, and welfare and revenue maximizing. The mechanism's relevance is broad, applying to any setting with made-to-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BLEEP&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26500;&#24314;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;&#33021;&#22815;&#20174;H&amp;E&#26579;&#33394;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#29983;&#25104;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22522;&#22240;&#34920;&#36798;&#35889;&#22320;&#22270;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01859</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#30340;H&amp;E&#32452;&#32455;&#23398;&#22270;&#20687;&#22522;&#22240;&#34920;&#36798;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatially Resolved Gene Expression Prediction from H&amp;E Histology Images via Bi-modal Contrastive Learning. (arXiv:2306.01859v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BLEEP&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26500;&#24314;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;&#33021;&#22815;&#20174;H&amp;E&#26579;&#33394;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#29983;&#25104;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22522;&#22240;&#34920;&#36798;&#35889;&#22320;&#22270;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#23398;&#25104;&#20687;&#26159;&#21307;&#23398;&#35786;&#26029;&#21644;&#30740;&#31350;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#33021;&#22815;&#22312;&#24494;&#35266;&#27700;&#24179;&#19978;&#26816;&#26597;&#32452;&#32455;&#32467;&#26500;&#21644;&#32452;&#25104;&#12290;&#20102;&#35299;&#32452;&#32455;&#32467;&#26500;&#30340;&#22522;&#26412;&#20998;&#23376;&#26426;&#21046;&#23545;&#25581;&#31034;&#30142;&#30149;&#26426;&#21046;&#21644;&#24320;&#21457;&#26377;&#25928;&#27835;&#30103;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#22240;&#34920;&#36798;&#35889;&#25552;&#20379;&#20102;&#28145;&#20837;&#20102;&#35299;&#32452;&#32455;&#32467;&#26500;&#32972;&#21518;&#20998;&#23376;&#36807;&#31243;&#30340;&#35270;&#35282;&#65292;&#20294;&#36825;&#19968;&#36807;&#31243;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLEEP&#65288;Bi-modaL Embedding for Expression Prediction&#65289;&#30340;&#21452;&#27169;&#24335;&#23884;&#20837;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#20840;&#24133;&#33487;&#26408;&#31934;-&#20234;&#32418;&#65288;H&amp;E&#65289;&#26579;&#33394;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#29983;&#25104;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22522;&#22240;&#34920;&#36798;&#35889;&#22270;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22312;&#24494;&#31859;&#20998;&#36776;&#29575;&#19979;&#20351;&#29992;&#25104;&#23545;&#30340;&#22270;&#20687;&#21644;&#34920;&#36798;&#35889;&#26469;&#26500;&#24314;&#20302;&#32500;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#30340;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#21487;&#29992;&#21608;&#22260;&#22270;&#20687;&#30340;&#32972;&#26223;&#19978;&#19979;&#25991;&#25512;&#26029;&#20986;&#20219;&#20309;&#26597;&#35810;&#22270;&#20687;&#34917;&#19969;&#30340;&#22522;&#22240;&#34920;&#36798;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22522;&#22240;&#34920;&#36798;&#35889;&#22320;&#22270;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#32452;&#32455;&#31867;&#22411;&#19978;&#23637;&#31034;&#20102;BLEEP&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31454;&#20105;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Histology imaging is an important tool in medical diagnosis and research, enabling the examination of tissue structure and composition at the microscopic level. Understanding the underlying molecular mechanisms of tissue architecture is critical in uncovering disease mechanisms and developing effective treatments. Gene expression profiling provides insight into the molecular processes underlying tissue architecture, but the process can be time-consuming and expensive. In this study, we present BLEEP (Bi-modaL Embedding for Expression Prediction), a bi-modal embedding framework capable of generating spatially resolved gene expression profiles of whole-slide Hematoxylin and eosin (H&amp;E) stained histology images. BLEEP uses a contrastive learning framework to construct a low-dimensional joint embedding space from a reference dataset using paired image and expression profiles at micrometer resolution. With this framework, the gene expression of any query image patch can be imputed using the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#26597;&#35810;&#37325;&#20889;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#20307;&#31995;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#27861;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21644;&#20462;&#22797;&#36825;&#20116;&#20010;&#20219;&#21153;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#23427;&#20204;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.01855</link><description>&lt;p&gt;
5IDER: &#32479;&#19968;&#26597;&#35810;&#37325;&#20889;&#25216;&#26415;&#29992;&#20110;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#35328;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21450;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair. (arXiv:2306.01855v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#26597;&#35810;&#37325;&#20889;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#20307;&#31995;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#27861;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21644;&#20462;&#22797;&#36825;&#20116;&#20010;&#20219;&#21153;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#23427;&#20204;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#35821;&#38899;&#21161;&#25163;&#23548;&#33322;&#22810;&#36718;&#23545;&#35805;&#30340;&#33021;&#21147;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290; &#22788;&#29702;&#22810;&#36718;&#20114;&#21160;&#38656;&#35201;&#31995;&#32479;&#29702;&#35299;&#21508;&#31181;&#20250;&#35805;&#29992;&#20363;&#65292;&#22914;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#35328;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21644;&#20462;&#22797;&#12290; &#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21152;&#21095;&#20102;&#36825;&#20123;&#29992;&#20363;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#20107;&#23454;&#65292;&#36890;&#24120;&#21516;&#26102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#20986;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#26597;&#35810;&#37325;&#20889;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#20116;&#20010;&#20219;&#21153;&#20197;&#21450;&#36825;&#20123;&#29992;&#20363;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#21333;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#29992;&#20363;&#32452;&#21512;&#26041;&#38754;&#29978;&#33267;&#20248;&#20110;&#32463;&#36807;&#35843;&#20248;&#30340;T5&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#21442;&#25968;&#19978;&#23567;15&#20493;&#65292;&#22312;&#24310;&#36831;&#19978;&#24555;25&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing voice assistants the ability to navigate multi-turn conversations is a challenging problem. Handling multi-turn interactions requires the system to understand various conversational use-cases, such as steering, intent carryover, disfluencies, entity carryover, and repair. The complexity of this problem is compounded by the fact that these use-cases mix with each other, often appearing simultaneously in natural language. This work proposes a non-autoregressive query rewriting architecture that can handle not only the five aforementioned tasks, but also complex compositions of these use-cases. We show that our proposed model has competitive single task performance compared to the baseline approach, and even outperforms a fine-tuned T5 model in use-case compositions, despite being 15 times smaller in parameters and 25 times faster in latency.
&lt;/p&gt;</description></item><item><title>Counterfactual World Modeling (CWM)&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#32479;&#19968;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#38459;&#30861;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#39046;&#22495;&#24212;&#29992;&#30340;&#38382;&#39064;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.01828</link><description>&lt;p&gt;
&#22522;&#20110;&#21453;&#20107;&#23454;&#19990;&#30028;&#24314;&#27169;&#23454;&#29616;&#26426;&#22120;&#35270;&#35273;&#30340;&#32479;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unifying (Machine) Vision via Counterfactual World Modeling. (arXiv:2306.01828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01828
&lt;/p&gt;
&lt;p&gt;
Counterfactual World Modeling (CWM)&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#32479;&#19968;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#38459;&#30861;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#39046;&#22495;&#24212;&#29992;&#30340;&#38382;&#39064;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#20027;&#35201;&#26041;&#27861;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#20351;&#29992;&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#22312;&#26114;&#36149;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#25233;&#21046;&#20102;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#20013;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#24378;&#20581;&#30340;&#20219;&#21153;&#36890;&#29992;&#24863;&#30693;&#20173;&#28982;&#26159;&#19968;&#20010;&#29942;&#39048;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#33258;&#28982;&#35821;&#35328;&#30340;&#8220;&#22522;&#30784;&#27169;&#22411;&#8221;&#24050;&#32463;&#35777;&#26126;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#25552;&#20379;&#24191;&#27867;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#65292;&#24212;&#29992;&#20110;&#34920;&#38754;&#19978;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#8220;&#21453;&#20107;&#23454;&#19990;&#30028;&#24314;&#27169;&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#26080;&#30417;&#30563;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#31034;&#25191;&#34892;&#21508;&#31181;&#35270;&#35273;&#35745;&#31639;&#12290;CWM&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#38459;&#30861;&#23558;&#22522;&#30784;&#27169;&#22411;&#27010;&#24565;&#24212;&#29992;&#20110;&#35270;&#35273;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#26159;&#32467;&#26500;&#21270;&#25513;&#27169;&#65292;&#36825;&#26159;&#36974;&#34109;&#39044;&#27979;&#26041;&#27861;&#30340;&#25512;&#24191;&#65292;&#40723;&#21169;&#39044;&#27979;&#27169;&#22411;&#25429;&#25417;&#35270;&#35273;&#25968;&#25454;&#20013;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#20174;&#32780;&#20998;&#35299;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#31532;&#20108;&#20010;&#26159;&#21453;&#20107;&#23454;&#23398;&#20064;&#65292;&#20854;&#20013;&#35270;&#35273;&#32534;&#30721;&#22120;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#19982;&#36755;&#20837;&#25511;&#21046;&#20559;&#24046;&#30340;&#25968;&#25454;&#12290;&#36825;&#40723;&#21169;&#27169;&#22411;&#33719;&#24471;&#31934;&#30830;&#30340;&#65292;&#32467;&#26500;&#21270;&#30340;&#35270;&#35273;&#19990;&#30028;&#34920;&#31034;&#65292;&#20026;&#26032;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CWM&#33021;&#22815;&#22788;&#29702;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20174;&#39044;&#27979;3D&#23545;&#35937;&#30340;&#23646;&#24615;&#21040;&#26816;&#27979;&#21644;&#23450;&#20301;&#22270;&#20687;&#20013;&#30340;&#26032;&#39062;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leading approaches in machine vision employ different architectures for different tasks, trained on costly task-specific labeled datasets. This complexity has held back progress in areas, such as robotics, where robust task-general perception remains a bottleneck. In contrast, "foundation models" of natural language have shown how large pre-trained neural networks can provide zero-shot solutions to a broad spectrum of apparently distinct tasks. Here we introduce Counterfactual World Modeling (CWM), a framework for constructing a visual foundation model: a unified, unsupervised network that can be prompted to perform a wide variety of visual computations. CWM has two key components, which resolve the core issues that have hindered application of the foundation model concept to vision. The first is structured masking, a generalization of masked prediction methods that encourages a prediction model to capture the low-dimensional structure in visual data. The model thereby factors the key 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#24191;&#27867;&#20351;&#29992;&#30340;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#35821;&#35328;&#65292;&#20197;&#22522;&#20110;&#23427;&#20204;&#30340;&#25216;&#26415;&#21644;&#29615;&#22659;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.01819</link><description>&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#35821;&#35328;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Widely use Object-Oriented Languages. (arXiv:2306.01819v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#24191;&#27867;&#20351;&#29992;&#30340;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#35821;&#35328;&#65292;&#20197;&#22522;&#20110;&#23427;&#20204;&#30340;&#25216;&#26415;&#21644;&#29615;&#22659;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#31243;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#31185;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#32534;&#31243;&#29615;&#22659;&#19981;&#20165;&#22312;&#36805;&#36895;&#22686;&#38271;&#65292;&#32780;&#19988;&#20063;&#22312;&#19981;&#26029;&#21464;&#21270;&#65292;&#32534;&#31243;&#35821;&#35328;&#20063;&#22312;&#19981;&#26029;&#28436;&#36827;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#19987;&#19994;&#20013;&#65292;&#23398;&#20064;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#33539;&#24335;&#26159;&#24517;&#20462;&#35838;&#31243;&#65292;&#22240;&#27492;&#36873;&#25321;&#29992;&#21738;&#31181;&#35821;&#35328;&#26469;&#25945;&#25480;&#38754;&#21521;&#23545;&#35937;&#21407;&#29702;&#38750;&#24120;&#37325;&#35201;&#12290;&#30001;&#20110;&#22823;&#37327;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#35821;&#35328;&#65292;&#38750;&#24120;&#38590;&#20197;&#36873;&#25321;&#21738;&#19968;&#31181;&#24212;&#35813;&#26159;&#25945;&#25480;&#38754;&#21521;&#23545;&#35937;&#21407;&#29702;&#30340;&#39318;&#36873;&#32534;&#31243;&#35821;&#35328;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20986;&#21738;&#31181;&#35821;&#35328;&#24212;&#35813;&#26159;&#25945;&#25480;&#38754;&#21521;&#23545;&#35937;&#27010;&#24565;&#30340;&#39318;&#36873;&#35821;&#35328;&#65292;&#20294;&#26159;&#36824;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#26469;&#27604;&#36739;&#21644;&#35780;&#20272;&#36825;&#20123;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#24191;&#27867;&#20351;&#29992;&#30340;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#35821;&#35328;&#12290;&#36825;&#20123;&#35821;&#35328;&#26159;&#22522;&#20110;&#23427;&#20204;&#30340;&#25216;&#26415;&#21644;&#29615;&#22659;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programming is an integral part of computer science discipline. Every day the programming environment is not only rapidly growing but also changing and languages are constantly evolving. Learning of object-oriented paradigm is compulsory in every computer science major so the choice of language to teach object-oriented principles is very important. Due to large pool of object-oriented languages, it is difficult to choose which should be the first programming language in order to teach object-oriented principles. Many studies shown which should be the first language to tech object-oriented concepts but there is no method to compare and evaluate these languages. In this article we proposed a comprehensive framework to evaluate the widely used object-oriented languages. The languages are evaluated basis of their technical and environmental features.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#24555;&#36895;&#12289;&#20415;&#23452;&#12289;&#26080;&#38656;&#31227;&#21160;&#35774;&#22791;&#22320;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#12290;</title><link>http://arxiv.org/abs/2306.01818</link><description>&lt;p&gt;
Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#26816;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beta Thalassemia Carriers detection empowered federated Learning. (arXiv:2306.01818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#24555;&#36895;&#12289;&#20415;&#23452;&#12289;&#26080;&#38656;&#31227;&#21160;&#35774;&#22791;&#22320;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#20013;&#28023;&#36139;&#34880;&#26159;&#19968;&#32452;&#36951;&#20256;&#24615;&#34880;&#28082;&#30142;&#30149;&#65292;&#24403;&#25658;&#24102;&#36755;&#27687;&#33267;&#36523;&#20307;&#21508;&#22788;&#30340;&#32418;&#32454;&#32990;&#20013;&#30340;&#34507;&#30333;&#36136;&#34880;&#32418;&#34507;&#30333;&#19981;&#36275;&#26102;&#20250;&#21457;&#29983;&#12290;&#22914;&#26524;&#29238;&#27597;&#21452;&#26041;&#37117;&#25658;&#24102;&#22320;&#20013;&#28023;&#36139;&#34880;&#22522;&#22240;&#65292;&#23401;&#23376;&#24739;&#30149;&#30340;&#20960;&#29575;&#20250;&#22686;&#21152;&#12290;&#30830;&#35786;&#21644;&#27835;&#30103;&#22320;&#20013;&#28023;&#36139;&#34880;&#26159;&#38450;&#27490;&#20854;&#20256;&#36882;&#32473;&#19979;&#19968;&#20195;&#30340;&#20851;&#38190;&#12290;&#30446;&#21069;&#30340;&#34880;&#28082;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#36807;&#20110;&#26114;&#36149;&#12289;&#32791;&#26102;&#65292;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#31579;&#26597;&#35774;&#22791;&#12290;&#39640;&#25928;&#28082;&#30456;&#33394;&#35889;&#26159;&#26631;&#20934;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20063;&#23384;&#22312;&#25104;&#26412;&#39640;&#12289;&#26102;&#38388;&#38271;&#12289;&#38656;&#35201;&#29305;&#27530;&#35774;&#22791;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23547;&#25214;&#19968;&#31181;&#24555;&#36895;&#12289;&#20415;&#23452;&#30340;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#30340;&#26041;&#27861;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thalassemia is a group of inherited blood disorders that happen when hemoglobin, the protein in red blood cells that carries oxygen, is not made enough. It is found all over the body and is needed for survival. If both parents have thalassemia, a child's chance of getting it increases. Genetic counselling and early diagnosis are essential for treating thalassemia and stopping it from being passed on to future generations. It may be hard for healthcare professionals to differentiate between people with thalassemia carriers and those without. The current blood tests for beta thalassemia carriers are too expensive, take too long, and require too much screening equipment. The World Health Organization says there is a high death rate for people with thalassemia. Therefore, it is essential to find thalassemia carriers to act quickly. High-performance liquid chromatography (HPLC), the standard test method, has problems such as cost, time, and equipment needs. So, there must be a quick and che
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21306;&#22359;&#38142;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#24515;&#33039;&#30142;&#30149;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26550;&#26500;&#19979;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#30340;&#19981;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#26377;&#26395;&#20248;&#21270;&#24515;&#33039;&#19987;&#19994;&#20154;&#22763;&#23545;&#20110;&#24515;&#33039;&#30149;&#26089;&#26399;&#35786;&#26029;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01817</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22359;&#38142;&#21644;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#24515;&#33039;&#30142;&#30149;
&lt;/p&gt;
&lt;p&gt;
Heart Diseases Prediction Using Block-chain and Machine Learning. (arXiv:2306.01817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21306;&#22359;&#38142;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#24515;&#33039;&#30142;&#30149;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26550;&#26500;&#19979;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#30340;&#19981;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#26377;&#26395;&#20248;&#21270;&#24515;&#33039;&#19987;&#19994;&#20154;&#22763;&#23545;&#20110;&#24515;&#33039;&#30149;&#26089;&#26399;&#35786;&#26029;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#22823;&#37096;&#20998;&#20154;&#27515;&#20110;&#24515;&#33039;&#30142;&#30149;&#12290;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#21307;&#30103;&#37096;&#38376;&#23578;&#26410;&#24314;&#31435;&#23433;&#20840;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#22522;&#30784;&#35774;&#26045;&#12290;&#30001;&#20110;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#24615;&#65292;&#24515;&#33039;&#19987;&#19994;&#20154;&#22763;&#24456;&#38590;&#26089;&#26399;&#39044;&#27979;&#30142;&#30149;&#12290;&#36825;&#31181;&#24555;&#36895;&#22686;&#21152;&#30340;&#27515;&#20129;&#29575;&#21487;&#20197;&#36890;&#36807;&#30417;&#27979;&#21644;&#28040;&#38500;&#26089;&#26399;&#20986;&#29616;&#30340;&#19968;&#20123;&#20851;&#38190;&#22240;&#32032;&#65288;&#20363;&#22914;&#34880;&#21387;&#12289;&#32966;&#22266;&#37255;&#27700;&#24179;&#12289;&#20307;&#37325;&#21644;&#21560;&#28895;&#25104;&#30270;&#65289;&#26469;&#25511;&#21046;&#12290;&#22312;&#21307;&#30103;&#37096;&#38376;&#20013;&#65292;&#24515;&#33039;&#19987;&#19994;&#20154;&#22763;&#65288;Cp&#65289;&#21487;&#20197;&#20351;&#29992;&#20808;&#36827;&#30340;&#31995;&#32479;&#26469;&#30417;&#25511;&#24739;&#32773;&#25968;&#25454;&#65292;&#20854;&#20013;&#21306;&#22359;&#38142;&#26159;&#26368;&#21487;&#38752;&#30340;&#25552;&#20379;&#32773;&#20043;&#19968;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#22788;&#29702;&#30142;&#30149;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#65292;&#31216;&#20026;&#27491;&#24358;&#20313;&#24358;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;Sine-Cosine SVM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most people around the globe are dying due to heart disease. The main reason behind the rapid increase in the death rate due to heart disease is that there is no infrastructure developed for the healthcare department that can provide a secure way of data storage and transmission. Due to redundancy in the patient data, it is difficult for cardiac Professionals to predict the disease early on. This rapid increase in the death rate due to heart disease can be controlled by monitoring and eliminating some of the key attributes in the early stages such as blood pressure, cholesterol level, body weight, and addiction to smoking. Patient data can be monitored by cardiac Professionals (Cp) by using the advanced framework in the healthcare departments. Blockchain is the world's most reliable provider. The use of advanced systems in the healthcare departments providing new ways of dealing with diseases has been developed as well. In this article Machine Learning (ML) algorithm known as a sine-co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#39044;&#27979;&#26577;&#27224;&#30149;&#23475;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#21644;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#38450;&#27835;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01816</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#26577;&#27224;&#30149;&#23475;&#65306;&#20998;&#31867;&#22120;&#12289;&#27169;&#22411;&#21644;SLR
&lt;/p&gt;
&lt;p&gt;
Prediction of Citrus Diseases Using Machine Learning And Deep Learning: Classifier, Models SLR. (arXiv:2306.01816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#39044;&#27979;&#26577;&#27224;&#30149;&#23475;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#21644;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#38450;&#27835;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#20197;&#26469;&#65292;&#26577;&#27224;&#30149;&#23475;&#19968;&#30452;&#26159;&#20840;&#29699;&#26577;&#27224;&#31181;&#26893;&#19994;&#30340;&#20027;&#35201;&#38382;&#39064;&#65292;&#23427;&#20204;&#20250;&#26174;&#33879;&#38477;&#20302;&#27700;&#26524;&#30340;&#36136;&#37327;&#12290;&#26368;&#21361;&#38505;&#30340;&#26577;&#27224;&#30149;&#23475;&#21253;&#25324;&#26577;&#27224;&#28291;&#30113;&#30149;&#12289;&#26577;&#27224;&#40644;&#40857;&#30149;&#12289;&#26577;&#27224;&#40657;&#26001;&#30149;&#12289;&#26577;&#27224;&#21494;&#34558;&#65292;&#36825;&#20123;&#30149;&#23475;&#21487;&#23548;&#33268;&#20840;&#29699;&#26577;&#27224;&#20135;&#19994;&#20986;&#29616;&#26174;&#33879;&#32463;&#27982;&#25439;&#22833;&#65292;&#38450;&#27835;&#31574;&#30053;&#21253;&#25324;&#21270;&#23398;&#27835;&#30103;&#31561;&#12290;&#26577;&#27224;&#30149;&#23475;&#20998;&#24067;&#22312;&#20840;&#29699;&#25152;&#26377;&#26577;&#27224;&#31181;&#26893;&#22320;&#21306;&#65292;&#24433;&#21709;&#26577;&#27224;&#26641;&#26681;&#12289;&#26577;&#27224;&#26641;&#21494;&#12289;&#26577;&#27224;&#31561;&#27700;&#26524;&#12290;&#26577;&#27224;&#30149;&#23475;&#30340;&#23384;&#22312;&#23545;&#32463;&#27982;&#22240;&#32032;&#20135;&#29983;&#39640;&#24230;&#24433;&#21709;&#65292;&#21516;&#26102;&#20063;&#20250;&#20135;&#29983;&#20302;&#21697;&#36136;&#27700;&#26524;&#65292;&#22686;&#21152;&#30149;&#23475;&#31649;&#29702;&#36153;&#29992;&#12290;&#21355;&#29983;&#21644;&#23450;&#26399;&#30417;&#27979;&#21487;&#20197;&#26377;&#25928;&#31649;&#29702;&#26576;&#20123;&#26577;&#27224;&#30149;&#23475;&#65292;&#20294;&#20854;&#20182;&#30149;&#23475;&#21487;&#33021;&#38656;&#35201;&#26356;&#21152;&#23494;&#38598;&#30340;&#27835;&#30103;&#65292;&#20363;&#22914;&#21270;&#23398;&#25110;&#29983;&#29289;&#25511;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Citrus diseases have been major issues for citrus growing worldwide for many years they can lead significantly reduce fruit quality. the most harmful citrus diseases are citrus canker, citrus greening, citrus black spot, citrus leaf miner which can have significant economic losses of citrus industry in worldwide prevention and management strategies like chemical treatments. Citrus diseases existing in all over the world where citrus is growing its effects the citrus tree root, citrus tree leaf, citrus tree orange etc. Existing of citrus diseases is highly impact on economic factor that can also produce low quality fruits and increased the rate for diseases management. Sanitation and routine monitoring can be effective in managing certain citrus diseases, but others may require more intensive treatments like chemical or biological control methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;-&#26657;&#27491;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#23545;&#25239;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26799;&#24230;&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01809</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;-&#26657;&#27491;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attack Based on Prediction-Correction. (arXiv:2306.01809v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;-&#26657;&#27491;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#23545;&#25239;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26799;&#24230;&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#23558;&#24494;&#23567;&#30340;&#25200;&#21160;&#28155;&#21152;&#21040;&#21407;&#26412;&#30340;&#26679;&#26412;&#20013;&#12290;&#29616;&#26377;&#25915;&#20987;&#26041;&#27861;&#20013;&#65292;&#28155;&#21152;&#30340;&#25200;&#21160;&#20027;&#35201;&#30001;&#25439;&#22833;&#20989;&#25968;&#23545;&#36755;&#20837;&#30340;&#26799;&#24230;&#20915;&#23450;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#26799;&#24230;&#25915;&#20987;&#19982;&#27714;&#35299;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243; (ODE) &#25968;&#20540;&#26041;&#27861;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#12290;&#21463;ODE&#25968;&#20540;&#35299;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#27979;-&#26657;&#27491;(PC)&#30340;&#26032;&#22411;&#23545;&#25239;&#25915;&#20987;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;PC-based&#25915;&#20987;&#20013;&#65292;&#21487;&#20197;&#20808;&#36873;&#25321;&#19968;&#20123;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#29983;&#25104;&#19968;&#20010;&#39044;&#27979;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#23558;&#39044;&#27979;&#26679;&#26412;&#21644;&#24403;&#21069;&#26679;&#26412;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#30830;&#23450;&#25152;&#28155;&#21152;&#30340;&#25200;&#21160;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#25152;&#26377;&#21487;&#29992;&#30340;&#26799;&#24230;&#25915;&#20987;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26799;&#24230;&#23545;&#25239;&#25915;&#20987;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#27604;&#36739;&#36827;&#34892;&#21387;&#32553;&#21644;&#21152;&#36895;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to adversarial examples obtained by adding small perturbations to original examples. The added perturbations in existing attacks are mainly determined by the gradient of the loss function with respect to the inputs. In this paper, the close relationship between gradient-based attacks and the numerical methods for solving ordinary differential equation (ODE) is studied for the first time. Inspired by the numerical solution of ODE, a new prediction-correction (PC) based adversarial attack is proposed. In our proposed PC-based attack, some existing attack can be selected to produce a predicted example first, and then the predicted example and the current example are combined together to determine the added perturbations. The proposed method possesses good extensibility and can be applied to all available gradient-based attacks easily. Extensive experiments demonstrate that compared with the state-of-the-art gradient-based adversarial attacks, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#38134;&#34892;&#19994;&#30340;&#35789;&#23884;&#20837;&#65292;&#21457;&#29616;&#20854;&#27604;&#24191;&#27867;&#21487;&#29992;&#30340;&#35789;&#23884;&#20837;&#26356;&#33021;&#25429;&#25417;&#38134;&#34892;&#29305;&#23450;&#35821;&#20041;&#21644;&#35789;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#21487;&#20316;&#20026;NL&#20219;&#21153;&#30340;&#19968;&#20010;&#33391;&#22909;&#29420;&#31435;&#26469;&#28304;&#25110;&#32773;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2306.01807</link><description>&lt;p&gt;
&#38754;&#21521;&#38134;&#34892;&#19994;&#30340;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Word Embeddings for Banking Industry. (arXiv:2306.01807v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#38134;&#34892;&#19994;&#30340;&#35789;&#23884;&#20837;&#65292;&#21457;&#29616;&#20854;&#27604;&#24191;&#27867;&#21487;&#29992;&#30340;&#35789;&#23884;&#20837;&#26356;&#33021;&#25429;&#25417;&#38134;&#34892;&#29305;&#23450;&#35821;&#20041;&#21644;&#35789;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#21487;&#20316;&#20026;NL&#20219;&#21153;&#30340;&#19968;&#20010;&#33391;&#22909;&#29420;&#31435;&#26469;&#28304;&#25110;&#32773;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#24191;&#27867;&#65292;&#20174;&#24773;&#24863;&#20998;&#26512;&#21040;&#25991;&#26412;&#20998;&#31867;&#12290;&#20174;&#38745;&#24577;&#35789;&#23884;&#20837;&#65288;&#22914;Word2Vec&#25110;GloVe&#65289;&#21040;&#19978;&#19979;&#25991;&#27169;&#22411;&#25552;&#21462;&#30340;&#38745;&#24577;&#35789;&#34920;&#24449;&#65288;&#22914;BERT&#25110;ELMo&#65289;&#65292;&#20174;&#36825;&#20123;NLP&#20219;&#21153;&#20013;&#65292;&#20174;&#19994;&#20154;&#21592;&#24448;&#24448;&#20381;&#36182;&#20110;&#36825;&#20123;&#24191;&#27867;&#21487;&#29992;&#30340;&#35789;&#23884;&#20837;&#12290;&#36825;&#20123;&#35789;&#23884;&#20837;&#26159;&#20174;&#22823;&#37327;&#25991;&#26412;&#26500;&#24314;&#32780;&#25104;&#65292;&#22240;&#27492;&#21487;&#33021;&#24050;&#32463;&#25429;&#25417;&#20102;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#22823;&#37096;&#20998;&#35789;&#27719;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#33021;&#22810;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#20041;&#21644;&#35789;&#30456;&#20851;&#24615;&#65311;&#26412;&#25991;&#36890;&#36807;&#21019;&#24314;&#19968;&#31181;&#38134;&#34892;&#29305;&#23450;&#30340;&#35789;&#23884;&#20837;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26469;&#28304;&#30340;&#35789;&#23884;&#20837;&#65288;&#22914;GloVe&#21644;BERT&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#36825;&#20010;&#24819;&#27861;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#20174;&#38134;&#34892;&#29305;&#23450;&#35821;&#26009;&#24211;&#26500;&#24314;&#30340;&#23884;&#20837;&#26356;&#33021;&#25429;&#25417;&#38134;&#34892;&#29305;&#23450;&#35821;&#20041;&#21644;&#35789;&#30456;&#20851;&#24615;&#12290;&#35813;&#21457;&#29616;&#34920;&#26126;&#65292;&#38754;&#21521;&#38134;&#34892;&#19994;&#30340;&#35789;&#23884;&#20837;&#22312;&#25191;&#34892;NL&#20219;&#21153;&#26102;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#29420;&#31435;&#26469;&#28304;&#25110;&#32773;&#20316;&#20026;&#20854;&#20182;&#21487;&#24191;&#27867;&#33719;&#24471;&#30340;&#23884;&#20837;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications of Natural Language Processing (NLP) are plentiful, from sentiment analysis to text classification. Practitioners rely on static word embeddings (e.g. Word2Vec or GloVe) or static word representation from contextual models (e.g. BERT or ELMo) to perform many of these NLP tasks. These widely available word embeddings are built from large amount of text, so they are likely to have captured most of the vocabulary in different context. However, how well would they capture domain-specific semantics and word relatedness? This paper explores this idea by creating a bank-specific word embeddings and evaluates them against other sources of word embeddings such as GloVe and BERT. Not surprising that embeddings built from bank-specific corpora does a better job of capturing the bank-specific semantics and word relatedness. This finding suggests that bank-specific word embeddings could be a good stand-alone source or a complement to other widely available embeddings when performing NL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#26041;&#27861;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#65292;&#23558;&#28921;&#39274;&#34892;&#20026;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#23454;&#29616;&#26356;&#20840;&#38754;&#30340;&#20581;&#24247;&#39135;&#35889;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.01805</link><description>&lt;p&gt;
Cook-Gen&#65306;&#20174;&#39135;&#35889;&#20013;&#29983;&#25104;&#20581;&#24247;&#28921;&#39274;&#21160;&#20316;&#30340;&#40065;&#26834;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes. (arXiv:2306.01805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#26041;&#27861;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#65292;&#23558;&#28921;&#39274;&#34892;&#20026;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#23454;&#29616;&#26356;&#20840;&#38754;&#30340;&#20581;&#24247;&#39135;&#35889;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#33258;&#24049;&#30340;&#39278;&#39135;&#36873;&#25321;&#65292;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#22312;&#24110;&#21161;&#20154;&#20204;&#20445;&#25345;&#20581;&#24247;&#39278;&#39135;&#20064;&#24815;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#20363;&#22914;&#65292;&#39135;&#21697;&#25512;&#33616;&#31995;&#32479;&#20998;&#26512;&#39135;&#35889;&#25351;&#20196;&#20197;&#35780;&#20272;&#33829;&#20859;&#25104;&#20998;&#24182;&#25552;&#20379;&#39135;&#35889;&#25512;&#33616;&#12290;&#32780;&#29983;&#25104;AI&#26041;&#27861;&#65288;&#22914;&#33258;&#22238;&#24402;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#25104;&#21151;&#24212;&#29992;&#21487;&#20197;&#35753;&#25105;&#20204;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#39135;&#35889;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20026;&#20581;&#24247;&#20840;&#38754;&#30340;&#39135;&#35889;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;&#29983;&#25104;AI&#26041;&#27861;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#28921;&#39274;&#34892;&#20026;&#65288;&#20363;&#22914;&#21152;&#30416;&#12289;&#29006;&#32905;&#12289;&#29038;&#34092;&#33756;&#31561;&#65289;&#32435;&#20837;&#32771;&#34385;&#12290;&#28921;&#39274;&#34892;&#20026;&#30001;&#20110;&#20854;&#19981;&#35268;&#21017;&#30340;&#25968;&#25454;&#27169;&#24335;&#32780;&#38590;&#20197;&#20351;&#29992;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
As people become more aware of their food choices, food computation models have become increasingly popular in assisting people in maintaining healthy eating habits. For example, food recommendation systems analyze recipe instructions to assess nutritional contents and provide recipe recommendations. The recent and remarkable successes of generative AI methods, such as auto-regressive large language models, can lead to robust methods for a more comprehensive understanding of recipes for healthy food recommendations beyond surface-level nutrition content assessments. In this study, we explore the use of generative AI methods to extend current food computation models, primarily involving the analysis of nutrition and ingredients, to also incorporate cooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.). Cooking actions are notoriously hard to model using statistical learning methods due to irregular data patterns - significantly varying natural language descriptions f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20004;&#20010;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#23454;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#25511;&#21046;Diffusion&#27169;&#22411;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.01804</link><description>&lt;p&gt;
&#20174;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Extracting Reward Functions from Diffusion Models. (arXiv:2306.01804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20004;&#20010;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#23454;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#25511;&#21046;Diffusion&#27169;&#22411;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Diffusion&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20063;&#34987;&#29992;&#20110;&#23398;&#20064;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#39640;&#24615;&#33021;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#27604;&#36739;&#24314;&#27169;&#20302;&#22870;&#21169;&#34892;&#20026;&#21644;&#24314;&#27169;&#39640;&#22870;&#21169;&#34892;&#20026;&#30340;&#20915;&#31574;&#20256;&#25773;&#27169;&#22411;&#26469;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65307;&#36825;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26799;&#24230;&#19982;&#20004;&#20010;Diffusion&#27169;&#22411;&#30340;&#36755;&#20986;&#24046;&#24322;&#23545;&#40784;&#26469;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;Diffusion&#27169;&#22411;&#26469;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved remarkable results in image generation, and have similarly been used to learn high-performing policies in sequential decision-making tasks. Decision-making diffusion models can be trained on lower-quality data, and then be steered with a reward function to generate near-optimal trajectories. We consider the problem of extracting a reward function by comparing a decision-making diffusion model that models low-reward behavior and one that models high-reward behavior; a setting related to inverse reinforcement learning. We first define the notion of a relative reward function of two diffusion models and show conditions under which it exists and is unique. We then devise a practical learning algorithm for extracting it by aligning the gradients of a reward function -- parametrized by a neural network -- to the difference in outputs of both diffusion models. Our method finds correct reward functions in navigation environments, and we demonstrate that steering 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#30740;&#31350;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#30740;&#31350;&#20262;&#29702;&#35268;&#33539;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24314;&#35758;&#25913;&#36827;&#21644;&#22686;&#24378;AI&#30740;&#31350;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#20262;&#29702;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.01800</link><description>&lt;p&gt;
AI&#25968;&#25454;&#22686;&#24378;&#30340;&#20262;&#29702;&#27495;&#20041;&#65306;&#30740;&#31350;&#20262;&#29702;&#35268;&#33539;&#21644;&#23454;&#36341;&#20013;&#23384;&#22312;&#30340;&#24046;&#36317;&#30340;&#34913;&#37327;
&lt;/p&gt;
&lt;p&gt;
The ethical ambiguity of AI data enrichment: Measuring gaps in research ethics norms and practices. (arXiv:2306.01800v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#30740;&#31350;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#30740;&#31350;&#20262;&#29702;&#35268;&#33539;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24314;&#35758;&#25913;&#36827;&#21644;&#22686;&#24378;AI&#30740;&#31350;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25216;&#26415;&#36827;&#27493;&#24314;&#31435;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#32479;&#35745;&#23398;&#21644;&#25968;&#23398;&#31561;&#39046;&#22495;&#30340;&#31361;&#30772;&#20043;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#20511;&#37492;&#31038;&#20250;&#31185;&#23398;&#65292;&#36716;&#21521;&#20154;&#31867;&#20114;&#21160;&#26469;&#35299;&#20915;&#27169;&#22411;&#24320;&#21457;&#30340;&#25361;&#25112;&#12290;&#21521;&#20247;&#21253;&#24037;&#20154;&#25903;&#20184;&#36153;&#29992;&#20197;&#29983;&#25104;&#25110;&#31579;&#36873;&#25968;&#25454;&#65292;&#20063;&#23601;&#26159;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#24050;&#32463;&#25104;&#20026;&#24517;&#19981;&#21487;&#23569;&#30340;&#25163;&#27573;&#65292;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#20854;&#20182;&#32463;&#24120;&#19982;&#20247;&#21253;&#24037;&#20154;&#20114;&#21160;&#30340;&#39046;&#22495;&#65292;&#22914;&#24515;&#29702;&#23398;&#65292;&#24050;&#32463;&#21046;&#23450;&#20102;&#24120;&#35265;&#30340;&#27835;&#29702;&#35201;&#27714;&#21644;&#35268;&#33539;&#65292;&#20197;&#30830;&#20445;&#30740;&#31350;&#22312;&#20262;&#29702;&#19978;&#24471;&#21040;&#20102;&#23454;&#26045;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#30740;&#31350;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#30456;&#20851;&#30740;&#31350;&#20262;&#29702;&#35201;&#27714;&#21644;&#35268;&#33539;&#30340;&#24320;&#23637;&#31243;&#24230;&#21644;&#31243;&#24230;&#12290;&#25105;&#20204;&#20851;&#27880;&#20004;&#20010;&#39046;&#20808;&#30340;&#20250;&#35758;ICLR&#21644;NeurIPS&#20197;&#21450;&#26399;&#21002;&#20986;&#29256;&#31038;Springer&#25152;&#37319;&#21462;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#36825;&#19977;&#20010;&#26469;&#28304;&#30340;1,468&#31687;&#35770;&#25991;&#30340;&#32437;&#21521;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#30740;&#31350;&#20262;&#29702;&#35268;&#33539;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#21644;&#22686;&#24378;AI&#30740;&#31350;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#20262;&#29702;&#23454;&#36341;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The technical progression of artificial intelligence (AI) research has been built on breakthroughs in fields such as computer science, statistics, and mathematics. However, in the past decade AI researchers have increasingly looked to the social sciences, turning to human interactions to solve the challenges of model development. Paying crowdsourcing workers to generate or curate data, or data enrichment, has become indispensable for many areas of AI research, from natural language processing to reinforcement learning from human feedback (RLHF). Other fields that routinely interact with crowdsourcing workers, such as Psychology, have developed common governance requirements and norms to ensure research is undertaken ethically. This study explores how, and to what extent, comparable research ethics requirements and norms have developed for AI research and data enrichment. We focus on the approach taken by two leading conferences: ICLR and NeurIPS, and journal publisher Springer. In a lo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24212;&#29992;&#27963;&#21160;&#29702;&#35770;&#20998;&#26512;&#20102;&#39321;&#28207;&#20013;&#23398;&#29983;&#22312;&#30701;&#31687;&#25925;&#20107;&#21019;&#20316;&#20013;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#26041;&#24335;&#21644;&#30446;&#30340;&#65292;&#21457;&#29616;&#20854;&#20013;&#32570;&#20047;&#30446;&#30340;&#24847;&#35782;&#12289;&#20811;&#26381;&#21019;&#20316;&#38556;&#30861;&#20197;&#21450;&#21457;&#23637;&#12289;&#25193;&#23637;&#21644;&#25913;&#36827;&#25925;&#20107;&#20026;&#20027;&#35201;&#30446;&#30340;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#27963;&#21160;&#31995;&#32479;&#30340;&#20849;&#21516;&#29305;&#24449;&#20063;&#34987;&#30740;&#31350;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2306.01798</link><description>&lt;p&gt;
&#20197;&#27963;&#21160;&#29702;&#35770;&#35270;&#35282;&#25506;&#31350;&#22806;&#35821;&#33521;&#35821;&#23398;&#20064;&#32773;&#22312;&#20154;&#24037;&#26234;&#33021;&#20889;&#20316;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Exploring EFL students' prompt engineering in human-AI story writing: an Activity Theory perspective. (arXiv:2306.01798v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#27963;&#21160;&#29702;&#35770;&#20998;&#26512;&#20102;&#39321;&#28207;&#20013;&#23398;&#29983;&#22312;&#30701;&#31687;&#25925;&#20107;&#21019;&#20316;&#20013;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#26041;&#24335;&#21644;&#30446;&#30340;&#65292;&#21457;&#29616;&#20854;&#20013;&#32570;&#20047;&#30446;&#30340;&#24847;&#35782;&#12289;&#20811;&#26381;&#21019;&#20316;&#38556;&#30861;&#20197;&#21450;&#21457;&#23637;&#12289;&#25193;&#23637;&#21644;&#25913;&#36827;&#25925;&#20107;&#20026;&#20027;&#35201;&#30446;&#30340;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#27963;&#21160;&#31995;&#32479;&#30340;&#20849;&#21516;&#29305;&#24449;&#20063;&#34987;&#30740;&#31350;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#27963;&#21160;&#29702;&#35770;&#25506;&#31350;&#20102;&#39321;&#28207;&#20013;&#23398;&#29983;&#22312;&#30701;&#31687;&#25925;&#20107;&#21019;&#20316;&#20013;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#30740;&#31350;&#25910;&#38598;&#24182;&#20998;&#26512;&#20102;&#23398;&#29983;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12289;&#30701;&#31687;&#25925;&#20107;&#21644;&#26377;&#20851;&#25552;&#31034;&#30446;&#30340;&#30340;&#20070;&#38754;&#21453;&#24605;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#23398;&#29983;&#25552;&#31034;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#19977;&#20010;&#20027;&#35201;&#30446;&#30340;&#65306;&#32570;&#20047;&#30446;&#30340;&#24847;&#35782;&#12289;&#20811;&#26381;&#21019;&#20316;&#38556;&#30861;&#20197;&#21450;&#21457;&#23637;&#12289;&#25193;&#23637;&#21644;&#25913;&#36827;&#25925;&#20107;&#12290;&#30740;&#31350;&#36824;&#30830;&#23450;&#20102;&#23398;&#29983;&#27963;&#21160;&#31995;&#32479;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#22797;&#26434;&#24615;&#12289;&#25925;&#20107;&#30340;&#36136;&#37327;&#20197;&#21450;&#25152;&#22312;&#23398;&#26657;&#30340;&#25972;&#20307;&#23398;&#26415;&#25104;&#23601;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study applies Activity Theory to investigate how English as a foreign language (EFL) students prompt generative artificial intelligence (AI) tools during short story writing. Sixty-seven Hong Kong secondary school students created generative-AI tools using open-source language models and wrote short stories with them. The study collected and analyzed the students' generative-AI tools, short stories, and written reflections on their conditions or purposes for prompting. The research identified three main themes regarding the purposes for which students prompt generative-AI tools during short story writing: a lack of awareness of purposes, overcoming writer's block, and developing, expanding, and improving the story. The study also identified common characteristics of students' activity systems, including the sophistication of their generative-AI tools, the quality of their stories, and their school's overall academic achievement level, for their prompting of generative-AI tools for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#22312;&#21019;&#24847;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33402;&#26415;&#12289;&#35821;&#35328;&#21644;&#31639;&#27861;&#65292;&#21516;&#26102;&#20063;&#32771;&#34385;&#20102;AI&#19982;&#21019;&#36896;&#21147;&#30340;&#21746;&#23398;&#24847;&#20041;&#21644;&#28508;&#22312;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01795</link><description>&lt;p&gt;
AI&#19982;&#21019;&#24847;&#39046;&#22495;: &#24403;&#21069;&#21644;&#26410;&#26469;&#24212;&#29992;&#30340;&#31616;&#35201;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI and the creative realm: A short review of current and future applications. (arXiv:2306.01795v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#22312;&#21019;&#24847;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33402;&#26415;&#12289;&#35821;&#35328;&#21644;&#31639;&#27861;&#65292;&#21516;&#26102;&#20063;&#32771;&#34385;&#20102;AI&#19982;&#21019;&#36896;&#21147;&#30340;&#21746;&#23398;&#24847;&#20041;&#21644;&#28508;&#22312;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21019;&#36896;&#21147;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#27010;&#24565;&#21450;&#20854;&#26368;&#36817;&#30340;&#25972;&#21512;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#35748;&#20026;AI&#26080;&#27861;&#20135;&#29983;&#26032;&#24605;&#24819;&#25110;&#21019;&#36896;&#33402;&#26415;&#65292;&#20294;&#26356;&#22797;&#26434;&#30340;AI&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#20154;&#26426;&#20132;&#20114;&#24037;&#20855;&#30340;&#26222;&#21450;&#20026;AI&#22312;&#33402;&#26415;&#21019;&#20316;&#20013;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;AI&#22312;&#21019;&#24847;&#32972;&#26223;&#19979;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#21306;&#20998;&#20102;&#25152;&#20351;&#29992;&#30340;&#33402;&#26415;&#31867;&#22411;&#12289;&#35821;&#35328;&#21644;&#31639;&#27861;&#12290;&#23427;&#36824;&#32771;&#34385;&#20102;AI&#21644;&#21019;&#36896;&#21147;&#30340;&#21746;&#23398;&#24847;&#20041;&#65292;&#36136;&#30097;&#24847;&#35782;&#26159;&#21542;&#21487;&#20197;&#22312;&#26426;&#22120;&#20013;&#30740;&#31350;&#65292;&#20197;&#21450;AI&#30340;&#28508;&#22312;&#20852;&#36259;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#24341;&#21457;&#23545;AI&#22312;&#21019;&#24847;&#32972;&#26223;&#19979;&#20351;&#29992;&#21644;&#20262;&#29702;&#24433;&#21709;&#30340;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the concept of creativity and artificial intelligence (AI) and their recent integration. While AI has traditionally been perceived as incapable of generating new ideas or creating art, the development of more sophisticated AI models and the proliferation of human-computer interaction tools have opened up new possibilities for AI in artistic creation. This study investigates the various applications of AI in a creative context, differentiating between the type of art, language, and algorithms used. It also considers the philosophical implications of AI and creativity, questioning whether consciousness can be researched in machines and AI's potential interests and decision-making capabilities. Overall, we aim to stimulate a reflection on AI's use and ethical implications in creative contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01792</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#31995;&#24863;&#30693;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#26159;&#22522;&#20110;&#20854;&#36807;&#21435;&#34892;&#20026;&#23398;&#20064;&#23558;&#29992;&#25143;&#34920;&#31034;&#20026;&#20302;&#32500;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23427;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#24314;&#27169;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#20026;&#21333;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#21363;&#19982;&#22810;&#31181;&#20219;&#21153;&#30456;&#20851;&#30340;&#26356;&#24191;&#20041;&#29992;&#25143;&#34920;&#31034;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38656;&#27714;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#20197;&#21450;&#20026;&#25345;&#32493;&#28155;&#21152;&#30340;&#20219;&#21153;&#25552;&#20379;&#26377;&#38480;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#19981;&#21463;&#20219;&#21153;&#25968;&#37327;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.01788</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Responsible Design Patterns for Machine Learning Pipelines. (arXiv:2306.01788v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36947;&#24503;&#23454;&#36341;&#25972;&#21512;&#21040;&#20154;&#24037;&#26234;&#33021;(AI)&#24320;&#21457;&#36807;&#31243;&#20013;&#23545;&#20110;&#30830;&#20445;AI&#30340;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36127;&#36131;&#20219;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;AI&#20262;&#29702;&#28041;&#21450;&#23558;&#20262;&#29702;&#21407;&#21017;&#24212;&#29992;&#20110;AI&#31995;&#32479;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#12290;&#36825;&#23545;&#20110;&#20943;&#36731;&#19982;AI&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#20260;&#23475;&#65288;&#22914;&#31639;&#27861;&#20559;&#35265;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#65288;RDPs&#65289;&#23545;&#20110;&#30830;&#20445;&#20262;&#29702;&#21644;&#20844;&#24179;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;RDPs&#32435;&#20837;ML&#27969;&#31243;&#20013;&#65292;&#20197;&#20943;&#36731;&#39118;&#38505;&#24182;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#36890;&#36807;&#23545;AI&#20262;&#29702;&#21644;&#25968;&#25454;&#31649;&#29702;&#19987;&#23478;&#30340;&#35843;&#26597;&#30830;&#23450;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#21453;&#39304;&#30340;&#23454;&#38469;&#24773;&#20917;&#36827;&#34892;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsibl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;Codewars&#19978;&#23545;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#30340;&#32534;&#31243;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;3 kyu&#32423;&#21035;&#20197;&#19978;&#30340;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#32771;&#34385;&#38382;&#39064;&#38590;&#24230;&#21644;&#25152;&#38656;&#26102;&#38388;&#30340;&#32534;&#31243;&#38382;&#39064;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#24378;&#35843;AI&#27169;&#22411;&#38656;&#35201;&#39564;&#35777;&#21644;&#21019;&#36896;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01784</link><description>&lt;p&gt;
&#36890;&#36807;Codewars&#32534;&#31243;&#38382;&#39064;&#35780;&#20272;GPT&#27169;&#22411;&#30340;&#32534;&#31243;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT's Programming Capability through CodeWars' Katas. (arXiv:2306.01784v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;Codewars&#19978;&#23545;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#30340;&#32534;&#31243;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;3 kyu&#32423;&#21035;&#20197;&#19978;&#30340;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#32771;&#34385;&#38382;&#39064;&#38590;&#24230;&#21644;&#25152;&#38656;&#26102;&#38388;&#30340;&#32534;&#31243;&#38382;&#39064;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#24378;&#35843;AI&#27169;&#22411;&#38656;&#35201;&#39564;&#35777;&#21644;&#21019;&#36896;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#29702;&#35299;&#38754;&#21521;&#32534;&#31243;&#30340;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;Codewars&#30340;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#32534;&#31243;&#38382;&#39064;&#36827;&#34892;&#35780;&#20272;&#65292;&#35780;&#20272;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;GPT-3.5&#21644;GPT-4&#12290;&#23454;&#39564;&#25581;&#31034;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36793;&#30028;&#65292;&#21363;&#36229;&#36807;3kyu&#32423;&#21035;&#65292;&#36825;&#20123;GPT&#27169;&#22411;&#38590;&#20197;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20102;&#19968;&#20010;&#32771;&#34385;&#38382;&#39064;&#38590;&#24230;&#21644;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#26102;&#38388;&#30340;&#32534;&#31243;&#38382;&#39064;&#22797;&#26434;&#24230;&#24230;&#37327;&#25552;&#35758;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#38656;&#35201;&#39564;&#35777;&#21644;&#21019;&#36896;&#24615;&#24605;&#32500;&#33021;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#25216;&#24039;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#26088;&#22312;&#23436;&#21892;&#36825;&#20010;&#25152;&#25552;&#35758;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#12289;&#22686;&#24378;AI&#27169;&#22411;&#30340;&#36825;&#20123;&#24314;&#35758;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#19968;&#20010;&#34913;&#37327;&#32534;&#31243;&#38382;&#39064;&#38590;&#24230;&#30340;&#23458;&#35266;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of artificial intelligence (AI), understanding the capabilities and limitations of programming-oriented models is crucial. This paper presents a novel evaluation of the programming proficiency of Generative Pretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against coding problems of varying difficulty levels drawn from Codewars. The experiments reveal a distinct boundary at the 3kyu level, beyond which these GPT models struggle to provide solutions. These findings led to the proposal of a measure for coding problem complexity that incorporates both problem difficulty and the time required for solution. The research emphasizes the need for validation and creative thinking capabilities in AI models to better emulate human problem-solving techniques. Future work aims to refine this proposed complexity measure, enhance AI models with these suggested capabilities, and develop an objective measure for programming problem difficulty. The results of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#19982;&#20247;&#21253;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#12290;&#19987;&#23478;&#35780;&#20272;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#65292;&#20247;&#21253;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#21152;&#20855;&#26377;&#21019;&#24847;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01779</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#35774;&#35745;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conceptual Design Generation Using Large Language Models. (arXiv:2306.01779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#19982;&#20247;&#21253;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#12290;&#19987;&#23478;&#35780;&#20272;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#65292;&#20247;&#21253;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#21152;&#20855;&#26377;&#21019;&#24847;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29983;&#25104;&#26159;&#27010;&#24565;&#35774;&#35745;&#38454;&#27573;&#30340;&#21019;&#36896;&#24615;&#27493;&#39588;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#20013;&#65292;&#35774;&#35745;&#24072;&#32463;&#24120;&#23581;&#35797;&#36890;&#36807;&#22836;&#33041;&#39118;&#26292;&#12289;&#24605;&#32500;&#23548;&#22270;&#25110;&#20247;&#21253;&#35774;&#35745;&#24605;&#36335;&#26469;&#34917;&#20805;&#20182;&#20204;&#33258;&#24049;&#23545;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20852;&#36215;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#30475;&#20284;&#21019;&#36896;&#24615;&#30340;&#36755;&#20986;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#21151;&#24212;&#29992;&#36328;&#36275;&#20102;&#33402;&#26415;&#12289;&#23089;&#20048;&#21644;&#20854;&#20182;&#21019;&#20316;&#24037;&#20316;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#21033;&#29992;LLMs&#20026;&#19968;&#32452;12&#20010;&#35774;&#35745;&#38382;&#39064;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#19982;&#20247;&#21253;&#35299;&#20915;&#26041;&#26696;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#35780;&#20272;&#29983;&#25104;&#21644;&#20247;&#21253;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#20154;&#24037;&#19987;&#23478;&#35780;&#20272;&#21644;&#35745;&#31639;&#25351;&#26631;&#12290;&#19987;&#23478;&#35780;&#20272;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#24179;&#22343;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#26356;&#39640;&#65292;&#32780;&#20247;&#21253;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#21152;&#20855;&#26377;&#21019;&#24847;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept generation is a creative step in the conceptual design phase, where designers often turn to brainstorming, mindmapping, or crowdsourcing design ideas to complement their own knowledge of the domain. Recent advances in natural language processing (NLP) and machine learning (ML) have led to the rise of Large Language Models (LLMs) capable of generating seemingly creative outputs from textual prompts. The success of these models has led to their integration and application across a variety of domains, including art, entertainment, and other creative work. In this paper, we leverage LLMs to generate solutions for a set of 12 design problems and compare them to a baseline of crowdsourced solutions. We evaluate the differences between generated and crowdsourced design solutions through multiple perspectives, including human expert evaluations and computational metrics. Expert evaluations indicate that the LLM-generated solutions have higher average feasibility and usefulness while th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;AI&#31995;&#32479;&#24320;&#21457;&#20013;&#32570;&#23569;&#35201;&#27714;&#24037;&#31243;&#65288;RE&#65289;&#36825;&#19968;&#29615;&#33410;&#19988;&#20262;&#29702;&#25351;&#21335;&#26415;&#35821;&#21644;&#21407;&#21017;&#35206;&#30422;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20026;&#35299;&#20915;&#35813;&#38382;&#39064;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26415;&#35821;&#34920;&#24182;&#30740;&#31350;&#20102;&#20262;&#29702;AI&#24320;&#21457;&#26694;&#26550;&#22312;&#25191;&#34892;RE&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01774</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#20449;&#33258;&#21160;&#31995;&#32479;&#24320;&#21457;&#30340;RE&#20013;&#24515;&#21270;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
RE-centric Recommendations for the Development of Trustworthy(er) Autonomous Systems. (arXiv:2306.01774v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;AI&#31995;&#32479;&#24320;&#21457;&#20013;&#32570;&#23569;&#35201;&#27714;&#24037;&#31243;&#65288;RE&#65289;&#36825;&#19968;&#29615;&#33410;&#19988;&#20262;&#29702;&#25351;&#21335;&#26415;&#35821;&#21644;&#21407;&#21017;&#35206;&#30422;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20026;&#35299;&#20915;&#35813;&#38382;&#39064;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26415;&#35821;&#34920;&#24182;&#30740;&#31350;&#20102;&#20262;&#29702;AI&#24320;&#21457;&#26694;&#26550;&#22312;&#25191;&#34892;RE&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27431;&#30431;&#20869;&#24320;&#21457;&#21644;&#23454;&#26045;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26102;&#31526;&#21512;&#27431;&#30431;AI&#27861;&#26696;&#65288;AIA&#65289;&#25351;&#21335;&#23558;&#24456;&#24555;&#26159;&#24378;&#21046;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#20174;&#34892;&#21160;&#25351;&#21335;&#26041;&#38754;&#65292;&#23454;&#36341;&#32773;&#32570;&#20047;&#22312;AI&#31995;&#32479;&#24320;&#21457;&#26399;&#38388;&#23454;&#26045;&#20262;&#29702;&#30340;&#21487;&#25805;&#20316;&#35828;&#26126;&#12290;&#23545;&#19981;&#21516;&#20262;&#29702;&#25351;&#21335;&#30340;&#25991;&#29486;&#32508;&#36848;&#25581;&#31034;&#20102;&#25152;&#28041;&#21450;&#21407;&#21017;&#21644;&#26415;&#35821;&#25551;&#36848;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35201;&#22312;AI&#24320;&#21457;&#36807;&#31243;&#30340;&#26089;&#26399;&#38454;&#27573;&#22521;&#20859;&#20449;&#20219;&#30340;&#35201;&#27714;&#24037;&#31243;&#65288;RE&#65289;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#25903;&#25345;&#36947;&#24503;&#21644;&#21487;&#20449;AI&#24320;&#21457;&#30340;&#26694;&#26550;&#20013;&#32570;&#22833;&#12290;&#36825;&#31181;&#19981;&#21327;&#35843;&#30340;&#25514;&#36766;&#21152;&#19978;&#32570;&#20047;&#20855;&#20307;&#30340;&#24320;&#21457;&#23454;&#36341;&#20351;&#21487;&#20449;AI&#24320;&#21457;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26415;&#35821;&#34920;&#65292;&#29992;&#20110;&#27604;&#36739;&#20027;&#35201;&#20262;&#29702;AI&#25351;&#21335;&#20013;&#20351;&#29992;&#30340;&#26415;&#35821;&#21644;&#20262;&#29702;AI&#21407;&#21017;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20262;&#29702;AI&#24320;&#21457;&#26694;&#26550;&#22312;&#25191;&#34892;RE&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complying with the EU AI Act (AIA) guidelines while developing and implementing AI systems will soon be mandatory within the EU. However, practitioners lack actionable instructions to operationalise ethics during AI systems development. A literature review of different ethical guidelines revealed inconsistencies in the principles addressed and the terminology used to describe them. Furthermore, requirements engineering (RE), which is identified to foster trustworthiness in the AI development process from the early stages was observed to be absent in a lot of frameworks that support the development of ethical and trustworthy AI. This incongruous phrasing combined with a lack of concrete development practices makes trustworthy AI development harder. To address this concern, we formulated a comparison table for the terminology used and the coverage of the ethical AI principles in major ethical AI guidelines. We then examined the applicability of ethical AI development frameworks for perfo
&lt;/p&gt;</description></item><item><title>ProcessGPT&#26159;&#19968;&#31181;&#20351;&#29992;GPT&#25216;&#26415;&#30340;&#26032;&#22411;&#25216;&#26415;&#65292;&#21487;&#22312;&#38656;&#35201;&#26102;&#29983;&#25104;&#26032;&#30340;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20915;&#31574;&#36741;&#21161;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#22686;&#24378;&#25968;&#25454;&#20013;&#24515;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#27969;&#31243;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01771</link><description>&lt;p&gt;
ProcessGPT: &#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#36716;&#21464;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
ProcessGPT: Transforming Business Process Management with Generative Artificial Intelligence. (arXiv:2306.01771v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01771
&lt;/p&gt;
&lt;p&gt;
ProcessGPT&#26159;&#19968;&#31181;&#20351;&#29992;GPT&#25216;&#26415;&#30340;&#26032;&#22411;&#25216;&#26415;&#65292;&#21487;&#22312;&#38656;&#35201;&#26102;&#29983;&#25104;&#26032;&#30340;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20915;&#31574;&#36741;&#21161;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#22686;&#24378;&#25968;&#25454;&#20013;&#24515;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#27969;&#31243;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#12290;GPT&#36890;&#36807;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#20851;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#36866;&#24403;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;GPT&#25216;&#26415;&#22312;&#38656;&#35201;&#26102;&#29983;&#25104;&#26032;&#30340;&#27969;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ProcessGPT&#20316;&#20026;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#23427;&#26377;&#28508;&#21147;&#22686;&#24378;&#25968;&#25454;&#20013;&#24515;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#27969;&#31243;&#30340;&#20915;&#31574;&#12290;ProcessGPT&#21487;&#36890;&#36807;&#22312;&#22823;&#22411;&#19994;&#21153;&#27969;&#31243;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#26469;&#35774;&#35745;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#23545;&#29305;&#23450;&#27969;&#31243;&#22495;&#36827;&#34892;&#24494;&#35843;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#21644;&#29992;&#25143;&#36755;&#20837;&#23545;&#20854;&#36827;&#34892;&#27969;&#31243;&#27969;&#31243;&#30340;&#29983;&#25104;&#21644;&#20915;&#31574;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#19982;NLP&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38598;&#25104;&#20197;&#25552;&#20379;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) is a state-of-the-art machine learning model capable of generating human-like text through natural language processing (NLP). GPT is trained on massive amounts of text data and uses deep learning techniques to learn patterns and relationships within the data, enabling it to generate coherent and contextually appropriate text. This position paper proposes using GPT technology to generate new process models when/if needed. We introduce ProcessGPT as a new technology that has the potential to enhance decision-making in data-centric and knowledge-intensive processes. ProcessGPT can be designed by training a generative pre-trained transformer model on a large dataset of business process data. This model can then be fine-tuned on specific process domains and trained to generate process flows and make decisions based on context and user input. The model can be integrated with NLP and machine learning techniques to provide insights and recommendations f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDADSS-IPM&#30340;&#33258;&#36866;&#24212;&#36335;&#38754;&#21644;&#20859;&#25252;&#25112;&#30053;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#25972;&#30340;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#23545;&#27668;&#20505;&#26465;&#20214;&#19979;&#36947;&#36335;&#24773;&#20917;&#30340;&#23454;&#38469;&#20102;&#35299;&#65292;&#20197;&#20415;&#20110;&#36827;&#34892;&#36866;&#24403;&#30340;&#31649;&#29702;&#21644;&#32500;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.01769</link><description>&lt;p&gt;
&#22522;&#20110;&#25216;&#26415;&#30340;&#36335;&#38754;&#21644;&#20859;&#25252;&#25112;&#30053;&#33258;&#36866;&#24212;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65306;&#27668;&#20505;&#21464;&#21270;&#36866;&#24212;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#30340;&#28966;&#28857;
&lt;/p&gt;
&lt;p&gt;
Towards a Technology-Driven Adaptive Decision Support System for Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk assessment framework for climate change adaptation. (arXiv:2306.01769v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDADSS-IPM&#30340;&#33258;&#36866;&#24212;&#36335;&#38754;&#21644;&#20859;&#25252;&#25112;&#30053;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#25972;&#30340;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#23545;&#27668;&#20505;&#26465;&#20214;&#19979;&#36947;&#36335;&#24773;&#20917;&#30340;&#23454;&#38469;&#20102;&#35299;&#65292;&#20197;&#20415;&#20110;&#36827;&#34892;&#36866;&#24403;&#30340;&#31649;&#29702;&#21644;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#65292;&#29992;&#20110;&#36335;&#38754;&#21644;&#20859;&#25252;&#25112;&#30053;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#19968;&#33324;&#34987;&#35774;&#35745;&#25104;&#23616;&#37096;&#26368;&#20248;&#31995;&#32479;&#12290;&#30001;&#20110;&#24037;&#19994;4.0&#26102;&#20195;&#30340;&#22823;&#25968;&#25454;&#24212;&#29992;&#23578;&#26410;&#20986;&#29616;&#65292;&#22240;&#27492;&#36825;&#20123;DSS&#26368;&#21021;&#24182;&#26410;&#34987;&#35774;&#35745;&#20026;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#65292;&#23548;&#33268;&#20915;&#31574;&#19981;&#22815;&#28789;&#27963;&#12290;&#26412;&#25991;&#30001;&#20110;&#20844;&#36335;&#36164;&#20135;&#23545;&#27668;&#20505;&#29616;&#35937;&#30340;&#33030;&#24369;&#24615;&#65292;&#37319;&#21462;&#20102;&#20855;&#26377;&#36828;&#35265;&#30340;&#25514;&#26045;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;TDADSS-IPM&#65288;&#22522;&#20110;&#25216;&#26415;&#30340;&#33258;&#36866;&#24212;&#36335;&#38754;&#21644;&#20859;&#25252;&#25112;&#30053;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65289;&#30340;&#32508;&#21512;&#31995;&#32479;&#12290;&#20316;&#20026;&#36825;&#31181;DSS&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#20449;&#24565;&#32593;&#32476;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#65292;&#20197;&#20102;&#35299;&#20025;&#40614;&#36947;&#36335;&#30001;&#20110;&#27668;&#20505;&#26465;&#20214;&#30340;&#23454;&#38469;&#29366;&#20917;&#12290;&#36825;&#31181;&#27169;&#22411;&#22635;&#34917;&#20102;&#30693;&#35782;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36827;&#34892;&#22521;&#35757;&#65292;&#24182;&#22312;&#23454;&#38469;&#20107;&#20214;&#20013;&#36827;&#34892;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Support Systems for pavement and maintenance strategies have traditionally been designed as silos led to local optimum systems. Moreover, since big data usage didn't exist as result of Industry 4.0 as of today, DSSs were not initially designed adaptive to the sources of uncertainties led to rigid decisions. Motivated by the vulnerability of the road assets to the climate phenomena, this paper takes a visionary step towards introducing a Technology-Driven Adaptive Decision Support System for Integrated Pavement and Maintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk assessment model is met via Bayesian Belief Networks (BBN) to realize the actual condition of the Danish roads due to weather condition. Such model fills the gaps in the knowledge domain and develops a platform that can be trained over time, and applied in real-time to the actual event.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#26689;&#26550;&#35774;&#35745;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#35780;&#20272;&#20505;&#36873;&#35774;&#35745;&#24182;&#26356;&#26032;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#24335;&#20248;&#21270;&#26689;&#26550;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#20248;&#21270;&#22797;&#26434;&#31995;&#32479;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01763</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26689;&#26550;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization for truss design using Bayesian optimization. (arXiv:2306.01763v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#26689;&#26550;&#35774;&#35745;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#35780;&#20272;&#20505;&#36873;&#35774;&#35745;&#24182;&#26356;&#26032;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#24335;&#20248;&#21270;&#26689;&#26550;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#20248;&#21270;&#22797;&#26434;&#31995;&#32479;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35745;&#31639;&#26426;&#36741;&#21161;&#26377;&#38480;&#20803;&#20998;&#26512;&#36827;&#34892;&#26426;&#26800;&#26689;&#26550;&#20960;&#20309;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#26689;&#26550;&#30340;&#24418;&#29366;&#26159;&#30830;&#23450;&#20854;&#25215;&#36733;&#33021;&#21147;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#22312;&#32473;&#23450;&#30340;&#21442;&#25968;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#26368;&#22823;&#21270;&#25215;&#36733;&#33021;&#21147;&#19988;&#19981;&#20250;&#20135;&#29983;&#24212;&#21147;&#30340;&#22806;&#22771;&#21442;&#25968;&#12290;&#25105;&#20204;&#36873;&#25321;&#36125;&#21494;&#26031;&#20248;&#21270;&#20316;&#20026;&#25105;&#20204;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#31181;&#26114;&#36149;&#35745;&#31639;&#30340;&#35774;&#35745;&#35780;&#20272;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26689;&#26550;&#35774;&#35745;&#28041;&#21450;&#36845;&#20195;&#35780;&#20272;&#19968;&#32452;&#20505;&#36873;&#26689;&#26550;&#35774;&#35745;&#65292;&#24182;&#22522;&#20110;&#32467;&#26524;&#26356;&#26032;&#35774;&#35745;&#31354;&#38388;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#27599;&#20010;&#20505;&#36873;&#35774;&#35745;&#30340;&#24615;&#33021;&#65292;&#19979;&#19968;&#20010;&#20505;&#36873;&#35774;&#35745;&#26159;&#22522;&#20110;&#27169;&#22411;&#23545;&#24615;&#33021;&#25913;&#36827;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27010;&#29575;&#27169;&#22411;&#26377;&#21161;&#20110;&#35780;&#20272;&#26689;&#26550;&#35774;&#35745;&#31354;&#38388;&#65292;&#24182;&#19988;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#20248;&#21270;&#22797;&#26434;&#31995;&#32479;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, geometry optimization of mechanical truss using computer-aided finite element analysis is presented. The shape of the truss is a dominant factor in determining the capacity of load it can bear. At a given parameter space, our goal is to find the parameters of a hull that maximize the load-bearing capacity and also don't yield to the induced stress. We rely on finite element analysis, which is a computationally costly design analysis tool for design evaluation. For such expensive to-evaluate functions, we chose Bayesian optimization as our optimization framework which has empirically proven sample efficient than other simulation-based optimization methods.  By utilizing Bayesian optimization algorithms, the truss design involves iteratively evaluating a set of candidate truss designs and updating a probabilistic model of the design space based on the results. The model is used to predict the performance of each candidate design, and the next candidate design is selected ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01762</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#32431;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#37096;&#32626;&#20026;&#21508;&#31181;&#26085;&#24120;&#26381;&#21153;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36867;&#36991;&#25915;&#20987;&#26159;&#26368;&#26222;&#36941;&#30340;&#19968;&#31181;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25110;&#21033;&#29992;&#22823;&#37327;&#28165;&#27905;&#25968;&#25454;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#37096;&#32626;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#22312;&#32447;&#26381;&#21153;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#24403;&#26816;&#27979;&#21040;&#26576;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#26102;&#65292;&#26381;&#21153;&#25552;&#20379;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#32780;&#22823;&#37327;&#30340;&#28165;&#27905;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#21517;&#20026;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#26088;&#22312;&#24555;&#36895;&#38450;&#24481;&#20855;&#26377;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#38480;&#21046;&#30340;&#21407;&#22987;&#26381;&#21153;&#27169;&#22411;&#30340;&#26576;&#31181;&#25915;&#20987;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#36716;&#31227;&#23398;&#20064;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;Transformer&#26469;&#25552;&#32431;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#39044;&#35757;&#32451;&#30340;Transformer&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#40723;&#21169;&#25552;&#32431;&#21518;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#25509;&#36817;&#28165;&#26224;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RaPiD&#22312;&#38450;&#24481;&#21508;&#31181;&#20855;&#26377;&#38480;&#25968;&#25454;&#30340;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;11&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#23545;&#27604;&#20998;&#26512;&#65292;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;77%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01761</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#21644;ChatGPT&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning. (arXiv:2306.01761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;11&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#23545;&#27604;&#20998;&#26512;&#65292;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;77%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#20013;&#30340;&#19968;&#21592;&#65292;&#26159;&#19968;&#31181;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#31181;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#29983;&#25104;&#30475;&#20284;&#30001;&#33258;&#28982;&#26234;&#33021;&#25776;&#20889;&#30340;&#25991;&#26412;&#25991;&#20214;&#12290;&#34429;&#28982;&#36825;&#31181;&#29983;&#25104;&#27169;&#22411;&#26377;&#24456;&#22810;&#20248;&#28857;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#21512;&#29702;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#65292;&#20197;&#21450;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#20849;&#35745;11&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#23545;&#27604;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;Kaggle&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;10,000&#20010;&#25991;&#26412;&#65292;&#20854;&#20013;5,204&#20010;&#25991;&#26412;&#26159;&#20154;&#31867;&#20174;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#20889;&#20316;&#12290;&#22312;&#30001;GPT-3.5&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21576;&#29616;&#20986;77%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a conversational artificial intelligence that is a member of the generative pre-trained transformer of the large language model family. This text generative model was fine-tuned by both supervised learning and reinforcement learning so that it can produce text documents that seem to be written by natural intelligence. Although there are numerous advantages of this generative model, it comes with some reasonable concerns as well. This paper presents a machine learning-based solution that can identify the ChatGPT delivered text from the human written text along with the comparative analysis of a total of 11 machine learning and deep learning algorithms in the classification process. We have tested the proposed model on a Kaggle dataset consisting of 10,000 texts out of which 5,204 texts were written by humans and collected from news and social media. On the corpus generated by GPT-3.5, the proposed algorithm presents an accuracy of 77%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#25581;&#31034;&#20102;&#25910;&#20837;&#25345;&#32493;&#24615;&#30340;&#22797;&#26434;&#26412;&#36136;&#65292;&#24182;&#35777;&#23454;&#20102;&#25910;&#20837;&#20855;&#26377;&#38750;&#32447;&#24615;&#25345;&#32493;&#24615;&#12289;&#26465;&#20214;&#20559;&#26012;&#24615;&#21644;&#26465;&#20214;&#23792;&#24230;&#31561;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20102;ARCH&#25928;&#24212;&#20197;&#21450;&#38750;&#39640;&#26031;&#30636;&#26102;&#24615;&#25104;&#20998;&#25152;&#20135;&#29983;&#30340;&#26126;&#26174;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.01760</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#27169;&#22411;&#25581;&#31034;&#25910;&#20837;&#21160;&#24577;&#28436;&#21270;&#65306;&#22522;&#20110;PSID&#25968;&#25454;&#30340;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#30740;&#31350; (arXiv:2306.01760v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
Nonparametric Identification and Estimation of Earnings Dynamics using a Hidden Markov Model: Evidence from the PSID. (arXiv:2306.01760v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#25581;&#31034;&#20102;&#25910;&#20837;&#25345;&#32493;&#24615;&#30340;&#22797;&#26434;&#26412;&#36136;&#65292;&#24182;&#35777;&#23454;&#20102;&#25910;&#20837;&#20855;&#26377;&#38750;&#32447;&#24615;&#25345;&#32493;&#24615;&#12289;&#26465;&#20214;&#20559;&#26012;&#24615;&#21644;&#26465;&#20214;&#23792;&#24230;&#31561;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20102;ARCH&#25928;&#24212;&#20197;&#21450;&#38750;&#39640;&#26031;&#30636;&#26102;&#24615;&#25104;&#20998;&#25152;&#20135;&#29983;&#30340;&#26126;&#26174;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#65292;&#20197;&#25581;&#31034;&#25910;&#20837;&#25345;&#32493;&#24615;&#30340;&#22797;&#26434;&#26412;&#36136;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20551;&#23450;&#23545;&#25968;&#25910;&#20837;&#27531;&#24046;&#21253;&#25324;&#25345;&#20037;&#24615;&#21644;&#30636;&#26102;&#24615;&#20004;&#20010;&#37096;&#20998;&#65292;&#22343;&#36981;&#24490;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#32447;&#24615;&#31639;&#23376;&#36827;&#34892;&#35889;&#20998;&#35299;&#23454;&#29616;&#38750;&#21442;&#25968;&#35782;&#21035;&#65292;&#24182;&#24341;&#20837;&#25913;&#36827;&#30340;&#38543;&#26426;EM&#31639;&#27861;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12290;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#25910;&#20837;&#21160;&#24577;&#30740;&#31350;&#65288;PSID&#65289;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25910;&#20837;&#36807;&#31243;&#21576;&#29616;&#38750;&#32447;&#24615;&#25345;&#32493;&#24615;&#12289;&#26465;&#20214;&#20559;&#26012;&#24615;&#21644;&#26465;&#20214;&#23792;&#24230;&#12290;&#27492;&#22806;&#65292;&#30636;&#26102;&#24615;&#25104;&#20998;&#20855;&#26377;&#38750;&#39640;&#26031;&#24615;&#36136;&#65292;&#22312;&#39640;&#25910;&#20837;&#23478;&#24237;&#38754;&#20020;&#36127;&#20914;&#20987;&#25110;&#20302;&#25910;&#20837;&#23478;&#24237;&#36973;&#36935;&#27491;&#20914;&#20987;&#26102;&#20250;&#20135;&#29983;&#26126;&#26174;&#30340;&#19981;&#23545;&#31216;&#20998;&#24067;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22312;2&#33267;8&#24180;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#25910;&#20837;&#20855;&#26377;ARCH&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a hidden Markov model designed to investigate the complex nature of earnings persistence. The proposed model assumes that the residuals of log-earnings consist of a persistent component and a transitory component, both following general Markov processes. Nonparametric identification is achieved through spectral decomposition of linear operators, and a modified stochastic EM algorithm is introduced for model estimation. Applying the framework to the Panel Study of Income Dynamics (PSID) dataset, we find that the earnings process displays nonlinear persistence, conditional skewness, and conditional kurtosis. Additionally, the transitory component is found to possess non-Gaussian properties, resulting in a significantly asymmetric distributional impact when high-earning households face negative shocks or low-earning households encounter positive shocks. Our empirical findings also reveal the presence of ARCH effects in earnings at horizons ranging from 2 to 8 years, fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#35757;&#32451;&#20808;&#39564;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#39057;&#29575;&#26356;&#39640;&#30340;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#65292;&#20294;&#36825;&#20063;&#20250;&#38477;&#20302;&#20854;&#29983;&#25104;&#20197;&#32763;&#36716;&#19977;&#20803;&#32452;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.01755</link><description>&lt;p&gt;
&#35757;&#32451;&#20808;&#39564;&#24433;&#21709;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Training Priors Predict Text-To-Image Model Performance. (arXiv:2306.01755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#35757;&#32451;&#20808;&#39564;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#39057;&#29575;&#26356;&#39640;&#30340;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#65292;&#20294;&#36825;&#20063;&#20250;&#38477;&#20302;&#20854;&#29983;&#25104;&#20197;&#32763;&#36716;&#19977;&#20803;&#32452;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19968;&#20123;&#20851;&#31995;&#65292;&#27604;&#22914;&#8220;&#23431;&#33322;&#21592;&#39569;&#39532;&#8221;&#65292;&#20294;&#21364;&#19981;&#33021;&#29983;&#25104;&#30001;&#30456;&#21516;&#22522;&#26412;&#37096;&#20998;&#32452;&#25104;&#30340;&#20854;&#20182;&#20851;&#31995;&#65292;&#27604;&#22914;&#8220;&#39532;&#39569;&#23431;&#33322;&#21592;&#8221;&#12290;&#36825;&#20123;&#22833;&#36133;&#36890;&#24120;&#34987;&#35270;&#20026;&#27169;&#22411;&#20381;&#36182;&#35757;&#32451;&#20808;&#39564;&#32780;&#19981;&#26159;&#26500;&#24314;&#26032;&#39062;&#30340;&#22270;&#20687;&#32452;&#21512;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#30452;&#25509;&#22312;&#31283;&#23450;&#25193;&#25955;2.1&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#36890;&#36807;&#35266;&#23519;&#32452;&#25104;&#36825;&#20123;&#25552;&#31034;&#30340;&#20027;&#35821;-&#35859;&#35821;-&#23486;&#35821; (SVO) &#19977;&#20803;&#32452;&#65288;&#20363;&#22914;&#65292;&#8220;&#23431;&#33322;&#21592;&#8221;&#65292;&#8220;&#39569;&#8221;&#65292;&#8220;&#39532;&#8221;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;SVO&#19977;&#20803;&#32452;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#36234;&#22810;&#65292;&#35813;&#27169;&#22411;&#23601;&#33021;&#29983;&#25104;&#19982;&#35813;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#23601;&#36234;&#22909;&#12290;&#22312;&#36825;&#37324;&#65292;&#36890;&#36807;&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#24847;&#24605;&#26159;&#27599;&#20010;&#26415;&#35821;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#20197;&#27491;&#30830;&#30340;&#20851;&#31995;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#21152;&#30340;&#39057;&#29575;&#20063;&#20250;&#20943;&#23569;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19982;&#32763;&#36716;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#8220;&#23431;&#33322;&#21592;&#39569;&#39532;&#8221;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39057;&#32321;&#20986;&#29616;&#65292;&#37027;&#20040;&#8220;&#39532;&#39569;&#23431;&#33322;&#21592;&#8221;&#30340;&#23545;&#40784;&#36136;&#37327;&#23601;&#20250;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models can often generate some relations, i.e., "astronaut riding horse", but fail to generate other relations composed of the same basic parts, i.e., "horse riding astronaut". These failures are often taken as evidence that the models rely on training priors rather than constructing novel images compositionally. This paper tests this intuition directly on the stablediffusion 2.1 text-to-image model. By looking at the subject-verb-object (SVO) triads that form the backbone of these prompts (e.g., "astronaut", "ride", "horse"), we find that the more often an SVO triad appears in the training data, the better the model can generate an image aligned with that triad. Here, by aligned we mean that each of the terms appears in the generated image in the proper relation to each other. However, this increased frequency also diminishes how well the model can generate an image aligned with the flipped triad. For example, if "astronaut riding horse" appears frequently in the trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#36753;&#20195;&#30721;&#30340;&#21516;&#26102;&#26816;&#27979;&#28431;&#27934;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#12289;&#20302;&#24310;&#36831;&#22320;&#26816;&#27979;&#36229;&#36807;250&#31181;&#28431;&#27934;&#31867;&#22411;&#30340;&#22797;&#26434;&#28431;&#27934;&#20195;&#30721;&#27169;&#24335;&#65292;&#20351;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#22312;&#24341;&#20837;&#28508;&#22312;&#28431;&#27934;&#21040;&#20195;&#30721;&#24211;&#20043;&#21069;&#20462;&#22797;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2306.01754</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20195;&#30721;&#32534;&#36753;&#26102;&#28431;&#27934;&#26816;&#27979;&#65306;&#38646;&#26679;&#26412;&#12289;&#23567;&#26679;&#26412;&#36824;&#26159;&#24494;&#35843;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?. (arXiv:2306.01754v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#36753;&#20195;&#30721;&#30340;&#21516;&#26102;&#26816;&#27979;&#28431;&#27934;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#12289;&#20302;&#24310;&#36831;&#22320;&#26816;&#27979;&#36229;&#36807;250&#31181;&#28431;&#27934;&#31867;&#22411;&#30340;&#22797;&#26434;&#28431;&#27934;&#20195;&#30721;&#27169;&#24335;&#65292;&#20351;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#22312;&#24341;&#20837;&#28508;&#22312;&#28431;&#27934;&#21040;&#20195;&#30721;&#24211;&#20043;&#21069;&#20462;&#22797;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#28431;&#27934;&#20250;&#32473;&#20225;&#19994;&#24102;&#26469;&#37325;&#22823;&#25439;&#22833;&#12290;&#23613;&#31649;&#38024;&#23545;&#36719;&#20214;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21162;&#21147;&#65292;&#20294;&#26410;&#21457;&#29616;&#30340;&#28431;&#27934;&#20173;&#20250;&#23545;&#36719;&#20214;&#25152;&#26377;&#32773;&#21644;&#29992;&#25143;&#36896;&#25104;&#39118;&#38505;&#12290;&#35768;&#22810;&#24403;&#21069;&#30340;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#35201;&#27714;&#20195;&#30721;&#29255;&#27573;&#33021;&#22815;&#22312;&#23581;&#35797;&#26816;&#27979;&#20043;&#21069;&#32534;&#35793;&#21644;&#26500;&#24314;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20250;&#22312;&#27880;&#20837;&#28431;&#27934;&#21040;&#21024;&#38500;&#28431;&#27934;&#30340;&#26102;&#38388;&#20043;&#38388;&#24341;&#20837;&#24456;&#38271;&#30340;&#24310;&#36831;&#65292;&#36825;&#21487;&#33021;&#20250;&#22823;&#22823;&#22686;&#21152;&#20462;&#22797;&#28431;&#27934;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#21253;&#21547;250&#22810;&#31181;&#28431;&#27934;&#31867;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#22797;&#26434;&#30340;&#28431;&#27934;&#20195;&#30721;&#27169;&#24335;&#65292;&#24182;&#22312;&#20195;&#30721;&#32534;&#36753;&#26102;&#26816;&#27979;&#28431;&#27934;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#35757;&#32451;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#38646;&#26679;&#26412;&#12289;&#23567;&#26679;&#26412;&#21644;&#24494;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#39640;&#31934;&#24230;&#12289;&#20302;&#24310;&#36831;&#22320;&#26816;&#27979;&#28431;&#27934;&#65292;&#20351;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#22312;&#24341;&#20837;&#28508;&#22312;&#28431;&#27934;&#21040;&#20195;&#30721;&#24211;&#20043;&#21069;&#20462;&#22797;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software vulnerabilities bear enterprises significant costs. Despite extensive efforts in research and development of software vulnerability detection methods, uncaught vulnerabilities continue to put software owners and users at risk. Many current vulnerability detection methods require that code snippets can compile and build before attempting detection. This, unfortunately, introduces a long latency between the time a vulnerability is injected to the time it is removed, which can substantially increases the cost of fixing a vulnerability. We recognize that the current advances in machine learning can be used to detect vulnerable code patterns on syntactically incomplete code snippets as the developer is writing the code at EditTime. In this paper we present a practical system that leverages deep learning on a large-scale data set of vulnerable code patterns to learn complex manifestations of more than 250 vulnerability types and detect vulnerable code patterns at EditTime. We discus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVLIR&#30340;&#39044;&#22788;&#29702;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#21644;&#21512;&#29702;&#21270;&#20219;&#21153;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#39033;&#20219;&#21153;&#19978;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.01753</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;&#30340;&#39044;&#22788;&#29702;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Preconditioned Visual Language Inference with Weak Supervision. (arXiv:2306.01753v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVLIR&#30340;&#39044;&#22788;&#29702;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#21644;&#21512;&#29702;&#21270;&#20219;&#21153;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#39033;&#20219;&#21153;&#19978;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#25552;&#21462;&#27599;&#31181;&#24773;&#22659;&#19979;&#30456;&#20851;&#30340;&#21069;&#25552;&#26465;&#20214;&#26469;&#25512;&#26029;&#29289;&#20307;&#30340;&#21487;&#20379;&#24615;&#12290;&#20363;&#22914;&#65292;&#30475;&#21040;&#19968;&#24352;&#30772;&#30862;&#26479;&#23376;&#30340;&#29031;&#29255;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#36825;&#20010;&#21069;&#25552;&#26465;&#20214;&#38459;&#27490;&#20102;&#26479;&#23376;&#29992;&#20110;&#39278;&#29992;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30740;&#31350;&#20013;&#65292;&#27169;&#22411;&#26126;&#30830;&#33719;&#21462;&#19978;&#19979;&#25991;&#21069;&#25552;&#26465;&#20214;&#26469;&#25512;&#29702;&#24120;&#35782;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25552;&#21462;&#36825;&#26679;&#30340;&#21069;&#25552;&#26465;&#20214;&#24182;&#25512;&#26029;&#29289;&#20307;&#30340;&#21487;&#20379;&#24615;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVLIR&#30340;&#39044;&#22788;&#29702;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#21644;&#21512;&#29702;&#21270;&#20219;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#19977;&#20010;&#31574;&#30053;&#30340;&#23398;&#20064;&#36164;&#28304;&#26469;&#26816;&#32034;&#35813;&#20219;&#21153;&#30340;&#24369;&#30417;&#30563;&#20449;&#21495;&#65292;&#24182;&#21046;&#23450;&#20102;&#32463;&#36807;&#20154;&#24037;&#39564;&#35777;&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#30340;&#32570;&#38519;&#65292;&#24182;&#32472;&#21046;&#20102;&#26410;&#26469;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can infer the affordance of objects by extracting related contextual preconditions for each scenario. For example, upon seeing an image of a broken cup, we can infer that this precondition prevents the cup from being used for drinking. Reasoning with preconditions of commonsense is studied in NLP where the model explicitly gets the contextual precondition. However, it is unclear if SOTA visual language models (VLMs) can extract such preconditions and infer the affordance of objects with them. In this work, we introduce the task of preconditioned visual language inference and rationalization (PVLIR). We propose a learning resource based on three strategies to retrieve weak supervision signals for the task and develop a human-verified test set for evaluation. Our results reveal the shortcomings of SOTA VLM models in the task and draw a road map to address the challenges ahead in improving them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#23398;&#31185;&#65306;&#8220;&#35299;&#37322;&#24037;&#31243;&#23398;&#8221;&#65292;&#35813;&#23398;&#31185;&#21253;&#25324;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#21508;&#31181;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.01750</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#32508;&#36848;&#21450;&#23545;&#35299;&#37322;&#24037;&#31243;&#23398;&#31185;&#30340;&#25552;&#35758;
&lt;/p&gt;
&lt;p&gt;
A Survey of Explainable AI and Proposal for a Discipline of Explanation Engineering. (arXiv:2306.01750v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#23398;&#31185;&#65306;&#8220;&#35299;&#37322;&#24037;&#31243;&#23398;&#8221;&#65292;&#35813;&#23398;&#31185;&#21253;&#25324;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#21508;&#31181;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#22312;&#20171;&#32461;&#25991;&#31456;&#30340;&#33539;&#22260;&#21518;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#8220;&#35299;&#37322;&#8221;&#30340;&#30495;&#27491;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#26368;&#21463;&#27426;&#36814;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36825;&#20123;&#20197;&#21450;&#20854;&#20182;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#37329;&#34701;&#12289;&#33258;&#21160;&#39550;&#39542;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21046;&#36896;&#31561;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#23398;&#31185;&#65292;&#8220;&#35299;&#37322;&#24037;&#31243;&#23398;&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#21508;&#31181;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this survey paper, we deep dive into the field of Explainable Artificial Intelligence (XAI). After introducing the scope of this paper, we start by discussing what an "explanation" really is. We then move on to discuss some of the existing approaches to XAI and build a taxonomy of the most popular methods. Next, we also look at a few applications of these and other XAI techniques in four primary domains: finance, autonomous driving, healthcare and manufacturing. We end by introducing a promising discipline, "Explanation Engineering," which includes a systematic approach for designing explainability into AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#20013;&#24615;&#38598;&#36827;&#34892;&#20915;&#31574;&#21046;&#23450;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#20013;&#24615;&#19977;&#20803;&#32452;&#20195;&#26367;&#20108;&#36827;&#21046;&#20803;&#32032;&#26469;&#35299;&#20915;&#26576;&#20123;&#25110;&#25152;&#26377;&#20803;&#32032;&#30340;&#27169;&#31946;/&#23450;&#24615;&#29305;&#24449;&#23384;&#22312;&#30097;&#34385;&#30340;&#24773;&#20917;</title><link>http://arxiv.org/abs/2306.01746</link><description>&lt;p&gt;
&#20013;&#24615;&#38598;&#24212;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Application of Neutrosophic Sets to Decision Making. (arXiv:2306.01746v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#20013;&#24615;&#38598;&#36827;&#34892;&#20915;&#31574;&#21046;&#23450;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#20013;&#24615;&#19977;&#20803;&#32452;&#20195;&#26367;&#20108;&#36827;&#21046;&#20803;&#32032;&#26469;&#35299;&#20915;&#26576;&#20123;&#25110;&#25152;&#26377;&#20803;&#32032;&#30340;&#27169;&#31946;/&#23450;&#24615;&#29305;&#24449;&#23384;&#22312;&#30097;&#34385;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Maji&#31561;&#20154;&#22312;2002&#24180;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#38598;&#20316;&#20026;&#24037;&#20855;&#30340;&#21442;&#25968;&#21270;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#34920;&#26684;&#24418;&#24335;&#34920;&#31034;&#20026;&#20108;&#36827;&#21046;&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29992;&#20110;&#25551;&#36848;&#20840;&#38598;&#20803;&#32032;&#29305;&#24449;&#30340;&#26576;&#20123;&#25110;&#25152;&#26377;&#21442;&#25968;&#20855;&#26377;&#27169;&#31946;&#30340;&#36136;&#22320;&#26102;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#32473;&#20986;&#26368;&#20339;&#30340;&#20915;&#31574;&#21046;&#23450;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26089;&#26399;&#30340;&#30740;&#31350;&#20013;&#20462;&#25913;&#20102;Maji&#31561;&#20154;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30456;&#24212;&#30340;&#36719;&#38598;&#30340;&#34920;&#26684;&#24418;&#24335;&#20013;&#30340;&#20108;&#36827;&#21046;&#20803;&#32032;&#26367;&#25442;&#20026;&#28784;&#33394;&#25968;&#23383;&#25110;&#19977;&#35282;&#24418;&#27169;&#31946;&#25968;&#23383;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#20915;&#31574;&#21046;&#23450;&#20154;&#23545;&#20998;&#37197;&#32473;&#26576;&#20123;&#25110;&#25152;&#26377;&#20803;&#32032;&#30340;&#27169;&#31946;/&#23450;&#24615;&#29305;&#24449;&#30340;&#27491;&#30830;&#24615;&#23384;&#22312;&#30097;&#34385;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#23558;&#34920;&#26684;&#24418;&#24335;&#30340;&#20108;&#36827;&#21046;&#20803;&#32032;&#26367;&#25442;&#20026;&#20013;&#24615;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#26032;&#30340;&#12289;&#20013;&#24615;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#26159;&#36890;&#36807;&#19968;&#20010;&#26377;&#20851;&#36873;&#25321;&#26032;PL&#30340;&#24212;&#29992;&#26469;&#35828;&#26126;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maji et al. introduced in 2002 a method of parametric decision making using soft sets as tools and representing their tabular form as a binary matrix. In cases, however, where some or all of the parameters used for the characterization of the elements of the universal set are of fuzzy texture, their method does not give always the best decision making solution. In order to tackle this problem, we modified in earlier works the method of Maji et al. by replacing the binary elements in the tabular form of the corresponding soft set either by grey numbers or by triangular fuzzy numbers. In this work, in order to tackle more efficiently cases in which the decision maker has doubts about the correctness of the fuzzy/qualitative characterizations assigned to some or all of the elements of the universal set, we replace the binary elements of the tabular form by neutrosophic triplets. Our new, neutrosophic decision making method is illustrated by an application concerning the choice of a new pl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#19982;CLTA4&#36890;&#36335;&#30456;&#20851;&#30340;&#26032;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#19988;&#35813;&#27169;&#22411;&#32463;&#27982;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01745</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#65306;&#20197;CTLA4&#28608;&#27963;&#36890;&#36335;&#30340;&#26696;&#20363;&#30740;&#31350;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Biomarker Discovery with Quantum Neural Networks: A Case-study in CTLA4-Activation Pathways. (arXiv:2306.01745v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#19982;CLTA4&#36890;&#36335;&#30456;&#20851;&#30340;&#26032;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#19988;&#35813;&#27169;&#22411;&#32463;&#27982;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#25628;&#32034;&#31354;&#38388;&#24222;&#22823;&#12290;&#37327;&#23376;&#35745;&#31639;&#21644;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#65288;&#37327;&#23376;AI&#65289;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#29983;&#29289;&#26631;&#24535;&#29289;&#21457;&#29616;&#20219;&#21153;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#21457;&#29616;&#36755;&#20837;&#28608;&#27963;&#36890;&#36335;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#26368;&#22823;&#30456;&#20851;&#24615;&#65292;&#26368;&#23567;&#20887;&#20313;&#65288;mRMR&#65289;&#26631;&#20934;&#29992;&#20110;&#35780;&#20998;&#29983;&#29289;&#26631;&#24535;&#29289;&#20505;&#36873;&#38598;&#12290;&#30001;&#20110;&#31070;&#32463;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#21463;&#38480;&#30828;&#20214;&#19978;&#20132;&#20184;&#65292;&#25152;&#20197;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#32463;&#27982;&#30340;&#12290;&#25105;&#20204;&#22312;&#19982;CTLA4&#30456;&#20851;&#30340;&#22235;&#26465;&#28608;&#27963;&#36890;&#36335;&#19978;&#23637;&#31034;&#20102;&#35777;&#26126;&#27010;&#24565;&#65292;&#21253;&#25324;&#65288;1&#65289;CTLA4&#28608;&#27963;&#29420;&#31435;&#65292;&#65288;2&#65289;CTLA4-CD8A-CD8B&#20849;&#21516;&#28608;&#27963;&#65292;&#65288;3&#65289;CTLA4-CD2&#20849;&#21516;&#28608;&#27963;&#65292;&#20197;&#21450;&#65288;4&#65289;CTLA4-CD2-CD48-CD53-CD58-CD84&#20849;&#21516;&#28608;&#27963;&#12290;&#35813;&#27169;&#22411;&#25351;&#20986;&#19982;CLTA4&#30456;&#20851;&#36890;&#36335;&#30340;&#31361;&#21464;&#28608;&#27963;&#26377;&#26032;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#21253;&#25324;20&#20010;&#22522;&#22240;&#65306;CLIC4&#65292;CPE&#65292;ETS2&#65292;FAM107A&#65292;GPR116&#65292;HYOU1&#65292;LCN2&#65292;MACF1&#65292;MT1G&#65292;NAPA&#65292;NDUFS5&#65292;PAK1&#65292;PFN1&#65292;PGAP
&lt;/p&gt;
&lt;p&gt;
Biomarker discovery is a challenging task due to the massive search space. Quantum computing and quantum Artificial Intelligence (quantum AI) can be used to address the computational problem of biomarker discovery tasks. We propose a Quantum Neural Networks (QNNs) architecture to discover biomarkers for input activation pathways. The Maximum Relevance, Minimum Redundancy (mRMR) criteria is used to score biomarker candidate sets. Our proposed model is economical since the neural solution can be delivered on constrained hardware. We demonstrate the proof of concept on four activation pathways associated with CTLA4, including (1) CTLA4-activation stand-alone, (2) CTLA4-CD8A-CD8B co-activation, (3) CTLA4-CD2 co-activation, and (4) CTLA4-CD2-CD48-CD53-CD58-CD84 co-activation. The model indicates new biomarkers associated with the mutational activation of CLTA4-associated pathways, including 20 genes: CLIC4, CPE, ETS2, FAM107A, GPR116, HYOU1, LCN2, MACF1, MT1G, NAPA, NDUFS5, PAK1, PFN1, PGAP
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;XAI&#20013;&#26368;&#20005;&#37325;&#30340;&#19968;&#20123;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#25171;&#30772;&#36825;&#20123;&#35884;&#35265;&#21644;&#24320;&#21457;&#26356;&#23454;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.01744</link><description>&lt;p&gt;
&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#25171;&#30772;XAI&#31070;&#35805;-&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Disproving XAI Myths with Formal Methods -- Initial Results. (arXiv:2306.01744v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01744
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;XAI&#20013;&#26368;&#20005;&#37325;&#30340;&#19968;&#20123;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#25171;&#30772;&#36825;&#20123;&#35884;&#35265;&#21644;&#24320;&#21457;&#26356;&#23454;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#36827;&#23637;&#26082;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#21448;&#28145;&#36828;&#12290;&#28982;&#32780;&#65292;ML&#27169;&#22411;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#21040;&#20154;&#20204;&#23545;&#26368;&#20339;&#34920;&#29616;&#30340;ML&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#30340;&#20449;&#20219;&#32570;&#20047;&#30340;&#24433;&#21709;&#12290;&#22312;&#39640;&#39118;&#38505;&#25110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20351;&#29992;ML&#27169;&#22411;&#26102;&#65292;&#32570;&#20047;&#20449;&#20219;&#30340;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#20026;&#25552;&#20379;&#21487;&#20449;&#36182;AI&#32780;&#36827;&#34892;&#30340;&#19981;&#26029;&#21162;&#21147;&#30340;&#26680;&#24515;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;XAI&#20805;&#26021;&#30528;&#20851;&#38190;&#35823;&#35299;&#65292;&#36825;&#20123;&#35823;&#35299;&#21161;&#38271;&#20102;&#19981;&#20449;&#20219;&#32780;&#19981;&#26159;&#24314;&#31435;&#20449;&#20219;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;XAI&#20013;&#26368;&#26126;&#26174;&#30340;&#19968;&#20123;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#25512;&#32763;&#36825;&#20123;&#35823;&#35299;&#65292;&#20197;&#21450;&#35774;&#35745;&#23454;&#38469;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advances in Machine Learning (ML) in recent years have been both impressive and far-reaching. However, the deployment of ML models is still impaired by a lack of trust in how the best-performing ML models make predictions. The issue of lack of trust is even more acute in the uses of ML models in high-risk or safety-critical domains. eXplainable artificial intelligence (XAI) is at the core of ongoing efforts for delivering trustworthy AI. Unfortunately, XAI is riddled with critical misconceptions, that foster distrust instead of building trust. This paper details some of the most visible misconceptions in XAI, and shows how formal methods have been used, both to disprove those misconceptions, but also to devise practically effective alternatives.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;6&#20010;&#19981;&#21516;&#30340;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#24182;&#38024;&#23545;4&#31181;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20197;&#29992;&#20110;&#21028;&#20915;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22312;&#27861;&#24459;&#35785;&#35772;&#20013;&#36827;&#34892;&#24341;&#29992;&#21644;&#20808;&#20363;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2306.01739</link><description>&lt;p&gt;
&#22522;&#20110; Transformer &#27169;&#22411;&#30340;&#35009;&#21028;&#25991;&#26412;&#20998;&#31867;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative study on Judgment Text Classification for Transformer Based Models. (arXiv:2306.01739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;6&#20010;&#19981;&#21516;&#30340;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#24182;&#38024;&#23545;4&#31181;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20197;&#29992;&#20110;&#21028;&#20915;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22312;&#27861;&#24459;&#35785;&#35772;&#20013;&#36827;&#34892;&#24341;&#29992;&#21644;&#20808;&#20363;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#21028;&#20915;&#25991;&#20070;&#20013;&#25552;&#21462;&#21644;&#25688;&#35201;&#25991;&#26412;&#26469;&#39044;&#27979;&#29305;&#23450;&#21028;&#20915;&#30340;&#33719;&#32988;&#32773;&#12290;&#36825;&#20123;&#25991;&#26723;&#22312;&#27861;&#24459;&#35785;&#35772;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#20854;&#20013;&#19968;&#20010;&#20248;&#28857;&#26159;&#65292;&#36825;&#20123;&#25991;&#26723;&#21487;&#29992;&#20110;&#24341;&#29992;&#21644;&#20808;&#20363;&#21442;&#32771;&#65292;&#36825;&#20351;&#24471;&#20351;&#29992;&#32773;&#21487;&#20197;&#26356;&#26377;&#21147;&#22320;&#20026;&#20854;&#26696;&#20214;&#36777;&#25252;&#12290;&#24403;&#28041;&#21450;&#21040;&#26696;&#20363;&#20808;&#20363;&#26102;&#65292;&#24517;&#39035;&#21442;&#32771;&#22823;&#37327;&#25991;&#26723;&#65292;&#20197;&#25910;&#38598;&#19982;&#35813;&#26696;&#20214;&#26377;&#20851;&#30340;&#27861;&#24459;&#35201;&#28857;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25991;&#26723;&#30340;&#22797;&#26434;&#35789;&#27719;&#32467;&#26500;&#21644;&#25991;&#26723;&#22823;&#23567;&#65292;&#23457;&#26597;&#36825;&#20123;&#25991;&#26723;&#38656;&#35201;&#33457;&#36153;&#24456;&#38271;&#26102;&#38388;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#25991;&#28041;&#21450;&#22312;4&#31181;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#19979;&#35843;&#25972; 6 &#31181;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#35757;&#32451;&#20102; 200 &#20010;&#21028;&#20915;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#26681;&#25454;&#19981;&#21516;&#22522;&#20934;&#21442;&#25968;&#35780;&#20272;&#20854;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work involves the usage of various NLP models to predict the winner of a particular judgment by the means of text extraction and summarization from a judgment document. These documents are useful when it comes to legal proceedings. One such advantage is that these can be used for citations and precedence reference in Lawsuits and cases which makes a strong argument for their case by the ones using it. When it comes to precedence, it is necessary to refer to an ample number of documents in order to collect legal points with respect to the case. However, reviewing these documents takes a long time to analyze due to the complex word structure and the size of the document. This work involves the comparative study of 6 different self-attention-based transformer models and how they perform when they are being tweaked in 4 different activation functions. These models which are trained with 200 judgement contexts and their results are being judged based on different benchmark parameters. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#38480;&#21046;&#19979;&#30340;&#20004;&#31181;&#26694;&#26550;&#65292;&#21363;&#20215;&#20540;&#36845;&#20195;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65292;&#24182;&#20026;&#20004;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.01121</link><description>&lt;p&gt;
&#24102;&#26377;&#37325;&#23614;&#22870;&#21169;&#30340;&#24046;&#20998;&#38544;&#31169;&#24335;&#24773;&#33410;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards. (arXiv:2306.01121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#38480;&#21046;&#19979;&#30340;&#20004;&#31181;&#26694;&#26550;&#65292;&#21363;&#20215;&#20540;&#36845;&#20195;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65292;&#24182;&#20026;&#20004;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;(DP)&#38480;&#21046;&#19979;&#30340;&#37325;&#23614;&#22870;&#21169;&#30340;&#65288;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#65289;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#30340;&#31169;&#26377;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#22870;&#21169;&#26469;&#33258;&#19968;&#20123;&#26377;&#30028;&#25110;&#27425;&#39640;&#26031;&#20998;&#24067;&#20197;&#30830;&#20445;DP&#30456;&#27604;&#65292;&#25105;&#20204;&#32771;&#34385;&#22870;&#21169;&#20998;&#24067;&#21482;&#26377;&#26377;&#38480;&#30340;$(1+v)$&#38454;&#30697;&#30340;&#24773;&#20917;&#65292;$v \in (0,1]$&#12290;&#36890;&#36807;&#20351;&#29992;&#22870;&#21169;&#30340;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#37325;&#23614;MDP&#30340;&#26694;&#26550;&#65292;&#21363;&#19968;&#20010;&#29992;&#20110;&#20215;&#20540;&#36845;&#20195;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#12290;&#22312;&#27599;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;(JDP)&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;(LDP)&#27169;&#22411;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;JDP&#21644;LDP&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#65292;&#24182;&#34920;&#26126;&#20998;&#24067;&#30340;&#30697;&#21644;&#38544;&#31169;&#39044;&#31639;&#37117;&#23545;&#36951;&#25022;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of (finite horizon tabular) Markov decision processes (MDPs) with heavy-tailed rewards under the constraint of differential privacy (DP). Compared with the previous studies for private reinforcement learning that typically assume rewards are sampled from some bounded or sub-Gaussian distributions to ensure DP, we consider the setting where reward distributions have only finite $(1+v)$-th moments with some $v \in (0,1]$. By resorting to robust mean estimators for rewards, we first propose two frameworks for heavy-tailed MDPs, i.e., one is for value iteration and another is for policy optimization. Under each framework, we consider both joint differential privacy (JDP) and local differential privacy (LDP) models. Based on our frameworks, we provide regret upper bounds for both JDP and LDP cases and show that the moment of distribution and privacy budget both have significant impacts on regrets. Finally, we establish a lower bound of regret minimization
&lt;/p&gt;</description></item><item><title>STEVE-1 &#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;Minecraft&#20013;&#36319;&#38543;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.00937</link><description>&lt;p&gt;
STEVE-1: &#19968;&#20010;&#29992;&#20110;Minecraft&#20013;&#25991;&#26412;-&#34892;&#20026;&#29983;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. (arXiv:2306.00937v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00937
&lt;/p&gt;
&lt;p&gt;
STEVE-1 &#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;Minecraft&#20013;&#36319;&#38543;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#23545;&#25991;&#26412;&#25351;&#20196;&#20570;&#20986;&#21709;&#24212;&#30340;AI&#27169;&#22411;&#23545;&#20110;&#36830;&#32493;&#24615;&#20915;&#31574;&#20219;&#21153;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;STEVE-1&#30340;Minecraft&#25351;&#20196;&#35843;&#25972;&#22411;&#35270;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;DALL-E 2&#20013;&#20351;&#29992;&#30340;unCLIP&#26041;&#27861;&#20063;&#23545;&#21019;&#24314;&#25351;&#20196;&#36319;&#38543;&#36830;&#32493;&#20915;&#31574;&#20195;&#29702;&#38750;&#24120;&#26377;&#25928;&#12290;STEVE-1&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#35757;&#32451;&#65306;&#39318;&#20808;&#26159;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;VPT&#27169;&#22411;&#36866;&#24212;MineCLIP&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25351;&#20196;&#65292;&#28982;&#21518;&#35757;&#32451;&#19968;&#20010;&#20808;&#39564;&#27169;&#22411;&#20197;&#20174;&#25991;&#26412;&#39044;&#27979;&#28508;&#22312;&#20195;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;VPT&#65292;&#36991;&#20813;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#25991;&#26412;&#27880;&#37322;&#12290;&#36890;&#36807;&#21033;&#29992;VPT&#21644;MineCLIP&#31561;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;STEVE-1&#30340;&#35757;&#32451;&#25104;&#26412;&#20165;&#20026;60&#32654;&#20803;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Minecraft&#20013;&#36981;&#24490;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#20026;&#24320;&#25918;&#30340;&#25351;&#20196;&#36319;&#38543;&#36830;&#32493;&#20915;&#31574;&#20195;&#29702;&#35774;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces an instruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1, demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effective for creating instruction-following sequential decision-making agents. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, bypassing the need for costly human text annotations. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 costs just $60 to train and can follow a wide range of short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 sets a new bar for open-ended instructi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#12289;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;dropout&#12289;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#21644;&#26368;&#23567;&#21270;&#29109;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00418</link><description>&lt;p&gt;
&#8220;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#38750;&#20284;&#28982;&#23398;&#20064;&#25552;&#39640;&#29983;&#25104;&#24335;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#8221;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction. (arXiv:2306.00418v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#12289;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;dropout&#12289;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#21644;&#26368;&#23567;&#21270;&#29109;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#24191;&#27867;&#20851;&#27880;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20986;&#22235;&#20803;&#32452;&#65292;&#23558;&#21407;&#22987;&#21477;&#23376;&#36716;&#21270;&#20026;&#27169;&#26495;&#21270;&#30340;&#30446;&#26631;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#21482;&#20851;&#27880;&#29983;&#25104;&#20160;&#20040;&#65292;&#32780;&#24573;&#30053;&#20102;&#19981;&#38656;&#35201;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#35748;&#20026;&#32771;&#34385;&#36127;&#26679;&#26412;&#20063;&#20250;&#24102;&#26469;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#65292;&#21516;&#26102;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33945;&#29305;&#21345;&#27931;dropout&#26469;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#33719;&#21462;&#22122;&#22768;&#21644;&#38169;&#35823;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#26469;&#25233;&#21046;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#38169;&#35823;&#26631;&#35760;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#23567;&#21270;&#29109;&#26469;&#24179;&#34913;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to control the token-level generation, which boosts original learning and reduces mistakes simultaneously. Specifically, we introduce Monte Carlo dropout to understand the built-in uncertainty of pre-trained language models, acquiring the noises and errors. We further propose marginalized unlikelihood learning to suppress the uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to balance the effects of marginalized unlikelihood learning. Extensive experiments on four public
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;BetaZero&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDP&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00249</link><description>&lt;p&gt;
BetaZero&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#29992;&#20110;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDPs
&lt;/p&gt;
&lt;p&gt;
BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations. (arXiv:2306.00249v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;BetaZero&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDP&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#12289;&#30899;&#20648;&#23384;&#21644;&#36164;&#28304;&#21208;&#25506;&#31561;&#21487;&#25345;&#32493;&#33021;&#28304;&#24212;&#29992;&#65292;&#26368;&#36817;&#34987;&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#24182;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#12290;&#20026;&#20102;&#22312;&#23454;&#36341;&#20013;&#35299;&#20915;&#39640;&#32500;&#24230;POMDPs&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#38382;&#39064;&#29305;&#23450;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#65292;&#20197;&#20943;&#23569;&#35268;&#21010;&#26102;&#38388;&#36328;&#24230;&#24182;&#20351;&#38382;&#39064;&#26131;&#20110;&#35299;&#20915;&#12290;&#26368;&#36817;&#25104;&#21151;&#22320;&#22312;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#39046;&#22495;&#20013;&#25214;&#21040;&#20102;&#29992;&#20110;&#26367;&#25442;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#12290;&#20851;&#38190;&#27934;&#35265;&#26159;&#23558;&#22312;&#32447;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#19982;&#31163;&#32447;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#30456;&#32467;&#21512;&#65292;&#20197;&#20248;&#21270;&#31574;&#30053;&#21644;&#20540;&#20989;&#25968;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#27934;&#35265;&#24212;&#29992;&#21040;&#20102;&#37096;&#20998;&#35266;&#23519;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;BetaZero&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;POMDP&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world planning problems$\unicode{x2014}$including autonomous driving and sustainable energy applications like carbon storage and resource exploration$\unicode{x2014}$have recently been modeled as partially observable Markov decision processes (POMDPs) and solved using approximate methods. To solve high-dimensional POMDPs in practice, state-of-the-art methods use online planning with problem-specific heuristics to reduce planning horizons and make the problems tractable. Algorithms that learn approximations to replace heuristics have recently found success in large-scale problems in the fully observable domain. The key insight is the combination of online Monte Carlo tree search with offline neural network approximations of the optimal policy and value function. In this work, we bring this insight to partially observed domains and propose BetaZero, a belief-state planning algorithm for POMDPs. BetaZero learns offline approximations based on accurate belief models to enable online d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;&#21644;&#20116;&#20010;&#26376;&#20221;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00114</link><description>&lt;p&gt;
&#21152;&#25343;&#22823;&#20892;&#30000;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#20892;&#19994;&#22810;&#26102;&#30456;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#30340;&#26032;&#22320;&#34920;&#35206;&#30422;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture. (arXiv:2306.00114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;&#21644;&#20116;&#20010;&#26376;&#20221;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#30417;&#27979;&#22303;&#22320;&#35206;&#30422;&#26159;&#30740;&#31350;&#29615;&#22659;&#21464;&#21270;&#21644;&#36890;&#36807;&#31918;&#39135;&#20135;&#37327;&#39044;&#27979;&#30830;&#20445;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#23588;&#20854;&#26159;&#65292;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#25552;&#20379;&#20102;&#20851;&#20110;&#22330;&#26223;&#21160;&#24577;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#33719;&#21462;&#21487;&#38752;&#12289;&#32454;&#31890;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#26679;&#26412;&#25903;&#25345;&#20182;&#20204;&#30340;&#20551;&#35774;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21463;&#30410;&#20110;&#39640;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;10&#20010;&#20892;&#20316;&#29289;&#31867;&#21035;&#30340;78,536&#20010;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;(10&#31859;/&#20687;&#32032;&#65292;640 x 640&#31859;)&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#20102;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;(2017-2020)&#21644;&#20116;&#20010;&#26376;&#20221;(&#20845;&#26376;-&#21313;&#26376;)&#12290;&#27599;&#20010;&#23454;&#20363;&#37117;&#21253;&#21547;12&#20010;&#20809;&#35889;&#27874;&#27573;&#12289;&#19968;&#24352;RGB&#22270;&#20687;&#21644;&#39069;&#22806;&#30340;&#26893;&#34987;&#25351;&#25968;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring land cover using remote sensing is vital for studying environmental changes and ensuring global food security through crop yield forecasting. Specifically, multitemporal remote sensing imagery provides relevant information about the dynamics of a scene, which has proven to lead to better land cover classification results. Nevertheless, few studies have benefited from high spatial and temporal resolution data due to the difficulty of accessing reliable, fine-grained and high-quality annotated samples to support their hypotheses. Therefore, we introduce a temporal patch-based dataset of Canadian croplands, enriched with labels retrieved from the Canadian Annual Crop Inventory. The dataset contains 78,536 manually verified and curated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop classes collected over four crop production years (2017-2020) and five months (June-October). Each instance contains 12 spectral bands, an RGB image, and additional veget
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#22686;&#38271;&#35753;&#25163;&#24037;&#33402;&#26415;&#21697;&#21644;AI&#29983;&#25104;&#22270;&#20687;&#21306;&#20998;&#26085;&#28176;&#22256;&#38590;&#12290;&#28982;&#32780;&#25552;&#39640;&#20154;&#31867;&#29983;&#27963;&#21644;&#24037;&#20316;&#26631;&#20934;&#20197;&#21450;&#21033;&#29992;&#19968;&#32676;&#20154;&#26469;&#20805;&#23454;&#21478;&#19968;&#32676;&#20154;&#20043;&#38388;&#23547;&#27714;&#24179;&#34913;&#26159;&#20851;&#38190;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#30528;&#34987;AI&#22522;&#30784;&#35774;&#26045;&#25509;&#31649;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#23384;&#22312;&#36523;&#20221;&#30423;&#31363;&#12289;&#25968;&#25454;&#27927;&#30333;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00080</link><description>&lt;p&gt;
AI&#22270;&#20687;&#21644;Overton Window
&lt;/p&gt;
&lt;p&gt;
AI Imagery and the Overton Window. (arXiv:2306.00080v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00080
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#22686;&#38271;&#35753;&#25163;&#24037;&#33402;&#26415;&#21697;&#21644;AI&#29983;&#25104;&#22270;&#20687;&#21306;&#20998;&#26085;&#28176;&#22256;&#38590;&#12290;&#28982;&#32780;&#25552;&#39640;&#20154;&#31867;&#29983;&#27963;&#21644;&#24037;&#20316;&#26631;&#20934;&#20197;&#21450;&#21033;&#29992;&#19968;&#32676;&#20154;&#26469;&#20805;&#23454;&#21478;&#19968;&#32676;&#20154;&#20043;&#38388;&#23547;&#27714;&#24179;&#34913;&#26159;&#20851;&#38190;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#30528;&#34987;AI&#22522;&#30784;&#35774;&#26045;&#25509;&#31649;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#23384;&#22312;&#36523;&#20221;&#30423;&#31363;&#12289;&#25968;&#25454;&#27927;&#30333;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#22312;&#35270;&#35273;&#32508;&#21512;&#21644;&#32654;&#23398;&#24418;&#35937;&#30340;&#29983;&#20135;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#27493;&#65292;&#21040;&#20102;&#21306;&#20998;&#25163;&#24037;&#33402;&#26415;&#21697;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#24840;&#21457;&#22256;&#38590;&#30340;&#22320;&#27493;&#12290;&#20363;&#22914;&#31283;&#24577;&#25193;&#25955;&#12289;Midjourney&#31561;&#29983;&#25104;&#27169;&#22411;&#26377;&#26395;&#22312;&#25216;&#26415;&#21644;&#20262;&#29702;&#26041;&#38754;&#24433;&#21709;&#20960;&#20010;&#20027;&#35201;&#34892;&#19994;&#12290;&#22312;&#25552;&#39640;&#20154;&#31867;&#29983;&#27963;&#21644;&#24037;&#20316;&#26631;&#20934;&#20197;&#21450;&#21033;&#29992;&#19968;&#32676;&#20154;&#26469;&#20805;&#23454;&#21478;&#19968;&#32676;&#20154;&#20043;&#38388;&#23547;&#27714;&#24179;&#34913;&#26159;&#35752;&#35770;&#30340;&#22797;&#26434;&#21644;&#20851;&#38190;&#37096;&#20998;&#12290;&#30001;&#20110;&#36825;&#31181;&#25216;&#26415;&#30340;&#24555;&#36895;&#22686;&#38271;&#12289;&#27169;&#22411;&#36816;&#34892;&#26041;&#24335;&#21644;&#28784;&#33394;&#27861;&#24459;&#30340;&#23384;&#22312;&#65292;&#21253;&#25324;&#35270;&#39057;&#28216;&#25103;&#34892;&#19994;&#22312;&#20869;&#30340;&#35270;&#35273;&#21644;&#33402;&#26415;&#39046;&#22495;&#38754;&#20020;&#34987;AI&#22522;&#30784;&#35774;&#26045;&#25152;&#26377;&#32773;&#25509;&#31649;&#30340;&#39118;&#38505;&#12290;&#35813;&#25991;&#31456;&#26159;&#19968;&#31687;&#25991;&#29486;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#24403;&#20170;AI&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#36523;&#20221;&#30423;&#31363;&#12289;&#25968;&#25454;&#27927;&#30333;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-based text-to-image generation has undergone a significant leap in the production of visually comprehensive and aesthetic imagery over the past year, to the point where differentiating between a man-made piece of art and an AI-generated image is becoming more difficult. Generative Models such as Stable Diffusion, Midjourney and others are expected to affect several major industries in technological and ethical aspects. Striking the balance between raising human standard of life and work vs exploiting one group of people to enrich another is a complex and crucial part of the discussion. Due to the rapid growth of this technology, the way in which its models operate, and gray area legalities, visual and artistic domains - including the video game industry, are at risk of being taken over from creators by AI infrastructure owners. This paper is a literature review examining the concerns facing both AI developers and users today, including identity theft, data laundering and more. It di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#20803;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30740;&#31350;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2305.19591</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#65306;&#36817;&#26399;&#36827;&#23637;&#19982;&#26032;&#26426;&#36935;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Traffic Prediction using Artificial Intelligence: Review of Recent Advances and Emerging Opportunities. (arXiv:2305.19591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#20803;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30740;&#31350;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#22312;&#32531;&#35299;&#20840;&#29699;&#24615;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20854;&#36127;&#38754;&#24433;&#21709;&#21253;&#25324;&#39069;&#22806;&#26053;&#34892;&#26102;&#38388;&#30340;&#25439;&#22833;&#21644;&#29123;&#26009;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;&#23558;&#26032;&#20852;&#25216;&#26415;&#34701;&#20837;&#20132;&#36890;&#31995;&#32479;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20132;&#36890;&#39044;&#27979;&#65292;&#24182;&#24102;&#26469;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#20102;&#35299;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#65292;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20391;&#37325;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#21464;&#37327;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#30340;&#36817;&#26399;&#36827;&#23637;&#21644;&#26032;&#30340;&#30740;&#31350;&#26426;&#36935;&#65292;&#36825;&#26159;&#30001;&#20110;&#36817;&#24180;&#26469;&#36825;&#31867;&#26041;&#27861;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#25104;&#21151;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction plays a crucial role in alleviating traffic congestion which represents a critical problem globally, resulting in negative consequences such as lost hours of additional travel time and increased fuel consumption. Integrating emerging technologies into transportation systems provides opportunities for improving traffic prediction significantly and brings about new research problems. In order to lay the foundation for understanding the open research challenges in traffic prediction, this survey aims to provide a comprehensive overview of traffic prediction methodologies. Specifically, we focus on the recent advances and emerging research opportunities in Artificial Intelligence (AI)-based traffic prediction methods, due to their recent success and potential in traffic prediction, with an emphasis on multivariate traffic time series modeling. We first provide a list and explanation of the various data types and resources used in the literature. Next, the essential data 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#29289;&#20307;&#20998;&#21106;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.19550</link><description>&lt;p&gt;
Spotlight Attention: &#20855;&#22791;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#30340;&#40065;&#26834;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spotlight Attention: Robust Object-Centric Learning With a Spatial Locality Prior. (arXiv:2305.19550v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#29289;&#20307;&#20998;&#21106;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#20013;&#24515;&#35270;&#35273;&#30340;&#30446;&#30340;&#26159;&#26500;&#24314;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#26174;&#24335;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#26159;&#36890;&#36807;&#19968;&#32452;&#21487;&#20114;&#25442;&#30340;&#27169;&#22359;(&#31216;&#20026;slot&#25110;&#23545;&#35937;&#25991;&#20214;)&#33719;&#24471;&#30340;&#65292;&#23427;&#20204;&#31454;&#20105;&#22270;&#20687;&#30340;&#23616;&#37096;&#34917;&#19969;&#12290;&#35813;&#31454;&#20105;&#20855;&#26377;&#24369;&#24863;&#24615;&#20559;&#24046;&#65292;&#20197;&#20445;&#25345;&#31354;&#38388;&#36830;&#32493;&#24615;;&#22240;&#27492;&#65292;&#19968;&#20010;slot&#21487;&#33021;&#20250;&#23459;&#31216;&#22312;&#25972;&#20010;&#22270;&#20687;&#20013;&#25955;&#24067;&#30340;&#34917;&#19969;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20154;&#31867;&#35270;&#35273;&#30340;&#24863;&#24615;&#20559;&#24046;&#24456;&#24378;&#65292;&#21040;&#20102;&#27880;&#24847;&#21147;&#32463;&#20856;&#29992;&#32858;&#20809;&#28783;&#27604;&#21947;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#34701;&#20837;&#29616;&#20195;&#30446;&#26631;&#20013;&#24515;&#35270;&#35273;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#26174;&#30528;&#30340;&#29289;&#20307;&#20998;&#21106;&#25913;&#36827;&#12290;&#31867;&#20284;&#20110;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#22270;&#20687;&#20869;&#23481;&#21644;&#31354;&#38388;&#32422;&#26463;&#30340;&#32452;&#21512;&#20135;&#29983;&#20102;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26080;&#30417;&#30563;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#65292;&#21253;&#25324;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of object-centric vision is to construct an explicit representation of the objects in a scene. This representation is obtained via a set of interchangeable modules called \emph{slots} or \emph{object files} that compete for local patches of an image. The competition has a weak inductive bias to preserve spatial continuity; consequently, one slot may claim patches scattered diffusely throughout the image. In contrast, the inductive bias of human vision is strong, to the degree that attention has classically been described with a spotlight metaphor. We incorporate a spatial-locality prior into state-of-the-art object-centric vision models and obtain significant improvements in segmenting objects in both synthetic and real-world datasets. Similar to human visual attention, the combination of image content and spatial constraints yield robust unsupervised object-centric learning, including less sensitivity to model hyperparameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.18885</link><description>&lt;p&gt;
&#26631;&#20934;&#27604;&#35780;&#20998;&#26356;&#37325;&#35201;&#65306;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20934;&#21017;&#25512;&#33616;&#31995;&#32479;&#29616;&#22312;&#22312;&#24191;&#27867;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#21033;&#29992;&#22810;&#20934;&#21017; (MC) &#35780;&#20998;&#20449;&#24687;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;GNN&#36741;&#21161;&#35774;&#35745;MC&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#26041;&#27861;(CPA-LGC),&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#20197;&#21450;&#22797;&#26434;&#39640;&#38454;&#36830;&#25509;&#20013;&#30340;&#21327;&#20316;&#20449;&#21495;&#12290;&#26412;&#25991;&#22312;MC&#25193;&#23637;&#22270;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#29992;&#25143;-&#29289;&#21697;MC&#35780;&#20998;&#36716;&#25442;&#20026;&#25193;&#23637;&#20108;&#20998;&#22270;&#30340;MC&#25193;&#23637;&#22270;&#65292;&#20877;&#36827;&#19968;&#27493;&#23558;&#26631;&#20934;&#37325;&#35201;&#24615;&#32534;&#30721;&#21040;&#22270;&#21367;&#31215;&#36807;&#31243;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#32858;&#21512;&#26041;&#27861;&#26469;&#23558;&#29992;&#25143;&#23545;&#19981;&#21516;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18477</link><description>&lt;p&gt;
&#36229;&#36234;&#20803;&#25968;&#25454;&#65306;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#36827;&#34892;&#36328;&#29256;&#26412;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#26159;&#20840;&#29699;&#28216;&#25103;&#24066;&#22330;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#26159;&#22686;&#38271;&#26368;&#24555;&#30340;&#28216;&#25103;&#32454;&#20998;&#39046;&#22495;&#12290;&#36825;&#23548;&#33268;&#20102;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#30340;&#39046;&#22495;&#20135;&#29983;&#65292;&#20854;&#20351;&#29992;&#28216;&#25103;&#25552;&#21462;&#30340;&#36965;&#27979;&#25968;&#25454;&#26469;&#20026;&#29609;&#23478;&#12289;&#25945;&#32451;&#12289;&#25773;&#38899;&#21592;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#30340;&#20307;&#32946;&#27604;&#36187;&#30456;&#27604;&#65292;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#30340;&#26426;&#21046;&#21644;&#35268;&#21017;&#32463;&#24120;&#21457;&#29983;&#24555;&#36895;&#21464;&#21270;&#12290;&#30001;&#20110;&#28216;&#25103;&#21442;&#25968;&#30340;&#39057;&#32321;&#26356;&#25913;&#65292;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#27169;&#22411;&#30340;&#20351;&#29992;&#23551;&#21629;&#21487;&#33021;&#24456;&#30701;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#30053;&#20102;&#12290;&#26412;&#25991;&#25552;&#21462;&#28216;&#25103;&#35774;&#35745;&#20449;&#24687;&#65288;&#21363;&#34917;&#19969;&#35828;&#26126;&#65289;&#65292;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#12290;&#20197;Dota 2&#28216;&#25103;&#20013;&#20987;&#26432;&#27425;&#25968;&#30340;&#39044;&#27979;&#20026;&#26696;&#20363;&#65292;&#21033;&#29992;&#36825;&#31181;&#21019;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#25216;&#26415;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#27492;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21253;&#25324;&#24120;&#35268;&#25216;&#26415;&#22312;&#20869;&#30340;&#20004;&#20010;&#19981;&#21516;&#22522;&#32447;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#27700;&#24179;&#65292;&#36824;&#20811;&#26381;&#20102;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#20013;&#29256;&#26412;&#26356;&#36845;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;CoTASP&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20043;&#21069;&#20219;&#21153;&#30340;&#20849;&#21516;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.18444</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#25552;&#31034;&#30340;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#30340;&#25345;&#32493;&#20219;&#21153;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Continual Task Allocation in Meta-Policy Network via Sparse Prompting. (arXiv:2305.18444v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;CoTASP&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20043;&#21069;&#20219;&#21153;&#30340;&#20849;&#21516;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#19981;&#26029;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#26469;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#19968;&#33324;&#21270;&#33021;&#21147;&#30340;&#20803;&#31574;&#30053;&#65292;&#26159;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36830;&#32493;&#20219;&#21153;&#20998;&#37197;&#30340;&#31232;&#30095;&#25552;&#31034;&#65288;CoTASP&#65289;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#12290;&#36890;&#36807;&#20132;&#26367;&#20248;&#21270;&#23376;&#32593;&#32476;&#21644;&#25552;&#31034;&#65292;CoTASP&#26356;&#26032;&#20102;&#20803;&#31574;&#30053;&#65292;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#28982;&#21518;&#26356;&#26032;&#23383;&#20856;&#65292;&#20197;&#20351;&#20248;&#21270;&#21518;&#30340;&#25552;&#31034;&#19982;&#20219;&#21153;&#23884;&#20837;&#30456;&#21305;&#37197;&#65292;&#20174;&#32780;&#25429;&#25417;&#20854;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#30456;&#20851;&#20219;&#21153;&#36890;&#36807;&#30456;&#20284;&#30340;&#25552;&#31034;&#22312;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#20849;&#20139;&#26356;&#22810;&#30340;&#31070;&#32463;&#20803;&#65292;&#32780;&#36328;&#20219;&#21153;&#24178;&#25200;&#23548;&#33268;&#36951;&#24536;&#34987;&#26377;&#25928;&#22320;&#32422;&#26463;&#12290;&#32473;&#23450;&#32463;&#36807;&#35757;&#32451;&#30340;&#20803;&#31574;&#30053;&#21644;&#26356;&#26032;&#21518;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25512;&#23548;&#30456;&#24212;&#30340;&#25552;&#31034;&#26469;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#20013;&#25552;&#21462;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#23548;&#33322;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;CoTASP&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20219;&#21153;&#23436;&#25104;&#24230;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by "Continual Task Allocation via Sparse Prompting (CoTASP)", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. By optimizing the sub-network and prompts alternatively, CoTASP updates the meta-policy via training a task-specific policy. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing their semantic correlations. Hence, relevant tasks share more neurons in the meta-policy network via similar prompts while cross-task interference causing forgetting is effectively restrained. Given a trained meta-policy with updated dicti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#30340;&#24863;&#30693;&#29305;&#24449;&#32534;&#30721;&#25104;&#35821;&#35328;&#24418;&#24335;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38646;-shot&#20851;&#31995;&#25512;&#29702;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#21644;&#20154;&#31867;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.17626</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#31867;&#27604;&#25512;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Context Analogical Reasoning with Pre-Trained Language Models. (arXiv:2305.17626v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#30340;&#24863;&#30693;&#29305;&#24449;&#32534;&#30721;&#25104;&#35821;&#35328;&#24418;&#24335;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38646;-shot&#20851;&#31995;&#25512;&#29702;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#21644;&#20154;&#31867;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#22522;&#26412;&#33021;&#21147;&#20043;&#19968;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#26032;&#30340;&#24773;&#20917;&#19982;&#36807;&#21435;&#30340;&#32463;&#39564;&#20851;&#32852;&#26469;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#12290;&#34429;&#28982;&#23427;&#34987;&#35748;&#20026;&#23545;&#20110;AI&#31995;&#32479;&#30340;&#24378;&#22823;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#35757;&#32451;&#21644;/&#25110;&#22266;&#21270;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#25165;&#33021;&#24212;&#29992;&#20110;&#22522;&#20934;&#20219;&#21153;&#20013;&#12290;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#21457;&#29616;&#20154;&#31867;&#35821;&#35328;&#19982;&#31867;&#27604;&#21046;&#20316;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#30452;&#35266;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#25277;&#35937;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#31867;&#27604;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#35270;&#35273;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#36827;&#34892;&#31867;&#27604;&#25512;&#29702;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#30340;&#24863;&#30693;&#29305;&#24449;&#31616;&#21333;&#22320;&#32534;&#30721;&#25104;&#35821;&#35328;&#24418;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;PLMs&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#38646;-shot&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#65292;&#36229;&#36807;&#20102;&#20154;&#31867;&#34920;&#29616;&#24182;&#25509;&#36817;&#20110;&#21463;&#30417;&#30563;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#20197;&#21464;&#21270;&#25277;&#35937;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven's Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#30340;&#29616;&#29366;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#38590;&#20197;&#26816;&#27979;&#21040;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#21487;&#20197;&#26816;&#27979;&#21040;&#21160;&#24577;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#25351;&#26631;&#25193;&#23637;&#20102;&#29305;&#24322;&#24615;&#30340;&#34913;&#37327;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#29305;&#24322;&#24615;&#36739;&#20302;&#65292;&#24378;&#35843;&#20102;&#25913;&#36827;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17553</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#22833;&#36133;&#65306;&#19968;&#20010;&#25913;&#36827;&#30340;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark. (arXiv:2305.17553v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#30340;&#29616;&#29366;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#38590;&#20197;&#26816;&#27979;&#21040;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#21487;&#20197;&#26816;&#27979;&#21040;&#21160;&#24577;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#25351;&#26631;&#25193;&#23637;&#20102;&#29305;&#24322;&#24615;&#30340;&#34913;&#37327;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#29305;&#24322;&#24615;&#36739;&#20302;&#65292;&#24378;&#35843;&#20102;&#25913;&#36827;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#25215;&#35834;&#22312;LLM&#35757;&#32451;&#36807;&#31243;&#20013;&#20943;&#36731;&#35760;&#24518;&#38169;&#35823;&#25110;&#36807;&#26102;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#21487;&#33021;&#20250;&#24341;&#20837;&#22823;&#37327;&#26410;&#34987;&#29616;&#26377;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#26816;&#27979;&#21040;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;CounterFact&#22522;&#20934;&#27979;&#35797;&#20197;&#21253;&#25324;&#21160;&#24577;&#32452;&#20214;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#31216;&#20026;CounterFact+&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#26412;&#36136;&#25351;&#26631;&#25193;&#23637;&#20102;&#29992;&#20110;&#34913;&#37327;&#29305;&#24322;&#24615;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25913;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#29305;&#24322;&#24615;&#36739;&#20302;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20984;&#26174;&#20102;&#38656;&#35201;&#25913;&#36827;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#20197;&#35782;&#21035;&#21644;&#39044;&#38450;&#19981;&#33391;&#21103;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing CounterFact benchmark to include a dynamic component and dub our benchmark CounterFact+. Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric. We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity. Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.17537</link><description>&lt;p&gt;
&#20351;&#29992;&#22330;&#26223;&#22270;&#35760;&#24518;&#24314;&#27169;&#21160;&#24577;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#22914;&#23621;&#23460;&#31561;&#65292;&#23547;&#25214;&#29289;&#21697;&#30340;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#38656;&#35201;&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#39044;&#27979;&#29289;&#21697;&#20301;&#32622;&#26469;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#38142;&#36335;&#39044;&#27979;&#38382;&#39064;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#22270;&#34920;&#36798;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;&#25151;&#38388;&#21644;&#29289;&#21697;&#26159;&#33410;&#28857;&#65292;&#22312;&#36793;&#32536;&#20013;&#32534;&#30721;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#65292;&#20195;&#29702;&#20154;&#20165;&#30693;&#36947;&#26356;&#25913;&#22270;&#30340;&#37096;&#20998;&#12290;&#36825;&#31181;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#23545;&#20110;&#29616;&#26377;&#30340;&#38142;&#36335;&#39044;&#27979;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#34920;&#31034; - &#22330;&#26223;&#22270;&#35760;&#24518;&#65288;SGM&#65289; - &#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#20154;&#30340;&#32047;&#31215;&#35266;&#23519;&#38598;&#21512;&#65292;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#20174;SGM&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#12290;&#25105;&#20204;&#22312;&#21160;&#24577;&#25151;&#23627;&#27169;&#25311;&#22120;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23427;&#25353;&#29031;&#35821;&#20041;&#27169;&#24335;&#21019;&#24314;&#19981;&#21516;&#30340;&#21160;&#24577;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35821;&#20041;&#38382;&#31572;&#37325;&#26500;&#27169;&#22411; (SURF)&#65292;&#36890;&#36807;&#19977;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#25805;&#20316;&#26469;&#37325;&#20889;&#21475;&#35821;&#38382;&#31572;&#20013;&#23384;&#22312;&#30340;&#35789;&#27719;&#12289;&#21629;&#39064;&#12289;&#21477;&#27861;&#21644;&#29305;&#24322;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#29575;&#65292;&#33021;&#22815;&#24110;&#21161;&#35821;&#38899;&#21161;&#25163;&#26356;&#22909;&#22320;&#22238;&#31572;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17393</link><description>&lt;p&gt;
&#22312;&#21475;&#35821;&#38382;&#31572;&#20013;&#36890;&#36807;&#35821;&#20041;&#37325;&#26500;&#26469;&#22238;&#31572;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Answering Unanswered Questions through Semantic Reformulations in Spoken QA. (arXiv:2305.17393v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35821;&#20041;&#38382;&#31572;&#37325;&#26500;&#27169;&#22411; (SURF)&#65292;&#36890;&#36807;&#19977;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#25805;&#20316;&#26469;&#37325;&#20889;&#21475;&#35821;&#38382;&#31572;&#20013;&#23384;&#22312;&#30340;&#35789;&#27719;&#12289;&#21629;&#39064;&#12289;&#21477;&#27861;&#21644;&#29305;&#24322;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#29575;&#65292;&#33021;&#22815;&#24110;&#21161;&#35821;&#38899;&#21161;&#25163;&#26356;&#22909;&#22320;&#22238;&#31572;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#38382;&#31572;&#26159;&#35821;&#38899;&#21161;&#25163;&#30340;&#19968;&#20010;&#37325;&#35201;&#21151;&#33021;&#65292;&#36890;&#24120;&#30001;&#22810;&#20010;&#38382;&#31572;&#31995;&#32479;&#25903;&#25345;&#12290;&#29992;&#25143;&#36890;&#36807;&#33258;&#28982;&#35821;&#38899;&#35810;&#38382;&#38382;&#39064;&#65292;&#21487;&#33021;&#21253;&#21547;&#19981;&#27969;&#30021;&#12289;&#38169;&#35823;&#21644;&#38750;&#27491;&#24335;&#30340;&#35821;&#27861;&#25110;&#25514;&#36766;&#12290;&#36825;&#26159;&#38382;&#31572;&#31995;&#32479;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#23548;&#33268;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;&#25110;&#26080;&#20851;&#30340;&#31572;&#26696;&#65292;&#24182;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#24046;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26410;&#25104;&#21151;&#22238;&#31572;&#30340;&#38382;&#31572;&#35831;&#27714;&#65292;&#20197;&#30830;&#23450;&#26680;&#24515;&#25361;&#25112;&#65306;&#35789;&#27719;&#24046;&#36317;&#12289;&#21629;&#39064;&#31867;&#22411;&#12289;&#22797;&#26434;&#30340;&#21477;&#27861;&#32467;&#26500;&#21644;&#39640;&#29305;&#24322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#38382;&#31572;&#37325;&#26500;&#27169;&#22411; (SURF)&#65292;&#25552;&#20379;&#19977;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#25805;&#20316; (&#20462;&#22797;&#12289;&#21477;&#27861;&#37325;&#22609;&#12289;&#27010;&#25324;) &#26469;&#37325;&#20889;&#38382;&#39064;&#20197;&#20415;&#20110;&#22238;&#31572;&#12290;&#31163;&#32447;&#35780;&#20272;&#26469;&#33258;&#39046;&#20808;&#35821;&#38899;&#21161;&#25163;&#30340; 100 &#19975;&#20010;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#32467;&#26524;&#26174;&#31034; SURF &#26174;&#33879;&#25552;&#39640;&#20102;&#22238;&#31572;&#29575;&#65306;&#39640;&#36798; 24% &#30340;&#38382;&#39064;&#33719;&#24471;&#20102;&#30456;&#20851;&#31572;&#26696; (75%)&#12290;&#23454;&#26102;&#37096;&#32626;&#26174;&#31034;&#20986;&#23545;&#25968;&#30334;&#19975;&#26377;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;&#30340;&#23458;&#25143;&#20135;&#29983;&#20102;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech which can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant answers, and leading to bad user experiences. We analyze failed QA requests to identify core challenges: lexical gaps, proposition types, complex syntactic structure, and high specificity. We propose a Semantic Question Reformulation (SURF) model offering three linguistically-grounded operations (repair, syntactic reshaping, generalization) to rewrite questions to facilitate answering. Offline evaluation on 1M unanswered questions from a leading voice assistant shows that SURF significantly improves answer rates: up to 24% of previously unanswered questions obtain relevant answers (75%). Live deployment shows positive impact for millions of customers with unanswered questions; explicit relev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21547;&#26377;&#19981;&#30830;&#23450;&#24615;&#38556;&#30861;&#29289;&#30340;&#38750;&#20984;&#29615;&#22659;&#20013;&#30340;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36890;&#36807;&#39118;&#38505;&#36718;&#24275;&#30340;&#27010;&#24565;&#23558;&#39118;&#38505;&#26377;&#30028;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17291</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#38750;&#20984;&#29615;&#22659;&#20013;&#30340;&#20984;&#39118;&#38505;&#26377;&#30028;&#36830;&#32493;&#26102;&#38388;&#36712;&#36857;&#35268;&#21010;&#21644;&#31649;&#36947;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Convex Risk Bounded Continuous-Time Trajectory Planning and Tube Design in Uncertain Nonconvex Environments. (arXiv:2305.17291v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21547;&#26377;&#19981;&#30830;&#23450;&#24615;&#38556;&#30861;&#29289;&#30340;&#38750;&#20984;&#29615;&#22659;&#20013;&#30340;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36890;&#36807;&#39118;&#38505;&#36718;&#24275;&#30340;&#27010;&#24565;&#23558;&#39118;&#38505;&#26377;&#30028;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21547;&#26377;&#20855;&#26377;&#27010;&#29575;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#20960;&#20309;&#24418;&#29366;&#30340;&#38556;&#30861;&#29289;&#30340;&#19981;&#30830;&#23450;&#38750;&#20984;&#38745;&#24577;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#24102;&#26377;&#26377;&#30028;&#39118;&#38505;&#30340;&#36712;&#36857;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23547;&#25214;&#35268;&#21010;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#35777;&#26377;&#30028;&#39118;&#38505;&#30340;&#36830;&#32493;&#26102;&#38388;&#36712;&#36857;&#12290;&#39118;&#38505;&#34987;&#23450;&#20041;&#20026;&#19982;&#19981;&#30830;&#23450;&#38556;&#30861;&#29289;&#30896;&#25758;&#30340;&#27010;&#29575;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#39118;&#38505;&#26377;&#30028;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#35201;&#20040;&#20165;&#38480;&#20110;&#39640;&#26031;&#19981;&#30830;&#23450;&#24615;&#21644;&#20984;&#38556;&#30861;&#29289;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#38656;&#35201;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#21644;&#26102;&#38388;&#31163;&#25955;&#21270;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#39118;&#38505;&#26377;&#30028;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#39118;&#38505;&#36718;&#24275;&#30340;&#27010;&#24565;&#23558;&#39118;&#38505;&#26377;&#30028;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#39118;&#38505;&#36718;&#24275;&#26159;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#25152;&#26377;&#20855;&#26377;&#20445;&#35777;&#26377;&#30028;&#39118;&#38505;&#30340;&#28857;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the trajectory planning problem in uncertain nonconvex static and dynamic environments that contain obstacles with probabilistic location, size, and geometry. To address this problem, we provide a risk bounded trajectory planning method that looks for continuous-time trajectories with guaranteed bounded risk over the planning time horizon. Risk is defined as the probability of collision with uncertain obstacles. Existing approaches to address risk bounded trajectory planning problems either are limited to Gaussian uncertainties and convex obstacles or rely on sampling-based methods that need uncertainty samples and time discretization. To address the risk bounded trajectory planning problem, we leverage the notion of risk contours to transform the risk bounded planning problem into a deterministic optimization problem. Risk contours are the set of all points in the uncertain environment with guaranteed bounded risk. The obtained deterministic optimization is, 
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;NMT&#20013;&#30340;&#22797;&#21046;&#38382;&#39064;&#36890;&#24120;&#21457;&#29983;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#19988;&#20250;&#30452;&#25509;&#22797;&#21046;&#36755;&#20837;&#21477;&#23376;&#30340;&#37096;&#20998;&#20316;&#20026;&#32763;&#35793;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;&#26469;&#32531;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#31181;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17182</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#22797;&#21046;&#38382;&#39064;&#65306;&#20855;&#26377;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language Discriminator Loss. (arXiv:2305.17182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17182
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;NMT&#20013;&#30340;&#22797;&#21046;&#38382;&#39064;&#36890;&#24120;&#21457;&#29983;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#19988;&#20250;&#30452;&#25509;&#22797;&#21046;&#36755;&#20837;&#21477;&#23376;&#30340;&#37096;&#20998;&#20316;&#20026;&#32763;&#35793;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;&#26469;&#32531;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#31181;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24050;&#22312;&#35768;&#22810;&#35821;&#31181;&#38388;&#24471;&#21040;&#25104;&#21151;&#65292;&#20294;&#22797;&#21046;&#38382;&#39064;&#65288;&#21363;&#23558;&#36755;&#20837;&#21477;&#23376;&#30340;&#26576;&#20123;&#37096;&#20998;&#30452;&#25509;&#22797;&#21046;&#20316;&#20026;&#32763;&#35793;&#65289;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#24456;&#24120;&#35265;&#65292;&#23588;&#20854;&#28041;&#21450;&#20302;&#36164;&#28304;&#35821;&#31181;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#38382;&#39064;&#19982;&#22312;&#32447;&#22238;&#35793;&#65288;BT&#65289;&#26399;&#38388;&#20986;&#29616;&#30340;&#39044;&#26399;&#22797;&#21046;&#34892;&#20026;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#23427;&#21253;&#21547;&#20102;&#19968;&#20010;&#35821;&#35328;&#37492;&#21035;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#35813;&#25439;&#22833;&#26045;&#21152;&#32422;&#26463;&#20110;&#20013;&#38388;&#32763;&#35793;&#65292;&#20197;&#20351;&#32763;&#35793;&#26159;&#25152;&#38656;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#35821;&#35328;&#23545;&#12289; &#21253;&#25324;&#30456;&#20284;&#21644;&#36828;&#36317;&#31163;&#12289;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#22797;&#21046;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although unsupervised neural machine translation (UNMT) has achieved success in many language pairs, the copying problem, i.e., directly copying some parts of the input sentence as the translation, is common among distant language pairs, especially when low-resource languages are involved. We find this issue is closely related to an unexpected copying behavior during online back-translation (BT). In this work, we propose a simple but effective training schedule that incorporates a language discriminator loss. The loss imposes constraints on the intermediate translation so that the translation is in the desired language. By conducting extensive experiments on different language pairs, including similar and distant, high and low-resource languages, we find that our method alleviates the copying problem, thus improving the translation performance on low-resource languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#20219;&#26694;&#26550;&#30340;&#40065;&#26834;&#25511;&#21046;&#21644;&#21327;&#35843;&#26041;&#26696;&#65292;&#20174;&#24694;&#24847;&#20195;&#29702;&#30340;&#35282;&#24230;&#35782;&#21035;&#20851;&#38190;&#23545;&#25239;&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#30896;&#25758;&#21644;&#20132;&#36890;&#22581;&#22622;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#20219;&#26694;&#26550;&#30340;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2305.16818</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#20219;&#24863;&#30693;&#30340;&#36830;&#25509;&#21644;&#33258;&#20027;&#36710;&#36742;&#30340;&#40065;&#26834;&#25511;&#21046;&#19982;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Trust-Aware Resilient Control and Coordination of Connected and Automated Vehicles. (arXiv:2305.16818v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#20219;&#26694;&#26550;&#30340;&#40065;&#26834;&#25511;&#21046;&#21644;&#21327;&#35843;&#26041;&#26696;&#65292;&#20174;&#24694;&#24847;&#20195;&#29702;&#30340;&#35282;&#24230;&#35782;&#21035;&#20851;&#38190;&#23545;&#25239;&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#30896;&#25758;&#21644;&#20132;&#36890;&#22581;&#22622;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#20219;&#26694;&#26550;&#30340;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#20110;&#32593;&#32476;&#36830;&#25509;&#21644;&#33258;&#20027;&#36710;&#36742;&#65288;CAV&#65289;&#31561;&#29289;&#29702;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#36710;&#36742;&#36890;&#36807;&#21327;&#20316;&#23433;&#20840;&#22320;&#36890;&#36807;&#36947;&#36335;&#32593;&#32476;&#12290;&#26412;&#25991;&#20174;&#19981;&#37197;&#21512;/&#24694;&#24847;&#20195;&#29702;&#20154;&#30340;&#35282;&#24230;&#35782;&#21035;&#20851;&#38190;&#23545;&#25239;&#24615;&#30446;&#26631;&#65288;&#22914;&#30896;&#25758;&#21644;&#20132;&#36890;&#22581;&#22622;&#65289;&#65292;&#21033;&#29992;&#20449;&#20219;&#26694;&#26550;&#25552;&#20986;&#20102;&#40065;&#26834;&#30340;&#25511;&#21046;&#19982;&#21327;&#35843;&#26041;&#26696;&#65292;&#20197;&#32531;&#35299;&#24694;&#24847;&#20195;&#29702;&#24102;&#26469;&#30340;&#24433;&#21709;&#24182;&#20445;&#35777;&#23433;&#20840;&#21327;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992; Sybil &#25915;&#20987;&#39564;&#35777;&#20102;&#24314;&#35758;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#20219;&#26694;&#26550;&#30340;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security is crucial for cyber-physical systems, such as a network of Connected and Automated Vehicles (CAVs) cooperating to navigate through a road network safely. In this paper, we tackle the security of a cooperating network of CAVs in conflict areas by identifying the critical adversarial objectives from the point of view of uncooperative/malicious agents from our preliminary study, which are (i) safety violations resulting in collisions, and (ii) traffic jams. We utilize a trust framework (and our work doesn't depend on the specific choice of trust/reputation framework) to propose a resilient control and coordination framework that mitigates the effects of such agents and guarantees safe coordination. A class of attacks that can be used to achieve the adversarial objectives is Sybil attacks, which we use to validate our proposed framework through simulation studies. Besides that, we propose an attack detection and mitigation scheme using the trust framework. The simulation results 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.16579</link><description>&lt;p&gt;
&#20154;&#20154;&#21487;&#22797;&#29616;&#30340;NLP&#30740;&#31350;&#65306;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16579
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#36817;&#24180;&#26469;&#24322;&#24120;&#28779;&#29190;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24613;&#20110;&#36827;&#20837;&#35813;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#22797;&#29616;&#21162;&#21147;&#26159;&#21542;&#36275;&#20197;&#35753;&#36825;&#20123;&#21021;&#23398;&#32773;&#24212;&#29992;&#26368;&#26032;&#30340;&#36827;&#23637;&#36824;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#20102;&#35299;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20171;&#32461;&#24615;&#30340;NLP&#35838;&#31243;&#20013;&#24320;&#23637;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35753;&#23398;&#29983;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#32534;&#31243;&#25216;&#33021;&#21644;&#23545;&#30740;&#31350;&#35770;&#25991;&#30340;&#29702;&#35299;&#23545;&#23436;&#25104;&#32451;&#20064;&#30340;&#20184;&#20986;&#20165;&#26377;&#38480;&#30340;&#24433;&#21709;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#30740;&#31350;&#20316;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#21162;&#21147;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#32534;&#30721;&#23454;&#36341;&#21644;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#12290;&#21069;&#36827;&#26102;&#65292;&#25105;&#20204;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#23494;&#20999;&#20851;&#27880;&#36825;&#20123;&#24320;&#28304;&#24037;&#20316;&#30340;&#31616;&#21333;&#26041;&#38754;&#65292;&#24182;&#20351;&#29992;&#21021;&#23398;&#32773;&#30340;&#21453;&#39304;&#35265;&#35299;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#24819;&#27861;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20182;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language processing (NLP) has recently seen an unprecedented level of excitement, and more people are eager to enter the field, it is unclear whether current research reproducibility efforts are sufficient for this group of beginners to apply the latest developments. To understand their needs, we conducted a study with 93 students in an introductory NLP course, where students reproduced the results of recent NLP papers. Surprisingly, we find that their programming skill and comprehension of research papers have a limited impact on their effort spent completing the exercise. Instead, we find accessibility efforts by research authors to be the key to success, including complete documentation, better coding practice, and easier access to data files. Going forward, we recommend that NLP researchers pay close attention to these simple aspects of open-sourcing their work, and use insights from beginners' feedback to provide actionable ideas on how to better support them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16044</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23558;&#22122;&#22768;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#26159;&#22823;&#33041;&#38750;&#20961;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24182;&#24050;&#25104;&#20026;&#31070;&#32463;&#24418;&#24577;&#26234;&#33021;&#30340;&#25903;&#26609;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#37319;&#29992;&#24102;&#26377;&#22122;&#22768;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#29702;&#35770;&#19978;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;NDL&#20026;&#20195;&#29702;&#26799;&#24230;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#23558;&#21508;&#31181;SNN&#26550;&#26500;&#21644;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#30830;&#23450;&#24615;SNNs&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#23398;&#20064;&#20013;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;(IPS)&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#21487;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#35777;&#26126;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;IPS&#12290;</title><link>http://arxiv.org/abs/2305.15877</link><description>&lt;p&gt;
&#25351;&#25968;&#24179;&#28369;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exponential Smoothing for Off-Policy Learning. (arXiv:2305.15877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#23398;&#20064;&#20013;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;(IPS)&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#21487;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#35777;&#26126;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;IPS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#26469;&#23547;&#25214;&#25913;&#36827;&#30340;&#31574;&#30053;&#65292;&#36890;&#24120;&#20351;&#29992;&#35760;&#24405;&#30340;&#36172;&#21338;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;IPS&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#12290;&#35813;&#30028;&#38480;&#26159;&#21487;&#22788;&#29702;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#35777;&#26126;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23398;&#20064;&#20219;&#21153;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#21033;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#30028;&#38480;&#36866;&#29992;&#20110;&#26631;&#20934;IPS&#65292;&#22240;&#27492;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#20851;&#20110;&#20309;&#26102;&#27491;&#21017;&#21270;IPS&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#21363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#12290;&#36825;&#19982;&#22312;&#23454;&#36341;&#20013;&#65292;&#21098;&#36753;IPS&#24120;&#24120;&#27604;OPL&#20013;&#30340;&#26631;&#20934;IPS&#34920;&#29616;&#26356;&#22909;&#30340;&#20449;&#24565;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning (OPL) aims at finding improved policies from logged bandit data, often by minimizing the inverse propensity scoring (IPS) estimator of the risk. In this work, we investigate a smooth regularization for IPS, for which we derive a two-sided PAC-Bayes generalization bound. The bound is tractable, scalable, interpretable and provides learning certificates. In particular, it is also valid for standard IPS without making the assumption that the importance weights are bounded. We demonstrate the relevance of our approach and its favorable performance through a set of learning tasks. Since our bound holds for standard IPS, we are able to provide insight into when regularizing IPS is useful. Namely, we identify cases where regularization might not be needed. This goes against the belief that, in practice, clipped IPS often enjoys favorable performance than standard IPS in OPL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25197;&#26354;&#27169;&#22411;&#21442;&#25968;&#30340;&#20445;&#25252;&#26426;&#21046;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;&#20013;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#25928;&#29992;&#30340;&#24179;&#34913;&#12290;&#31639;&#27861;&#21487;&#20197;&#22312;&#27599;&#20010;&#36890;&#20449;&#36718;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#25928;&#29992;-&#38544;&#31169;&#25240;&#34935;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#27425;&#32447;&#24615;&#24615;&#36136;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#21512;&#23398;&#20064;&#25928;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15148</link><description>&lt;p&gt;
&#29702;&#35770;&#25351;&#23548;&#30340;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#25928;&#29992;&#30340;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Theoretically Principled Federated Learning for Balancing Privacy and Utility. (arXiv:2305.15148v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25197;&#26354;&#27169;&#22411;&#21442;&#25968;&#30340;&#20445;&#25252;&#26426;&#21046;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;&#20013;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#25928;&#29992;&#30340;&#24179;&#34913;&#12290;&#31639;&#27861;&#21487;&#20197;&#22312;&#27599;&#20010;&#36890;&#20449;&#36718;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#25928;&#29992;-&#38544;&#31169;&#25240;&#34935;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#27425;&#32447;&#24615;&#24615;&#36136;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#21512;&#23398;&#20064;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#26426;&#21046;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25197;&#26354;&#27169;&#22411;&#21442;&#25968;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#23454;&#29616;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#24847;&#23558;&#25197;&#26354;&#26144;&#23556;&#21040;&#23454;&#20540;&#30340;&#38544;&#31169;&#27979;&#37327;&#12290;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#23427;&#21487;&#20197;&#20026;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#65292;&#22312;&#27599;&#20010;&#36890;&#20449;&#36718;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#25928;&#29992;-&#38544;&#31169;&#25240;&#34935;&#12290;&#36825;&#31181;&#33258;&#36866;&#24212;&#21644;&#32454;&#31890;&#24230;&#30340;&#20445;&#25252;&#21487;&#20197;&#25552;&#39640;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#21512;&#23398;&#20064;&#30340;&#25928;&#21147;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#20445;&#25252;&#36229;&#21442;&#25968;&#30340;&#25928;&#29992;&#25439;&#22833;&#19982;&#26368;&#20248;&#20445;&#25252;&#36229;&#21442;&#25968;&#30340;&#25928;&#29992;&#25439;&#22833;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#24635;&#36845;&#20195;&#27425;&#25968;&#30340;&#27425;&#32447;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#27425;&#32447;&#24615;&#30340;&#29305;&#28857;&#34920;&#26126;&#65292;&#24403;&#36845;&#20195;&#27425;&#25968;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#31639;&#27861;&#24615;&#33021;&#21644;&#26368;&#20248;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#22343;&#24046;&#36317;&#36235;&#36817;&#20110;&#38646;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a general learning framework for the protection mechanisms that protects privacy via distorting model parameters, which facilitates the trade-off between privacy and utility. The algorithm is applicable to arbitrary privacy measurements that maps from the distortion to a real value. It can achieve personalized utility-privacy trade-off for each model parameter, on each client, at each communication round in federated learning. Such adaptive and fine-grained protection can improve the effectiveness of privacy-preserved federated learning.  Theoretically, we show that gap between the utility loss of the protection hyperparameter output by our algorithm and that of the optimal protection hyperparameter is sub-linear in the total number of iterations. The sublinearity of our algorithm indicates that the average gap between the performance of our algorithm and that of the optimal performance goes to zero when the number of iterations goes to infinity. Further, we provide the conv
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14449</link><description>&lt;p&gt;
&#22270;&#35889;&#36935;&#35265;LLM&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#30340;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. (arXiv:2305.14449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14449
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;Alexa&#65292;Siri&#65292;Google Assistant&#31561;&#65289;&#38656;&#35201;&#29702;&#35299;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#20197;&#30830;&#20445;&#31283;&#20581;&#30340;&#20250;&#35805;&#29702;&#35299;&#24182;&#20943;&#23569;&#29992;&#25143;&#25705;&#25830;&#12290;&#36825;&#20123;&#26377;&#32570;&#38519;&#30340;&#26597;&#35810;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#30340;&#27495;&#20041;&#21644;&#38169;&#35823;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20013;&#30340;&#38169;&#35823;&#24341;&#36215;&#30340;&#12290;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#65288;&#20010;&#24615;&#21270;QR&#65289;&#26088;&#22312;&#20943;&#23569;&#36523;&#20307;&#21644;&#23614;&#37096;&#29992;&#25143;&#26597;&#35810;&#27969;&#37327;&#20013;&#30340;&#32570;&#38519;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#21435;&#25104;&#21151;&#30340;&#29992;&#25143;&#20132;&#20114;&#30340;&#32034;&#24341;&#12290;&#26412;&#25991;&#25552;&#20986;&#25105;&#20204;&#30340;&#8220;&#21327;&#21516;&#26597;&#35810;&#37325;&#20889;&#8221;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#37325;&#20889;&#29992;&#25143;&#21382;&#21490;&#20013;&#27809;&#26377;&#20986;&#29616;&#36807;&#30340;&#26032;&#22411;&#29992;&#25143;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#29992;&#25143;&#21453;&#39304;&#20132;&#20114;&#22270;&#8221;&#65288;FIG&#65289;&#65292;&#30001;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#32452;&#25104;&#65292;&#24182;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#26469;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65288;&#21363;&#21327;&#21516;&#29992;&#25143;&#32034;&#24341;&#65289;&#65292;&#20174;&#32780;&#24110;&#21161;&#35206;&#30422;&#26410;&#26469;&#26410;&#26366;&#35265;&#36807;&#30340;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#26032;&#30340;&#20016;&#23500;&#32034;&#24341;&#34987;&#22122;&#22768;&#21453;&#39304;&#20132;&#20114;&#25152;&#25903;&#37197;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26377;&#38480;&#20869;&#23384;BFGS&#65288;LLM&#65289;&#31639;&#27861;&#21644;&#22238;&#36864;&#26041;&#26696;&#26469;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;QR&#26041;&#27861;&#65292;&#24182;&#22312;&#26410;&#30475;&#21040;&#30340;&#29992;&#25143;&#20132;&#20114;&#19978;&#21462;&#24471;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our "Collaborative Query Rewriting" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a "user Feedback Interaction Graph" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract th
&lt;/p&gt;</description></item><item><title>XRoute&#29615;&#22659;&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#36335;&#30001;&#29615;&#22659;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#31471;&#21040;&#31471;&#36335;&#30001;&#26694;&#26550;&#20013;&#36873;&#25321;&#21644;&#36335;&#30001;&#32593;&#32476;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#25903;&#25345;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#22810;&#23454;&#20363;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.13823</link><description>&lt;p&gt;
XRoute&#29615;&#22659;&#65306;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#36335;&#30001;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
XRoute Environment: A Novel Reinforcement Learning Environment for Routing. (arXiv:2305.13823v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13823
&lt;/p&gt;
&lt;p&gt;
XRoute&#29615;&#22659;&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#36335;&#30001;&#29615;&#22659;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#31471;&#21040;&#31471;&#36335;&#30001;&#26694;&#26550;&#20013;&#36873;&#25321;&#21644;&#36335;&#30001;&#32593;&#32476;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#25903;&#25345;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#22810;&#23454;&#20363;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#30001;&#26159;&#29616;&#20195;&#35774;&#35745;&#33258;&#21160;&#21270;&#27969;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#32791;&#26102;&#30340;&#38454;&#27573;&#65292;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#24040;&#22823;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#25913;&#21892;&#36335;&#30001;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30740;&#31350;&#20013;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#30340;&#36335;&#30001;&#38382;&#39064;&#35268;&#27169;&#22826;&#23567;&#65292;&#26080;&#27861;&#22312;&#21830;&#19994;EDA&#24037;&#20855;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;XRoute&#29615;&#22659;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#20854;&#20013;&#20195;&#29702;&#34987;&#35757;&#32451;&#22312;&#20808;&#36827;&#30340;&#31471;&#21040;&#31471;&#36335;&#30001;&#26694;&#26550;&#20013;&#36873;&#25321;&#21644;&#36335;&#30001;&#32593;&#32476;&#12290;&#36825;&#20010;&#29615;&#22659;&#21487;&#20197;&#24555;&#36895;&#23433;&#20840;&#19988;&#21487;&#37325;&#22797;&#22320;&#27979;&#35797;&#26032;&#31639;&#27861;&#21644;&#24819;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#26131;&#20110;&#20351;&#29992;&#65292;&#23450;&#21046;&#21644;&#28155;&#21152;&#20854;&#20182;&#22330;&#26223;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#23485;&#26494;&#30340;&#24320;&#28304;&#35768;&#21487;&#19979;&#25552;&#20379;&#25903;&#25345;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#22810;&#23454;&#20363;&#23454;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#23398;&#20064;&#20219;&#21153;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#33455;&#29255;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Routing is a crucial and time-consuming stage in modern design automation flow for advanced technology nodes. Great progress in the field of reinforcement learning makes it possible to use those approaches to improve the routing quality and efficiency. However, the scale of the routing problems solved by reinforcement learning-based methods in recent studies is too small for these methods to be used in commercial EDA tools. We introduce the XRoute Environment, a new reinforcement learning environment where agents are trained to select and route nets in an advanced, end-to-end routing framework. Novel algorithms and ideas can be quickly tested in a safe and reproducible manner in it. The resulting environment is challenging, easy to use, customize and add additional scenarios, and it is available under a permissive open-source license. In addition, it provides support for distributed deployment and multi-instance experiments. We propose two tasks for learning and build a full-chip test 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#12290;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12711</link><description>&lt;p&gt;
&#36890;&#36807;&#37051;&#23621;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21327;&#21516;&#23398;&#20064;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement. (arXiv:2305.12711v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#12290;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;(USL-VI-ReID)&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#36328;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27169;&#24577;&#19981;&#21464;&#29305;&#24449;&#65292;&#36825;&#22312;&#35270;&#39057;&#30417;&#25511;&#31995;&#32479;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#35299;&#20915;&#36328;&#27169;&#24577;&#25968;&#25454;&#20851;&#32852;&#38382;&#39064;&#23545;&#20110;&#36827;&#19968;&#27493;&#36827;&#34892;&#24322;&#36136;&#32852;&#21512;&#23398;&#20064;&#38750;&#24120;&#20851;&#38190;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;DOTLA&#26426;&#21046;formulate&#20102;&#19968;&#31181;&#30456;&#20114;&#22686;&#24378;&#21644;&#39640;&#25928;&#30340;&#36328;&#27169;&#24577;&#25968;&#25454;&#20851;&#32852;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#19968;&#20123;&#19981;&#36275;&#21644;&#22122;&#22768;&#26631;&#31614;&#20851;&#32852;&#30340;&#21103;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#28040;&#38500;&#30001;&#19981;&#20934;&#30830;&#30340;&#30417;&#30563;&#20449;&#21495;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;USL-VI-ReID&#27169;&#22411;&#19982;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#29978;&#33267;&#19968;&#20123;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) aims at learning modality-invariant features from unlabeled cross-modality dataset, which is crucial for practical applications in video surveillance systems. The key to essentially address the USL-VI-ReID task is to solve the cross-modality data association problem for further heterogeneous joint learning. To address this issue, we propose a Dual Optimal Transport Label Assignment (DOTLA) framework to simultaneously assign the generated labels from one modality to its counterpart modality. The proposed DOTLA mechanism formulates a mutual reinforcement and efficient solution to cross-modality data association, which could effectively reduce the side-effects of some insufficient and noisy label associations. Besides, we further propose a cross-modality neighbor consistency guided label refinement and regularization module, to eliminate the negative effects brought by the inaccurate supervised signals, under th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.12673</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21452;&#36793;&#36328;&#27169;&#24577;&#32858;&#31867;&#21305;&#37197;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;&#20809;-&#32418;&#22806;&#20154;&#29289;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised Visible-Infrared Person ReID. (arXiv:2305.12673v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12673
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21487;&#35265;&#20809;-&#32418;&#22806;&#20154;&#29289;&#35782;&#21035;&#65288;USL-VI-ReID&#65289;&#26088;&#22312;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#21305;&#37197;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#34892;&#20154;&#22270;&#20687;&#20013;&#30456;&#21516;&#36523;&#20221;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#38024;&#23545;&#27809;&#26377;&#24456;&#22909;&#25506;&#32034;&#36328;&#27169;&#24577;&#32858;&#31867;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20108;&#20998;&#22270;&#20013;&#20248;&#21270;&#26368;&#22823;&#21305;&#37197;&#38382;&#39064;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#23545;&#22810;&#21452;&#36793;&#36328;&#27169;&#24577;&#32858;&#31867;&#21305;&#37197;&#65288;MBCCM&#65289;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#21305;&#37197;&#30340;&#25104;&#23545;&#32858;&#31867;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#21033;&#29992;&#20849;&#20139;&#30340;&#21487;&#35265;&#20809;&#21644;&#32418;&#22806;&#20266;&#26631;&#31614;&#12290;&#22312;&#36825;&#26679;&#30340;&#30417;&#30563;&#20449;&#21495;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#65288;MSMA&#65289;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22312;&#32858;&#31867;&#32423;&#21035;&#19978;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36328;&#27169;&#24577;&#30340;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#29305;&#24449;&#20063;&#34987;&#32771;&#34385;&#36827;&#21435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to match pedestrian images of the same identity from different modalities without annotations. Existing works mainly focus on alleviating the modality gap by aligning instance-level features of the unlabeled samples. However, the relationships between cross-modality clusters are not well explored. To this end, we propose a novel bilateral cluster matching-based learning framework to reduce the modality gap by matching cross-modality clusters. Specifically, we design a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) algorithm through optimizing the maximum matching problem in a bipartite graph. Then, the matched pairwise clusters utilize shared visible and infrared pseudo-labels during the model training. Under such a supervisory signal, a Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework is proposed to align features jointly at a cluster-level. Meanwhile, the cross-modal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10847</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#24341;&#23548;&#26469;&#35268;&#36991;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#25324;&#35770;&#25991;&#20889;&#20316;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#28508;&#22312;&#30340;&#35823;&#29992;&#38382;&#39064;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#25220;&#34989;&#21644;&#22403;&#22334;&#20449;&#24687;&#31561;&#19981;&#33391;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#25442;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20248;&#21270;&#26041;&#27861;&#65288;SICO&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#25552;&#31034;&#35821;&#12290;&#22312;&#19977;&#20010;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#34987;&#35823;&#29992;&#65292;&#22312;SICO&#30340;&#24110;&#21161;&#19979;&#65292;ChatGPT&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;&#20845;&#39033;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#24179;&#22343;&#23548;&#33268;0.54&#30340;AUC&#19979;&#38477;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#36824;&#35201;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#22362;&#23450;&#22320;&#25581;&#31034;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.10638</link><description>&lt;p&gt;
&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#36827;&#34892;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65288;RCA&#65289;&#30340;&#20219;&#21153;&#26159;&#20998;&#26512;&#31995;&#32479;&#30417;&#25511;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#31995;&#32479;&#25925;&#38556;/&#22833;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26377;&#25928;&#30340;RCA&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#31995;&#32479;&#25925;&#38556;&#24674;&#22797;&#65292;&#24182;&#20943;&#36731;&#31995;&#32479;&#25439;&#22833;&#25110;&#36130;&#21153;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#24320;&#21457;&#31163;&#32447;RCA&#31639;&#27861;&#19978;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#21551;&#21160;RCA&#36807;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#25968;&#25454;&#26469;&#35757;&#32451;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#26032;&#30340;&#31995;&#32479;&#25925;&#38556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;RCA&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;RCA&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;RCA&#27169;&#22411;&#12290;CORAL&#21253;&#25324;&#35302;&#21457;&#28857;&#26816;&#27979;&#12289;&#22686;&#37327;&#35299;&#32544;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;&#35302;&#21457;&#28857;&#26816;&#27979;&#32452;&#20214;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#31995;&#32479;&#29366;&#24577;&#36716;&#25442;&#24182;&#36827;&#34892;&#20934;&#23454;&#26102;&#26816;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;m&#30340;&#22312;&#32447;&#35302;&#21457;&#28857;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.  In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#20250;&#23545;AI&#24615;&#33021;&#34920;&#29616;&#19981;&#21033;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.10201</link><description>&lt;p&gt;
&#20154;&#20204;&#20132;&#35848;&#65292;AI&#20542;&#21548;&#65306;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;AI&#21028;&#26029;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
People Talking and AI Listening: How Stigmatizing Language in EHR Notes Affect AI Performance. (arXiv:2305.10201v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#20250;&#23545;AI&#24615;&#33021;&#34920;&#29616;&#19981;&#21033;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;(EHRs)&#26159;&#26399;&#26395;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;(AI)-&#39537;&#21160;&#30340;&#21307;&#30103;&#36716;&#22411;&#30340;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#21453;&#26144;&#22312;EHR&#31508;&#35760;&#20013;&#30340;&#20020;&#24202;&#21307;&#24072;&#20559;&#35265;&#21487;&#33021;&#20250;&#23548;&#33268;AI&#27169;&#22411;&#32487;&#25215;&#24182;&#25918;&#22823;&#36825;&#20123;&#20559;&#35265;&#65292;&#20174;&#32780;&#19981;&#26029;&#21152;&#21095;&#20581;&#24247;&#19978;&#30340;&#19981;&#24179;&#31561;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;EHR&#31508;&#35760;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;(SL)&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#19981;&#21033;&#20110;AI&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#31361;&#20986;&#20102;SL&#20316;&#20026;AI&#27169;&#22411;&#21457;&#23637;&#20013;&#31181;&#26063;&#24046;&#24322;&#30340;&#19968;&#31181;&#26469;&#28304;&#12290;&#20026;&#25506;&#32034;&#19968;&#31181;&#25805;&#20316;&#19978;&#26377;&#25928;&#30340;&#32531;&#35299;SL&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20020;&#24202;&#21307;&#29983;&#21327;&#20316;&#32593;&#32476;&#20013;SL&#29983;&#25104;&#30340;&#27169;&#24335;&#65292;&#21457;&#29616;&#20013;&#22830;&#21307;&#29983;&#23545;AI&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#24046;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21024;&#38500;&#20013;&#22830;&#20020;&#24202;&#21307;&#29983;&#25776;&#20889;&#30340;SL&#26159;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#20020;&#24202;&#21307;&#29983;&#32780;&#35328;&#65292;&#32531;&#35299;SL&#23545;AI&#24615;&#33021;&#24433;&#21709;&#30340;&#26356;&#20026;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;&#21453;&#26144;&#22312;EHR&#31508;&#35760;&#20013;&#30340;&#20020;&#24202;&#21307;&#24072;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) serve as an essential data source for the envisioned artificial intelligence (AI)-driven transformation in healthcare. However, clinician biases reflected in EHR notes can lead to AI models inheriting and amplifying these biases, perpetuating health disparities. This study investigates the impact of stigmatizing language (SL) in EHR notes on mortality prediction using a Transformer-based deep learning model and explainable AI (XAI) techniques. Our findings demonstrate that SL written by clinicians adversely affects AI performance, particularly so for black patients, highlighting SL as a source of racial disparity in AI model development. To explore an operationally efficient way to mitigate SL's impact, we investigate patterns in the generation of SL through a clinicians' collaborative network, identifying central clinicians as having a stronger impact on racial disparity in the AI model. We find that removing SL written by central clinicians is a more 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;OffCEM&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#23545;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12290;&#35813;&#20272;&#35745;&#22120;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#20445;&#25345;&#26080;&#20559;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08062</link><description>&lt;p&gt;
&#22522;&#20110;&#36830;&#35789;&#25928;&#24212;&#24314;&#27169;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling. (arXiv:2305.08062v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;OffCEM&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#23545;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12290;&#35813;&#20272;&#35745;&#22120;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#20445;&#25345;&#26080;&#20559;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23545;&#20110;&#20256;&#32479;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#26041;&#24040;&#30340;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#26041;&#24040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;OffCEM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36830;&#35789;&#25928;&#24212;&#27169;&#22411;&#65288;CEM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25928;&#24212;&#20998;&#20026;&#32676;&#38598;&#25928;&#24212;&#21644;&#27531;&#24046;&#25928;&#24212;&#12290;OffCEM&#20165;&#23545;&#34892;&#21160;&#32676;&#38598;&#24212;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#65292;&#35813;&#20272;&#35745;&#22120;&#26159;&#26080;&#20559;&#30340;&#65292;&#35813;&#26465;&#20214;&#20165;&#35201;&#27714;&#27531;&#24046;&#25928;&#24212;&#27169;&#22411;&#20445;&#30041;&#27599;&#20010;&#32676;&#38598;&#20013;&#34892;&#21160;&#30340;&#30456;&#23545;&#26399;&#26395;&#22870;&#21169;&#24046;&#24322;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;CEM&#21644;&#26412;&#22320;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#36807;&#31243;&#65292;&#29992;&#20110;&#25191;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#20272;&#35745;&#65292;&#31532;&#19968;&#27493;&#26368;&#23567;&#21270;&#20559;&#24046;&#65292;&#31532;&#20108;&#27493;&#26368;&#23567;&#21270;&#26041;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#24471;&#21040;&#30340;OPE&#20272;&#35745;&#22120;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#37117;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study off-policy evaluation (OPE) of contextual bandit policies for large discrete action spaces where conventional importance-weighting approaches suffer from excessive variance. To circumvent this variance issue, we propose a new estimator, called OffCEM, that is based on the conjunct effect model (CEM), a novel decomposition of the causal effect into a cluster effect and a residual effect. OffCEM applies importance weighting only to action clusters and addresses the residual causal effect through model-based reward estimation. We show that the proposed estimator is unbiased under a new condition, called local correctness, which only requires that the residual-effect model preserves the relative expected reward differences of the actions within each cluster. To best leverage the CEM and local correctness, we also propose a new two-step procedure for performing model-based estimation that minimizes bias in the first step and variance in the second step. We find that the resulting O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ACTC&#30340;&#26041;&#27861;&#65292;&#22312;&#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26102;&#36827;&#34892;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#20803;&#32452;&#26469;&#25214;&#21040;&#27599;&#20010;&#20851;&#31995;&#30340;&#26368;&#20339;&#38408;&#20540;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#26410;&#26631;&#35760;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.06395</link><description>&lt;p&gt;
ACTC: &#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion. (arXiv:2305.06395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ACTC&#30340;&#26041;&#27861;&#65292;&#22312;&#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26102;&#36827;&#34892;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#20803;&#32452;&#26469;&#25214;&#21040;&#27599;&#20010;&#20851;&#31995;&#30340;&#26368;&#20339;&#38408;&#20540;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#26410;&#26631;&#35760;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;(KGC)&#20381;&#36182;&#20110;&#20272;&#35745;&#24471;&#20998;&#27169;&#22411;(&#23454;&#20307;&#65292;&#20851;&#31995;&#65292;&#23454;&#20307;)-&#20803;&#32452;&#65292;&#20363;&#22914;&#65292;&#36890;&#36807;&#23884;&#20837;&#21021;&#22987;&#30693;&#35782;&#22270;&#12290;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#38408;&#20540;(&#20351;&#29992;&#25163;&#21160;&#27880;&#37322;&#30340;&#31034;&#20363;)&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#27979;&#36136;&#37327;&#12290;&#26412;&#25991;&#23581;&#35797;&#39318;&#27425;&#38024;&#23545;KGC&#36827;&#34892;&#20919;&#21551;&#21160;&#26657;&#20934;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#21021;&#22987;&#27809;&#26377;&#27880;&#37322;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#21482;&#33021;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#20803;&#32452;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;ACTC&#22522;&#20110;&#26377;&#38480;&#30340;&#27880;&#37322;&#20803;&#32452;&#26377;&#25928;&#22320;&#25214;&#21040;&#22909;&#30340;&#27599;&#20010;&#20851;&#31995;&#30340;&#38408;&#20540;&#12290;&#38500;&#20102;&#19968;&#20123;&#27880;&#37322;&#30340;&#20803;&#32452;&#22806;&#65292;ACTC&#36824;&#21033;&#29992;Logistic&#22238;&#24402;&#25110;&#39640;&#26031;&#36807;&#31243;&#20998;&#31867;&#22120;&#20272;&#35745;&#30340;&#26410;&#26631;&#35760;&#20803;&#32452;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23494;&#24230;&#21644;&#38543;&#26426;&#36873;&#25321;&#31561;&#19981;&#21516;&#26041;&#27861;&#36873;&#25321;&#20505;&#36873;&#20803;&#32452;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#35780;&#20998;&#27169;&#22411;&#21644;&#19968;&#20010;oracle&#27880;&#37322;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Bug&#23450;&#20301;&#26041;&#27861;RLocator&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;Bug&#23450;&#20301;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05586</link><description>&lt;p&gt;
RLocator: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;Bug&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
RLocator: Reinforcement Learning for Bug Localization. (arXiv:2305.05586v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Bug&#23450;&#20301;&#26041;&#27861;RLocator&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;Bug&#23450;&#20301;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#32773;&#22312;&#20182;&#20204;&#30340;&#39033;&#30446;&#20013;&#33457;&#36153;&#20102;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#20462;&#22797;Bugs&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;Bug&#23450;&#20301;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20123;&#28304;&#20195;&#30721;&#25991;&#20214;&#21487;&#33021;&#26159;&#36127;&#36131;&#29305;&#23450;Bug&#30340;&#28304;&#22836;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;Bug&#23450;&#20301;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#30452;&#25509;&#20248;&#21270;&#35780;&#20272;&#25351;&#26631;&#12290;&#30456;&#21453;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20250;&#23545;&#26816;&#32034;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Bug&#23450;&#20301;&#26041;&#27861;RLocator&#12290;&#25105;&#20204;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#20248;&#21270;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23545;Bug&#23450;&#20301;&#38382;&#39064;&#36827;&#34892;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35813;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#20845;&#31181;&#39640;&#24230;&#27969;&#34892;&#30340;Apache&#39033;&#30446;&#30340;8,316&#20010;Bug&#25253;&#21578;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RLocator&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;Bug&#23450;&#20301;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software developers spend a significant portion of time fixing bugs in their projects. To streamline this process, bug localization approaches have been proposed to identify the source code files that are likely responsible for a particular bug. Prior work proposed several similarity-based machine-learning techniques for bug localization. Despite significant advances in these techniques, they do not directly optimize the evaluation measures. Instead, they use different metrics in the training and testing phases, which can negatively impact the model performance in retrieval tasks. In this paper, we propose RLocator, a Reinforcement Learning-based (RL) bug localization approach. We formulate the bug localization problem using a Markov Decision Process (MDP) to optimize the evaluation measures directly. We present the technique and experimentally evaluate it based on a benchmark dataset of 8,316 bug reports from six highly popular Apache projects. Our evaluation shows that RLocator achie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23545;&#26102;&#23578;&#26381;&#35013;&#30340;&#22270;&#20687;&#23646;&#24615;&#20998;&#31867;&#30340;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04379</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#23578;&#26816;&#27979;&#30340;&#19981;&#24179;&#34913;&#26631;&#31614;&#26679;&#26412;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Data Efficient Training with Imbalanced Label Sample Distribution for Fashion Detection. (arXiv:2305.04379v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23545;&#26102;&#23578;&#26381;&#35013;&#30340;&#22270;&#20687;&#23646;&#24615;&#20998;&#31867;&#30340;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#20110;&#35270;&#35273;&#30340;&#26631;&#31614;&#39044;&#27979;&#20197;&#21450;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#31867;&#12290;&#23454;&#29616;&#36825;&#20123;&#20219;&#21153;&#30340;&#19968;&#20010;&#20027;&#35201;&#38590;&#28857;&#26159;&#25968;&#25454;&#20998;&#24067;&#30340;&#26174;&#33879;&#19981;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#39640;&#25928;&#27169;&#22411;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#26102;&#23578;&#26381;&#35013;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#23646;&#24615;&#20998;&#31867;&#65292;&#24182;&#19988;&#32467;&#26524;&#34920;&#26126;&#65292;&#26032;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label classification models have a wide range of applications in E-commerce, including visual-based label predictions and language-based sentiment classifications. A major challenge in achieving satisfactory performance for these tasks in the real world is the notable imbalance in data distribution. For instance, in fashion attribute detection, there may be only six 'puff sleeve' clothes among 1000 products in most E-commerce fashion catalogs. To address this issue, we explore more data-efficient model training techniques rather than acquiring a huge amount of annotations to collect sufficient samples, which is neither economic nor scalable. In this paper, we propose a state-of-the-art weighted objective function to boost the performance of deep neural networks (DNNs) for multi-label classification with long-tailed data distribution. Our experiments involve image-based attribute classification of fashion apparels, and the results demonstrate favorable performance for the new weig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#34701;&#21512;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#24182;&#19988;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.03517</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#23454;&#29616;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#35270;&#35273;&#34701;&#21512;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Domain-Adaptive Visually-fused Event Detection from Text. (arXiv:2305.03517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#34701;&#21512;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#24182;&#19988;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#36741;&#21161;&#27169;&#24577;&#22914;&#22270;&#20687;&#25972;&#21512;&#21040;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#20013;&#24050;&#32463;&#24341;&#36215;&#20154;&#20204;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24773;&#22659;&#30340;&#22797;&#26434;&#24615;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#30456;&#20851;&#30340;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20107;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#35768;&#22810;&#26631;&#35760;&#22909;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#26029;&#26102;&#26080;&#27861;&#33719;&#24471;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#38480;&#21046;&#20063;&#20250;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#38590;&#20197;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#35270;&#35273;&#34701;&#21512;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#24456;&#23569;&#30340;&#26631;&#35760;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#35270;&#35273;&#24819;&#35937;&#22120;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#20174;&#25991;&#26412;&#20013;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#23450;&#21046;&#21040;&#29305;&#23450;&#30340;&#39046;&#22495;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21482;&#38656;&#23569;&#37327;&#30340;&#26631;&#35760;&#22270;&#20687;-&#25991;&#26412;&#23545;&#23601;&#21487;&#20197;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating auxiliary modalities such as images into event detection models has attracted increasing interest over the last few years. The complexity of natural language in describing situations has motivated researchers to leverage the related visual context to improve event detection performance. However, current approaches in this area suffer from data scarcity, where a large amount of labelled text-image pairs are required for model training. Furthermore, limited access to the visual context at inference time negatively impacts the performance of such models, which makes them practically ineffective in real-world scenarios. In this paper, we present a novel domain-adaptive visually-fused event detection approach that can be trained on a few labelled image-text paired data points. Specifically, we introduce a visual imaginator method that synthesises images from text in the absence of visual context. Moreover, the imaginator can be customised to a specific domain. In doing so, our
&lt;/p&gt;</description></item><item><title>SI-LSTM&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.03506</link><description>&lt;p&gt;
SI-LSTM: &#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#35828;&#35805;&#20154;&#28151;&#21512;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention for Emotion Recognition in Conversation. (arXiv:2305.03506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03506
&lt;/p&gt;
&lt;p&gt;
SI-LSTM&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#30340;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#23545;&#20110;&#26234;&#33021;&#21307;&#30103;&#12289;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#21644;&#32842;&#22825;&#21382;&#21490;&#35266;&#28857;&#25366;&#25496;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35828;&#35805;&#20154;&#20449;&#24687;&#22686;&#24378;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;SI-LSTM&#65289;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation~(ERC) across modalities is of vital importance for a variety of applications, including intelligent healthcare, artificial intelligence for conversation, and opinion mining over chat history. The crux of ERC is to model both cross-modality and cross-time interactions throughout the conversation. Previous methods have made progress in learning the time series information of conversation while lacking the ability to trace down the different emotional states of each speaker in a conversation. In this paper, we propose a recurrent structure called Speaker Information Enhanced Long-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states of the distinct speaker can be tracked in a sequential way to enhance the learning of the emotion in conversation. Further, to improve the learning of multimodal features in ERC, we utilize a cross-modal attention component to fuse the features between different modalities and model the interaction of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#36923;&#36753;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;</title><link>http://arxiv.org/abs/2305.03063</link><description>&lt;p&gt;
&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic model for cantilever beams damage detection. (arXiv:2305.03063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#36923;&#36753;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25439;&#20260;&#26816;&#27979;&#26041;&#27861;&#36805;&#36895;&#20174;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#36716;&#21464;&#20026;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#22320;&#12289;&#38750;&#20405;&#20837;&#24615;&#22320;&#20272;&#35745;&#26753;&#32467;&#26500;&#29366;&#24577;&#12290;&#20294;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36798;&#21040;&#24005;&#23792;&#34920;&#29616;&#65292;&#20154;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#23427;&#20204;&#36866;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#26131;&#21463;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30340;&#32570;&#22833;&#65292;&#30001;&#20110;&#30693;&#35782;&#32534;&#30721;&#22312;&#24352;&#37327;&#20540;&#20013;&#32780;&#27809;&#26377;&#21253;&#21547;&#36923;&#36753;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#22522;&#20110;&#26032;&#39062;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#30452;&#25509;&#23558;&#23454;&#38469;&#36923;&#36753;&#21253;&#21547;&#21040;&#27169;&#22411;&#20013;&#30340;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#12290;&#35813;&#28151;&#21512;&#21028;&#21035;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#31934;&#30830;&#22320;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#33021;&#22815;&#25552;&#20379;&#25439;&#20260;&#30340;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, damage detection approaches swiftly changed from advanced signal processing methods to machine learning and especially deep learning models, to accurately and non-intrusively estimate the state of the beam structures. But as the deep learning models reached their peak performances, also their limitations in applicability and vulnerabilities were observed. One of the most important reason for the lack of trustworthiness in operational conditions is the absence of intrinsic explainability of the deep learning system, due to the encoding of the knowledge in tensor values and without the inclusion of logical constraints. In this paper, we propose a neuro-symbolic model for the detection of damages in cantilever beams based on a novel cognitive architecture in which we join the processing power of convolutional networks with the interactive control offered by queries realized through the inclusion of real logic directly into the model. The hybrid discriminative model is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.02220</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#65306;&#26469;&#33258;MEDIQA-Chat&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#25552;&#20132;&#30340;&#33258;&#21160;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#26041;&#26696;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#65306;&#31532;&#19968;&#31181;&#26159;&#22312;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#31532;&#20108;&#31181;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#65292;&#22914;&#36890;&#36807;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#20363;&#22914;ROUGE&#65292;BERTScore&#65289;&#27979;&#37327;&#65292;&#24182;&#20998;&#21035;&#22312;&#25152;&#26377;&#25552;&#20132;&#30340;&#26041;&#26696;&#20013;&#25490;&#21517;&#31532;&#20108;&#21644;&#31532;&#19968;&#12290;&#19987;&#23478;&#23457;&#26680;&#34920;&#26126;&#65292;&#36890;&#36807;&#22522;&#20110;ICL&#30340;&#26041;&#27861;&#20351;&#29992;GPT-4&#29983;&#25104;&#30340;&#31508;&#35760;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#19968;&#26679;&#21463;&#27426;&#36814;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#31508;&#35760;&#30340;&#26377;&#21069;&#36884;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#36755;&#20837;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#34920;&#31034;&#26041;&#24335;&#65307;&#23545;&#19981;&#21516;&#23618;&#27425;&#30340;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#22810;&#31181;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#26377;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#34920;&#31034;&#39640;&#32423;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#22686;&#21152;&#35268;&#27169;&#20351;&#29305;&#24449;&#34920;&#31034;&#26356;&#21152;&#31232;&#30095;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.01610</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#25506;&#27979;&#20013;&#23547;&#25214;&#28023;&#37327;&#31070;&#32463;&#20803;: &#23454;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Finding Neurons in a Haystack: Case Studies with Sparse Probing. (arXiv:2305.01610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#36755;&#20837;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#34920;&#31034;&#26041;&#24335;&#65307;&#23545;&#19981;&#21516;&#23618;&#27425;&#30340;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#22810;&#31181;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#26377;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#34920;&#31034;&#39640;&#32423;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#22686;&#21152;&#35268;&#27169;&#20351;&#29305;&#24449;&#34920;&#31034;&#26356;&#21152;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24212;&#29992;&#21644;&#37096;&#32626;&#36805;&#36895;&#22686;&#21152;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#35745;&#31639;&#20173;&#28982;&#19981;&#36879;&#26126;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#39640;&#32423;&#21487;&#35299;&#37322;&#29305;&#24449;&#22312;LLM&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#20013;&#30340;&#34920;&#31034;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;(&#25506;&#38024;)&#26469;&#35757;&#32451;&#36825;&#20123;&#20869;&#37096;&#28608;&#27963;&#20540;&#65292;&#24182;&#39044;&#27979;&#36755;&#20837;&#30340;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65307;&#36890;&#36807;&#25913;&#21464;$k$&#20540;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#31232;&#30095;&#24615;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#24403;$k=1$&#26102;&#65292;&#25105;&#20204;&#23450;&#20301;&#26576;&#20010;&#29305;&#23450;&#29305;&#24449;&#38750;&#24120;&#30456;&#20851;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35828;&#26126;LLM&#30340;&#19968;&#33324;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#35768;&#22810;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#20284;&#20046;&#20855;&#26377;&#19987;&#38376;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#26356;&#39640;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#32780;&#22686;&#21152;&#30340;&#35268;&#27169;&#21017;&#23548;&#33268;&#34920;&#31034;&#31232;&#30095;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#12289;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#27867;&#21270;&#38590;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#20294;&#35201;&#27714;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2305.01034</link><description>&lt;p&gt;
&#26080;&#29305;&#23450;&#27169;&#22411;&#27867;&#21270;&#38590;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic Measure of Generalization Difficulty. (arXiv:2305.01034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#12289;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#27867;&#21270;&#38590;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#20294;&#35201;&#27714;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24230;&#37327;&#26159;&#20854;&#21487;&#20197;&#25191;&#34892;&#30340;&#20219;&#21153;&#38590;&#24230;&#65292;&#36275;&#22815;&#22256;&#38590;&#30340;&#20219;&#21153;&#26159;&#24378;&#22823;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#30340;&#27867;&#21270;&#38590;&#24230;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25454;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#20219;&#21153;&#22266;&#26377;&#27867;&#21270;&#38590;&#24230;&#30340;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#12290;&#36890;&#36807;&#27979;&#37327;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#20551;&#35774;&#22312;&#20219;&#21153;&#20013;&#27867;&#21270;&#30340;&#20998;&#25968;&#21344;&#25454;&#30340;&#23481;&#31215;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23427;&#19982;&#27169;&#22411;&#24517;&#39035;&#27867;&#21270;&#30340;&#31354;&#38388;&#30340;&#20869;&#22312;&#32500;&#25968;&#25104;&#25351;&#25968;&#27604;&#20363;&#65292;&#20294;&#20165;&#22312;&#27599;&#20010;&#32500;&#24230;&#30340;&#20998;&#36776;&#29575;&#19978;&#21576;&#22810;&#39033;&#24335;&#27604;&#20363;&#65292;&#34920;&#26126;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#30340;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The measure of a machine learning algorithm is the difficulty of the tasks it can perform, and sufficiently difficult tasks are critical drivers of strong machine learning models. However, quantifying the generalization difficulty of machine learning benchmarks has remained challenging. We propose what is to our knowledge the first model-agnostic measure of the inherent generalization difficulty of tasks. Our inductive bias complexity measure quantifies the total information required to generalize well on a task minus the information provided by the data. It does so by measuring the fractional volume occupied by hypotheses that generalize on a task given that they fit the training data. It scales exponentially with the intrinsic dimensionality of the space over which the model must generalize but only polynomially in resolution per dimension, showing that tasks which require generalizing over many dimensions are drastically more difficult than tasks involving more detail in fewer dimen
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;</title><link>http://arxiv.org/abs/2304.13567</link><description>&lt;p&gt;
&#20301;&#32622;&#20559;&#24046;&#23545;token&#20998;&#31867;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13567
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#25110;&#35789;&#24615;&#26631;&#27880;&#31561;&#19979;&#28216;&#20219;&#21153;&#24050;&#30693;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#36127;&#31034;&#20363;&#30340;&#27604;&#20363;&#21644;&#31867;&#19981;&#24179;&#34913;&#26041;&#38754;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#65292;&#21363;token&#20998;&#31867;&#20219;&#21153;&#20013;&#27491;&#31034;&#20363;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;Token&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20301;&#32622;&#20559;&#24046;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;CoNLL03&#21644;OntoNote5.0&#29992;&#20110;NER&#65292;English Tree Bank UD_en&#21644;TweeBank&#29992;&#20110;POS&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;Transformer&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#20687;BERT&#12289;ERNIE&#12289;ELECTRA&#36825;&#26679;&#30340;&#32534;&#30721;&#22120;&#21644;&#20687;GPT2 &#21644;BLOOM&#36825;&#26679;&#30340;&#35299;&#30721;&#22120;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.08897</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#25105;&#25913;&#36827;&#30828;&#32422;&#26463;&#30340;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning with self-improving hard constraints for multi-energy management systems. (arXiv:2304.08897v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#30828;&#32422;&#26463;&#20445;&#35777;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26159;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#26368;&#26377;&#21069;&#36884;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#21521;&#12290;&#23427;&#21482;&#38656;&#35201;&#22312;&#29615;&#22659;&#29305;&#23450;&#30340;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#19978;&#39044;&#20808;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#27169;&#22411;&#65288;&#21363;&#26893;&#29289;&#65292;&#24178;&#25200;&#21644;&#22122;&#22768;&#27169;&#22411;&#65292;&#20197;&#21450;&#26410;&#21253;&#25324;&#22312;&#26893;&#29289;&#27169;&#22411;&#20013;&#30340;&#29366;&#24577;&#30340;&#39044;&#27979;&#27169;&#22411; - &#20363;&#22914;&#38656;&#27714;&#65292;&#22825;&#27668;&#21644;&#20215;&#26684;&#39044;&#27979;&#65289;&#12290;&#22240;&#27492;&#65292;&#21487;&#20943;&#23569;&#39033;&#30446;&#29305;&#23450;&#30340;&#21069;&#26399;&#21644;&#25345;&#32493;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#20173;&#21487;&#20197;&#23398;&#20064;&#26356;&#22909;&#22320;&#34920;&#31034;&#22522;&#30784;&#31995;&#32479;&#21160;&#24577;&#65292;&#24182;&#20351;&#24314;&#27169;&#20559;&#24046;&#26368;&#23567;&#21270;&#65288;&#26080;&#22522;&#20110;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#65289;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20165;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26377;&#26102;&#20063;&#19981;&#24635;&#26159;&#23481;&#26131;&#25552;&#20379;&#20934;&#30830;&#30340;&#20808;&#39564;&#65288;&#20363;&#22914;&#33021;&#37327;&#24179;&#34913;&#32422;&#26463;&#38656;&#35201;&#35814;&#32454;&#30830;&#23450;&#25152;&#26377;&#33021;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36827;&#23637;&#65306;&#65288;I&#65289;&#23558;Optlayer&#21644;SafeFallback&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21629;&#21517;&#20026;O
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) with hard constraint guarantees is a promising optimal control direction for multi-energy management systems. It only requires the environment-specific constraint functions itself a prior and not a complete model (i.e. plant, disturbance and noise models, and prediction models for states not included in the plant model - e.g. demand, weather, and price forecasts). The project-specific upfront and ongoing engineering efforts are therefore still reduced, better representations of the underlying system dynamics can still be learned and modeling bias is kept to a minimum (no model-based objective function). However, even the constraint functions alone are not always trivial to accurately provide in advance (e.g. an energy balance constraint requires the detailed determination of all energy inputs and outputs), leading to potentially unsafe behavior. In this paper, we present two novel advancements: (I) combining the Optlayer and SafeFallback method, named O
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#23884;&#20837;&#35805;&#35821;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.08216</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#23884;&#20837;&#35805;&#35821;&#34920;&#31034;&#22312;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations. (arXiv:2304.08216v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#23884;&#20837;&#35805;&#35821;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35805;&#20195;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#65288;ERC&#65289;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35782;&#21035;&#24773;&#24863;&#23545;&#20110;&#26377;&#25928;&#30340;&#20132;&#27969;&#33267;&#20851;&#37325;&#35201;&#65292;&#26159;&#24320;&#21457;&#26377;&#25928;&#24182;&#19988;&#20855;&#26377;&#20849;&#24773;&#33021;&#21147;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#23545;&#35805;&#32972;&#26223;&#30340;&#30693;&#35782;&#21644;&#29702;&#35299;&#23545;&#20110;&#35782;&#21035;&#20132;&#27969;&#32773;&#30340;&#24773;&#24863;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#35805;&#32972;&#26223;&#26469;&#36827;&#34892;ERC&#65292;&#21363;&#20851;&#27880;&#20808;&#21069;&#30340;&#23545;&#35805;&#22238;&#21512;&#12290;&#36890;&#24120;&#65292;&#24314;&#27169;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#26159;&#20135;&#29983;&#27599;&#20010;&#35805;&#35821;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#34920;&#31034;&#65292;&#28982;&#21518;&#23545;&#36825;&#20123;&#35805;&#35821;&#36827;&#34892;&#19978;&#19979;&#25991;&#27169;&#22411;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#23884;&#20837;&#35805;&#35821;&#34920;&#31034;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#23545;&#35805;&#32972;&#26223;&#36861;&#21152;&#21040;&#35805;&#35821;&#20013;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#20837;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#20013;&#65292;&#35813;&#27169;&#22411;&#23558;&#20135;&#29983;&#30456;&#24212;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#19982;&#19978;&#19979;&#25991;&#26080;&#20851;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversations (ERC) has been gaining increasing importance as conversational agents become more and more common. Recognizing emotions is key for effective communication, being a crucial component in the development of effective and empathetic conversational agents. Knowledge and understanding of the conversational context are extremely valuable for identifying the emotions of the interlocutor. We thus approach Emotion Recognition in Conversations leveraging the conversational context, i.e., taking into attention previous conversational turns. The usual approach to model the conversational context has been to produce context-independent representations of each utterance and subsequently perform contextual modeling of these. Here we propose context-dependent embedding representations of each utterance by leveraging the contextual representational power of pre-trained transformer language models. In our approach, we feed the conversational context appended to the ut
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#22312;&#19981;&#22823;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#32422;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.02721</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#23545;&#31216;&#24615;&#65306;&#32467;&#26500;&#21098;&#26525;&#25552;&#39640;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#25512;&#26029;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#22312;&#19981;&#22823;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#32422;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#36830;&#36143;&#65292;&#30456;&#20851;&#21644;&#31616;&#27905;&#30340;&#25277;&#35937;&#25688;&#35201;&#12290;&#20294;&#26159;&#65292;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#20351;&#24471;&#22312;&#24310;&#36831;&#25935;&#24863;&#25110; Web &#35268;&#27169;&#30340;&#23454;&#29616;&#20013;&#37096;&#32626;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#30340;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#32534;&#30721;&#22120;&#22823;&#23567;&#26377;&#20851;&#65292;&#32780;&#25512;&#29702;&#25928;&#29575;&#19982;&#35299;&#30721;&#22120;&#26377;&#20851;&#12290;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#23548;&#33268;&#25512;&#26029;&#24310;&#36831;&#30340;&#36817;3&#20493;&#25552;&#39640;&#65292;Rouge-2&#30340;&#25439;&#22833;&#32422;&#20026;1&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24179;&#22343;&#24615;&#33021;&#38477;&#20302;&#21644;&#19981;&#23545;&#31216;&#24615;&#30340;&#20316;&#29992;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#38598;&#21464;&#21270;&#26041;&#38754;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence language models can be used to produce abstractive summaries which are coherent, relevant, and concise. Still, model sizes can make deployment in latency-sensitive or web-scale implementations difficult. This paper studies the relationship between model size, structured pruning, inference efficiency, and summarization accuracy on widely used summarization datasets. We show that model accuracy is tied to the encoder size while inference efficiency is connected to the decoder. Using asymmetric pruning can lead to nearly 3x improvement in inference latency with ~1 point loss in Rouge-2. Moreover, we find both the average degradation and the role of asymmetry to be consistent across model sizes and variations in datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02396</link><description>&lt;p&gt;
AutoRL&#36229;&#21442;&#25968;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21462;&#24471;&#20196;&#20154;&#30633;&#30446;&#25104;&#26524;&#30340;&#21516;&#26102;&#65292;&#20854;&#36229;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#12290;&#36825;&#32463;&#24120;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#33258;&#21160;&#21270;RL&#65288;AutoRL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38590;&#39064;&#65292;&#20294;&#26377;&#20851;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26041;&#27861;&#22312;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#26102;&#25152;&#36941;&#21382;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#21160;&#24577;&#21464;&#21270;&#30340;&#20449;&#24687;&#24456;&#23569;&#12290;&#37492;&#20110;&#29616;&#26377;AutoRL&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#20165;&#22312;&#19968;&#20010;&#26102;&#38388;&#28857;&#65292;&#32780;&#19988;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#24314;&#31435;&#21644;&#20998;&#26512;&#36825;&#20123;&#36229;&#21442;&#25968;&#26223;&#35266;&#12290;&#38024;&#23545;&#20851;&#20110;&#36825;&#31181;&#21160;&#24577;AutoRL&#26041;&#27861;&#21512;&#27861;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20805;&#20998;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#29615;&#22659;&#65288;Cartpole&#21644;Pendulum&#65289;&#20013;&#65292;&#26469;&#33258;RL&#25991;&#29486;&#30340;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#24378;&#28872;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16132</link><description>&lt;p&gt;
Transformer&#21644;Snowball&#22270;&#21367;&#31215;&#23398;&#20064;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification. (arXiv:2303.16132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25110;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#25551;&#36848;&#21644;&#24314;&#27169;&#29983;&#29289;&#21307;&#23398;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#23398;&#20064;&#21644;&#39044;&#27979;&#36825;&#31181;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;&#30340;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#65292;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;</title><link>http://arxiv.org/abs/2303.11702</link><description>&lt;p&gt;
&#36830;&#25509;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25506;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#20197;&#21069;&#27809;&#26377;&#27491;&#24335;&#23558;SSL&#21644;OSR&#32852;&#31995;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#21508;&#33258;&#30340;&#26041;&#27861;&#26377;&#24778;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSL-GAN&#21644;OSR-GAN&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;SSL&#21644;OSR&#20998;&#31867;&#22120;&#37117;&#21487;&#20197;&#23436;&#20840;&#35782;&#21035;&#24320;&#25918;&#31354;&#38388;&#12290;&#20026;&#20102;&#35777;&#26126;SSL&#21644;OSR&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;SSL-GAN&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;OSR-GAN&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#29486;&#22522;&#30784;&#26356;&#21152;&#29282;&#22266;&#30340;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#22312;&#26576;&#20123;&#19968;&#33324;&#30340;OSR&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;OSR&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#20114;&#24800;&#28857;&#65288;ARP&#65289;-GAN&#22312;&#19968;&#20123;OSR&#20219;&#21153;&#20013;&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2303.07205</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#23383;&#30340;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
The Science of Detecting LLM-Generated Texts. (arXiv:2303.07205v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#39640;&#24230;&#22797;&#26434;&#19988;&#20960;&#20046;&#38590;&#20197;&#21306;&#20998;&#20986;&#26159;&#21542;&#20026;&#20154;&#31867;&#21019;&#20316;&#30340; LLM &#29983;&#25104;&#25991;&#23383;&#12290;&#20294;&#26159;&#65292;&#36825;&#20063;&#24341;&#21457;&#20102;&#23545;&#27492;&#31867;&#25991;&#23383;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#65292;&#20363;&#22914;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#22312;&#25945;&#32946;&#31995;&#32479;&#20013;&#36896;&#25104;&#28151;&#20081;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20851;&#20110;&#20854;&#25104;&#23601;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#25991;&#26088;&#22312;&#27010;&#36848;&#29616;&#26377;&#30340; LLM &#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#65292;&#24182;&#22686;&#24378;&#23545;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#25511;&#21046;&#21644;&#30417;&#31649;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#65292;&#21253;&#25324;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#20197;&#25512;&#21160; LLM &#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) has resulted in the production of LLM-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. Although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. This survey aims to provide an overview of existing LLM-generated text detection techniques and enhance the control and regulation of language generation models. Furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source LLMs, to drive progress in the area of LLM-generated text detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#38646;&#37096;&#20214;&#35013;&#37197;&#20219;&#21153;&#30340;&#20132;&#20114;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20108;&#32500;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#25628;&#32034;&#26032;&#30340;&#35302;&#35273;&#35266;&#23519;&#65292;&#20197;&#26368;&#22823;&#21270;&#20854;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06034</link><description>&lt;p&gt;
Tactile-Filter: &#29992;&#20110;&#38646;&#37096;&#20214;&#35013;&#37197;&#30340;&#20132;&#20114;&#24335;&#35302;&#35273;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Tactile-Filter: Interactive Tactile Perception for Part Mating. (arXiv:2303.06034v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#38646;&#37096;&#20214;&#35013;&#37197;&#20219;&#21153;&#30340;&#20132;&#20114;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20108;&#32500;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#25628;&#32034;&#26032;&#30340;&#35302;&#35273;&#35266;&#23519;&#65292;&#20197;&#26368;&#22823;&#21270;&#20854;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20381;&#36182;&#35302;&#24863;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#26469;&#23436;&#25104;&#24456;&#22810;&#24039;&#22937;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#25509;&#35302;&#24418;&#24335;&#20197;&#21450;&#20219;&#20309;&#20132;&#20114;&#20013;&#29289;&#20307;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#25511;&#21046;&#20219;&#21153;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#38646;&#37096;&#20214;&#35013;&#37197;&#20219;&#21153;&#30340;&#20132;&#20114;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#21487;&#20197;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#31890;&#23376;&#28388;&#27874;&#22120;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#36880;&#27493;&#25913;&#36827;&#20854;&#23545;&#38646;&#37096;&#20214;&#65288;&#38144;&#23376;&#21644;&#23380;&#65289;&#30340;&#25311;&#21512;&#20272;&#35745;&#65292;&#26412;&#35770;&#25991;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20108;&#32500;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#33258;&#21160;&#25628;&#32034;&#26032;&#30340;&#35302;&#35273;&#35266;&#23519;&#65292;&#20197;&#26368;&#22823;&#21270;&#20854;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans rely on touch and tactile sensing for a lot of dexterous manipulation tasks. Our tactile sensing provides us with a lot of information regarding contact formations as well as geometric information about objects during any interaction. With this motivation, vision-based tactile sensors are being widely used for various robotic perception and control tasks. In this paper, we present a method for interactive perception using vision-based tactile sensors for a part mating task, where a robot can use tactile sensors and a feedback mechanism using a particle filter to incrementally improve its estimate of objects (pegs and holes) that fit together. To do this, we first train a deep neural network that makes use of tactile images to predict the probabilistic correspondence between arbitrarily shaped objects that fit together. The trained model is used to design a particle filter which is used twofold. First, given one partial (or non-unique) observation of the hole, it incrementally im
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#34920;&#31034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#21521;&#37327;&#22330;&#65292;&#32467;&#21512;&#26174;&#24335;&#23398;&#20064;&#21644;&#38544;&#24335;&#20989;&#25968;&#34920;&#31034;&#65292;&#21487;&#20197;&#25805;&#32437;&#32593;&#26684;&#24182;&#25171;&#30772;&#20998;&#36776;&#29575;&#21644;&#25299;&#25169;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.04341</link><description>&lt;p&gt;
&#31070;&#32463;&#21521;&#37327;&#22330;&#65306;&#26174;&#24335;&#23398;&#20064;&#30340;&#38544;&#24335;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Neural Vector Fields: Implicit Representation by Explicit Learning. (arXiv:2303.04341v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#34920;&#31034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#21521;&#37327;&#22330;&#65292;&#32467;&#21512;&#26174;&#24335;&#23398;&#20064;&#21644;&#38544;&#24335;&#20989;&#25968;&#34920;&#31034;&#65292;&#21487;&#20197;&#25805;&#32437;&#32593;&#26684;&#24182;&#25171;&#30772;&#20998;&#36776;&#29575;&#21644;&#25299;&#25169;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#21487;&#36827;&#19968;&#27493;&#20998;&#20026;&#20004;&#31867;&#65306;&#36890;&#36807;&#31227;&#21160;&#39030;&#28857;&#26126;&#30830;&#20256;&#36882;&#30340;&#26126;&#30830;&#21464;&#24418;&#21644;&#23558;&#19977;&#32500;&#34920;&#38754;&#38544;&#24335;&#34920;&#31034;&#20026;&#31614;&#21517;&#25110;&#26410;&#31614;&#21517;&#36317;&#31163;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#26174;&#24335;&#23398;&#20064;&#36807;&#31243;&#21644;&#38544;&#24335;&#20989;&#25968;&#30340;&#24378;&#22823;&#34920;&#31034;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;3D&#34920;&#31034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#21521;&#37327;&#22330;&#65288;NVF&#65289;&#12290;&#23427;&#19981;&#20165;&#37319;&#29992;&#26174;&#24335;&#23398;&#20064;&#36807;&#31243;&#30452;&#25509;&#25805;&#32437;&#32593;&#26684;&#65292;&#36824;&#21033;&#29992;&#26410;&#31614;&#21517;&#36317;&#31163;&#20989;&#25968;&#65288;UDF&#65289;&#30340;&#38544;&#24335;&#34920;&#31034;&#26041;&#24335;&#25171;&#30772;&#20102;&#20998;&#36776;&#29575;&#21644;&#25299;&#25169;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are widely applied for nowadays 3D surface reconstruction tasks and such methods can be further divided into two categories, which respectively warp templates explicitly by moving vertices or represent 3D surfaces implicitly as signed or unsigned distance functions. Taking advantage of both advanced explicit learning process and powerful representation ability of implicit functions, we propose a novel 3D representation method, Neural Vector Fields (NVF). It not only adopts the explicit learning process to manipulate meshes directly, but also leverages the implicit representation of unsigned distance functions (UDFs) to break the barriers in resolution and topology. Specifically, our method first predicts the displacements from queries towards the surface and models the shapes as \textit{Vector Fields}. Rather than relying on network differentiation to obtain direction fields as most existing UDF-based methods, the produced vector fields encode the distance a
&lt;/p&gt;</description></item><item><title>LIDA&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19982;&#35821;&#27861;&#26080;&#20851;&#30340;&#21487;&#35270;&#21270;&#21644;&#20449;&#24687;&#22270;&#34920;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.02927</link><description>&lt;p&gt;
LIDA&#65306;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19982;&#35821;&#27861;&#26080;&#20851;&#30340;&#21487;&#35270;&#21270;&#19982;&#20449;&#24687;&#22270;&#34920;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models. (arXiv:2303.02927v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02927
&lt;/p&gt;
&lt;p&gt;
LIDA&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19982;&#35821;&#27861;&#26080;&#20851;&#30340;&#21487;&#35270;&#21270;&#21644;&#20449;&#24687;&#22270;&#34920;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#29992;&#25143;&#33258;&#21160;&#21019;&#24314;&#21487;&#35270;&#21270;&#30340;&#31995;&#32479;&#24517;&#39035;&#35299;&#20915;&#22810;&#20010;&#23376;&#20219;&#21153;&#8212;&#8212;&#29702;&#35299;&#25968;&#25454;&#30340;&#35821;&#20041;&#12289;&#21015;&#20030;&#30456;&#20851;&#30340;&#21487;&#35270;&#21270;&#30446;&#26631;&#20197;&#21450;&#29983;&#25104;&#21487;&#35270;&#21270;&#35268;&#33539;&#12290;&#26412;&#25991;&#23558;&#21487;&#35270;&#21270;&#29983;&#25104;&#35270;&#20026;&#19968;&#20010;&#22810;&#38454;&#27573;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#35748;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#20363;&#22914;ChatGPT/GPT-4&#65289;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;IGM&#65289;&#30340;&#33391;&#22909;&#32534;&#37197;&#30340;&#31649;&#36947;&#36866;&#21512;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LIDA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#29992;&#20110;&#29983;&#25104;&#19982;&#35821;&#27861;&#26080;&#20851;&#30340;&#21487;&#35270;&#21270;&#21644;&#20449;&#24687;&#22270;&#34920;&#30340;&#24037;&#20855;&#12290;LIDA&#30001;4&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;SUMMARIZER&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#23500;&#20294;&#32039;&#20945;&#30340;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#12289;GOAL EXPLORER&#22312;&#32473;&#23450;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21015;&#20030;&#21487;&#35270;&#21270;&#30446;&#26631;&#12289;VISGENERATOR&#29983;&#25104;&#12289;&#25913;&#36827;&#12289;&#25191;&#34892;&#21644;&#36807;&#28388;&#21487;&#35270;&#21270;&#20195;&#30721;&#65292;&#20197;&#21450;INFOGRAPHER&#20351;&#29992;IGM&#29983;&#25104;&#25968;&#25454;&#24544;&#23454;&#30340;&#39118;&#26684;&#21270;&#22270;&#24418;&#12290;LIDA&#25552;&#20379;&#20102;&#19968;&#20010;Python API&#21644;&#28151;&#21512;&#29992;&#25143;&#30028;&#38754;&#65288;&#30452;&#25509;&#25805;&#20316;&#21644;&#22810;&#27169;&#24577;&#25991;&#26412;&#36755;&#20837;&#65289;&#65292;&#20379;&#29992;&#25143;&#25351;&#23450;&#25968;&#25454;&#21644;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LIDA&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#26377;&#24847;&#20041;&#19988;&#32654;&#35266;&#30340;&#21487;&#35270;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) such as ChatGPT/GPT-4 and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for generating grammar-agnostic visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER that converts data into a rich but compact natural language summary, a GOAL EXPLORER that enumerates visualization goals given the data, a VISGENERATOR that generates, refines, executes and filters visualization code and an INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA provides a python api, and a hybrid user interface (direct manipulation and mu
&lt;/p&gt;</description></item><item><title>TTIG&#27169;&#22411;&#26159;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#34917;&#20805;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19987;&#19994;&#20154;&#22763;&#23545;&#20110;TTIG&#24212;&#29992;&#21644;&#35748;&#30693;&#31561;&#26041;&#38754;&#23384;&#22312;12&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25903;&#25345;TTIG&#30340;&#21487;&#25345;&#32493;&#37319;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.12601</link><description>&lt;p&gt;
"&#19968;&#31181;&#36866;&#24212;&#25110;&#28781;&#20129;&#30340;&#23616;&#38754;": &#28216;&#25103;&#34892;&#19994;&#19987;&#19994;&#20154;&#22763;&#23545;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#20154;&#24037;&#26234;&#33021;&#30340;&#24863;&#30693;&#12289;&#37319;&#29992;&#21644;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
"An Adapt-or-Die Type of Situation": Perception, Adoption, and Use of Text-To-Image-Generation AI by Game Industry Professionals. (arXiv:2302.12601v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12601
&lt;/p&gt;
&lt;p&gt;
TTIG&#27169;&#22411;&#26159;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#34917;&#20805;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19987;&#19994;&#20154;&#22763;&#23545;&#20110;TTIG&#24212;&#29992;&#21644;&#35748;&#30693;&#31561;&#26041;&#38754;&#23384;&#22312;12&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25903;&#25345;TTIG&#30340;&#21487;&#25345;&#32493;&#37319;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;(TTIG)&#27169;&#22411;&#26159;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#34917;&#20805;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#65292;&#24320;&#22987;&#19982;&#19987;&#19994;&#21019;&#20316;&#32773;&#30340;&#20316;&#21697;&#31454;&#20105;&#24182;&#24341;&#21457;&#20102;&#26377;&#20851;&#21019;&#20316;&#24037;&#20316;&#12289;&#22833;&#19994;&#21644;&#29256;&#26435;&#31561;&#37325;&#35201;&#24433;&#21709;&#30340;&#35752;&#35770;&#12290;&#20026;&#20102;&#25903;&#25345;TTIG&#30340;&#21487;&#25345;&#32493;&#37319;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#25552;&#20379;&#19987;&#19994;&#20154;&#22763;&#24863;&#30693;&#12289;&#37319;&#29992;&#21644;&#20351;&#29992;TTIG&#30340;&#20016;&#23500;&#12289;&#21487;&#38752;&#21644;&#36879;&#26126;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#20844;&#20849;&#36777;&#35770;&#27973;&#34180;&#12289;&#29421;&#31364;&#19988;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#23398;&#26415;&#24037;&#20316;&#21017;&#38598;&#20013;&#20110;&#30740;&#31350;TTIG&#22312;&#19968;&#33324;&#33402;&#26415;&#23478;&#20154;&#32676;&#20013;&#30340;&#20351;&#29992;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#29305;&#23450;&#34892;&#19994;&#30340;&#19987;&#19994;&#20154;&#22763;&#30340;&#24863;&#30693;&#21644;&#24577;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#33452;&#20848;&#28216;&#25103;&#34892;&#19994;&#36827;&#34892;&#20102;&#19968;&#39033;&#23450;&#24615;&#30340;&#25506;&#32034;&#24615;&#35775;&#35848;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;TTIG&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;14&#20010;&#28216;&#25103;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#30340;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#30340;&#27169;&#26495;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;12&#20010;&#24635;&#20307;&#20027;&#39064;&#65292;&#32467;&#26500;&#21270;&#25104;49&#20010;&#23376;&#20027;&#39064;&#65292;&#25506;&#35752;&#20102;TTIG&#30340;&#24212;&#29992;&#21644;&#35748;&#30693;&#31561;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation (TTIG) models, a recent addition to creative AI, can generate images based on a text description. These models have begun to rival the work of professional creatives, and sparked discussions on the future of creative work, loss of jobs, and copyright issues, amongst other important implications. To support the sustainable adoption of TTIG, we must provide rich, reliable and transparent insights into how professionals perceive, adopt and use TTIG. Crucially though, the public debate is shallow, narrow and lacking transparency, while academic work has focused on studying the use of TTIG in a general artist population, but not on the perceptions and attitudes of professionals in a specific industry. In this paper, we contribute a qualitative, exploratory interview study on TTIG in the Finnish videogame industry. Through a Template Analysis on semi-structured interviews with 14 game professionals, we reveal 12 overarching themes, structured into 49 sub-themes on pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31561;&#21464;&#22810;&#39033;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#21487;&#20197;&#26356;&#22909;&#30340;&#25351;&#23548;GNN&#30340;&#27169;&#22411;&#25913;&#36827;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#24213;&#65292;&#20840;&#38754;&#21051;&#30011;&#20102;&#25152;&#26377;&#31561;&#21464;&#22270;&#22810;&#39033;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#26032;&#30340;GNN&#26550;&#26500;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.11556</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#22810;&#39033;&#24335;
&lt;/p&gt;
&lt;p&gt;
Equivariant Polynomials for Graph Neural Networks. (arXiv:2302.11556v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31561;&#21464;&#22810;&#39033;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#21487;&#20197;&#26356;&#22909;&#30340;&#25351;&#23548;GNN&#30340;&#27169;&#22411;&#25913;&#36827;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#24213;&#65292;&#20840;&#38754;&#21051;&#30011;&#20102;&#25152;&#26377;&#31561;&#21464;&#22270;&#22810;&#39033;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#26032;&#30340;GNN&#26550;&#26500;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#22312;&#20854;&#34920;&#36798;&#33021;&#21147;&#19978;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#37325;&#35201;&#24037;&#20316;(Xu&#31561;&#65292;2019&#65307;Morris&#31561;&#65292;2019b)&#24341;&#20837;&#20102;Weisfeiler-Lehman(WL)&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#34920;&#36798;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#34429;&#28982;&#36825;&#20010;&#23618;&#27425;&#32467;&#26500;&#25512;&#21160;&#20102;GNN&#20998;&#26512;&#21644;&#26550;&#26500;&#21457;&#23637;&#19978;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#23384;&#22312;&#30528;&#19968;&#20123;&#26174;&#33879;&#30340;&#38480;&#21046;&#12290;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22797;&#26434;&#30340;&#23450;&#20041;&#65292;&#32570;&#20047;&#25351;&#23548;&#27169;&#22411;&#25913;&#36827;&#30340;&#30452;&#25509;&#25351;&#23548;&#20197;&#21450;&#19968;&#20010;&#36807;&#20110;&#31895;&#31961;&#26080;&#27861;&#30740;&#31350;&#24403;&#21069;GNN&#30340;WL&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#33021;&#22815;&#35745;&#31639;&#29305;&#23450;&#27425;&#25968;&#30340;&#31561;&#21464;&#22810;&#39033;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#24213;&#65292;&#26174;&#33879;&#25512;&#24191;&#20102;&#20197;&#21069;&#30340;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#31561;&#21464;&#22270;&#22810;&#39033;&#24335;&#30340;&#20840;&#38754;&#21051;&#30011;&#12290;&#27599;&#20010;&#22522;&#24213;&#20803;&#32032;&#23545;&#24212;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#22810;&#22270;&#65292;&#20854;&#22312;&#26576;&#20123;&#22270;&#25968;&#25454;&#36755;&#20837;&#19978;&#30340;&#35745;&#31639;&#23545;&#24212;&#20110;&#19968;&#20010;&#24352;&#37327;&#25910;&#32553;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#31561;&#21464;&#22810;&#39033;&#24335;&#26469;&#23450;&#20041;&#26032;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#65292;&#25193;&#23637;&#20102;WL&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#26356;&#26131;&#20110;&#35745;&#31639;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#31934;&#32454;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#21644;&#20998;&#26512;&#26032;&#30340;GNN&#26550;&#26500;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.11552</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#37325;&#22797;&#21033;&#29992;&#12289;&#22238;&#25910;&#65306;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25193;&#25955;&#27169;&#22411;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#32463;&#36805;&#36895;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#31995;&#21015;&#26102;&#21464;&#30340;&#23545;&#25968;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#31181;&#35299;&#37322;&#24050;&#32463;&#28608;&#21457;&#20102;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#24605;&#24819;&#25104;&#20026;&#21518;&#32493;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#36825;&#20123;&#24819;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;-based&#35299;&#37322;&#65292;&#25506;&#32034;&#20102;&#29992;&#20110;&#28041;&#21450;&#32452;&#21512;&#29983;&#25104;&#21644;&#25351;&#23548;&#30340;&#26465;&#20214;&#12289;&#20462;&#25913;&#21644;&#37325;&#22797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#31867;&#22411;&#30340;&#32452;&#21512;&#20351;&#29992;&#24403;&#21069;&#25216;&#26415;&#22833;&#36133;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37319;&#26679;&#32773;(&#32780;&#19981;&#26159;&#27169;&#22411;)&#23545;&#27492;&#22833;&#36133;&#36127;&#26377;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#37319;&#26679;&#22120;&#65292;&#21463;MCMC&#30340;&#21551;&#21457;&#65292;&#20351;&#32452;&#21512;&#29983;&#25104;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#30693;&#39118;&#38505;&#30340;&#26032;&#22411;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#36716;&#21270;&#20026;&#20215;&#20540;&#65292;&#40723;&#21169;&#26234;&#33021;&#20307;&#25506;&#32034;&#26410;&#30693;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20013;&#23454;&#29616;&#39640;&#25928;&#25506;&#32034;&#65292;&#21363;&#20351;&#22312;&#20989;&#25968;&#36924;&#36817;&#19979;&#20063;&#20855;&#26377;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.09339</link><description>&lt;p&gt;
&#36890;&#36807;&#35748;&#30693;&#39118;&#38505;&#23548;&#21521;&#31574;&#30053;&#20248;&#21270;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Exploration via Epistemic-Risk-Seeking Policy Optimization. (arXiv:2302.09339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#30693;&#39118;&#38505;&#30340;&#26032;&#22411;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#36716;&#21270;&#20026;&#20215;&#20540;&#65292;&#40723;&#21169;&#26234;&#33021;&#20307;&#25506;&#32034;&#26410;&#30693;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20013;&#23454;&#29616;&#39640;&#25928;&#25506;&#32034;&#65292;&#21363;&#20351;&#22312;&#20989;&#25968;&#36924;&#36817;&#19979;&#20063;&#20855;&#26377;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#65292;&#20048;&#35266;&#20027;&#20041;&#26159;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#20294;&#22914;&#20309;&#23558;&#35813;&#21407;&#21017;&#26368;&#22909;&#22320;&#36716;&#21270;&#21040;&#28041;&#21450;&#22312;&#32447;&#38543;&#26426;&#26799;&#24230;&#21644;&#28145;&#24230;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23578;&#26410;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20048;&#35266;&#30446;&#26631;&#65292;&#24403;&#20248;&#21270;&#26102;&#65292;&#20135;&#29983;&#19968;&#31181;&#21487;&#35777;&#26126;&#26377;&#25928;&#25506;&#32034;&#30340;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#20989;&#25968;&#36924;&#36817;&#19979;&#20063;&#20855;&#26377;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26032;&#30446;&#26631;&#26159;&#19968;&#31181;&#38646;&#21644;&#20108;&#20154;&#21338;&#24328;&#65292;&#28304;&#20110;&#36171;&#20104;&#20195;&#29702;&#19968;&#20010;&#35748;&#30693;&#39118;&#38505;&#23548;&#21521;&#25928;&#29992;&#20989;&#25968;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#36716;&#21270;&#20026;&#20215;&#20540;&#65292;&#24182;&#40723;&#21169;&#20195;&#29702;&#20154;&#25506;&#32034;&#19981;&#30830;&#23450;&#29366;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#28216;&#25103;&#30340;&#35299;&#20915;&#26041;&#26696;&#26368;&#23567;&#21270;&#20102;&#24724;&#24680;&#30340;&#19968;&#20010;&#19978;&#30028;&#65292;&#20854;&#20013;&#8220;&#29609;&#23478;&#8221;&#21508;&#33258;&#23581;&#35797;&#26368;&#23567;&#21270;&#29305;&#23450;&#24724;&#24680;&#20998;&#35299;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploration remains a key challenge in deep reinforcement learning (RL). Optimism in the face of uncertainty is a well-known heuristic with theoretical guarantees in the tabular setting, but how best to translate the principle to deep reinforcement learning, which involves online stochastic gradients and deep network function approximators, is not fully understood. In this paper we propose a new, differentiable optimistic objective that when optimized yields a policy that provably explores efficiently, with guarantees even under function approximation. Our new objective is a zero-sum two-player game derived from endowing the agent with an epistemic-risk-seeking utility function, which converts uncertainty into value and encourages the agent to explore uncertain states. We show that the solution to this game minimizes an upper bound on the regret, with the 'players' each attempting to minimize one component of a particular regret decomposition. We derive a new model-free algorithm which
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#30446;&#21069;&#20581;&#22766;&#31639;&#27861;&#22312;&#38750;IID&#29615;&#22659;&#19979;&#34920;&#29616;&#19979;&#38477;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;GAS&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#23558;&#29616;&#26377;&#30340;&#20581;&#22766;&#31639;&#27861;&#29992;&#20110;&#38750;IID&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.06079</link><description>&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#20998;&#21106;&#23454;&#29616;&#23545;&#24322;&#26500;&#25968;&#25454;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting. (arXiv:2302.06079v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#30446;&#21069;&#20581;&#22766;&#31639;&#27861;&#22312;&#38750;IID&#29615;&#22659;&#19979;&#34920;&#29616;&#19979;&#38477;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;GAS&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#23558;&#29616;&#26377;&#30340;&#20581;&#22766;&#31639;&#27861;&#29992;&#20110;&#38750;IID&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#23545;&#25308;&#21344;&#24237;&#25915;&#20987;&#20855;&#26377; vulnerabilities&#65292;&#21363;&#25915;&#20987;&#32773;&#21487;&#20197;&#21521;&#20013;&#22830;&#26381;&#21153;&#22120;&#21457;&#36865;&#20219;&#24847;&#26799;&#24230;&#20197;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;&#19968;&#20123;&#20581;&#22766;&#30340;&#32858;&#21512;&#35268;&#21017;&#65288;AGRs&#65289;&#24050;&#34987;&#25552;&#20986;&#26469;&#20197;&#25269;&#24481;&#23545;&#25239;&#25308;&#21344;&#24237;&#25915;&#20987;&#12290;&#20294;&#26159;&#65292;&#24403;&#25968;&#25454;&#19981;&#26381;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#26102;&#65292;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#20173;&#28982;&#21487;&#20197;&#35268;&#36991;&#20581;&#22766;&#30340; AGRs&#12290;&#26412;&#25991;&#39318;&#20808;&#25581;&#31034;&#20102;&#24403;&#21069;&#20581;&#22766; AGRs &#22312;&#38750;IID&#29615;&#22659;&#19979;&#34920;&#29616;&#19979;&#38477;&#30340;&#26681;&#26412;&#21407;&#22240;&#65306;&#32500;&#24230;&#28798;&#38590;&#21644;&#26799;&#24230;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; GAS&#65292;&#19968;&#31181;&#32553;&#30701;&#26041;&#27861;&#65292;&#21487;&#20197;&#25104;&#21151;&#22320;&#23558;&#29616;&#26377;&#30340;&#20581;&#22766; AGRs &#36866;&#24212;&#20110;&#38750;IID&#29615;&#22659;&#12290;&#24403;&#29616;&#26377;&#20581;&#22766; AGRs &#19982; GAS &#32452;&#21512;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340; GAS &#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#29616;&#20195;&#30721;&#21487;&#22312; https://github.com/Y &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has exhibited vulnerabilities to Byzantine attacks, where the Byzantine attackers can send arbitrary gradients to a central server to destroy the convergence and performance of the global model. A wealth of robust AGgregation Rules (AGRs) have been proposed to defend against Byzantine attacks. However, Byzantine clients can still circumvent robust AGRs when data is non-Identically and Independently Distributed (non-IID). In this paper, we first reveal the root causes of performance degradation of current robust AGRs in non-IID settings: the curse of dimensionality and gradient heterogeneity. In order to address this issue, we propose GAS, a \shorten approach that can successfully adapt existing robust AGRs to non-IID settings. We also provide a detailed convergence analysis when the existing robust AGRs are combined with GAS. Experiments on various real-world datasets verify the efficacy of our proposed GAS. The implementation code is provided in https://github.com/Y
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#36965;&#24863;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;Viewmaker&#32593;&#32476;&#65292;&#26412;&#25991;&#21457;&#29616;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#20135;&#29983;&#35270;&#22270;&#65292;&#24182;&#22312;&#22235;&#20010;&#22810;&#20809;&#35889;&#25104;&#20687;&#38382;&#39064;&#19978;&#23454;&#29616;&#20248;&#20110;&#22522;&#20110;&#35009;&#21098;&#21644;&#21453;&#23556;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.05757</link><description>&lt;p&gt;
&#22810;&#20809;&#35889;&#23545;&#27604;&#23398;&#20064;&#19982;Viewmaker&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multispectral Contrastive Learning with Viewmaker Networks. (arXiv:2302.05757v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#36965;&#24863;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;Viewmaker&#32593;&#32476;&#65292;&#26412;&#25991;&#21457;&#29616;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#20135;&#29983;&#35270;&#22270;&#65292;&#24182;&#22312;&#22235;&#20010;&#22810;&#20809;&#35889;&#25104;&#20687;&#38382;&#39064;&#19978;&#23454;&#29616;&#20248;&#20110;&#22522;&#20110;&#35009;&#21098;&#21644;&#21453;&#23556;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#39046;&#22495;&#21644;&#27169;&#24577;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#25968;&#25454;&#28857;&#30340;&#30456;&#20284;&#8220;&#35270;&#22270;&#8221;&#12290;&#28982;&#32780;&#65292;&#19987;&#19994;&#30340;&#31185;&#23398;&#27169;&#24577;&#23545;&#36825;&#31181;&#33539;&#24335; pose challenge&#65292;&#22240;&#20026;&#20026;&#27599;&#20010;&#31185;&#23398;&#20202;&#22120;&#35782;&#21035;&#22909;&#30340;&#35270;&#22270;&#26159;&#22797;&#26434;&#21644;&#32791;&#26102;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23558;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#36965;&#24863;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Viewmaker&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#29983;&#25104;&#35270;&#22270;&#30340;&#26041;&#27861;&#65292;&#24456;&#26377;&#21069;&#36884;&#22320;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#30693;&#35782;&#21644;&#35797;&#38169;&#30340;&#24773;&#20917;&#19979;&#22312;&#36825;&#19968;&#39046;&#22495;&#20869;&#20135;&#29983;&#35270;&#22270;&#12290;&#25105;&#20204;&#23558;Viewmaker&#24212;&#29992;&#20110;&#22235;&#20010;&#22810;&#20809;&#35889;&#25104;&#20687;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#20855;&#26377;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21457;&#29616;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#35780;&#20272;&#20013;&#65292;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;Viewmaker&#37117;&#21487;&#20197;&#20248;&#20110;&#22522;&#20110;&#35009;&#21098;&#21644;&#21453;&#23556;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#23545;&#27604;&#23398;&#20064;&#25193;&#23637;&#21040;&#23454;&#38469;&#30340;&#31185;&#23398;&#24212;&#29992;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning methods have been applied to a range of domains and modalities by training models to identify similar "views" of data points. However, specialized scientific modalities pose a challenge for this paradigm, as identifying good views for each scientific instrument is complex and time-intensive. In this paper, we focus on applying contrastive learning approaches to a variety of remote sensing datasets. We show that Viewmaker networks, a recently proposed method for generating views, are promising for producing views in this setting without requiring extensive domain knowledge and trial and error. We apply Viewmaker to four multispectral imaging problems, each with a different format, finding that Viewmaker can outperform croppingand reflection-based methods for contrastive learning in every case when evaluated on downstream classification tasks. This provides additional evidence that domain-agnostic methods can empower contrastive learning to scale to real-world scie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25511;&#24615;&#24863;&#30693;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#30340;&#36317;&#31163;&#20989;&#25968;&#38477;&#20302;&#31616;&#21333;&#26131;&#23454;&#29616;&#25216;&#33021;&#30340;&#22870;&#21169;&#65292;&#36880;&#27493;&#23398;&#20064;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05103</link><description>&lt;p&gt;
&#21487;&#25511;&#24615;&#24863;&#30693;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Controllability-Aware Unsupervised Skill Discovery. (arXiv:2302.05103v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25511;&#24615;&#24863;&#30693;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#30340;&#36317;&#31163;&#20989;&#25968;&#38477;&#20302;&#31616;&#21333;&#26131;&#23454;&#29616;&#25216;&#33021;&#30340;&#22870;&#21169;&#65292;&#36880;&#27493;&#23398;&#20064;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20195;&#29702;&#30340;&#20851;&#38190;&#33021;&#21147;&#20043;&#19968;&#26159;&#22312;&#27809;&#26377;&#22806;&#37096;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#26377;&#29992;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#24448;&#24448;&#21482;&#33021;&#33719;&#24471;&#31616;&#21333;&#12289;&#26131;&#23398;&#30340;&#25216;&#33021;&#65292;&#22240;&#20026;&#32570;&#20047;&#21457;&#29616;&#26356;&#22797;&#26434;&#12289;&#26377;&#25361;&#25112;&#24615;&#34892;&#20026;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#25511;&#24615;&#24863;&#30693;&#25216;&#33021;&#21457;&#29616;&#65288;CSD&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20027;&#21160;&#23547;&#25214;&#22797;&#26434;&#12289;&#38590;&#20197;&#25511;&#21046;&#30340;&#25216;&#33021;&#12290;CSD&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21487;&#25511;&#24615;&#24863;&#30693;&#36317;&#31163;&#20989;&#25968;&#65292;&#23427;&#32473;&#24403;&#21069;&#25216;&#33021;&#23454;&#29616;&#26356;&#38590;&#30340;&#29366;&#24577;&#36716;&#25442;&#20998;&#37197;&#26356;&#22823;&#30340;&#20540;&#12290;&#19982;&#36317;&#31163;&#26368;&#22823;&#21270;&#30340;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;CSD&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#27493;&#23398;&#20064;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25216;&#33021;&#65292;&#22240;&#20026;&#25105;&#20204;&#32852;&#21512;&#35757;&#32451;&#30340;&#36317;&#31163;&#20989;&#25968;&#38477;&#20302;&#20102;&#31616;&#21333;&#26131;&#23454;&#29616;&#25216;&#33021;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#21644;&#36816;&#21160;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CSD&#21487;&#20197;&#21457;&#29616;&#21508;&#31181;&#19981;&#21516;&#30340;&#22797;&#26434;&#25216;&#33021;&#65292;&#32988;&#36807;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key capabilities of intelligent agents is the ability to discover useful skills without external supervision. However, the current unsupervised skill discovery methods are often limited to acquiring simple, easy-to-learn skills due to the lack of incentives to discover more complex, challenging behaviors. We introduce a novel unsupervised skill discovery method, Controllability-aware Skill Discovery (CSD), which actively seeks complex, hard-to-control skills without supervision. The key component of CSD is a controllability-aware distance function, which assigns larger values to state transitions that are harder to achieve with the current skills. Combined with distance-maximizing skill discovery, CSD progressively learns more challenging skills over the course of training as our jointly trained distance function reduces rewards for easy-to-achieve skills. Our experimental results in six robotic manipulation and locomotion environments demonstrate that CSD can discover diver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04831</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21327;&#21516;&#21512;&#20316;&#23398;&#20064;&#26694;&#26550;&#30340;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Open-ended Learning Framework for Zero-shot Coordination. (arXiv:2302.04831v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38646;&#26679;&#26412;&#21327;&#35843;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#21327;&#35843;&#19968;&#31995;&#21015;&#30475;&#19981;&#35265;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#20808;&#21069;&#30340;&#31639;&#27861;&#35797;&#22270;&#36890;&#36807;&#20248;&#21270;&#31181;&#32676;&#20013;&#30340;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#21892;&#31574;&#30053;&#25110;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#25439;&#22833;&#21644;&#19982;&#31181;&#32676;&#20013;&#26576;&#20123;&#31574;&#30053;&#26080;&#27861;&#21512;&#20316;&#65292;&#21363;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;&#65288;COLE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#26500;&#24314;&#20102;&#21327;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20197;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26126;&#30830;&#20102;&#26694;&#26550;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#21338;&#24328;&#35770;&#21644;&#22270;&#35770;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23545;&#31639;&#27861;&#30340;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20811;&#26381;&#23398;&#20064;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behaviour diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#39044;&#27979;&#30340;MDP&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#23558;&#21407;&#22987;MDP&#36716;&#25442;&#20026;&#23398;&#20064;&#34892;&#21160;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#20934;&#30830;&#21644;&#31283;&#23450;&#65292;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.03921</link><description>&lt;p&gt;
&#21487;&#39044;&#27979;&#30340;MDP&#25277;&#35937;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Predictable MDP Abstraction for Unsupervised Model-Based RL. (arXiv:2302.03921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#39044;&#27979;&#30340;MDP&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#23558;&#21407;&#22987;MDP&#36716;&#25442;&#20026;&#23398;&#20064;&#34892;&#21160;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#20934;&#30830;&#21644;&#31283;&#23450;&#65292;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#19968;&#20010;&#33021;&#39044;&#27979;&#34892;&#21160;&#32467;&#26524;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20250;&#38477;&#20302;&#27169;&#22411;&#21270;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#65292;&#22797;&#26434;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#21487;&#33021;&#20250;&#24102;&#26469;&#26497;&#20854;&#22256;&#38590;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#39044;&#27979;&#30340;MDP&#25277;&#35937;&#65288;PMA&#65289;&#65306;&#19981;&#26159;&#22312;&#21407;&#22987;MDP&#19978;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#65292;&#32780;&#26159;&#22312;&#19968;&#20010;&#36716;&#25442;&#21518;&#30340;&#20855;&#26377;&#23398;&#20064;&#34892;&#21160;&#31354;&#38388;&#30340;MDP&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#34892;&#21160;&#31354;&#38388;&#21482;&#20801;&#35768;&#21487;&#39044;&#27979;&#12289;&#26131;&#24314;&#27169;&#30340;&#34892;&#21160;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#35206;&#30422;&#21407;&#22987;&#29366;&#24577;-&#34892;&#21160;&#31354;&#38388;&#12290;&#32467;&#26524;&#26159;&#65292;&#27169;&#22411;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#21644;&#20934;&#30830;&#65292;&#36825;&#20801;&#35768;&#40065;&#26834;&#12289;&#31283;&#23450;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#25110;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#12290;&#36825;&#31181;&#36716;&#25442;&#26159;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#30340;&#65292;&#22312;&#29992;&#25143;&#25351;&#23450;&#20219;&#20309;&#20219;&#21153;&#20043;&#21069;&#12290;&#38543;&#21518;&#65292;&#19979;&#28216;&#20219;&#21153;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#36890;&#36807;&#27169;&#22411;&#21270;&#25511;&#21046;&#35299;&#20915;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#29615;&#22659;&#20132;&#20114;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key component of model-based reinforcement learning (RL) is a dynamics model that predicts the outcomes of actions. Errors in this predictive model can degrade the performance of model-based controllers, and complex Markov decision processes (MDPs) can present exceptionally difficult prediction problems. To mitigate this issue, we propose predictable MDP abstraction (PMA): instead of training a predictive model on the original MDP, we train a model on a transformed MDP with a learned action space that only permits predictable, easy-to-model actions, while covering the original state-action space as much as possible. As a result, model learning becomes easier and more accurate, which allows robust, stable model-based planning or model-based RL. This transformation is learned in an unsupervised manner, before any task is specified by the user. Downstream tasks can then be solved with model-based control in a zero-shot fashion, without additional environment interactions. We theoretical
&lt;/p&gt;</description></item><item><title>ANTM&#26159;&#19968;&#31181;&#23545;&#40784;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#26469;&#32500;&#25252;&#28436;&#21464;&#20027;&#39064;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#36827;&#34892;&#23545;&#40784;&#26469;&#25429;&#25417;&#20986;&#29616;&#21644;&#28040;&#36864;&#30340;&#36235;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;ANTM&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.01501</link><description>&lt;p&gt;
ANTM: &#19968;&#31181;&#23545;&#40784;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25506;&#32034;&#28436;&#21464;&#30340;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
ANTM: An Aligned Neural Topic Model for Exploring Evolving Topics. (arXiv:2302.01501v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01501
&lt;/p&gt;
&lt;p&gt;
ANTM&#26159;&#19968;&#31181;&#23545;&#40784;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#26469;&#32500;&#25252;&#28436;&#21464;&#20027;&#39064;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#36827;&#34892;&#23545;&#40784;&#26469;&#25429;&#25417;&#20986;&#29616;&#21644;&#28040;&#36864;&#30340;&#36235;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;ANTM&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#40784;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;ANTM&#65289;&#30340;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#31639;&#27861;&#23478;&#26063;&#65292;&#23427;&#32467;&#21512;&#20102;&#26032;&#39062;&#30340;&#25968;&#25454;&#25366;&#25496;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21457;&#29616;&#28436;&#21464;&#30340;&#20027;&#39064;&#12290;ANTM&#21033;&#29992;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#26102;&#38388;&#24863;&#30693;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#36827;&#34892;&#39034;&#24207;&#25991;&#26723;&#32858;&#31867;&#65292;&#20174;&#32780;&#32500;&#25252;&#20102;&#28436;&#21464;&#20027;&#39064;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#12290;&#36825;&#31181;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#22312;&#27599;&#20010;&#26102;&#38388;&#26694;&#26550;&#20869;&#26631;&#35782;&#19981;&#21516;&#25968;&#37327;&#30340;&#20027;&#39064;&#65292;&#24182;&#22312;&#26102;&#38388;&#27573;&#20869;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#32858;&#31867;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#20010;&#36807;&#31243;&#25429;&#25417;&#20102;&#19981;&#21516;&#26102;&#26399;&#20986;&#29616;&#21644;&#28040;&#36864;&#30340;&#36235;&#21183;&#65292;&#24182;&#20801;&#35768;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#28436;&#21464;&#20027;&#39064;&#34920;&#31034;&#12290;&#38024;&#23545;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ANTM&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#27010;&#29575;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#25913;&#21892;&#20102;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an algorithmic family of dynamic topic models called Aligned Neural Topic Models (ANTM), which combine novel data mining algorithms to provide a modular framework for discovering evolving topics. ANTM maintains the temporal continuity of evolving topics by extracting time-aware features from documents using advanced pre-trained Large Language Models (LLMs) and employing an overlapping sliding window algorithm for sequential document clustering. This overlapping sliding window algorithm identifies a different number of topics within each time frame and aligns semantically similar document clusters across time periods. This process captures emerging and fading trends across different periods and allows for a more interpretable representation of evolving topics. Experiments on four distinct datasets show that ANTM outperforms probabilistic dynamic topic models in terms of topic coherence and diversity metrics. Moreover, it improves the scalability and flexibility of dy
&lt;/p&gt;</description></item><item><title>&#21453;&#21319;&#32423;&#25110;&#27010;&#25324;&#26159;&#24402;&#32435;&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#26159;&#23450;&#29702;&#35777;&#26126;&#30340;&#21452;&#37325;&#25805;&#20316;&#20043;&#19968;&#12290;&#35813;&#35843;&#26597;&#25253;&#21578;&#23545;&#21453;&#21319;&#32423;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#24402;&#32435;&#21644;&#24635;&#32467;&#12290;</title><link>http://arxiv.org/abs/2302.00277</link><description>&lt;p&gt;
&#21453;&#21319;&#32423;&#19982;&#27010;&#25324;&#65306;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Anti-unification and Generalization: A Survey. (arXiv:2302.00277v3 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00277
&lt;/p&gt;
&lt;p&gt;
&#21453;&#21319;&#32423;&#25110;&#27010;&#25324;&#26159;&#24402;&#32435;&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#26159;&#23450;&#29702;&#35777;&#26126;&#30340;&#21452;&#37325;&#25805;&#20316;&#20043;&#19968;&#12290;&#35813;&#35843;&#26597;&#25253;&#21578;&#23545;&#21453;&#21319;&#32423;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#24402;&#32435;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21319;&#32423;&#65288;AU&#65289;&#21448;&#31216;&#27010;&#25324;&#65292;&#26159;&#24402;&#32435;&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#26159;&#23450;&#29702;&#35777;&#26126;&#22522;&#30784;&#19978;&#30340;&#21452;&#37325;&#25805;&#20316;&#20043;&#19968;&#12290; AI&#21644;&#30456;&#20851;&#31038;&#21306;&#23545;AU&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20294;&#27809;&#26377;&#31995;&#32479;&#30740;&#31350;&#35813;&#27010;&#24565;&#65292;&#20063;&#27809;&#26377;&#29616;&#26377;&#24037;&#20316;&#30340;&#35843;&#26597;&#65292;&#35843;&#26597;&#24448;&#24448;&#20250;&#37319;&#29992;&#29305;&#23450;&#20110;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#24050;&#32463;&#34987;&#29616;&#26377;&#26041;&#27861;&#35206;&#30422;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#26377;&#20851;AU&#30740;&#31350;&#21450;&#20854;&#24212;&#29992;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#20197;&#21450;&#19968;&#31181;&#23558;&#29616;&#26377;&#21644;&#26410;&#26469;&#21457;&#23637;&#20998;&#31867;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anti-unification (AU), also known as generalization, is a fundamental operation used for inductive inference and is the dual operation to unification, an operation at the foundation of theorem proving. Interest in AU from the AI and related communities is growing, but without a systematic study of the concept, nor surveys of existing work, investigations7 often resort to developing application-specific methods that may be covered by existing approaches. We provide the first survey of AU research and its applications, together with a general framework for categorizing existing and future developments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTape&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30913;&#24102;&#31526;&#21495;&#65292;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#24577;&#35745;&#31639;&#65292;&#33021;&#22815;&#23454;&#29616;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#20449;&#24687;&#30340;&#33258;&#36866;&#24212;&#35745;&#31639;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#31243;&#24207;&#32508;&#21512;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13195</link><description>&lt;p&gt;
&#24377;&#24615;&#36755;&#20837;&#24207;&#21015;&#30340;&#33258;&#36866;&#24212;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Adaptive Computation with Elastic Input Sequence. (arXiv:2301.13195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTape&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30913;&#24102;&#31526;&#21495;&#65292;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#24577;&#35745;&#31639;&#65292;&#33021;&#22815;&#23454;&#29616;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#20449;&#24687;&#30340;&#33258;&#36866;&#24212;&#35745;&#31639;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#31243;&#24207;&#32508;&#21512;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#12289;&#19981;&#21516;&#30340;&#22788;&#29702;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#26102;&#38388;&#33457;&#36153;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#26080;&#35770;&#26679;&#26412;&#30340;&#24615;&#36136;&#25110;&#38590;&#24230;&#37117;&#26377;&#22266;&#23450;&#30340;&#20989;&#25968;&#31867;&#22411;&#21644;&#35745;&#31639;&#39044;&#31639;&#12290;&#33258;&#36866;&#24212;&#35745;&#31639;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#36171;&#20104;&#20174;&#19994;&#32773;&#28789;&#27963;&#24615;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#20316;&#20026;&#35299;&#20915;&#26576;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#30340;&#24378;&#22823;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTape&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30913;&#24102;&#31526;&#21495;&#65292;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#24577;&#35745;&#31639;&#12290;AdaTape&#21033;&#29992;&#24377;&#24615;&#36755;&#20837;&#24207;&#21015;&#65292;&#36890;&#36807;&#35013;&#22791;&#24102;&#26377;&#21160;&#24577;&#35835;&#20889;&#30913;&#24102;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#30913;&#24102;&#24211;&#30340;&#30913;&#24102;&#31526;&#21495;&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#36755;&#20837;&#24207;&#21015;&#65292;&#36825;&#20123;&#31526;&#21495;&#21487;&#35757;&#32451;&#25110;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#27966;&#29983;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33719;&#24471;&#21160;&#24577;&#24207;&#21015;&#35745;&#31639;&#25152;&#38656;&#30340;&#25361;&#25112;&#21644;&#35201;&#27714;&#65292;&#20197;&#21450;AdaTape&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AdaTape&#33021;&#22815;&#23398;&#20064;&#33258;&#36866;&#24212;&#35745;&#31639;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#65288;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#31243;&#24207;&#32508;&#21512;&#65289;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have the ability to adapt the type of information they use, the procedure they employ, and the amount of time they spend when solving problems. However, most standard neural networks have a fixed function type and computation budget regardless of the sample's nature or difficulty. Adaptivity is a powerful paradigm as it not only imbues practitioners with flexibility pertaining to the downstream usage of these models but can also serve as a powerful inductive bias for solving certain challenging classes of problems. In this work, we introduce a new approach called AdaTape, which allows for dynamic computation in neural networks through adaptive tape tokens. AdaTape utilizes an elastic input sequence by equipping an architecture with a dynamic read-and-write tape. Specifically, we adaptively generate input sequences using tape tokens obtained from a tape bank which can be either trainable or derived from input data. We examine the challenges and requirements to obtain dynamic sequ
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;U&#24418;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#23545;&#25968;&#25454;&#30340;&#21033;&#29992;&#26356;&#21152;&#26377;&#25928;&#65292;&#20294;&#19982;&#20154;&#31867;&#23545;&#40784;&#24182;&#38750;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2301.11990</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#25903;&#25345;&#40065;&#26834;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Alignment with human representations supports robust few-shot learning. (arXiv:2301.11990v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11990
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;U&#24418;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#23545;&#25968;&#25454;&#30340;&#21033;&#29992;&#26356;&#21152;&#26377;&#25928;&#65292;&#20294;&#19982;&#20154;&#31867;&#23545;&#40784;&#24182;&#38750;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#20851;&#24515;AI&#31995;&#32479;&#26159;&#21542;&#20855;&#26377;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#19990;&#30028;&#34920;&#24449;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#20998;&#26512;&#65292;&#24314;&#35758;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#29616;&#24230;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#20043;&#38388;&#24212;&#35813;&#23384;&#22312;&#19968;&#20010;U&#24418;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;491&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24615;&#33021;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20010;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#34920;&#26126;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#20110;&#23545;&#25239;&#25915;&#20987;&#21644;&#22495;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20154;&#31867;&#23545;&#40784;&#24448;&#24448;&#26159;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#12289;&#40065;&#26834;&#24615; &#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#30340;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.10886</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21644;&#36866;&#24212;&#24615;&#30340;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;AIRS&#21487;&#20197;&#26681;&#25454;&#23454;&#26102;&#20272;&#35745;&#30340;&#20219;&#21153;&#22238;&#25253;&#20174;&#39044;&#23450;&#20041;&#30340;&#20989;&#25968;&#38598;&#20013;&#36873;&#25321;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#21487;&#38752;&#30340;&#25506;&#32034;&#28608;&#21169;&#24182;&#35299;&#20915;&#20559;&#32622;&#30446;&#26631;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#22810;&#31181;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#30340;&#39640;&#25928;&#21487;&#38752;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;AIRS&#24212;&#29992;&#22312;MiniGrid&#12289;Procgen&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#30340;&#22810;&#39033;&#20219;&#21153;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#22823;&#37327;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;AIRS&#21487;&#20197;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ExaRanker&#30340;&#35299;&#37322;&#22686;&#24378;&#22411;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#21487;&#22312;&#36755;&#20986;&#30456;&#20851;&#24230;&#26631;&#31614;&#19982;&#35299;&#37322;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10521</link><description>&lt;p&gt;
ExaRanker: &#35299;&#37322;&#22686;&#24378;&#22411;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ExaRanker: Explanation-Augmented Neural Ranker. (arXiv:2301.10521v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ExaRanker&#30340;&#35299;&#37322;&#22686;&#24378;&#22411;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#21487;&#22312;&#36755;&#20986;&#30456;&#20851;&#24230;&#26631;&#31614;&#19982;&#35299;&#37322;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36755;&#20986;&#31572;&#26696;&#21069;&#29983;&#25104;&#35299;&#37322;&#26159;&#25552;&#39640;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#20063;&#21463;&#30410;&#20110;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#31561;&#35821;&#35328;&#27169;&#22411;&#26469;&#22686;&#24378;&#20855;&#26377;&#35299;&#37322;&#30340;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#25490;&#24207;&#27169;&#22411;&#65292;&#20197;&#36755;&#20986;&#32473;&#23450;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#30456;&#20851;&#24230;&#26631;&#31614;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#31216;&#20026;ExaRanker&#65292;&#22312;&#20351;&#29992;&#21512;&#25104;&#35299;&#37322;&#30340;&#20960;&#21315;&#20010;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#24615;&#33021;&#19982;&#26080;&#35299;&#37322;&#30340;3&#20493;&#26679;&#26412;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;ExaRanker&#27169;&#22411;&#22312;&#25490;&#24207;&#36807;&#31243;&#20013;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20801;&#35768;&#26681;&#25454;&#38656;&#35201;&#35831;&#27714;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that inducing a large language model (LLM) to generate explanations prior to outputting an answer is an effective strategy to improve performance on a wide range of reasoning tasks. In this work, we show that neural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to augment retrieval datasets with explanations and train a sequence-to-sequence ranking model to output a relevance label and an explanation for a given query-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand examples with synthetic explanations performs on par with models finetuned on 3x more examples without explanations. Furthermore, the ExaRanker model incurs no additional computational cost during ranking and allows explanations to be requested on demand.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;LUMEN&#65292;&#23427;&#39044;&#20808;&#35745;&#31639;&#22823;&#37096;&#20998;&#26816;&#32034;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#23454;&#26102;&#32534;&#30721;&#22120;&#36827;&#34892;&#23436;&#25104;&#32534;&#30721;&#65292;&#30456;&#36739;&#20110;&#32431;&#20869;&#23384;&#21644;FiD&#65292;LUMEN&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19988;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2301.10448</link><description>&lt;p&gt;
&#39044;&#35745;&#31639;&#20869;&#23384;&#25110;&#23454;&#26102;&#32534;&#30721;&#65311;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#28151;&#21512;&#26041;&#27861;&#20351;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#26368;&#22823;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute. (arXiv:2301.10448v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;LUMEN&#65292;&#23427;&#39044;&#20808;&#35745;&#31639;&#22823;&#37096;&#20998;&#26816;&#32034;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#23454;&#26102;&#32534;&#30721;&#22120;&#36827;&#34892;&#23436;&#25104;&#32534;&#30721;&#65292;&#30456;&#36739;&#20110;&#32431;&#20869;&#23384;&#21644;FiD&#65292;LUMEN&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19988;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#35299;&#30721;&#22120;&#20013;&#30340;Fusion&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#35774;&#32622;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#23545;&#22823;&#37327;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#36827;&#34892;&#32534;&#30721;&#65292;&#23427;&#20204;&#20063;&#38750;&#24120;&#26114;&#36149;&#12290;&#19968;&#20123;&#24037;&#20316;&#36890;&#36807;&#23558;&#25991;&#26412;&#35821;&#26009;&#24211;&#39044;&#32534;&#30721;&#20026;&#20869;&#23384;&#65292;&#24182;&#30452;&#25509;&#26816;&#32034;&#23494;&#38598;&#34920;&#31034;&#26469;&#36991;&#20813;&#36825;&#31181;&#25104;&#26412;&#12290;&#20294;&#26159;&#65292;&#39044;&#32534;&#30721;&#20869;&#23384;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36136;&#37327;&#24809;&#32602;&#65292;&#22240;&#20026;&#20869;&#23384;&#34920;&#31034;&#26410;&#38024;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LUMEN&#65292;&#23427;&#26159;&#36825;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#30340;&#28151;&#21512;&#20307;&#65292;&#39044;&#20808;&#35745;&#31639;&#22823;&#37096;&#20998;&#26816;&#32034;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23454;&#26102;&#32534;&#30721;&#22120;&#23436;&#25104;&#32534;&#30721;&#65292;&#35813;&#23454;&#26102;&#32534;&#30721;&#22120;&#26159;&#22522;&#20110;&#38382;&#39064;&#36827;&#34892;&#26465;&#20214;&#21270;&#30340;&#65292;&#24182;&#20026;&#20219;&#21153;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;LUMEN&#26126;&#26174;&#20248;&#20110;&#32431;&#20869;&#23384;&#65292;&#21516;&#26102;&#27604;FiD&#20415;&#23452;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#32473;&#23450;&#30340;&#35745;&#31639;&#36164;&#28304;&#39044;&#31639;&#19979;&#65292;LUMEN&#30340;&#25928;&#26524;&#20248;&#20110;&#20004;&#32773;&#12290;&#27492;&#22806;&#65292;&#24403;&#27169;&#22411;&#35268;&#27169;&#22686;&#22823;&#26102;&#65292;LUMEN&#30456;&#23545;&#20110;FiD&#30340;&#20248;&#21183;&#20063;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models such as Fusion-in-Decoder are powerful, setting the state of the art on a variety of knowledge-intensive tasks. However, they are also expensive, due to the need to encode a large number of retrieved passages. Some work avoids this cost by pre-encoding a text corpus into a memory and retrieving dense representations directly. However, pre-encoding memory incurs a severe quality penalty as the memory representations are not conditioned on the current input. We propose LUMEN, a hybrid between these two extremes, pre-computing the majority of the retrieval representation and completing the encoding on the fly using a live encoder that is conditioned on the question and fine-tuned for the task. We show that LUMEN significantly outperforms pure memory on multiple question-answering tasks while being much cheaper than FiD, and outperforms both for any given compute budget. Moreover, the advantage of LUMEN over FiD increases with model size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26080;&#20154;&#26426;&#35270;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#38567;&#36947;&#29615;&#22659;&#32780;&#26080;&#38656;&#20351;&#29992;&#20808;&#21069;&#30340;&#22320;&#22270;&#65292;&#36890;&#36807;&#20998;&#23618;&#35268;&#21010;&#26041;&#26696;&#23558;&#26816;&#27979;&#38382;&#39064;&#20998;&#35299;&#25104;&#19981;&#21516;&#23618;&#27425;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#33258;&#20027;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2301.08422</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#38567;&#36947;&#24314;&#35774;&#22330;&#22320;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#35270;&#26816;&#27979;&#26694;&#26550;&#65288;&#26356;&#26032;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles. (arXiv:2301.08422v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26080;&#20154;&#26426;&#35270;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#38567;&#36947;&#29615;&#22659;&#32780;&#26080;&#38656;&#20351;&#29992;&#20808;&#21069;&#30340;&#22320;&#22270;&#65292;&#36890;&#36807;&#20998;&#23618;&#35268;&#21010;&#26041;&#26696;&#23558;&#26816;&#27979;&#38382;&#39064;&#20998;&#35299;&#25104;&#19981;&#21516;&#23618;&#27425;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#33258;&#20027;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38075;&#28856;&#27861;&#30340;&#38567;&#36947;&#24314;&#35774;&#38656;&#35201;&#23545;&#25366;&#25496;&#21069;&#37096;&#20301;&#36827;&#34892;&#19977;&#32500;&#27979;&#37327;&#20197;&#35780;&#20272;&#21387;&#30862;&#20301;&#32622;&#12290;&#20026;&#20102;&#32771;&#34385;&#26816;&#27979;&#21644;&#27979;&#37327;&#20219;&#21153;&#30340;&#23433;&#20840;&#12289;&#25104;&#26412;&#21644;&#25928;&#30410;&#65292;&#37096;&#32626;&#36731;&#20415;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#65292;&#22914;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#65292;&#21464;&#24471;&#26356;&#21152;&#24517;&#35201;&#21644;&#26222;&#36941;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#20351;&#29992;&#20808;&#21069;&#30340;&#22320;&#22270;&#36827;&#34892;&#26816;&#39564;&#35270;&#28857;&#30340;&#30830;&#23450;&#24182;&#27809;&#26377;&#32771;&#34385;&#21160;&#24577;&#38556;&#30861;&#29289;&#12290;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#22686;&#21152;&#33258;&#20027;&#27700;&#24179;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26080;&#20154;&#26426;&#35270;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#38567;&#36947;&#29615;&#22659;&#32780;&#26080;&#38656;&#20351;&#29992;&#20808;&#21069;&#30340;&#22320;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20998;&#23618;&#35268;&#21010;&#26041;&#26696;&#23558;&#26816;&#27979;&#38382;&#39064;&#20998;&#35299;&#25104;&#19981;&#21516;&#23618;&#27425;&#12290;&#39640;&#23618;&#20915;&#31574;&#32773;&#39318;&#20808;&#30830;&#23450;&#26426;&#22120;&#20154;&#30340;&#20219;&#21153;&#24182;&#29983;&#25104;&#30446;&#26631;&#28857;&#12290;&#28982;&#21518;&#65292;&#20013;&#23618;&#36335;&#24452;&#35268;&#21010;&#22120;&#25214;&#21040;&#36884;&#24452;&#28857;&#36335;&#24452;&#24182;&#20248;&#21270;&#20813;&#30896;&#25758;&#38745;&#24577;&#36712;&#36857;&#12290;&#26368;&#21518;&#65292;&#38745;&#24577;&#36712;&#36857;&#23558;&#36755;&#20837;&#20302;&#23618;&#25511;&#21046;&#22120;&#20197;&#23454;&#29616;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28436;&#21270;&#36335;&#24452;&#36861;&#36394;&#22120;&#65292;&#36890;&#36807;&#25552;&#20986;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#21644;&#30456;&#24212;&#30340;&#36335;&#24452;&#22270;&#34920;&#24449;&#26089;&#26399;&#20132;&#26131;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#28436;&#21270;&#36335;&#24452;&#32534;&#30721;&#22120; LSTM &#21644;&#28436;&#21270;&#36335;&#24452;&#22270; GCN &#22312;&#28436;&#21270;&#32467;&#26500;&#35774;&#32622;&#19979;&#32534;&#30721;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#21644;&#36335;&#24452;&#22270;&#65292;&#20197;&#21450;&#20998;&#23618;&#29983;&#23384;&#39044;&#27979;&#22120;&#36827;&#19968;&#27493;&#39044;&#27979;&#30446;&#26631;&#22320;&#22336;&#26159;&#21542;&#20250;&#25104;&#20026;&#24694;&#24847;&#22320;&#22336;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#21152;&#23494;&#36135;&#24065;&#20013;&#24694;&#24847;&#22320;&#22336;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.05412</link><description>&lt;p&gt;
&#28436;&#21270;&#36335;&#24452;&#36861;&#36394;&#22120;: &#21152;&#23494;&#36135;&#24065;&#20013;&#24694;&#24847;&#22320;&#22336;&#30340;&#26089;&#26399;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Evolve Path Tracer: Early Detection of Malicious Addresses in Cryptocurrency. (arXiv:2301.05412v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28436;&#21270;&#36335;&#24452;&#36861;&#36394;&#22120;&#65292;&#36890;&#36807;&#25552;&#20986;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#21644;&#30456;&#24212;&#30340;&#36335;&#24452;&#22270;&#34920;&#24449;&#26089;&#26399;&#20132;&#26131;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#28436;&#21270;&#36335;&#24452;&#32534;&#30721;&#22120; LSTM &#21644;&#28436;&#21270;&#36335;&#24452;&#22270; GCN &#22312;&#28436;&#21270;&#32467;&#26500;&#35774;&#32622;&#19979;&#32534;&#30721;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#21644;&#36335;&#24452;&#22270;&#65292;&#20197;&#21450;&#20998;&#23618;&#29983;&#23384;&#39044;&#27979;&#22120;&#36827;&#19968;&#27493;&#39044;&#27979;&#30446;&#26631;&#22320;&#22336;&#26159;&#21542;&#20250;&#25104;&#20026;&#24694;&#24847;&#22320;&#22336;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#21152;&#23494;&#36135;&#24065;&#20013;&#24694;&#24847;&#22320;&#22336;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21152;&#23494;&#36135;&#24065;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#26816;&#27979;&#27450;&#35784;&#34892;&#20026;&#21644;&#30456;&#20851;&#30340;&#24694;&#24847;&#22320;&#22336;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20173;&#28982;&#20381;&#36182;&#20110;&#23436;&#25972;&#30340;&#21382;&#21490;&#20132;&#26131;&#35760;&#24405;&#25110;&#23436;&#25972;&#30340;&#22320;&#22336;&#20132;&#26131;&#32593;&#32476;&#65292;&#22240;&#27492;&#19981;&#33021;&#28385;&#36275;&#26089;&#26399;&#24694;&#24847;&#22320;&#22336;&#26816;&#27979;&#30340;&#35201;&#27714;&#65292;&#36825;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#24456;&#23569;&#35752;&#35770;&#12290;&#20026;&#20102;&#22312;&#26089;&#26399;&#21457;&#29616;&#24694;&#24847;&#22320;&#22336;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28436;&#21270;&#36335;&#24452;&#36861;&#36394;&#22120;&#65292;&#23427;&#30001;&#28436;&#21270;&#36335;&#24452;&#32534;&#30721;&#22120; LSTM&#12289;&#28436;&#21270;&#36335;&#24452;&#22270; GCN &#21644;&#20998;&#23618;&#29983;&#23384;&#39044;&#27979;&#22120;&#32452;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38500;&#20102;&#19968;&#33324;&#30340;&#22320;&#22336;&#29305;&#24449;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#21644;&#30456;&#24212;&#30340;&#36335;&#24452;&#22270;&#20197;&#34920;&#24449;&#26089;&#26399;&#20132;&#26131;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20132;&#26131;&#27169;&#24335;&#22312;&#26089;&#26399;&#38454;&#27573;&#27491;&#22312;&#24555;&#36895;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28436;&#21270;&#36335;&#24452;&#32534;&#30721;&#22120; LSTM &#21644;&#28436;&#21270;&#36335;&#24452;&#22270; GCN &#22312;&#28436;&#21270;&#32467;&#26500;&#35774;&#32622;&#19979;&#32534;&#30721;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#21644;&#36335;&#24452;&#22270;&#12290;&#20998;&#23618;&#29983;&#23384;&#39044;&#27979;&#22120;&#36827;&#19968;&#27493;&#39044;&#27979;&#30446;&#26631;&#22320;&#22336;&#26159;&#21542;&#20250;&#25104;&#20026;&#24694;&#24847;&#22320;&#22336;&#12290;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#28436;&#21270;&#36335;&#24452;&#36861;&#36394;&#22120;&#22312;&#26089;&#26399;&#26816;&#27979;&#24694;&#24847;&#22320;&#22336;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ever-increasing boom of Cryptocurrency, detecting fraudulent behaviors and associated malicious addresses draws significant research effort. However, most existing studies still rely on the full history features or full-fledged address transaction networks, thus cannot meet the requirements of early malicious address detection, which is urgent but seldom discussed by existing studies. To detect fraud behaviors of malicious addresses in the early stage, we present Evolve Path Tracer, which consists of Evolve Path Encoder LSTM, Evolve Path Graph GCN, and Hierarchical Survival Predictor. Specifically, in addition to the general address features, we propose asset transfer paths and corresponding path graphs to characterize early transaction patterns. Further, since the transaction patterns are changing rapidly during the early stage, we propose Evolve Path Encoder LSTM and Evolve Path Graph GCN to encode asset transfer path and path graph under an evolving structure setting. Hiera
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#20013;&#35299;&#31163;&#20851;&#32852;&#24615;&#21644;&#20559;&#24046;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19977;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.13937</link><description>&lt;p&gt;
&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#20013;&#35299;&#31163;&#20851;&#32852;&#24615;&#21644;&#20559;&#24046;&#24615;&#30340;&#36861;&#27714;
&lt;/p&gt;
&lt;p&gt;
Towards Disentangling Relevance and Bias in Unbiased Learning to Rank. (arXiv:2212.13937v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#20013;&#35299;&#31163;&#20851;&#32852;&#24615;&#21644;&#20559;&#24046;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19977;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;(ULTR)&#30740;&#31350;&#30340;&#38382;&#39064;&#22312;&#20110;&#20174;&#38544;&#21547;&#30340;&#29992;&#25143;&#21453;&#39304;&#25968;&#25454;&#65288;&#22914;&#28857;&#20987;&#65289;&#20013;&#20943;&#36731;&#21508;&#31181;&#20559;&#24046;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26368;&#36817;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#19968;&#31181;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#27969;&#34892;ULTR&#26041;&#27861;&#26159;&#20351;&#29992;&#21452;&#22612;&#32467;&#26500;&#65292;&#20854;&#20013;&#23558;&#28857;&#20987;&#24314;&#27169;&#20998;&#35299;&#20026;&#19968;&#20010;&#20855;&#26377;&#24120;&#35268;&#36755;&#20837;&#29305;&#24449;&#30340;&#20851;&#32852;&#22612;&#21644;&#19968;&#20010;&#20855;&#26377;&#20559;&#24046;&#30456;&#20851;&#36755;&#20837;&#65288;&#22914;&#25991;&#20214;&#20301;&#32622;&#65289;&#30340;&#20559;&#24046;&#22612;&#12290;&#25104;&#21151;&#30340;&#20998;&#35299;&#23558;&#20801;&#35768;&#20851;&#32852;&#22612;&#20813;&#21463;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#29616;&#26377;ULTR&#26041;&#27861;&#24573;&#30053;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#8212;&#8212;&#36890;&#36807;&#24213;&#23618;&#30495;&#23454;&#20851;&#32852;&#24615;&#65292;&#20559;&#24046;&#22612;&#21487;&#33021;&#20250;&#19982;&#20851;&#32852;&#22612;&#28151;&#28102;&#12290;&#29305;&#21035;&#26159;&#65292;&#20301;&#32622;&#26159;&#30001;&#35760;&#24405;&#31574;&#30053;&#65288;&#21363;&#20197;&#21069;&#30340;&#29983;&#20135;&#27169;&#22411;&#65289;&#30830;&#23450;&#30340;&#65292;&#23427;&#23558;&#20855;&#26377;&#20851;&#32852;&#20449;&#24687;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#20197;&#23637;&#31034;&#30001;&#20110;&#36825;&#31181;&#30456;&#20851;&#24615;&#23545;&#20110;&#20851;&#32852;&#22612;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unbiased learning to rank (ULTR) studies the problem of mitigating various biases from implicit user feedback data such as clicks, and has been receiving considerable attention recently. A popular ULTR approach for real-world applications uses a two-tower architecture, where click modeling is factorized into a relevance tower with regular input features, and a bias tower with bias-relevant inputs such as the position of a document. A successful factorization will allow the relevance tower to be exempt from biases. In this work, we identify a critical issue that existing ULTR methods ignored - the bias tower can be confounded with the relevance tower via the underlying true relevance. In particular, the positions were determined by the logging policy, i.e., the previous production model, which would possess relevance information. We give both theoretical analysis and empirical results to show the negative effects on relevance tower due to such a correlation. We then propose three method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;Z-ICL&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#21407;&#22987;&#25991;&#26412;&#30340;&#20266;&#28436;&#31034;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#27700;&#24179;&#65292;&#21516;&#26102;&#25903;&#25345;&#26410;&#26469;&#20248;&#21270;&#20266;&#28436;&#31034;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#32780;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2212.09865</link><description>&lt;p&gt;
Z-ICL: &#20351;&#29992;&#20266;&#26679;&#26412;&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35282;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations. (arXiv:2212.09865v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;Z-ICL&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#21407;&#22987;&#25991;&#26412;&#30340;&#20266;&#28436;&#31034;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#27700;&#24179;&#65292;&#21516;&#26102;&#25903;&#25345;&#26410;&#26469;&#20248;&#21270;&#20266;&#28436;&#31034;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#32780;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#20294;&#24403;&#27809;&#26377;&#25552;&#20379;&#28436;&#31034;&#26102;&#65292;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;Z-ICL&#65292;&#22312;&#32473;&#23450;&#27979;&#35797;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21407;&#22987;&#25991;&#26412;&#35821;&#26009;&#24211;&#26500;&#24314;&#20266;&#28436;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#20266;&#28436;&#31034;&#26159;&#36890;&#36807; (1) &#20174;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#19982;&#27979;&#35797;&#36755;&#20837;&#26368;&#30456;&#36817;&#30340;&#37051;&#23621;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#38543;&#26426;&#20219;&#21153;&#26631;&#31614;&#37197;&#23545;&#65292;&#20197;&#21450; (2) &#24212;&#29992;&#19968;&#32452;&#25216;&#26415;&#26469;&#20943;&#23569;&#27169;&#22411;&#20174;&#29983;&#25104;&#30340;&#28436;&#31034;&#20013;&#30452;&#25509;&#22797;&#21046;&#30340;&#25968;&#37327;&#26469;&#26500;&#24314;&#30340;&#12290;&#22312;&#20061;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Z-ICL &#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#19982;&#20351;&#29992;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;Z-ICL &#25552;&#20379;&#20102;&#19968;&#20010;&#26174;&#33879;&#26356;&#39640;&#30340;&#27169;&#22411;&#38646;&#26679;&#26412;&#24615;&#33021;&#27700;&#24179;&#20272;&#35745;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#21162;&#21147;&#21457;&#23637;&#26356;&#22909;&#30340;&#20266;&#28436;&#31034;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#32780;&#26080;&#38656;&#20219;&#20309;&#26377;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying a set of techniques to reduce the amount of direct copying the model does from the resulting demonstrations. Evaluation on nine classification datasets shows that Z-ICL outperforms previous zero-shot methods by a significant margin, and is on par with in-context learning with labeled training data in the few-shot setting. Overall, Z-ICL provides a significantly higher estimate of the zero-shot performance levels of a model, and supports future efforts to develop better pseudo-demonstrations that 
&lt;/p&gt;</description></item><item><title>NusaCrowd&#26159;&#19968;&#20010;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;&#65292;&#24050;&#27719;&#38598;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#65292;&#20026;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#22810;&#31181;&#23454;&#39564;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2212.09648</link><description>&lt;p&gt;
NusaCrowd&#65306;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;
&lt;/p&gt;
&lt;p&gt;
NusaCrowd: Open Source Initiative for Indonesian NLP Resources. (arXiv:2212.09648v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09648
&lt;/p&gt;
&lt;p&gt;
NusaCrowd&#26159;&#19968;&#20010;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;&#65292;&#24050;&#27719;&#38598;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#65292;&#20026;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#22810;&#31181;&#23454;&#39564;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NusaCrowd&#65292;&#36825;&#26159;&#19968;&#20010;&#21327;&#20316;&#20513;&#35758;&#65292;&#26088;&#22312;&#25910;&#38598;&#21644;&#32479;&#19968;&#21360;&#23612;&#35821;&#35328;&#30340;&#29616;&#26377;&#36164;&#28304;&#65292;&#21253;&#25324;&#24320;&#25918;&#20197;&#21069;&#38750;&#20844;&#24320;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#35813;&#20513;&#35758;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#26631;&#20934;&#21270;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#12290;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#24050;&#32463;&#32463;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#23427;&#20204;&#30340;&#20215;&#20540;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;NusaCrowd&#30340;&#25968;&#25454;&#25910;&#38598;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#25512;&#36827;&#23545;&#22312;&#20351;&#29992;&#24191;&#27867;&#30340;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#20174;&#32780;&#20351;&#20043;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments. NusaCrowd's data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;APOLLO&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#30340;Wikipedia&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#33258;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09282</link><description>&lt;p&gt;
APOLLO&#65306;&#38754;&#21521;&#36923;&#36753;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning. (arXiv:2212.09282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;APOLLO&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#30340;Wikipedia&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#33258;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#30340;&#36923;&#36753;&#25512;&#29702;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#29702;&#35299;&#25991;&#26412;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#12289;&#23427;&#20204;&#30340;&#30456;&#20114;&#32852;&#31995;&#65292;&#28982;&#21518;&#36890;&#36807;&#23427;&#20204;&#26469;&#25512;&#26029;&#26032;&#30340;&#32467;&#35770;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;APOLLO&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#25913;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;Wikipedia&#30340;&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22522;&#20110;&#19968;&#32452;&#36923;&#36753;&#25512;&#29702;&#20851;&#38190;&#35789;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#33258;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65306;&#20462;&#25913;&#36807;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#21482;&#23545;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#25512;&#29702;&#32780;&#19981;&#20165;&#20165;&#26159;&#22522;&#26412;&#35821;&#35328;&#29702;&#35299;&#30340;&#29305;&#23450;&#35789;&#24615;&#30340;&#21333;&#35789;&#36827;&#34892;&#25513;&#30721;&#65292;&#20197;&#21450;&#21477;&#23376;&#32423;&#20998;&#31867;&#25439;&#22833;&#65292;&#25945;&#23548;&#27169;&#22411;&#21306;&#20998;&#36923;&#36753;&#19978;&#36830;&#25509;&#21644;&#19981;&#36830;&#25509;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;APOLLO&#22312;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19981;&#25439;&#22833;&#20854;&#22312;&#20854;&#20182;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical reasoning of text is an important ability that requires understanding the information present in the text, their interconnections, and then reasoning through them to infer new conclusions. Prior works on improving the logical reasoning ability of language models require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation solutions that restrict the learning of general logical reasoning skills. In this work, we propose APOLLO, an adaptively pretrained language model that has improved logical reasoning abilities. We select a subset of Wikipedia, based on a set of logical inference keywords, for continued pretraining of a language model. We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence-level classification loss that teaches the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22270;&#23398;&#20064;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#34920;&#31034;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2212.08966</link><description>&lt;p&gt;
&#22270;&#23398;&#20064;&#21450;&#20854;&#24212;&#29992;&#65306;&#19968;&#31687;&#20840;&#38754;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Learning and Its Applications: A Holistic Survey. (arXiv:2212.08966v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22270;&#23398;&#20064;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#34920;&#31034;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive survey of the development and application scenarios of graph learning, with a focus on the remarkable performance of representation learning in various fields such as text, image, chemistry, and biology. It also points out the need to investigate previous valuable works.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20123;&#20851;&#31995;&#20351;&#24471;&#22270;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#25968;&#25454;&#30456;&#27604;&#20855;&#26377;&#29420;&#29305;&#24615;&#65292;&#22240;&#20026;&#33410;&#28857;&#20381;&#36182;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#21253;&#21547;&#20016;&#23500;&#30340;&#20449;&#24687;&#21487;&#20379;&#21033;&#29992;&#12290;&#38543;&#30528;&#34920;&#31034;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#22270;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#22270;&#23398;&#20064;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#30340;&#22823;&#37327;&#20851;&#27880;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#24037;&#20316;&#25552;&#20986;&#20102;&#35299;&#20915;&#22270;&#23398;&#20064;&#20013;&#19981;&#21516;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24847;&#35782;&#21040;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#22312;&#22270;&#23398;&#20064;&#26041;&#38754;&#23436;&#25104;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35843;&#26597;&#65292;&#20294;&#20182;&#20204;&#26410;&#33021;&#20197;&#26356;&#36830;&#36143;&#30340;&#26041;&#24335;&#36830;&#25509;&#30456;&#20851;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning is a prevalent domain that endeavors to learn the intricate relationships among nodes and the topological structure of graphs. These relationships endow graphs with uniqueness compared to conventional tabular data, as nodes rely on non-Euclidean space and encompass rich information to exploit. Over the years, graph learning has transcended from graph theory to graph data mining. With the advent of representation learning, it has attained remarkable performance in diverse scenarios, including text, image, chemistry, and biology. Owing to its extensive application prospects, graph learning attracts copious attention from the academic community. Despite numerous works proposed to tackle different problems in graph learning, there is a demand to survey previous valuable works. While some researchers have perceived this phenomenon and accomplished impressive surveys on graph learning, they failed to connect related objectives, methods, and applications in a more coherent way.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FiDO&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#26356;&#25913;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20869;&#23384;&#24102;&#23485;&#32422;&#26463;&#65292;&#21152;&#24555;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#39046;&#20808;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2212.08153</link><description>&lt;p&gt;
FiDO&#65306;&#38024;&#23545;&#26356;&#24378;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36827;&#34892;&#20248;&#21270;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference. (arXiv:2212.08153v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08153
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FiDO&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#26356;&#25913;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20869;&#23384;&#24102;&#23485;&#32422;&#26463;&#65292;&#21152;&#24555;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#39046;&#20808;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fusion-in-Decoder (FiD)&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#30693;&#35782;&#23494;&#38598;&#22411;NLP&#20219;&#21153;&#19978;&#26641;&#31435;&#20102;&#19994;&#30028;&#26631;&#26438;&#12290;&#20294;&#26159;&#65292;FiD&#25152;&#20351;&#29992;&#30340;&#26550;&#26500;&#26159;&#36890;&#36807;&#23545;&#26631;&#20934;T5&#27169;&#22411;&#20570;&#26368;&#23567;&#20462;&#25913;&#32780;&#36873;&#25321;&#30340;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#36825;&#23545;&#20110;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#26469;&#35828;&#26159;&#39640;&#24230;&#19981;&#20248;&#21270;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;FiD&#23558;&#22823;&#37096;&#20998;FLOPs&#20998;&#37197;&#32473;&#20102;&#32534;&#30721;&#22120;&#65292;&#32780;&#22823;&#22810;&#25968;&#25512;&#29702;&#26102;&#38388;&#26159;&#30001;&#20110;&#35299;&#30721;&#22120;&#20013;&#30340;&#20869;&#23384;&#24102;&#23485;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#31616;&#21333;&#30340;&#26356;&#25913;&#65292;&#20197;&#32531;&#35299;&#20869;&#23384;&#24102;&#23485;&#32422;&#26463;&#65292;&#24182;&#20351;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;7&#20493;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#36866;&#24230;&#30340;&#25104;&#26412;&#20351;&#29992;&#26356;&#22823;&#30340;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#23558;&#32463;&#36807;&#19978;&#36848;&#20462;&#25913;&#30340;FiD&#31216;&#20026;FiDO&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#24191;&#27867;&#30340;&#25512;&#29702;&#39044;&#31639;&#33539;&#22260;&#20869;&#27604;&#29616;&#26377;&#30340;FiD&#27169;&#22411;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;FiDO-Large-XXL&#27604;FiD-Base&#36827;&#34892;&#26356;&#24555;&#30340;&#25512;&#29702;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;FiD-Large&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#27169;&#22411;&#35780;&#20272;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#22312; ImageNet 256x256 &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID &#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.17091</link><description>&lt;p&gt;
&#21033;&#29992;&#37492;&#21035;&#22120;&#24341;&#23548;&#22312;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#23436;&#21892;&#29983;&#25104;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models. (arXiv:2211.17091v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#27169;&#22411;&#35780;&#20272;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#22312; ImageNet 256x256 &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID &#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#26088;&#22312;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#37492;&#21035;&#22120;&#65292;&#26126;&#30830;&#22320;&#30417;&#30563;&#21435;&#22122;&#26679;&#26412;&#36335;&#24452;&#26159;&#21542;&#30495;&#23454;&#12290;&#19982; GAN &#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#32852;&#21512;&#35757;&#32451;&#35780;&#20998;&#21644;&#37492;&#21035;&#22120;&#32593;&#32476;&#12290;&#30456;&#21453;&#65292;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#37492;&#21035;&#22120;&#35757;&#32451;&#31283;&#23450;&#19988;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;&#26679;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#21521;&#39044;&#35757;&#32451;&#30340;&#35780;&#20998;&#28155;&#21152;&#19968;&#20010;&#36741;&#21161;&#39033;&#20197;&#27450;&#39575;&#37492;&#21035;&#22120;&#12290;&#35813;&#39033;&#23558;&#27169;&#22411;&#35780;&#20998;&#30699;&#27491;&#20026;&#26368;&#20248;&#37492;&#21035;&#22120;&#22788;&#30340;&#25968;&#25454;&#35780;&#20998;&#65292;&#36825;&#24847;&#21619;&#30528;&#37492;&#21035;&#22120;&#20197;&#34917;&#20805;&#30340;&#26041;&#24335;&#24110;&#21161;&#26356;&#22909;&#22320;&#35780;&#20272;&#20998;&#25968;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#22312; ImageNet 256x256 &#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID&#65288;1.68&#65289;&#21644;&#21484;&#22238;&#29575;&#65288;0.66&#65289;&#12290;&#25105;&#20204;&#22312; https://github.com/alsdudrla10/DG &#19978;&#20844;&#24320;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26041;&#27861;&#30340;&#29983;&#25104;&#20219;&#24847;&#23610;&#23544;&#30340;&#27491;&#30830;&#26631;&#35760;&#30340;&#38543;&#26426;&#20844;&#24335;&#30340;&#26041;&#27861;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#30446;&#21069;&#22256;&#38590;&#19988;&#24120;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.15368</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#20219;&#24847;&#22823;&#30340;&#26631;&#35760;&#38543;&#26426;&#21487;&#28385;&#36275;&#24615;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Arbitrarily Large Labelled Random Satisfiability Formulas for Machine Learning Training. (arXiv:2211.15368v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26041;&#27861;&#30340;&#29983;&#25104;&#20219;&#24847;&#23610;&#23544;&#30340;&#27491;&#30830;&#26631;&#35760;&#30340;&#38543;&#26426;&#20844;&#24335;&#30340;&#26041;&#27861;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#30446;&#21069;&#22256;&#38590;&#19988;&#24120;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#20013;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#26041;&#21521;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#38382;&#39064;&#19978;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#30340;&#29702;&#35770;&#26680;&#24515;&#24615;&#21644;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#38556;&#30861;&#26159;&#65292;&#35757;&#32451;&#38598;&#20165;&#38480;&#20110;&#27604;&#23454;&#38469;&#24863;&#20852;&#36259;&#30340;&#20844;&#24335;&#23567;&#25968;&#20010;&#25968;&#37327;&#32423;&#30340;&#38543;&#26426;&#20844;&#24335;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#27867;&#21270;&#30340;&#20005;&#37325;&#25285;&#24551;&#65292;&#22240;&#20026;&#26631;&#35760;&#36234;&#26469;&#36234;&#22823;&#30340;&#38543;&#26426;&#20844;&#24335;&#21464;&#24471;&#19981;&#21487;&#35299;&#12290;&#36890;&#36807;&#22522;&#26412;&#24605;&#24819;&#20013;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#25105;&#20204;&#23436;&#20840;&#28040;&#38500;&#20102;&#36825;&#20010;&#38556;&#30861;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#29983;&#25104;&#20219;&#24847;&#25152;&#38656;&#23610;&#23544;&#30340;&#27491;&#30830;&#26631;&#35760;&#30340;&#38543;&#26426;&#20844;&#24335;&#65292;&#32780;&#26080;&#38656;&#35299;&#20915;&#24213;&#23618;&#20915;&#31574;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25913;&#21464;&#31616;&#21333;&#26631;&#37327;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#29983;&#25104;&#30340;&#20844;&#24335;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#26159;&#21487;&#35843;&#30340;&#12290;&#36825;&#25171;&#24320;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#22797;&#26434;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying deep learning to solve real-life instances of hard combinatorial problems has tremendous potential. Research in this direction has focused on the Boolean satisfiability (SAT) problem, both because of its theoretical centrality and practical importance. A major roadblock faced, though, is that training sets are restricted to random formulas of size several orders of magnitude smaller than formulas of practical interest, raising serious concerns about generalization. This is because labeling random formulas of increasing size rapidly becomes intractable. By exploiting the probabilistic method in a fundamental way, we remove this roadblock entirely: we show how to generate correctly labeled random formulas of any desired size, without having to solve the underlying decision problem. Moreover, the difficulty of the classification task for the formulas produced by our generator is tunable by varying a simple scalar parameter. This opens up an entirely new level of sophistication fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#38750;&#23545;&#31216;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#36755;&#20837;&#24182;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#27169;&#22411;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11255</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#21435;&#22122;&#36807;&#31243;&#30340;&#24863;&#30693;&#22120;&#20559;&#32622;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion Denoising Process for Perceptron Bias in Out-of-distribution Detection. (arXiv:2211.11255v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#38750;&#23545;&#31216;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#36755;&#20837;&#24182;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#27169;&#22411;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#20445;&#35777;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#21028;&#21035;&#22120;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21028;&#21035;&#22120;&#27169;&#22411;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#23481;&#26131;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#65292;&#30041;&#19979;&#19981;&#33391;&#24773;&#20917;&#21644;&#24694;&#24847;&#25915;&#20987;&#30340;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24863;&#30693;&#22120;&#20559;&#32622;&#20551;&#35774;&#65292;&#23427;&#34920;&#26126;&#21028;&#21035;&#22120;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#26576;&#20123;&#29305;&#24449;&#26356;&#20026;&#25935;&#24863;&#65292;&#23548;&#33268;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#21028;&#21035;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23558;&#25193;&#25955;&#27169;&#22411;(DMs)&#38598;&#25104;&#21040;OOD&#26816;&#27979;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#25955;&#21435;&#22122;&#36807;&#31243;(DDP)&#20316;&#20026;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#38750;&#23545;&#31216;&#25554;&#20540;&#65292;&#24456;&#36866;&#21512;&#22686;&#24378;&#36755;&#20837;&#24182;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#22312;DDP&#19979;&#65292;OOD&#25968;&#25454;&#30340;&#21028;&#21035;&#22120;&#27169;&#22411;&#29305;&#24449;&#34920;&#29616;&#20026;&#23574;&#38160;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#21033;&#29992;&#33539;&#25968;...
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a crucial task for ensuring the reliability and safety of deep learning. Currently, discriminator models outperform other methods in this regard. However, the feature extraction process used by discriminator models suffers from the loss of critical information, leaving room for bad cases and malicious attacks. In this paper, we introduce a new perceptron bias assumption that suggests discriminator models are more sensitive to certain features of the input, leading to the overconfidence problem. To address this issue, we propose a novel framework that combines discriminator and generation models and integrates diffusion models (DMs) into OOD detection. We demonstrate that the diffusion denoising process (DDP) of DMs serves as a novel form of asymmetric interpolation, which is well-suited to enhance the input and mitigate the overconfidence problem. The discriminator model features of OOD data exhibit sharp changes under DDP, and we utilize the norm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CausalCF&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23436;&#25972;&#30340;&#22240;&#26524;RL&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#36890;&#36807;&#22240;&#26524;&#21453;&#20107;&#23454;&#25512;&#26029;&#25552;&#39640;RL&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#24050;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#32437;&#20219;&#21153;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.05551</link><description>&lt;p&gt;
&#29992;&#22240;&#26524;&#21453;&#20107;&#23454;&#25512;&#26029;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Counterfactuals for Improving the Robustness of Reinforcement Learning. (arXiv:2211.05551v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CausalCF&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23436;&#25972;&#30340;&#22240;&#26524;RL&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#36890;&#36807;&#22240;&#26524;&#21453;&#20107;&#23454;&#25512;&#26029;&#25552;&#39640;RL&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#24050;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#32437;&#20219;&#21153;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#34987;&#20351;&#29992;&#12290;&#24378;&#21270;&#23398;&#20064;&#20351;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#33258;&#20027;&#22320;&#23398;&#20064;&#20219;&#21153;&#12290;&#20219;&#21153;&#36234;&#37325;&#35201;&#65292;&#23545;RL&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#30340;&#38656;&#27714;&#23601;&#36234;&#39640;&#12290;&#22240;&#26524;RL&#23558;RL&#21644;&#22240;&#26524;&#25512;&#26029;&#30456;&#32467;&#21512;&#65292;&#20351;RL&#26356;&#21152;&#40065;&#26834;&#12290;&#22240;&#26524;RL&#20195;&#29702;&#20351;&#29992;&#22240;&#26524;&#34920;&#31034;&#26469;&#25429;&#25417;&#21487;&#20197;&#20174;&#19968;&#20010;&#20219;&#21153;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#19981;&#21464;&#22240;&#26524;&#26426;&#21046;&#12290;&#30446;&#21069;&#65292;&#22240;&#26524;RL&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#19981;&#23436;&#25972;&#25110;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CausalCF&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23436;&#25972;&#30340;&#22240;&#26524;RL&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;Causal Curiosity&#21644;CoPhy&#30340;&#24605;&#24819;&#12290;Causal Curiosity&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#24178;&#39044;&#30340;&#26041;&#27861;&#65292;&#24182;&#20462;&#25913;&#20102;CoPhy&#65292;&#20351;RL&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#21453;&#20107;&#23454;&#25512;&#26029;&#12290;Causal Curiosity&#24050;&#24212;&#29992;&#20110;CausalWorld&#20013;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#32437;&#20219;&#21153;&#12290;CausalWorld&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#20223;&#30495;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is used in various robotic applications. RL enables agents to learn tasks autonomously by interacting with the environment. The more critical the tasks are, the higher the demand for the robustness of the RL systems. Causal RL combines RL and causal inference to make RL more robust. Causal RL agents use a causal representation to capture the invariant causal mechanisms that can be transferred from one task to another. Currently, there is limited research in Causal RL, and existing solutions are usually not complete or feasible for real-world applications. In this work, we propose CausalCF, the first complete Causal RL solution incorporating ideas from Causal Curiosity and CoPhy. Causal Curiosity provides an approach for using interventions, and CoPhy is modified to enable the RL agent to perform counterfactuals. Causal Curiosity has been applied to robotic grasping and manipulation tasks in CausalWorld. CausalWorld provides a realistic simulation environment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#21644;GAN&#30340;ECG&#20449;&#21495;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;ECG&#20449;&#21495;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#38382;&#39064;&#12290;&#20351;&#29992;&#26469;&#33258;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#29983;&#25104;&#30340;&#20449;&#21495;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GAN&#30340;&#29983;&#25104;&#22522;&#32447;&#26356;&#20026;&#36924;&#30495;&#65292;&#23545;&#20110;&#25552;&#39640;ECG&#35757;&#32451;&#25968;&#25454;&#38598;&#36136;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21487;&#25552;&#39640;ECG&#20998;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02626</link><description>&lt;p&gt;
&#22522;&#20110;GAN&#30340;&#24515;&#30005;&#22270;&#21512;&#25104;&#20013;&#21033;&#29992;&#32479;&#35745;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Statistical Shape Priors in GAN-based ECG Synthesis. (arXiv:2211.02626v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#21644;GAN&#30340;ECG&#20449;&#21495;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;ECG&#20449;&#21495;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#38382;&#39064;&#12290;&#20351;&#29992;&#26469;&#33258;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#29983;&#25104;&#30340;&#20449;&#21495;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GAN&#30340;&#29983;&#25104;&#22522;&#32447;&#26356;&#20026;&#36924;&#30495;&#65292;&#23545;&#20110;&#25552;&#39640;ECG&#35757;&#32451;&#25968;&#25454;&#38598;&#36136;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21487;&#25552;&#39640;ECG&#20998;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#36827;&#34892;&#24515;&#30005;&#22270;(ECG)&#25968;&#25454;&#25910;&#38598;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;,&#22240;&#27492;&#24515;&#30005;&#22270;&#25968;&#25454;&#21512;&#25104;&#26159;&#24212;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;ECG&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#21644;&#32479;&#35745;ECG&#25968;&#25454;&#24314;&#27169;&#26469;&#29983;&#25104;ECG&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;ECG&#21160;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#21512;&#25104;&#36924;&#30495;&#30340;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;ECG&#20449;&#21495;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#38382;&#39064;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;ECG&#20449;&#21495;&#30340;&#26102;&#38388;&#21644;&#24133;&#24230;&#21464;&#21270;&#24314;&#27169;&#20026;2-D&#24418;&#29366;&#65292;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GAN&#30340;&#29983;&#25104;&#22522;&#32447;&#65292;&#29983;&#25104;&#30340;&#20449;&#21495;&#26356;&#36924;&#30495;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#25552;&#39640;ECG&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#26368;&#32456;&#21487;&#20197;&#25552;&#39640;ECG&#20998;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) data collection during emergency situations is challenging, making ECG data generation an efficient solution for dealing with highly imbalanced ECG training datasets. In this paper, we propose a novel approach for ECG signal generation using Generative Adversarial Networks (GANs) and statistical ECG data modeling. Our approach leverages prior knowledge about ECG dynamics to synthesize realistic signals, addressing the complex dynamics of ECG signals. To validate our approach, we conducted experiments using ECG signals from the MIT-BIH arrhythmia database. Our results demonstrate that our approach, which models temporal and amplitude variations of ECG signals as 2-D shapes, generates more realistic signals compared to state-of-the-art GAN based generation baselines. Our proposed approach has significant implications for improving the quality of ECG training datasets, which can ultimately lead to better performance of ECG classification algorithms. This research c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#24182;&#20419;&#36827;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01842</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#30340;&#20998;&#23618;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars. (arXiv:2211.01842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#24182;&#20419;&#36827;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31616;&#21333;&#30340;&#26500;&#24314;&#22359;&#20013;&#21457;&#29616;&#31070;&#32463;&#32467;&#26500;&#26159;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#27493;&#39588;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#24120;&#20165;&#25628;&#32034;&#19968;&#20123;&#38480;&#23450;&#26041;&#38754;&#30340;&#26550;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#30340;&#32479;&#19968;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#33258;&#28982;&#32780;&#32039;&#20945;&#22320;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#27604;&#25991;&#29486;&#20013;&#24120;&#35265;&#30340;&#31354;&#38388;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36890;&#36807;&#22686;&#24378;&#21644;&#21033;&#29992;&#23427;&#20204;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#65292;&#24182;&#20419;&#36827;&#20102;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#23618;&#26680;&#35774;&#35745;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#39640;&#25928;&#25628;&#32034;&#22914;&#27492;&#24222;&#22823;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#25628;&#32034;&#31574;&#30053;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#30340;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#22788;&#29702;&#31639;&#27861;&#65292;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#25512;&#23548;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36798;&#21040;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#32676;&#20307;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.01528</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#22788;&#29702;&#23454;&#29616;&#20844;&#24179;&#21644;&#26368;&#20248;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fair and Optimal Classification via Post-Processing. (arXiv:2211.01528v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#22788;&#29702;&#31639;&#27861;&#65292;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#25512;&#23548;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36798;&#21040;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#32676;&#20307;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#21576;&#29616;&#30340;&#20559;&#35265;&#65292;&#20844;&#24179;&#24615;&#26631;&#20934;&#21487;&#20197;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;&#22312;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#20013;&#23454;&#29616;&#20844;&#24179;&#23545;&#24453;&#65292;&#28982;&#32780;&#36825;&#24448;&#24448;&#26159;&#20197;&#27169;&#22411;&#34920;&#29616;&#20026;&#20195;&#20215;&#30340;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#36825;&#31181;&#26435;&#34913;&#26159;&#20844;&#24179;&#31639;&#27861;&#35774;&#35745;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#22312;&#26368;&#26222;&#36941;&#30340;&#22810;&#32452;&#12289;&#22810;&#31867;&#21035;&#21644;&#22024;&#26434;&#35774;&#32622;&#19979;&#65292;&#23436;&#25972;&#22320;&#34920;&#24449;&#20102;&#20844;&#24179;&#12289;&#36798;&#25705;&#23572;&#24179;&#31561;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20869;&#22312;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#21644;&#23646;&#24615;&#24863;&#30693;&#20844;&#24179;&#20998;&#31867;&#22120;&#23454;&#29616;&#30340;&#26368;&#23567;&#38169;&#35823;&#29575;&#26159;&#30001;&#27779;&#29791;&#26031;&#22374;&#37325;&#24515;&#38382;&#39064;&#30340;&#26368;&#20248;&#20540;&#32473;&#20986;&#30340;&#12290;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#65292;&#20174;&#35780;&#20998;&#20989;&#25968;&#20013;&#25512;&#23548;&#20986;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#35780;&#20998;&#20026;&#36125;&#21494;&#26031;&#26368;&#20248;&#26102;&#24471;&#21040;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#27425;&#20248;&#24615;&#20998;&#26512;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the bias exhibited by machine learning models, fairness criteria can be integrated into the training process to ensure fair treatment across all demographics, but it often comes at the expense of model performance. Understanding such tradeoffs, therefore, underlies the design of fair algorithms. To this end, this paper provides a complete characterization of the inherent tradeoff of demographic parity on classification problems, under the most general multi-group, multi-class, and noisy setting. Specifically, we show that the minimum error rate achievable by randomized and attribute-aware fair classifiers is given by the optimal value of a Wasserstein-barycenter problem. On the practical side, our findings lead to a simple post-processing algorithm that derives fair classifiers from score functions, which yields the optimal fair classifier when the score is Bayes optimal. We provide suboptimality analysis and sample complexity for our algorithm, and demonstrate its effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MMPLM&#65289;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#20013;&#25104;&#21151;&#24212;&#29992;&#20110;&#23436;&#20840;&#26410;&#30693;&#30340;&#35821;&#35328;&#23545;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;fine-tune&#36807;&#30340;MMPLM&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.06068</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning. (arXiv:2210.06068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MMPLM&#65289;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#20013;&#25104;&#21151;&#24212;&#29992;&#20110;&#23436;&#20840;&#26410;&#30693;&#30340;&#35821;&#35328;&#23545;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;fine-tune&#36807;&#30340;MMPLM&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MMPLM&#65289;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#30340;&#36229;&#33021;&#21147;&#21644;&#39044;&#30693;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;MMPLM&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20013;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#29992;&#20110;&#23436;&#20840;&#26410;&#30693;&#30340;&#35821;&#35328;&#23545;&#12290;&#25105;&#20204;&#20351;&#29992;Meta-AI&#30340;MMPLM&#8220;wmt21-dense-24-wide-en-X&#21644;X-en&#65288;WMT21fb&#65289;&#8221;&#65292;&#36825;&#20123;&#27169;&#22411;&#39044;&#20808;&#35757;&#32451;&#20102;7&#31181;&#35821;&#35328;&#23545;&#21644;14&#20010;&#32763;&#35793;&#26041;&#21521;&#65292;&#21253;&#25324;&#33521;&#35821;&#21040;&#25463;&#20811;&#35821;&#12289;&#24503;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#20912;&#23707;&#35821;&#12289;&#26085;&#35821;&#12289;&#20420;&#35821;&#21644;&#27721;&#35821;&#20197;&#21450;&#30456;&#21453;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;MMPLM&#36827;&#34892;fine-tune&#65292;&#38024;&#23545;&#23427;&#20204;&#21407;&#22987;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;\textit{&#23436;&#20840;&#19981;&#23384;&#22312;}&#30340;&#33521;&#25991;-\textit{&#35199;&#29677;&#29273;&#35821;}&#35821;&#35328;&#23545;&#65292;&#26174;&#24335;&#21644;&#38544;&#24335;&#22320;&#36827;&#34892;fine-tune&#12290;&#25105;&#20204;&#20026;&#27492; fine-tune &#20570;&#22909;&#32463;&#36807;&#20180;&#32454;&#23545;&#40784;&#30340;\textit{&#20020;&#24202;}&#39046;&#22495;&#25968;&#25454;&#65292;&#36825;&#19982;&#23427;&#20204;&#21407;&#22987;&#30340;&#28151;&#21512;&#39046;&#22495;&#30693;&#35782;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;fine-tune&#36807;&#30340;MMPLM&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. This work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning. We carry out an experimental investigation using Meta-AI's MMPLMs ``wmt21-dense-24-wide-en-X and X-en (WMT21fb)'' which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction. We fine-tune these MMPLMs towards English-\textit{Spanish} language pair which \textit{did not exist at all} in their original pre-trained corpora both implicitly and explicitly. We prepare carefully aligned \textit{clinical} domain data for this fine-tuning, which is different from their original mixed domain knowledge. Our experimental result shows that the fine-tunin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#33324;&#29702;&#35770;&#26469;&#38480;&#23450;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05015</link><description>&lt;p&gt;
&#31890;&#23376;&#20449;&#24565;&#36817;&#20284;POMDP&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimality Guarantees for Particle Belief Approximation of POMDPs. (arXiv:2210.05015v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#33324;&#29702;&#35770;&#26469;&#38480;&#23450;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#25552;&#20379;&#20102;&#29616;&#23454;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#28789;&#27963;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;POMDP&#30340;&#27714;&#35299;&#38750;&#24120;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#24403;&#29366;&#24577;&#21644;&#35266;&#27979;&#31354;&#38388;&#26159;&#36830;&#32493;&#25110;&#28151;&#21512;&#30340;&#26102;&#20505;&#65292;&#36825;&#22312;&#29289;&#29702;&#31995;&#32479;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#23613;&#31649;&#26368;&#36817;&#20351;&#29992;&#35266;&#27979;&#20284;&#28982;&#26435;&#37325;&#31574;&#21010;&#30340;&#22312;&#32447;&#37319;&#26679;POMDP&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#23454;&#29992;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20808;&#21069;&#24182;&#27809;&#26377;&#25552;&#20986;&#19968;&#33324;&#29702;&#35770;&#26469;&#21051;&#30011;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#31890;&#23376;&#28388;&#27874;&#25216;&#26415;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#38480;&#23450;&#20219;&#20309;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#36825;&#31181;PB-MDP&#21644;POMDP&#20043;&#38388;&#30340;&#22522;&#30784;&#26725;&#26753;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#30456;&#24212;&#30340;&#31890;&#23376;&#20449;&#24565;MDP&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#23558;MDP&#31639;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#25193;&#23637;&#21040;POMDP&#20013;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#22312;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) provide a flexible representation for real-world decision and control problems. However, POMDPs are notoriously difficult to solve, especially when the state and observation spaces are continuous or hybrid, which is often the case for physical systems. While recent online sampling-based POMDP algorithms that plan with observation likelihood weighting have shown practical effectiveness, a general theory characterizing the approximation error of the particle filtering techniques that these algorithms use has not previously been proposed. Our main contribution is bounding the error between any POMDP and its corresponding finite sample particle belief MDP (PB-MDP) approximation. This fundamental bridge between PB-MDPs and POMDPs allows us to adapt any sampling-based MDP algorithm to a POMDP by solving the corresponding particle belief MDP, thereby extending the convergence guarantees of the MDP algorithm to the POMDP. Practically, thi
&lt;/p&gt;</description></item><item><title>UCEpic&#36890;&#36807;&#23558;&#26041;&#38754;&#35268;&#21010;&#21644;&#35789;&#27719;&#32422;&#26463;&#32479;&#19968;&#32771;&#34385;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#29983;&#25104;&#30340;&#20010;&#24615;&#21270;&#35299;&#37322;&#22411;&#25512;&#33616;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#21305;&#37197;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.13885</link><description>&lt;p&gt;
UCEpic&#65306;&#32479;&#19968;&#32771;&#34385;&#26041;&#38754;&#35268;&#21010;&#21644;&#35789;&#27719;&#32422;&#26463;&#65292;&#29983;&#25104;&#35299;&#37322;&#22411;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
UCEpic: Unifying Aspect Planning and Lexical Constraints for Generating Explanations in Recommendation. (arXiv:2209.13885v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13885
&lt;/p&gt;
&lt;p&gt;
UCEpic&#36890;&#36807;&#23558;&#26041;&#38754;&#35268;&#21010;&#21644;&#35789;&#27719;&#32422;&#26463;&#32479;&#19968;&#32771;&#34385;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#29983;&#25104;&#30340;&#20010;&#24615;&#21270;&#35299;&#37322;&#22411;&#25512;&#33616;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#21305;&#37197;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#35299;&#37322;&#22411;&#25512;&#33616;&#36827;&#34892;&#20010;&#24615;&#21270;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26041;&#38754;&#35268;&#21010;&#21644;&#35789;&#27719;&#32422;&#26463;&#32479;&#19968;&#32771;&#34385;&#30340;&#27169;&#22411;UCEpic&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#29983;&#25104;&#30340;&#26041;&#24335;&#29983;&#25104;&#39640;&#36136;&#37327;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#32467;&#26524;&#35299;&#37322;&#12290;&#36890;&#36807;&#22312;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#29983;&#25104;&#36807;&#31243;&#20013;&#24341;&#20837;&#35789;&#27719;&#32422;&#26463;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#35299;&#37322;&#30340;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#21305;&#37197;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UCEpic&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized natural language generation for explainable recommendations plays a key role in justifying why a recommendation might match a user's interests. Existing models usually control the generation process by aspect planning. While promising, these aspect-planning methods struggle to generate specific information correctly, which prevents generated explanations from being convincing. In this paper, we claim that introducing lexical constraints can alleviate the above issues. We propose a model, UCEpic, that generates high-quality personalized explanations for recommendation results by unifying aspect planning and lexical constraints in an insertion-based generation manner.  Methodologically, to ensure text generation quality and robustness to various lexical constraints, we pre-train a non-personalized text generator via our proposed robust insertion process. Then, to obtain personalized explanations under this framework of insertion-based generation, we design a method of incorp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.00755</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#27493; Q-learning &#32531;&#35299; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#31163;&#31574;&#30053;&#20559;&#24046;&#65306;&#19968;&#31181;&#26032;&#30340;&#32416;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach. (arXiv:2208.00755v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#31163;&#31574;&#30053;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#25968;&#25454;&#20351;&#29992;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#29702;&#30340;&#31574;&#30053;&#21644;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#30340;&#22522;&#26412;&#20998;&#24067;&#20043;&#38388;&#30340;&#20559;&#24046;&#22686;&#21152;&#26102;&#65292;&#31163;&#31574;&#30053;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#30740;&#31350;&#20102;&#37325;&#35201;&#24615;&#37319;&#26679;&#21644;&#31163;&#31574;&#30053;&#31574;&#30053;&#26799;&#24230;&#25216;&#26415;&#26469;&#34917;&#20607;&#36825;&#31181;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#19968;&#31995;&#21015;&#38271;&#36712;&#36857;&#65292;&#24182;&#23548;&#33268;&#39069;&#22806;&#30340;&#38382;&#39064;&#65292;&#22914;&#28040;&#22833;/&#29190;&#28856;&#26799;&#24230;&#25110;&#25243;&#24323;&#35768;&#22810;&#26377;&#29992;&#30340;&#32463;&#39564;&#65292;&#26368;&#32456;&#22686;&#21152;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23545;&#36830;&#32493;&#21160;&#20316;&#22495;&#25110;&#30001;&#30830;&#23450;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#30340;&#31574;&#30053;&#30340;&#27867;&#21270;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#36830;&#32493;&#25511;&#21046;&#20013;&#36825;&#31181;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#36731; Actor-Critic &#26041;&#27861;&#20013;&#31163;&#25919;&#31574;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can improve data efficiency by repeatedly using the previously gathered data. However, off-policy learning becomes challenging when the discrepancy between the underlying distributions of the agent's policy and collected data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories and induce additional problems such as vanishing/exploding gradients or discarding many useful experiences, which eventually increases the computational complexity. Moreover, their generalization to either continuous action domains or policies approximated by deterministic deep neural networks is strictly limited. To overcome these limitations, we introduce a novel policy similarity measure to mitigate the effects of such discrepancy in continuous control. Our method offers an ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36807;&#21435;&#38382;&#39064;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#26469;&#36805;&#36895;&#39044;&#27979;&#21644;&#35299;&#20915;&#26032;&#38382;&#39064;&#65292;&#37325;&#22797;&#22320;&#35299;&#20915;&#19981;&#21516;&#24230;&#37327;&#20043;&#38388;&#30340;&#31867;&#20284;OT&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2206.05262</link><description>&lt;p&gt;
&#20803;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Meta Optimal Transport. (arXiv:2206.05262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36807;&#21435;&#38382;&#39064;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#26469;&#36805;&#36895;&#39044;&#27979;&#21644;&#35299;&#20915;&#26032;&#38382;&#39064;&#65292;&#37325;&#22797;&#22320;&#35299;&#20915;&#19981;&#21516;&#24230;&#37327;&#20043;&#38388;&#30340;&#31867;&#20284;OT&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20998;&#25674;&#20248;&#21270;&#26469;&#39044;&#27979;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20803;OT&#12290;&#36825;&#26377;&#21161;&#20110;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#38382;&#39064;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#26469;&#36805;&#36895;&#39044;&#27979;&#21644;&#35299;&#20915;&#26032;&#38382;&#39064;&#65292;&#20174;&#32780;&#37325;&#22797;&#22320;&#35299;&#20915;&#19981;&#21516;&#24230;&#37327;&#20043;&#38388;&#30340;&#31867;&#20284;OT&#38382;&#39064;&#12290;&#21542;&#21017;&#65292;&#26631;&#20934;&#26041;&#27861;&#20250;&#24573;&#30053;&#36807;&#21435;&#35299;&#20915;&#26041;&#26696;&#30340;&#30693;&#35782;&#65292;&#20174;&#22836;&#24320;&#22987;&#27425;&#20248;&#22320;&#37325;&#26032;&#35299;&#20915;&#27599;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#28784;&#24230;&#22270;&#20687;&#12289;&#29699;&#24418;&#25968;&#25454;&#12289;&#20998;&#31867;&#26631;&#31614;&#21644;&#39068;&#33394;&#35843;&#33394;&#26495;&#20043;&#38388;&#23454;&#20363;&#21270;&#20803;OT&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#25913;&#21892;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;&#27492;http URL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the use of amortized optimization to predict optimal transport (OT) maps from the input measures, which we call Meta OT. This helps repeatedly solve similar OT problems between different measures by leveraging the knowledge and information present from past problems to rapidly predict and solve new problems. Otherwise, standard methods ignore the knowledge of the past solutions and suboptimally re-solve each problem from scratch. We instantiate Meta OT models in discrete and continuous settings between grayscale images, spherical data, classification labels, and color palettes and use them to improve the computational time of standard OT solvers. Our source code is available at this http URL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#20316;&#22122;&#22768;&#31867;&#22411;&#12289;&#22122;&#22768;&#27604;&#20363;&#21644;&#20943;&#23569;&#35268;&#27169;&#22240;&#23376;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#20013;&#31574;&#30053;&#24615;&#33021;&#21644;&#25506;&#32034;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.03787</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#21160;&#20316;&#22122;&#22768;&#23545;&#25506;&#32034;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance. (arXiv:2206.03787v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#20316;&#22122;&#22768;&#31867;&#22411;&#12289;&#22122;&#22768;&#27604;&#20363;&#21644;&#20943;&#23569;&#35268;&#27169;&#22240;&#23376;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#20013;&#31574;&#30053;&#24615;&#33021;&#21644;&#25506;&#32034;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#25506;&#32034;&#24418;&#24335;&#65292;&#22914;&#36830;&#32493;&#25511;&#21046;&#39046;&#22495;&#24120;&#29992;&#30340;&#21152;&#24615;&#21160;&#20316;&#22122;&#22768;&#12290;&#36890;&#24120;&#65292;&#22312;&#22521;&#35757;&#26399;&#38388;&#20445;&#25345;&#21160;&#20316;&#22122;&#22768;&#30340;&#27604;&#20363;&#19981;&#21464;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#36830;&#32493;&#25511;&#21046;&#30340;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#20316;&#22122;&#22768;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22122;&#22768;&#31867;&#22411;&#12289;&#22122;&#22768;&#27604;&#20363;&#21644;&#20943;&#23569;&#35268;&#27169;&#22240;&#23376;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#26368;&#24120;&#35265;&#30340;&#21160;&#20316;&#22122;&#22768;&#31867;&#22411;&#65292;&#39640;&#26031;&#22122;&#22768;&#21644; Ornstein-Uhlenbeck &#22122;&#22768;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#22320;&#25913;&#21464;&#22122;&#22768;&#31867;&#22411;&#21644;&#27604;&#20363;&#21442;&#25968;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#27979;&#37327;&#31574;&#30053;&#30340;&#39044;&#26399;&#22238;&#25253;&#21644;&#25506;&#32034;&#26399;&#38388;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#31561;&#26377;&#36259;&#30340;&#21464;&#37327;&#26469;&#35780;&#20272;&#32467;&#26524;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#24230;&#37327;X_&#119984;rel&#65292;&#35813;&#26041;&#27861;&#23545;&#20272;&#35745;&#24341;&#36215;&#30340;&#20272;&#35745;&#35823;&#24046;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms of exploration such as the additive action noise often used in continuous control domains. Typically, the scaling factor of this action noise is chosen as a hyper-parameter and is kept constant during training. In this paper, we focus on action noise in off-policy deep reinforcement learning for continuous control. We analyze how the learned policy is impacted by the noise type, noise scale, and impact scaling factor reduction schedule. We consider the two most prominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, and perform a vast experimental campaign by systematically varying the noise type and scale parameter, and by measuring variables of interest like the expected return of the policy and the state-space coverage during exploration. For the latter, we propose a novel state-space coverage measure $\operatorname{X}_{\mathcal{U}\text{rel}}$ that is more robust to estimation artifacts caused by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#29366;&#24577;&#26426;&#65288;RM&#65289;&#30340;&#23618;&#27425;&#21270;&#32467;&#26500;&#65288;HRM&#65289;&#65292;&#21033;&#29992;&#23427;&#21487;&#20197;&#23558;&#20219;&#21153;&#36827;&#19968;&#27493;&#25277;&#35937;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#37117;&#21487;&#20197;&#29420;&#31435;&#35299;&#20915;&#65307;&#20351;&#29992; HRM &#21487;&#20197;&#24110;&#21161;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#19988;&#22312;&#23398;&#20064;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2205.15752</link><description>&lt;p&gt;
&#22870;&#21169;&#29366;&#24577;&#26426;&#30340;&#23618;&#27425;&#21270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hierarchies of Reward Machines. (arXiv:2205.15752v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#29366;&#24577;&#26426;&#65288;RM&#65289;&#30340;&#23618;&#27425;&#21270;&#32467;&#26500;&#65288;HRM&#65289;&#65292;&#21033;&#29992;&#23427;&#21487;&#20197;&#23558;&#20219;&#21153;&#36827;&#19968;&#27493;&#25277;&#35937;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#37117;&#21487;&#20197;&#29420;&#31435;&#35299;&#20915;&#65307;&#20351;&#29992; HRM &#21487;&#20197;&#24110;&#21161;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#19988;&#22312;&#23398;&#20064;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#29366;&#24577;&#26426;&#65288;RM&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#24418;&#24335;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#36890;&#36807;&#19968;&#20010;&#26377;&#38480;&#29366;&#24577;&#26426;&#26469;&#34920;&#31034;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#36793;&#32536;&#20351;&#29992;&#39640;&#32423;&#20107;&#20214;&#32534;&#30721;&#20219;&#21153;&#30340;&#23376;&#30446;&#26631;&#12290; RM&#30340;&#32467;&#26500;&#20351;&#24471;&#23558;&#19968;&#20010;&#20219;&#21153;&#20998;&#35299;&#25104;&#31616;&#21333;&#21644;&#29420;&#31435;&#21487;&#35299;&#30340;&#23376;&#20219;&#21153;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#26377;&#21161;&#20110;&#22788;&#29702;&#38271;&#26399;&#35268;&#21010;&#21644;/&#25110;&#22870;&#21169;&#31232;&#30095;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#36171;&#20104;RM&#35843;&#29992;&#20854;&#20182;RM&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#32452;&#21512;&#19968;&#20010;RM&#30340;&#23618;&#27425;&#32467;&#26500;&#65288;HRM&#65289;&#26469;&#36827;&#19968;&#27493;&#25277;&#35937;&#23376;&#20219;&#21153;&#32467;&#26500;&#12290;&#25105;&#20204;&#21033;&#29992;HRM&#36890;&#36807;&#23558;&#23545;RM&#30340;&#27599;&#20010;&#35843;&#29992;&#35270;&#20026;&#21333;&#29420;&#21487;&#35299;&#30340;&#23376;&#20219;&#21153;&#26469;&#20351;&#29992;&#36873;&#39033;&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#30340;&#26041;&#27861;&#26469;&#20174;&#20195;&#29702;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20013;&#23398;&#20064;HRM&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;HRM&#27604;&#25153;&#24179;&#30340;HRM&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#22312;&#31561;&#20215;&#30340;&#25153;&#24179;&#34920;&#31034;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;HRM&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward machines (RMs) are a recent formalism for representing the reward function of a reinforcement learning task through a finite-state machine whose edges encode subgoals of the task using high-level events. The structure of RMs enables the decomposition of a task into simpler and independently solvable subtasks that help tackle long-horizon and/or sparse reward tasks. We propose a formalism for further abstracting the subtask structure by endowing an RM with the ability to call other RMs, thus composing a hierarchy of RMs (HRM). We exploit HRMs by treating each call to an RM as an independently solvable subtask using the options framework, and describe a curriculum-based method to learn HRMs from traces observed by the agent. Our experiments reveal that exploiting a handcrafted HRM leads to faster convergence than with a flat HRM, and that learning an HRM is feasible in cases where its equivalent flat representation is not.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#20844;&#24179;&#26631;&#35760;&#32858;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#32771;&#34385;&#20102;&#19979;&#28216;&#24212;&#29992;&#21644;&#22242;&#20307;&#20844;&#24179;&#24615;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2205.14358</link><description>&lt;p&gt;
&#20844;&#24179;&#26631;&#35760;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fair Labeled Clustering. (arXiv:2205.14358v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#20844;&#24179;&#26631;&#35760;&#32858;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#32771;&#34385;&#20102;&#19979;&#28216;&#24212;&#29992;&#21644;&#22242;&#20307;&#20844;&#24179;&#24615;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32858;&#31867;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#19981;&#21516;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#31639;&#27861;&#12290;&#30446;&#21069;&#26368;&#24120;&#35265;&#30340;&#20844;&#24179;&#27010;&#24565;&#26159;&#22242;&#20307;&#20844;&#24179;&#24615;&#65292;&#21363;&#27599;&#20010;&#32858;&#31867;&#20013;&#37117;&#35201;&#20445;&#35777;&#22242;&#20307;&#27604;&#20363;&#30340;&#21512;&#29702;&#24615;&#12290;&#26412;&#25991;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#32771;&#34385;&#20102;&#32858;&#31867;&#20219;&#21153;&#30340;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#22312;&#35813;&#24212;&#29992;&#20013;&#22914;&#20309;&#23454;&#29616;&#22242;&#20307;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20855;&#20307;&#30740;&#31350;&#20102;&#19968;&#31181;&#24120;&#35265;&#24773;&#20917;&#65292;&#21363;&#20915;&#31574;&#32773;&#36816;&#34892;&#32858;&#31867;&#31639;&#27861;&#65292;&#26816;&#26597;&#27599;&#20010;&#32858;&#31867;&#30340;&#20013;&#24515;&#24182;&#20026;&#30456;&#24212;&#30340;&#32858;&#31867;&#30830;&#23450;&#19968;&#20010;&#36866;&#24403;&#30340;&#32467;&#26524;&#65288;&#26631;&#31614;&#65289;&#65292;&#20363;&#22914;&#25307;&#32856;&#20013;&#30340;&#8220;&#24405;&#29992;&#8221;&#25110;&#8220;&#25298;&#32477;&#8221;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#30830;&#20445;&#22242;&#20307;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#24076;&#26395;&#27599;&#20010;&#26631;&#31614;&#37117;&#26377;&#31526;&#21512;&#27604;&#20363;&#30340;&#22242;&#20307;&#20195;&#34920;&#65292;&#20294;&#19981;&#19968;&#23450;&#35201;&#27714;&#27599;&#20010;&#32858;&#31867;&#37117;&#20855;&#26377;&#20004;&#20010;&#22242;&#20307;&#30340;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous algorithms have been produced for the fundamental problem of clustering under many different notions of fairness. Perhaps the most common family of notions currently studied is group fairness, in which proportional group representation is ensured in every cluster. We extend this direction by considering the downstream application of clustering and how group fairness should be ensured for such a setting. Specifically, we consider a common setting in which a decision-maker runs a clustering algorithm, inspects the center of each cluster, and decides an appropriate outcome (label) for its corresponding cluster. In hiring for example, there could be two outcomes, positive (hire) or negative (reject), and each cluster would be assigned one of these two outcomes. To ensure group fairness in such a setting, we would desire proportional group representation in every label but not necessarily in every cluster as is done in group fair clustering. We provide algorithms for such problems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24418;&#24335;&#21270;&#20102;&#20559;&#22909;&#36816;&#34892;&#26102;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25928;&#29992;&#29702;&#35770;&#30340;&#26367;&#20195;&#26041;&#26696;&#26469;&#25551;&#36848;&#31639;&#27861;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#19982;&#38543;&#26102;&#38388;&#30340;&#25512;&#31227;&#21644;&#28040;&#36153;&#26102;&#38388;&#30340;&#20998;&#24067;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2205.13028</link><description>&lt;p&gt;
&#24418;&#24335;&#21270;&#36816;&#34892;&#26102;&#20998;&#24067;&#19978;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Formalizing Preferences Over Runtime Distributions. (arXiv:2205.13028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24418;&#24335;&#21270;&#20102;&#20559;&#22909;&#36816;&#34892;&#26102;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25928;&#29992;&#29702;&#35770;&#30340;&#26367;&#20195;&#26041;&#26696;&#26469;&#25551;&#36848;&#31639;&#27861;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#19982;&#38543;&#26102;&#38388;&#30340;&#25512;&#31227;&#21644;&#28040;&#36153;&#26102;&#38388;&#30340;&#20998;&#24067;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#35745;&#31639;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#22312;&#33021;&#22815;&#36820;&#22238;&#27491;&#30830;&#32467;&#26524;&#30340;&#31639;&#27861;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#20998;&#24067;&#19981;&#21516;&#65288;&#20363;&#22914;SAT&#27714;&#35299;&#22120;&#65292;&#25490;&#24207;&#31639;&#27861;&#65289;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24418;&#24335;&#21270;&#36816;&#34892;&#26102;&#20998;&#24067;&#19978;&#30340;&#20559;&#22909;&#20026;&#36825;&#20123;&#36873;&#25321;&#22880;&#23450;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#24448;&#24448;&#24076;&#26395;&#36873;&#25321;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#26368;&#30701;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20559;&#22909;&#23558;&#23436;&#20840;&#21463;&#21040;&#31639;&#27861;&#22312;&#22351;&#36755;&#20837;&#19978;&#34920;&#29616;&#22914;&#20309;&#32780;&#24433;&#21709;&#65292;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#24120;&#24895;&#24847;&#22312;&#38271;&#26102;&#38388;&#30340;&#36816;&#34892;&#27809;&#26377;&#32467;&#26463;&#20043;&#21069;&#23558;&#20854;&#20999;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25928;&#29992;&#29702;&#35770;&#30340;&#21487;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#25551;&#36848;&#31639;&#27861;&#20559;&#22909;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#21462;&#20915;&#20110;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#38382;&#39064;&#35299;&#20915;&#30340;&#20215;&#20540;&#22914;&#20309;&#19979;&#38477;&#20197;&#21450;&#28040;&#36153;&#26102;&#38388;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#30495;&#23454;&#30340;&#25928;&#29992;&#20989;&#25968;&#31034;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#20248;&#21183;&#20915;&#31574;&#31639;&#27861;&#30340;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#23545;&#36825;&#20123;&#20989;&#25968;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
When trying to solve a computational problem, we are often faced with a choice between algorithms that are guaranteed to return the right answer but differ in their runtime distributions (e.g., SAT solvers, sorting algorithms). This paper aims to lay theoretical foundations for such choices by formalizing preferences over runtime distributions. It might seem that we should simply prefer the algorithm that minimizes expected runtime. However, such preferences would be driven by exactly how slow our algorithm is on bad inputs, whereas in practice we are typically willing to cut off occasional, sufficiently long runs before they finish. We propose a principled alternative, taking a utility-theoretic approach to characterize the scoring functions that describe preferences over algorithms. These functions depend on the way our value for solving our problem decreases with time and on the distribution from which captimes are drawn. We describe examples of realistic utility functions and show 
&lt;/p&gt;</description></item><item><title>CLIP-Dissect&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#21363;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.10965</link><description>&lt;p&gt;
CLIP-Dissect&#65306;&#28145;&#24230;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#34920;&#31034;&#30340;&#33258;&#21160;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10965
&lt;/p&gt;
&lt;p&gt;
CLIP-Dissect&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#21363;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;CLIP-Dissect&#65292;&#21487;&#20197;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#21333;&#20010;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;CLIP-Dissect&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#22810;&#27169;&#24577;&#35270;&#35273;/&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CLIP-Dissect&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#65292;&#20854;&#20013;&#21253;&#25324;&#20855;&#22791;&#8220;&#22320;&#38754;&#30495;&#30456;&#8221;&#65288;ground-truth&#65289;&#30340;&#26368;&#21518;&#19968;&#23618;&#31070;&#32463;&#20803;&#20197;&#21450;&#20855;&#22791;&#23450;&#24615;&#22909;&#30340;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#38750;&#24120;&#28789;&#27963;&#65306;&#23427;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#26032;&#27010;&#24565;&#65292;&#21487;&#20197;&#25193;&#23637;&#20197;&#21033;&#29992;&#26410;&#26469;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;CLIP-Dissect&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;4&#20998;&#38047;&#20869;&#26631;&#35760;ResNet-50&#30340;&#20116;&#23618;&#25152;&#26377;&#31070;&#32463;&#20803;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/Trustworthy-ML-Lab/CLIP-dissect &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10 times faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#38024;&#23545;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#21644;&#38450;&#30149;&#27602;&#20135;&#21697;&#36827;&#34892;&#20102;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#21644;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#26041;&#27861;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#39640;&#36798;99%&#30340;&#20195;&#29702;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.06241</link><description>&lt;p&gt;
&#22312;&#20302;&#35823;&#25253;&#29575;&#26465;&#20214;&#19979;&#31363;&#21462;&#21644;&#36867;&#36991;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#21644;&#38450;&#30149;&#27602;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
Stealing and Evading Malware Classifiers and Antivirus at Low False Positive Conditions. (arXiv:2204.06241v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#38024;&#23545;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#21644;&#38450;&#30149;&#27602;&#20135;&#21697;&#36827;&#34892;&#20102;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#21644;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#26041;&#27861;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#39640;&#36798;99%&#30340;&#20195;&#29702;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#24050;&#25104;&#21151;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20294;&#23545;&#20110;&#25191;&#34892;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#25915;&#20987;&#30340;&#24037;&#20316;&#21407;&#29702;&#23578;&#19981;&#28165;&#26970;&#12290;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#23433;&#20840;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#26465;&#20214;&#65292;&#29305;&#21035;&#26159;&#23545;&#20302;&#35823;&#25253;&#29575;&#26377;&#30528;&#26497;&#24378;&#30340;&#35201;&#27714;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38024;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;&#29420;&#31435;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#21644;&#38450;&#30149;&#27602;&#20135;&#21697;&#30340;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#29992;&#20110;&#20195;&#29702;&#27169;&#22411;&#65288;dualFFNN&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#23558;&#36716;&#31227;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#20195;&#29702;&#21019;&#24314;&#65288;FFNN-TL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model stealing attacks have been successfully used in many machine learning domains, but there is little understanding of how these attacks work against models that perform malware detection. Malware detection and, in general, security domains have unique conditions. In particular, there are very strong requirements for low false positive rates (FPR). Antivirus products (AVs) that use machine learning are very complex systems to steal, malware binaries continually change, and the whole environment is adversarial by nature. This study evaluates active learning model stealing attacks against publicly available stand-alone machine learning malware classifiers and also against antivirus products. The study proposes a new neural network architecture for surrogate models (dualFFNN) and a new model stealing attack that combines transfer and active learning for surrogate creation (FFNN-TL). We achieved good surrogates of the stand-alone classifiers with up to 99\% agreement with the target mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35821;&#35328;&#30340;&#35780;&#20215;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#36890;&#36807;&#39640;&#20142;&#30340;&#35299;&#37322;&#25193;&#20805;&#20102;XNLI&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#21487;&#20449;&#24230;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#30340;&#24402;&#22240;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2204.05428</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#24402;&#22240;&#26041;&#27861;&#35780;&#20272;&#30340;&#22810;&#35821;&#35328;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference. (arXiv:2204.05428v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35821;&#35328;&#30340;&#35780;&#20215;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#36890;&#36807;&#39640;&#20142;&#30340;&#35299;&#37322;&#25193;&#20805;&#20102;XNLI&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#21487;&#20449;&#24230;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#30340;&#24402;&#22240;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24402;&#22240;&#26041;&#27861;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#33521;&#35821;&#35821;&#35328;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#30340;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#30340;&#26032;&#22411;&#36328;&#35821;&#35328;&#31574;&#30053;&#26469;&#34913;&#37327;&#24544;&#23454;&#24230;&#65292;&#25490;&#38500;&#20102;&#21024;&#20943;&#35780;&#20272;&#30340;&#32570;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#36755;&#20986;&#26426;&#21046;&#21644;&#32858;&#21512;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#39640;&#20142;&#30340;&#35299;&#37322;&#25193;&#20805;&#20102;XNLI&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#26377;&#39640;&#20142;&#30340;&#22810;&#35821;&#35328;NLI&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#26410;&#26469;&#30340;ExNLP&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24615;&#33021;&#26368;&#20339;&#30340;&#24402;&#22240;&#26041;&#27861;&#23545;&#20110;&#21487;&#20449;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#34920;&#29616;&#26159;&#19981;&#21516;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of faithfulness and plausibility. First, we introduce a novel cross-lingual strategy to measure faithfulness based on word alignments, which eliminates the drawbacks of erasure-based evaluations.We then perform a comprehensive evaluation of attribution methods, considering different output mechanisms and aggregation methods. Finally, we augment the XNLI dataset with highlight-based explanations, providing a multilingual NLI dataset with highlights, to support future exNLP studies. Our results show that attribution methods performing best for plausibility and faithfulness are different.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#25104;&#21592;&#20851;&#31995;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#31070;&#32463;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#29305;&#24449;&#65292;&#24182;&#35299;&#37322;&#20854;&#20915;&#31574;&#12290;&#26041;&#27861;&#33021;&#22815;&#26631;&#35782;&#36129;&#29486;&#27599;&#20010;&#29305;&#24449;&#23545;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#65292;&#24182;&#35299;&#37322;&#20998;&#31867;&#22120;&#22788;&#29702;&#36755;&#20837;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2204.02241</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#25104;&#21592;&#20851;&#31995;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#29305;&#24449;&#30456;&#20851;&#24615;&#24182;&#35299;&#37322;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
A Set Membership Approach to Discovering Feature Relevance and Explaining Neural Classifier Decisions. (arXiv:2204.02241v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#25104;&#21592;&#20851;&#31995;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#31070;&#32463;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#29305;&#24449;&#65292;&#24182;&#35299;&#37322;&#20854;&#20915;&#31574;&#12290;&#26041;&#27861;&#33021;&#22815;&#26631;&#35782;&#36129;&#29486;&#27599;&#20010;&#29305;&#24449;&#23545;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#65292;&#24182;&#35299;&#37322;&#20998;&#31867;&#22120;&#22788;&#29702;&#36755;&#20837;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20998;&#31867;&#22120;&#26159;&#25552;&#20379;&#27169;&#24335;&#31867;&#21035;&#20915;&#31574;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#26500;&#25104;&#20102;&#26576;&#20010;&#26410;&#30693;&#20989;&#25968;&#30340;&#36755;&#20986;&#30340;&#36817;&#20284;&#65292;&#35813;&#20989;&#25968;&#23558;&#27169;&#24335;&#25968;&#25454;&#26144;&#23556;&#21040;&#20854;&#30456;&#24212;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#35813;&#20989;&#25968;&#30340;&#30693;&#35782;&#20197;&#21450;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#22797;&#26434;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#24448;&#24448;&#26080;&#27861;&#33719;&#24471;&#26377;&#20851;&#22914;&#20309;&#36827;&#34892;&#20855;&#20307;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#24378;&#22823;&#30340;&#23398;&#20064;&#31995;&#32479;&#34987;&#35748;&#20026;&#26159;&#40657;&#21283;&#23376;&#65292;&#22312;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#23427;&#20204;&#24448;&#24448;&#34987;&#35748;&#20026;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#22522;&#20110;&#38598;&#21512;&#25104;&#21592;&#20998;&#26512;&#65292;&#25105;&#20204;&#25226;&#36755;&#20837;&#27169;&#24335;&#20998;&#25104;&#23376;&#38598;&#65292;&#20197;&#20851;&#32852;&#19981;&#21516;&#30340;&#36755;&#20986;&#22522;&#31867;&#12290;&#36825;&#31181;&#20851;&#32852;&#26159;&#36890;&#36807;&#35745;&#31639;&#23558;&#20998;&#31867;&#22120;&#20915;&#31574;&#24341;&#23548;&#21040;&#26367;&#20195;&#31867;&#21035;&#25152;&#38656;&#30340;&#26368;&#23567;&#36755;&#20837;&#25668;&#21160;&#26469;&#25512;&#26029;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#21738;&#20123;&#29305;&#24449;&#34987;&#35748;&#20026;&#26159;&#30456;&#20851;&#30340;&#65292;&#24182;&#37327;&#21270;&#20854;&#23545;&#39044;&#27979;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#20998;&#31867;&#22120;&#22312;&#22788;&#29702;&#36755;&#20837;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#34892;&#20026;&#12290;&#26631;&#20934;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21457;&#29616;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#21644;&#35299;&#37322;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20570;&#20986;&#30340;&#20998;&#31867;&#20915;&#31574;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural classifiers are non linear systems providing decisions on the classes of patterns, for a given problem they have learned. The output computed by a classifier for each pattern constitutes an approximation of the output of some unknown function, mapping pattern data to their respective classes. The lack of knowledge of such a function along with the complexity of neural classifiers, especially when these are deep learning architectures, do not permit to obtain information on how specific predictions have been made. Hence, these powerful learning systems are considered as black boxes and in critical applications their use tends to be considered inappropriate. Gaining insight on such a black box operation constitutes a one way approach in interpreting operation of neural classifiers and assessing the validity of their decisions. In this paper we tackle this problem introducing a novel methodology for discovering which features are considered relevant by a trained neural classifier a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;Deeper&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#19982;&#31454;&#36187;&#20013;&#30340;&#20854;&#20182;&#24037;&#20855;&#30340;&#27604;&#36739;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2203.12026</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;&#22312;ADAS&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Testing in an ADAS Case Study Using Simulation-Integrated Bio-Inspired Search-Based Testing. (arXiv:2203.12026v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;Deeper&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#19982;&#31454;&#36187;&#20013;&#30340;&#20854;&#20182;&#24037;&#20855;&#30340;&#27604;&#36739;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;Deeper&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#23454;&#29616;&#30340;&#20223;&#30495;&#38598;&#25104;&#27979;&#35797;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#12290;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#32452;&#26032;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;-&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#12289;&#65288;&#956;+&#955;&#65289;&#21644;&#65288;&#956;&#65292;&#955;&#65289;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#20197;&#21450;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#65292;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#36136;&#37327;&#31181;&#23376;&#31181;&#32676;&#20197;&#21450;&#20026;&#24314;&#27169;&#27979;&#35797;&#22330;&#26223;&#20351;&#29992;&#30340;&#29305;&#23450;&#39046;&#22495;&#20132;&#21449;&#21644;&#31361;&#21464;&#25805;&#20316;&#12290;&#20026;&#20102;&#23637;&#31034;Deeper&#20013;&#26032;&#27979;&#35797;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#19982;SBST 2021&#30340;&#20116;&#20010;&#21442;&#36187;&#24037;&#20855;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;Deeper&#20013;&#30340;&#26032;&#27979;&#35797;&#29983;&#25104;&#22120;&#19981;&#20165;&#22312;&#20197;&#21069;&#30340;&#29256;&#26412;&#19978;&#26377;&#20102;&#24456;&#22823;&#25552;&#21319;&#65292;&#32780;&#19988;...
&lt;/p&gt;
&lt;p&gt;
This paper presents an extended version of Deeper, a search-based simulation-integrated test solution that generates failure-revealing test scenarios for testing a deep neural network-based lane-keeping system. In the newly proposed version, we utilize a new set of bio-inspired search algorithms, genetic algorithm (GA), $({\mu}+{\lambda})$ and $({\mu},{\lambda})$ evolution strategies (ES), and particle swarm optimization (PSO), that leverage a quality population seed and domain-specific cross-over and mutation operations tailored for the presentation model used for modeling the test scenarios. In order to demonstrate the capabilities of the new test generators within Deeper, we carry out an empirical evaluation and comparison with regard to the results of five participating tools in the cyber-physical systems testing competition at SBST 2021. Our evaluation shows the newly proposed test generators in Deeper not only represent a considerable improvement on the previous version but also 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#26469;&#25226;&#29992;&#25143;&#25551;&#36848;&#21830;&#21697;&#30340;&#23646;&#24615;&#30340;&#35821;&#20041;&#34920;&#36798;&#20986;&#26469;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.02830</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21457;&#29616;&#36719;&#23646;&#24615;&#30340;&#20010;&#24615;&#21270;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Discovering Personalized Semantics for Soft Attributes in Recommender Systems using Concept Activation Vectors. (arXiv:2202.02830v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02830
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#26469;&#25226;&#29992;&#25143;&#25551;&#36848;&#21830;&#21697;&#30340;&#23646;&#24615;&#30340;&#35821;&#20041;&#34920;&#36798;&#20986;&#26469;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25152;&#20351;&#29992;&#30340;&#21407;&#22987;&#29992;&#25143;&#21453;&#39304;&#30340;&#23616;&#38480;&#24615;&#65288;&#20363;&#22914;&#28857;&#20987;&#12289;&#39033;&#30446;&#28040;&#36153;&#12289;&#35780;&#20998;&#65289;&#12290;&#23427;&#20204;&#20801;&#35768;&#29992;&#25143;&#20197;&#26356;&#20016;&#23500;&#30340;&#26041;&#24335;&#34920;&#36798;&#24847;&#22270;&#12289;&#20559;&#22909;&#12289;&#32422;&#26463;&#21644;&#19978;&#19979;&#25991;&#65292;&#36890;&#24120;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#65288;&#21253;&#25324;&#20998;&#31867;&#25628;&#32034;&#21644;&#23545;&#35805;&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#30740;&#31350;&#26469;&#25214;&#21040;&#20351;&#29992;&#36825;&#20123;&#21453;&#39304;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#12290;&#19968;&#20010;&#25361;&#25112;&#26159;&#20174;&#32463;&#24120;&#29992;&#20110;&#25551;&#36848;&#25152;&#38656;&#39033;&#30446;&#30340;&#24320;&#25918;&#24335;&#26415;&#35821;&#25110;&#23646;&#24615;&#20013;&#25512;&#26029;&#29992;&#25143;&#30340;&#35821;&#20041;&#24847;&#22270;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#25913;&#36827;&#25512;&#33616;&#32467;&#26524;&#12290;&#21033;&#29992;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#8212;&#8212;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23398;&#20064;&#19968;&#31181;&#34920;&#31034;&#65292;&#25429;&#25417;&#36825;&#20123;&#23646;&#24615;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#23427;&#20204;&#36830;&#25509;&#21040;&#29992;&#25143;&#30340;&#20559;&#22909;&#21644;&#34892;&#20026;&#20013;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#21151;&#33021;&#26159;&#23427;&#33021;&#22815;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Interactive recommender systems have emerged as a promising paradigm to overcome the limitations of the primitive user feedback used by traditional recommender systems (e.g., clicks, item consumption, ratings). They allow users to express intent, preferences, constraints, and contexts in a richer fashion, often using natural language (including faceted search and dialogue). Yet more research is needed to find the most effective ways to use this feedback. One challenge is inferring a user's semantic intent from the open-ended terms or attributes often used to describe a desired item, and using it to refine recommendation results. Leveraging concept activation vectors (CAVs) [26], a recently developed approach for model interpretability in machine learning, we develop a framework to learn a representation that captures the semantics of such attributes and connects them to user preferences and behaviors in recommender systems. One novel feature of our approach is its ability to distinguis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#32676;&#20307;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#22320;&#36136;&#31185;&#23398;&#27169;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#22522;&#20934;&#20248;&#21270;&#38382;&#39064;&#21644;Badlands&#39118;&#35980;&#28436;&#21270;&#27169;&#22411;&#20013;&#37117;&#21462;&#24471;&#20102;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2201.06843</link><description>&lt;p&gt;
&#29992;&#20195;&#29702;&#27169;&#22411;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#32676;&#20307;&#20248;&#21270;&#26041;&#27861;&#26469;&#22788;&#29702;&#35745;&#31639;&#26114;&#36149;&#30340;&#22320;&#36136;&#31185;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Surrogate-assisted distributed swarm optimisation for computationally expensive geoscientific models. (arXiv:2201.06843v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#32676;&#20307;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#22320;&#36136;&#31185;&#23398;&#27169;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#22522;&#20934;&#20248;&#21270;&#38382;&#39064;&#21644;Badlands&#39118;&#35980;&#28436;&#21270;&#27169;&#22411;&#20013;&#37117;&#21462;&#24471;&#20102;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#25552;&#20379;&#26080;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#23545;&#20110;&#38590;&#20197;&#33719;&#21462;&#26799;&#24230;&#30340;&#27169;&#22411;&#65288;&#22914;&#22320;&#36136;&#31185;&#23398;&#39118;&#35980;&#28436;&#21270;&#27169;&#22411;&#65289;&#26377;&#30410;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#21363;&#20351;&#26159;&#24182;&#34892;&#35745;&#31639;&#30340;&#20998;&#24067;&#24335;&#32676;&#20307;&#20248;&#21270;&#20063;&#24456;&#21507;&#21147;&#12290;&#25105;&#20204;&#21487;&#20197;&#37319;&#29992;&#20195;&#29702;&#27169;&#22411;&#36741;&#21161;&#20248;&#21270;&#31561;&#39640;&#25928;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#26159;&#23454;&#26045;&#20195;&#29702;&#27169;&#22411;&#30340;&#36827;&#31243;&#38388;&#36890;&#20449;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#32676;&#20307;&#20248;&#21270;&#20013;&#20195;&#29702;&#27169;&#22411;&#35780;&#20272;&#36866;&#24212;&#20540;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#24182;&#34892;&#35745;&#31639;&#26550;&#26500;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#19968;&#32452;&#22522;&#20934;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#39118;&#35980;&#28436;&#21270;&#27169;&#22411;&#30340;&#22320;&#36136;&#31185;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#22522;&#20934;&#20989;&#25968;&#21644;Badlands&#39118;&#35980;&#28436;&#21270;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#35745;&#31639;&#26102;&#38388;&#30340;&#32553;&#30701;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms provide gradient-free optimisation which is beneficial for models that have difficulty in obtaining gradients; for instance, geoscientific landscape evolution models. However, such models are at times computationally expensive and even distributed swarm-based optimisation with parallel computing struggles. We can incorporate efficient strategies such as surrogate-assisted optimisation to address the challenges; however, implementing inter-process communication for surrogate-based model training is difficult. In this paper, we implement surrogate-based estimation of fitness evaluation in distributed swarm optimisation over a parallel computing architecture. We first test the framework on a set of benchmark optimisation problems and then apply it to a geoscientific model that features a landscape evolution model. Our results demonstrate very promising results for benchmark functions and the Badlands landscape evolution model. We obtain a reduction in computational
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22312;&#32447;&#21305;&#37197;&#31639;&#27861;&#65292;&#23427;&#20026;&#22312;&#32447;&#20108;&#20998;&#21305;&#37197;&#24179;&#21488;&#20013;&#30340;&#20004;&#20010;&#21452;&#26041;&#21516;&#26102;&#25552;&#20379;&#20844;&#24179;&#24615;&#20445;&#35777;&#65292;&#32780;&#20197;&#38477;&#20302;&#25805;&#20316;&#32773;&#21033;&#28070;&#20026;&#20195;&#20215;&#12290;</title><link>http://arxiv.org/abs/2201.06021</link><description>&lt;p&gt;
&#22312;&#32447;&#20108;&#20998;&#21305;&#37197;&#20013;&#30340;Rawlsian&#20844;&#24179;&#24615;&#65306;&#21452;&#20391;&#12289;&#32676;&#20307;&#21644;&#20010;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rawlsian Fairness in Online Bipartite Matching: Two-sided, Group, and Individual. (arXiv:2201.06021v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22312;&#32447;&#21305;&#37197;&#31639;&#27861;&#65292;&#23427;&#20026;&#22312;&#32447;&#20108;&#20998;&#21305;&#37197;&#24179;&#21488;&#20013;&#30340;&#20004;&#20010;&#21452;&#26041;&#21516;&#26102;&#25552;&#20379;&#20844;&#24179;&#24615;&#20445;&#35777;&#65292;&#32780;&#20197;&#38477;&#20302;&#25805;&#20316;&#32773;&#21033;&#28070;&#20026;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20108;&#20998;&#21305;&#37197;&#24179;&#21488;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20247;&#21253;&#21644;&#25340;&#36710;&#31561;&#37325;&#35201;&#39046;&#22495;&#12290;&#24179;&#21488;&#30001;&#19977;&#20010;&#23454;&#20307;&#32452;&#25104;&#65306;&#20004;&#20010;&#38656;&#35201;&#21305;&#37197;&#30340;&#21452;&#26041;&#65292;&#20197;&#21450;&#19968;&#20010;&#20915;&#23450;&#21305;&#37197;&#30340;&#24179;&#21488;&#25805;&#20316;&#32773;&#12290;&#20256;&#32479;&#19978;&#65292;&#23545;&#20110;&#36825;&#31181;&#24179;&#21488;&#30340;&#31639;&#27861;&#35774;&#35745;&#20851;&#27880;&#30340;&#26159;&#25805;&#20316;&#32773;&#30340;&#65288;&#39044;&#26399;&#65289;&#21033;&#28070;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20844;&#24179;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21516;&#26102;&#24573;&#30053;&#24066;&#22330;&#20013;&#20219;&#24847;&#19968;&#26041;&#30340;&#20844;&#24179;&#24615;&#20445;&#35777;&#30340;&#29616;&#26377;&#31639;&#27861;&#24050;&#32463;&#26080;&#27861;&#28385;&#36275;&#35201;&#27714;&#12290;&#26412;&#25991;&#23558;&#29616;&#26377;&#24037;&#20316;&#25512;&#24191;&#21040;&#21516;&#26102;&#20026;&#24066;&#22330;&#20013;&#30340;&#21452;&#26041;&#25552;&#20379;&#20844;&#24179;&#24615;&#20445;&#35777;&#65292;&#20197;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#25805;&#20316;&#32773;&#21033;&#28070;&#30340;&#38477;&#20302;&#20026;&#20195;&#20215;&#12290;&#25105;&#20204;&#32771;&#34385;&#32452;&#21644;&#20010;&#20154;Rawlsian&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#20855;&#26377;&#21487;&#20197;&#35843;&#25972;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online bipartite-matching platforms are ubiquitous and find applications in important areas such as crowdsourcing and ridesharing. In the most general form, the platform consists of three entities: two sides to be matched and a platform operator that decides the matching. The design of algorithms for such platforms has traditionally focused on the operator's (expected) profit. Since fairness has become an important consideration that was ignored in the existing algorithms a collection of online matching algorithms have been developed that give a fair treatment guarantee for one side of the market at the expense of a drop in the operator's profit. In this paper, we generalize the existing work to offer fair treatment guarantees to both sides of the market simultaneously, at a calculated worst case drop to operator profit. We consider group and individual Rawlsian fairness criteria. Moreover, our algorithms have theoretical guarantees and have adjustable parameters that can be tuned as d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27010;&#29575;&#20998;&#37197;&#33719;&#24471;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#19981;&#23436;&#32654;&#30693;&#35782;&#30340;&#20844;&#24179;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#22312;&#36825;&#31181;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#20013;&#32473;&#20986;&#20102;&#36924;&#36817;&#27604;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2006.10916</link><description>&lt;p&gt;
&#27010;&#29575;&#20844;&#24179;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Fair Clustering. (arXiv:2006.10916v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27010;&#29575;&#20998;&#37197;&#33719;&#24471;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#19981;&#23436;&#32654;&#30693;&#35782;&#30340;&#20844;&#24179;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#22312;&#36825;&#31181;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#20013;&#32473;&#20986;&#20102;&#36924;&#36817;&#27604;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#31867;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#20013;&#22830;&#20915;&#31574;&#32773;&#34987;&#36171;&#20104;&#20102;&#19968;&#20010;&#39030;&#28857;&#30340;&#23436;&#25972;&#24230;&#37327;&#22270;&#65292;&#24182;&#19988;&#24517;&#39035;&#25552;&#20379;&#39030;&#28857;&#30340;&#32858;&#31867;&#65292;&#20197;&#26368;&#23567;&#21270;&#26576;&#20123;&#23458;&#35266;&#20989;&#25968;&#12290;&#22312;&#20844;&#24179;&#32858;&#31867;&#38382;&#39064;&#20013;&#65292;&#39030;&#28857;&#34987;&#36171;&#20104;&#20102;&#39068;&#33394;&#65288;&#20363;&#22914;&#65292;&#23646;&#20110;&#19968;&#20010;&#32452;&#30340;&#25104;&#21592;&#36164;&#26684;&#65289;&#65292;&#26377;&#25928;&#32858;&#31867;&#30340;&#29305;&#24449;&#20063;&#21487;&#33021;&#21253;&#25324;&#39068;&#33394;&#22312;&#35813;&#32858;&#31867;&#20013;&#30340;&#34920;&#31034;&#12290;&#20043;&#21069;&#30340;&#20844;&#24179;&#32858;&#31867;&#24037;&#20316;&#20551;&#35774;&#23436;&#20840;&#30693;&#36947;&#32452;&#25104;&#21592;&#36523;&#20221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20551;&#35774;&#36890;&#36807;&#27010;&#29575;&#20998;&#37197;&#26469;&#33719;&#24471;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#19981;&#23436;&#32654;&#30693;&#35782;&#65292;&#23545;&#20197;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#36825;&#31181;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#36924;&#36817;&#27604;&#25285;&#20445;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#8220;&#24230;&#37327;&#25104;&#21592;&#36523;&#20221;&#8221;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#32452;&#20855;&#26377;&#39034;&#24207;&#21644;&#36317;&#31163;&#30340;&#27010;&#24565;&#12290;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#20197;&#21450;&#22522;&#32447;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#22320;&#30693;&#36947;&#32452;&#25104;&#21592;&#36523;&#20221;&#26102;&#25581;&#31034;&#24494;&#22937;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clustering problems, a central decision-maker is given a complete metric graph over vertices and must provide a clustering of vertices that minimizes some objective function. In fair clustering problems, vertices are endowed with a color (e.g., membership in a group), and the features of a valid clustering might also include the representation of colors in that clustering. Prior work in fair clustering assumes complete knowledge of group membership. In this paper, we generalize prior work by assuming imperfect knowledge of group membership through probabilistic assignments. We present clustering algorithms in this more general setting with approximation ratio guarantees. We also address the problem of "metric membership", where different groups have a notion of order and distance. Experiments are conducted using our proposed algorithms as well as baselines to validate our approach and also surface nuanced concerns when group membership is not known deterministically.
&lt;/p&gt;</description></item></channel></rss>