<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Mini-Gemini &#25366;&#25496;&#20102;VLMs&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#35270;&#35273;&#26631;&#35760;&#12289;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;VLM&#24341;&#23548;&#29983;&#25104;&#31561;&#26041;&#24335;&#65292;&#32553;&#23567;&#20102;&#19982;&#20808;&#36827;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;</title><link>https://arxiv.org/abs/2403.18814</link><description>&lt;p&gt;
Mini-Gemini: &#25366;&#25496;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18814
&lt;/p&gt;
&lt;p&gt;
Mini-Gemini &#25366;&#25496;&#20102;VLMs&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#35270;&#35273;&#26631;&#35760;&#12289;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;VLM&#24341;&#23548;&#29983;&#25104;&#31561;&#26041;&#24335;&#65292;&#32553;&#23567;&#20102;&#19982;&#20808;&#36827;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mini-Gemini&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22686;&#24378;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#12290;&#23613;&#31649;VLMs&#22312;&#20419;&#36827;&#22522;&#26412;&#35270;&#35273;&#23545;&#35805;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#19982;GPT-4&#21644;Gemini&#31561;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#20173;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#20174;&#39640;&#20998;&#36776;&#29575;&#35270;&#35273;&#26631;&#35760;&#12289;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;VLM&#24341;&#23548;&#29983;&#25104;&#19977;&#20010;&#26041;&#38754;&#25366;&#25496;VLMs&#30340;&#28508;&#21147;&#65292;&#20197;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#12290;&#20026;&#20102;&#22686;&#24378;&#35270;&#35273;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#39069;&#22806;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#32454;&#21270;&#65292;&#32780;&#19981;&#22686;&#21152;&#35270;&#35273;&#26631;&#35760;&#25968;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#20419;&#36827;&#31934;&#30830;&#30340;&#22270;&#20687;&#29702;&#35299;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#29983;&#25104;&#65292;&#25299;&#23637;&#20102;&#24403;&#21069;VLMs&#30340;&#25805;&#20316;&#33539;&#22260;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;Mini-Gemini&#36827;&#19968;&#27493;&#25366;&#25496;&#20102;VLMs&#30340;&#28508;&#21147;&#65292;&#24182;&#36171;&#20104;&#24403;&#21069;&#26694;&#26550;&#22270;&#20687;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18814v1 Announce Type: cross  Abstract: In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and gen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18807</link><description>&lt;p&gt;
ECoDepth: &#26377;&#25928;&#35843;&#25972;&#25193;&#25955;&#27169;&#22411;&#20197;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#35270;&#24046;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#65288;SIDE&#65289;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#22270;&#20687;&#20013;&#30340;&#38452;&#24433;&#21644;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;&#25105;&#20204;&#20174;&#24050;&#26377;&#30740;&#31350;&#30340;&#21551;&#21457;&#20013;&#25506;&#35752;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#30340;SIDE&#27169;&#22411;&#65292;&#20854;&#21463;&#21040;ViT&#23884;&#20837;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18807v1 Announce Type: cross  Abstract: In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embedding
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18802</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form factuality in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#24320;&#25918;&#24615;&#20027;&#39064;&#30340;&#20107;&#23454;&#24615;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#29983;&#25104;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#23545;&#27169;&#22411;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;LongFact&#30340;&#25552;&#31034;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#22218;&#25324;38&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Search-Augmented Factuality Evaluator&#65288;SAFE&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#22120;&#12290;SAFE&#21033;&#29992;LLM&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#29420;&#30340;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#20197;&#21450;&#30830;&#23450;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#24471;&#21040;&#25628;&#32034;&#32467;&#26524;&#25903;&#25345;&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#35780;&#20272;&#27599;&#20010;&#20107;&#23454;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#23558;F1&#20998;&#25968;&#25193;&#23637;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#22238;&#24212;&#20013;&#25903;&#25345;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#65288;&#31934;&#24230;&#65289;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Gamba&#65292;&#19968;&#31181;&#21333;&#35270;&#22270;3D&#37325;&#24314;&#27169;&#22411;&#65292;&#21019;&#26032;&#22320;&#32467;&#21512;&#20102;&#22823;&#37327;&#30340;3D&#39640;&#26031;&#28857;&#36827;&#34892;&#39640;&#25928;&#37325;&#24314;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26364;&#24052;&#30340;&#39034;&#24207;&#32593;&#32476;&#65292;&#20419;&#36827;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18795</link><description>&lt;p&gt;
Gamba&#65306;&#23558;&#39640;&#26031;&#39128;&#28857;&#19982;&#26364;&#24052;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#21333;&#35270;&#22270;3D&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18795
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Gamba&#65292;&#19968;&#31181;&#21333;&#35270;&#22270;3D&#37325;&#24314;&#27169;&#22411;&#65292;&#21019;&#26032;&#22320;&#32467;&#21512;&#20102;&#22823;&#37327;&#30340;3D&#39640;&#26031;&#28857;&#36827;&#34892;&#39640;&#25928;&#37325;&#24314;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26364;&#24052;&#30340;&#39034;&#24207;&#32593;&#32476;&#65292;&#20419;&#36827;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#20174;&#21333;&#20010;&#22270;&#20687;&#39640;&#25928;&#37325;&#24314;3D&#36164;&#20135;&#30340;&#25361;&#25112;&#65292;&#38543;&#30528;&#23545;&#33258;&#21160;&#21270;3D&#20869;&#23481;&#21019;&#24314;&#27969;&#27700;&#32447;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Gamba&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#36827;&#34892;&#25674;&#20313;&#21270;3D&#37325;&#24314;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20004;&#20010;&#20027;&#35201;&#35265;&#35299;&#65306;(1) 3D&#34920;&#31034;&#65306;&#21033;&#29992;&#22823;&#37327;3D&#39640;&#26031;&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;3D&#39640;&#26031;&#39128;&#28857;&#36807;&#31243;&#65307;(2) &#20027;&#24178;&#35774;&#35745;&#65306;&#24341;&#20837;&#22522;&#20110;&#26364;&#24052;&#30340;&#39034;&#24207;&#32593;&#32476;&#65292;&#20419;&#36827;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#65292;&#24182;&#20855;&#26377;&#19982;&#24207;&#21015;&#65288;&#26631;&#35760;&#65289;&#38271;&#24230;&#30340;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#24212;&#22823;&#37327;&#39640;&#26031;&#28857;&#12290;Gamba&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#27491;&#21017;&#21270;&#31561;&#26041;&#38754;&#34701;&#20837;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18795v1 Announce Type: cross  Abstract: We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines. Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural Radiance Fields (NeRF). Despite their significant success, these approaches encounter practical limitations due to lengthy optimization and considerable memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction model from single-view images, emphasizing two main insights: (1) 3D representation: leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting process; (2) Backbone design: introducing a Mamba-based sequential network that facilitates context-dependent reasoning and linear scalability with the sequence (token) length, accommodating a substantial number of Gaussians. Gamba incorporates significant advancements in data preprocessing, regularization
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#26448;&#26009;&#22270;&#20687;&#65292;&#24314;&#31435;&#20102;ImageNet-D&#22522;&#20934;&#35780;&#20272;&#28145;&#24230;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19968;&#31995;&#21015;&#35270;&#35273;&#27169;&#22411;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#39640;&#36798;60%&#12290;</title><link>https://arxiv.org/abs/2403.18775</link><description>&lt;p&gt;
ImageNet-D: &#22312;&#25193;&#25955;&#21512;&#25104;&#23545;&#35937;&#19978;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#26448;&#26009;&#22270;&#20687;&#65292;&#24314;&#31435;&#20102;ImageNet-D&#22522;&#20934;&#35780;&#20272;&#28145;&#24230;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19968;&#31995;&#21015;&#35270;&#35273;&#27169;&#22411;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#39640;&#36798;60%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#35270;&#35273;&#24863;&#30693;&#40065;&#26834;&#24615;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#22522;&#20934;&#12290;&#21512;&#25104;&#22270;&#20687;&#65292;&#22914;ImageNet-C&#12289;ImageNet-9&#21644;Stylized ImageNet&#65292;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#30772;&#22351;&#12289;&#32972;&#26223;&#21644;&#32441;&#29702;&#30340;&#29305;&#23450;&#31867;&#22411;&#35780;&#20272;&#65292;&#28982;&#32780;&#36825;&#20123;&#40065;&#26834;&#24615;&#22522;&#20934;&#21463;&#38480;&#20110;&#25351;&#23450;&#30340;&#21464;&#20307;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#21512;&#25104;&#38590;&#22270;&#20687;&#30340;&#25968;&#25454;&#28304;&#26469;&#35780;&#20272;&#28145;&#24230;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#27604;&#20219;&#20309;&#20808;&#21069;&#24037;&#20316;&#26356;&#22810;&#26679;&#21270;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#26448;&#26009;&#22270;&#20687;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#22522;&#20934;&#31216;&#20026;ImageNet-D&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ImageNet-D&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#35270;&#35273;&#27169;&#22411;&#30340;&#26174;&#33879;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#20174;&#26631;&#20934;ResNet&#35270;&#35273;&#20998;&#31867;&#22120;&#21040;&#26368;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;MiniGPT-4&#65292;&#23558;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#38477;&#20302;&#20102;&#39640;&#36798;60\%&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18775v1 Announce Type: cross  Abstract: We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\%. Our work suggests that diffusion models 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;K-means&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#31454;&#20105;&#24615;&#38543;&#26426;&#26679;&#26412;&#22823;&#23567;&#20248;&#21270;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;Big-means&#20013;&#30340;&#24182;&#34892;&#22823;&#25968;&#25454;&#32858;&#31867;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18766</link><description>&lt;p&gt;
&#36890;&#36807;&#31454;&#20105;&#24615;&#38543;&#26426;&#26679;&#26412;&#22823;&#23567;&#20248;&#21270;&#22312;Big-means&#20013;&#23454;&#29616;&#20248;&#36234;&#30340;&#24182;&#34892;&#22823;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;K-means&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#31454;&#20105;&#24615;&#38543;&#26426;&#26679;&#26412;&#22823;&#23567;&#20248;&#21270;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;Big-means&#20013;&#30340;&#24182;&#34892;&#22823;&#25968;&#25454;&#32858;&#31867;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-means&#32858;&#31867;&#31639;&#27861;&#65292;&#26159;&#23545;&#20256;&#32479;Big-means&#26041;&#27861;&#30340;&#36827;&#27493;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#24182;&#34892;&#22788;&#29702;&#12289;&#38543;&#26426;&#25277;&#26679;&#21644;&#31454;&#20105;&#24615;&#20248;&#21270;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#20026;&#22823;&#25968;&#25454;&#24212;&#29992;&#35774;&#35745;&#30340;&#21487;&#25193;&#23637;&#21464;&#20307;&#12290;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;&#25216;&#26415;&#36890;&#24120;&#38754;&#20020;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#24037;&#20316;&#20154;&#21592;&#30340;&#26679;&#26412;&#22823;&#23567;&#65292;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#20123;&#26679;&#26412;&#22823;&#23567;&#30340;&#25968;&#25454;&#19981;&#26029;&#34987;&#20998;&#26512;&#65292;&#20419;&#36827;&#20102;&#25214;&#21040;&#26368;&#26377;&#25928;&#37197;&#32622;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#22312;&#20351;&#29992;&#19981;&#21516;&#26679;&#26412;&#22823;&#23567;&#30340;&#24037;&#20316;&#20154;&#21592;&#20043;&#38388;&#24341;&#20837;&#31454;&#20105;&#22240;&#32032;&#65292;&#36827;&#19968;&#27493;&#21050;&#28608;&#20102;Big-means&#31639;&#27861;&#20869;&#30340;&#25928;&#29575;&#12290;&#26412;&#36136;&#19978;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#24182;&#34892;&#35745;&#31639;&#29615;&#22659;&#20013;&#37319;&#29992;&#38543;&#26426;&#12289;&#31454;&#20105;&#24615;&#25277;&#26679;&#31574;&#30053;&#65292;&#24179;&#34913;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18766v1 Announce Type: cross  Abstract: This paper introduces a novel K-means clustering algorithm, an advancement on the conventional Big-means methodology. The proposed method efficiently integrates parallel processing, stochastic sampling, and competitive optimization to create a scalable variant designed for big data applications. It addresses scalability and computation time challenges typically faced with traditional techniques. The algorithm adjusts sample sizes dynamically for each worker during execution, optimizing performance. Data from these sample sizes are continually analyzed, facilitating the identification of the most efficient configuration. By incorporating a competitive element among workers using different sample sizes, efficiency within the Big-means algorithm is further stimulated. In essence, the algorithm balances computational time and clustering quality by employing a stochastic, competitive sampling strategy in a parallel computing setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#21644;&#28857;&#20113;&#32534;&#30721;&#20026;&#22320;&#28857;&#29305;&#24449;&#25551;&#36848;&#31526;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;Field of View&#65288;FoV&#65289;&#36716;&#25442;&#27169;&#22359;&#26469;&#28040;&#38500;&#28145;&#24230;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18762</link><description>&lt;p&gt;
ModaLink&#65306;&#32479;&#19968;&#27169;&#24577;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#22320;&#28857;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#21644;&#28857;&#20113;&#32534;&#30721;&#20026;&#22320;&#28857;&#29305;&#24449;&#25551;&#36848;&#31526;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;Field of View&#65288;FoV&#65289;&#36716;&#25442;&#27169;&#22359;&#26469;&#28040;&#38500;&#28145;&#24230;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#25152;&#35782;&#21035;&#23545;&#20110;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26469;&#23450;&#20301;&#33258;&#36523;&#24182;&#20851;&#38381;&#22312;&#39044;&#20808;&#26500;&#24314;&#30340;&#22320;&#22270;&#20013;&#30340;&#24490;&#29615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#32780;&#21333;&#27169;&#24577;&#20256;&#24863;&#22120;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#32780;&#26816;&#32034;&#22270;&#20687;&#30340;&#20132;&#21449;&#27169;&#24577;&#22320;&#28857;&#35782;&#21035;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#20132;&#21449;&#27169;&#24577;&#26041;&#27861;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;3D&#28857;&#20351;&#29992;&#28145;&#24230;&#20272;&#35745;&#36827;&#34892;&#27169;&#24577;&#36716;&#25442;&#65292;&#36825;&#36890;&#24120;&#35745;&#31639;&#23494;&#38598;&#19988;&#38656;&#35201;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#30417;&#30563;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24555;&#36895;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#21644;&#28857;&#20113;&#32534;&#30721;&#20026;&#22320;&#28857;&#29305;&#24449;&#25551;&#36848;&#31526;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35270;&#22330;&#65288;FoV&#65289;&#36716;&#25442;&#27169;&#22359;&#65292;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#19982;&#22270;&#20687;&#31867;&#20284;&#30340;&#27169;&#24577;&#12290;&#35813;&#27169;&#22359;&#28040;&#38500;&#20102;&#28145;&#24230;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#24110;&#21161;&#21518;&#32493;&#27169;&#22359;&#23454;&#29616;&#23454;&#26102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18762v1 Announce Type: cross  Abstract: Place recognition is an important task for robots and autonomous cars to localize themselves and close loops in pre-built maps. While single-modal sensor-based methods have shown satisfactory performance, cross-modal place recognition that retrieving images from a point-cloud database remains a challenging problem. Current cross-modal methods transform images into 3D points using depth estimation for modality conversion, which are usually computationally intensive and need expensive labeled data for depth supervision. In this work, we introduce a fast and lightweight framework to encode images and point clouds into place-distinctive descriptors. We propose an effective Field of View (FoV) transformation module to convert point clouds into an analogous modality as images. This module eliminates the necessity for depth estimation and helps subsequent modules achieve real-time performance. We further design a non-negative factorization-ba
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#23556;&#32447;&#19978;&#35782;&#21035;&#20122;&#20020;&#24202;&#21160;&#33033;&#30828;&#21270;&#65292;&#20026;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18756</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#28145;&#24230;&#23398;&#20064;&#22312;&#33016;&#37096;X&#23556;&#32447;&#19978;&#26816;&#27979;&#20122;&#20020;&#24202;&#21160;&#33033;&#30828;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detection of subclinical atherosclerosis by image-based deep learning on chest x-ray
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#23556;&#32447;&#19978;&#35782;&#21035;&#20122;&#20020;&#24202;&#21160;&#33033;&#30828;&#21270;&#65292;&#20026;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#35782;&#21035;&#33016;&#37096;&#27491;&#20301;X&#23556;&#32447;&#19978;&#30340;&#20122;&#20020;&#24202;&#21160;&#33033;&#30828;&#21270;&#12290;&#36890;&#36807;460&#20363;&#21021;&#32423;&#39044;&#38450;&#24739;&#32773;&#65288;58.4%&#30007;&#24615;&#65292;&#20013;&#38388;&#24180;&#40836;63 [51-74]&#23681;&#65289;&#30340;&#33016;&#37096;X&#23556;&#32447;&#65288;80%&#35757;&#32451;&#38431;&#21015;&#65292;20%&#20869;&#37096;&#39564;&#35777;&#38431;&#21015;&#65289;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#20896;&#29366;&#21160;&#33033;&#38041;&#21270;&#65288;CAC&#65289;&#35780;&#20998;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65288;AI-CAC&#27169;&#22411;&#65289;&#65292;&#36825;&#20123;&#24739;&#32773;&#22312;&#20020;&#24202;&#21407;&#22240;&#19979;&#26377;&#21487;&#29992;&#30340;&#25104;&#23545;&#33016;&#37096;X&#23556;&#32447;&#21644;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#65292;&#32780;&#19988;&#36825;&#20004;&#39033;&#26816;&#26597;&#22312;3&#20010;&#26376;&#20869;&#23436;&#25104;&#12290;&#22312;&#26469;&#33258;&#30456;&#21516;&#26426;&#26500;&#30340;90&#21517;&#30149;&#20154;&#30340;&#26102;&#38388;&#29420;&#31435;&#38431;&#21015;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#65288;&#22806;&#37096;&#39564;&#35777;&#65289;&#12290;&#36890;&#36807;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#35780;&#20272;AI-CAC&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#20026;&#20027;&#35201;&#32467;&#26524;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#20013;&#20301;AI-CAC&#35780;&#20998;&#20026;35&#65288;0-388&#65289;&#65292;28.9%&#30340;&#24739;&#32773;&#27809;&#26377;AI-CAC&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18756v1 Announce Type: cross  Abstract: Aims. To develop a deep-learning based system for recognition of subclinical atherosclerosis on a plain frontal chest x-ray. Methods and Results. A deep-learning algorithm to predict coronary artery calcium (CAC) score (the AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20% internal validation cohort) of primary prevention patients (58.4% male, median age 63 [51-74] years) with available paired chest x-ray and chest computed tomography (CT) indicated for any clinical reason and performed within 3 months. The CAC score calculated on chest CT was used as ground truth. The model was validated on an temporally-independent cohort of 90 patients from the same institution (external validation). The diagnostic accuracy of the AI-CAC model assessed by the area under the curve (AUC) was the primary outcome. Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC. AUC of the AI-CAC model to identify a CA
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#26368;&#22823;&#21270;&#24433;&#21709;&#21644;&#26368;&#23567;&#21270;&#31181;&#23376;&#38598;&#22823;&#23567;&#30340;&#22522;&#30784;&#19978;&#65292;&#20248;&#21270;&#20102;&#22810;&#20010;IM&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#21253;&#25324;&#39044;&#31639;&#12289;&#20844;&#24179;&#24615;&#12289;&#31038;&#21306;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.18755</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#24433;&#21709;&#26368;&#22823;&#21270;&#65306;&#24179;&#34913;&#20256;&#25773;&#12289;&#39044;&#31639;&#12289;&#20844;&#24179;&#24615;&#21644;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Many-Objective Evolutionary Influence Maximization: Balancing Spread, Budget, Fairness, and Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18755
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#26368;&#22823;&#21270;&#24433;&#21709;&#21644;&#26368;&#23567;&#21270;&#31181;&#23376;&#38598;&#22823;&#23567;&#30340;&#22522;&#30784;&#19978;&#65292;&#20248;&#21270;&#20102;&#22810;&#20010;IM&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#21253;&#25324;&#39044;&#31639;&#12289;&#20844;&#24179;&#24615;&#12289;&#31038;&#21306;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#26368;&#22823;&#21270;&#65288;IM&#65289;&#38382;&#39064;&#26088;&#22312;&#21457;&#29616;&#22270;&#20013;&#21487;&#20197;&#26368;&#22823;&#31243;&#24230;&#20256;&#25773;&#20449;&#24687;&#20256;&#25773;&#30340;&#33410;&#28857;&#38598;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;NP&#38590;&#39064;&#65292;&#36890;&#24120;&#36890;&#36807;&#26368;&#22823;&#21270;&#24433;&#21709;&#65288;&#20256;&#25773;&#65289;&#20197;&#21450;&#36873;&#25321;&#24615;&#20248;&#21270;&#31532;&#20108;&#20010;&#30446;&#26631;&#65288;&#20363;&#22914;&#26368;&#23567;&#21270;&#31181;&#23376;&#38598;&#22823;&#23567;&#25110;&#26368;&#22823;&#21270;&#24433;&#21709;&#20844;&#24179;&#24615;&#65289;&#26469;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;IM&#38382;&#39064;&#30340;&#22810;&#20010;&#26041;&#38754;&#24517;&#39035;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31532;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20248;&#21270;&#20102;&#20960;&#20010;IM&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#21363;&#39044;&#31639;&#12289;&#20844;&#24179;&#24615;&#12289;&#31038;&#21306;&#21644;&#26102;&#38388;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#20256;&#25773;&#24433;&#21709;&#24182;&#26368;&#23567;&#21270;&#31181;&#23376;&#38598;&#22823;&#23567;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MOEIM&#65288;&#29992;&#20110;&#24433;&#21709;&#26368;&#22823;&#21270;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;NSGA-II&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEA&#65289;&#65292;&#32467;&#21512;&#20102;&#20855;&#26377;&#22270;&#24863;&#30693;&#24615;&#30340;&#31639;&#23376;&#21644;&#26234;&#33021;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18755v1 Announce Type: cross  Abstract: The Influence Maximization (IM) problem seeks to discover the set of nodes in a graph that can spread the information propagation at most. This problem is known to be NP-hard, and it is usually studied by maximizing the influence (spread) and, optionally, optimizing a second objective, such as minimizing the seed set size or maximizing the influence fairness. However, in many practical scenarios multiple aspects of the IM problem must be optimized at the same time. In this work, we propose a first case study where several IM-specific objective functions, namely budget, fairness, communities, and time, are optimized on top of the maximization of influence and minimization of the seed set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm (MOEA) based on NSGA-II incorporating graph-aware operators and a smart initialization. We compare MOEIM in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18742</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Learning Dynamics of Alignment with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#27169;&#22411;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#34429;&#28982;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29702;&#35770;&#19978;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#27491;&#24335;&#23637;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#29616;&#35937;&#65292;&#21363;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24403;&#20195;LLMs&#21644;&#23545;&#40784;&#20219;&#21153;&#19978;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24378;&#21270;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23545;&#40784;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#20813;&#36131;&#22768;&#26126;&#65306;&#26412;&#25991;&#21253;&#21547;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#26469;&#22686;&#24378;&#21046;&#36896;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#26080;&#20851;&#29305;&#24449;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20026;&#38477;&#20302;&#21046;&#36896;&#25104;&#26412;&#21644;&#26356;&#22909;&#29702;&#35299;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.18731</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22686;&#24378;&#21046;&#36896;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#26469;&#22686;&#24378;&#21046;&#36896;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#26080;&#20851;&#29305;&#24449;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20026;&#38477;&#20302;&#21046;&#36896;&#25104;&#26412;&#21644;&#26356;&#22909;&#29702;&#35299;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#26469;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#22312;&#39044;&#27979;&#38115;&#21066;&#36807;&#31243;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#21046;&#36896;&#19994;&#26696;&#20363;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#39318;&#20808;&#35757;&#32451;ML&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#35782;&#21035;&#24182;&#28040;&#38500;&#26080;&#20851;&#29305;&#24449;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#36807;&#31243;&#30340;&#23436;&#21892;&#32467;&#26524;&#23548;&#33268;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#20026;&#38477;&#20302;&#21046;&#36896;&#25104;&#26412;&#21644;&#26356;&#22909;&#29702;&#35299;&#35757;&#32451;&#21518;&#30340;ML&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#35813;&#30740;&#31350;&#31361;&#20986;&#20102;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#22312;&#35299;&#37322;&#21644;&#20248;&#21270;&#21046;&#36896;&#39046;&#22495;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#29992;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18731v1 Announce Type: new  Abstract: This research presents a method that utilizes explainability techniques to amplify the performance of machine learning (ML) models in forecasting the quality of milling processes, as demonstrated in this paper through a manufacturing use case. The methodology entails the initial training of ML models, followed by a fine-tuning phase where irrelevant features identified through explainability methods are eliminated. This procedural refinement results in performance enhancements, paving the way for potential reductions in manufacturing costs and a better understanding of the trained ML models. This study highlights the usefulness of explainability techniques in both explaining and optimizing predictive models in the manufacturing realm.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#20171;&#32461;&#20102;&#19968;&#31181;&#39564;&#35777;&#38543;&#26426;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#26816;&#39564;&#25216;&#26415;&#19982;RL&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#33021;&#22815;&#36890;&#36807;&#27169;&#22411;&#26816;&#39564;&#22120;&#39564;&#35777;&#30340;&#24418;&#24335;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18725</link><description>&lt;p&gt;
&#38543;&#26426;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#27010;&#29575;&#27169;&#22411;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Model Checking of Stochastic Reinforcement Learning Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18725
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#20171;&#32461;&#20102;&#19968;&#31181;&#39564;&#35777;&#38543;&#26426;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#26816;&#39564;&#25216;&#26415;&#19982;RL&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#33021;&#22815;&#36890;&#36807;&#27169;&#22411;&#26816;&#39564;&#22120;&#39564;&#35777;&#30340;&#24418;&#24335;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39564;&#35777;&#38543;&#26426;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#19982;&#20219;&#20309;RL&#31639;&#27861;&#20860;&#23481;&#65292;&#21482;&#35201;&#31639;&#27861;&#21450;&#20854;&#23545;&#24212;&#30340;&#29615;&#22659;&#20849;&#21516;&#36981;&#23432;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29615;&#22659;&#30340;&#26410;&#26469;&#29366;&#24577;&#24212;&#20165;&#21462;&#20915;&#20110;&#20854;&#24403;&#21069;&#29366;&#24577;&#21644;&#25191;&#34892;&#30340;&#21160;&#20316;&#65292;&#32780;&#19982;&#20219;&#20309;&#20808;&#21069;&#30340;&#29366;&#24577;&#25110;&#21160;&#20316;&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19968;&#31181;&#31216;&#20026;&#27169;&#22411;&#26816;&#39564;&#30340;&#39564;&#35777;&#25216;&#26415;&#19982;RL&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12289;&#35757;&#32451;&#21518;&#30340;RL&#31574;&#30053;&#21644;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#26500;&#24314;&#19968;&#20010;&#24418;&#24335;&#27169;&#22411;&#65292;&#38543;&#21518;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#26816;&#39564;&#22120;Storm&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#31216;&#20026;&#30830;&#23450;&#24615;&#23433;&#20840;&#20272;&#35745;&#21644;&#22825;&#30495;&#30340;&#25972;&#20307;&#27169;&#22411;&#26816;&#39564;&#30340;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#39564;&#35777;&#38543;&#26426;RL&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18725v1 Announce Type: new  Abstract: We introduce a method to verify stochastic reinforcement learning (RL) policies. This approach is compatible with any RL algorithm as long as the algorithm and its corresponding environment collectively adhere to the Markov property. In this setting, the future state of the environment should depend solely on its current state and the action executed, independent of any previous states or actions. Our method integrates a verification technique, referred to as model checking, with RL, leveraging a Markov decision process, a trained RL policy, and a probabilistic computation tree logic (PCTL) formula to build a formal model that can be subsequently verified via the model checker Storm. We demonstrate our method's applicability across multiple benchmarks, comparing it to baseline methods called deterministic safety estimates and naive monolithic model checking. Our results show that our method is suited to verify stochastic RL policies.
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.18717</link><description>&lt;p&gt;
&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning for Deep Causal Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18717
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#22238;&#31572;&#8220;&#22914;&#26524;$y$&#21464;&#20026;$z$&#65292;$x$&#20250;&#22914;&#20309;&#21464;&#21270;&#65311;&#8221;&#36825;&#31867;&#38382;&#39064;&#30340;&#27169;&#22411;&#23545;&#20110;&#25512;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#33021;&#22815;&#35299;&#20915;&#36825;&#31867;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30446;&#21069;&#35201;&#27714;&#25152;&#26377;&#30456;&#20851;&#21464;&#37327;&#22343;&#24050;&#34987;&#35266;&#23519;&#21040;&#65292;&#24182;&#19988;&#30456;&#24212;&#30340;&#26631;&#31614;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#29992;&#12290;&#25105;&#20204;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18717v1 Announce Type: cross  Abstract: Developing models that can answer questions of the form "How would $x$ change if $y$ had been $z$?" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;(ICD)&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#24187;&#35273;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#21644;&#25351;&#31034;&#25200;&#21160;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#27604;&#65292;&#20174;&#21407;&#22987;&#20998;&#24067;&#20013;&#20943;&#21435;&#24187;&#35273;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2403.18715</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;&#20943;&#36731;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18715
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;(ICD)&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#24187;&#35273;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#21644;&#25351;&#31034;&#25200;&#21160;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#27604;&#65292;&#20174;&#21407;&#22987;&#20998;&#24067;&#20013;&#20943;&#21435;&#24187;&#35273;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36234;&#26469;&#36234;&#25797;&#38271;&#20174;&#35270;&#35273;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#32454;&#33410;&#21644;&#36830;&#36143;&#24615;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#27169;&#24335;&#20915;&#31574;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#24212;&#29992;&#23427;&#20204;&#26102;&#65292;&#20854;&#24212;&#29992;&#21463;&#21040;&#24187;&#35273;&#30340;&#38459;&#30861;&#65292;&#21363;&#29983;&#25104;&#30340;&#25991;&#26412;&#19981;&#20934;&#30830;&#22320;&#20195;&#34920;&#20102;&#35270;&#35273;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;(ICD)&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;LVLM&#25512;&#26029;&#36807;&#31243;&#20013;&#20943;&#23569;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#25200;&#21160;&#25351;&#31034;&#26174;&#33879;&#21152;&#21095;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#20013;&#30340;&#24187;&#35273;&#30340;&#21551;&#21457;&#12290;ICD&#23545;&#26631;&#20934;&#21644;&#25351;&#31034;&#25200;&#21160;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#27604;&#65292;&#20174;&#32780;&#22686;&#21152;&#23545;&#40784;&#19981;&#30830;&#23450;&#24615;&#24182;&#26377;&#25928;&#22320;&#20174;&#21407;&#22987;&#20998;&#24067;&#20013;&#20943;&#21435;&#24187;&#35273;&#27010;&#24565;&#12290;&#36890;&#36807;&#22312;&#21028;&#21035;&#22522;&#20934;(POPE&#21644;MME)&#21644;&#29983;&#25104;&#22522;&#20934;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18715v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generativ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAT-NGP&#30340;&#27169;&#22411;&#65292;&#22312;&#22810;&#26085;&#26399;&#21355;&#26143;&#22270;&#20687;&#30340;&#19977;&#32500;&#37325;&#24314;&#20013;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37319;&#26679;&#31574;&#30053;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32534;&#30721;&#65292;&#23558;&#23398;&#20064;&#26102;&#38388;&#32553;&#30701;&#33267;15&#20998;&#38047;&#65292;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18711</link><description>&lt;p&gt;
SAT-NGP: &#37322;&#25918;&#31070;&#32463;&#22270;&#24418;&#22522;&#20803;&#65292;&#20174;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#21487;&#37325;&#20809;&#26080;&#26242;&#21464;&#30340;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable Transient-Free 3D reconstruction from Satellite Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18711
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAT-NGP&#30340;&#27169;&#22411;&#65292;&#22312;&#22810;&#26085;&#26399;&#21355;&#26143;&#22270;&#20687;&#30340;&#19977;&#32500;&#37325;&#24314;&#20013;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37319;&#26679;&#31574;&#30053;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32534;&#30721;&#65292;&#23558;&#23398;&#20064;&#26102;&#38388;&#32553;&#30701;&#33267;15&#20998;&#38047;&#65292;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#22810;&#23545;&#25110;&#19977;&#20803;&#21355;&#26143;&#22270;&#20687;&#26102;&#65292;&#24403;&#21069;&#30340;&#31435;&#20307;&#35270;&#35273;&#27969;&#27700;&#32447;&#22312;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#19977;&#32500;&#37325;&#24314;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27969;&#27700;&#32447;&#23545;&#22270;&#20687;&#20043;&#38388;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#20123;&#21464;&#21270;&#21487;&#33021;&#26159;&#30001;&#20110;&#22810;&#26085;&#26399;&#33719;&#21462;&#23548;&#33268;&#30340;&#12290;&#36825;&#31181;&#21464;&#21270;&#20027;&#35201;&#26159;&#30001;&#20110;&#21464;&#21270;&#30340;&#38452;&#24433;&#12289;&#21453;&#23556;&#21644;&#30636;&#21464;&#29289;&#20307;&#65288;&#36710;&#36742;&#12289;&#26893;&#34987;&#65289;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#32771;&#34385;&#36825;&#20123;&#21464;&#21270;&#65292;&#26368;&#36817;&#24050;&#23558;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#24212;&#29992;&#20110;&#22810;&#26085;&#26399;&#21355;&#26143;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#26041;&#27861;&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#38656;&#35201;&#25968;&#21313;&#23567;&#26102;&#30340;&#23398;&#20064;&#26102;&#38388;&#65292;&#32780;&#26631;&#20934;&#31435;&#20307;&#35270;&#35273;&#27969;&#27700;&#32447;&#21482;&#38656;&#20960;&#20998;&#38047;&#12290;&#26681;&#25454;&#21363;&#26102;&#31070;&#32463;&#22270;&#24418;&#22522;&#20803;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#39640;&#25928;&#30340;&#37319;&#26679;&#31574;&#30053;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32534;&#30721;&#26469;&#21152;&#36895;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#21355;&#26143;&#31070;&#32463;&#22270;&#24418;&#22522;&#20803;&#65288;SAT-NGP&#65289;&#23558;&#23398;&#20064;&#26102;&#38388;&#32553;&#30701;&#33267;15&#20998;&#38047;&#65292;&#21516;&#26102;&#20445;&#25345;&#19977;&#32500;&#37325;&#24314;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18711v1 Announce Type: cross  Abstract: Current stereo-vision pipelines produce high accuracy 3D reconstruction when using multiple pairs or triplets of satellite images. However, these pipelines are sensitive to the changes between images that can occur as a result of multi-date acquisitions. Such variations are mainly due to variable shadows, reflexions and transient objects (cars, vegetation). To take such changes into account, Neural Radiance Fields (NeRF) have recently been applied to multi-date satellite imagery. However, Neural methods are very compute-intensive, taking dozens of hours to learn, compared with minutes for standard stereo-vision pipelines. Following the ideas of Instant Neural Graphics Primitives we propose to use an efficient sampling strategy and multi-resolution hash encoding to accelerate the learning. Our model, Satellite Neural Graphics Primitives (SAT-NGP) decreases the learning time to 15 minutes while maintaining the quality of the 3D reconstru
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#31216;&#20026;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;</title><link>https://arxiv.org/abs/2403.18699</link><description>&lt;p&gt;
&#20855;&#26377;&#27491;&#20132;&#38170;&#28857;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;CLOA&#65289;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning with Orthonormal Anchors (CLOA)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#31216;&#20026;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#26816;&#26597;InfoNCE&#25439;&#22833;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#65292;&#21363;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#34920;&#29616;&#20986;&#38480;&#21046;&#24615;&#34892;&#20026;&#65292;&#23548;&#33268;&#23884;&#20837;&#36235;&#20110;&#34701;&#21512;&#20026;&#19968;&#20010;&#22855;&#24322;&#28857;&#30340;&#25910;&#25947;&#29616;&#35937;&#12290;&#36825;&#31181;&#8220;&#36807;&#24230;&#34701;&#21512;&#8221;&#25928;&#24212;&#23545;&#21518;&#32493;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23884;&#20837;&#22312;&#31561;&#20110;&#25110;&#23616;&#38480;&#20110;&#31209;-1&#32447;&#24615;&#23376;&#31354;&#38388;&#26102;&#34920;&#31034;InfoNCE&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#19982;&#24494;&#35843;&#38454;&#27573;&#20856;&#22411;&#20351;&#29992;&#30340;&#30456;&#21516;&#25110;&#26356;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#26088;&#22312;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#27599;&#20010;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18699v1 Announce Type: cross  Abstract: This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives. We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point. This "over-fusion" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks. Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding 
&lt;/p&gt;</description></item><item><title>Annolid&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#26088;&#22312;&#23545;&#35270;&#39057;&#25991;&#20214;&#20013;&#30340;&#30740;&#31350;&#30446;&#26631;&#36827;&#34892;&#20998;&#21106;&#12289;&#26631;&#35760;&#21644;&#36319;&#36394;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21160;&#29289;&#34892;&#20026;&#20998;&#26512;&#19978;&#65292;&#36890;&#36807;&#24377;&#24615;&#12289;&#26080;&#26631;&#35760;&#36319;&#36394;&#22810;&#20010;&#21160;&#29289;&#24182;&#36890;&#36807;&#25991;&#26412;&#21629;&#20196;&#33258;&#21160;&#36974;&#32617;&#21644;&#20998;&#21106;&#21487;&#35782;&#21035;&#30340;&#21160;&#29289;&#21644;&#29289;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.18690</link><description>&lt;p&gt;
Annolid: Annotate, Segment, and Track Anything You Need
&lt;/p&gt;
&lt;p&gt;
Annolid: Annotate, Segment, and Track Anything You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18690
&lt;/p&gt;
&lt;p&gt;
Annolid&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#26088;&#22312;&#23545;&#35270;&#39057;&#25991;&#20214;&#20013;&#30340;&#30740;&#31350;&#30446;&#26631;&#36827;&#34892;&#20998;&#21106;&#12289;&#26631;&#35760;&#21644;&#36319;&#36394;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21160;&#29289;&#34892;&#20026;&#20998;&#26512;&#19978;&#65292;&#36890;&#36807;&#24377;&#24615;&#12289;&#26080;&#26631;&#35760;&#36319;&#36394;&#22810;&#20010;&#21160;&#29289;&#24182;&#36890;&#36807;&#25991;&#26412;&#21629;&#20196;&#33258;&#21160;&#36974;&#32617;&#21644;&#20998;&#21106;&#21487;&#35782;&#21035;&#30340;&#21160;&#29289;&#21644;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Annolid&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#26088;&#22312;&#23545;&#35270;&#39057;&#25991;&#20214;&#20013;&#30340;&#30740;&#31350;&#30446;&#26631;&#36827;&#34892;&#20998;&#21106;&#12289;&#26631;&#35760;&#21644;&#36319;&#36394;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21160;&#29289;&#34892;&#20026;&#20998;&#26512;&#19978;&#12290;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;Annolid&#29616;&#22312;&#21033;&#29992;Cutie&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#27169;&#22411;&#23454;&#29616;&#20174;&#21333;&#20010;&#27880;&#37322;&#24103;&#20013;&#23454;&#29616;&#22810;&#20010;&#21160;&#29289;&#30340;&#24377;&#24615;&#12289;&#26080;&#26631;&#35760;&#36319;&#36394;&#65292;&#21363;&#20351;&#23427;&#20204;&#21487;&#33021;&#37096;&#20998;&#25110;&#23436;&#20840;&#34987;&#29615;&#22659;&#29305;&#24449;&#25110;&#24444;&#27492;&#36974;&#25377;&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#8220;Segment Anything&#8221;&#21644;&#8220;Grounding-DINO&#8221;&#31574;&#30053;&#65292;&#36890;&#36807;&#25991;&#26412;&#21629;&#20196;&#33258;&#21160;&#36974;&#32617;&#21644;&#20998;&#21106;&#21487;&#35782;&#21035;&#30340;&#21160;&#29289;&#21644;&#29289;&#20307;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#35201;&#12290;Annolid&#30340;&#32508;&#21512;&#23545;&#35937;&#20998;&#21106;&#26041;&#27861;&#28789;&#27963;&#36866;&#24212;&#24191;&#27867;&#30340;&#34892;&#20026;&#20998;&#26512;&#24212;&#29992;&#65292;&#33021;&#22815;&#23545;&#19981;&#21516;&#30340;&#34892;&#20026;&#29366;&#24577;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#8220;freezing&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18690v1 Announce Type: cross  Abstract: Annolid is a deep learning-based software package designed for the segmentation, labeling, and tracking of research targets within video files, focusing primarily on animal behavior analysis. Based on state-of-the-art instance segmentation methods, Annolid now harnesses the Cutie video object segmentation model to achieve resilient, markerless tracking of multiple animals from single annotated frames, even in environments in which they may be partially or entirely concealed by environmental features or by one another. Our integration of Segment Anything and Grounding-DINO strategies additionally enables the automatic masking and segmentation of recognizable animals and objects by text command, removing the need for manual annotation. Annolid's comprehensive approach to object segmentation flexibly accommodates a broad spectrum of behavior analysis applications, enabling the classification of diverse behavioral states such as freezing, 
&lt;/p&gt;</description></item><item><title>TransFusion&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#23450;&#20041;&#20102;&#23545;&#27604;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#20197;&#25913;&#21892;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.18681</link><description>&lt;p&gt;
TransFusion&#65306;&#20855;&#26377;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TransFusion: Contrastive Learning with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18681
&lt;/p&gt;
&lt;p&gt;
TransFusion&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#23450;&#20041;&#20102;&#23545;&#27604;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#20197;&#25913;&#21892;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;TransFusion&#65292;&#26088;&#22312;&#20351;&#23545;&#27604;&#23398;&#20064;&#30340;&#36807;&#31243;&#26356;&#20855;&#20998;&#26512;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290; TransFusion&#30001;&#27880;&#24847;&#21147;&#22359;&#32452;&#25104;&#65292;&#20854;&#20013;&#30340;softmax&#34987;&#26367;&#25442;&#20026;ReLU&#65292;&#24182;&#19988;&#20854;&#26368;&#32456;&#22359;&#30340;&#21152;&#26435;&#21644;&#25805;&#20316;&#34987;&#25130;&#26029;&#65292;&#20197;&#20351;&#37051;&#25509;&#30697;&#38453;&#25104;&#20026;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36755;&#20986;&#19982;&#30446;&#26631;&#20851;&#32852;&#30697;&#38453;&#20043;&#38388;&#30340;Jensen-Shannon&#25955;&#24230;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#30697;&#38453;&#25351;&#31034;&#27599;&#23545;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30456;&#21516;&#31867;&#21035;&#25110;&#19981;&#21516;&#31867;&#21035;&#12290; TransFusion&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23450;&#20041;&#20102;&#22238;&#31572;&#35813;&#39046;&#22495;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65306;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#22823;&#32423;&#21035;&#21644;&#26377;&#25928;&#23545;&#27604;&#23398;&#20064;&#25152;&#38656;&#30340;&#26368;&#23567;&#25209;&#37327;&#22823;&#23567;&#12290; &#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TransFusion&#25104;&#21151;&#22320;&#25552;&#21462;&#20986;&#33021;&#22815;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#20998;&#31163;&#38598;&#32676;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18681v1 Announce Type: cross  Abstract: This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning. Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19982;&#20020;&#24202;&#32972;&#26223;&#30456;&#19968;&#33268;&#30340;&#26032;&#39062;&#29983;&#21629;&#20307;&#24449;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#65292;&#36890;&#36807;&#25429;&#25417;&#19982;&#20020;&#24202;&#35268;&#33539;&#30340;&#20559;&#24046;&#12289;&#25972;&#20307;&#36235;&#21183;&#21644;&#36235;&#21183;&#20559;&#24046;&#65292;&#20026;&#26089;&#26399;&#21457;&#29616;&#19981;&#33391;&#20107;&#20214;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.18668</link><description>&lt;p&gt;
&#20197;&#30456;&#20851;&#24615;&#20026;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Aiming for Relevance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18668
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19982;&#20020;&#24202;&#32972;&#26223;&#30456;&#19968;&#33268;&#30340;&#26032;&#39062;&#29983;&#21629;&#20307;&#24449;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#65292;&#36890;&#36807;&#25429;&#25417;&#19982;&#20020;&#24202;&#35268;&#33539;&#30340;&#20559;&#24046;&#12289;&#25972;&#20307;&#36235;&#21183;&#21644;&#36235;&#21183;&#20559;&#24046;&#65292;&#20026;&#26089;&#26399;&#21457;&#29616;&#19981;&#33391;&#20107;&#20214;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#65292;&#29983;&#21629;&#20307;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#20204;&#29992;&#20110;&#36319;&#36394;&#24739;&#32773;&#30340;&#29366;&#24577;&#65292;&#24182;&#35782;&#21035;&#20020;&#24202;&#19978;&#26174;&#33879;&#30340;&#21464;&#21270;&#12290;&#39044;&#27979;&#29983;&#21629;&#20307;&#24449;&#36712;&#36857;&#23545;&#20110;&#26089;&#26399;&#21457;&#29616;&#19981;&#33391;&#20107;&#20214;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25351;&#26631;&#22914;RMSE&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#36825;&#20123;&#39044;&#27979;&#30340;&#30495;&#27491;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#29983;&#21629;&#20307;&#24449;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#65292;&#19982;&#20020;&#24202;&#32972;&#26223;&#30456;&#19968;&#33268;&#65292;&#20851;&#27880;&#19982;&#20020;&#24202;&#35268;&#33539;&#30340;&#20559;&#24046;&#12289;&#25972;&#20307;&#36235;&#21183;&#21644;&#36235;&#21183;&#20559;&#24046;&#12290;&#36825;&#20123;&#25351;&#26631;&#28304;&#33258;&#36890;&#36807;&#19982;ICU&#20020;&#24202;&#21307;&#29983;&#30340;&#35775;&#35848;&#33719;&#24471;&#30340;&#23454;&#35777;&#25928;&#29992;&#26354;&#32447;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;MIMIC&#21644;eICU&#65289;&#39564;&#35777;&#20102;&#36825;&#20123;&#25351;&#26631;&#30340;&#26377;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#25351;&#26631;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#22312;&#39044;&#27979;&#20020;&#24202;&#37325;&#35201;&#20107;&#20214;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#27169;&#22411;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#20020;&#24202;&#23454;&#36341;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18668v1 Announce Type: cross  Abstract: Vital signs are crucial in intensive care units (ICUs). They are used to track the patient's state and to identify clinically significant changes. Predicting vital sign trajectories is valuable for early detection of adverse events. However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions. We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations. These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians. We validate the metrics' usefulness using simulated and real clinical datasets (MIMIC and eICU). Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events. This research paves the way for clin
&lt;/p&gt;</description></item><item><title>INEXA&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#12289;&#21487;&#35299;&#37322;&#30340;&#36807;&#31243;&#27169;&#22411;&#25277;&#35937;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25506;&#32034;&#21644;&#29702;&#35299;&#21457;&#29616;&#30340;&#36807;&#31243;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18659</link><description>&lt;p&gt;
INEXA: &#36890;&#36807;&#38754;&#21521;&#23545;&#35937;&#30340;&#36807;&#31243;&#25366;&#25496;&#23454;&#29616;&#20132;&#20114;&#21644;&#21487;&#35299;&#37322;&#30340;&#36807;&#31243;&#27169;&#22411;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
INEXA: Interactive and Explainable Process Model Abstraction Through Object-Centric Process Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18659
&lt;/p&gt;
&lt;p&gt;
INEXA&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#12289;&#21487;&#35299;&#37322;&#30340;&#36807;&#31243;&#27169;&#22411;&#25277;&#35937;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25506;&#32034;&#21644;&#29702;&#35299;&#21457;&#29616;&#30340;&#36807;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#20449;&#24687;&#31995;&#32479;&#35760;&#24405;&#30340;&#36807;&#31243;&#20107;&#20214;&#20855;&#26377;&#19981;&#21516;&#30340;&#31890;&#24230;&#32423;&#21035;&#12290;&#26681;&#25454;&#29983;&#25104;&#30340;&#20107;&#20214;&#26085;&#24535;&#65292;&#22312;&#19981;&#21516;&#30340;&#31890;&#24230;&#32423;&#21035;&#19978;&#21457;&#29616;&#36807;&#31243;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#23384;&#20648;&#22312;&#32454;&#31890;&#24230;&#32423;&#21035;&#30340;&#20107;&#20214;&#21487;&#33021;&#30001;&#20110;&#32467;&#26524;&#27169;&#22411;&#20803;&#32032;&#36807;&#22810;&#32780;&#22952;&#30861;&#26174;&#31034;&#25152;&#21457;&#29616;&#30340;&#36807;&#31243;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#30495;&#23454;&#21046;&#36896;&#36807;&#31243;&#30340;&#21457;&#29616;&#36807;&#31243;&#27169;&#22411;&#30001;1,489&#20010;&#27169;&#22411;&#20803;&#32032;&#21644;2,000&#22810;&#20010;&#24359;&#32452;&#25104;&#12290;&#29616;&#26377;&#30340;&#36807;&#31243;&#27169;&#22411;&#25277;&#35937;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#20294;&#20250;&#23558;&#20854;&#19982;&#22522;&#30784;&#20107;&#20214;&#26085;&#24535;&#26029;&#24320;&#32852;&#31995;&#12290;&#29616;&#26377;&#30340;&#20107;&#20214;&#25277;&#35937;&#25216;&#26415;&#26082;&#19981;&#25903;&#25345;&#28151;&#21512;&#31890;&#24230;&#32423;&#21035;&#30340;&#20998;&#26512;&#65292;&#20063;&#19981;&#25903;&#25345;&#20132;&#20114;&#24335;&#25506;&#32034;&#21512;&#36866;&#30340;&#31890;&#24230;&#32423;&#21035;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25506;&#32034;&#21457;&#29616;&#30340;&#36807;&#31243;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;INEXA&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#12289;&#21487;&#35299;&#37322;&#30340;&#36807;&#31243;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18659v1 Announce Type: new  Abstract: Process events are recorded by multiple information systems at different granularity levels. Based on the resulting event logs, process models are discovered at different granularity levels, as well. Events stored at a fine-grained granularity level, for example, may hinder the discovered process model to be displayed due the high number of resulting model elements. The discovered process model of a real-world manufacturing process, for example, consists of 1,489 model elements and over 2,000 arcs. Existing process model abstraction techniques could help reducing the size of the model, but would disconnect it from the underlying event log. Existing event abstraction techniques do neither support the analysis of mixed granularity levels, nor interactive exploration of a suitable granularity level. To enable the exploration of discovered process models at different granularity levels, we propose INEXA, an interactive, explainable process m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#26102;&#38388;&#20998;&#21106;&#22797;&#29992;&#27010;&#24565;&#30340;Spikewhisper&#65292;&#20801;&#35768;&#25915;&#20987;&#32773;&#22312;&#32852;&#21512;&#31070;&#32463;&#24418;&#24577;&#23398;&#23398;&#20064;&#31995;&#32479;&#20013;&#23454;&#26045;&#38590;&#20197;&#34987;&#26816;&#27979;&#30340;&#26031;&#27966;&#20811;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.18607</link><description>&lt;p&gt;
Spikewhisper&#65306;&#22522;&#20110;&#26102;&#38388;&#30340;&#26031;&#27966;&#20811;&#21518;&#38376;&#25915;&#20987;&#23545;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#30340;&#32852;&#21512;&#31070;&#32463;&#24418;&#24577;&#23398;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#26102;&#38388;&#20998;&#21106;&#22797;&#29992;&#27010;&#24565;&#30340;Spikewhisper&#65292;&#20801;&#35768;&#25915;&#20987;&#32773;&#22312;&#32852;&#21512;&#31070;&#32463;&#24418;&#24577;&#23398;&#23398;&#20064;&#31995;&#32479;&#20013;&#23454;&#26045;&#38590;&#20197;&#34987;&#26816;&#27979;&#30340;&#26031;&#27966;&#20811;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#31070;&#32463;&#24418;&#24577;&#23398;&#23398;&#20064;&#65288;FedNL&#65289;&#21033;&#29992;&#20107;&#20214;&#39537;&#21160;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#26469;&#26377;&#25928;&#25191;&#34892;&#26234;&#33021;&#20998;&#26512;&#20219;&#21153;&#65292;&#28041;&#21450;&#20998;&#24067;&#24335;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#21516;&#26102;&#20063;&#23481;&#26131;&#21463;&#21040;&#27602;&#21270;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20256;&#32479;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#36890;&#24120;&#26469;&#33258;&#20110;&#26102;&#38388;&#19981;&#21464;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;FedNL&#20013;&#65292;&#26410;&#30693;&#30340;&#23041;&#32961;&#21487;&#33021;&#38544;&#34255;&#22312;&#26102;&#21464;&#30340;&#33033;&#20914;&#20449;&#21495;&#20013;&#12290;&#26412;&#25991;&#24320;&#22987;&#25506;&#32034;&#22522;&#20110;FedNL&#31995;&#32479;&#27010;&#24565;&#30340;&#19968;&#31181;&#26032;&#22411;&#33030;&#24369;&#24615;&#65292;&#31216;&#20026;&#8220;Spikewhisper&#8221;&#65292;&#35813;&#27010;&#24565;&#21033;&#29992;&#26102;&#38388;&#20998;&#21106;&#22797;&#29992;&#65292;&#20801;&#35768;&#25915;&#20987;&#32773;&#23613;&#21487;&#33021;&#35268;&#36991;&#26816;&#27979;&#65292;&#22240;&#20026;&#22810;&#20010;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#20197;&#22312;&#19981;&#21516;&#26102;&#38388;&#29255;&#27573;&#20351;&#29992;&#19981;&#21516;&#30340;&#35302;&#21457;&#22120;&#19981;&#34987;&#23519;&#35273;&#22320;&#36827;&#34892;&#27602;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;Spikewhisper&#30340;&#38544;&#34109;&#24615;&#28304;&#33258;&#20110;&#20840;&#23616;&#35302;&#21457;&#22120;&#30340;&#26102;&#22495;&#21487;&#20998;&#21106;&#24615;&#65292;&#27599;&#20010;&#24694;&#24847;&#23458;&#25143;&#31471;&#20165;&#31896;&#36148;&#19968;&#20010;&#26412;&#22320;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18607v1 Announce Type: cross  Abstract: Federated neuromorphic learning (FedNL) leverages event-driven spiking neural networks and federated learning frameworks to effectively execute intelligent analysis tasks over amounts of distributed low-power devices but also perform vulnerability to poisoning attacks. The threat of backdoor attacks on traditional deep neural networks typically comes from time-invariant data. However, in FedNL, unknown threats may be hidden in time-varying spike signals. In this paper, we start to explore a novel vulnerability of FedNL-based systems with the concept of time division multiplexing, termed Spikewhisper, which allows attackers to evade detection as much as possible, as multiple malicious clients can imperceptibly poison with different triggers at different timeslices. In particular, the stealthiness of Spikewhisper is derived from the time-domain divisibility of global triggers, in which each malicious client pastes only one local trigger 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#38469;&#35774;&#32622;&#65292;&#31216;&#20026;&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#33258;&#36866;&#24212;&#31243;&#24207;&#35268;&#21010;&#65292;&#20811;&#26381;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#27493;&#39588;&#38271;&#24230;&#21464;&#21270;&#30340;&#27169;&#22411;&#19981;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12289;&#29702;&#35299;&#27493;&#39588;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#30693;&#35782;&#23545;&#20110;&#29983;&#25104;&#21512;&#29702;&#19988;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#20197;&#21450;&#29992;&#27493;&#39588;&#32423;&#26631;&#31614;&#25110;&#24207;&#21015;&#32423;&#26631;&#31614;&#26631;&#27880;&#25351;&#23548;&#35270;&#39057;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.18600</link><description>&lt;p&gt;
RAP&#65306;&#26816;&#32034;&#22686;&#24378;&#22411;&#35268;&#21010;&#22120;&#29992;&#20110;&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#33258;&#36866;&#24212;&#31243;&#24207;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18600
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#38469;&#35774;&#32622;&#65292;&#31216;&#20026;&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#33258;&#36866;&#24212;&#31243;&#24207;&#35268;&#21010;&#65292;&#20811;&#26381;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#27493;&#39588;&#38271;&#24230;&#21464;&#21270;&#30340;&#27169;&#22411;&#19981;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12289;&#29702;&#35299;&#27493;&#39588;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#30693;&#35782;&#23545;&#20110;&#29983;&#25104;&#21512;&#29702;&#19988;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#20197;&#21450;&#29992;&#27493;&#39588;&#32423;&#26631;&#31614;&#25110;&#24207;&#21015;&#32423;&#26631;&#31614;&#26631;&#27880;&#25351;&#23548;&#35270;&#39057;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#31243;&#24207;&#35268;&#21010;&#28041;&#21450;&#26681;&#25454;&#21021;&#22987;&#21644;&#30446;&#26631;&#29366;&#24577;&#30340;&#35270;&#35273;&#35266;&#23519;&#29983;&#25104;&#19968;&#31995;&#21015;&#21160;&#20316;&#27493;&#39588;&#12290;&#23613;&#31649;&#36825;&#19968;&#20219;&#21153;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#65306;&#65288;1&#65289;&#33258;&#36866;&#24212;&#31243;&#24207;&#65306;&#20808;&#21069;&#30340;&#24037;&#20316;&#23384;&#22312;&#19968;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#21160;&#20316;&#27493;&#39588;&#30340;&#25968;&#37327;&#26159;&#24050;&#30693;&#19988;&#22266;&#23450;&#30340;&#65292;&#23548;&#33268;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27493;&#39588;&#38271;&#24230;&#21464;&#21270;&#30340;&#27169;&#22411;&#19981;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#65288;2&#65289;&#26102;&#38388;&#20851;&#31995;&#65306;&#29702;&#35299;&#27493;&#39588;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#30693;&#35782;&#23545;&#20110;&#29983;&#25104;&#21512;&#29702;&#19988;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#65288;3&#65289;&#27880;&#37322;&#25104;&#26412;&#65306;&#29992;&#27493;&#39588;&#32423;&#26631;&#31614;&#65288;&#21363;&#26102;&#38388;&#25139;&#65289;&#25110;&#24207;&#21015;&#32423;&#26631;&#31614;&#65288;&#21363;&#21160;&#20316;&#31867;&#21035;&#65289;&#26631;&#27880;&#25351;&#23548;&#35270;&#39057;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#38469;&#35774;&#32622;&#65292;&#31216;&#20026;&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#33258;&#36866;&#24212;&#31243;&#24207;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18600v1 Announce Type: cross  Abstract: Procedure Planning in instructional videos entails generating a sequence of action steps based on visual observations of the initial and target states. Despite the rapid progress in this task, there remain several critical challenges to be solved: (1) Adaptive procedures: Prior works hold an unrealistic assumption that the number of action steps is known and fixed, leading to non-generalizable models in real-world scenarios where the sequence length varies. (2) Temporal relation: Understanding the step temporal relation knowledge is essential in producing reasonable and executable plans. (3) Annotation cost: Annotating instructional videos with step-level labels (i.e., timestamp) or sequence-level labels (i.e., action category) is demanding and labor-intensive, limiting its generalizability to large-scale datasets.In this work, we propose a new and practical setting, called adaptive procedure planning in instructional videos, where the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;&#35821;&#20041;&#29420;&#31435;&#21306;&#22495;&#65288;SIRs&#65289;&#24182;&#35774;&#35745;HOmogeneous&#35270;&#35273;tOKenizer (HOOK)&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#22522;&#26412;&#20803;&#32032;&#26469;&#21152;&#24378;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.18593</link><description>&lt;p&gt;
&#22343;&#21248;&#20998;&#35789;&#22120;&#30340;&#37325;&#35201;&#24615;&#65306;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#30340;&#22343;&#21248;&#35270;&#35273;&#20998;&#35789;&#22120;
&lt;/p&gt;
&lt;p&gt;
Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18593
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#35821;&#20041;&#29420;&#31435;&#21306;&#22495;&#65288;SIRs&#65289;&#24182;&#35774;&#35745;HOmogeneous&#35270;&#35273;tOKenizer (HOOK)&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#22522;&#26412;&#20803;&#32032;&#26469;&#21152;&#24378;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#22120;&#20316;&#20026;&#22823;&#22411;&#27169;&#22411;&#30340;&#22522;&#26412;&#32452;&#20214;&#20043;&#19968;&#65292;&#38271;&#26399;&#20197;&#26469;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#34987;&#24573;&#35270;&#29978;&#33267;&#35823;&#35299;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#33258;&#28982;&#35821;&#35328;&#26631;&#35760;&#22120;&#21033;&#29992;&#26377;&#24847;&#20041;&#30340;&#35789;&#25110;&#23376;&#35789;&#20316;&#20026;&#35821;&#35328;&#30340;&#22522;&#26412;&#20803;&#32032;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20197;&#22522;&#20110;&#34917;&#19969;&#30340;&#26041;&#27861;&#22914;Patch Embed&#20026;&#20195;&#34920;&#30340;&#20027;&#27969;&#35270;&#35273;&#26631;&#35760;&#22120;&#20381;&#36182;&#20110;&#26080;&#24847;&#20041;&#30340;&#30697;&#24418;&#34917;&#19969;&#20316;&#20026;&#35270;&#35273;&#30340;&#22522;&#26412;&#20803;&#32032;&#65292;&#36825;&#19981;&#33021;&#20687;&#35821;&#35328;&#20013;&#30340;&#35789;&#25110;&#23376;&#35789;&#19968;&#26679;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#12290;&#20174;&#26631;&#35760;&#22120;&#30340;&#26412;&#36136;&#20986;&#21457;&#65292;&#25105;&#20204;&#20026;&#35270;&#35273;&#23450;&#20041;&#20102;&#35821;&#20041;&#29420;&#31435;&#21306;&#22495;&#65288;SIRs&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;HOmogeneous&#35270;&#35273;tOKenizer: HOOK&#12290;HOOK&#20027;&#35201;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#29289;&#20307;&#24863;&#30693;&#27169;&#22359;&#65288;OPM&#65289;&#21644;&#29289;&#20307;&#30690;&#37327;&#21270;&#27169;&#22359;&#65288;OVM&#65289;&#12290;&#20026;&#23454;&#29616;&#22343;&#21248;&#24615;&#65292;OPM&#23558;&#22270;&#20687;&#20998;&#21106;&#20026;4*4&#20687;&#32032;&#31181;&#23376;&#65292;&#28982;&#21518;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18593v1 Announce Type: cross  Abstract: The tokenizer, as one of the fundamental components of large models, has long been overlooked or even misunderstood in visual tasks. One key factor of the great comprehension power of the large language model is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language. In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot serve as effectively as words or subwords in language. Starting from the essence of the tokenizer, we defined semantically independent regions (SIRs) for vision. We designed a simple HOmogeneous visual tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity, the OPM splits the image into 4*4 pixel seeds and then utilizes the attention mechanism to pe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27700;&#37197;&#31995;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#27700;&#21147;&#21407;&#29702;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#37325;&#24314;&#27700;&#21147;&#29366;&#24577;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.18570</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27700;&#37197;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Graph Neural Networks for Water Distribution Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27700;&#37197;&#31995;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#27700;&#21147;&#21407;&#29702;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#37325;&#24314;&#27700;&#21147;&#29366;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#37197;&#31995;&#32479;&#65288;WDS&#65289;&#26159;&#22478;&#24066;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#12290;&#30001;&#20110;&#19990;&#30028;70%&#30340;&#20154;&#21475;&#21487;&#33021;&#20250;&#22312;2050&#24180;&#29983;&#27963;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#22240;&#27492;&#23545;&#20110;WDS&#30340;&#39640;&#25928;&#20223;&#30495;&#21644;&#35268;&#21010;&#24037;&#20855;&#22312;&#23454;&#29616;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDG&#65289;6 - &#8220;&#20026;&#25152;&#26377;&#20154;&#25552;&#20379;&#28165;&#27905;&#27700;&#21644;&#21355;&#29983;&#35774;&#26045;&#8221;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#20223;&#30495;&#22120;&#65292;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#26159;&#19968;&#20010;&#29992;&#20110;WDS&#20013;&#30340;&#27700;&#21147;&#29366;&#24577;&#20272;&#35745;&#30340;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#31181;&#36882;&#24402;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#23618;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#27700;&#21147;&#21407;&#29702;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#25512;&#26029;&#20986;&#20004;&#20010;&#39069;&#22806;&#30340;&#27700;&#21147;&#29366;&#24577;&#29305;&#24449;&#65292;&#20174;&#32780;&#37325;&#24314;&#21487;&#29992;&#30340;&#22320;&#38754;&#23454;&#20917;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18570v1 Announce Type: cross  Abstract: Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - "Clean water and sanitation for all". In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner. To the best of our k
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;PDNGraph&#22270;&#32467;&#26500;&#21644;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;PDNNet&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;PDN&#32467;&#26500;&#21644;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;IR&#25481;&#30005;&#12290;</title><link>https://arxiv.org/abs/2403.18569</link><description>&lt;p&gt;
PDNNet&#65306;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;PDNGraph&#22270;&#32467;&#26500;&#21644;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;PDNNet&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;PDN&#32467;&#26500;&#21644;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;IR&#25481;&#30005;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#28304;&#20379;&#24212;&#32593;&#32476;&#65288;PDN&#65289;&#19978;&#30340;IR&#25481;&#30005;&#19982;PDN&#30340;&#37197;&#32622;&#21644;&#30005;&#27969;&#28040;&#32791;&#23494;&#20999;&#30456;&#20851;&#12290;&#38543;&#30528;&#38598;&#25104;&#30005;&#36335;&#65288;IC&#65289;&#35774;&#35745;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#21160;&#24577;IR&#25481;&#30005;&#20223;&#30495;&#21464;&#24471;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;IR&#25481;&#30005;&#39044;&#27979;&#34987;&#25506;&#32034;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#32771;&#34385;&#19981;&#20165;&#22914;&#20309;&#27491;&#30830;&#34920;&#31034;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#36824;&#32771;&#34385;&#22914;&#20309;&#22312;&#29305;&#24449;&#32858;&#21512;&#36807;&#31243;&#20013;&#27169;&#25311;IR&#25481;&#30005;&#36981;&#24490;&#20854;&#29289;&#29702;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#65292;PDNGraph&#65292;&#32479;&#19968;&#20102;PDN&#32467;&#26500;&#21644;&#32454;&#31890;&#24230;&#21333;&#20803;-PDN&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;&#65292;PDNNet&#65292;&#23558;&#20004;&#20010;&#24182;&#34892;&#30340;GNN-CNN&#20998;&#25903;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26377;&#21033;&#20110;&#25429;&#25417;&#19978;&#36848;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18569v1 Announce Type: cross  Abstract: IR drop on the power delivery network (PDN) is closely related to PDN's configuration and cell current consumption. As the integrated circuit (IC) design is growing larger, dynamic IR drop simulation becomes computationally unaffordable and machine learning based IR drop prediction has been explored as a promising solution. Although CNN-based methods have been adapted to IR drop prediction task in several works, the shortcomings of overlooking PDN configuration is non-negligible. In this paper, we consider not only how to properly represent cell-PDN relation, but also how to model IR drop following its physical nature in the feature aggregation procedure. Thus, we propose a novel graph structure, PDNGraph, to unify the representations of the PDN structure and the fine-grained cell-PDN relation. We further propose a dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN branches to favorably capture the above feat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36136;&#30097;&#20102;&#20165;&#22312;BERT&#32593;&#32476;&#39030;&#37096;&#28155;&#21152;&#21333;&#20010;&#36755;&#20986;&#23618;&#20316;&#20026;&#20998;&#31867;&#22836;&#30340;&#24120;&#35268;&#20570;&#27861;&#65292;&#36890;&#36807;&#36827;&#34892;AutoML&#25628;&#32034;&#25214;&#21040;&#20102;&#22312;&#36739;&#23567;&#35745;&#31639;&#25104;&#26412;&#19979;&#20248;&#20110;&#24403;&#21069;&#21333;&#23618;&#30340;&#26550;&#26500;&#65292;&#24182;&#22312;GLUE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.18547</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#36827;&#34892;&#21477;&#23376;&#20998;&#31867;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search for Sentence Classification with BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36136;&#30097;&#20102;&#20165;&#22312;BERT&#32593;&#32476;&#39030;&#37096;&#28155;&#21152;&#21333;&#20010;&#36755;&#20986;&#23618;&#20316;&#20026;&#20998;&#31867;&#22836;&#30340;&#24120;&#35268;&#20570;&#27861;&#65292;&#36890;&#36807;&#36827;&#34892;AutoML&#25628;&#32034;&#25214;&#21040;&#20102;&#22312;&#36739;&#23567;&#35745;&#31639;&#25104;&#26412;&#19979;&#20248;&#20110;&#24403;&#21069;&#21333;&#23618;&#30340;&#26550;&#26500;&#65292;&#24182;&#22312;GLUE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23545;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26159;&#24120;&#35265;&#20570;&#27861;&#65292;&#38543;&#21518;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#26412;&#25991;&#36136;&#30097;&#20165;&#22312;&#32593;&#32476;&#39030;&#37096;&#28155;&#21152;&#21333;&#20010;&#36755;&#20986;&#23618;&#20316;&#20026;&#20998;&#31867;&#22836;&#30340;&#24120;&#35268;&#20570;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#25628;&#32034;&#65292;&#25214;&#21040;&#19968;&#20123;&#22312;&#36739;&#23567;&#35745;&#31639;&#25104;&#26412;&#19979;&#32988;&#36807;&#24403;&#21069;&#21333;&#23618;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;GLUE&#25968;&#25454;&#38598;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20998;&#31867;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18547v1 Announce Type: new  Abstract: Pre training of language models on large text corpora is common practice in Natural Language Processing. Following, fine tuning of these models is performed to achieve the best results on a variety of tasks. In this paper we question the common practice of only adding a single output layer as a classification head on top of the network. We perform an AutoML search to find architectures that outperform the current single layer at only a small compute cost. We validate our classification architecture on a variety of NLP benchmarks from the GLUE dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25235;&#21462;&#28909;&#22270;&#24341;&#23548;&#30340;&#39640;&#25928;&#23616;&#37096;&#25235;&#21462;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#20840;&#23616;&#21040;&#23616;&#37096;&#35821;&#20041;&#21040;&#28857;&#30340;&#26041;&#24335;&#25512;&#26029;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25235;&#21462;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18546</link><description>&lt;p&gt;
&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#39640;&#25928;&#30340;&#22522;&#20110;&#28909;&#22270;&#24341;&#23548;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25235;&#21462;&#28909;&#22270;&#24341;&#23548;&#30340;&#39640;&#25928;&#23616;&#37096;&#25235;&#21462;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#20840;&#23616;&#21040;&#23616;&#37096;&#35821;&#20041;&#21040;&#28857;&#30340;&#26041;&#24335;&#25512;&#26029;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25235;&#21462;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#24555;&#36895;&#32780;&#31283;&#20581;&#30340;&#29289;&#20307;&#25235;&#21462;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25351;&#20986;&#24403;&#21069;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#20165;&#21033;&#29992;&#25972;&#20010;&#35266;&#23519;&#21040;&#30340;&#28857;&#20113;&#26469;&#29983;&#25104;&#20845;&#33258;&#30001;&#24230;&#30340;&#25235;&#21462;&#65292;&#24573;&#30053;&#20102;&#20174;&#20840;&#23616;&#35821;&#20041;&#20013;&#25366;&#25496;&#30340;&#24341;&#23548;&#20449;&#24687;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#39640;&#36136;&#37327;&#25235;&#21462;&#30340;&#29983;&#25104;&#21644;&#23454;&#26102;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#28909;&#22270;&#22312;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#29983;&#25104;&#25928;&#29575;&#26041;&#38754;&#34987;&#20302;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#23616;&#37096;&#25235;&#21462;&#29983;&#25104;&#22120;&#65292;&#24182;&#32467;&#21512;&#25235;&#21462;&#28909;&#22270;&#20316;&#20026;&#24341;&#23548;&#65292;&#20197;&#20840;&#23616;&#21040;&#23616;&#37096;&#35821;&#20041;&#21040;&#28857;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#26029;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#39640;&#26031;&#32534;&#30721;&#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#31574;&#30053;&#26469;&#39044;&#27979;&#25235;&#21462;&#28909;&#22270;&#20197;&#25351;&#23548;&#23558;&#23616;&#37096;&#28857;&#32858;&#21512;&#21040;&#21487;&#25235;&#21462;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#22343;&#21248;&#38170;&#23450;&#37319;&#26679;&#26426;&#21046;&#26469;&#25552;&#39640;&#25235;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#21463;&#30410;&#20110;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18546v1 Announce Type: cross  Abstract: Fast and robust object grasping in clutter is a crucial component of robotics. Most current works resort to the whole observed point cloud for 6-Dof grasp generation, ignoring the guidance information excavated from global semantics, thus limiting high-quality grasp generation and real-time performance. In this work, we show that the widely used heatmaps are underestimated in the efficiency of 6-Dof grasp generation. Therefore, we propose an effective local grasp generator combined with grasp heatmaps as guidance, which infers in a global-to-local semantic-to-point way. Specifically, Gaussian encoding and the grid-based strategy are applied to predict grasp heatmaps as guidance to aggregate local points into graspable regions and provide global semantic information. Further, a novel non-uniform anchor sampling mechanism is designed to improve grasp accuracy and diversity. Benefiting from the high-efficiency encoding in the image space 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20114;&#25805;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#25552;&#21462;&#12289;&#36716;&#25442;&#12289;&#21152;&#36733;&#21644;&#35745;&#31639;&#27861;&#24459;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#36890;&#21521;&#27861;&#24459;&#33258;&#27835;&#30340;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.18537</link><description>&lt;p&gt;
&#36890;&#24448;&#27861;&#24459;&#33258;&#27835;&#30340;&#36335;&#24452;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#25552;&#21462;&#12289;&#36716;&#25442;&#12289;&#21152;&#36733;&#21644;&#35745;&#31639;&#27861;&#24459;&#20449;&#24687;&#30340;&#21487;&#20114;&#25805;&#20316;&#21644;&#21487;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20114;&#25805;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#25552;&#21462;&#12289;&#36716;&#25442;&#12289;&#21152;&#36733;&#21644;&#35745;&#31639;&#27861;&#24459;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#36890;&#21521;&#27861;&#24459;&#33258;&#27835;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#33258;&#27835; - &#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#21512;&#27861;&#27963;&#21160; - &#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#23454;&#29616;&#12290;&#19968;&#31181;&#26041;&#24335;&#26159;&#23545;AI&#34892;&#20026;&#32773;&#65288;&#22914;&#24320;&#21457;&#32773;&#12289;&#37096;&#32626;&#32773;&#21644;&#29992;&#25143;&#65289;&#21644;AI&#36164;&#28304;&#65288;&#22914;&#25968;&#25454;&#65289;&#26045;&#21152;&#32422;&#26463;&#65292;&#25110;&#32773;&#26159;&#23545;AI&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#21487;&#33021;&#20135;&#29983;&#24433;&#21709;&#30340;&#33539;&#22260;&#21644;&#24433;&#21709;&#31243;&#24230;&#26045;&#21152;&#32422;&#26463;&#12290;&#21518;&#19968;&#31181;&#26041;&#27861;&#28041;&#21450;&#23558;&#20851;&#20110;&#30001;AI&#39537;&#21160;&#30340;&#35774;&#22791;&#30340;&#29616;&#26377;&#35268;&#21017;&#32534;&#30721;&#21040;&#25511;&#21046;&#36825;&#20123;&#35774;&#22791;&#30340;AI&#20195;&#29702;&#36719;&#20214;&#20013;&#65288;&#20363;&#22914;&#65292;&#23558;&#20851;&#20110;&#26080;&#20154;&#26426;&#35774;&#22791;&#25805;&#20316;&#33539;&#22260;&#38480;&#21046;&#30340;&#35268;&#21017;&#32534;&#30721;&#21040;&#26080;&#20154;&#26426;&#35774;&#22791;&#30340;&#20195;&#29702;&#36719;&#20214;&#20013;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#38656;&#35201;&#19968;&#31181;&#25552;&#21462;&#12289;&#21152;&#36733;&#12289;&#36716;&#25442;&#21644;&#35745;&#31639;&#27861;&#24459;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#21487;&#35299;&#37322;&#21448;&#21487;&#22312;&#27861;&#24459;&#19978;&#30456;&#20114;&#25805;&#20316;&#65292;&#24182;&#33021;&#35753;AI&#20195;&#29702;&#25512;&#29702;&#27861;&#24459;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18537v1 Announce Type: new  Abstract: Legal autonomy - the lawful activity of artificial intelligence agents - can be achieved in one of two ways. It can be achieved either by imposing constraints on AI actors such as developers, deployers and users, and on AI resources such as data, or by imposing constraints on the range and scope of the impact that AI agents can have on the environment. The latter approach involves encoding extant rules concerning AI driven devices into the software of AI agents controlling those devices (e.g., encoding rules about limitations on zones of operations into the agent software of an autonomous drone device). This is a challenge since the effectivity of such an approach requires a method of extracting, loading, transforming and computing legal information that would be both explainable and legally interoperable, and that would enable AI agents to reason about the law. In this paper, we sketch a proof of principle for such a method using large 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39038;&#23458;&#34892;&#20026;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39038;&#23458;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#33258;&#28982;&#34892;&#20026;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#25512;&#33616;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.18536</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#30005;&#23376;&#21830;&#21153;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Novel Behavior-Based Recommendation System for E-commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18536
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39038;&#23458;&#34892;&#20026;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39038;&#23458;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#33258;&#28982;&#34892;&#20026;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#25512;&#33616;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#20381;&#36182;&#20110;&#29992;&#25143;&#35780;&#20998;&#65292;&#36825;&#21463;&#21040;&#29992;&#25143;&#21327;&#20316;&#27424;&#32570;&#21644;&#31232;&#30095;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#20026;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#39038;&#23458;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#33258;&#28982;&#34892;&#20026;&#65292;&#22914;&#27983;&#35272;&#21644;&#28857;&#20987;&#12290;&#25552;&#20986;&#30340;&#25512;&#33616;&#31995;&#32479;&#28041;&#21450;&#23545;&#27963;&#36291;&#39038;&#23458;&#36827;&#34892;&#32858;&#31867;&#12289;&#30830;&#23450;&#37051;&#22495;&#12289;&#25910;&#38598;&#30456;&#20284;&#29992;&#25143;&#12289;&#22522;&#20110;&#30456;&#20284;&#29992;&#25143;&#35745;&#31639;&#20135;&#21697;&#22768;&#35465;&#20197;&#21450;&#25512;&#33616;&#39640;&#22768;&#35465;&#20135;&#21697;&#12290;&#20026;&#20102;&#20811;&#26381;&#39038;&#23458;&#34892;&#20026;&#21644;&#20256;&#32479;&#32858;&#31867;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20135;&#21697;&#31867;&#21035;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25512;&#33616;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#22312;&#20960;&#20010;&#26041;&#38754;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18536v1 Announce Type: cross  Abstract: The majority of existing recommender systems rely on user ratings, which are limited by the lack of user collaboration and the sparsity problem. To address these issues, this study proposes a behavior-based recommender system that leverages customers' natural behaviors, such as browsing and clicking, on e-commerce platforms. The proposed recommendation system involves clustering active customers, determining neighborhoods, collecting similar users, calculating product reputation based on similar users, and recommending high-reputation products. To overcome the complexity of customer behaviors and traditional clustering methods, an unsupervised clustering approach based on product categories is developed to enhance the recommendation methodology. This study makes notable contributions in several aspects. Firstly, a groundbreaking behavior-based recommendation methodology is developed, incorporating customer behavior to generate accurate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32447;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ADAM&#30340;&#21160;&#37327;&#39033;&#38598;&#25104;&#21040;Armijo&#32447;&#25628;&#32034;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#24182;&#19988;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#21644;Adam&#30340;&#35843;&#25972;&#23398;&#20064;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18519</link><description>&lt;p&gt;
&#25913;&#36827;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32447;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Line Search Methods for Large Scale Neural Network Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32447;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ADAM&#30340;&#21160;&#37327;&#39033;&#38598;&#25104;&#21040;Armijo&#32447;&#25628;&#32034;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#24182;&#19988;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#21644;Adam&#30340;&#35843;&#25972;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#32447;&#25628;&#32034;&#26041;&#27861;&#22312;&#20256;&#32479;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#29305;&#23450;&#23398;&#20064;&#29575;&#35843;&#24230;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#32447;&#25628;&#32034;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22686;&#24378;&#25514;&#26045;&#65292;&#24182;&#23545;&#20854;&#25928;&#26524;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#27604;&#20197;&#24448;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#39046;&#22495;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;ADAM&#30340;&#21160;&#37327;&#39033;&#38598;&#25104;&#21040;Armijo&#32447;&#25628;&#32034;&#20013;&#30340;&#25628;&#32034;&#26041;&#21521;&#20013;&#65292;&#25913;&#36827;&#20102;Armijo&#32447;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#36825;&#26159;&#20197;&#21069;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#26041;&#27861;&#23481;&#26131;&#22833;&#36133;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;&#26041;&#27861;&#32988;&#36807;&#20197;&#21069;&#30340;Armijo&#23454;&#29616;&#21644;Adam&#30340;&#35843;&#25972;&#23398;&#20064;&#29575;&#35843;&#24230;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#25918;&#22312;NLP&#21644;&#22270;&#20687;&#25968;&#25454;&#39046;&#22495;&#30340;Transformer&#21644;CNN&#19978;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20197;Python&#21253;&#30340;&#24418;&#24335;&#20844;&#24320;&#21457;&#24067;&#65292;&#21487;&#20197;&#19979;&#36733;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18519v1 Announce Type: cross  Abstract: In recent studies, line search methods have shown significant improvements in the performance of traditional stochastic gradient descent techniques, eliminating the need for a specific learning rate schedule. In this paper, we identify existing issues in state-of-the-art line search methods, propose enhancements, and rigorously evaluate their effectiveness. We test these methods on larger datasets and more complex data domains than before. Specifically, we improve the Armijo line search by integrating the momentum term from ADAM in its search direction, enabling efficient large-scale training, a task that was previously prone to failure using Armijo line search methods. Our optimization approach outperforms both the previous Armijo implementation and tuned learning rate schedules for Adam. Our evaluation focuses on Transformers and CNNs in the domains of NLP and image data. Our work is publicly available as a Python package, which prov
&lt;/p&gt;</description></item><item><title>&#23558;Armijo&#32447;&#25628;&#32034;&#19982;Adam&#20248;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#21333;&#20803;&#25191;&#34892;&#32447;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#22312;Transformer&#24494;&#35843;&#20013;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18506</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#25628;&#32034;&#26041;&#27861;&#21152;&#36895;Transformer&#24494;&#35843;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Faster Convergence for Transformer Fine-tuning with Line Search Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18506
&lt;/p&gt;
&lt;p&gt;
&#23558;Armijo&#32447;&#25628;&#32034;&#19982;Adam&#20248;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#21333;&#20803;&#25191;&#34892;&#32447;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#22312;Transformer&#24494;&#35843;&#20013;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32447;&#25628;&#32034;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20256;&#32479;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#32447;&#25628;&#32034;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#26032;&#39062;&#19988;&#22791;&#21463;&#27426;&#36814;&#30340;Transformer&#26550;&#26500;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;Armijo&#32447;&#25628;&#32034;&#19982;Adam&#20248;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#23558;&#32593;&#32476;&#26550;&#26500;&#32454;&#20998;&#20026;&#21512;&#29702;&#30340;&#21333;&#20803;&#65292;&#22312;&#36825;&#20123;&#26412;&#22320;&#21333;&#20803;&#19978;&#20998;&#21035;&#25191;&#34892;&#32447;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;Adam&#20248;&#21270;&#22120;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#25110;&#23567;&#35757;&#32451;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#27979;&#35797;&#26696;&#20363;&#20013;&#34920;&#29616;&#30456;&#31561;&#25110;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20316;&#20026;&#19968;&#20010;Python&#21253;&#20844;&#24320;&#21487;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;PyTorch&#20248;&#21270;&#22120;&#65292;&#19982;&#20219;&#24847;&#32593;&#32476;&#26550;&#26500;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18506v1 Announce Type: cross  Abstract: Recent works have shown that line search methods greatly increase performance of traditional stochastic gradient descent methods on a variety of datasets and architectures [1], [2]. In this work we succeed in extending line search methods to the novel and highly popular Transformer architecture and dataset domains in natural language processing. More specifically, we combine the Armijo line search with the Adam optimizer and extend it by subdividing the networks architecture into sensible units and perform the line search separately on these local units. Our optimization method outperforms the traditional Adam optimizer and achieves significant performance improvements for small data sets or small training budgets, while performing equal or better for other tested cases. Our work is publicly available as a python package, which provides a hyperparameter-free pytorch optimizer that is compatible with arbitrary network architectures.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22825;&#27668;&#39044;&#25253;&#25968;&#25454;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20272;&#35745;&#33976;&#25955;&#21457;&#30340;&#36755;&#20837;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;FAO56-PM&#26041;&#27861;&#35745;&#31639;ET0&#26102;&#22826;&#38451;&#36752;&#23556;&#21442;&#25968;&#19981;&#26131;&#33719;&#21462;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.18489</link><description>&lt;p&gt;
&#21033;&#29992;&#22825;&#27668;&#39044;&#25253;&#25968;&#25454;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20272;&#35745;&#33976;&#25955;&#21457;&#30340;&#36755;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Employing Weather Forecast Data as Input to the Estimation of Evapotranspiration by Deep Neural Network Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18489
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22825;&#27668;&#39044;&#25253;&#25968;&#25454;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20272;&#35745;&#33976;&#25955;&#21457;&#30340;&#36755;&#20837;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;FAO56-PM&#26041;&#27861;&#35745;&#31639;ET0&#26102;&#22826;&#38451;&#36752;&#23556;&#21442;&#25968;&#19981;&#26131;&#33719;&#21462;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#32771;&#33976;&#25955;&#21457;&#65288;ET0&#65289;&#26159;&#35774;&#35745;&#26234;&#33021;&#28748;&#28297;&#35843;&#24230;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#22240;&#20026;&#23427;&#36890;&#36807;&#31995;&#25968;&#19982;&#20316;&#29289;&#30340;&#27700;&#38656;&#27714;&#30456;&#20851;&#12290;&#32852;&#21512;&#22269;&#31918;&#39135;&#21644;&#20892;&#19994;&#32452;&#32455;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#30340;ET0&#35745;&#31639;&#26041;&#27861;&#65288;FAO56PM&#65289;&#65292;&#22522;&#20110;Penman-Monteith&#26041;&#31243;&#30340;&#21442;&#25968;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#20351;&#29992;FAO56-PM&#26041;&#27861;&#35745;&#31639;ET0&#38656;&#35201;&#22235;&#20010;&#20027;&#35201;&#30340;&#22825;&#27668;&#21442;&#25968;&#65306;&#28201;&#24230;&#12289;&#28287;&#24230;&#12289;&#39118;&#36895;&#21644;&#22826;&#38451;&#36752;&#23556;&#65288;SR&#65289;&#12290;&#19968;&#31181;&#39044;&#27979;&#26410;&#26469;&#20960;&#22825;&#30340;&#27599;&#26085;ET0&#20540;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#20813;&#36153;&#25552;&#20379;&#30340;&#22825;&#27668;&#39044;&#25253;&#26381;&#21153;&#65288;WFSs&#65289;&#65292;&#36825;&#20123;&#26381;&#21153;&#21487;&#20272;&#35745;&#22810;&#31181;&#27668;&#35937;&#21442;&#25968;&#38271;&#36798;&#26410;&#26469;15&#22825;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#38382;&#39064;&#22312;&#20110;&#24403;&#21069;&#22823;&#22810;&#25968;&#22312;&#32447;&#26381;&#21153;&#27809;&#26377;&#25552;&#20379;SR&#20316;&#20026;&#20813;&#36153;&#30340;&#39044;&#27979;&#21442;&#25968;&#65292;&#36890;&#24120;&#36825;&#26679;&#30340;&#39044;&#27979;&#38656;&#35201;&#25903;&#20184;&#37329;&#38065;&#12290;&#22240;&#27492;&#65292;&#20986;&#29616;&#20102;&#20960;&#31181;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18489v1 Announce Type: new  Abstract: Reference Evapotranspiration (ET0) is a key parameter for designing smart irrigation scheduling, since it is related by a coefficient to the water needs of a crop. The United Nations Food and Agriculture Organization, proposed a standard method for ET0 computation (FAO56PM), based on the parameterization of the Penman-Monteith equation, that is widely adopted in the literature. To compute ET0 using the FAO56-PM method, four main weather parameters are needed: temperature, humidity, wind, and solar radiation (SR). One way to make daily ET0 estimations for future days is to use freely available weather forecast services (WFSs), where many meteorological parameters are estimated up to the next 15 days. A problem with this method is that currently, SR is not provided as a free forecast parameter on most of those online services or, normally, such forecasts present a financial cost penalty. For this reason, several ET0 estimation models using
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#29305;&#23450;&#20027;&#20307;&#12289;&#20250;&#35805;&#21644;&#31867;&#21035;&#30340;EEG&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#12290;</title><link>https://arxiv.org/abs/2403.18486</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20174;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#33539;&#24335;&#20013;&#21512;&#25104;&#33041;&#30005;&#22270;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#29305;&#23450;&#20027;&#20307;&#12289;&#20250;&#35805;&#21644;&#31867;&#21035;&#30340;EEG&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#24471;&#20197;&#32531;&#35299;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#20808;&#21069;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#22312;&#37319;&#26679;&#28789;&#27963;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#25110;&#38656;&#35201;EEG&#25968;&#25454;&#30340;&#26367;&#20195;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30452;&#25509;&#29983;&#25104;&#29305;&#23450;&#20027;&#20307;&#12289;&#20250;&#35805;&#21644;&#31867;&#21035;&#30340;EEG&#25968;&#25454;&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;&#25351;&#26631;&#22806;&#65292;&#36824;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#29983;&#25104;&#26679;&#26412;&#30340;&#29305;&#23450;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#27599;&#20010;&#20027;&#20307;&#12289;&#20250;&#35805;&#21644;&#31867;&#21035;&#30340;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#30340;EEG&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18486v1 Announce Type: cross  Abstract: Data scarcity in the brain-computer interface field can be alleviated through the use of generative models, specifically diffusion models. While diffusion models have previously been successfully applied to electroencephalogram (EEG) data, existing models lack flexibility w.r.t.~sampling or require alternative representations of the EEG data. To overcome these limitations, we introduce a novel approach to conditional diffusion models that utilizes classifier-free guidance to directly generate subject-, session-, and class-specific EEG data. In addition to commonly used metrics, domain-specific metrics are employed to evaluate the specificity of the generated samples. The results indicate that the proposed model can generate EEG data that resembles real data for each subject, session, and class.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#24230;&#24341;&#23548;&#30340;&#32763;&#35793;&#22120;&#65288;DGT&#65289;&#20197;&#25913;&#21892;&#19977;&#32500;&#28857;&#20113;&#30340;&#21512;&#25104;&#21040;&#30495;&#23454;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#20256;&#24863;&#22120;&#37319;&#26679;&#27169;&#24335;&#21644;&#19981;&#23436;&#25972;&#35757;&#32451;&#31574;&#30053;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.18469</link><description>&lt;p&gt;
&#23494;&#24230;&#24341;&#23548;&#30340;&#32763;&#35793;&#22120;&#25512;&#21160;&#20102;&#19977;&#32500;&#28857;&#20113;&#30340;&#21512;&#25104;&#21040;&#30495;&#23454;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18469
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#24230;&#24341;&#23548;&#30340;&#32763;&#35793;&#22120;&#65288;DGT&#65289;&#20197;&#25913;&#21892;&#19977;&#32500;&#28857;&#20113;&#30340;&#21512;&#25104;&#21040;&#30495;&#23454;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#20256;&#24863;&#22120;&#37319;&#26679;&#27169;&#24335;&#21644;&#19981;&#23436;&#25972;&#35757;&#32451;&#31574;&#30053;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#21512;&#25104;&#21040;&#30495;&#23454;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20998;&#21106;&#23545;&#20110;&#26631;&#27880;&#26032;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#24230;&#24341;&#23548;&#30340;&#32763;&#35793;&#22120;&#65288;DGT&#65289;&#65292;&#29992;&#20110;&#22312;&#20004;&#38454;&#27573;&#33258;&#35757;&#32451;&#31649;&#36947;&#20013;&#20256;&#36755;&#28857;&#20113;&#23494;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#19981;&#21487;&#23398;&#20064;&#30340;DGT&#26469;&#22312;&#36755;&#20837;&#23618;&#32423;&#19978;&#24357;&#21512;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20854;&#27425;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#25552;&#20986;&#20102;&#19968;&#20010;&#31867;&#21035;&#32423;&#23545;&#25239;&#32593;&#32476;&#65292;&#21033;&#29992;&#21407;&#22411;&#26469;&#20026;&#33258;&#35757;&#32451;&#25552;&#20379;&#33391;&#22909;&#21021;&#22987;&#21270;&#27169;&#22411;&#65292;&#20197;&#38450;&#27490;&#36127;&#36801;&#31227;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#36848;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#28151;&#21512;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18469v1 Announce Type: cross  Abstract: 3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to annotating new domains. Self-training is a competitive approach for this task, but its performance is limited by different sensor sampling patterns (i.e., variations in point density) and incomplete training strategies. In this work, we propose a density-guided translator (DGT), which translates point density between domains, and integrates it into a two-stage self-training pipeline named DGT-ST. First, in contrast to existing works that simultaneously conduct data generation and feature/output alignment within unstable adversarial training, we employ the non-learnable DGT to bridge the domain gap at the input level. Second, to provide a well-initialized model for self-training, we propose a category-level adversarial network in stage one that utilizes the prototype to prevent negative transfer. Finally, by leveraging the designs above, a domain-mixed self-tra
&lt;/p&gt;</description></item><item><title>CoBOS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#22522;&#20110;&#32422;&#26463;&#30340;&#35843;&#24230;&#26041;&#27861;&#65292;&#22312;&#20154;&#26426;&#21327;&#20316;&#20013;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#23545;&#19981;&#30830;&#23450;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#29992;&#25143;&#30340;&#21387;&#21147;&#65292;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18459</link><description>&lt;p&gt;
CoBOS: &#22522;&#20110;&#32422;&#26463;&#30340;&#20154;&#26426;&#21327;&#20316;&#22312;&#32447;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18459
&lt;/p&gt;
&lt;p&gt;
CoBOS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#22522;&#20110;&#32422;&#26463;&#30340;&#35843;&#24230;&#26041;&#27861;&#65292;&#22312;&#20154;&#26426;&#21327;&#20316;&#20013;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#23545;&#19981;&#30830;&#23450;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#29992;&#25143;&#30340;&#21387;&#21147;&#65292;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28041;&#21450;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#30340;&#35013;&#37197;&#36807;&#31243;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#22240;&#20026;&#20010;&#20154;&#27963;&#21160;&#21644;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#30340;&#35775;&#38382;&#24517;&#39035;&#21327;&#35843;&#12290;&#22266;&#23450;&#30340;&#26426;&#22120;&#20154;&#31243;&#24207;&#19981;&#20801;&#35768;&#20559;&#31163;&#22266;&#23450;&#21327;&#35758;&#12290;&#22312;&#36825;&#26679;&#30340;&#36807;&#31243;&#20013;&#24037;&#20316;&#21487;&#33021;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26377;&#21387;&#21147;&#65292;&#24182;&#23548;&#33268;&#34892;&#20026;&#26080;&#25928;&#25110;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#22522;&#20110;&#32422;&#26463;&#30340;&#35843;&#24230;&#26041;&#27861;&#65292;&#20301;&#20110;&#25903;&#25345;&#34892;&#20026;&#26641;&#30340;&#21453;&#24212;&#24335;&#25191;&#34892;&#25511;&#21046;&#26694;&#26550;&#20013;&#65292;&#21517;&#20026;CoBOS&#12290;&#36825;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#36866;&#24212;&#24310;&#36831;&#27963;&#21160;&#23436;&#25104;&#21644;&#27963;&#21160;&#36873;&#25321;&#65288;&#30001;&#20154;&#31867;&#65289;&#31561;&#19981;&#30830;&#23450;&#20107;&#20214;&#12290;&#29992;&#25143;&#23558;&#20307;&#39564;&#21040;&#36739;&#23569;&#30340;&#21387;&#21147;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#21516;&#20107;&#20250;&#35843;&#25972;&#20854;&#34892;&#20026;&#20197;&#26368;&#22909;&#22320;&#34917;&#20805;&#20154;&#31867;&#36873;&#25321;&#30340;&#27963;&#21160;&#65292;&#20197;&#23436;&#25104;&#20849;&#21516;&#20219;&#21153;&#12290;&#38500;&#20102;&#25913;&#21892;&#30340;&#24037;&#20316;&#26465;&#20214;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#23548;&#33268;&#20102;&#25928;&#29575;&#30340;&#25552;&#39640;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;si&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18459v1 Announce Type: cross  Abstract: Assembly processes involving humans and robots are challenging scenarios because the individual activities and access to shared workspace have to be coordinated. Fixed robot programs leave no room to diverge from a fixed protocol. Working on such a process can be stressful for the user and lead to ineffective behavior or failure. We propose a novel approach of online constraint-based scheduling in a reactive execution control framework facilitating behavior trees called CoBOS. This allows the robot to adapt to uncertain events such as delayed activity completions and activity selection (by the human). The user will experience less stress as the robotic coworkers adapt their behavior to best complement the human-selected activities to complete the common task. In addition to the improved working conditions, our algorithm leads to increased efficiency, even in highly uncertain scenarios. We evaluate our algorithm using a probabilistic si
&lt;/p&gt;</description></item><item><title>CoRAST&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;(FMs)&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#12289;&#30456;&#20851;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.18451</link><description>&lt;p&gt;
CoRAST&#65306;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;CPS&#21644;IoT&#20013;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#30456;&#20851;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18451
&lt;/p&gt;
&lt;p&gt;
CoRAST&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;(FMs)&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#12289;&#30456;&#20851;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;(FMs)&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#29702;&#35299;&#24322;&#26500;&#25968;&#25454;&#38598;&#20013;&#22797;&#26434;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#21033;&#29992;&#20998;&#24067;&#24335;&#21644;&#22810;&#26679;&#21270;&#30340;&#29615;&#22659;&#25968;&#25454;&#12290;&#19982;&#32852;&#21512;&#23398;&#20064;&#31561;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#19981;&#21516;&#65292;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;FMs&#21487;&#20197;&#23558;&#19981;&#21516;&#30340;&#36755;&#20837;&#36716;&#25442;&#20026;&#23884;&#20837;&#12290;&#36825;&#19968;&#36807;&#31243;&#26377;&#21161;&#20110;&#25972;&#21512;&#21508;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#23558;&#20808;&#21069;&#30340;&#23398;&#20064;&#24212;&#29992;&#20110;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#31995;&#32479;&#20013;&#37096;&#32626;FMs&#20250;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoRAST&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;FMs&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#12289;&#30456;&#20851;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;FM&#65292;CoRAST&#21487;&#20197;&#21033;&#29992;&#29616;&#26377;&#30340;&#29615;&#22659;&#20449;&#24687;&#26469;&#25552;&#21462;&#20256;&#24863;&#22120;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#12289;&#31354;&#38388;&#21644;&#36328;&#27169;&#24577;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18451v1 Announce Type: cross  Abstract: Foundation models (FMs) emerge as a promising solution to harness distributed and diverse environmental data by leveraging prior knowledge to understand the complicated temporal and spatial correlations within heterogeneous datasets. Unlike distributed learning frameworks such as federated learning, which often struggle with multimodal data, FMs can transform diverse inputs into embeddings. This process facilitates the integration of information from various modalities and the application of prior learning to new domains. However, deploying FMs in resource-constrained edge systems poses significant challenges. To this end, we introduce CoRAST, a novel learning framework that utilizes FMs for enhanced analysis of distributed, correlated heterogeneous data. Utilizing a server-based FM, CoRAST can exploit existing environment information to extract temporal, spatial, and cross-modal correlations among sensor data. This enables CoRAST to o
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;U-Sketch&#26694;&#26550;&#65292;&#20855;&#26377;U-Net&#31867;&#22411;&#30340;&#28508;&#22312;&#36793;&#32536;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#36827;&#33609;&#22270;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#31354;&#38388;&#24067;&#23616;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.18425</link><description>&lt;p&gt;
U-Sketch: &#19968;&#31181;&#29992;&#20110;&#33609;&#22270;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18425
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;U-Sketch&#26694;&#26550;&#65292;&#20855;&#26377;U-Net&#31867;&#22411;&#30340;&#28508;&#22312;&#36793;&#32536;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#36827;&#33609;&#22270;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#31354;&#38388;&#24067;&#23616;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20135;&#29983;&#20102;&#31526;&#21512;&#30456;&#24212;&#25991;&#26412;&#25552;&#31034;&#30340;&#36924;&#30495;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#33609;&#22270;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#20173;&#26377;&#25152;&#27424;&#32570;&#65292;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#24067;&#23616;&#19981;&#20165;&#35201;&#36981;&#24490;&#25991;&#26412;&#25552;&#31034;&#65292;&#36824;&#24517;&#39035;&#32039;&#23494;&#36319;&#38543;&#26576;&#20123;&#21442;&#32771;&#33609;&#22270;&#30340;&#36718;&#24275;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#20351;&#29992;MLP&#28508;&#22312;&#36793;&#32536;&#39044;&#27979;&#22120;&#26469;&#24341;&#23548;&#21512;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#24067;&#23616;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#39044;&#27979;&#36793;&#32536;&#22320;&#22270;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;MLP&#30340;&#36880;&#20687;&#32032;&#25805;&#20316;&#24182;&#26410;&#23558;&#31354;&#38388;&#24067;&#23616;&#20316;&#20026;&#25972;&#20307;&#32771;&#34385;&#36827;&#26469;&#65292;&#38656;&#35201;&#22823;&#37327;&#21435;&#22122;&#36845;&#20195;&#25165;&#33021;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;&#22270;&#20687;&#65292;&#23548;&#33268;&#26102;&#38388;&#25928;&#29575;&#20302;&#19979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;U-Sketch&#65292;&#36825;&#26159;&#19968;&#20010;&#24341;&#20837;&#20102;U-Net&#31867;&#22411;&#28508;&#22312;&#36793;&#32536;&#39044;&#27979;&#22120;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18425v1 Announce Type: cross  Abstract: Diffusion models have demonstrated remarkable performance in text-to-image synthesis, producing realistic and high resolution images that faithfully adhere to the corresponding text-prompts. Despite their great success, they still fall behind in sketch-to-image synthesis tasks, where in addition to text-prompts, the spatial layout of the generated images has to closely follow the outlines of certain reference sketches. Employing an MLP latent edge predictor to guide the spatial layout of the synthesized image by predicting edge maps at each denoising step has been recently proposed. Despite yielding promising results, the pixel-wise operation of the MLP does not take into account the spatial layout as a whole, and demands numerous denoising iterations to produce satisfactory images, leading to time inefficiency. To this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge predictor, which is capable of efficiently
&lt;/p&gt;</description></item><item><title>BioMedLM&#26159;&#19968;&#20010;27&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;PubMed&#25991;&#29486;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#21644;&#24739;&#32773;&#25552;&#38382;&#12290;</title><link>https://arxiv.org/abs/2403.18421</link><description>&lt;p&gt;
BioMedLM&#65306;&#22522;&#20110;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#35757;&#32451;&#30340;27&#20159;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18421
&lt;/p&gt;
&lt;p&gt;
BioMedLM&#26159;&#19968;&#20010;27&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;PubMed&#25991;&#29486;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#21644;&#24739;&#32773;&#25552;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18421v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;GPT-4&#21644;Med-PaLM 2&#31561;&#27169;&#22411;&#22312;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#25968;&#21315;&#20159;&#20010;&#21442;&#25968;&#65292;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#65292;&#38656;&#35201;&#29992;&#25143;&#36890;&#36807;&#20114;&#32852;&#32593;&#21457;&#36865;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#19988;&#26159;&#22312;&#26410;&#30693;&#25968;&#25454;&#26469;&#28304;&#19978;&#35757;&#32451;&#30340;&#12290;&#26356;&#23567;&#19988;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#27169;&#22411;&#33021;&#21542;&#31454;&#20105;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;BioMedLM&#65292;&#19968;&#20010;&#20165;&#22312;PubMed&#25688;&#35201;&#21644;&#20840;&#25991;&#19978;&#35757;&#32451;&#30340;27&#20159;&#21442;&#25968;GPT&#39118;&#26684;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#22312;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;BioMedLM&#21487;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#22810;&#39033;&#36873;&#25321;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#32467;&#26524;&#65292;&#19982;&#26356;&#22823;&#30340;&#27169;&#22411;&#31454;&#20105;&#65292;&#20363;&#22914;&#22312;MedMCQA&#65288;dev&#65289;&#19978;&#21462;&#24471;57.3%&#30340;&#24471;&#20998;&#65292;&#22312;MMLU&#21307;&#23398;&#36951;&#20256;&#23398;&#32771;&#35797;&#19978;&#21462;&#24471;69.0%&#30340;&#24471;&#20998;&#12290;BioMedLM&#36824;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#23545;&#21307;&#23398;&#35805;&#39064;&#19978;&#24739;&#32773;&#25552;&#20986;&#30340;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#30340;&#31572;&#26696;&#12290;&#36825;&#34920;&#26126;&#36739;&#23567;&#30340;&#27169;&#22411;&#28508;&#22312;&#22320;&#21487;&#20197;&#20316;&#20026;&#36879;&#26126;&#19988;&#38544;&#31169;&#24615;&#30340;&#26381;&#21153;&#25552;&#20379;&#32773;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18421v1 Announce Type: cross  Abstract: Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources. Can smaller, more targeted models compete? To address this question, we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive model trained exclusively on PubMed abstracts and full articles. When fine-tuned, BioMedLM can produce strong multiple-choice biomedical question-answering results competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to patient questions on medical topics. This demonstrates that smaller models can potentially serve as transparent, privacy-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36731;&#37327;&#32423;&#22522;&#20110;&#36890;&#36947;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#36739;&#24046;&#30340;&#20266;&#26631;&#31614;&#26377;&#25928;&#22320;&#21512;&#24182;&#20026;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#26080;&#20559;&#24046;&#21644;&#20302;&#26041;&#24046;&#30340;&#19968;&#20010;&#20266;&#26631;&#31614;&#65292;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#20135;&#29983;&#30340;&#26377;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.18407</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36947;&#38598;&#25104;&#26041;&#27861;&#65306;&#26080;&#20559;&#24046;&#21644;&#20302;&#26041;&#24046;&#30340;&#20266;&#26631;&#31614;&#23545;&#21322;&#30417;&#30563;&#20998;&#31867;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is Critical for Semi-supervised Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36731;&#37327;&#32423;&#22522;&#20110;&#36890;&#36947;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#36739;&#24046;&#30340;&#20266;&#26631;&#31614;&#26377;&#25928;&#22320;&#21512;&#24182;&#20026;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#26080;&#20559;&#24046;&#21644;&#20302;&#26041;&#24046;&#30340;&#19968;&#20010;&#20266;&#26631;&#31614;&#65292;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#20135;&#29983;&#30340;&#26377;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26159;&#19968;&#20010;&#23454;&#38469;&#25361;&#25112;&#12290;&#20266;&#26631;&#31614;&#65288;PL&#65289;&#26041;&#27861;&#65292;&#20363;&#22914;FixMatch&#21644;FreeMatch&#65292;&#22312;SSL&#20013;&#33719;&#24471;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#38408;&#20540;&#21040;&#20266;&#26631;&#31614;&#65288;T2L&#65289;&#22788;&#29702;&#36890;&#36807;&#25130;&#26029;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#39044;&#27979;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#32622;&#20449;&#24230;&#24471;&#21040;PL&#12290;&#28982;&#32780;&#65292;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#20250;&#20135;&#29983;&#26377;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#30340;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#20379;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#22522;&#20110;&#36890;&#36947;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#36739;&#24046;&#30340;&#20266;&#26631;&#31614;&#26377;&#25928;&#22320;&#21512;&#24182;&#20026;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#26080;&#20559;&#24046;&#21644;&#20302;&#26041;&#24046;&#30340;&#20266;&#26631;&#31614;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20219;&#20309;SSL&#26694;&#26550;&#65292;&#20363;&#22914;FixMatch&#25110;FreeMatch&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CIFAR10/100&#30340;&#25928;&#26524;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18407v1 Announce Type: cross  Abstract: Semi-supervised learning (SSL) is a practical challenge in computer vision. Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of The Art (SOTA) performances in SSL. These approaches employ a threshold-to-pseudo-label (T2L) process to generate PLs by truncating the confidence scores of unlabeled data predicted by the self-training method. However, self-trained models typically yield biased and high-variance predictions, especially in the scenarios when a little labeled data are supplied. To address this issue, we propose a lightweight channel-based ensemble method to effectively consolidate multiple inferior PLs into the theoretically guaranteed unbiased and low-variance one. Importantly, our approach can be readily extended to any SSL framework, such as FixMatch or FreeMatch. Experimental results demonstrate that our method significantly outperforms state-of-the-art techniques on CIFAR10/100 in terms of effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#21333;&#19968;&#30340;Vision Language Model (VLM) &#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#39057;&#38382;&#31572;&#65292;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#21333;&#20010;&#21512;&#25104;&#22270;&#20687;&#20197;&#23454;&#29616;&#35270;&#39057;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.18406</link><description>&lt;p&gt;
&#22270;&#20687;&#32593;&#26684;&#21487;&#33021;&#27604;&#35270;&#39057;&#26356;&#26377;&#20215;&#20540;&#65306;&#20351;&#29992;VLM&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#21333;&#19968;&#30340;Vision Language Model (VLM) &#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#39057;&#38382;&#31572;&#65292;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#21333;&#20010;&#21512;&#25104;&#22270;&#20687;&#20197;&#23454;&#29616;&#35270;&#39057;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#29992;&#20110;&#36830;&#25509;&#35270;&#39057;&#27169;&#24577;&#30340;&#31574;&#30053;&#12290;&#20854;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#31574;&#30053;&#28041;&#21450;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#65288;VideoLMs&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25509;&#21475;&#23558;&#20808;&#36827;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#19982;LLMs&#36830;&#25509;&#36215;&#26469;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21478;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#36328;&#27169;&#24577;&#36827;&#34892;&#27169;&#24577;&#26725;&#25509;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;VideoLMs&#21644;LLMs&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#21364;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#12290;&#25105;&#20204;&#30340;&#20986;&#21457;&#28857;&#26159;&#35270;&#39057;&#21253;&#21547;&#19968;&#31995;&#21015;&#22270;&#20687;&#25110;&#24103;&#65292;&#36825;&#20123;&#22270;&#20687;&#19982;&#26102;&#38388;&#20449;&#24687;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#31616;&#21333;&#27934;&#23519;&#12290;&#35270;&#39057;&#29702;&#35299;&#30340;&#31934;&#39635;&#22312;&#20110;&#24039;&#22937;&#22320;&#31649;&#29702;&#27599;&#20010;&#24103;&#30340;&#26102;&#38388;&#26041;&#38754;&#20197;&#21450;&#31354;&#38388;&#32454;&#33410;&#12290;&#21021;&#22987;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#25490;&#21015;&#22810;&#20010;&#24103;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#21333;&#20010;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18406v1 Announce Type: cross  Abstract: Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging mul
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#36718;&#24037;&#20316;&#27969;&#31243;&#65292;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#30340;&#30456;&#20851;&#21028;&#26029;&#65292;&#33021;&#22815;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#36807;&#31243;&#24182;&#25972;&#21512;&#19987;&#23478;&#25512;&#29702;&#65292;&#25552;&#39640;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18405</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18405
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#36718;&#24037;&#20316;&#27969;&#31243;&#65292;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#30340;&#30456;&#20851;&#21028;&#26029;&#65292;&#33021;&#22815;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#36807;&#31243;&#24182;&#25972;&#21512;&#19987;&#23478;&#25512;&#29702;&#65292;&#25552;&#39640;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#30340;&#30456;&#20851;&#21028;&#20915;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#20934;&#30830;&#21028;&#26029;&#20004;&#20010;&#27861;&#24459;&#26696;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#38656;&#35201;&#38405;&#35835;&#20887;&#38271;&#30340;&#25991;&#26412;&#24182;&#20855;&#22791;&#39640;&#27700;&#24179;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#20197;&#25552;&#21462;&#27861;&#24459;&#20107;&#23454;&#24182;&#20316;&#20986;&#21496;&#27861;&#21028;&#26029;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;LLM&#65288;Large Language Models&#65289;&#36827;&#34892;&#30456;&#20851;&#24615;&#21028;&#26029;&#26159;&#26377;&#21069;&#36884;&#30340;&#12290;&#28982;&#32780;&#65292;&#23558;&#19968;&#33324;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#21487;&#38752;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26041;&#27861;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#36718;&#24037;&#20316;&#27969;&#31243;&#65292;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#30340;&#30456;&#20851;&#21028;&#26029;&#12290;&#25152;&#25552;&#20986;&#30340;&#24037;&#20316;&#27969;&#31243;&#23558;&#27880;&#37322;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#38454;&#27573;&#65292;&#27169;&#20223;&#20154;&#31867;&#27880;&#37322;&#32773;&#25152;&#20351;&#29992;&#30340;&#36807;&#31243;&#65292;&#24182;&#20351;&#19987;&#23478;&#25512;&#29702;&#33021;&#22815;&#28789;&#27963;&#22320;&#25972;&#21512;&#20197;&#22686;&#24378;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18405v1 Announce Type: new  Abstract: Collecting relevant judgments for legal case retrieval is a challenging and time-consuming task. Accurately judging the relevance between two legal cases requires a considerable effort to read the lengthy text and a high level of domain expertise to extract Legal Facts and make juridical judgments. With the advent of advanced large language models, some recent studies have suggested that it is promising to use LLMs for relevance judgment. Nonetheless, the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored. To fill this research gap, we devise a novel few-shot workflow tailored to the relevant judgment of legal cases. The proposed workflow breaks down the annotation process into a series of stages, imitating the process employed by human annotators and enabling a flexible integration of expert reasoning to enhance the accuracy of relevance judgments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(mDCGAN)&#65292;&#38024;&#23545;&#39640;&#36136;&#37327;&#33402;&#26415;&#21697;&#29983;&#25104;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35299;&#20915;&#20102;&#26222;&#36941;&#35757;&#32451;&#38382;&#39064;&#65292;&#26377;&#25928;&#25506;&#32034;&#25277;&#35937;&#32472;&#30011;&#20013;&#30340;&#39068;&#33394;&#21644;&#31508;&#35302;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.18397</link><description>&lt;p&gt;
&#20351;&#29992;&#25913;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#25277;&#35937;&#33402;&#26415;&#20013;&#36827;&#34892;&#39068;&#33394;&#21644;&#31508;&#35302;&#27169;&#24335;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(mDCGAN)&#65292;&#38024;&#23545;&#39640;&#36136;&#37327;&#33402;&#26415;&#21697;&#29983;&#25104;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35299;&#20915;&#20102;&#26222;&#36941;&#35757;&#32451;&#38382;&#39064;&#65292;&#26377;&#25928;&#25506;&#32034;&#25277;&#35937;&#32472;&#30011;&#20013;&#30340;&#39068;&#33394;&#21644;&#31508;&#35302;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#33402;&#26415;&#26159;&#19968;&#31181;&#24191;&#21463;&#27426;&#36814;&#12289;&#34987;&#24191;&#27867;&#35752;&#35770;&#30340;&#33402;&#26415;&#24418;&#24335;&#65292;&#36890;&#24120;&#33021;&#22815;&#25551;&#32472;&#20986;&#33402;&#26415;&#23478;&#30340;&#24773;&#24863;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#36793;&#32536;&#26816;&#27979;&#12289;&#31508;&#35302;&#21644;&#24773;&#24863;&#35782;&#21035;&#31639;&#27861;&#26469;&#30740;&#31350;&#25277;&#35937;&#33402;&#26415;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;(GAN)&#23545;&#24191;&#27867;&#20998;&#24067;&#30340;&#25277;&#35937;&#32472;&#30011;&#36827;&#34892;&#30740;&#31350;&#12290; GAN&#20855;&#26377;&#23398;&#20064;&#21644;&#20877;&#29616;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#30740;&#31350;&#29983;&#25104;&#30340;&#22270;&#20687;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#20811;&#26381;&#24120;&#35265;&#35757;&#32451;&#38382;&#39064;&#30340;&#39640;&#25928;GAN&#26550;&#26500;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#39640;&#36136;&#37327;&#33402;&#26415;&#21697;&#29983;&#25104;&#30340;&#25913;&#36827;DCGAN(mDCGAN)&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23545;&#25152;&#20570;&#20462;&#25913;&#30340;&#28145;&#20837;&#25506;&#35752;&#65292;&#28145;&#20837;&#30740;&#31350;DCGAN&#30340;&#22797;&#26434;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18397v1 Announce Type: cross  Abstract: Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, opt
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21069;&#21521;&#26102;&#38388;&#20559;&#24046;&#26657;&#27491;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;ANN-SNN&#36716;&#25442;&#20934;&#30830;&#24615;&#12289;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21457;&#29616;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#26102;&#38388;&#20559;&#24046;&#26657;&#20934;&#33021;&#22815;&#23558;&#36716;&#25442;&#35823;&#24046;&#38477;&#20302;&#33267;&#38646;&#12290;</title><link>https://arxiv.org/abs/2403.18388</link><description>&lt;p&gt;
FTBC: &#29992;&#20110;&#20248;&#21270;ANN-SNN&#36716;&#25442;&#30340;&#21069;&#21521;&#26102;&#38388;&#20559;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18388
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21069;&#21521;&#26102;&#38388;&#20559;&#24046;&#26657;&#27491;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;ANN-SNN&#36716;&#25442;&#20934;&#30830;&#24615;&#12289;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21457;&#29616;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#26102;&#38388;&#20559;&#24046;&#26657;&#20934;&#33021;&#22815;&#23558;&#36716;&#25442;&#35823;&#24046;&#38477;&#20302;&#33267;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30456;&#23545;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#33410;&#33021;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#36884;&#24452;&#65292;&#19982;&#29983;&#29289;&#31070;&#32463;&#36807;&#31243;&#23494;&#20999;&#30456;&#20284;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#36890;&#36807;&#26102;&#31354;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;SNNs&#23384;&#22312;&#22266;&#26377;&#25361;&#25112;&#65292;&#28304;&#33258;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#26102;&#38388;&#21160;&#24577;&#21644;&#20854;&#31163;&#25955;&#20449;&#21495;&#22788;&#29702;&#65292;&#36825;&#38656;&#35201;&#36890;&#36807;ANN-SNN&#36716;&#25442;&#31561;&#26367;&#20195;&#35757;&#32451;&#26041;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21069;&#21521;&#26102;&#38388;&#20559;&#24046;&#26657;&#27491;&#65288;FTBC&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#36716;&#25442;&#20934;&#30830;&#24615;&#32780;&#26080;&#38656;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#22522;&#20110;&#25552;&#20379;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#36890;&#36807;&#27491;&#30830;&#30340;&#26102;&#38388;&#20559;&#24046;&#26657;&#20934;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#21518;ANN-SNN&#36716;&#25442;&#30340;&#39044;&#26399;&#35823;&#24046;&#21487;&#20197;&#38477;&#33267;&#38646;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#25214;&#21040;&#26102;&#38388;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18388v1 Announce Type: new  Abstract: Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient computing compared with Artificial Neural Networks (ANNs), closely mirroring biological neural processes. However, this potential comes with inherent challenges in directly training SNNs through spatio-temporal backpropagation -- stemming from the temporal dynamics of spiking neurons and their discrete signal processing -- which necessitates alternative ways of training, most notably through ANN-SNN conversion. In this work, we introduce a lightweight Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing conversion accuracy without the computational overhead. We ground our method on provided theoretical findings that through proper temporal bias calibration the expected error of ANN-SNN conversion can be reduced to be zero after each time step. We further propose a heuristic algorithm for finding the temporal bias only in the forward pass, thus e
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20026;&#31867;&#22686;&#37327;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20026;&#22270;&#20687;&#29983;&#25104;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.18383</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#26159;&#33391;&#22909;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative Multi-modal Models are Good Class-Incremental Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18383
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20026;&#31867;&#22686;&#37327;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20026;&#22270;&#20687;&#29983;&#25104;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#20998;&#31867;&#22120;&#23545;&#24403;&#21069;&#20219;&#21153;&#30340;&#20559;&#35265;&#32780;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#21028;&#21035;&#27169;&#22411;&#30340;&#29305;&#24615;&#25152;&#33268;&#12290;&#38543;&#30528;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#23558;&#21028;&#21035;&#27169;&#22411;&#26367;&#25442;&#20026;&#29983;&#25104;&#27169;&#22411;&#20197;&#29992;&#20110;CIL&#12290;&#28982;&#32780;&#65292;&#20174;&#21028;&#21035;&#27169;&#22411;&#36807;&#28193;&#21040;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#20449;&#24687;&#36716;&#31227;&#21040;&#19981;&#21516;&#31867;&#21035;&#30340;&#20998;&#31867;&#20013;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#38656;&#35201;&#23558;CIL&#20219;&#21153;&#32622;&#20110;&#29983;&#25104;&#26694;&#26550;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#65288;GMM&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#20351;&#29992;&#32463;&#35843;&#25972;&#30340;&#29983;&#25104;&#27169;&#22411;&#20026;&#22270;&#20687;&#29983;&#25104;&#26631;&#31614;&#12290;&#22312;&#33719;&#24471;&#35814;&#32454;&#25991;&#26412;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18383v1 Announce Type: cross  Abstract: In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic forgetting caused by the classifier's bias towards the current task has long posed a significant challenge. It is mainly caused by the characteristic of discriminative models. With the growing popularity of the generative multi-modal models, we would explore replacing discriminative models with generative ones for CIL. However, transitioning from discriminative to generative models requires addressing two key challenges. The primary challenge lies in transferring the generated textual information into the classification of distinct categories. Additionally, it requires formulating the task of CIL within a generative framework. To this end, we propose a novel generative multi-modal model (GMM) framework for class-incremental learning. Our approach directly generates labels for images using an adapted generative model. After obtaining the detailed text, we use 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#24314;&#27169;&#21644;&#24341;&#20837;&#33258;&#21160;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#21487;&#38752;&#20869;&#23481;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#24402;&#22240;&#20559;&#22909;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18381</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23646;&#24615;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Attributed Text Generation of Large Language Models via Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18381
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#24314;&#27169;&#21644;&#24341;&#20837;&#33258;&#21160;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#21487;&#38752;&#20869;&#23481;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#24402;&#22240;&#20559;&#22909;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#29983;&#25104;&#19981;&#21487;&#38752;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24402;&#22240;&#20316;&#20026;&#25552;&#20379;&#35777;&#25454;&#65288;&#21363;&#24341;&#29992;&#65289;&#30340;&#25163;&#27573;&#26469;&#20943;&#23569;&#38169;&#35823;&#20449;&#24687;&#21644;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#24402;&#22240;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#32034;&#38454;&#27573;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#22312;&#20154;&#31867;&#23398;&#26415;&#20889;&#20316;&#20013;&#21453;&#26144;&#24341;&#25991;&#26426;&#21046;&#20197;&#22686;&#24378;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#24402;&#22240;&#20219;&#21153;&#24314;&#27169;&#20026;&#20559;&#22909;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#33258;&#21160;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#25910;&#38598;&#21644;&#36807;&#28388;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;6,330&#20010;&#31034;&#20363;&#20379;&#21518;&#26399;&#35757;&#32451;&#20351;&#29992;&#30340;&#31934;&#24515;&#25910;&#38598;&#38598;&#21512;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#26631;&#35760;&#20559;&#22909;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#24402;&#22240;&#20559;&#22909;&#25968;&#25454;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;95,263&#23545;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18381v1 Announce Type: cross  Abstract: Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content. Recent works aim to reduce misinformation and hallucinations by resorting to attribution as a means to provide evidence (i.e., citations). However, current attribution methods usually focus on the retrieval stage and automatic evaluation that neglect mirroring the citation mechanisms in human scholarly writing to bolster credibility. In this paper, we address these challenges by modelling the attribution task as preference learning and introducing an Automatic Preference Optimization (APO) framework. First, we create a curated collection for post-training with 6,330 examples by collecting and filtering from existing datasets. Second, considering the high cost of labelling preference data, we further propose an automatic method to synthesize attribution preference data resulting in 95,263 pairs. Mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLPs&#30340;IIP-Mixer&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#20869;&#37096;&#34917;&#19969;&#21644;&#36328;&#34917;&#19969;&#32500;&#24230;&#36827;&#34892;&#28151;&#21512;&#25805;&#20316;&#65292;&#23454;&#29616;&#38146;&#31163;&#23376;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.18379</link><description>&lt;p&gt;
IIP-Mixer&#65306;&#29992;&#20110;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;Intra-Inter Patch&#28151;&#21512;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18379
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLPs&#30340;IIP-Mixer&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#20869;&#37096;&#34917;&#19969;&#21644;&#36328;&#34917;&#19969;&#32500;&#24230;&#36827;&#34892;&#28151;&#21512;&#25805;&#20316;&#65292;&#23454;&#29616;&#38146;&#31163;&#23376;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#23545;&#20110;&#32500;&#25345;&#21487;&#20805;&#30005;&#30005;&#27744;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#31283;&#23450;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#36825;&#39033;&#20219;&#21153;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32593;&#32476;&#65292;&#22914;&#21464;&#21387;&#22120;&#21644;Informer&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24050;&#32463;&#25104;&#20026;&#27969;&#34892;&#30340;&#26550;&#26500;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#20855;&#26377;&#20016;&#23500;&#21442;&#25968;&#30340;&#27169;&#22411;&#24517;&#39035;&#32791;&#36153;&#22823;&#37327;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#25581;&#31034;&#26102;&#38388;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;MLP-Mixer&#30340;&#26550;&#26500;&#65292;&#21517;&#20026;&#8220;Intra-Inter Patch Mixer&#8221;&#65288;IIP-Mixer&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#20165;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#27839;&#30528;&#20869;&#37096;&#34917;&#19969;&#21644;&#36328;&#34917;&#19969;&#32500;&#24230;&#36827;&#34892;&#28151;&#21512;&#25805;&#20316;&#26469;&#25552;&#21462;&#26377;&#20851;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;IIP-Mixer&#21253;&#25324;&#24182;&#34892;&#30340;&#21452;&#22836;&#28151;&#21512;&#22120;&#23618;&#65306;&#20869;&#37096;&#34917;&#19969;&#28151;&#21512;ML
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18379v1 Announce Type: cross  Abstract: Accurately estimating the Remaining Useful Life (RUL) of lithium-ion batteries is crucial for maintaining the safe and stable operation of rechargeable battery management systems. However, this task is often challenging due to the complex temporal dynamics involved. Recently, attention-based networks, such as Transformers and Informer, have been the popular architecture in time series forecasting. Despite their effectiveness, these models with abundant parameters necessitate substantial training time to unravel temporal patterns. To tackle these challenges, we propose a simple MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which is an architecture based exclusively on multi-layer perceptrons (MLPs), extracting information by mixing operations along both intra-patch and inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer comprises parallel dual-head mixer layers: the intra-patch mixing ML
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#29289;&#32852;&#32593;&#29992;&#25143;&#35774;&#22791;&#65288;IIoT UEs&#65289;&#30340;&#24847;&#22270;&#24863;&#30693;DRL&#19978;&#34892;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#23398;&#20064;&#22914;&#20309;&#35843;&#24230;&#36890;&#20449;&#36164;&#28304;&#65292;&#24182;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#31616;&#21270;&#26041;&#26696;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#35843;&#24230;&#26041;&#26696;&#65292;&#33021;&#26377;&#25928;&#20445;&#35777;IIoT UEs&#30340;&#24847;&#22270;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2403.18364</link><description>&lt;p&gt;
&#38754;&#21521;5G-NR&#30340;&#24847;&#22270;&#24863;&#30693;DRL&#19978;&#34892;&#21160;&#24577;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#29289;&#32852;&#32593;&#29992;&#25143;&#35774;&#22791;&#65288;IIoT UEs&#65289;&#30340;&#24847;&#22270;&#24863;&#30693;DRL&#19978;&#34892;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#23398;&#20064;&#22914;&#20309;&#35843;&#24230;&#36890;&#20449;&#36164;&#28304;&#65292;&#24182;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#31616;&#21270;&#26041;&#26696;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#35843;&#24230;&#26041;&#26696;&#65292;&#33021;&#26377;&#25928;&#20445;&#35777;IIoT UEs&#30340;&#24847;&#22270;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25903;&#25345;&#24037;&#19994;&#29289;&#32852;&#32593;&#29992;&#25143;&#35774;&#22791;&#65288;IIoT UEs&#65289;&#20855;&#26377;&#24847;&#22270;&#65288;&#21363;&#25152;&#35831;&#27714;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#65289;&#21644;&#38543;&#26426;&#27969;&#37327;&#21040;&#36798;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#38598;&#20013;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#23398;&#20064;&#22914;&#20309;&#22312;IIoT UEs&#20043;&#38388;&#35843;&#24230;&#21487;&#29992;&#36890;&#20449;&#36164;&#28304;&#30340;&#26102;&#38388;&#39057;&#29575;&#36164;&#28304;&#12290;&#25152;&#25552;&#20986;&#30340;&#35843;&#24230;&#22120;&#21033;&#29992;RL&#26694;&#26550;&#26469;&#36866;&#24212;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#21644;&#27969;&#37327;&#21040;&#36798;&#20013;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#31616;&#21270;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;RL&#26694;&#26550;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20960;&#31181;&#20256;&#32479;&#35843;&#24230;&#26041;&#26696;&#65288;&#22914;&#36718;&#35810;&#12289;&#21322;&#38745;&#24577;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65289;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26234;&#33021;&#35843;&#24230;&#22120;&#22312;&#20445;&#35777;IIoT UEs&#25152;&#34920;&#36798;&#30340;&#24847;&#22270;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18364v1 Announce Type: cross  Abstract: We investigate the problem of supporting Industrial Internet of Things user equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and random traffic arrival. A deep reinforcement learning (DRL) based centralized dynamic scheduler for time-frequency resources is proposed to learn how to schedule the available communication resources among the IIoT UEs. The proposed scheduler leverages an RL framework to adapt to the dynamic changes in the wireless communication system and traffic arrivals. Moreover, a graph-based reduction scheme is proposed to reduce the state and action space of the RL framework to allow fast convergence and a better learning strategy. Simulation results demonstrate the effectiveness of the proposed intelligent scheduler in guaranteeing the expressed intent of IIoT UEs compared to several traditional scheduling schemes, such as round-robin, semi-static, and heuristic approaches. The proposed sche
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20892;&#19994;&#22330;&#26223;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;&#65292;&#33021;&#22815;&#27169;&#25311;&#26893;&#29289;&#30340;&#29983;&#38271;&#38454;&#27573;&#12289;&#22303;&#22756;&#26465;&#20214;&#21644;&#20809;&#29031;&#21464;&#21270;&#65292;&#20026;&#31934;&#20934;&#20892;&#19994;&#20013;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#25552;&#20379;&#20102;&#20840;&#38754;&#36164;&#28304;&#65292;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#22312;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2403.18351</link><description>&lt;p&gt;
&#29983;&#25104;&#29992;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#20892;&#19994;&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#20892;&#19994;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Diverse Agricultural Data for Vision-Based Farming Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18351
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20892;&#19994;&#22330;&#26223;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;&#65292;&#33021;&#22815;&#27169;&#25311;&#26893;&#29289;&#30340;&#29983;&#38271;&#38454;&#27573;&#12289;&#22303;&#22756;&#26465;&#20214;&#21644;&#20809;&#29031;&#21464;&#21270;&#65292;&#20026;&#31934;&#20934;&#20892;&#19994;&#20013;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#25552;&#20379;&#20102;&#20840;&#38754;&#36164;&#28304;&#65292;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#22312;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#30340;&#20892;&#19994;&#22330;&#26223;&#65292;&#37325;&#28857;&#26159;&#22823;&#35910;&#20316;&#29289;&#20197;&#21450;&#21508;&#31181;&#26434;&#33609;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#36825;&#20123;&#26893;&#29289;&#30340;&#19981;&#21516;&#29983;&#38271;&#38454;&#27573;&#12289;&#22810;&#26679;&#21270;&#30340;&#22303;&#22756;&#26465;&#20214;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#19979;&#30340;&#38543;&#26426;&#30000;&#22320;&#24067;&#23616;&#12290;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#32441;&#29702;&#21644;&#29615;&#22659;&#22240;&#32032;&#25972;&#21512;&#21040;&#31243;&#24207;&#21270;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#22686;&#24378;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#36924;&#30495;&#24230;&#21644;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#24102;&#26377;&#35821;&#20041;&#26631;&#31614;&#30340;12,000&#24352;&#22270;&#20687;&#65292;&#20026;&#31934;&#20934;&#20892;&#19994;&#20013;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#36164;&#28304;&#65292;&#22914;&#29992;&#20110;&#33258;&#20027;&#38500;&#33609;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#36890;&#36807;&#23558;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#20892;&#19994;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#20026;&#20892;&#19994;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;-effective&#30340;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18351v1 Announce Type: cross  Abstract: We present a specialized procedural model for generating synthetic agricultural scenes, focusing on soybean crops, along with various weeds. This model is capable of simulating distinct growth stages of these plants, diverse soil conditions, and randomized field arrangements under varying lighting conditions. The integration of real-world textures and environmental factors into the procedural generation process enhances the photorealism and applicability of the synthetic data. Our dataset includes 12,000 images with semantic labels, offering a comprehensive resource for computer vision tasks in precision agriculture, such as semantic segmentation for autonomous weed control. We validate our model's effectiveness by comparing the synthetic data against real agricultural images, demonstrating its potential to significantly augment training data for machine learning models in agriculture. This approach not only provides a cost-effective s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#27169;&#31946;c-&#22343;&#20540;&#25216;&#26415;&#65292;&#29992;&#20110;&#24555;&#36895;&#26816;&#27979;&#22826;&#38451;&#26085;&#20885;&#31354;&#27934;&#65288;CHs&#65289;&#30340;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.18347</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#27169;&#31946;&#30340;&#26041;&#27861;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#22826;&#38451;&#26085;&#20885;&#31354;&#27934;
&lt;/p&gt;
&lt;p&gt;
A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal Holes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#27169;&#31946;c-&#22343;&#20540;&#25216;&#26415;&#65292;&#29992;&#20110;&#24555;&#36895;&#26816;&#27979;&#22826;&#38451;&#26085;&#20885;&#31354;&#27934;&#65288;CHs&#65289;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#26085;&#20885;&#31354;&#27934;&#65288;CHs&#65289;&#30340;&#26816;&#27979;&#21644;&#20998;&#26512;&#26159;&#22826;&#38451;&#29289;&#29702;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#22320;&#30913;&#39118;&#26292;&#65292;&#36825;&#23545;&#21508;&#31181;&#31354;&#38388;&#21644;&#22320;&#38754;&#31995;&#32479;&#37117;&#20135;&#29983;&#30452;&#25509;&#25110;&#38388;&#25509;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22826;&#38451;&#31185;&#23398;&#23478;&#20381;&#36182;&#25163;&#21160;&#32472;&#21046;&#26041;&#27861;&#26469;&#26816;&#27979;CHs&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#19968;&#20123;&#33258;&#21160;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#26816;&#27979;CHs&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24555;&#36895;&#20934;&#30830;&#26816;&#27979;CHs&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#27169;&#31946;c-&#22343;&#20540;&#25216;&#26415;&#65292;&#29992;&#20110;&#24555;&#36895;&#26816;&#27979;CHs&#21306;&#22495;&#12290;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#27169;&#31946;c-&#22343;&#20540;&#65288;QCFFCM&#65289;&#23545;&#22826;&#38451;&#22270;&#20687;&#36827;&#34892;&#20102;&#20998;&#21106;&#65292;&#28982;&#21518;&#22312;&#21518;&#32493;&#38454;&#27573;&#20174;&#20013;&#25552;&#21462;&#20986;CHs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18347v1 Announce Type: cross  Abstract: The detection and analysis of the solar coronal holes (CHs) is an important field of study in the domain of solar physics. Mainly, it is required for the proper prediction of the geomagnetic storms which directly or indirectly affect various space and ground-based systems. For the detection of CHs till date, the solar scientist depends on manual hand-drawn approaches. However, with the advancement of image processing technologies, some automated image segmentation methods have been used for the detection of CHs. In-spite of this, fast and accurate detection of CHs are till a major issues. Here in this work, a novel quantum computing-based fast fuzzy c-mean technique has been developed for fast detection of the CHs region. The task has been carried out in two stages, in first stage the solar image has been segmented using a quantum computing based fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted out from the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#27169;&#22411;&#65292;&#23558;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#38271;&#26399;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18344</link><description>&lt;p&gt;
LC-LLM: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#36710;&#36947;&#21464;&#25442;&#24847;&#22270;&#21644;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#27169;&#22411;&#65292;&#23558;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#38271;&#26399;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23433;&#20840;&#39550;&#39542;&#65292;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24212;&#20855;&#22791;&#20934;&#30830;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#36710;&#36947;&#21464;&#25442;&#24847;&#22270;&#24182;&#39044;&#27979;&#20854;&#26410;&#26469;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;LC-LLM&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#21644;&#33258;&#25105;&#35299;&#37322;&#33021;&#21147;&#26469;&#35299;&#20915;&#29616;&#26377;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#22312;&#38271;&#26399;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24322;&#26500;&#39550;&#39542;&#22330;&#26223;&#20449;&#24687;&#20316;&#20026;LLM&#30340;&#36755;&#20837;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#24494;&#35843;&#25216;&#26415;&#19987;&#38376;&#20026;&#25105;&#20204;&#30340;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#20219;&#21153;&#23450;&#21046;LLM&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;LLM&#30340;&#24378;&#22823;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18344v1 Announce Type: new  Abstract: To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict the lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information in natural language as prompts for input into the LLM and employing a supervised fine-tuning technique to tailor the LLM specifically for our lane change prediction task. This allows us to utilize the LLM's powerfu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2403.18327</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#36827;&#34892;&#27491;&#24335;&#23545;&#35805;&#21527;&#65311;&#33258;&#21160;&#35780;&#20272;LLMs&#22312;&#36716;&#25442;&#21644;&#35299;&#37322;&#27491;&#24335;&#35268;&#33539;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#30410;&#30456;&#20851;&#32773;&#32463;&#24120;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#31995;&#32479;&#38656;&#27714;&#65292;&#28982;&#21518;&#30001;&#39046;&#22495;&#19987;&#23478;&#23558;&#20854;&#36716;&#25442;&#20026;&#24418;&#24335;&#21270;&#35821;&#27861;&#65292;&#20174;&#32780;&#22686;&#21152;&#35774;&#35745;&#25104;&#26412;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#29616;&#25104;&#30340;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#23601;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#20854;&#32763;&#35793;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#35821;&#35328;&#35821;&#27861;&#29983;&#25104;&#24418;&#24335;&#21270;&#35821;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#20197;&#34913;&#37327;&#36825;&#31181;&#32763;&#35793;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18327v1 Announce Type: cross  Abstract: Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task 
&lt;/p&gt;</description></item><item><title>&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.18314</link><description>&lt;p&gt;
&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Chinese Offensive Language Detection:Current Status and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18314
&lt;/p&gt;
&lt;p&gt;
&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#20570;&#20986;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#30417;&#27979;&#21644;&#35268;&#33539;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65292;&#20294;&#22312;&#25968;&#23383;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#35821;&#35328;&#65288;&#22914;&#20167;&#24680;&#35328;&#35770;&#25110;&#32593;&#32476;&#27450;&#20940;&#65289;&#30340;&#26222;&#36941;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#37492;&#20110;&#32500;&#25252;&#25991;&#26126;&#21644;&#23562;&#37325;&#30340;&#22312;&#32447;&#29615;&#22659;&#30340;&#37325;&#35201;&#24615;&#65292;&#36843;&#20999;&#38656;&#35201;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#30340;&#33258;&#21160;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#24320;&#21457;&#22788;&#29702;&#27721;&#35821;&#31561;&#35821;&#35328;&#30340;&#26377;&#25928;&#31995;&#32479;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35821;&#35328;&#30340;&#22797;&#26434;&#21644;&#24494;&#22937;&#24615;&#20351;&#24471;&#33258;&#21160;&#22788;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#20102;&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#24773;&#20917;&#65292;&#23457;&#26597;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#22312;&#36825;&#31181;&#22797;&#26434;&#35821;&#35328;&#20013;&#26816;&#27979;&#24694;&#24847;&#35821;&#35328;&#30340;&#29420;&#29305;&#25361;&#25112;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18314v1 Announce Type: cross  Abstract: Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary object
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#30340;&#28909;&#21147;&#23398;&#19968;&#33268;&#26448;&#26009;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#30701;&#32420;&#32500;/&#32858;&#21512;&#29289;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#32452;&#21512;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#39044;&#27979;&#20869;&#37096;&#21464;&#37327;&#24182;&#23450;&#20041;&#25972;&#20010;&#31995;&#32479;&#30340;&#28909;&#21147;&#23398;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.18310</link><description>&lt;p&gt;
&#19968;&#31181;&#28909;&#21147;&#23398;&#19968;&#33268;&#30340;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#26448;&#26009;&#27169;&#22411;&#65292;&#29992;&#20110;&#30701;&#32420;&#32500;/&#32858;&#21512;&#29289;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#30340;&#28909;&#21147;&#23398;&#19968;&#33268;&#26448;&#26009;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#30701;&#32420;&#32500;/&#32858;&#21512;&#29289;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#32452;&#21512;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#39044;&#27979;&#20869;&#37096;&#21464;&#37327;&#24182;&#23450;&#20041;&#25972;&#20010;&#31995;&#32479;&#30340;&#28909;&#21147;&#23398;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#65288;PIDL&#65289;&#30340;&#26412;&#26500;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#22312;&#21508;&#31181;&#29615;&#22659;&#26465;&#20214;&#19979;&#30701;&#32420;&#32500;&#22686;&#24378;&#32435;&#31859;&#31890;&#23376;&#22635;&#20805;&#29615;&#27687;&#26641;&#33026;&#30340;&#31896;&#24377;-&#31896;&#22609;&#34892;&#20026;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#20197;&#24378;&#21046;&#25191;&#34892;&#28909;&#21147;&#23398;&#21407;&#29702;&#65292;&#20174;&#32780;&#24471;&#21040;&#28909;&#21147;&#23398;&#19968;&#33268;&#30340;&#26412;&#26500;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#23558;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#39044;&#27979;&#34920;&#24449;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#20869;&#37096;&#32791;&#25955;&#25152;&#38656;&#30340;&#20869;&#37096;&#21464;&#37327;&#12290;&#27492;&#22806;&#65292;&#21478;&#19968;&#20010;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25351;&#31034;&#33258;&#30001;&#33021;&#20989;&#25968;&#65292;&#20174;&#32780;&#23450;&#20041;&#25972;&#20010;&#31995;&#32479;&#30340;&#28909;&#21147;&#23398;&#29366;&#24577;&#12290;PIDL&#27169;&#22411;&#26368;&#21021;&#38024;&#23545;&#19977;&#32500;&#24773;&#20917;&#36827;&#34892;&#24320;&#21457;&#65292;&#36890;&#36807;&#20174;&#32463;&#20856;&#26412;&#26500;&#27169;&#22411;&#20013;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#30452;&#25509;&#25552;&#21462;&#24490;&#29615;&#21152;&#36733;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18310v1 Announce Type: cross  Abstract: This work proposes a physics-informed deep learning (PIDL)-based constitutive model for investigating the viscoelastic-viscoplastic behavior of short fiber-reinforced nanoparticle-filled epoxies under various ambient conditions. The deep-learning model is trained to enforce thermodynamic principles, leading to a thermodynamically consistent constitutive model. To accomplish this, a long short-term memory network is combined with a feed-forward neural network to predict internal variables required for characterizing the internal dissipation of the nanocomposite materials. In addition, another feed-forward neural network is used to indicate the free-energy function, which enables defining the thermodynamic state of the entire system. The PIDL model is initially developed for the three-dimensional case by generating synthetic data from a classical constitutive model. The model is then trained by extracting the data directly from cyclic lo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32508;&#21512;&#21033;&#29992;NFT&#20132;&#26131;&#35760;&#24405;&#21644;&#22806;&#37096;&#39033;&#30446;&#29305;&#24449;&#31561;&#22810;&#31181;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#25968;&#25454;&#39640;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#24182;&#21033;&#29992;&#36229;&#20986;&#29992;&#25143;-&#39033;&#30446;&#20114;&#21160;&#30340;&#36755;&#20837;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18305</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#39033;&#30446;&#29305;&#24449;&#30340;NFT&#21487;&#25910;&#34255;&#21697;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Recommender System for NFT Collectibles with Item Feature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32508;&#21512;&#21033;&#29992;NFT&#20132;&#26131;&#35760;&#24405;&#21644;&#22806;&#37096;&#39033;&#30446;&#29305;&#24449;&#31561;&#22810;&#31181;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#25968;&#25454;&#39640;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#24182;&#21033;&#29992;&#36229;&#20986;&#29992;&#25143;-&#39033;&#30446;&#20114;&#21160;&#30340;&#36755;&#20837;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#34987;&#31215;&#26497;&#30740;&#31350;&#24182;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#20197;&#35299;&#20915;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20851;&#20110;&#30005;&#24433;&#12289;&#38899;&#20048;&#21644;&#30005;&#23376;&#21830;&#21153;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#20294;&#30456;&#27604;&#20043;&#19979;&#65292;&#23613;&#31649;NFT&#24066;&#22330;&#25345;&#32493;&#22686;&#38271;&#65292;&#23545;&#20110;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#21364;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#20174;NFT&#20132;&#26131;&#35760;&#24405;&#21040;&#22806;&#37096;&#39033;&#30446;&#29305;&#24449;&#65292;&#29983;&#25104;&#31526;&#21512;&#20010;&#20154;&#20559;&#22909;&#30340;&#31934;&#30830;&#25512;&#33616;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#27599;&#20010;&#39033;&#30446;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;&#33410;&#28857;&#29305;&#24449;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#30340;&#33410;&#28857;&#65288;&#39033;&#30446;&#65289;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36229;&#20986;&#29992;&#25143;-&#39033;&#30446;&#20114;&#21160;&#30340;&#36755;&#20837;&#65292;&#22914;&#22270;&#20687;&#29305;&#24449;&#12289;&#25991;&#26412;&#29305;&#24449;&#21644;&#20215;&#26684;&#29305;&#24449;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18305v1 Announce Type: cross  Abstract: Recommender systems have been actively studied and applied in various domains to deal with information overload. Although there are numerous studies on recommender systems for movies, music, and e-commerce, comparatively less attention has been paid to the recommender system for NFTs despite the continuous growth of the NFT market. This paper presents a recommender system for NFTs that utilizes a variety of data sources, from NFT transaction records to external item features, to generate precise recommendations that cater to individual preferences. We develop a data-efficient graph-based recommender system to efficiently capture the complex relationship between each item and users and generate node(item) embeddings which incorporate both node feature information and graph structure. Furthermore, we exploit inputs beyond user-item interactions, such as image feature, text feature, and price feature. Numerical experiments verify the perf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SelMix&#65292;&#19968;&#31181;&#36873;&#25321;&#24615;&#28151;&#21512;&#30340;&#24265;&#20215;&#24494;&#35843;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#38750;&#21487;&#20998;&#35299;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.18301</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#28151;&#21512;&#24494;&#35843;&#20197;&#20248;&#21270;&#38750;&#21487;&#20998;&#35299;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18301
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SelMix&#65292;&#19968;&#31181;&#36873;&#25321;&#24615;&#28151;&#21512;&#30340;&#24265;&#20215;&#24494;&#35843;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#38750;&#21487;&#20998;&#35299;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#20351;&#29992;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#22823;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#20174;&#32780;&#37319;&#29992;&#20102;&#21508;&#31181;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#20043;&#21069;&#65292;&#24517;&#39035;&#20005;&#26684;&#35780;&#20272;&#23427;&#20204;&#22312;&#35832;&#22914;&#26368;&#22351;&#24773;&#20917;&#21484;&#22238;&#29575;&#20043;&#31867;&#30340;&#24615;&#33021;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#28385;&#36275;&#20844;&#24179;&#24615;&#31561;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32463;&#39564;&#25216;&#26415;&#22312;&#36825;&#20123;&#23454;&#38469;&#30340;&#12289;&#38750;&#21487;&#20998;&#35299;&#30340;&#24615;&#33021;&#30446;&#26631;&#19978;&#25552;&#20379;&#20102;&#27425;&#20248;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29702;&#35770;&#25216;&#26415;&#38656;&#35201;&#20026;&#27599;&#20010;&#24615;&#33021;&#30446;&#26631;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#26032;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SelMix&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;&#28151;&#21512;&#30340;&#24265;&#20215;&#24494;&#35843;&#25216;&#26415;&#65292;&#29992;&#20110;&#38024;&#23545;&#25152;&#38656;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18301v1 Announce Type: cross  Abstract: The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a
&lt;/p&gt;</description></item><item><title>GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2403.18296</link><description>&lt;p&gt;
GeNet:&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18296
&lt;/p&gt;
&lt;p&gt;
GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35821;&#20041;&#36890;&#20449;&#20219;&#21153;&#26041;&#27861;&#20381;&#36182;&#20110;&#20102;&#35299;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26469;&#20943;&#36731;&#36890;&#36947;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#30340;SNR&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#26088;&#22312;&#25269;&#25239;&#22122;&#22768;&#65292;&#20174;&#32780;&#20419;&#36827;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#65288;TOC&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#25968;&#25454;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12290;&#28982;&#21518;&#21033;&#29992;&#22522;&#20110;GNN&#30340;&#32534;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#25552;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#28982;&#21518;&#36890;&#36807;&#36890;&#36947;&#20256;&#36755;&#12290;&#22312;&#25509;&#25910;&#31471;&#65292;&#20351;&#29992;&#22522;&#20110;GNN&#30340;&#35299;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#37325;&#24314;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#29992;&#20110;TOC&#12290;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GeNet&#22312;&#25239;&#22122;&#22768;TOC&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18296v1 Announce Type: cross  Abstract: Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoup
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#29305;&#23450;&#20999;&#29255;&#37325;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#20998;&#24067;&#29255;&#27573;&#19978;&#33719;&#24471;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.18286</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#37325;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Recalibration of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18286
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#29305;&#23450;&#20999;&#29255;&#37325;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#20998;&#24067;&#29255;&#27573;&#19978;&#33719;&#24471;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#25552;&#21462;&#20986;&#26657;&#20934;&#33391;&#22909;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#21453;&#26144;&#20102;&#20854;&#27491;&#30830;&#24615;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;LMs&#22312;&#24191;&#27867;&#20998;&#24067;&#19978;&#21487;&#33021;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#65292;&#20294;&#36825;&#24448;&#24448;&#38544;&#34255;&#22312;&#26356;&#31364;&#20998;&#29255;&#20869;&#23384;&#22312;&#26174;&#33879;&#30340;&#26657;&#20934;&#19981;&#20934;&#30830;&#24615;&#65288;&#20363;&#22914;&#65292;&#22312;&#25968;&#23398;&#20013;&#23384;&#22312;&#31995;&#32479;&#24615;&#36807;&#24230;&#33258;&#20449;&#21487;&#33021;&#20250;&#24179;&#34913;&#21382;&#21490;&#20013;&#30340;&#31995;&#32479;&#24615;&#19981;&#36275;&#33258;&#20449;&#65292;&#20174;&#32780;&#22312;&#24635;&#20307;&#19978;&#23454;&#29616;&#23436;&#32654;&#26657;&#20934;&#65289;&#12290;&#20026;&#20102;&#33719;&#24471;&#20219;&#20309;&#20998;&#24067;&#29255;&#27573;&#30340;&#26657;&#20934;&#33391;&#22909;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#29305;&#23450;&#20999;&#29255;&#37325;&#26657;&#20934;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#37325;&#26032;&#26657;&#20934;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25509;&#21463;&#26469;&#33258;&#20219;&#20309;&#32473;&#23450;&#20999;&#29255;&#30340;&#23569;&#37327;&#26080;&#26631;&#31614;&#31034;&#20363;&#65292;&#24182;&#39044;&#27979;&#19968;&#26465;&#37325;&#26032;&#26144;&#23556;&#32622;&#20449;&#24230;&#20998;&#25968;&#20197;&#20351;&#20854;&#23545;&#35813;&#20999;&#29255;&#26356;&#20934;&#30830;&#30340;&#26354;&#32447;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#37325;&#26032;&#26657;&#20934;&#20219;&#24847;&#26032;&#30340;&#20999;&#29255;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#35813;&#20999;&#29255;&#30340;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18286v1 Announce Type: cross  Abstract: Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#24335;&#25366;&#25496;&#35782;&#21035;&#21644;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39592;&#24178;&#65292;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#30340;&#26680;&#24515;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#38598;&#21512;&#35206;&#30422;&#39118;&#26684;&#38382;&#39064;&#21644;&#30456;&#20851;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#20854;&#25910;&#25947;&#21040;ILP&#20844;&#24335;&#30340;&#24085;&#32047;&#25176;&#22343;&#34913;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.18278</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#24335;&#25366;&#25496;&#35782;&#21035;&#21644;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39592;&#24178;
&lt;/p&gt;
&lt;p&gt;
Identification and Uses of Deep Learning Backbones via Pattern Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18278
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#24335;&#25366;&#25496;&#35782;&#21035;&#21644;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39592;&#24178;&#65292;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#30340;&#26680;&#24515;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#38598;&#21512;&#35206;&#30422;&#39118;&#26684;&#38382;&#39064;&#21644;&#30456;&#20851;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#20854;&#25910;&#25947;&#21040;ILP&#20844;&#24335;&#30340;&#24085;&#32047;&#25176;&#22343;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18278v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#40657;&#30418;&#26041;&#27861;&#22312;&#35768;&#22810;&#25968;&#25454;&#25366;&#25496;&#39046;&#22495;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#30340;&#26680;&#24515;&#26426;&#21046;&#26159;&#19968;&#20010;&#30456;&#23545;&#23569;&#34987;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20026;&#32473;&#23450;&#32452;&#23454;&#20363;&#35782;&#21035;&#28145;&#24230;&#23398;&#20064;&#30340;&#39592;&#24178;&#30340;&#27010;&#24565;&#12290;&#36825;&#37324;&#30340;&#8220;&#32452;&#8221;&#21487;&#20197;&#26159;&#21516;&#19968;&#31867;&#21035;&#30340;&#23454;&#20363;&#65292;&#29978;&#33267;&#26159;&#21516;&#19968;&#31867;&#21035;&#30340;&#35823;&#20998;&#31867;&#23454;&#20363;&#12290;&#25105;&#20204;&#23558;&#32473;&#23450;&#32452;&#30340;&#27599;&#20010;&#23454;&#20363;&#35270;&#20026;&#28608;&#27963;&#19968;&#32452;&#31070;&#32463;&#20803;&#65292;&#24182;&#23581;&#35797;&#25214;&#21040;&#19982;&#32473;&#23450;&#27010;&#24565;/&#32452;&#30456;&#20851;&#32852;&#30340;&#31070;&#32463;&#20803;&#23376;&#22270;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#23450;&#20041;&#20026;&#38598;&#21512;&#35206;&#30422;&#39118;&#26684;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#26159;&#19981;&#21487;&#35299;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#24230;&#21463;&#38480;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#20844;&#24335;&#12290;&#20316;&#20026;&#26367;&#20195;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#29575;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#19982;&#27169;&#24335;&#25366;&#25496;&#30456;&#20851;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#25910;&#25947;&#21040;ILP&#20844;&#24335;&#30340;&#24085;&#32047;&#25176;&#22343;&#34913;&#28857;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20123;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18278v1 Announce Type: new  Abstract: Deep learning is extensively used in many areas of data mining as a black-box method with impressive results. However, understanding the core mechanism of how deep learning makes predictions is a relatively understudied problem. Here we explore the notion of identifying a backbone of deep learning for a given group of instances. A group here can be instances of the same class or even misclassified instances of the same class. We view each instance for a given group as activating a subset of neurons and attempt to find a subgraph of neurons associated with a given concept/group. We formulate this problem as a set cover style problem and show it is intractable and presents a highly constrained integer linear programming (ILP) formulation. As an alternative, we explore a coverage-based heuristic approach related to pattern mining, and show it converges to a Pareto equilibrium point of the ILP formulation. Experimentally we explore these bac
&lt;/p&gt;</description></item><item><title>DSF-GAN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#32467;&#21512;&#19979;&#28216;&#39044;&#27979;&#27169;&#22411;&#30340;&#21453;&#39304;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21512;&#25104;&#26679;&#26412;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18267</link><description>&lt;p&gt;
DSF-GAN: &#19979;&#28216;&#21453;&#39304;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DSF-GAN: DownStream Feedback Generative Adversarial Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18267
&lt;/p&gt;
&lt;p&gt;
DSF-GAN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#32467;&#21512;&#19979;&#28216;&#39044;&#27979;&#27169;&#22411;&#30340;&#21453;&#39304;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21512;&#25104;&#26679;&#26412;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#24615;&#26159;&#34913;&#37327;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#36136;&#37327;&#30340;&#20004;&#20010;&#20851;&#38190;&#25351;&#26631;&#12290;&#23613;&#31649;&#22312;&#38544;&#31169;&#25514;&#26045;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#20855;&#26377;&#39640;&#23454;&#29992;&#24615;&#30340;&#21512;&#25104;&#26679;&#26412;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#21512;&#25104;&#26679;&#26412;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DownStream Feedback&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;DSF-GAN&#65289;&#30340;&#26032;&#26550;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32467;&#21512;&#20102;&#19979;&#28216;&#39044;&#27979;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#29992;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#22686;&#24378;&#29983;&#25104;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;DSF-GAN&#21033;&#29992;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26469;&#22686;&#24378;&#21512;&#25104;&#26679;&#26412;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;DSF-GAN&#29983;&#25104;&#30340;&#21512;&#25104;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#30456;&#27604;&#20110;&#27809;&#26377;&#21453;&#39304;&#30340;&#30456;&#21516;GAN&#26550;&#26500;&#29983;&#25104;&#30340;&#26679;&#26412;&#12290;&#35780;&#20272;&#26159;&#22312;&#21516;&#19968;&#20010;&#39564;&#35777;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;re
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18267v1 Announce Type: cross  Abstract: Utility and privacy are two crucial measurements of the quality of synthetic tabular data. While significant advancements have been made in privacy measures, generating synthetic samples with high utility remains challenging. To enhance the utility of synthetic samples, we propose a novel architecture called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This approach incorporates feedback from a downstream prediction model during training to augment the generator's loss function with valuable information. Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of synthetic samples. To evaluate our method, we tested it using two popular datasets. Our experiments demonstrate improved model performance when training on synthetic samples generated by DSF-GAN, compared to those generated by the same GAN architecture without feedback. The evaluation was conducted on the same validation set comprising re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36951;&#24536;&#26426;&#21046;&#25552;&#21319;&#29983;&#25104;&#24335;&#31867;&#22686;&#37327;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18258</link><description>&lt;p&gt;
&#29992;&#36951;&#24536;&#26426;&#21046;&#26041;&#27861;&#25552;&#21319;&#29983;&#25104;&#24335;&#31867;&#22686;&#37327;&#23398;&#20064;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Generative Class Incremental Learning Performance with Model Forgetting Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36951;&#24536;&#26426;&#21046;&#25552;&#21319;&#29983;&#25104;&#24335;&#31867;&#22686;&#37327;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36951;&#24536;&#26426;&#21046;&#26469;&#22686;&#24378;&#29983;&#25104;&#24335;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;GCIL&#65289;&#65292;&#26088;&#22312;&#21160;&#24577;&#31649;&#29702;&#31867;&#20449;&#24687;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25968;&#25454;&#27969;&#12290; GCIL &#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#28909;&#38376;&#35805;&#39064;&#20043;&#19968;&#65292;&#34987;&#35748;&#20026;&#26159;&#31038;&#20250;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290; &#36951;&#24536;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#22823;&#33041;&#21151;&#33021;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20002;&#24323;&#23545;&#20154;&#31867;&#19981;&#22826;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39046;&#22495;&#65292;&#25925;&#24847;&#24536;&#35760;&#30340;&#27010;&#24565;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23558;&#36951;&#24536;&#26426;&#21046;&#32435;&#20837;GCIL&#20013;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#20174;&#32780;&#26816;&#39564;&#23427;&#20204;&#23545;&#27169;&#22411;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290; &#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25972;&#21512;&#36951;&#24536;&#26426;&#21046;&#26174;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18258v1 Announce Type: cross  Abstract: This study presents a novel approach to Generative Class Incremental Learning (GCIL) by introducing the forgetting mechanism, aimed at dynamically managing class information for better adaptation to streaming data. GCIL is one of the hot topics in the field of computer vision, and this is considered one of the crucial tasks in society, specifically the continual learning of generative models. The ability to forget is a crucial brain function that facilitates continual learning by selectively discarding less relevant information for humans. However, in the field of machine learning models, the concept of intentionally forgetting has not been extensively investigated. In this study we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL, thereby examining their impact on the models' ability to learn in continual learning. Through our experiments, we have found that integrating the forgetting mechanisms significantl
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36731;&#24494;&#25200;&#21160;&#26469;&#25351;&#23450;&#21644;&#27880;&#20837;&#21508;&#31181;&#38544;&#34255;&#30340;&#24694;&#24847;&#34892;&#20026;&#65292;&#21363;&#21518;&#38376;&#65292;&#21040;&#31070;&#32463;&#36335;&#24452;&#35268;&#21010;&#22120;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.18256</link><description>&lt;p&gt;
&#36890;&#36807;&#36731;&#24494;&#25200;&#21160;&#25805;&#32437;&#31070;&#32463;&#36335;&#24452;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Manipulating Neural Path Planners via Slight Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18256
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36731;&#24494;&#25200;&#21160;&#26469;&#25351;&#23450;&#21644;&#27880;&#20837;&#21508;&#31181;&#38544;&#34255;&#30340;&#24694;&#24847;&#34892;&#20026;&#65292;&#21363;&#21518;&#38376;&#65292;&#21040;&#31070;&#32463;&#36335;&#24452;&#35268;&#21010;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#36335;&#24452;&#35268;&#21010;&#22120;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#37096;&#20214;&#36890;&#24120;&#20316;&#20026;&#40657;&#21283;&#23376;&#21576;&#29616;&#65292;&#25513;&#30422;&#20102;&#20854;&#22522;&#30784;&#20915;&#31574;&#36807;&#31243;&#12290;&#23427;&#20204;&#30340;&#40657;&#21283;&#23376;&#24615;&#36136;&#20351;&#23427;&#20204;&#38754;&#20020;&#34987;&#36890;&#36807;&#25554;&#20837;&#38544;&#34255;&#24694;&#24847;&#34892;&#20026;&#26469;&#31713;&#25913;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23450;&#21644;&#27880;&#20837;&#21508;&#31181;&#38544;&#34255;&#24694;&#24847;&#34892;&#20026;&#65292;&#31216;&#20026;&#21518;&#38376;&#65292;&#21040;&#31070;&#32463;&#36335;&#24452;&#35268;&#21010;&#22120;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#27905;&#20294;&#28789;&#27963;&#30340;&#23450;&#20041;&#36825;&#20123;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38544;&#34255;&#34892;&#20026;&#21487;&#20197;&#36890;&#36807;&#36731;&#24494;&#25200;&#21160;&#65288;&#20363;&#22914;&#65292;&#25554;&#20837;&#24494;&#23567;&#30340;&#19981;&#26126;&#26174;&#30340;&#25200;&#21160;&#65289;&#26469;&#35302;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18256v1 Announce Type: cross  Abstract: Data-driven neural path planners are attracting increasing interest in the robotics community. However, their neural network components typically come as black boxes, obscuring their underlying decision-making processes. Their black-box nature exposes them to the risk of being compromised via the insertion of hidden malicious behaviors. For example, an attacker may hide behaviors that, when triggered, hijack a delivery robot by guiding it to a specific (albeit wrong) destination, trapping it in a predefined region, or inducing unnecessary energy expenditure by causing the robot to repeatedly circle a region. In this paper, we propose a novel approach to specify and inject a range of hidden malicious behaviors, known as backdoors, into neural path planners. Our approach provides a concise but flexible way to define these behaviors, and we show that hidden behaviors can be triggered by slight perturbations (e.g., inserting a tiny unnotic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.18252</link><description>&lt;p&gt;
&#36229;&#36234;&#23884;&#20837;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35270;&#35273;&#34920;&#26684;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#30707;&#65292;&#20174;&#20855;&#26377;&#20154;&#31867;&#27880;&#37322;&#26631;&#31614;&#30340;&#30417;&#30563;&#23398;&#20064;&#21457;&#23637;&#21040;&#23545;&#40784;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#30340;&#35270;&#35273;&#34920;&#31034;&#65288;&#22914;CLIP&#23884;&#20837;&#65289;&#36890;&#24120;&#32570;&#20047;&#20851;&#38190;&#30340;&#22806;&#37096;&#19990;&#30028;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Visual Table&#65292;&#36825;&#26159;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#25552;&#20379;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#30340;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#65292;&#21253;&#25324;&#22330;&#26223;&#25551;&#36848;&#21644;&#28085;&#30422;&#31867;&#21035;&#12289;&#23646;&#24615;&#21644;&#23454;&#20363;&#32423;&#21035;&#30693;&#35782;&#30340;&#22810;&#20010;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#20174;GPT4V&#30340;&#23567;&#35268;&#27169;&#27880;&#37322;&#20013;&#29983;&#25104;&#35270;&#35273;&#34920;&#26684;&#65292;&#24182;&#35757;&#32451;&#23427;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#35270;&#35273;&#34920;&#26684;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#34920;&#31034;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18252v1 Announce Type: cross  Abstract: Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35805;&#24335;&#38382;&#31572;&#30340;&#20250;&#35805;&#32423;RAG&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#26816;&#32034;&#22686;&#24378;&#21644;&#33258;&#26816;&#65292;&#23454;&#29616;&#20102;&#38382;&#21477;&#29702;&#35299;&#21644;&#30456;&#20851;&#20449;&#24687;&#33719;&#21462;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.18243</link><description>&lt;p&gt;
&#29992;&#32454;&#31890;&#24230;&#26816;&#32034;&#22686;&#24378;&#21644;&#33258;&#26816;&#25552;&#21319;&#23545;&#35805;&#24335;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35805;&#24335;&#38382;&#31572;&#30340;&#20250;&#35805;&#32423;RAG&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#26816;&#32034;&#22686;&#24378;&#21644;&#33258;&#26816;&#65292;&#23454;&#29616;&#20102;&#38382;&#21477;&#29702;&#35299;&#21644;&#30456;&#20851;&#20449;&#24687;&#33719;&#21462;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26088;&#22312;&#36890;&#36807;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#21152;&#22806;&#37096;&#24222;&#22823;&#32780;&#21160;&#24577;&#30340;&#30693;&#35782;&#65292;&#29983;&#25104;&#26356;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35805;&#24335;&#38382;&#31572;&#65288;CQA&#65289;&#30340;&#20250;&#35805;&#32423;RAG&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#32454;&#31890;&#24230;&#26816;&#32034;&#22686;&#24378;&#21644;&#33258;&#26816;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23545;&#35805;&#24335;&#38382;&#39064;&#32454;&#21270;&#22120;&#12289;&#32454;&#31890;&#24230;&#26816;&#32034;&#22120;&#21644;&#22522;&#20110;&#33258;&#26816;&#30340;&#21709;&#24212;&#29983;&#25104;&#22120;&#19977;&#20010;&#32452;&#20214;&#65292;&#23427;&#20204;&#22312;&#23545;&#35805;&#24773;&#22659;&#20013;&#21327;&#20316;&#24037;&#20316;&#65292;&#29992;&#20110;&#38382;&#21477;&#29702;&#35299;&#21644;&#30456;&#20851;&#20449;&#24687;&#33719;&#21462;&#12290;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#35805;&#24335;&#38382;&#31572;&#20013;&#20855;&#26377;&#24040;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18243v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) aims to generate more reliable and accurate responses, by augmenting large language models (LLMs) with the external vast and dynamic knowledge. Most previous work focuses on using RAG for single-round question answering, while how to adapt RAG to the complex conversational setting wherein the question is interdependent on the preceding context is not well studied. In this paper, we propose a conversation-level RAG approach, which incorporates fine-grained retrieval augmentation and self-check for conversational question answering (CQA). In particular, our approach consists of three components, namely conversational question refiner, fine-grained retriever and self-check based response generator, which work collaboratively for question understanding and relevant information acquisition in conversational settings. Extensive experiments demonstrate the great advantages of our approach over the state-of-t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#22686;&#24378;&#24314;&#27169;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#21644;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.18241</link><description>&lt;p&gt;
NeuSDFusion: &#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;3D&#24418;&#29366;&#30340;&#23436;&#25104;&#12289;&#37325;&#24314;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18241
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#22686;&#24378;&#24314;&#27169;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#21644;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24418;&#29366;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#26465;&#20214;&#21644;&#32422;&#26463;&#30340;&#21019;&#26032;&#24615;3D&#20869;&#23481;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;3D&#24418;&#29366;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23616;&#37096;&#32452;&#20214;&#65292;&#23558;&#27599;&#20010;&#20803;&#32032;&#23396;&#31435;&#22788;&#29702;&#32780;&#19981;&#32771;&#34385;&#31354;&#38388;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;3D&#25968;&#25454;&#34920;&#31034;&#21644;&#24418;&#29366;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#22810;&#26679;&#24615;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#19988;&#31526;&#21512;&#25351;&#23450;&#32422;&#26463;&#30340;3D&#24418;&#29366;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#26469;&#22686;&#24378;3D&#24418;&#29366;&#24314;&#27169;&#12290;&#20026;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#24182;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31181;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#65292;&#30452;&#25509;&#20351;&#29992;&#27491;&#20132;&#30340;2D&#24179;&#38754;&#23398;&#20064;3D&#24418;&#29366;&#30340;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20256;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18241v1 Announce Type: cross  Abstract: 3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a tra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34892;&#20026;&#27169;&#25311;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#25512;&#29702;&#24378;&#21270;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;&#20154;&#31867;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#30456;&#23218;&#32654;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18230</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#30740;&#31350;&#21592;&#36827;&#34892;&#25512;&#29702;&#65306;&#36890;&#36807;&#34892;&#20026;&#27169;&#25311;&#22312;&#22797;&#26434;&#20154;&#31867;&#31995;&#32479;&#20013;&#25104;&#20026;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18230
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34892;&#20026;&#27169;&#25311;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#25512;&#29702;&#24378;&#21270;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;&#20154;&#31867;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#30456;&#23218;&#32654;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32467;&#21512;&#21508;&#31181;&#25512;&#29702;&#24378;&#21270;&#26041;&#27861;&#65292;&#22312;&#25968;&#23398;&#12289;&#27861;&#24459;&#12289;&#32534;&#30721;&#12289;&#24120;&#35782;&#21644;&#19990;&#30028;&#30693;&#35782;&#31561;&#39046;&#22495;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#30456;&#23218;&#32654;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#22797;&#26434;&#20154;&#31867;&#31995;&#32479;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#39532;&#36187;&#20811;&#19987;&#23478;&#35266;&#23519;&#22681;&#8221;&#65288;MEOW&#65289;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#27169;&#25311;&#25216;&#26415;&#12290;&#22312;MEOW&#26694;&#26550;&#20013;&#65292;&#21033;&#29992;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#19987;&#23478;&#27169;&#22411;&#65292;&#38598;&#20013;&#20110;&#27599;&#20010;&#29420;&#31435;&#27169;&#25311;&#26102;&#38388;&#20869;&#20851;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#8220;&#32463;&#39564;&#8221;&#12290;&#27491;&#26159;&#36890;&#36807;&#27169;&#25311;&#32047;&#31215;&#30340;&#8220;&#32463;&#39564;&#8221;&#20351;&#20043;&#25104;&#20026;&#22797;&#26434;&#20154;&#31867;&#31995;&#32479;&#20013;&#20219;&#21153;&#19987;&#23478;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21453;&#26144;&#30495;&#23454;&#23433;&#20840;&#22330;&#26223;&#30340;&#36890;&#20449;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18230v1 Announce Type: new  Abstract: Large language models (LLMs), in conjunction with various reasoning reinforcement methodologies, have demonstrated remarkable capabilities comparable to humans in fields such as mathematics, law, coding, common sense, and world knowledge. In this paper, we delve into the reasoning abilities of LLMs within complex human systems. We propose a novel reasoning framework, termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting generative-agents-based simulation technique. In the MEOW framework, simulated data are utilized to train an expert model concentrating ``experience'' about a specific task in each independent time of simulation. It is the accumulated ``experience'' through the simulation that makes for an expert on a task in a complex human system. We conduct the experiments within a communication game that mirrors real-world security scenarios. The results indicate that our proposed methodology can cooperate with existing methodol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;DPI&#31639;&#27861;&#65292;&#26088;&#22312;&#26816;&#27979;&#24694;&#24847;&#27969;&#37327;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#26434;&#30340;&#24207;&#21015;&#25968;&#25454;&#20869;&#23481;&#24182;&#25512;&#24191;&#21040;&#31867;&#20284;&#22330;&#26223;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.18223</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#26377;&#25928;&#36733;&#33655;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Transformer-Based Framework for Payload Malware Detection and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;DPI&#31639;&#27861;&#65292;&#26088;&#22312;&#26816;&#27979;&#24694;&#24847;&#27969;&#37327;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#26434;&#30340;&#24207;&#21015;&#25968;&#25454;&#20869;&#23481;&#24182;&#25512;&#24191;&#21040;&#31867;&#20284;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#32593;&#32476;&#23041;&#32961;&#22312;&#20837;&#20405;&#35745;&#31639;&#26426;&#32593;&#32476;&#26041;&#38754;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#26377;&#25928;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDSs&#65289;&#30340;&#38656;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;IDSs&#20381;&#36182;&#20110;&#24322;&#24120;&#26816;&#27979;&#21644;&#22522;&#20110;&#29305;&#24449;&#24211;&#30340;&#26816;&#27979;&#25216;&#26415;&#26469;&#26816;&#27979;&#26410;&#35782;&#21035;&#21644;&#21487;&#30097;&#27963;&#21160;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;DPI&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#20174;&#36890;&#36807;&#32593;&#32476;&#20256;&#36755;&#30340;&#25968;&#25454;&#21253;&#20869;&#23481;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#22522;&#20110;Transformer&#30340;DPI&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24102;&#26377;&#20998;&#31867;&#22120;&#22836;&#30340;&#36716;&#21464;&#22120;&#26816;&#27979;&#24694;&#24847;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18223v1 Announce Type: cross  Abstract: As malicious cyber threats become more sophisticated in breaching computer networks, the need for effective intrusion detection systems (IDSs) becomes crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced to allow IDSs analyze the content of network packets, providing more context for identifying potential threats. IDSs traditionally rely on using anomaly-based and signature-based detection techniques to detect unrecognized and suspicious activity. Deep learning techniques have shown great potential in DPI for IDSs due to their efficiency in learning intricate patterns from the packet content being transmitted through the network. In this paper, we propose a revolutionary DPI algorithm based on transformers adapted for the purpose of detecting malicious traffic with a classifier head. Transformers learn the complex content of sequence data and generalize them well to similar scenarios thanks to their self-attent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20108;&#32500;&#21644;&#19977;&#32500;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#39044;&#21046;&#24211;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#35745;&#31639;&#25968;&#23398;&#24320;&#21457;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18219</link><description>&lt;p&gt;
&#20174;&#20108;&#32500;&#21040;&#19977;&#32500;&#29615;&#22659;&#30340;Q&#23398;&#20064;&#65306;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#24314;&#27169;&#33258;&#20027;&#23548;&#33322;&#32780;&#26080;&#38656;&#24211;
&lt;/p&gt;
&lt;p&gt;
From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20108;&#32500;&#21644;&#19977;&#32500;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#39044;&#21046;&#24211;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#35745;&#31639;&#25968;&#23398;&#24320;&#21457;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#20351;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#21453;&#39304;&#26426;&#21046;&#30340;&#20132;&#20114;&#33719;&#24471;&#26368;&#20248;&#20915;&#31574;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;RL&#20195;&#29702;&#22312;&#20108;&#32500;&#65288;2D&#65289;&#21644;&#19977;&#32500;&#65288;3D&#65289;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#30740;&#31350;&#36328;&#19981;&#21516;&#31354;&#38388;&#32500;&#24230;&#23398;&#20064;&#30340;&#21160;&#24577;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#27809;&#26377;&#29992;&#20110;&#23398;&#20064;&#30340;&#39044;&#21046;&#24211;&#65292;&#31639;&#27861;&#23436;&#20840;&#36890;&#36807;&#35745;&#31639;&#25968;&#23398;&#24320;&#21457;&#12290;&#26041;&#27861;&#35770;&#26694;&#26550;&#38598;&#20013;&#22312;RL&#21407;&#21017;&#19978;&#65292;&#37319;&#29992;Q&#23398;&#20064;&#20195;&#29702;&#31867;&#21644;&#38024;&#23545;&#27599;&#20010;&#31354;&#38388;&#32500;&#24230;&#37327;&#36523;&#23450;&#21046;&#30340;&#19981;&#21516;&#29615;&#22659;&#31867;&#12290;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19968;&#20010;&#38382;&#39064;&#65306;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#19981;&#21516;&#31354;&#38388;&#32500;&#24230;&#30340;&#29615;&#22659;&#20013;&#22914;&#20309;&#36866;&#24212;&#21644;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;2D&#21644;3D&#35774;&#32622;&#20013;&#65311;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18219v1 Announce Type: cross  Abstract: Reinforcement learning (RL) algorithms have become indispensable tools in artificial intelligence, empowering agents to acquire optimal decision-making policies through interactions with their environment and feedback mechanisms. This study explores the performance of RL agents in both two-dimensional (2D) and three-dimensional (3D) environments, aiming to research the dynamics of learning across different spatial dimensions. A key aspect of this investigation is the absence of pre-made libraries for learning, with the algorithm developed exclusively through computational mathematics. The methodological framework centers on RL principles, employing a Q-learning agent class and distinct environment classes tailored to each spatial dimension. The research aims to address the question: How do reinforcement learning agents adapt and perform in environments of varying spatial dimensions, particularly in 2D and 3D settings? Through empirical
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#25919;&#27835;&#31185;&#23398;&#20013;&#27169;&#31946;&#23383;&#31526;&#20018;&#21305;&#37197;&#38382;&#39064;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#24179;&#22343;&#31934;&#24230;&#36798;39%&#65292;&#26356;&#26131;&#20351;&#29992;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2403.18218</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25919;&#27835;&#31185;&#23398;&#20013;&#27169;&#31946;&#23383;&#31526;&#20018;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Fuzzy String Matching in Political Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18218
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#25919;&#27835;&#31185;&#23398;&#20013;&#27169;&#31946;&#23383;&#31526;&#20018;&#21305;&#37197;&#38382;&#39064;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#24179;&#22343;&#31934;&#24230;&#36798;39%&#65292;&#26356;&#26131;&#20351;&#29992;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#23383;&#31526;&#20018;&#21305;&#37197;&#22312;&#25919;&#27835;&#31185;&#23398;&#23478;&#20174;&#19981;&#21516;&#26469;&#28304;&#32452;&#21512;&#25968;&#25454;&#26102;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#21305;&#37197;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23383;&#31526;&#20018;&#36317;&#31163;&#65292;&#22914;Levenshtein&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26080;&#27861;&#21305;&#37197;&#37027;&#20123;&#29992;&#19981;&#21516;&#21517;&#31216;&#25351;&#20195;&#21516;&#19968;&#23454;&#20307;&#30340;&#23383;&#31526;&#20018;&#65292;&#27604;&#22914;&#8220;JP Morgan&#8221;&#21644;&#8220;Chase Bank&#8221;&#65292;&#8220;DPRK&#8221;&#21644;&#8220;&#21271;&#38889;&#8221;&#65292;&#8220;Chuck Fleischmann&#65288;R&#65289;&#8221;&#21644;&#8220;Charles Fleischmann&#65288;R&#65289;&#8221;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#35268;&#36991;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#19968;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#26041;&#24335;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#24179;&#22343;&#31934;&#24230;&#26041;&#38754;&#23558;&#19994;&#30028;&#27700;&#24179;&#25552;&#39640;&#22810;&#36798;39%&#65292;&#21516;&#26102;&#25919;&#27835;&#31185;&#23398;&#23478;&#20351;&#29992;&#36215;&#26469;&#26356;&#21152;&#31616;&#21333;&#30452;&#35266;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#21508;&#31181;&#28201;&#24230;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27880;&#24847;&#21040;&#65292;&#26356;&#22909;&#30340;&#25552;&#31034;&#21487;&#20197;&#24102;&#26469;&#39069;&#22806;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18218v1 Announce Type: new  Abstract: Fuzzy string matching remains a key issue when political scientists combine data from different sources. Existing matching methods invariably rely on string distances, such as Levenshtein distance and cosine similarity. As such, they are inherently incapable of matching strings that refer to the same entity with different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and ''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In this letter, we propose to use large language models to entirely sidestep this problem in an easy and intuitive manner. Extensive experiments show that our proposed methods can improve the state of the art by as much as 39% in terms of average precision while being substantially easier and more intuitive to use by political scientists. Moreover, our results are robust against various temperatures. We further note that enhanced prompting can lead to additional performance improvement
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20559;&#24207;&#26102;&#24207;&#30446;&#26631;&#65292;&#23558;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#26144;&#23556;&#21040;MDP&#31574;&#30053;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24207;&#29702;&#35770;&#23454;&#29616;&#26368;&#20248;&#31574;&#30053;&#30340;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.18212</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#22522;&#20110;&#20559;&#24207;&#26102;&#24207;&#30446;&#26631;&#30340;&#39318;&#36873;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18212
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20559;&#24207;&#26102;&#24207;&#30446;&#26631;&#65292;&#23558;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#26144;&#23556;&#21040;MDP&#31574;&#30053;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24207;&#29702;&#35770;&#23454;&#29616;&#26368;&#20248;&#31574;&#30053;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#24182;&#38750;&#24635;&#26159;&#36890;&#36807;&#23436;&#20840;&#30340;&#32447;&#24615;&#39034;&#24207;&#26469;&#34920;&#31034;&#65306;&#20351;&#29992;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#26469;&#34920;&#36798;&#19981;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#26159;&#33258;&#28982;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#38543;&#26426;&#31995;&#32479;&#20013;&#20570;&#20915;&#31574;&#21644;&#27010;&#29575;&#35268;&#21010;&#65292;&#36825;&#20123;&#31995;&#32479;&#34987;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#65292;&#32473;&#23450;&#19968;&#32452;&#26377;&#24207;&#20559;&#22909;&#30340;&#26102;&#38388;&#24310;&#20280;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#26102;&#38388;&#24310;&#20280;&#30446;&#26631;&#37117;&#26159;&#20351;&#29992;&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#26377;&#38480;&#36712;&#36857;&#65288;LTL$_f$&#65289;&#20013;&#30340;&#20844;&#24335;&#26469;&#34920;&#31034;&#30340;&#12290;&#20026;&#20102;&#26681;&#25454;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24207;&#29702;&#35770;&#26469;&#23558;&#23545;&#26102;&#38388;&#30446;&#26631;&#30340;&#20559;&#22909;&#26144;&#23556;&#21040;&#23545;MDP&#31574;&#30053;&#30340;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#38543;&#26426;&#39034;&#24207;&#19979;&#30340;&#19968;&#20010;&#26368;&#20248;&#36873;&#31574;&#30053;&#23558;&#23548;&#33268;MDP&#20013;&#26377;&#38480;&#36335;&#24452;&#19978;&#30340;&#19968;&#20010;&#38543;&#26426;&#38750;&#25903;&#37197;&#27010;&#29575;&#20998;&#24067;&#12290;&#20026;&#20102;&#21512;&#25104;&#19968;&#20010;&#26368;&#20248;&#36873;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31243;&#24207;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18212v1 Announce Type: cross  Abstract: Human preferences are not always represented via complete linear orders: It is natural to employ partially-ordered preferences for expressing incomparable outcomes. In this work, we consider decision-making and probabilistic planning in stochastic systems modeled as Markov decision processes (MDPs), given a partially ordered preference over a set of temporally extended goals. Specifically, each temporally extended goal is expressed using a formula in Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially ordered preference, we introduce order theory to map a preference over temporal goals to a preference over policies for the MDP. Accordingly, a most preferred policy under a stochastic ordering induces a stochastic nondominated probability distribution over the finite paths in the MDP. To synthesize a most preferred policy, our technical approach includes two key steps. In the first step, we develop a procedure to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#36710;&#36742;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18209</link><description>&lt;p&gt;
&#38271;&#30701;&#26399;&#32422;&#26463;&#39537;&#21160;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#36710;&#36742;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#26080;&#27861;&#20445;&#35777;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#65288;LSTC&#65289;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;RL&#12290;&#30701;&#26399;&#32422;&#26463;&#26088;&#22312;&#30830;&#20445;&#36710;&#36742;&#25506;&#27979;&#21040;&#30340;&#30701;&#26399;&#29366;&#24577;&#23433;&#20840;&#65292;&#32780;&#38271;&#26399;&#32422;&#26463;&#21017;&#30830;&#20445;&#25972;&#20307;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18209v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has been widely used in decision-making tasks, but it cannot guarantee the agent's safety in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving. Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but they still permit unsafe state occurrence, which is unacceptable in autonomous driving tasks. Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms. In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL. The short-term constraint aims to guarantee the short-term state safety that the vehicle explores, while the long-term constraint ensures the overall safety of the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#21270;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#20855;&#26377;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#24182;&#32771;&#34385;&#20102;&#26469;&#33258;&#19981;&#21516;&#36755;&#20837;&#27969;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.18208</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#28436;&#21270;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#29992;&#20110;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
An Evolutionary Network Architecture Search Framework with Adaptive Multimodal Fusion for Hand Gesture Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18208
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#21270;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#20855;&#26377;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#24182;&#32771;&#34385;&#20102;&#26469;&#33258;&#19981;&#21516;&#36755;&#20837;&#27969;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#25163;&#21183;&#35782;&#21035;(HGR)&#22240;&#20854;&#22312;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21508;&#31181;&#25163;&#21160;&#35774;&#35745;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#32593;&#32476;&#22312;&#22810;&#27169;&#24577;HGR&#65288;MHGR&#65289;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#32463;&#39564;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#35797;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#28436;&#21270;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;(AMF-ENAS)&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32534;&#30721;&#31354;&#38388;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#34701;&#21512;&#20301;&#32622;&#21644;&#27604;&#20363;&#65292;&#20801;&#35768;&#36890;&#36807;&#35299;&#30721;&#33258;&#21160;&#26500;&#24314;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#24212;&#20110;&#21333;&#27169;&#24577;&#34920;&#38754;&#32908;&#30005;&#22270;(sEMG)&#12289;&#21333;&#27169;&#24577;&#21152;&#36895;&#24230;&#35745;(ACC)&#21644;&#36328;&#27169;&#24577;(sEMG-ACC)&#30340;&#19977;&#20010;&#36755;&#20837;&#27969;&#12290;&#20026;&#20102;&#33258;&#21160;&#36866;&#24212;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;ENAS&#26694;&#26550;&#34987;&#35774;&#35745;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18208v1 Announce Type: cross  Abstract: Hand gesture recognition (HGR) based on multimodal data has attracted considerable attention owing to its great potential in applications. Various manually designed multimodal deep networks have performed well in multimodal HGR (MHGR), but most of existing algorithms require a lot of expert experience and time-consuming manual trials. To address these issues, we propose an evolutionary network architecture search framework with the adaptive multimodel fusion (AMF-ENAS). Specifically, we design an encoding space that simultaneously considers fusion positions and ratios of the multimodal data, allowing for the automatic construction of multimodal networks with different architectures through decoding. Additionally, we consider three input streams corresponding to intra-modal surface electromyography (sEMG), intra-modal accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to various datasets, the ENAS framework is designe
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#23618;&#28176;&#36827;&#24335;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#65292;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31169;&#20154;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#20854;&#38450;&#33539;&#38544;&#31169;&#20405;&#29359;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.18205</link><description>&lt;p&gt;
&#25506;&#31350;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Privacy Protection Capabilities of Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18205
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#23618;&#28176;&#36827;&#24335;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#65292;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31169;&#20154;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#20854;&#38450;&#33539;&#38544;&#31169;&#20405;&#29359;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20986;&#33394;&#34920;&#29616;&#32780;&#38395;&#21517;&#65292;&#22312;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#27493;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#24433;&#21709;&#26085;&#30410;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#39118;&#38505;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#20026;&#19977;&#23618;&#30340;&#28176;&#36827;&#24335;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#12290;&#35813;&#26694;&#26550;&#22312;&#27599;&#20010;&#23618;&#27425;&#37117;&#21253;&#21547;&#36880;&#28176;&#22797;&#26434;&#21644;&#28145;&#20837;&#30340;&#38544;&#31169;&#27979;&#35797;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31169;&#20154;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#65292;&#32771;&#23519;&#23427;&#20204;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#36776;&#21035;&#12289;&#31649;&#29702;&#21644;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#12290;&#36825;&#31181;&#31995;&#32479;&#21270;&#35780;&#20272;&#26377;&#21161;&#20110;&#25105;&#20204;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#36981;&#23432;&#38544;&#31169;&#20445;&#25252;&#20934;&#21017;&#30340;&#31243;&#24230;&#65292;&#20197;&#21450;&#20854;&#22266;&#26377;&#30340;&#38450;&#33539;&#38544;&#31169;&#20405;&#29359;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18205v1 Announce Type: new  Abstract: Large language models (LLMs), renowned for their impressive capabilities in various tasks, have significantly advanced artificial intelligence. Yet, these advancements have raised growing concerns about privacy and security implications. To address these issues and explain the risks inherent in these models, we have devised a three-tiered progressive framework tailored for evaluating privacy in language systems. This framework consists of progressively complex and in-depth privacy test tasks at each tier. Our primary objective is to comprehensively evaluate the sensitivity of large language models to private information, examining how effectively they discern, manage, and safeguard sensitive data in diverse scenarios. This systematic evaluation helps us understand the degree to which these models comply with privacy protection guidelines and the effectiveness of their inherent safeguards against privacy breaches. Our observations indicat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#29983;&#21629;&#31185;&#23398;&#30340;&#24320;&#28304;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#20026;&#29983;&#29289;&#20449;&#24687;&#23398;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#38656;&#32534;&#31243;&#25216;&#33021;&#21363;&#21487;&#20998;&#26512;&#22797;&#26434;&#29983;&#29289;&#25968;&#25454;&#30340;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.18203</link><description>&lt;p&gt;
EndToEndML: &#19968;&#20010;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#24320;&#28304;&#31471;&#21040;&#31471;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18203
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#29983;&#21629;&#31185;&#23398;&#30340;&#24320;&#28304;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#20026;&#29983;&#29289;&#20449;&#24687;&#23398;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#38656;&#32534;&#31243;&#25216;&#33021;&#21363;&#21487;&#20998;&#26512;&#22797;&#26434;&#29983;&#29289;&#25968;&#25454;&#30340;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18203v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20154;&#24037;&#26234;&#33021; (AI) &#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#12290;&#28982;&#32780;&#65292;&#23558;&#21019;&#26032;&#30340; AI &#25216;&#26415;&#24212;&#29992;&#20110;&#29702;&#35299;&#21644;&#25581;&#31034;&#29983;&#29289;&#22797;&#26434;&#24615;&#21463;&#21040;&#29983;&#21629;&#31185;&#23398;&#31185;&#23398;&#23478;&#29702;&#35299;&#21644;&#20351;&#29992;&#35745;&#31639;&#35821;&#35328;&#30340;&#23398;&#20064;&#26354;&#32447;&#30340;&#38459;&#30861;&#12290;&#19968;&#20010;&#38754;&#21521; AI &#27169;&#22411;&#30340;&#24320;&#28304;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#19981;&#38656;&#35201;&#32534;&#31243;&#25216;&#33021;&#26469;&#20998;&#26512;&#22797;&#26434;&#30340;&#29983;&#29289;&#25968;&#25454;&#65292;&#23545;&#29983;&#29289;&#20449;&#24687;&#23398;&#31038;&#21306;&#23558;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#38543;&#30528;&#23545;&#19981;&#21516;&#27979;&#24207;&#25216;&#26415;&#30340;&#26131;&#20110;&#23384;&#21462;&#20197;&#21450;&#23545;&#19981;&#21516; '&#32452;&#23398;' &#30740;&#31350;&#30340;&#22686;&#21152;&#20852;&#36259;&#65292;&#29983;&#25104;&#30340;&#29983;&#29289;&#25968;&#25454;&#38598;&#25968;&#37327;&#22686;&#21152;&#65292;&#24182;&#19988;&#20998;&#26512;&#36825;&#20123;&#39640;&#36890;&#37327;&#25968;&#25454;&#38598;&#35201;&#27714;&#35745;&#31639;&#33021;&#21147;&#12290;&#20170;&#22825;&#30340;&#22823;&#22810;&#25968; AI &#24211;&#37117;&#38656;&#35201;&#39640;&#32423;&#32534;&#31243;&#25216;&#33021;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21487;&#35270;&#21270;&#25216;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; web &#30340;&#31471;&#21040;&#31471;&#31649;&#36947;&#65292;&#33021;&#22815;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18203v1 Announce Type: new  Abstract: Artificial intelligence (AI) techniques are widely applied in the life sciences. However, applying innovative AI techniques to understand and deconvolute biological complexity is hindered by the learning curve for life science scientists to understand and use computing languages. An open-source, user-friendly interface for AI models, that does not require programming skills to analyze complex biological data will be extremely valuable to the bioinformatics community. With easy access to different sequencing technologies and increased interest in different 'omics' studies, the number of biological datasets being generated has increased and analyzing these high-throughput datasets is computationally demanding. The majority of AI libraries today require advanced programming skills as well as machine learning, data preprocessing, and visualization skills. In this research, we propose a web-based end-to-end pipeline that is capable of preproc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#36328;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#20934;&#30830;&#35786;&#26029;&#32467;&#26524;&#21644;&#20844;&#24179;&#24615;&#65292;&#20026;&#39640;&#32500;&#33016;&#37096;X&#23556;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#24102;&#26469;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2403.18196</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#25152;&#35265;&#65306;&#22522;&#20110;&#31181;&#26063;&#20581;&#24247;&#19981;&#24179;&#31561;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#30340;&#22810;&#26631;&#31614;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#30340;&#23376;&#32452;&#20132;&#21449;&#20844;&#24179;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18196
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#36328;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#20934;&#30830;&#35786;&#26029;&#32467;&#26524;&#21644;&#20844;&#24179;&#24615;&#65292;&#20026;&#39640;&#32500;&#33016;&#37096;X&#23556;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#24102;&#26469;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#33016;&#37096;X&#23556;&#32447;&#36827;&#34892;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#65292;&#23454;&#26045;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#36328;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#35786;&#26029;&#32467;&#26524;&#65292;&#24182;&#30830;&#20445;&#22312;&#39640;&#32500;&#33016;&#37096;X&#23556;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#36328;&#20132;&#21449;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#19981;&#25304;&#19968;&#26684;&#22320;&#32771;&#34385;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#26356;&#31934;&#32454;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#20351;&#29992;&#36328;&#32452;&#24179;&#34913;&#25968;&#25454;&#38598;&#37325;&#26032;&#35757;&#32451;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#21518;&#20998;&#31867;&#23618;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#24182;&#20026;&#22810;&#26631;&#31614;&#35774;&#32622;&#25972;&#21512;&#20102;&#31867;&#24179;&#34913;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;MIMIC-CXR&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18196v1 Announce Type: cross  Abstract: There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays. Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups. In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure fairness across intersectional groups in high-dimensional chest X- ray multi-label classification. Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular benchmark and evaluation of fairness. We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups. Additionally, we account for fairness constraints and integrate class-balanced fine-tuning for multi-label settings. The evaluation of our method on the MIMIC-CXR dataset demonstrates that
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#21644;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.18195</link><description>&lt;p&gt;
&#29992;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#32416;&#27491;LEGO&#32452;&#35013;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18195
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#21644;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#21644;3D&#35270;&#35273;&#20013;&#65292;&#33258;&#20027;&#32452;&#35013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#30830;&#20445;&#32452;&#35013;&#27491;&#30830;&#24615;&#12290;&#20027;&#27969;&#26041;&#27861;&#22914;MEPNet&#30446;&#21069;&#19987;&#27880;&#20110;&#22522;&#20110;&#25163;&#21160;&#25552;&#20379;&#30340;&#22270;&#20687;&#36827;&#34892;&#32452;&#20214;&#32452;&#35013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#24448;&#24448;&#38590;&#20197;&#21462;&#24471;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#21516;&#19968;&#26102;&#38388;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25972;&#21512;&#33258;&#26657;&#27491;&#27169;&#22359;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#21463;&#27492;&#38382;&#39064;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#35782;&#21035;&#21644;&#32416;&#27491;&#32452;&#20214;&#32452;&#35013;&#38169;&#35823;&#12290;&#20026;&#25903;&#25345;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#21253;&#25324;&#29992;&#20110;&#32452;&#35013;&#27493;&#39588;&#21644;&#32452;&#35013;&#22833;&#36133;&#23454;&#20363;&#30340;&#25163;&#21160;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;SCANet&#23558;&#32452;&#35013;&#30340;&#37096;&#20214;&#35270;&#20026;&#26597;&#35810;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18195v1 Announce Type: cross  Abstract: Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the single-step assembly error correction task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#27169;&#22411;&#22312;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#20013;&#23545;&#20110;&#24067;&#23616;&#21644;&#22270;&#20687;&#25968;&#25454;&#26377;&#30410;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;AI&#26159;&#21542;&#33021;&#26377;&#25928;&#25429;&#25417;&#25991;&#20214;&#32654;&#23398;&#30340;&#24494;&#22937;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.18183</link><description>&lt;p&gt;
AI&#27169;&#22411;&#33021;&#21542;&#27427;&#36175;&#25991;&#26723;&#32654;&#23398;&#65311;&#25506;&#31350;&#21487;&#35835;&#24615;&#19982;&#29256;&#38754;&#36136;&#37327;&#19982;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Can AI Models Appreciate Document Aesthetics? An Exploration of Legibility and Layout Quality in Relation to Prediction Confidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#27169;&#22411;&#22312;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#20013;&#23545;&#20110;&#24067;&#23616;&#21644;&#22270;&#20687;&#25968;&#25454;&#26377;&#30410;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;AI&#26159;&#21542;&#33021;&#26377;&#25928;&#25429;&#25417;&#25991;&#20214;&#32654;&#23398;&#30340;&#24494;&#22937;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20221;&#31934;&#24515;&#35774;&#35745;&#30340;&#25991;&#20214;&#19981;&#20165;&#36890;&#36807;&#25991;&#23383;&#20256;&#36798;&#20449;&#24687;&#65292;&#36824;&#36890;&#36807;&#35270;&#35273;&#20248;&#38597;&#20256;&#36798;&#20449;&#24687;&#12290;&#20316;&#32773;&#21033;&#29992;&#39068;&#33394;&#12289;&#23383;&#20307;&#12289;&#22270;&#24418;&#21644;&#24067;&#23616;&#31561;&#32654;&#23398;&#20803;&#32032;&#26469;&#22609;&#36896;&#20449;&#24687;&#30340;&#24863;&#30693;&#12290;&#32463;&#36807;&#24515;&#29702;&#27934;&#23519;&#21147;&#21551;&#21457;&#30340;&#21608;&#21040;&#25991;&#20214;&#35774;&#35745;&#26082;&#22686;&#24378;&#20102;&#35270;&#35273;&#21560;&#24341;&#21147;&#65292;&#20063;&#22686;&#36827;&#20102;&#20869;&#23481;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#25991;&#20214;AI&#27169;&#22411;&#23637;&#31034;&#20102;&#23558;&#24067;&#23616;&#21644;&#22270;&#20687;&#25968;&#25454;&#34701;&#20837;&#30340;&#22909;&#22788;&#65292;&#20294;&#25991;&#20214;&#32654;&#23398;&#30340;&#24494;&#22937;&#20043;&#22788;&#26159;&#21542;&#34987;&#26377;&#25928;&#25429;&#25417;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#24357;&#21512;&#20154;&#31867;&#35748;&#30693;&#19982;AI&#23545;&#32654;&#23398;&#20803;&#32032;&#35299;&#37322;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20551;&#35774;&#65292;&#28041;&#21450;AI&#22312;&#25991;&#20214;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#26893;&#26681;&#20110;&#25991;&#20214;&#35774;&#35745;&#21407;&#21017;&#12290;&#22312;&#20851;&#27880;&#21487;&#35835;&#24615;&#21644;&#29256;&#38754;&#36136;&#37327;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#32654;&#23398;&#25928;&#26524;&#30340;&#22235;&#20010;&#26041;&#38754;&#65306;&#22122;&#38899;&#12289;&#23383;&#20307;&#22823;&#23567;&#23545;&#27604;&#12289;&#23545;&#40784;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#27169;&#22411;&#32622;&#20449;&#24230;&#20026;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18183v1 Announce Type: new  Abstract: A well-designed document communicates not only through its words but also through its visual eloquence. Authors utilize aesthetic elements such as colors, fonts, graphics, and layouts to shape the perception of information. Thoughtful document design, informed by psychological insights, enhances both the visual appeal and the comprehension of the content. While state-of-the-art document AI models demonstrate the benefits of incorporating layout and image data, it remains unclear whether the nuances of document aesthetics are effectively captured. To bridge the gap between human cognition and AI interpretation of aesthetic elements, we formulated hypotheses concerning AI behavior in document understanding tasks, specifically anchored in document design principles. With a focus on legibility and layout quality, we tested four aspects of aesthetic effects: noise, font-size contrast, alignment, and complexity, on model confidence using corre
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#20004;&#20010;&#36890;&#29992;&#26426;&#21046;&#65306;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#21644;&#26410;&#33021;&#27491;&#30830;&#36873;&#25321;&#23545;&#35937;&#23646;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.18167</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Mechanisms of non-factual hallucinations in language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18167
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#20004;&#20010;&#36890;&#29992;&#26426;&#21046;&#65306;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#21644;&#26410;&#33021;&#27491;&#30830;&#36873;&#25321;&#23545;&#35937;&#23646;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26377;&#26102;&#20250;&#20135;&#29983;&#19982;&#19990;&#30028;&#30693;&#35782;&#19981;&#31526;&#30340;&#38750;&#20107;&#23454;&#24187;&#35273;&#12290;&#23613;&#31649;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#24187;&#35273;&#65292;&#20294;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290; &#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#24187;&#35273;&#30340;&#26426;&#21046;&#21407;&#22240;&#65292;&#29305;&#21035;&#26159; LM &#22312;&#23545;&#20027;&#39064;&#20851;&#31995;&#26597;&#35810;&#20570;&#20986;&#22238;&#31572;&#26102;&#38169;&#35823;&#22320;&#39044;&#27979;&#23545;&#35937;&#23646;&#24615;&#30340;&#38750;&#20107;&#23454;&#24418;&#24335;&#12290;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21644;&#23884;&#20837;&#31354;&#38388;&#25237;&#24433;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#36328;&#19981;&#21516;&#35268;&#27169;&#21644;&#35774;&#35745;&#30340; LM &#20013;&#20849;&#20139;&#30340;&#20004;&#20010;&#36896;&#25104;&#24187;&#35273;&#30340;&#19968;&#33324;&#26426;&#21046;&#21407;&#22240;&#65306;1&#65289;&#22312;&#36739;&#20302;&#23618; MLPs &#20013;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#65292;&#20197;&#21450;2&#65289;&#22312;&#36739;&#39640;&#23618;&#27880;&#24847;&#21147;&#22836;&#21644; MLPs &#20013;&#26410;&#33021;&#36873;&#25321;&#27491;&#30830;&#30340;&#23545;&#35937;&#23646;&#24615;&#12290;&#36825;&#20004;&#20010;&#26426;&#21046;&#23637;&#31034;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#20027;&#23486;&#20851;&#31995;&#12289;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#25200;&#21160;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102; LM &#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18167v1 Announce Type: cross  Abstract: State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.18159</link><description>&lt;p&gt;
&#22114;&#65281;&#25105;&#20204;&#20919;&#20923;&#65306;&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18159
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20998;&#21035;&#22312;NLP&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#39640;&#65292;&#36825;&#20351;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#23427;&#20204;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#37327;&#21270;&#24863;&#30693;&#24494;&#35843;&#25216;&#26415;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD-QAT&#65289;&#26469;&#25913;&#21892;&#20351;&#29992;&#24120;&#29992;&#25968;&#25454;&#38598;&#25913;&#36827;4&#20301;&#37325;&#37327;&#37327;&#21270;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#23454;&#29616;&#27969;&#34892;&#30340;&#35821;&#35328;&#20351;&#29992;&#26696;&#20363;&#65292;&#22312;&#35774;&#22791;&#32842;&#22825;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24494;&#35843;&#33539;&#24335;&#65292;&#20316;&#20026;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#20256;&#25773;&#65292;&#25552;&#20379;&#23545;KD-QAT&#31283;&#23450;&#24615;&#30340;&#27934;&#23519;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;KD-QAT&#30340;&#26041;&#27861;&#23545;&#20302;&#20301;&#37327;&#21270;&#35823;&#24046;&#30340;&#33030;&#24369;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ov-freeze&#65292;&#19968;&#31181;&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18159v1 Announce Type: cross  Abstract: Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we expe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20004;&#39033;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#22797;&#22312;&#20849;&#24773;&#24615;&#26041;&#38754;&#34987;&#35748;&#20026;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#22238;&#22797;&#26356;&#20855;&#26377;&#20849;&#24773;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#22312;&#20154;&#38469;&#25903;&#25345;&#26041;&#38754;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18148</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#34987;&#35748;&#20026;&#20855;&#26377;&#20849;&#24773;&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Produce Responses Perceived to be Empathic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18148
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20004;&#39033;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#22797;&#22312;&#20849;&#24773;&#24615;&#26041;&#38754;&#34987;&#35748;&#20026;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#22238;&#22797;&#26356;&#20855;&#26377;&#20849;&#24773;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#22312;&#20154;&#38469;&#25903;&#25345;&#26041;&#38754;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25776;&#20889;&#26174;&#31034;&#20849;&#24773;&#30340;&#25903;&#25345;&#24615;&#28040;&#24687;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35753;&#36825;&#20123;&#27169;&#22411;&#26681;&#25454;&#25551;&#36848;&#24120;&#35265;&#29983;&#27963;&#32463;&#21382;&#30340;&#24086;&#23376;&#29983;&#25104;&#20849;&#24773;&#28040;&#24687;&#65292;&#22914;&#24037;&#20316;&#22330;&#26223;&#12289;&#32946;&#20799;&#12289;&#20154;&#38469;&#20851;&#31995;&#20197;&#21450;&#20854;&#20182;&#24341;&#21457;&#28966;&#34385;&#21644;&#24868;&#24594;&#30340;&#24773;&#20917;&#12290;&#22312;&#20004;&#39033;&#30740;&#31350;&#20013;&#65288;N=192&#65292;202&#65289;&#65292;&#25105;&#20204;&#21521;&#20154;&#31867;&#35780;&#20998;&#21592;&#23637;&#31034;&#20102;&#30001;&#20960;&#31181;&#27169;&#22411;&#65288;GPT4 Turbo&#65292;Llama2 &#21644; Mistral&#65289;&#25776;&#20889;&#30340;&#21508;&#31181;&#22238;&#22797;&#65292;&#24182;&#35753;&#20154;&#20204;&#26681;&#25454;&#36825;&#20123;&#22238;&#22797;&#22312;&#20849;&#24773;&#31243;&#24230;&#19978;&#35780;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLM &#29983;&#25104;&#30340;&#22238;&#22797;&#34987;&#19968;&#33268;&#35780;&#20026;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#22238;&#22797;&#26356;&#20855;&#20849;&#24773;&#24615;&#12290;&#35821;&#35328;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20351;&#29992;&#26631;&#28857;&#31526;&#21495;&#12289;&#34920;&#24773;&#31526;&#21495;&#21644;&#26576;&#20123;&#35789;&#27719;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#12289;&#21487;&#39044;&#27979;&#30340;&#8220;&#39118;&#26684;&#8221;&#12290;&#36825;&#20123;&#32467;&#26524;&#20984;&#26174;&#20102;&#22312;&#38656;&#35201;&#20849;&#24773;&#30340;&#24773;&#22659;&#20013;&#21033;&#29992;LLMs&#25552;&#21319;&#20154;&#31867;&#21516;&#34892;&#25903;&#25345;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18148v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy. Here, we had these models generate empathic messages in response to posts describing common life experiences, such as workplace situations, parenting, relationships, and other anxiety- and anger-eliciting situations. Across two studies (N=192, 202), we showed human raters a variety of responses written by several models (GPT4 Turbo, Llama2, and Mistral), and had people rate these responses on how empathic they seemed to be. We found that LLM-generated responses were consistently rated as more empathic than human-written responses. Linguistic analyses also show that these models write in distinct, predictable ``styles", in terms of their use of punctuation, emojis, and certain words. These results highlight the potential of using LLMs to enhance human peer support in contexts where empathy is i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Switchable-Edge Search&#65288;SES&#65289;&#30340;A*&#39118;&#26684;&#31639;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#26234;&#33021;&#20307;&#36890;&#36807;&#39034;&#24207;&#30340;&#37325;&#26032;&#35843;&#24230;&#65292;&#26368;&#20339;&#21464;&#20307;&#22312;&#25928;&#29575;&#19978;&#27604;&#22522;&#32447;&#24555;4&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.18145</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#22810;&#26426;&#22120;&#20154;&#35745;&#21010;&#25191;&#34892;&#30340;&#23454;&#26102;&#37325;&#35843;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Switchable-Edge Search&#65288;SES&#65289;&#30340;A*&#39118;&#26684;&#31639;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#26234;&#33021;&#20307;&#36890;&#36807;&#39034;&#24207;&#30340;&#37325;&#26032;&#35843;&#24230;&#65292;&#26368;&#20339;&#21464;&#20307;&#22312;&#25928;&#29575;&#19978;&#27604;&#22522;&#32447;&#24555;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#26159;&#30830;&#23450;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#26234;&#33021;&#20307;&#24310;&#36831;&#26102;&#22914;&#20309;&#26377;&#25928;&#23454;&#29616;&#37325;&#26032;&#35268;&#21010;&#12290;&#19968;&#31181;&#36873;&#25321;&#26159;&#37325;&#26032;&#23433;&#25490;&#26234;&#33021;&#20307;&#30340;&#36890;&#36807;&#39034;&#24207;&#65292;&#21363;&#26234;&#33021;&#20307;&#35775;&#38382;&#30456;&#21516;&#20301;&#32622;&#30340;&#39034;&#24207;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Switchable-Edge Search&#65288;SES&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#25214;&#21040;&#26368;&#20248;&#36890;&#36807;&#39034;&#24207;&#30340;A*&#39118;&#26684;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SES&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#35780;&#20272;&#20102;&#20854;&#25928;&#29575;&#12290;SES&#30340;&#26368;&#20339;&#21464;&#20307;&#23545;&#20110;&#23567;&#22411;&#21644;&#20013;&#31561;&#35268;&#27169;&#38382;&#39064;&#38656;&#35201;&#19981;&#21040;1&#31186;&#38047;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#38382;&#39064;&#19978;&#27604;&#22522;&#32447;&#36816;&#34892;&#24555;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18145v1 Announce Type: new  Abstract: One area of research in multi-agent path finding is to determine how replanning can be efficiently achieved in the case of agents being delayed during execution. One option is to reschedule the passing order of agents, i.e., the sequence in which agents visit the same location. In response, we propose Switchable-Edge Search (SES), an A*-style algorithm designed to find optimal passing orders. We prove the optimality of SES and evaluate its efficiency via simulations. The best variant of SES takes less than 1 second for small- and medium-sized problems and runs up to 4 times faster than baselines for large-sized problems.
&lt;/p&gt;</description></item><item><title>Juru &#27169;&#22411;&#36890;&#36807;&#20174;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#25552;&#21462;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#65292;&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#21487;&#20197;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#36825;&#31181;&#19987;&#38376;&#21270;&#20250;&#23548;&#33268;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.18140</link><description>&lt;p&gt;
Juru: &#26469;&#33258;&#21487;&#38752;&#26469;&#28304;&#30340;&#24052;&#35199;&#27861;&#24459;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Juru: Legal Brazilian Large Language Model from Reputable Sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18140
&lt;/p&gt;
&lt;p&gt;
Juru &#27169;&#22411;&#36890;&#36807;&#20174;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#25552;&#21462;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#65292;&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#21487;&#20197;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#36825;&#31181;&#19987;&#38376;&#21270;&#20250;&#23548;&#33268;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#30456;&#20851;&#30740;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#39046;&#22495;&#19987;&#38376;&#21270;&#21644;&#20351;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20026;&#25506;&#32034;&#36825;&#20123;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#21487;&#38752;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#19987;&#38376;&#21270;&#20102;Sabi\'a-2 Small&#27169;&#22411;&#65292;&#24182;&#22312;&#27861;&#24459;&#21644;&#19968;&#33324;&#30693;&#35782;&#32771;&#35797;&#20013;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;Juru&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19987;&#38376;&#21270;&#26159;&#20197;&#22312;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#22686;&#21152;&#30340;&#31185;&#23398;&#35777;&#25454;&#65292;&#34920;&#26126;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36873;&#25321;&#21487;&#33021;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#33021;&#22815;&#20197;&#36739;&#20302;&#25104;&#26412;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18140v1 Announce Type: cross  Abstract: The high computational cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;GNN&#20013;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#25915;&#20987;&#36827;&#34892;&#20102;&#26041;&#27861;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.18136</link><description>&lt;p&gt;
&#20445;&#25252;GNN&#65306;&#22522;&#20110;&#35299;&#37322;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;GNN&#20013;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#25915;&#20987;&#36827;&#34892;&#20102;&#26041;&#27861;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs)&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#27969;&#34892;&#36215;&#26469;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#36947;&#24503;&#24212;&#29992;&#12290;&#26816;&#27979;&#36825;&#20123;&#25915;&#20987;&#23545;&#20110;&#20445;&#25345;GNN&#20998;&#31867;&#20219;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#25216;&#26415;&#24182;&#19981;&#22810;&#35265;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#22270;&#32423;&#35299;&#37322;&#33021;&#22815;&#25552;&#20379;&#19968;&#20123;&#26377;&#38480;&#30340;&#35265;&#35299;&#65292;&#20294;&#23427;&#20204;&#22312;&#26816;&#27979;&#21518;&#38376;&#35302;&#21457;&#22120;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#26159;&#19981;&#19968;&#33268;&#19988;&#19981;&#23436;&#25972;&#30340;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#21462;&#24182;&#36716;&#25442;GNN&#35299;&#37322;&#26426;&#21046;&#30340;&#27425;&#35201;&#36755;&#20986;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25915;&#20987;&#26469;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26816;&#26597;&#20854;&#23545;&#21508;&#31181;&#25915;&#20987;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#36739;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18136v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple benchmark datasets and examine its efficacy against various attack models. Our results show that our method can achieve high de
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#20851;&#32852;&#20851;&#31995;&#65292;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#36895;&#24230;&#24555;&#19978;&#30334;&#20493;</title><link>https://arxiv.org/abs/2403.18133</link><description>&lt;p&gt;
AE SemRL&#65306;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
AE SemRL: Learning Semantic Association Rules with Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18133
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#20851;&#32852;&#20851;&#31995;&#65292;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#36895;&#24230;&#24555;&#19978;&#30334;&#20493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#65288;ARM&#65289;&#26159;&#23398;&#20064;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#36923;&#36753;&#35268;&#21017;&#20851;&#32852;&#30340;&#20219;&#21153;&#12290;&#20174;&#39640;&#32500;&#25968;&#20540;&#25968;&#25454;&#20013;&#25366;&#25496;&#20851;&#32852;&#35268;&#21017;&#65292;&#20363;&#22914;&#20174;&#26234;&#33021;&#29615;&#22659;&#20013;&#22823;&#37327;&#20256;&#24863;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26159;&#19968;&#39033;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#25552;&#21462;&#20851;&#32852;&#35268;&#21017;&#65288;AE SemRL&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#19982;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#28304;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#23384;&#22312;&#26102;&#65292;&#35821;&#20041;&#21487;&#20197;&#20419;&#36827;&#23398;&#20064;&#21487;&#25512;&#24191;&#21644;&#21487;&#35299;&#37322;&#30340;&#20851;&#32852;&#35268;&#21017;&#12290;&#23613;&#31649;&#36890;&#36807;&#39069;&#22806;&#30340;&#35821;&#20041;&#29305;&#24449;&#20016;&#23500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;AE SemRL&#20351;&#24471;&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#23398;&#20064;&#20851;&#32852;&#35268;&#21017;&#21464;&#24471;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21487;&#20197;&#20174;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#25552;&#21462;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#65292;&#24182;&#19988;&#27492;&#26041;&#27861;&#36895;&#24230;&#24555;&#19978;&#30334;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18133v1 Announce Type: cross  Abstract: Association Rule Mining (ARM) is the task of learning associations among data features in the form of logical rules. Mining association rules from high-dimensional numerical data, for example, time series data from a large number of sensors in a smart environment, is a computationally intensive task. In this study, we propose an Autoencoder-based approach to learn and extract association rules from time series data (AE SemRL). Moreover, we argue that in the presence of semantic information related to time series data sources, semantics can facilitate learning generalizable and explainable association rules. Despite enriching time series data with additional semantic features, AE SemRL makes learning association rules from high-dimensional data feasible. Our experiments show that semantic association rules can be extracted from a latent representation created by an Autoencoder and this method has in the order of hundreds of times faster
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#25968;&#25454;&#27969;&#65292;&#25512;&#33616;&#26080;&#25968;&#25454;&#30340;&#31867;&#36882;&#22686;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26041;&#27861;&#32988;&#36807;&#31454;&#20105;&#22522;&#32447;</title><link>https://arxiv.org/abs/2403.18132</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#25968;&#25454;&#25512;&#33616;&#26080;&#25968;&#25454;&#30340;&#31867;&#36882;&#22686;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recommendation of data-free class-incremental learning algorithms by simulating future data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18132
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#25968;&#25454;&#27969;&#65292;&#25512;&#33616;&#26080;&#25968;&#25454;&#30340;&#31867;&#36882;&#22686;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26041;&#27861;&#32988;&#36807;&#31454;&#20105;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#36882;&#22686;&#23398;&#20064;&#22788;&#29702;&#30001;&#31867;&#21035;&#25209;&#27425;&#32452;&#25104;&#30340;&#39034;&#24207;&#25968;&#25454;&#27969;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;&#20174;&#26080;&#27861;&#23384;&#20648;&#36807;&#21435;&#31867;&#21035;&#30340;&#26679;&#26412;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#20026;&#29992;&#25143;&#23450;&#20041;&#30340;&#35774;&#32622;&#36873;&#25321;&#36866;&#24403;&#30340;&#31639;&#27861;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#65292;&#22240;&#20026;&#36825;&#20123;&#31639;&#27861;&#30340;&#30456;&#23545;&#24615;&#33021;&#21462;&#20915;&#20110;&#36882;&#22686;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#25311;&#26410;&#26469;&#25968;&#25454;&#27969;&#30340;&#31639;&#27861;&#25512;&#33616;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#31867;&#21035;&#38598;&#21512;&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20174;&#30456;&#21516;&#30340;&#35270;&#35273;&#22495;&#27169;&#25311;&#26410;&#26469;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#27969;&#19978;&#35780;&#20272;&#20102;&#26368;&#36817;&#30340;&#31639;&#27861;&#65292;&#24182;&#25512;&#33616;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#36882;&#22686;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#31639;&#27861;&#21644;&#20845;&#20010;&#36882;&#22686;&#35774;&#32622;&#65292;&#22312;&#19977;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#31454;&#20105;&#22522;&#32447;&#65292;&#24182;&#19988;&#24615;&#33021;&#25509;&#36817;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18132v1 Announce Type: cross  Abstract: Class-incremental learning deals with sequential data streams composed of batches of classes. Various algorithms have been proposed to address the challenging case where samples from past classes cannot be stored. However, selecting an appropriate algorithm for a user-defined setting is an open problem, as the relative performance of these algorithms depends on the incremental settings. To solve this problem, we introduce an algorithm recommendation method that simulates the future data stream. Given an initial set of classes, it leverages generative models to simulate future classes from the same visual domain. We evaluate recent algorithms on the simulated stream and recommend the one which performs best in the user-defined incremental setting. We illustrate the effectiveness of our method on three large datasets using six algorithms and six incremental settings. Our method outperforms competitive baselines, and performance is close 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#38750;&#27491;&#24335;&#30340;&#25968;&#23398;&#38472;&#36848;&#32763;&#35793;&#20026;&#24418;&#24335;&#30340;Isabelle&#20195;&#30721;&#24182;&#36827;&#34892;&#33258;&#21160;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#21160;&#25298;&#32477;&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#26041;&#38754;&#19982;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#19981;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.18120</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#65306;&#39564;&#35777;--&#29992;&#33258;&#21160;&#24418;&#24335;&#21270;&#20026;&#22522;&#30784;&#30340;LLM&#23450;&#37327;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18120
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#38750;&#27491;&#24335;&#30340;&#25968;&#23398;&#38472;&#36848;&#32763;&#35793;&#20026;&#24418;&#24335;&#30340;Isabelle&#20195;&#30721;&#24182;&#36827;&#34892;&#33258;&#21160;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#21160;&#25298;&#32477;&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#26041;&#38754;&#19982;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#19981;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;Google&#30340;Minerva&#21644;OpenAI&#30340;GPT&#31995;&#21015;&#65292;&#27491;&#22312;&#36234;&#26469;&#36234;&#33021;&#22815;&#35299;&#20915;&#25968;&#23398;&#23450;&#37327;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#27493;&#39588;&#21644;&#31572;&#26696;&#20013;&#20173;&#28982;&#23384;&#22312;&#27809;&#26377;&#29702;&#30001;&#30340;&#36923;&#36753;&#21644;&#35745;&#31639;&#38169;&#35823;&#12290;&#26412;&#25991;&#21033;&#29992;LLMs&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#21253;&#21547;&#36275;&#22815;&#22810;&#30340;&#24418;&#24335;&#21270;&#25968;&#23398;&#31034;&#20363;&#65288;&#20363;&#22914;&#22312;Isabelle&#20013;&#65292;&#19968;&#20010;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#29615;&#22659;&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#34987;&#25552;&#31034;&#23558;&#38750;&#27491;&#24335;&#30340;&#25968;&#23398;&#38472;&#36848;&#32763;&#35793;&#21363;&#33258;&#21160;&#24418;&#24335;&#21270;&#20026;&#24418;&#24335;&#30340;Isabelle&#20195;&#30721;--&#35813;&#20195;&#30721;&#21487;&#20197;&#34987;&#33258;&#21160;&#39564;&#35777;&#20869;&#37096;&#19968;&#33268;&#24615;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#21160;&#25298;&#32477;&#37027;&#20123;&#20854;&#24418;&#24335;&#21270;&#29256;&#26412;&#22312;&#20854;&#20869;&#37096;&#25110;&#19982;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#19981;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;GSM8K&#12289;MATH&#21644;MultiArith&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#30452;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18120v1 Announce Type: new  Abstract: Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than 
&lt;/p&gt;</description></item><item><title>QuakeSet&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20302;&#36164;&#28304;&#27169;&#22411;&#65292;&#21033;&#29992;Sentinel-1&#26469;&#30417;&#27979;&#22320;&#38663;&#12290;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#22270;&#20687;&#36827;&#34892;&#22320;&#38663;&#30417;&#27979;&#22312;&#21361;&#26426;&#31649;&#29702;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#20272;&#35745;&#22320;&#38663;&#30340;&#20005;&#37325;&#24615;&#21644;&#29305;&#24449;&#26041;&#38754;&#20173;&#23384;&#22312;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.18116</link><description>&lt;p&gt;
QuakeSet&#65306;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20302;&#36164;&#28304;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;Sentinel-1&#30417;&#27979;&#22320;&#38663;
&lt;/p&gt;
&lt;p&gt;
QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18116
&lt;/p&gt;
&lt;p&gt;
QuakeSet&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20302;&#36164;&#28304;&#27169;&#22411;&#65292;&#21033;&#29992;Sentinel-1&#26469;&#30417;&#27979;&#22320;&#38663;&#12290;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#22270;&#20687;&#36827;&#34892;&#22320;&#38663;&#30417;&#27979;&#22312;&#21361;&#26426;&#31649;&#29702;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#20272;&#35745;&#22320;&#38663;&#30340;&#20005;&#37325;&#24615;&#21644;&#29305;&#24449;&#26041;&#38754;&#20173;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#38663;&#30417;&#27979;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#36805;&#36895;&#30830;&#23450;&#21463;&#24433;&#21709;&#21306;&#22495;&#12289;&#20107;&#20214;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#26368;&#32456;&#20272;&#35745;&#25439;&#23475;&#24182;&#35745;&#21010;&#25152;&#38656;&#30340;&#24674;&#22797;&#36807;&#31243;&#12290;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#22270;&#20687;&#22312;&#21361;&#26426;&#31649;&#29702;&#20013;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22320;&#38663;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#20173;&#21463;&#38480;&#20110;&#20351;&#29992;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#35813;&#22320;&#21306;&#30340;&#20154;&#21592;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#22270;&#20687;&#21644;&#28040;&#24687;&#26080;&#27861;&#26377;&#25928;&#22320;&#29992;&#20110;&#20272;&#35745;&#22320;&#38663;&#30340;&#23454;&#38469;&#20005;&#37325;&#24615;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18116v1 Announce Type: cross  Abstract: Earthquake monitoring is necessary to promptly identify the affected areas, the severity of the events, and, finally, to estimate damages and plan the actions needed for the restoration process. The use of seismic stations to monitor the strength and origin of earthquakes is limited when dealing with remote areas (we cannot have global capillary coverage). Identification and analysis of all affected areas is mandatory to support areas not monitored by traditional stations. Using social media images in crisis management has proven effective in various situations. However, they are still limited by the possibility of using communication infrastructures in case of an earthquake and by the presence of people in the area. Moreover, social media images and messages cannot be used to estimate the actual severity of earthquakes and their characteristics effectively. The employment of satellites to monitor changes around the globe grants the po
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#35843;&#30740;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#29983;&#21644;&#25945;&#24072;&#36741;&#21161;&#12289;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.18105</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#65306;&#35843;&#30740;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Education: A Survey and Outlook
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18105
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#35843;&#30740;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#29983;&#21644;&#25945;&#24072;&#36741;&#21161;&#12289;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#25945;&#32946;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#31687;&#35843;&#30740;&#35770;&#25991;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#65292;&#28085;&#30422;&#20102;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#36741;&#21161;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#27599;&#20010;&#35270;&#35282;&#20013;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#25972;&#29702;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#25945;&#32946;&#20013;&#37096;&#32626;LLMs&#25152;&#28041;&#21450;&#30340;&#39118;&#38505;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#65292;&#31361;&#20986;&#20102;&#28508;&#22312;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#26088;&#22312;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#20840;&#38754;&#30340;&#25216;&#26415;&#22270;&#26223;&#65292;&#20197;&#21033;&#29992;LLMs&#30340;&#21147;&#37327;&#65292;&#24443;&#24213;&#25913;&#38761;&#25945;&#32946;&#23454;&#36341;&#65292;&#24182;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#23398;&#20064;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18105v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#32422;&#26463;&#32858;&#31867;&#26041;&#27861;ECS&#65292;&#26088;&#22312;&#25214;&#21040;&#39640;&#36136;&#37327;&#19988;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#65292;&#24378;&#35843;&#22312;&#26500;&#24314;&#32858;&#31867;&#26102;&#24212;&#32771;&#34385;&#32463;&#20856;&#32858;&#31867;&#26631;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18101</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#65306;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#22768;&#26126;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Clustering: A Constrained Declarative based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#32422;&#26463;&#32858;&#31867;&#26041;&#27861;ECS&#65292;&#26088;&#22312;&#25214;&#21040;&#39640;&#36136;&#37327;&#19988;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#65292;&#24378;&#35843;&#22312;&#26500;&#24314;&#32858;&#31867;&#26102;&#24212;&#32771;&#34385;&#32463;&#20856;&#32858;&#31867;&#26631;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;AI&#39046;&#22495;&#22312;&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#37117;&#22791;&#21463;&#20851;&#27880;&#65292;&#32780;&#22312;&#32858;&#31867;&#20013;&#26356;&#20026;&#37325;&#35201;&#65292;&#32858;&#31867;&#26159;&#19968;&#39033;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#20854;&#32467;&#26524;&#24517;&#39035;&#30001;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#22312;&#32463;&#20856;&#32858;&#31867;&#26631;&#20934;&#26041;&#38754;&#20855;&#26377;&#39640;&#36136;&#37327;&#19988;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#26500;&#24314;&#32858;&#31867;&#26102;&#24517;&#39035;&#32771;&#34385;&#36825;&#20004;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#32858;&#31867;&#30340;&#19968;&#20010;&#33391;&#22909;&#30340;&#20840;&#23616;&#35299;&#37322;&#24212;&#35813;&#32771;&#34385;&#27599;&#20010;&#31751;&#30340;&#29305;&#24449;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#25551;&#36848;&#20854;&#23545;&#35937;&#30340;&#33021;&#21147;&#65288;&#35206;&#30422;&#29575;&#65289;&#65292;&#21516;&#26102;&#23558;&#20854;&#19982;&#20854;&#20182;&#31751;&#21306;&#20998;&#24320;&#65288;&#21306;&#20998;&#24230;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#30693;&#35782;&#19987;&#23478;&#22312;&#26399;&#26395;&#32858;&#31867;&#30340;&#32467;&#26500;&#25110;&#20854;&#35299;&#37322;&#26041;&#38754;&#30340;&#19981;&#21516;&#32423;&#21035;&#30340;&#30693;&#35782;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#19968;&#20010;&#31751;&#30340;&#35299;&#37322;&#26159;&#19968;&#32452;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#32422;&#26463;&#32858;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;ECS&#26469;&#25903;&#25345;&#22768;&#26126;&#24615;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18101v1 Announce Type: new  Abstract: The domain of explainable AI is of interest in all Machine Learning fields, and it is all the more important in clustering, an unsupervised task whose result must be validated by a domain expert. We aim at finding a clustering that has high quality in terms of classic clustering criteria and that is explainable, and we argue that these two dimensions must be considered when building the clustering. We consider that a good global explanation of a clustering should give the characteristics of each cluster taking into account their abilities to describe its objects (coverage) while distinguishing it from the other clusters (discrimination). Furthermore, we aim at leveraging expert knowledge, at different levels, on the structure of the expected clustering or on its explanations. In our framework an explanation of a cluster is a set of patterns, and we propose a novel interpretable constrained clustering method called ECS for declarative clu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36793;&#32536;&#35745;&#31639;&#21644;&#29289;&#32852;&#32593;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#20943;&#23569;&#24310;&#36831;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18100</link><description>&lt;p&gt;
&#36890;&#36807;&#20113;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#26234;&#33021;&#29289;&#32852;&#32593;&#30417;&#25511;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Driving Intelligent IoT Monitoring and Control through Cloud Computing and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18100
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36793;&#32536;&#35745;&#31639;&#21644;&#29289;&#32852;&#32593;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#20943;&#23569;&#24310;&#36831;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20113;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#26234;&#33021;&#29289;&#32852;&#32593;&#30340;&#30417;&#25511;&#21644;&#25511;&#21046;&#12290;&#38543;&#30528;&#29289;&#32852;&#32593;&#21644;&#20113;&#32487;&#32493;&#22312;&#32593;&#32476;&#20013;&#29983;&#25104;&#22823;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#20316;&#20026;&#20256;&#24863;&#22120;&#35774;&#22791;&#65292;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#34987;&#21457;&#36865;&#21040;&#20113;&#31471;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#12289;&#39044;&#27979;&#21644;&#25968;&#25454;&#20998;&#26512;&#20197;&#23454;&#29616;&#19994;&#21153;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20113;&#35745;&#31639;&#27169;&#22411;&#21463;&#38480;&#20110;&#36317;&#31163;&#65292;&#23545;&#20110;&#32593;&#32476;&#36830;&#25509;&#36136;&#37327;&#19981;&#29702;&#24819;&#30340;&#29615;&#22659;&#21487;&#33021;&#22312;&#20851;&#38190;&#25805;&#20316;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#36793;&#32536;&#35745;&#31639;&#20316;&#20026;&#20998;&#24067;&#24335;&#35745;&#31639;&#26550;&#26500;&#65292;&#23558;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#12289;&#25968;&#25454;&#21644;&#26381;&#21153;&#30340;&#20301;&#32622;&#20174;&#32593;&#32476;&#30340;&#20013;&#24515;&#33410;&#28857;&#31227;&#21160;&#21040;&#32593;&#32476;&#30340;&#36923;&#36753;&#36793;&#32536;&#33410;&#28857;&#65292;&#20197;&#20943;&#23569;&#23545;&#20113;&#22788;&#29702;&#21644;&#25968;&#25454;&#20998;&#26512;&#30340;&#20381;&#36182;&#65292;&#23454;&#29616;&#36817;&#31471;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#12290;&#29289;&#32852;&#32593;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#32467;&#21512;&#21487;&#20197;&#20943;&#23569;&#24310;&#36831;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18100v1 Announce Type: new  Abstract: This article explores how to drive intelligent iot monitoring and control through cloud computing and machine learning. As iot and the cloud continue to generate large and diverse amounts of data as sensor devices in the network, the collected data is sent to the cloud for statistical analysis, prediction, and data analysis to achieve business objectives. However, because the cloud computing model is limited by distance, it can be problematic in environments where the quality of the Internet connection is not ideal for critical operations. Therefore, edge computing, as a distributed computing architecture, moves the location of processing applications, data and services from the central node of the network to the logical edge node of the network to reduce the dependence on cloud processing and analysis of data, and achieve near-end data processing and analysis. The combination of iot and edge computing can reduce latency, improve efficie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT&#22312;&#36328;&#35821;&#35328;&#27861;&#24459;&#38382;&#31572;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#26512;&#33521;&#35821;&#21644;&#26085;&#35821;&#25552;&#31034;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#21457;&#23637;&#26356;&#39640;&#25928;&#20934;&#30830;&#30340;&#36328;&#35821;&#35328;&#38382;&#31572;&#35299;&#20915;&#26041;&#26696;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.18098</link><description>&lt;p&gt;
GPT&#19982;&#35821;&#35328;&#38556;&#30861;&#65306;&#36328;&#35821;&#35328;&#27861;&#24459;&#38382;&#31572;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPTs and Language Barrier: A Cross-Lingual Legal QA Examination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT&#22312;&#36328;&#35821;&#35328;&#27861;&#24459;&#38382;&#31572;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#26512;&#33521;&#35821;&#21644;&#26085;&#35821;&#25552;&#31034;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#21457;&#23637;&#26356;&#39640;&#25928;&#20934;&#30830;&#30340;&#36328;&#35821;&#35328;&#38382;&#31572;&#35299;&#20915;&#26041;&#26696;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;COLIEE Task 4&#25968;&#25454;&#38598;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPTs&#65289;&#22312;&#36328;&#35821;&#35328;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;COLIEE Task 4&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#38472;&#36848;&#21644;&#19968;&#32452;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#35813;&#38472;&#36848;&#26159;&#21542;&#22312;&#27861;&#24459;&#19978;&#26377;&#25928;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#20174;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#25991;&#31456;&#20013;&#25512;&#26029;&#20986;&#26469;&#65292;&#20063;&#31216;&#20026;&#34164;&#28085;&#20219;&#21153;&#12290;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#19981;&#21516;&#30340;&#33521;&#35821;&#21644;&#26085;&#35821;&#25552;&#31034;&#21644;&#25968;&#25454;&#32452;&#21512;&#65292;&#25105;&#20204;&#20026;&#22810;&#35821;&#35328;&#27861;&#24459;&#38382;&#31572;&#22330;&#26223;&#20013;GPTs&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#22312;&#27861;&#24459;&#39046;&#22495;&#26356;&#39640;&#25928;&#20934;&#30830;&#30340;&#36328;&#35821;&#35328;&#38382;&#31572;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18098v1 Announce Type: cross  Abstract: In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain.
&lt;/p&gt;</description></item><item><title>&#23558;&#25552;&#31034;&#25216;&#26415;&#20316;&#20026;&#26816;&#32034;&#31995;&#32479;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#36890;&#36807;BM25&#39044;&#25490;&#24207;&#21644;&#22522;&#20110;BERT&#30340;&#37325;&#26032;&#25490;&#24207;&#30340;&#25903;&#25345;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27861;&#24459;&#25991;&#20214;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18093</link><description>&lt;p&gt;
&#25552;&#21319;&#27861;&#24459;&#25991;&#20214;&#26816;&#32034;&#65306;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18093
&lt;/p&gt;
&lt;p&gt;
&#23558;&#25552;&#31034;&#25216;&#26415;&#20316;&#20026;&#26816;&#32034;&#31995;&#32479;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#36890;&#36807;BM25&#39044;&#25490;&#24207;&#21644;&#22522;&#20110;BERT&#30340;&#37325;&#26032;&#25490;&#24207;&#30340;&#25903;&#25345;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27861;&#24459;&#25991;&#20214;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#12289;GPT-4&#21644;LLaMA&#31561;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#35768;&#22810;&#30740;&#31350;&#25506;&#32034;&#20102;&#26377;&#25928;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;LLM&#22312;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;&#22312;&#27861;&#24459;&#25968;&#25454;&#39046;&#22495;&#65292;&#20855;&#20307;&#22312;&#26816;&#32034;&#19978;&#65292;&#30001;&#20110;&#27861;&#24459;&#25991;&#31456;&#25968;&#37327;&#24222;&#22823;&#19988;&#38271;&#24230;&#21487;&#35266;&#65292;&#30452;&#25509;&#24212;&#29992;&#25552;&#31034;&#25216;&#26415;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#19987;&#27880;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#25552;&#31034;&#30340;&#28508;&#21147;&#65292;&#23558;&#20854;&#32622;&#20110;&#26816;&#32034;&#31995;&#32479;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#21069;&#38754;&#26377;&#20004;&#20010;&#38454;&#27573;&#30340;&#25903;&#25345;&#65306;BM25&#39044;&#25490;&#24207;&#21644;&#22522;&#20110;BERT&#30340;&#37325;&#26032;&#25490;&#24207;&#12290;&#22312;COLIEE 2023&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#25216;&#26415;&#25972;&#21512;&#21040;&#26816;&#32034;&#31995;&#32479;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;&#26816;&#32034;&#31995;&#32479;&#20013;&#20173;&#38656;&#35201;&#35299;&#20915;&#30340;&#19968;&#20123;&#29616;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18093v1 Announce Type: cross  Abstract: Large language models with billions of parameters, such as GPT-3.5, GPT-4, and LLaMA, are increasingly prevalent. Numerous studies have explored effective prompting techniques to harness the power of these LLMs for various research problems. Retrieval, specifically in the legal data domain, poses a challenging task for the direct application of Prompting techniques due to the large number and substantial length of legal articles. This research focuses on maximizing the potential of prompting by placing it as the final phase of the retrieval system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating prompting techniques on LLMs into the retrieval system significantly improves retrieval accuracy. However, error analysis reveals several existing issues in the retrieval system that still need resolution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#25506;&#35752;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#31574;&#30053;&#36335;&#24452;&#65292;&#21363;&#28385;&#36275;&#36335;&#24452;&#65292;&#23545;&#20110;&#26500;&#24314;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#36335;&#24452;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.18079</link><description>&lt;p&gt;
&#27491;&#24577;&#24418;&#24335;&#21338;&#24328;&#20013;&#30340;&#22343;&#34913;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Paths to Equilibrium in Normal-Form Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#25506;&#35752;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#31574;&#30053;&#36335;&#24452;&#65292;&#21363;&#28385;&#36275;&#36335;&#24452;&#65292;&#23545;&#20110;&#26500;&#24314;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#36335;&#24452;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#65292;&#26234;&#20307;&#20250;&#21453;&#22797;&#22312;&#26102;&#38388;&#19978;&#20132;&#20114;&#65292;&#24182;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#21040;&#26469;&#20462;&#35746;&#20182;&#20204;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31995;&#21015;&#31574;&#30053;&#27010;&#20917;&#12290;&#26412;&#25991;&#30740;&#31350;&#28385;&#36275;&#19968;&#31181;&#30001;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#26356;&#26032;&#21551;&#21457;&#30340;&#25104;&#23545;&#32422;&#26463;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#20854;&#20013;&#22312;&#31532; $t$ &#26399;&#26368;&#20248;&#24212;&#31572;&#30340;&#26234;&#20307;&#22312;&#19979;&#19968;&#26399; $t+1$ &#19981;&#20250;&#25913;&#21464;&#20854;&#31574;&#30053;&#12290;&#36825;&#31181;&#32422;&#26463;&#20165;&#35201;&#27714;&#20248;&#21270;&#26234;&#20307;&#19981;&#26356;&#25913;&#31574;&#30053;&#65292;&#20294;&#24182;&#19981;&#20197;&#20219;&#20309;&#26041;&#24335;&#38480;&#21046;&#20854;&#20182;&#38750;&#26368;&#20248;&#21270;&#26234;&#20307;&#65292;&#22240;&#27492;&#20801;&#35768;&#25506;&#32034;&#12290;&#20855;&#26377;&#27492;&#23646;&#24615;&#30340;&#24207;&#21015;&#34987;&#31216;&#20026;&#28385;&#36275;&#36335;&#24452;&#65292;&#24182;&#22312;&#35768;&#22810; MARL &#31639;&#27861;&#20013;&#33258;&#28982;&#20986;&#29616;&#12290;&#20851;&#20110;&#25112;&#30053;&#21160;&#24577;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#21338;&#24328;&#21644;&#21021;&#22987;&#31574;&#30053;&#27010;&#20917;&#65292;&#26159;&#21542;&#24635;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#28385;&#36275;&#36335;&#24452;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#23545;&#24212;&#30528;&#19968;&#20123;&#37325;&#35201;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18079v1 Announce Type: cross  Abstract: In multi-agent reinforcement learning (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy? The resolution of this question has implication
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#28023;&#27915;&#22403;&#22334;&#39046;&#22495;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;YOLO&#31995;&#21015;&#22312;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#30740;&#31350;&#26174;&#31034;&#24403;&#21069;&#32570;&#20047;&#20840;&#38754;&#30340;&#27700;&#19979;&#22403;&#22334;&#25968;&#25454;&#24211;&#65292;&#36825;&#25104;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#29942;&#39048;&#12290;</title><link>https://arxiv.org/abs/2403.18067</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#28023;&#27915;&#22403;&#22334;&#36319;&#36394;&#21644;&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
State of the art applications of deep learning within tracking and detecting marine debris: A survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18067
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#28023;&#27915;&#22403;&#22334;&#39046;&#22495;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;YOLO&#31995;&#21015;&#22312;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#30740;&#31350;&#26174;&#31034;&#24403;&#21069;&#32570;&#20047;&#20840;&#38754;&#30340;&#27700;&#19979;&#22403;&#22334;&#25968;&#25454;&#24211;&#65292;&#36825;&#25104;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#28023;&#27915;&#22403;&#22334;&#38382;&#39064;&#20013;&#24050;&#32463;&#34987;&#25506;&#32034;&#20102;&#22823;&#32422;20&#24180;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#22312;&#36807;&#21435;&#20116;&#24180;&#20869;&#24555;&#36895;&#21457;&#23637;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26368;&#36817;28&#39033;&#28145;&#24230;&#23398;&#20064;&#22312;&#28023;&#27915;&#22403;&#22334;&#39046;&#22495;&#20013;&#26368;&#26032;&#21644;&#26368;&#37325;&#35201;&#36129;&#29486;&#30340;&#28145;&#20837;&#12289;&#26368;&#26032;&#30340;&#24635;&#32467;&#21644;&#20998;&#26512;&#12290;&#36890;&#36807;&#20132;&#21449;&#24341;&#29992;&#30740;&#31350;&#35770;&#25991;&#32467;&#26524;&#65292;YOLO&#31995;&#21015;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#35768;&#22810;&#21463;&#23562;&#25964;&#30340;&#36129;&#29486;&#32773;&#26126;&#30830;&#34920;&#31034;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#25104;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#38754;&#27700;&#19979;&#22403;&#22334;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#21644;&#26631;&#35760;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#22312;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;YOLOv5&#65292;&#24182;&#21457;&#29616;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20551;&#38451;&#24615;&#29575;&#36739;&#39640;&#65292;&#31361;&#26174;&#20102;&#20840;&#38754;&#25968;&#25454;&#24211;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#24635;&#32467;&#20102;&#36229;&#36807;40&#39033;&#26410;&#26469;&#30340;&#30740;&#31350;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18067v1 Announce Type: cross  Abstract: Deep learning techniques have been explored within the marine litter problem for approximately 20 years but the majority of the research has developed rapidly in the last five years. We provide an in-depth, up to date, summary and analysis of 28 of the most recent and significant contributions of deep learning in marine debris. From cross referencing the research paper results, the YOLO family significantly outperforms all other methods of object detection but there are many respected contributions to this field that have categorically agreed that a comprehensive database of underwater debris is not currently available for machine learning. Using a small dataset curated and labelled by us, we tested YOLOv5 on a binary classification task and found the accuracy was low and the rate of false positives was high; highlighting the importance of a comprehensive database. We conclude this survey with over 40 future research recommendations an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18063</link><description>&lt;p&gt;
&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65306;&#21327;&#35843;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18063
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;Transformer&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#32467;&#26500;&#36827;&#34892;&#20102;&#30740;&#31350; - &#22914;ViT&#12289;PVT&#21644;Swin&#12290;&#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#25913;&#36827;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20351;&#20854;&#26356;&#21152;&#39640;&#25928;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#20154;&#20204;&#24863;&#21463;&#21040;&#20102;&#21253;&#21547;&#23616;&#37096;&#20449;&#24687;&#30340;&#38656;&#35201;&#65292;&#36825;&#23548;&#33268;&#22312;Transformer&#20013;&#24341;&#20837;&#21367;&#31215;&#65292;&#22914;CPVT&#21644;CvT&#12290;&#25105;&#20204;&#20351;&#29992;&#22797;&#26434;&#20613;&#31435;&#21494;&#22522;&#30784;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;AFNO&#12289;GFNet&#21644;Spectformer&#23454;&#29616;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20513;&#32467;&#21512;&#25968;&#25454;&#30340;&#19977;&#31181;&#19981;&#21516;&#35270;&#22270; - &#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#23454;&#22495;&#20809;&#35889;&#34920;&#31034;&#30340;&#26368;&#31616;&#21333;&#20840;&#23616;&#34920;&#31034; - &#36890;&#36807;Hartley&#21464;&#25442;&#33719;&#24471;&#12290;&#25105;&#20204;&#22312;&#21021;&#22987;&#23618;&#20013;&#20351;&#29992;&#21367;&#31215;&#31639;&#23376;&#25429;&#25417;&#23616;&#37096;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#24182;&#33719;&#24471;&#19968;&#20010;&#25552;&#20379;&#25913;&#36827;&#24615;&#33021;&#30340;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65288;SCT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18063v1 Announce Type: cross  Abstract: Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20960;&#20309;&#20998;&#35299;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20219;&#21153;&#23548;&#21521;&#25235;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23569;&#30340;&#20449;&#24687;&#23454;&#29616;&#26234;&#33021;&#25235;&#21462;&#12290;</title><link>https://arxiv.org/abs/2403.18062</link><description>&lt;p&gt;
ShapeGrasp&#65306;&#21033;&#29992;&#20960;&#20309;&#20998;&#35299;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#20219;&#21153;&#23548;&#21521;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18062
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#20998;&#35299;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20219;&#21153;&#23548;&#21521;&#25235;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23569;&#30340;&#20449;&#24687;&#23454;&#29616;&#26234;&#33021;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#24237;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#38024;&#23545;&#38476;&#29983;&#29289;&#20307;&#36827;&#34892;&#20219;&#21153;&#23548;&#21521;&#30340;&#25235;&#21462;&#26159;&#26426;&#22120;&#20154;&#24517;&#22791;&#30340;&#25216;&#33021;&#12290;&#21463;&#21040;&#20154;&#31867;&#36890;&#36807;&#23545;&#29289;&#20307;&#24418;&#29366;&#21644;&#32467;&#26500;&#30340;&#30452;&#35273;&#26469;&#25235;&#21462;&#29289;&#20307;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#23548;&#21521;&#25235;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#23558;&#30446;&#26631;&#29289;&#20307;&#20960;&#20309;&#20998;&#35299;&#20026;&#31616;&#21333;&#20984;&#24418;&#29366;&#30340;&#22270;&#32467;&#26500;&#65292;&#21253;&#25324;&#20960;&#20309;&#23646;&#24615;&#21644;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26368;&#23567;&#30340;&#24517;&#35201;&#20449;&#24687; - &#29289;&#20307;&#30340;&#21517;&#31216;&#21644;&#39044;&#26399;&#20219;&#21153; - &#20419;&#36827;&#38646;&#26679;&#26412;&#20219;&#21153;&#23548;&#21521;&#25235;&#21462;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#21160;&#24577;&#22320;&#20026;&#27599;&#20010;&#20998;&#35299;&#37096;&#20998;&#20998;&#37197;&#35821;&#20041;&#21547;&#20041;&#65292;&#38543;&#21518;&#23545;&#27599;&#20010;&#37096;&#20998;&#23545;&#20110;&#39044;&#26399;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#36827;&#34892;&#25512;&#29702;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25235;&#21462;&#26041;&#27861;&#30340;&#20998;&#35299;&#21644;&#25512;&#29702;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18062v1 Announce Type: cross  Abstract: Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments. Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel zero-shot task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a graph structure, including geometric attributes and spatial relationships. Our approach employs minimal essential information - the object's name and the intended task - to facilitate zero-shot task-oriented grasping. We utilize the commonsense reasoning capabilities of large language models to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task. Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approach's decomposition and reason
&lt;/p&gt;</description></item><item><title>COIG-CQIA &#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2403.18058</link><description>&lt;p&gt;
COIG-CQIA&#65306;&#21482;&#38656;&#36136;&#37327;&#8212;&#8212;&#38754;&#21521;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18058
&lt;/p&gt;
&lt;p&gt;
COIG-CQIA &#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#33521;&#35821;&#39046;&#22495;&#12290;&#36825;&#20123;&#36827;&#23637;&#20351;&#24471;&#36825;&#20123;LLMs&#33021;&#22815;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#29702;&#35299;&#24182;&#25191;&#34892;&#22797;&#26434;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#30340;&#21457;&#23637;&#20173;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#20013;&#25991;&#35821;&#35328;&#30340;&#29420;&#29305;&#35821;&#35328;&#29305;&#24449;&#21644;&#25991;&#21270;&#28145;&#24230;&#20026;&#25351;&#20196;&#24494;&#35843;&#20219;&#21153;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#35201;&#20040;&#28304;&#33258;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#65292;&#35201;&#20040;&#19981;&#36866;&#21512;&#19982;&#29616;&#23454;&#20013;&#25991;&#29992;&#25143;&#30340;&#20132;&#20114;&#27169;&#24335;&#30456;&#31526;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;COIG-CQIA&#65292;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#26469;&#28304;&#25910;&#38598;&#20102;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#32534;&#20889;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18058v1 Announce Type: cross  Abstract: Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language. These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various so
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#20808;&#32423;&#24322;&#26500;&#32852;&#30431;&#24378;&#21270;&#23398;&#20064;&#65288;PHLRL&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#24322;&#26500;&#21512;&#20316;&#38382;&#39064;&#65292;&#36890;&#36807;&#35760;&#24405;&#26234;&#33021;&#20307;&#25506;&#32034;&#36807;&#30340;&#21508;&#31181;&#31574;&#30053;&#24182;&#24314;&#31435;&#24322;&#26500;&#32852;&#30431;&#26469;&#20248;&#21270;&#26410;&#26469;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.18057</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#24322;&#26500;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20248;&#20808;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18057
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#20808;&#32423;&#24322;&#26500;&#32852;&#30431;&#24378;&#21270;&#23398;&#20064;&#65288;PHLRL&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#24322;&#26500;&#21512;&#20316;&#38382;&#39064;&#65292;&#36890;&#36807;&#35760;&#24405;&#26234;&#33021;&#20307;&#25506;&#32034;&#36807;&#30340;&#21508;&#31181;&#31574;&#30053;&#24182;&#24314;&#31435;&#24322;&#26500;&#32852;&#30431;&#26469;&#20248;&#21270;&#26410;&#26469;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#24322;&#26500;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#21508;&#31181;&#30495;&#23454;&#22240;&#32032;&#65292;&#22914;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#25972;&#20307;&#31995;&#32479;&#25104;&#26412;&#30340;&#26234;&#33021;&#20307;&#12290;&#19982;&#21516;&#36136;&#31995;&#32479;&#30456;&#27604;&#65292;&#24322;&#26500;&#31995;&#32479;&#20855;&#26377;&#26174;&#33879;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#25361;&#25112;&#65292;&#21253;&#25324;&#35299;&#20915;&#38750;&#38745;&#24577;&#38382;&#39064;&#21644;&#31649;&#29702;&#20855;&#26377;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#24179;&#34913;&#25968;&#37327;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20248;&#20808;&#32423;&#24322;&#26500;&#32852;&#30431;&#24378;&#21270;&#23398;&#20064;&#65288;PHLRL&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24322;&#26500;&#21512;&#20316;&#38382;&#39064;&#12290;PHLRL&#35760;&#24405;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25506;&#32034;&#36807;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#24182;&#24314;&#31435;&#19968;&#20010;&#30001;&#22810;&#26679;&#31574;&#30053;&#32452;&#25104;&#30340;&#24322;&#26500;&#32852;&#30431;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20248;&#20808;&#32423;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#20197;&#34917;&#20607;&#30001;&#19981;&#21516;&#31867;&#22411;&#26234;&#33021;&#20307;&#25968;&#37327;&#24046;&#24322;&#24341;&#36215;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18057v1 Announce Type: new  Abstract: Large-scale heterogeneous multiagent systems feature various realistic factors in the real world, such as agents with diverse abilities and overall system cost. In comparison to homogeneous systems, heterogeneous systems offer significant practical advantages. Nonetheless, they also present challenges for multiagent reinforcement learning, including addressing the non-stationary problem and managing an imbalanced number of agents with different types. We propose a Prioritized Heterogeneous League Reinforcement Learning (PHLRL) method to address large-scale heterogeneous cooperation problems. PHLRL maintains a record of various policies that agents have explored during their training and establishes a heterogeneous league consisting of diverse policies to aid in future policy optimization. Furthermore, we design a prioritized policy gradient approach to compensate for the gap caused by differences in the number of different types of agent
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Cooperation Graph Learning&#65288;HCGL&#65289;&#30340;&#26032;&#22411;&#20998;&#23618;MARL&#27169;&#22411;&#65292;&#20854;&#20013;&#20195;&#29702;&#30340;&#34892;&#20026;&#21463;&#21487;&#25193;&#23637;&#21512;&#20316;&#22270;&#65288;ECG&#65289;&#30340;&#25299;&#25169;&#32467;&#26500;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#19968;&#33324;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.18056</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#25193;&#23637;&#21512;&#20316;&#22270;&#30340;&#33258;&#20027;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Cooperation Graph Learning&#65288;HCGL&#65289;&#30340;&#26032;&#22411;&#20998;&#23618;MARL&#27169;&#22411;&#65292;&#20854;&#20013;&#20195;&#29702;&#30340;&#34892;&#20026;&#21463;&#21487;&#25193;&#23637;&#21512;&#20316;&#22270;&#65288;ECG&#65289;&#30340;&#25299;&#25169;&#32467;&#26500;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#19968;&#33324;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#22312;&#35299;&#20915;&#35768;&#22810;&#21512;&#20316;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;&#38750;&#20998;&#23618;MARL&#31639;&#27861;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#38656;&#35201;&#20998;&#23618;&#21512;&#20316;&#34892;&#20026;&#30340;&#21508;&#31181;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;&#38750;&#20998;&#23618;&#31639;&#27861;&#23398;&#20064;&#30340;&#21512;&#20316;&#30693;&#35782;&#21644;&#31574;&#30053;&#26159;&#38544;&#24335;&#30340;&#65292;&#19981;&#26131;&#35299;&#37322;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#29616;&#26377;&#30693;&#35782;&#30340;&#25972;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#19968;&#33324;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#26032;&#22411;&#20998;&#23618;MARL&#27169;&#22411;&#65292;&#21517;&#20026;Hierarchical Cooperation Graph Learning&#65288;HCGL&#65289;&#12290;HCGL&#26377;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#29992;&#20110;&#23454;&#29616;&#33258;&#25105;&#32858;&#31867;&#21512;&#20316;&#30340;&#21160;&#24577;&#21487;&#25193;&#23637;&#21512;&#20316;&#22270;&#65288;ECG&#65289;&#65307;&#19968;&#32452;&#29992;&#20110;&#35843;&#25972;ECG&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#25805;&#20316;&#22120;&#65307;&#20197;&#21450;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#22270;&#25805;&#20316;&#22120;&#30340;MARL&#20248;&#21270;&#22120;&#12290;HCGL&#19982;&#20854;&#20182;MARL&#27169;&#22411;&#30340;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#65292;&#20195;&#29702;&#30340;&#34892;&#20026;&#21463;ECG&#25299;&#25169;&#32467;&#26500;&#30340;&#24341;&#23548;&#65292;&#32780;&#19981;&#26159;&#31574;&#30053;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18056v1 Announce Type: new  Abstract: Multi-Agent Reinforcement Learning (MARL) has been successful in solving many cooperative challenges. However, classic non-hierarchical MARL algorithms still cannot address various complex multi-agent problems that require hierarchical cooperative behaviors. The cooperative knowledge and policies learned in non-hierarchical algorithms are implicit and not interpretable, thereby restricting the integration of existing knowledge. This paper proposes a novel hierarchical MARL model called Hierarchical Cooperation Graph Learning (HCGL) for solving general multi-agent problems. HCGL has three components: a dynamic Extensible Cooperation Graph (ECG) for achieving self-clustering cooperation; a group of graph operators for adjusting the topology of ECG; and an MARL optimizer for training these graph operators. HCGL's key distinction from other MARL models is that the behaviors of agents are guided by the topology of ECG instead of policy neural
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30563;&#23548;&#25552;&#31034;&#35757;&#32451;&#65288;SPT&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;LLM&#31995;&#32479;&#29983;&#25104;&#39640;&#25928;&#25552;&#31034;&#24182;&#24341;&#20837;&#24433;&#21709;&#20998;&#25968;&#27010;&#24565;&#65292;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#25104;&#21151;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18051</link><description>&lt;p&gt;
&#30563;&#23548;&#25552;&#31034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Supervisory Prompt Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18051
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30563;&#23548;&#25552;&#31034;&#35757;&#32451;&#65288;SPT&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;LLM&#31995;&#32479;&#29983;&#25104;&#39640;&#25928;&#25552;&#31034;&#24182;&#24341;&#20837;&#24433;&#21709;&#20998;&#25968;&#27010;&#24565;&#65292;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#25104;&#21151;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25552;&#31034;&#30340;&#36136;&#37327;&#65292;&#36825;&#20123;&#25552;&#31034;&#36890;&#24120;&#26159;&#25163;&#24037;&#35774;&#35745;&#30340;&#24182;&#19988;&#29305;&#23450;&#20110;&#20219;&#21153;&#65292;&#20351;&#24471;&#23427;&#20204;&#26114;&#36149;&#19988;&#19981;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#30563;&#23548;&#25552;&#31034;&#35757;&#32451;&#65288;SPT&#65289;&#12290;SPT&#21033;&#29992;&#21452;LLM&#31995;&#32479;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#30340;&#25552;&#31034;&#12290;&#22312;&#35813;&#31995;&#32479;&#20013;&#65292;&#19968;&#20010;LLM&#65292;&#21363;&#29983;&#25104;&#22120;&#65292;&#25191;&#34892;&#20219;&#21153;&#65292;&#32780;&#21478;&#19968;&#20010;LLM&#65292;&#21363;&#26657;&#27491;&#22120;&#65292;&#25552;&#20379;&#21453;&#39304;&#24182;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#12290;&#19982;&#20808;&#21069;&#30340;&#25216;&#26415;&#30456;&#27604;&#65292;&#29983;&#25104;&#22120;&#21644;&#26657;&#27491;&#22120;&#22312;&#26102;&#38388;&#19978;&#20849;&#21516;&#24182;&#25345;&#32493;&#25913;&#36827;&#23427;&#20204;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#8220;&#24433;&#21709;&#20998;&#25968;&#8221;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34913;&#37327;&#25552;&#31034;&#30340;&#21477;&#23376;&#32423;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#27979;&#35797;LLMs&#20013;&#24187;&#35273;&#30340;&#27700;&#24179;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;GPT-4&#22312;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;65.8%&#25552;&#39640;&#21040;94.1%&#65288;&#22686;&#21152;28.3%&#65289;&#12290;SPT&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18051v1 Announce Type: cross  Abstract: The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable. We propose a novel approach, Supervisory Prompt Training (SPT). SPT automates the generation of highly effective prompts using a dual LLM system. In this system, one LLM, the generator, performs a task while the other, the corrector, provides feedback and generates improved prompts. In contrast to earlier techniques, both the generator and corrector collaboratively and continuously improve their prompts over time. We also introduce the concept of \textit{impact scores} to measure the sentence-level effectiveness of the prompts. Our method was tested on four benchmarks, testing the level of hallucinations in LLMs. Notably, we were able to increase the accuracy of GPT-4 on GSM8K from 65.8\% to 94.1\% (28.3\% increase). SPT advances LLMs by refining prompts to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#26469;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#21487;&#20197;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18028</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Predicting species occurrence patterns from partial observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#26469;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#21487;&#20197;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#21644;&#27668;&#20505;&#21361;&#26426;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#29289;&#31181;&#20998;&#24067;&#30340;&#20301;&#32622;&#20197;&#21450;&#36825;&#20123;&#27169;&#24335;&#22914;&#20309;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29289;&#31181;&#30340;&#35266;&#27979;&#25968;&#25454;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#65292;&#21487;&#29992;&#25968;&#25454;&#30340;&#37327;&#22312;&#19981;&#21516;&#20998;&#31867;&#32676;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#12290;&#20026;&#20102;&#22312;&#27492;&#20219;&#21153;&#19978;&#35780;&#20272;&#31639;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SatButterfly&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#34676;&#34678;&#30340;&#21355;&#26143;&#22270;&#20687;&#12289;&#29615;&#22659;&#25968;&#25454;&#21644;&#35266;&#27979;&#25968;&#25454;&#65292;&#26088;&#22312;&#19982;&#29616;&#26377;&#30340;&#40479;&#31867;&#35266;&#27979;&#25968;&#25454;&#38598;SatBird&#37197;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#29992;&#20110;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#22320;&#26041;&#20351;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;R-Tran&#22312;&#39044;&#27979;&#29289;&#31181;&#36973;&#36935;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18028v1 Announce Type: cross  Abstract: To address the interlinked biodiversity and climate crises, we need an understanding of where species occur and how these patterns are changing. However, observational data on most species remains very limited, and the amount of data available varies greatly between taxonomic groups. We introduce the problem of predicting species occurrence patterns given (a) satellite imagery, and (b) known information on the occurrence of other species. To evaluate algorithms on this task, we introduce SatButterfly, a dataset of satellite images, environmental data and observational data for butterflies, which is designed to pair with the existing SatBird dataset of bird observational data. To address this task, we propose a general model, R-Tran, for predicting species occurrence patterns that enables the use of partial observational data wherever found. We find that R-Tran outperforms other methods in predicting species encounter rates with partial
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.18025</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#23450;&#25513;&#30721;&#25439;&#22833;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65306;&#20197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#21040;&#26032;&#39046;&#22495;&#36890;&#24120;&#36890;&#36807;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;LM&#65288;PLM&#65289;&#26469;&#23454;&#29616;&#12290;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#24341;&#20837;LM&#65292;&#20351;&#23427;&#33021;&#22815;&#29702;&#35299;&#21644;&#26377;&#25928;&#25191;&#34892;&#30446;&#26631;&#22495;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#21464;&#24471;&#19981;&#22815;&#25935;&#24863;&#65292;&#22914;&#26524;&#23427;&#24573;&#35270;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24191;&#27867;&#24046;&#24322;&#65288;&#20363;&#22914;&#22312;&#35789;&#20041;&#19978;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36866;&#24403;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65288;DS-terms&#65289;&#30340;&#37325;&#35201;&#24615;&#26469;&#26377;&#25928;&#33719;&#21462;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;MSLM&#21516;&#26102;&#23631;&#34109;DS&#26415;&#35821;&#21644;&#36890;&#29992;&#35789;&#65292;&#28982;&#21518;&#36890;&#36807;&#30830;&#20445;LM&#21463;&#21040;&#26356;&#22823;&#24809;&#32602;&#26469;&#23398;&#20064;&#29305;&#23450;&#20110;&#25513;&#30721;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;Wasserstein&#22270;&#21305;&#37197;&#30340;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#22256;&#22659;</title><link>https://arxiv.org/abs/2403.17995</link><description>&lt;p&gt;
&#32771;&#34385;Wasserstein&#22270;&#21305;&#37197;&#30340;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Image Captioning Considering Wasserstein Graph Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;Wasserstein&#22270;&#21305;&#37197;&#30340;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#32473;&#23450;&#22270;&#20687;&#30340;&#23383;&#24149;&#65292;&#20854;&#20851;&#38190;&#25361;&#25112;&#26159;&#23398;&#20064;&#20174;&#35270;&#35273;&#29305;&#24449;&#21040;&#33258;&#28982;&#35821;&#35328;&#29305;&#24449;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#25551;&#36848;&#22270;&#20687;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#30340;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;Wasserstein&#22270;&#21305;&#37197;&#30340;&#26032;&#22411;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65288;SSIC-WGM&#65289;&#65292;&#20197;&#30417;&#30563;&#29983;&#25104;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17995v1 Announce Type: cross  Abstract: Image captioning can automatically generate captions for the given images, and the key challenge is to learn a mapping function from visual features to natural language features. Existing approaches are mostly supervised ones, i.e., each image has a corresponding sentence in the training set. However, considering that describing images always requires a huge of manpower, we usually have limited amount of described images (i.e., image-text pairs) and a large number of undescribed images in real-world applications. Thereby, a dilemma is the "Semi-Supervised Image Captioning". To solve this problem, we propose a novel Semi-Supervised Image Captioning method considering Wasserstein Graph Matching (SSIC-WGM), which turns to adopt the raw image inputs to supervise the generated sentences. Different from traditional single modal semi-supervised methods, the difficulty of semi-supervised cross-modal learning lies in constructing intermediately
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.17993</link><description>&lt;p&gt;
&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#28982;&#26234;&#33021;&#30456;&#34701;&#21512;&#65306;&#20174;&#32479;&#35745;&#21147;&#23398;&#21040;&#20154;&#24037;&#26234;&#33021;&#20877;&#21040;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17993
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#21453;&#24605;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#26410;&#26469;&#35282;&#33394;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#28237;&#27969;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#26681;&#26893;&#20110;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#26816;&#39564;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#27169;&#22411;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#23457;&#26597;&#20102;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#21508;&#31181;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#32479;&#35745;&#27969;&#20307;&#21147;&#23398;&#30340;&#21516;&#26102;&#21457;&#23637;&#20013;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#24615;&#30284;&#32454;&#32990;&#26816;&#27979;&#30340;&#22768;&#23376;&#26174;&#24494;&#38236;&#65292;&#23454;&#29616;&#20102;&#25209;&#38388;&#26657;&#20934;&#21644;&#26102;&#38388;&#20998;&#36776;&#22768;&#23376;&#20449;&#21495;&#31934;&#20934;&#32454;&#32990;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;89.22%&#30340;&#24179;&#34913;&#31934;&#24230;&#21644;0.5&#31186;&#30340;&#24555;&#36895;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.17992</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20219;&#21153;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#24615;&#30284;&#32454;&#32990;&#26816;&#27979;&#30340;&#22768;&#23376;&#26174;&#24494;&#38236;
&lt;/p&gt;
&lt;p&gt;
Interpretable cancer cell detection with phonon microscopy using multi-task conditional neural networks for inter-batch calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#24615;&#30284;&#32454;&#32990;&#26816;&#27979;&#30340;&#22768;&#23376;&#26174;&#24494;&#38236;&#65292;&#23454;&#29616;&#20102;&#25209;&#38388;&#26657;&#20934;&#21644;&#26102;&#38388;&#20998;&#36776;&#22768;&#23376;&#20449;&#21495;&#31934;&#20934;&#32454;&#32990;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;89.22%&#30340;&#24179;&#34913;&#31934;&#24230;&#21644;0.5&#31186;&#30340;&#24555;&#36895;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#22312;&#25581;&#31034;&#22768;&#23376;&#26174;&#24494;&#38236;&#65288;&#39640;&#39057;&#36229;&#22768;&#27874;&#65289;&#25968;&#25454;&#20013;&#30340;&#22522;&#26412;&#20449;&#24687;&#20197;&#35782;&#21035;&#30284;&#32454;&#32990;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#25216;&#26415;&#21463;&#21040;&#8220;&#25209;&#27425;&#25928;&#24212;&#8221;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#27599;&#27425;&#23454;&#39564;&#20043;&#38388;&#26080;&#27861;&#36991;&#20813;&#30340;&#25216;&#26415;&#21464;&#21270;&#25152;&#36896;&#25104;&#30340;&#65292;&#36825;&#20123;&#21464;&#21270;&#20250;&#20135;&#29983;&#28151;&#28102;&#21464;&#37327;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23398;&#20064;&#21040;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#26088;&#22312;&#21516;&#26102;&#23454;&#29616;&#25209;&#38388;&#26657;&#20934;&#65292;&#36890;&#36807;&#28040;&#38500;&#28151;&#28102;&#21464;&#37327;&#65292;&#24182;&#23545;&#22522;&#20110;&#26102;&#38388;&#20998;&#36776;&#22768;&#23376;&#20449;&#21495;&#30340;&#32454;&#32990;&#20934;&#30830;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#25209;&#27425;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#32972;&#26223;&#12289;&#20581;&#24247;&#21644;&#30284;&#30151;&#21306;&#22495;&#20998;&#31867;&#30340;&#24179;&#34913;&#31934;&#24230;&#20026;89.22&#65285;&#65292;&#20132;&#21449;&#39564;&#35777;&#24179;&#22343;&#31934;&#24230;&#20026;89.07&#65285;&#12290;&#20998;&#31867;&#21487;&#22312;0.5&#31186;&#20869;&#23436;&#25104;&#65292;&#21482;&#38656;&#35201;&#31616;&#21333;&#30340;&#20808;&#21069;&#25209;&#27425;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17992v1 Announce Type: cross  Abstract: Advances in artificial intelligence (AI) show great potential in revealing underlying information from phonon microscopy (high-frequency ultrasound) data to identify cancerous cells. However, this technology suffers from the 'batch effect' that comes from unavoidable technical variations between each experiment, creating confounding variables that the AI model may inadvertently learn. We therefore present a multi-task conditional neural network framework to simultaneously achieve inter-batch calibration, by removing confounding variables, and accurate cell classification of time-resolved phonon-derived signals. We validate our approach by training and validating on different experimental batches, achieving a balanced precision of 89.22% and an average cross-validated precision of 89.07% for classifying background, healthy and cancerous regions. Classification can be performed in 0.5 seconds with only simple prior batch information requ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20840;&#24687;&#20840;&#23616;&#21367;&#31215;&#32593;&#32476;&#65288;HGConv&#65289;&#21644;&#20840;&#24687;&#31616;&#21270;&#34920;&#31034;&#65288;HRR&#65289;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#26680;&#35745;&#31639;&#25110;&#35774;&#35745;&#65292;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17978</link><description>&lt;p&gt;
&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#38271;&#31243;&#39044;&#27979;&#20219;&#21153;&#30340;&#20840;&#24687;&#20840;&#23616;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Holographic Global Convolutional Networks for Long-Range Prediction Tasks in Malware Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20840;&#24687;&#20840;&#23616;&#21367;&#31215;&#32593;&#32476;&#65288;HGConv&#65289;&#21644;&#20840;&#24687;&#31616;&#21270;&#34920;&#31034;&#65288;HRR&#65289;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#26680;&#35745;&#31639;&#25110;&#35774;&#35745;&#65292;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26159;&#19968;&#20010;&#26377;&#36259;&#19988;&#26377;&#20215;&#20540;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#23545;&#29616;&#23454;&#19990;&#30028;&#26377;&#37325;&#35201;&#24433;&#21709;&#24182;&#20855;&#26377;&#29420;&#29305;&#30340;&#26426;&#22120;&#23398;&#20064;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#29616;&#26377;&#30340;&#38271;&#31243;&#25216;&#26415;&#21644;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20010;&#38382;&#39064;&#39046;&#22495;&#19981;&#22826;&#36866;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21033;&#29992;&#20840;&#24687;&#31616;&#21270;&#34920;&#31034;&#65288;HRR&#65289;&#23646;&#24615;&#26469;&#32534;&#30721;&#21644;&#35299;&#30721;&#24207;&#21015;&#20803;&#32032;&#29305;&#24449;&#30340;&#20840;&#24687;&#20840;&#23616;&#21367;&#31215;&#32593;&#32476;&#65288;HGConv&#65289;&#12290;&#19982;&#20854;&#20182;&#20840;&#23616;&#21367;&#31215;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#22797;&#26434;&#30340;&#26680;&#35745;&#31639;&#25110;&#31934;&#24515;&#35774;&#35745;&#30340;&#26680;&#12290;HGConv&#26680;&#34987;&#23450;&#20041;&#20026;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#30340;&#31616;&#21333;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#22312;Microsoft&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25361;&#25112;&#36187;&#12289;Drebin&#21644;EMBER&#24694;&#24847;&#36719;&#20214;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;&#22312;&#24207;&#21015;&#38271;&#24230;&#30340;&#23545;&#25968;&#32423;&#22797;&#26434;&#24230;&#19979;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;HGConv&#30340;&#36816;&#34892;&#26102;&#38388;&#26126;&#26174;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17978v1 Announce Type: cross  Abstract: Malware detection is an interesting and valuable domain to work in because it has significant real-world impact and unique machine-learning challenges. We investigate existing long-range techniques and benchmarks and find that they're not very suitable in this problem area. In this paper, we introduce Holographic Global Convolutional Networks (HGConv) that utilize the properties of Holographic Reduced Representations (HRR) to encode and decode features from sequence elements. Unlike other global convolutional methods, our method does not require any intricate kernel computation or crafted kernel design. HGConv kernels are defined as simple parameters learned through backpropagation. The proposed method has achieved new SOTA results on Microsoft Malware Classification Challenge, Drebin, and EMBER malware benchmarks. With log-linear complexity in sequence length, the empirical results demonstrate substantially faster run-time by HGConv c
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861; DGDATA &#29420;&#29305;&#22320;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#20102;&#26102;&#38388;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.17958</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;&#19982;&#26102;&#38388;&#27880;&#24847;&#21147;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Domain Adaptation with Temporal Attention for Cross-User Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17958
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861; DGDATA &#29420;&#29305;&#22320;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#20102;&#26102;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#20551;&#35774;&#26159;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30446;&#30340;&#30340;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#23454;&#29616;&#20013;&#65292;&#24448;&#24448;&#25361;&#25112;&#36825;&#19968;&#27010;&#24565;&#65292;&#23588;&#20854;&#26159;&#22312;&#36328;&#29992;&#25143;HAR&#31561;&#22330;&#26223;&#20013;&#34920;&#29616;&#20026;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#12290;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#24212;&#23545;&#36328;&#29992;&#25143;HAR&#20219;&#21153;&#20013;&#36825;&#20123;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#20013;&#19968;&#20010;&#26126;&#26174;&#30340;&#32570;&#38519;&#26159;&#24573;&#35270;&#20102;&#22312;&#35843;&#25972;&#25968;&#25454;&#20998;&#24067;&#38454;&#27573;&#23884;&#20837;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#38024;&#23545;&#36825;&#19968;&#30095;&#28431;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#24102;&#26377;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29983;&#25104;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DGDATA&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17958v1 Announce Type: cross  Abstract: In Human Activity Recognition (HAR), a predominant assumption is that the data utilized for training and evaluation purposes are drawn from the same distribution. It is also assumed that all data samples are independent and identically distributed ($\displaystyle i.i.d.$). Contrarily, practical implementations often challenge this notion, manifesting data distribution discrepancies, especially in scenarios such as cross-user HAR. Domain adaptation is the promising approach to address these challenges inherent in cross-user HAR tasks. However, a clear gap in domain adaptation techniques is the neglect of the temporal relation embedded within time series data during the phase of aligning data distributions. Addressing this oversight, our research presents the Deep Generative Domain Adaptation with Temporal Attention (DGDATA) method. This novel method uniquely recognises and integrates temporal relations during the domain adaptation proce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23398;&#20064;&#12289;&#29702;&#35299;&#21644;&#25506;&#32034;&#21069;&#30651;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#26032;&#39062;&#30340;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.17942</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#23454;&#29983;&#27963;&#21644;&#35745;&#31639;&#20013;&#30340;&#21069;&#30651;&#24615;&#30340;&#27880;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Note On Lookahead In Real Life And Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23398;&#20064;&#12289;&#29702;&#35299;&#21644;&#25506;&#32034;&#21069;&#30651;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#26032;&#39062;&#30340;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#20026;&#20102;&#20182;&#20204;&#30340;&#23384;&#22312;&#21644;&#25104;&#38271;&#32780;&#23450;&#20041;&#30340;&#19977;&#20010;&#26102;&#38388;&#21644;&#36923;&#36753;&#27010;&#24565;&#12290;&#25105;&#20204;&#20316;&#20026;&#20154;&#31867;&#26377;&#24184;&#33021;&#22815;&#21033;&#29992;&#25105;&#20204;&#30340;&#26234;&#24935;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#29289;&#29702;&#21457;&#29983;&#20043;&#21069;&#22312;&#22836;&#33041;&#20013;&#25191;&#34892;&#26576;&#39033;&#27963;&#21160;&#12290;&#36807;&#21435;&#30340;&#30693;&#35782;&#12289;&#29616;&#22312;&#30340;&#20919;&#38745;&#21644;&#26410;&#26469;&#30340;&#23637;&#26395;&#23545;&#24212;&#20110;&#29616;&#23454;&#29983;&#27963;&#20197;&#21450;&#35745;&#31639;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#19977;&#20010;&#27010;&#24565;&#65292;&#20998;&#21035;&#26159;&#22238;&#39038;&#12289;&#35266;&#23519;&#21644;&#21069;&#30651;&#12290;&#21069;&#30651;&#65288;LA&#65289;&#22788;&#29702;&#20449;&#24687;&#30340;&#26410;&#26469;&#39044;&#27979;&#21644;&#22788;&#29702;&#36755;&#20837;&#20197;&#25552;&#21069;&#29983;&#25104;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#12289;&#29702;&#35299;&#21644;&#25506;&#32034;LA&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#26032;&#39062;&#30340;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#22522;&#20110;&#36755;&#20837;&#20449;&#24687;&#21487;&#29992;&#24615;&#30340;&#30693;&#21517;&#31639;&#27861;&#26694;&#26550;&#65292;&#20363;&#22914;&#33073;&#26426;&#12289;&#22312;&#32447;&#21644;&#21322;&#22312;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17942v1 Announce Type: cross  Abstract: Past, Present and Future are considered to be three temporal and logical concepts which are well defined by human beings for their existence and growth. We, as human beings, have the privilege of using our intelligence to mentally execute an activity before physical occurrence of the same in the real world. Knowledge of the past, aplomb of present and visualisation for the future correspond to three concepts such as look-back, look-at and look-ahead respectively in real life as well as in diversified domains of computing. Look-Ahead(LA) deals with the future prediction of information and processing of input to produce the output in advance. In this article, our main objective is to learn, understand and explore the concept of LA and design novel models as solution for real world problems. We present three well known algorithmic frameworks used in practice based on availability of input information such as offline, online and semi-onlin
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#23384;&#22312;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#30456;&#20851;&#30340;&#20559;&#35265;&#65292;&#21435;&#38500;&#36825;&#20123;&#20559;&#35265;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;JPEG&#21387;&#32553;&#30340;&#31283;&#20581;&#24615;&#24182;&#26174;&#33879;&#25913;&#21464;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17608</link><description>&lt;p&gt;
&#20266;&#36896;&#36824;&#26159;JPEG&#65311;&#25581;&#31034;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#24120;&#35265;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17608
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#23384;&#22312;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#30456;&#20851;&#30340;&#20559;&#35265;&#65292;&#21435;&#38500;&#36825;&#20123;&#20559;&#35265;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;JPEG&#21387;&#32553;&#30340;&#31283;&#20581;&#24615;&#24182;&#26174;&#33879;&#25913;&#21464;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#20984;&#26174;&#20102;&#26816;&#27979;&#20154;&#36896;&#20869;&#23481;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#36825;&#26159;&#25171;&#20987;&#24191;&#27867;&#25805;&#32437;&#21644;&#35823;&#23548;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26816;&#27979;&#22120;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#25968;&#25454;&#38598;&#19981;&#32463;&#24847;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#20559;&#35265;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26816;&#27979;&#22120;&#30340;&#25928;&#26524;&#21644;&#35780;&#20272;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#35768;&#22810;&#29992;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#26377;&#20851;&#30340;&#20559;&#35265;&#12290;&#20351;&#29992;GenImage&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#26816;&#27979;&#22120;&#30830;&#23454;&#20174;&#36825;&#20123;&#19981;&#21463;&#27426;&#36814;&#30340;&#22240;&#32032;&#20013;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#21435;&#38500;&#36825;&#20123;&#21629;&#21517;&#20559;&#35265;&#20250;&#26174;&#33879;&#22686;&#21152;&#38024;&#23545;JPEG&#21387;&#32553;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#26174;&#33879;&#25913;&#21464;&#35780;&#20272;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;ResNet50&#21644;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17608v1 Announce Type: cross  Abstract: The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and S
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17456</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Cost-Constrained Behaviors in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#22797;&#26434;&#30340;&#35745;&#21010;&#21644;&#35843;&#24230;&#38382;&#39064;&#19968;&#30452;&#36890;&#36807;&#21508;&#31181;&#20248;&#21270;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#30340;&#27169;&#20223;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#27169;&#22411;&#25110;&#30452;&#25509;&#34892;&#20026;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26080;&#38480;&#21046;&#35774;&#32622;&#19979;&#30340;&#27169;&#20223;&#65288;&#20363;&#22914;&#65292;&#36710;&#36742;&#28040;&#32791;&#30340;&#29123;&#27833;&#37327;&#27809;&#26377;&#38480;&#21046;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#19987;&#23478;&#30340;&#34892;&#20026;&#19981;&#20165;&#21463;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#30340;&#24433;&#21709;&#65292;&#36824;&#21463;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#33258;&#21160;&#39550;&#39542;&#36865;&#36135;&#36710;&#30340;&#20915;&#31574;&#19981;&#20165;&#21462;&#20915;&#20110;&#36335;&#24452;&#20559;&#22909;/&#22870;&#21169;&#65288;&#26681;&#25454;&#36807;&#21435;&#30340;&#38656;&#27714;&#25968;&#25454;&#65289;&#65292;&#36824;&#21462;&#20915;&#20110;&#36710;&#36742;&#20869;&#30340;&#29123;&#27833;&#21644;&#36865;&#36798;&#26102;&#38388;&#31561;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;MA4DIV&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#65292;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20197;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17421</link><description>&lt;p&gt;
MA4DIV&#65306;&#29992;&#20110;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17421
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;MA4DIV&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#65292;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20197;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#65288;SRD&#65289;&#30340;&#30446;&#26631;&#26159;&#30830;&#20445;&#25152;&#36873;&#25991;&#26723;&#28085;&#30422;&#23613;&#21487;&#33021;&#22810;&#30340;&#19981;&#21516;&#23376;&#20027;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#8220;&#36138;&#23146;&#36873;&#25321;&#8221;&#33539;&#24335;&#65292;&#21363;&#19968;&#27425;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26368;&#39640;&#22810;&#26679;&#24615;&#20998;&#25968;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#25928;&#29575;&#20302;&#19979;&#65292;&#23481;&#26131;&#38519;&#20837;&#27425;&#20248;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#20854;&#20182;&#26041;&#27861;&#26088;&#22312;&#36817;&#20284;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20294;&#32467;&#26524;&#20173;&#28982;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;MA4DIV&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#25991;&#26723;&#37117;&#26159;&#19968;&#20010;&#26234;&#33021;&#20307;&#65292;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#34987;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17421v1 Announce Type: cross  Abstract: The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of "greedy selection", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experi
&lt;/p&gt;</description></item><item><title>SeSaMe&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#27169;&#25311;&#33258;&#25105;&#25253;&#21578;&#65292;&#20943;&#36731;&#20102;&#20182;&#20204;&#30340;&#36127;&#25285;</title><link>https://arxiv.org/abs/2403.17219</link><description>&lt;p&gt;
SeSaMe&#65306;&#27169;&#25311;&#33258;&#25105;&#25253;&#21578;&#30340;&#22320;&#38754;&#30495;&#23454;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#24863;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17219
&lt;/p&gt;
&lt;p&gt;
SeSaMe&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#27169;&#25311;&#33258;&#25105;&#25253;&#21578;&#65292;&#20943;&#36731;&#20102;&#20182;&#20204;&#30340;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#24471;&#21487;&#20197;&#34987;&#21160;&#30417;&#27979;&#19968;&#20010;&#20154;&#30340;&#24515;&#29702;&#12289;&#34892;&#20026;&#21644;&#24773;&#24863;&#20581;&#24247;&#30340;&#28508;&#21147;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#33258;&#25105;&#25253;&#21578;&#32467;&#26524;&#30340;&#38271;&#26399;&#25910;&#38598;&#65292;&#20363;&#22914;&#25233;&#37057;&#12289;&#21387;&#21147;&#21644;&#28966;&#34385;&#65292;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25345;&#32493;&#33258;&#25105;&#25253;&#21578;&#20250;&#32473;&#21442;&#19982;&#32773;&#24102;&#26469;&#37325;&#22823;&#36127;&#25285;&#65292;&#32463;&#24120;&#23548;&#33268;&#27969;&#22833;&#12289;&#32570;&#22833;&#26631;&#31614;&#25110;&#19981;&#30495;&#35802;&#30340;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;&#24515;&#29702;&#27169;&#22411;&#27169;&#25311;&#35268;&#27169;&#20998;&#25968;&#30340;SeSaMe&#26694;&#26550;&#65292;&#20197;&#20943;&#36731;&#25968;&#23383;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#20013;&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;SeSaMe&#33021;&#22815;&#27169;&#25311;&#21442;&#19982;&#32773;&#22312;&#24515;&#29702;&#37327;&#34920;&#19978;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17219v1 Announce Type: cross  Abstract: Advances in mobile and wearable technologies have enabled the potential to passively monitor a person's mental, behavioral, and affective health. These approaches typically rely on longitudinal collection of self-reported outcomes, e.g., depression, stress, and anxiety, to train machine learning (ML) models. However, the need to continuously self-report adds a significant burden on the participants, often resulting in attrition, missing labels, or insincere responses. In this work, we introduce the Scale Scores Simulation using Mental Models (SeSaMe) framework to alleviate participants' burden in digital mental health studies. By leveraging pre-trained large language models (LLMs), SeSaMe enables the simulation of participants' responses on psychological scales. In SeSaMe, researchers can prompt LLMs with information on participants' internal behavioral dispositions, enabling LLMs to construct mental models of participants to simulate 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16512</link><description>&lt;p&gt;
LLMs&#26159;&#23569;&#26679;&#26412;&#24773;&#22659;&#20302;&#36164;&#28304;&#35821;&#35328;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs Are Few-Shot In-Context Low-Resource Language Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#30701;&#26102;&#30340;&#24773;&#22659;&#20449;&#24687;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#36825;&#20026;&#32553;&#23567;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#25552;&#20379;&#20102;&#37325;&#35201;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#38598;&#20013;&#22312;&#30456;&#23545;&#39640;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#27604;&#22914;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;ICL&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#65288;X-ICL&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#35780;&#20272;&#20102;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#21457;&#29616;&#20102;&#24773;&#22659;&#26631;&#31614;&#23545;&#40784;&#30340;&#32570;&#38519;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24635;&#32467;&#20102;&#23569;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16512v1 Announce Type: cross  Abstract: In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-cont
&lt;/p&gt;</description></item><item><title>DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2403.16451</link><description>&lt;p&gt;
DeepMachining: &#38115;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#22312;&#32447;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeepMachining: Online Prediction of Machining Errors of Lathe Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16451
&lt;/p&gt;
&lt;p&gt;
DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;DeepMachining&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#21152;&#24037;&#35823;&#24046;&#12290;&#25105;&#20204;&#22522;&#20110;&#24037;&#21378;&#30340;&#21046;&#36896;&#25968;&#25454;&#26500;&#24314;&#24182;&#35780;&#20272;&#20102;DeepMachining&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29305;&#23450;&#36710;&#24202;&#26426;&#24202;&#25805;&#20316;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21152;&#24037;&#29366;&#24577;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#29305;&#23450;&#21152;&#24037;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepMachining&#22312;&#28041;&#21450;&#19981;&#21516;&#24037;&#20214;&#21644;&#20992;&#20855;&#30340;&#22810;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#39318;&#25209;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#35813;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2403.16432</link><description>&lt;p&gt;
$\textit{LinkPrompt}$: &#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#21644;&#36890;&#29992;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16432
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#35813;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-based learning &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#22522;&#20934;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#25552;&#31034;&#27169;&#26495;&#26469;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#31181;&#22522;&#20110;&#25552;&#31034;&#20248;&#21270;&#36807;&#31243;&#23545;PLMs&#30340;&#23398;&#20064;&#20063;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#29983;&#25104;&#36890;&#29992;&#23545;&#25239;&#35302;&#21457;&#22120;&#65288;UATs&#65289;&#26469;&#25913;&#21464;&#19981;&#20165;&#30446;&#26631;PLMs&#30340;&#39044;&#27979;&#65292;&#36824;&#26377;&#23545;&#24212;Prompt-based Fine-tuning Models&#65288;PFMs&#65289;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#20316;&#21697;&#20013;&#21457;&#29616;&#30340;UATs&#36890;&#24120;&#26159;&#26080;&#27861;&#38405;&#35835;&#30340;&#20196;&#29260;&#25110;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16432v1 Announce Type: cross  Abstract: Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters a
&lt;/p&gt;</description></item><item><title>Re2LLM&#26159;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25552;&#20986;&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.16427</link><description>&lt;p&gt;
Re2LLM: &#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16427
&lt;/p&gt;
&lt;p&gt;
Re2LLM&#26159;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25552;&#20986;&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#26085;&#30410;&#34987;&#30475;&#20316;&#26159;&#22686;&#24378;&#22522;&#20110;&#20250;&#35805;&#25512;&#33616;(SBR)&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24050;&#24191;&#27867;&#30740;&#31350;&#20102;&#22522;&#20110;&#25552;&#31034;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;LLMs&#19982;SBR&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#22240;&#32570;&#20047;&#20219;&#21153;&#29305;&#23450;&#21453;&#39304;&#32780;&#38590;&#20197;&#25214;&#21040;&#24341;&#23548;LLMs&#27491;&#30830;&#25512;&#29702;&#30340;&#26368;&#20339;&#25552;&#31034;&#65292;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#19981;&#20339;&#12290;&#23613;&#31649;&#21518;&#32773;&#35797;&#22270;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#24494;&#35843;LLMs&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#35832;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20381;&#36182;&#24320;&#28304;&#39592;&#24178;&#30340;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;SBR&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Re2LLM)&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#21453;&#23556;&#24335;&#25506;&#32034;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16427v1 Announce Type: new  Abstract: Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.   However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.   Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones.   To address such issues, we propose a \underline{Re}flective \underline{Re}inforcement \underline{L}arge \underline{L}anguage \underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.   In particular, we first design the Reflective Exploration Module to effective
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;ChatGPT&#25972;&#21512;&#21040;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;ChatGPT&#30340;&#31215;&#26497;&#24577;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#20307;&#39564;&#20013;&#20316;&#29992;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.15472</link><description>&lt;p&gt;
&#29992;ChatGPT&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#65306;&#38024;&#23545;Python&#35838;&#31243;&#20013;&#23398;&#29983;&#24863;&#30693;&#21644;&#20114;&#21160;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Programming Education with ChatGPT: A Case Study on Student Perceptions and Interactions in a Python Course
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;ChatGPT&#25972;&#21512;&#21040;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;ChatGPT&#30340;&#31215;&#26497;&#24577;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#20307;&#39564;&#20013;&#20316;&#29992;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#25903;&#25345;&#24615;&#24037;&#20855;&#25972;&#21512;&#21040;&#25945;&#32946;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#32534;&#31243;&#35838;&#31243;&#20013;&#65292;&#36890;&#36807;&#25552;&#20379;&#35843;&#35797;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#35299;&#37322;&#26041;&#38754;&#30340;&#24110;&#21161;&#65292;&#35299;&#20915;&#20102;&#32534;&#31243;&#25945;&#32946;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#39564;&#35777;&#20102;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20854;&#22312;&#22823;&#23398;&#32423;&#21035;&#30340;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#23398;&#29983;&#20114;&#21160;&#21644;&#35266;&#28857;&#30340;&#35814;&#32454;&#29702;&#35299;&#20173;&#26377;&#38480;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20026;&#22823;&#19968;&#23398;&#29983;&#37327;&#36523;&#23450;&#21046;&#30340;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#65292;&#22312;&#20843;&#21608;&#30340;&#26102;&#38388;&#20869;&#65292;ChatGPT&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#26469;&#33258;&#35843;&#26597;&#12289;&#24320;&#25918;&#24615;&#38382;&#39064;&#20197;&#21450;&#23398;&#29983;-ChatGPT&#23545;&#35805;&#25968;&#25454;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;ChatGPT&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#30830;&#23450;&#23398;&#29983;&#25152;&#24863;&#30693;&#30340;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;ChatGPT&#26222;&#36941;&#25345;&#32943;&#23450;&#24577;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#20854;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#20307;&#39564;&#20013;&#30340;&#20316;&#29992;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15472v1 Announce Type: cross  Abstract: The integration of ChatGPT as a supportive tool in education, notably in programming courses, addresses the unique challenges of programming education by providing assistance with debugging, code generation, and explanations. Despite existing research validating ChatGPT's effectiveness, its application in university-level programming education and a detailed understanding of student interactions and perspectives remain limited. This paper explores ChatGPT's impact on learning in a Python programming course tailored for first-year students over eight weeks. By analyzing responses from surveys, open-ended questions, and student-ChatGPT dialog data, we aim to provide a comprehensive view of ChatGPT's utility and identify both its advantages and limitations as perceived by students. Our study uncovers a generally positive reception toward ChatGPT and offers insights into its role in enhancing the programming education experience. These fin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27714;&#35299;&#22120; Q4RPD&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#21253;&#35065;&#25237;&#36882;&#36335;&#24452;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#38382;&#39064;&#31616;&#21270;&#21644;&#25216;&#26415;&#25463;&#24452;&#65292;&#38024;&#23545;&#21253;&#35065;&#37325;&#37327;&#21644;&#23610;&#23544;&#30340;&#30495;&#23454;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.15114</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#21253;&#35065;&#25237;&#36882;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving a Real-World Package Delivery Routing Problem Using Quantum Annealers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27714;&#35299;&#22120; Q4RPD&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#21253;&#35065;&#25237;&#36882;&#36335;&#24452;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#38382;&#39064;&#31616;&#21270;&#21644;&#25216;&#26415;&#25463;&#24452;&#65292;&#38024;&#23545;&#21253;&#35065;&#37325;&#37327;&#21644;&#23610;&#23544;&#30340;&#30495;&#23454;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#20851;&#20110;&#37327;&#23376;&#35745;&#31639;&#21644;&#36335;&#24452;&#38382;&#39064;&#20043;&#38388;&#30340;&#30740;&#31350;&#38750;&#24120;&#22810;&#20135;&#12290;&#22823;&#37096;&#20998;&#20316;&#21697;&#22260;&#32469;&#30528;&#32463;&#20856;&#38382;&#39064;&#65292;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#25110;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#23613;&#31649;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#19981;&#21487;&#21542;&#35748;&#30340;&#26159;&#23427;&#20204;&#20197;&#23398;&#26415;&#20026;&#23548;&#21521;&#30340;&#29305;&#28857;&#26080;&#27861;&#28385;&#36275;&#29616;&#23454;&#19990;&#30028;&#30340;&#35201;&#27714;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#38382;&#39064;&#31616;&#21270;&#25110;&#25216;&#26415;&#25463;&#24452;&#12290;&#30456;&#21453;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27714;&#35299;&#22120;&#65292;&#21629;&#21517;&#20026;Q4RPD&#65292;&#32771;&#34385;&#21040;&#19968;&#31995;&#21015;&#30495;&#23454;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#24322;&#26500;&#36710;&#38431;&#12289;&#20248;&#20808;&#25237;&#36882;&#21644;&#26681;&#25454;&#21253;&#35065;&#37325;&#37327;&#21644;&#23610;&#23544;&#30830;&#23450;&#30340;&#23481;&#37327;&#12290;Q4RPD&#37319;&#29992;&#20102;D-Wave&#30340;Leap&#21463;&#38480;&#20108;&#27425;&#27169;&#22411;&#28151;&#21512;&#27714;&#35299;&#22120;&#12290;&#20026;&#20102;&#35777;&#26126;Q4RPD&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#39564;&#32452;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15114v1 Announce Type: cross  Abstract: Research focused on the conjunction between quantum computing and routing problems has been very prolific in recent years. Most of the works revolve around classical problems such as the Traveling Salesman Problem or the Vehicle Routing Problem. Even though working on these problems is valuable, it is also undeniable that their academic-oriented nature falls short of real-world requirements. The main objective of this research is to present a solving method for realistic instances, avoiding problem relaxations or technical shortcuts. Instead, a quantum-classical hybrid solver has been developed, coined Q4RPD, that considers a set of real constraints such as a heterogeneous fleet of vehicles, priority deliveries, and capacities characterized by two values: weight and dimensions of the packages. Q4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the application of Q4RPD, an experimentation compo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14864</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#20223;&#30495;&#23398;&#20064;&#22235;&#36275;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Quadruped Locomotion Using Differentiable Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#37096;&#20998;&#26426;&#22120;&#20154;&#36816;&#21160;&#25511;&#21046;&#30340;&#36827;&#23637;&#37117;&#26159;&#30001;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#24494;&#20998;&#20223;&#30495;&#30340;&#28508;&#21147;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#35745;&#31639;&#20302;&#21464;&#24322;&#19968;&#38454;&#26799;&#24230;&#65292;&#25215;&#35834;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20854;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#30001;&#20110;&#25509;&#35302;&#20016;&#23500;&#29615;&#22659;&#65288;&#22914;&#22235;&#36275;&#21160;&#20316;&#65289;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22797;&#26434;&#20248;&#21270;&#26223;&#35266;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#20851;&#38190;&#24819;&#27861;&#21253;&#25324;&#23558;&#21487;&#33021;&#30001;&#20110;&#25509;&#35302;&#32780;&#20986;&#29616;&#19981;&#36830;&#32493;&#24615;&#30340;&#22797;&#26434;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#31616;&#21270;&#27169;&#22411;&#20135;&#29983;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#19982;&#26356;&#31934;&#30830;&#30340;&#19981;&#21487;&#24494;&#20998;&#27169;&#22411;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14864v1 Announce Type: cross  Abstract: While most recent advancements in legged robot control have been driven by model-free reinforcement learning, we explore the potential of differentiable simulation. Differentiable simulation promises faster convergence and more stable training by computing low-variant first-order gradients using the robot model, but so far, its use for legged robot control has remained limited to simulation. The main challenge with differentiable simulation lies in the complex optimization landscape of robotic tasks due to discontinuities in contact-rich environments, e.g., quadruped locomotion. This work proposes a new, differentiable simulation framework to overcome these challenges. The key idea involves decoupling the complex whole-body simulation, which may exhibit discontinuities due to contact, into two separate continuous domains. Subsequently, we align the robot state resulting from the simplified model with a more precise, non-differentiable 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14814</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The opportunities and risks of large language models in mental health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14814
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21457;&#29983;&#29575;&#27491;&#22312;&#19978;&#21319;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#29616;&#26377;&#30340;&#24515;&#29702;&#20445;&#20581;&#27169;&#24335;&#26080;&#27861;&#20805;&#20998;&#25193;&#23637;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#20855;&#26377;&#21019;&#36896;&#26032;&#39062;&#12289;&#22823;&#35268;&#27169;&#35299;&#20915;&#26041;&#26696;&#20197;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#30340;&#25215;&#35834;&#24863;&#21040;&#20048;&#35266;&#12290;&#23613;&#31649;&#23427;&#20204;&#36824;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;LLMs&#24050;&#34987;&#24212;&#29992;&#20110;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#21033;&#29992;LLMs&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25945;&#32946;&#12289;&#35780;&#20272;&#21644;&#24178;&#39044;&#30340;&#21162;&#21147;&#65292;&#24182;&#31361;&#20986;&#20102;&#27599;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#30340;&#20851;&#38190;&#26426;&#20250;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#25152;&#20276;&#38543;&#30340;&#39118;&#38505;&#65292;&#24182;&#40723;&#21169;&#37319;&#29992;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#36843;&#20999;&#38656;&#27714;&#24517;&#39035;&#19982;&#36127;&#36131;&#20219;&#30340;&#24515;&#29702;&#20581;&#24247;LLMs&#30340;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#30456;&#24179;&#34913;&#12290;&#29305;&#21035;&#20851;&#38190;&#30340;&#26159;&#30830;&#20445;&#24515;&#29702;&#20581;&#24247;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13374</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#33258;&#36866;&#24212;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13374
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22788;&#29702;&#20102;&#22312;&#23384;&#22312;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#24179;&#22343;&#26799;&#24230;&#31639;&#27861;&#65288;RAGA&#65289;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20960;&#20309;&#20013;&#20301;&#25968;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#26412;&#22320;&#26356;&#26032;&#30340;&#36718;&#25968;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24377;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#25110;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25910;&#25947;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#24378;&#20984;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#22312;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21482;&#35201;&#24694;&#24847;&#29992;&#25143;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23567;&#20110;&#19968;&#21322;&#65292;RAGA&#23601;&#21487;&#20197;&#20197;$\mathcal{O}({1}/{T^{2/3- \delta}})$&#30340;&#36895;&#24230;&#23454;&#29616;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#65292;&#20854;&#20013;$T$&#20026;&#36845;&#20195;&#27425;&#25968;&#65292;$\delta \in (0, 2/3)$&#65292;&#23545;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#21017;&#21576;&#32447;&#24615;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#31283;&#23450;&#28857;&#25110;&#20840;&#23616;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#36827;&#34892;&#25968;&#25454;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.10158</link><description>&lt;p&gt;
&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#20419;&#36827;&#20581;&#24247;&#21644;&#31038;&#20250;&#20851;&#24576;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#36827;&#34892;&#25968;&#25454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;funGCN&#65289;&#26694;&#26550;&#65292;&#23558;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12290;&#38543;&#30528;&#20581;&#24247;&#35299;&#20915;&#26041;&#26696;&#23545;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#21644;&#31038;&#20250;&#25903;&#25345;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#38271;&#65292;&#30830;&#20445;&#21508;&#24180;&#40836;&#27573;&#30340;&#20581;&#24247;&#29983;&#27963;&#21644;&#20419;&#36827;&#24184;&#31119;&#24863;&#65292;funGCN&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22810;&#20010;&#23454;&#20307;&#30340;&#22810;&#20803;&#32437;&#21521;&#25968;&#25454;&#65292;&#24182;&#30830;&#20445;&#21363;&#20351;&#22312;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#20063;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#31649;&#29702;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#20197;&#33719;&#21462;&#27934;&#23519;&#24615;&#25968;&#25454;&#35299;&#37322;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#39564;&#35777;&#20102;funGCN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10158v1 Announce Type: cross  Abstract: This paper introduces a novel Functional Graph Convolutional Network (funGCN) framework that combines Functional Data Analysis and Graph Convolutional Networks to address the complexities of multi-task and multi-modal learning in digital health and longitudinal studies. With the growing importance of health solutions to improve health care and social support, ensure healthy lives, and promote well-being at all ages, funGCN offers a unified approach to handle multivariate longitudinal data for multiple entities and ensures interpretability even with small sample sizes. Key innovations include task-specific embedding components that manage different data types, the ability to perform classification, regression, and forecasting, and the creation of a knowledge graph for insightful data interpretation. The efficacy of funGCN is validated through simulation experiments and a real-data application.
&lt;/p&gt;</description></item><item><title>Sabi'a-2&#26159;&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;Sabi'a-2 Medium&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;GPT-4&#65292;&#19988;&#22312;&#22823;&#22810;&#25968;&#32771;&#35797;&#20013;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#21516;&#26102;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#22312;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#30340;&#24773;&#20917;&#19979;&#20197;&#27604;GPT-4&#20415;&#23452;10&#20493;&#30340;&#20215;&#26684;&#25552;&#20379;&#12290;</title><link>https://arxiv.org/abs/2403.09887</link><description>&lt;p&gt;
Sabi\'a-2:&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sabi\'a-2: A New Generation of Portuguese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09887
&lt;/p&gt;
&lt;p&gt;
Sabi'a-2&#26159;&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;Sabi'a-2 Medium&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;GPT-4&#65292;&#19988;&#22312;&#22823;&#22810;&#25968;&#32771;&#35797;&#20013;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#21516;&#26102;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#22312;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#30340;&#24773;&#20917;&#19979;&#20197;&#27604;GPT-4&#20415;&#23452;10&#20493;&#30340;&#20215;&#26684;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Sabi'a-2&#65292;&#36825;&#26159;&#19968;&#26063;&#22312;&#33889;&#33796;&#29273;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#24052;&#35199;&#22823;&#23398;&#30340;&#20837;&#23398;&#32771;&#35797;&#12289;&#19987;&#19994;&#35748;&#35777;&#32771;&#35797;&#20197;&#21450;&#21508;&#31181;&#23398;&#31185;&#65288;&#22914;&#20250;&#35745;&#12289;&#32463;&#27982;&#23398;&#12289;&#24037;&#31243;&#23398;&#12289;&#27861;&#24459;&#21644;&#21307;&#23398;&#65289;&#30340;&#30740;&#31350;&#29983;&#20837;&#23398;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#25105;&#20204;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;Sabi'a-2 Medium&#65292;&#22312;64&#22330;&#32771;&#35797;&#20013;&#26377;23&#22330;&#19982;GPT-4&#30340;&#34920;&#29616;&#30456;&#21305;&#25932;&#25110;&#36229;&#36234;&#65292;&#24182;&#19988;&#22312;64&#22330;&#32771;&#35797;&#20013;&#26377;58&#22330;&#36229;&#36807;&#20102;GPT-3.5&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;Sabi'a-2 Medium&#65292;&#27599;&#20010;&#35760;&#21495;&#30340;&#20215;&#26684;&#27604;GPT-4&#20415;&#23452;10&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#23398;&#21644;&#32534;&#30721;&#26159;&#38656;&#35201;&#25913;&#36827;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09887v1 Announce Type: cross  Abstract: We introduce Sabi\'a-2, a family of large language models trained on Portuguese texts. The models are evaluated on a diverse range of exams, including entry-level tests for Brazilian universities, professional certification exams, and graduate-level exams for various disciplines such as accounting, economics, engineering, law and medicine. Our results reveal that our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64 exams. Notably, specialization has a significant impact on a model's performance without the need to increase its size, allowing us to offer Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4. Finally, we identified that math and coding are key abilities that need improvement.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Shapley Values&#37327;&#21270;&#33402;&#26415;&#23478;&#23545;&#29983;&#25104;&#27169;&#22411;&#25152;&#20570;&#36129;&#29486;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#24320;&#21457;&#32773;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#21512;&#20316;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2403.09700</link><description>&lt;p&gt;
Shapley Values &#39537;&#21160;&#30340;&#29992;&#20110;GenAI&#29983;&#25104;&#20869;&#23481;&#30340;&#20844;&#24179;&#22870;&#21169;&#20998;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Shapley Values-Powered Framework for Fair Reward Split in Content Produced by GenAI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09700
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Shapley Values&#37327;&#21270;&#33402;&#26415;&#23478;&#23545;&#29983;&#25104;&#27169;&#22411;&#25152;&#20570;&#36129;&#29486;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#24320;&#21457;&#32773;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#21512;&#20316;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26126;&#26174;&#30340;&#26159;&#65292;&#24403;&#21069;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#34987;&#20154;&#31867;&#19987;&#19994;&#20154;&#22763;&#36229;&#36234;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#36825;&#31181;&#24046;&#36317;&#23558;&#20250;&#32553;&#23567;&#65292;&#23548;&#33268;&#37027;&#20123;&#25237;&#20837;&#20102;&#22810;&#24180;&#26102;&#38388;&#26469;&#25484;&#25569;&#19968;&#39033;&#25216;&#33021;&#30340;&#20010;&#20307;&#22240;&#20854;&#39640;&#26114;&#25104;&#26412;&#32780;&#21464;&#24471;&#36807;&#26102;&#65292;&#36825;&#31181;&#25104;&#26412;&#19982;&#20182;&#20204;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#32039;&#23494;&#30456;&#36830; -- &#19968;&#39033;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#25110;&#20960;&#31186;&#38047;&#20869;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#36991;&#20813;&#26410;&#26469;&#30340;&#31038;&#20250;&#21160;&#33633;&#65292;&#25105;&#20204;&#24517;&#39035;&#21363;&#21051;&#24605;&#32771;&#22914;&#20309;&#20844;&#24179;&#35780;&#20272;&#36825;&#20123;&#20010;&#20307;&#22312;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#36129;&#29486;&#65292;&#24182;&#22914;&#20309;&#34917;&#20607;&#20182;&#20204;&#30001;&#27492;&#32780;&#23548;&#33268;&#30340;&#25910;&#20837;&#20943;&#23569;&#25110;&#23436;&#20840;&#20007;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#26500;&#24314;&#27169;&#22411;&#24320;&#21457;&#32773;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;Shapley Values&#26469;&#37327;&#21270;Stable Diffusion-v1.5&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#33402;&#26415;&#23478;&#30340;&#36129;&#29486;&#65292;&#24182;&#20844;&#24179;&#22320;&#23545;&#24453;&#20182;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09700v1 Announce Type: cross  Abstract: It is evident that, currently, generative models are surpassed in quality by human professionals. However, with the advancements in Artificial Intelligence, this gap will narrow, leading to scenarios where individuals who have dedicated years of their lives to mastering a skill become obsolete due to their high costs, which are inherently linked to the time they require to complete a task -- a task that AI could accomplish in minutes or seconds. To avoid future social upheavals, we must, even now, contemplate how to fairly assess the contributions of such individuals in training generative models and how to compensate them for the reduction or complete loss of their incomes. In this work, we propose a method to structure collaboration between model developers and data providers. To achieve this, we employ Shapley Values to quantify the contribution of artist(s) in an image generated by the Stable Diffusion-v1.5 model and to equitably a
&lt;/p&gt;</description></item><item><title>ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09131</link><description>&lt;p&gt;
ProSwitch&#65306;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09131
&lt;/p&gt;
&lt;p&gt;
ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25991;&#26412;&#25688;&#35201;&#21644;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#22312;&#19981;&#21516;&#39118;&#26684;&#38388;&#20999;&#25442;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#25991;&#26412;&#19987;&#19994;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ProSwitch&#65292;&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;ProSwitch&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#25968;&#25454;&#20934;&#22791;&#65292;&#29992;&#20110;&#25910;&#38598;&#39046;&#22495;&#30693;&#35782;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#65307;&#25351;&#20196;&#24494;&#35843;&#65292;&#29992;&#20110;&#20248;&#21270;&#24102;&#26377;&#22810;&#31181;&#25351;&#20196;&#26684;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20840;&#38754;&#35780;&#20272;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#19987;&#19994;&#24615;&#21306;&#20998;&#33021;&#21147;&#21644;&#22522;&#20110;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290; ProSwitch&#30456;&#23545;&#20110;&#36890;&#29992;&#21644;&#19987;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#23618;&#20869;&#23384;&#28040;&#32791;&#22686;&#38271;&#24555;&#12289;&#38480;&#21046;&#36739;&#22823;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07711</link><description>&lt;p&gt;
SSM&#36935;&#19978;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;: &#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#19979;&#30340;&#39640;&#25928;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07711
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#23618;&#20869;&#23384;&#28040;&#32791;&#22686;&#38271;&#24555;&#12289;&#38480;&#21046;&#36739;&#22823;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22270;&#20687;&#29983;&#25104;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#30740;&#31350;&#30028;&#23545;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#35270;&#39057;&#29983;&#25104;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#26469;&#25552;&#21462;&#26102;&#38388;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#23618;&#30340;&#20869;&#23384;&#28040;&#32791;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#36825;&#31181;&#38480;&#21046;&#22312;&#23581;&#35797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26356;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#20250;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#30001;&#20110;&#30456;&#23545;&#20110;&#24207;&#21015;&#38271;&#24230;&#65292;SSMs&#20855;&#26377;&#32447;&#24615;&#20869;&#23384;&#28040;&#32791;&#65292;&#26368;&#36817;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;UCF101&#36825;&#19968;&#35270;&#39057;&#29983;&#25104;&#30340;&#26631;&#20934;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#22522;&#20110;SSM&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#25506;&#35752;SSMs&#22312;&#26356;&#38271;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07711v1 Announce Type: cross  Abstract: Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06054</link><description>&lt;p&gt;
&#20855;&#26377;&#25193;&#25955;&#20928;&#21270;&#30340;&#20998;&#31163;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Decoupled Data Consistency with Diffusion Purification for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#31867;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#22320;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#65292;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#20284;&#28982;&#26799;&#24230;&#27493;&#39588;&#32435;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#26799;&#24230;&#27493;&#39588;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#24403;&#20351;&#29992;&#21152;&#36895;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#22120;&#26102;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#27493;&#39588;&#36824;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#30340;&#25968;&#37327;&#21463;&#38480;&#20110;&#21453;&#21521;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#20998;&#31163;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#65288;LP&#65289;&#21644;LP&#37327;&#21270;&#65288;LPQ&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#22240;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;LP&#21442;&#25968;&#65292;&#35774;&#35745;&#20102;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#21160;&#24577;&#36866;&#24212;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.05465</link><description>&lt;p&gt;
&#20998;&#24067;&#24863;&#30693;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#30340;&#31639;&#27861;&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;DNN&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05465
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#65288;LP&#65289;&#21644;LP&#37327;&#21270;&#65288;LPQ&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#22240;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;LP&#21442;&#25968;&#65292;&#35774;&#35745;&#20102;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#21160;&#24577;&#36866;&#24212;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37327;&#21270;&#26041;&#27861;&#20351;&#29992;&#25972;&#25968;&#12289;&#23450;&#28857;&#25110;&#28014;&#28857;&#25968;&#25454;&#31867;&#22411;&#26102;&#65292;&#24448;&#24448;&#38590;&#20197;&#22312;&#20302;&#31934;&#24230;&#19979;&#25429;&#25417;&#19981;&#21516;&#30340;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30789;&#24320;&#38144;&#21644;&#23494;&#38598;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#65288;LP&#65289;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#27491;&#23450;&#21551;&#21457;&#30340;&#33258;&#36866;&#24212;&#12289;&#30828;&#20214;&#21451;&#22909;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;LP&#20301;&#22495;&#21160;&#24577;&#36866;&#24212;DNN&#26435;&#37325;/&#28608;&#27963;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;LP&#37327;&#21270;&#65288;LPQ&#65289;&#65292;&#29992;&#20110;&#23547;&#25214;&#26368;&#20248;&#30340;&#36880;&#23618;LP&#21442;&#25968;&#65292;&#21516;&#26102;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;-&#23616;&#37096;&#23545;&#27604;&#30446;&#26631;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21253;&#25324;&#23558;LP&#32435;&#20837;&#35745;&#31639;&#25968;&#25454;&#36890;&#36335;&#20013;&#30340;&#22788;&#29702;&#21333;&#20803;&#65288;PEs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05465v1 Announce Type: cross  Abstract: Traditional Deep Neural Network (DNN) quantization methods using integer, fixed-point, or floating-point data types struggle to capture diverse DNN parameter distributions at low precision, and often require large silicon overhead and intensive quantization-aware training. In this study, we introduce Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by posits that dynamically adapts to DNN weight/activation distributions by parameterizing LP bit fields. We also develop a novel genetic-algorithm based framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters while reducing representational divergence between quantized and full-precision models through a novel global-local contrastive objective. Additionally, we design a unified mixed-precision LP accelerator (LPA) architecture comprising of processing elements (PEs) incorporating LP in the computational datapath. Our algorithm-hardware co-design
&lt;/p&gt;</description></item><item><title>NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;</title><link>https://arxiv.org/abs/2403.03100</link><description>&lt;p&gt;
NaturalSpeech 3: &#21033;&#29992;&#20998;&#35299;&#32534;&#35299;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03100
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#22312;&#35821;&#38899;&#36136;&#37327;&#12289;&#30456;&#20284;&#24230;&#21644;&#38901;&#24459;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#37492;&#20110;&#35821;&#38899;&#22797;&#26434;&#22320;&#21253;&#21547;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#65289;&#65292;&#32473;&#29983;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#23558;&#35821;&#38899;&#22240;&#23376;&#20998;&#35299;&#20026;&#20195;&#34920;&#19981;&#21516;&#23646;&#24615;&#30340;&#21508;&#20010;&#23376;&#31354;&#38388;&#65292;&#24182;&#21333;&#29420;&#29983;&#25104;&#23427;&#20204;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NaturalSpeech 3&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#20998;&#35299;&#21521;&#37327;&#37327;&#21270;&#65288;FVQ&#65289;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#20998;&#35299;&#20026;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#30340;&#23376;&#31354;&#38388;&#65307;2) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#30456;&#24212;&#30340;&#25552;&#31034;&#29983;&#25104;&#27599;&#20010;&#23376;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#20998;&#35299;&#35774;&#35745;&#65292;NaturalSpeech 3&#33021;&#22815;ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
&lt;/p&gt;</description></item><item><title>SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.00868</link><description>&lt;p&gt;
SoftTiger: &#29992;&#20110;&#21307;&#30103;&#24037;&#20316;&#27969;&#30340;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SoftTiger: A Clinical Foundation Model for Healthcare Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00868
&lt;/p&gt;
&lt;p&gt;
SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#24182;&#20171;&#32461;&#20102;SoftTiger&#65292;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CLaM&#65289;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#20020;&#24202;&#31508;&#35760;&#30340;&#21465;&#36848;&#24615;&#21644;&#38750;&#32467;&#26500;&#21270;&#29305;&#24615;&#26159;&#21307;&#30103;&#26234;&#33021;&#21270;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#25353;&#29031;&#22269;&#38469;&#20114;&#25805;&#20316;&#24615;&#26631;&#20934;&#23558;&#20020;&#24202;&#31508;&#35760;&#32467;&#26500;&#21270;&#20026;&#20020;&#24202;&#25968;&#25454;&#65292;&#28041;&#21450;&#22269;&#38469;&#24739;&#32773;&#25688;&#35201;&#12289;&#20020;&#24202;&#21360;&#35937;&#21644;&#21307;&#30103;&#25509;&#35302;&#19977;&#20010;&#20851;&#38190;&#23376;&#20219;&#21153;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21644;&#39564;&#35777;&#30340;&#20020;&#24202;&#25968;&#25454;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30446;&#26631;&#27169;&#22411;&#39318;&#20808;&#33021;&#22815;&#25903;&#25345;&#22522;&#26412;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#32553;&#20889;&#25193;&#23637;&#21644;&#26102;&#38388;&#20449;&#24687;&#25552;&#21462;&#65292;&#28982;&#21518;&#23398;&#20064;&#25191;&#34892;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#21360;&#35937;&#21644;&#25509;&#35302;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21307;&#30103;&#27169;&#22411;&#20013;&#30340;&#19968;&#20123;&#24314;&#27169;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00868v1 Announce Type: cross  Abstract: We release and introduce SoftTiger, a clinical large language model (CLaM) designed as a foundation model for healthcare workflows. The narrative and unstructured nature of clinical notes is a major obstacle for healthcare intelligentization. We address a critical problem of structuring clinical notes into clinical data, according to international interoperability standards. We collect and annotate data for three critical subtasks, namely, international patient summary, clinical impression and medical encounter. We then supervised fine-tuned a state-of-the-art LLM using public and credentialed clinical data. The training is orchestrated in a way that the target model can first support basic clinical tasks such as abbreviation expansion and temporal information extraction, and then learn to perform more complex downstream clinical tasks such as impression and encounter summary. Moreover, we address, several modeling challenges in the he
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#20351;&#29992;Gemini&#36827;&#34892;&#25919;&#27835;&#31185;&#23398;&#22270;&#20687;&#20869;&#23481;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#24847;&#35782;&#65292;&#24182;&#23637;&#31034;Gemini&#22312;&#23545;&#35937;&#26816;&#27979;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00154</link><description>&lt;p&gt;
&#25919;&#27835;&#31185;&#23398;&#20013;&#30340;LLMs&#65306;&#24320;&#21551;&#35270;&#35273;&#20998;&#26512;&#26032;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
LLMs in Political Science: Heralding a New Era of Visual Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#20351;&#29992;Gemini&#36827;&#34892;&#25919;&#27835;&#31185;&#23398;&#22270;&#20687;&#20869;&#23481;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#24847;&#35782;&#65292;&#24182;&#23637;&#31034;Gemini&#22312;&#23545;&#35937;&#26816;&#27979;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#23398;&#23478;&#20013;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#21033;&#29992;&#22270;&#20687;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#35299;&#35835;&#36825;&#20123;&#22270;&#20687;&#30340;&#25361;&#25112;&#22312;&#20110;&#38656;&#35201;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#19987;&#38376;&#30828;&#20214;&#30340;&#35775;&#38382;&#12290;&#22240;&#27492;&#65292;&#22270;&#20687;&#20998;&#26512;&#19968;&#30452;&#23616;&#38480;&#20110;&#25919;&#27835;&#31185;&#23398;&#30028;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#20154;&#32676;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#65292;&#36825;&#31181;&#24773;&#20917;&#26377;&#21487;&#33021;&#21457;&#29983;&#21464;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#20351;&#29992;Gemini&#36827;&#34892;&#22270;&#20687;&#20869;&#23481;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#24847;&#35782;&#12290;&#23545;688&#24133;&#22270;&#20687;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#22238;&#39038;&#24615;&#20998;&#26512;&#12290;&#23545;&#27599;&#24133;&#22270;&#20687;&#20174;Gemini&#20013;&#33719;&#21462;&#20869;&#23481;&#25253;&#21578;&#65292;&#28982;&#21518;&#30001;&#20316;&#32773;&#25163;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;Gemini&#22312;&#25191;&#34892;&#23545;&#35937;&#26816;&#27979;&#26041;&#38754;&#38750;&#24120;&#20934;&#30830;&#65292;&#36825;&#22312;&#25919;&#27835;&#31185;&#23398;&#20013;&#26159;&#21487;&#33021;&#26368;&#24120;&#35265;&#21644;&#22522;&#26412;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;Gemini&#36827;&#34892;&#22270;&#20687;&#20869;&#23481;&#20998;&#26512;&#26159;&#31616;&#21333;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00154v1 Announce Type: cross  Abstract: Interest is increasing among political scientists in leveraging the extensive information available in images. However, the challenge of interpreting these images lies in the need for specialized knowledge in computer vision and access to specialized hardware. As a result, image analysis has been limited to a relatively small group within the political science community. This landscape could potentially change thanks to the rise of large language models (LLMs). This paper aims to raise awareness of the feasibility of using Gemini for image content analysis. A retrospective analysis was conducted on a corpus of 688 images. Content reports were elicited from Gemini for each image and then manually evaluated by the authors. We find that Gemini is highly accurate in performing object detection, which is arguably the most common and fundamental task in image analysis for political scientists. Equally important, we show that it is easy to im
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#30340;&#26144;&#23556;&#65292;&#20197;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#65292;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#20934;&#30830;&#12289;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.18920</link><description>&lt;p&gt;
&#20809;&#35889;&#36935;&#35265;&#31354;&#38388;: &#21644;&#35856;3D&#24418;&#29366;&#21305;&#37197;&#21644;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#30340;&#26144;&#23556;&#65292;&#20197;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#65292;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#20934;&#30830;&#12289;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;3D&#24418;&#29366;&#21305;&#37197;&#21644;&#25554;&#20540;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#34987;&#20998;&#24320;&#30740;&#31350;&#24182;&#20381;&#27425;&#24212;&#29992;&#20110;&#20851;&#32852;&#19981;&#21516;&#30340;3D&#24418;&#29366;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#21151;&#33021;&#26144;&#23556;&#26694;&#26550;&#19982;&#32463;&#20856;&#34920;&#38754;&#21464;&#24418;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#20013;&#26144;&#23556;&#24418;&#29366;&#12290;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#25972;&#21512;&#31354;&#38388;&#26144;&#23556;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20808;&#21069;&#29992;&#20110;&#24418;&#29366;&#21305;&#37197;&#30340;&#21151;&#33021;&#26144;&#23556;&#26041;&#27861;&#33719;&#24471;&#26356;&#31934;&#30830;&#21644;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#24341;&#20837;&#20809;&#35889;&#26144;&#23556;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25670;&#33073;&#20102;&#36890;&#24120;&#20351;&#29992;&#20294;&#35745;&#31639;&#26114;&#36149;&#30340;&#20165;&#23545;&#36817;&#31561;&#36317;&#24418;&#29366;&#21464;&#24418;&#26377;&#25928;&#30340;&#27979;&#22320;&#36317;&#31163;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18920v1 Announce Type: cross  Abstract: Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both 
&lt;/p&gt;</description></item><item><title>Agent-Pro&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#36880;&#27493;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.17574</link><description>&lt;p&gt;
Agent-Pro: &#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#21453;&#24605;&#21644;&#20248;&#21270;&#23398;&#20064;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17574
&lt;/p&gt;
&lt;p&gt;
Agent-Pro&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#36880;&#27493;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#37117;&#26159;&#29305;&#23450;&#20219;&#21153;&#27714;&#35299;&#22120;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#32780;&#19981;&#26159;&#33021;&#22815;&#36890;&#36807;&#20114;&#21160;&#23398;&#20064;&#21644;&#36827;&#21270;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Agent-Pro&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#20855;&#26377;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#20016;&#23500;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36880;&#28176;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#28041;&#21450;&#19968;&#20010;&#21160;&#24577;&#20449;&#24565;&#29983;&#25104;&#21644;&#21453;&#24605;&#36807;&#31243;&#65292;&#29992;&#20110;&#31574;&#30053;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17574v1 Announce Type: new  Abstract: Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.17304</link><description>&lt;p&gt;
&#25506;&#31350;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Probing Multimodal Large Language Models for Global and Local Semantic Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#23558;&#20854;&#20248;&#31168;&#30340;&#34920;&#31034;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#25551;&#36848;&#23545;&#40784;&#25968;&#25454;&#38598;&#35757;&#32451;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;MLLMs&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#23436;&#25972;&#30340;&#22270;&#20687;&#20449;&#24687;&#65292;&#21363;&#20840;&#23616;&#20449;&#24687;&#65292;&#25110;&#32773;&#23427;&#20204;&#21482;&#33021;&#25429;&#25417;&#19968;&#20123;&#23616;&#37096;&#23545;&#35937;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#21487;&#20197;&#32534;&#30721;&#26356;&#22810;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#20854;&#34920;&#31034;&#21521;&#37327;&#22312;&#35270;&#35273;-&#35821;&#35328;&#34164;&#28085;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#19981;&#26159;&#39030;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#36827;&#19968;&#27493;&#25506;&#31350;&#27169;&#22411;&#30340;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#19987;&#27880;&#20110;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#20943;&#24369;&#20102;&#23545;&#20840;&#23616;&#20449;&#24687;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17304v1 Announce Type: cross  Abstract: The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to en
&lt;/p&gt;</description></item><item><title>PEP&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#20043;&#21069;&#32454;&#21270;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#25552;&#21319;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.15764</link><description>&lt;p&gt;
&#22312;&#36339;&#27133;&#20043;&#21069;&#19977;&#24605;&#65306;&#38382;&#39064;&#32454;&#21270;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15764
&lt;/p&gt;
&lt;p&gt;
PEP&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#20043;&#21069;&#32454;&#21270;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#25552;&#21319;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#38382;&#39064;&#32454;&#21270;&#25552;&#31034;&#65288;PEP&#65289;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#20043;&#21069;&#20998;&#35299;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#20174;&#32780;&#22686;&#24378;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PEP&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23545;&#20110;&#38382;&#39064;&#25552;&#20986;&#30340;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15764v1 Announce Type: cross  Abstract: Large language models~(LLMs) have exhibited impressive performance across NLP tasks. So far they still face challenges in complex reasoning tasks and can be sensitive to input context. Despite significant efforts have been invested in enhancing reasoning process and improving prefix-prompts robustness, the crucial role of problem context has been overlooked. In this study, we propose a new approach to improve the mathematical capacities of LLMs, named Problem Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the problem context before reasoning, thus enhancing the global context modeling and reducing the parsing difficulties. Experiments on datasets demonstrate promising performances on complex reasoning and indicate the beneficial impact for ill-formed problems. For instance, with the GPT-3.5 model~(\texttt{text-davinci-003}), we observed a 9.93\% improvement with greedy decoding and 8.80\% improvement with self
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13284</link><description>&lt;p&gt;
&#32467;&#26500;&#24341;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;SQL&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Structure Guided Large Language Model for SQL Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13284
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20934;&#30830;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#29992;&#25143;&#30340;&#35821;&#20041;&#26597;&#35810;&#19982;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21305;&#37197;&#65292;&#28982;&#21518;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#26041;&#38754;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#23558;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#27169;&#24335;&#36755;&#20837;&#21040;LLM&#20013;&#65292;&#24182;&#20381;&#36182;LLM&#25191;&#34892;&#35821;&#20041;-&#32467;&#26500;&#21305;&#37197;&#24182;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24573;&#30053;&#20102;&#29992;&#25143;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#32467;&#26500;&#21270;SQL&#30340;&#29983;&#25104;&#12290;&#36825;&#19968;&#30095;&#24573;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#25110;&#26080;&#27861;&#25191;&#34892;&#30340;SQL&#29983;&#25104;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21040;SQL&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22266;&#26377;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#25913;&#21892;LLM&#30340;SQL&#29983;&#25104;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#32467;&#26500;&#24341;&#23548;SQL&#65288;SGU-SQL&#65289;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13284v1 Announce Type: cross  Abstract: Generating accurate Structured Querying Language (SQL) is a long-standing problem, especially in matching users' semantic queries with structured databases and then generating structured SQL. Existing models typically input queries and database schemas into the LLM and rely on the LLM to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation. To fully exploit the structure, we propose a structure-to-SQL framework, which leverages the inherent structure information to improve the SQL generation of LLMs. Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model. SGU-SQL first links user queries and databases in a structure-enhanced manner. It then decomposes complicated linked str
&lt;/p&gt;</description></item><item><title>&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.11800</link><description>&lt;p&gt;
&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65306;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#30340;&#26377;&#38480;&#26102;&#38388;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11800
&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22823;&#35268;&#27169;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#26041;&#26696;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;&#34429;&#28982;&#24310;&#36831;&#30340;&#24433;&#21709;&#22312;&#20248;&#21270;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#19982;&#24213;&#23618;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#20197;&#22609;&#36896;SA&#30340;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#30340;&#26041;&#24335;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#35777;&#26126;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#24310;&#36831;&#30340;SA&#26356;&#26032;&#35268;&#21017;&#30830;&#20445;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21040;SA&#36816;&#31639;&#31526;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#20855;&#26377;&#25351;&#25968;&#24555;&#36895;&#30340;&#36895;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;$\tau_{max}$&#21644;&#28151;&#21512;&#26102;&#38388;$\tau_{mix}$&#26041;&#38754;&#26159;\emph{&#32039;&#33268;&#30340;}&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#32039;&#23494;&#30028;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24402;&#32435;&#35777;&#26126;&#25216;&#26415;&#65292;&#19982;&#21508;&#31181;&#29616;&#26377;&#24310;&#36831;&#20248;&#21270;&#20998;&#26512;&#19981;&#21516;&#65292;&#23427;&#20381;&#36182;&#20110;&#24314;&#31435;&#26410;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#23545;&#30456;&#23545;&#32622;&#20449;&#24230;&#26377;&#24433;&#21709;&#65292;&#20294;&#24182;&#19981;&#19968;&#33268;&#22320;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.09654</link><description>&lt;p&gt;
GPT-4&#22312;&#22522;&#20110;USMLE&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GPT-4's assessment of its performance in a USMLE-based case study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#23545;&#30456;&#23545;&#32622;&#20449;&#24230;&#26377;&#24433;&#21709;&#65292;&#20294;&#24182;&#19981;&#19968;&#33268;&#22320;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#32654;&#22269;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65288;USMLE&#65289;&#38382;&#21367;&#20013;&#25552;&#21462;&#38382;&#39064;&#30340;&#26041;&#24335;&#65292;&#20219;&#21153;&#26159;&#35780;&#20272;&#27169;&#22411;&#22312;&#25552;&#38382;&#20043;&#21069;&#21644;&#25552;&#38382;&#20043;&#21518;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#12290;&#38382;&#21367;&#26681;&#25454;&#26159;&#21542;&#26377;&#21453;&#39304;&#20998;&#20026;&#20004;&#32452;&#65306;&#21453;&#39304;&#32452;&#65288;WF&#65289;&#21644;&#26080;&#21453;&#39304;&#32452;&#65288;NF&#65289;&#12290;&#35201;&#27714;&#27169;&#22411;&#22312;&#27599;&#20010;&#38382;&#39064;&#20043;&#21069;&#21644;&#20043;&#21518;&#25552;&#20379;&#32477;&#23545;&#21644;&#30456;&#23545;&#32622;&#20449;&#24230;&#24471;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#24037;&#20855;&#20998;&#26512;&#23454;&#39564;&#32467;&#26524;&#65292;&#30740;&#31350;&#20102;WF&#21644;NF&#32452;&#30340;&#32622;&#20449;&#24230;&#21464;&#24322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#39034;&#24207;&#20998;&#26512;&#20197;&#35266;&#23519;WF&#21644;NF&#32452;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#20250;&#24433;&#21709;&#30456;&#23545;&#32622;&#20449;&#24230;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09654v1 Announce Type: new  Abstract: This study investigates GPT-4's assessment of its performance in healthcare applications. A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it. Understanding the p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.09283</link><description>&lt;p&gt;
&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24212;&#29992;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#22238;&#22797;&#30340;&#39118;&#38505;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#24182;&#28608;&#21457;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#27492;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#25688;&#35201;&#65292;&#22686;&#36827;&#23545;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#35838;&#39064;&#12290;&#20026;&#20102;&#26041;&#20415;&#21442;&#32771;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#23545;&#25152;&#26377;&#22312;&#27492;&#35843;&#26597;&#20013;&#25552;&#21040;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/niconi19/LLM-conversation-safety&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
&lt;/p&gt;</description></item><item><title>OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01739</link><description>&lt;p&gt;
OpenMoE&#65306;&#24320;&#28304;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#26089;&#26399;&#21162;&#21147;
&lt;/p&gt;
&lt;p&gt;
OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01739
&lt;/p&gt;
&lt;p&gt;
OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24110;&#21161;&#24320;&#28304;&#31038;&#21306;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;(MoE)&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;OpenMoE&#65292;&#19968;&#31995;&#21015;&#23436;&#20840;&#24320;&#25918;&#28304;&#30721;&#21644;&#21487;&#22797;&#29616;&#30340;&#20165;&#35299;&#30721;&#22120;MoE LLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;650M&#21040;34B&#65292;&#35757;&#32451;&#25968;&#25454;&#36229;&#36807;1T&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#23454;&#65292;MoE-based LLM&#21487;&#20197;&#25552;&#20379;&#27604;&#23494;&#38598;LLM&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;LLM&#24320;&#21457;&#30340;&#28508;&#22312;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#23545;&#25105;&#20204;&#30340;OpenMoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#26426;&#21046;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#19978;&#19979;&#25991;&#26080;&#20851;&#19987;&#19994;&#21270;&#12289;&#26089;&#26399;&#36335;&#30001;&#23398;&#20064;&#21644;&#26411;&#23614;&#38477;&#20302;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#20915;&#31574;&#20027;&#35201;&#22522;&#20110;&#26631;&#35760;ID&#65292;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#24456;&#23567;&#12290;&#26631;&#35760;&#21040;&#19987;&#23478;&#30340;&#20998;&#37197;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26089;&#26399;&#30830;&#23450;&#65292;&#24182;&#19988;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#19981;&#23436;&#20840;&#30340;&#36335;&#30001;&#21487;&#33021;&#23548;&#33268;...
&lt;/p&gt;
&lt;p&gt;
To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#28508;&#22312;&#21160;&#20316;&#20449;&#24687;&#65292;LAPO&#33021;&#22815;&#35757;&#32451;&#21487;&#20197;&#36805;&#36895;&#24494;&#35843;&#20026;&#19987;&#23478;&#32423;&#31574;&#30053;&#30340;&#28508;&#22312;&#21160;&#20316;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2312.10812</link><description>&lt;p&gt;
&#26080;&#38656;&#21160;&#20316;&#30340;&#34892;&#20026;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Act without Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10812
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#28508;&#22312;&#21160;&#20316;&#20449;&#24687;&#65292;LAPO&#33021;&#22815;&#35757;&#32451;&#21487;&#20197;&#36805;&#36895;&#24494;&#35843;&#20026;&#19987;&#23478;&#32423;&#31574;&#30053;&#30340;&#28508;&#22312;&#21160;&#20316;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#26159;&#33719;&#21462;&#24378;&#22823;&#36890;&#29992;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#33539;&#24335;&#23578;&#26410;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24471;&#20197;&#25512;&#24191;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Latent Action Policies&#65288;LAPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#35270;&#39057;&#20013;&#32431;&#31929;&#24674;&#22797;&#28508;&#22312;&#21160;&#20316;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20135;&#29983;&#28508;&#22312;&#21160;&#20316;&#31574;&#30053;&#12289;&#19990;&#30028;&#27169;&#22411;&#21644;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;LAPO&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20165;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#30495;&#23454;&#21160;&#20316;&#31354;&#38388;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#29983;&#25104;&#29615;&#22659;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10812v2 Announce Type: replace-cross  Abstract: Pre-training large models on vast amounts of web data has proven to be an effective approach for obtaining powerful, general models in domains such as language and vision. However, this paradigm has not yet taken hold in reinforcement learning. This is because videos, the most abundant form of embodied behavioral data on the web, lack the action labels required by existing methods for imitating behavior from demonstrations. We introduce Latent Action Policies (LAPO), a method for recovering latent action information, and thereby latent-action policies, world models, and inverse dynamics models, purely from videos. LAPO is the first method able to recover the structure of the true action space just from observed dynamics, even in challenging procedurally-generated environments. LAPO enables training latent-action policies that can be rapidly fine-tuned into expert-level policies, either offline using a small action-labeled datas
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#65292;&#36991;&#20813;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#32780;&#31215;&#32047;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2312.08533</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#24341;&#23548;&#30340;&#36712;&#36857;&#25193;&#25955;&#23454;&#29616;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
World Models via Policy-Guided Trajectory Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08533
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#65292;&#36991;&#20813;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#32780;&#31215;&#32047;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#26159;&#24320;&#21457;&#26234;&#33021;agent&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36890;&#36807;&#39044;&#27979;&#19968;&#31995;&#21015;&#34892;&#21160;&#30340;&#32467;&#26524;&#65292;&#19990;&#30028;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#22312;&#8220;&#24819;&#35937;&#20013;&#8221;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#20248;&#21270;&#31574;&#30053;&#65292;&#21363;&#36890;&#36807;&#22312;&#32447;&#31574;&#30053;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#23454;&#29616;&#12290;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#26159;&#33258;&#22238;&#24402;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340;&#21516;&#26102;&#20174;&#31574;&#30053;&#20013;&#37319;&#26679;&#19979;&#19968;&#20010;&#34892;&#21160;&#12290;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#30340;&#22686;&#38271;&#65292;&#39044;&#27979;&#35823;&#24046;&#24517;&#28982;&#20250;&#32047;&#31215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#19981;&#26159;&#33258;&#22238;&#24402;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#21033;&#29992;&#20102;&#38500;&#20102;&#31574;&#30053;&#30340;&#21160;&#20316;&#20998;&#24067;&#26799;&#24230;&#20043;&#22806;&#30340;&#19968;&#20010;&#21435;&#22122;&#27169;&#22411;&#65292;&#23558;&#26368;&#21021;&#38543;&#26426;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#36712;&#36857;&#25193;&#25955;&#25104;&#19968;&#20010;&#22312;&#32447;&#21512;&#25104;&#36712;&#36857;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;PolyGRAD&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08533v3 Announce Type: replace-cross  Abstract: World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FoundationPose&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25903;&#25345;&#26032;&#29289;&#20307;&#30340;6D&#23039;&#21183;&#20272;&#35745;&#21644;&#36319;&#36394;&#65292;&#36890;&#36807;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#21644;&#22823;&#35268;&#27169;&#35757;&#32451;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.08344</link><description>&lt;p&gt;
FoundationPose: &#32479;&#19968;&#30340;&#26032;&#29289;&#20307;6D&#23039;&#21183;&#20272;&#35745;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08344
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FoundationPose&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25903;&#25345;&#26032;&#29289;&#20307;&#30340;6D&#23039;&#21183;&#20272;&#35745;&#21644;&#36319;&#36394;&#65292;&#36890;&#36807;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#21644;&#22823;&#35268;&#27169;&#35757;&#32451;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FoundationPose&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;6D&#29289;&#20307;&#23039;&#21183;&#20272;&#35745;&#21644;&#36319;&#36394;&#65292;&#25903;&#25345;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#31435;&#21363;&#24212;&#29992;&#20110;&#26032;&#29289;&#20307;&#65292;&#26080;&#38656;&#24494;&#35843;&#65292;&#21482;&#35201;&#32473;&#20986;&#20854;CAD&#27169;&#22411;&#65292;&#25110;&#32773;&#25429;&#33719;&#23569;&#37327;&#21442;&#32771;&#22270;&#20687;&#12290;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26469;&#24357;&#21512;&#36825;&#20004;&#31181;&#35774;&#32622;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#34920;&#31034;&#20801;&#35768;&#26377;&#25928;&#30340;&#26032;&#35270;&#22270;&#21512;&#25104;&#65292;&#24182;&#20351;&#19979;&#28216;&#23039;&#21183;&#20272;&#35745;&#27169;&#22359;&#22312;&#30456;&#21516;&#32479;&#19968;&#26694;&#26550;&#19979;&#20445;&#25345;&#19981;&#21464;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#21512;&#25104;&#35757;&#32451;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12289;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#20197;&#21450;&#23545;&#27604;&#23398;&#20064;&#20844;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#28041;&#21450;&#25361;&#25112;&#24615;&#22330;&#26223;&#21644;&#29289;&#20307;&#30340;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32479;&#19968;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#19987;&#38376;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08344v2 Announce Type: replace-cross  Abstract: We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without fine-tuning, as long as its CAD model is given, or a small number of reference images are captured. We bridge the gap between these two setups with a neural implicit representation that allows for effective novel view synthesis, keeping the downstream pose estimation modules invariant under the same unified framework. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In additi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.05677</link><description>&lt;p&gt;
&#22522;&#20110;&#25209;&#22788;&#29702;&#30340;&#22522;&#30784;&#27169;&#22411;&#20302;&#31209;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Batched Low-Rank Adaptation of Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05677
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#22240;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#24182;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#32780;&#24341;&#36215;&#20851;&#27880;&#12290;&#34429;&#28982;LoRA&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#20854;&#22312;&#23454;&#26102;&#20026;&#21508;&#31181;&#20840;&#29699;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26102;&#26080;&#27861;&#39640;&#25928;&#22788;&#29702;&#22810;&#20010;&#29305;&#23450;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#36825;&#20026;&#38656;&#35201;&#20026;&#27599;&#20010;&#20256;&#20837;&#35831;&#27714;&#20010;&#24615;&#21270;&#12289;&#29305;&#23450;&#20219;&#21153;&#36866;&#24212;&#30340;&#22330;&#26223;&#20013;&#36896;&#25104;&#20102;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#25209;&#22788;&#29702;&#20013;&#30340;&#27599;&#20010;&#36755;&#20837;&#31034;&#20363;&#37117;&#21487;&#20197;&#19982;&#20854;&#29420;&#29305;&#30340;&#20302;&#31209;&#36866;&#24212;&#26435;&#37325;&#30456;&#20851;&#32852;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24322;&#26500;&#35831;&#27714;&#30340;&#39640;&#25928;&#25209;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#34920;&#26126;&#65292;FLoRA&#20445;&#30041;&#20102;LoRA&#30340;&#32489;&#25928;&#20248;&#28857;&#65292;&#22312;&#36328;&#36234;8&#31181;&#35821;&#35328;&#30340;MultiPL-E&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20986;&#31454;&#20105;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05677v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;SplaTAM&#65292;&#36890;&#36807;&#21033;&#29992;3D&#39640;&#26031;&#20989;&#25968;&#30340;&#26174;&#24335;&#20307;&#31215;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#26410;&#23450;&#20301;RGB-D&#30456;&#26426;&#36827;&#34892;&#39640;&#20445;&#30495;&#37325;&#24314;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2312.02126</link><description>&lt;p&gt;
SplaTAM: &#29992;&#20110;&#23494;&#38598;RGB-D SLAM&#30340;Splat&#12289;&#36861;&#36394;&#21644;&#26144;&#23556;3D&#39640;&#26031;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;SplaTAM&#65292;&#36890;&#36807;&#21033;&#29992;3D&#39640;&#26031;&#20989;&#25968;&#30340;&#26174;&#24335;&#20307;&#31215;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#26410;&#23450;&#20301;RGB-D&#30456;&#26426;&#36827;&#34892;&#39640;&#20445;&#30495;&#37325;&#24314;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#30340;&#21516;&#26102;&#23450;&#20301;&#21644;&#22320;&#22270;&#26500;&#24314;&#65288;SLAM&#65289;&#23545;&#20110;&#26426;&#22120;&#20154;&#21644;&#22686;&#24378;&#29616;&#23454;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#21463;&#21040;&#23427;&#20204;&#34920;&#31034;&#22330;&#26223;&#30340;&#38750;&#20307;&#31215;&#25110;&#38544;&#24335;&#26041;&#24335;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SplaTAM&#65292;&#19968;&#31181;&#39318;&#27425;&#21033;&#29992;&#26174;&#24335;&#20307;&#31215;&#34920;&#31034;&#65288;&#21363;3D&#39640;&#26031;&#20989;&#25968;&#65289;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#26410;&#23450;&#20301;RGB-D&#30456;&#26426;&#23454;&#29616;&#39640;&#20445;&#30495;&#37325;&#24314;&#65292;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;SplaTAM&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22312;&#32447;&#36861;&#36394;&#21644;&#26144;&#23556;&#31995;&#32479;&#65292;&#19987;&#20026;&#24213;&#23618;&#39640;&#26031;&#20989;&#25968;&#34920;&#31034;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#23427;&#21033;&#29992;&#21098;&#24433;&#25513;&#33180;&#20248;&#38597;&#22320;&#25429;&#25417;&#22330;&#26223;&#23494;&#24230;&#30340;&#23384;&#22312;&#12290;&#35813;&#32452;&#21512;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#34920;&#31034;&#25552;&#20379;&#20102;&#22810;&#20010;&#20248;&#28857;&#65292;&#21253;&#25324;&#24555;&#36895;&#28210;&#26579;&#21644;&#23494;&#38598;&#20248;&#21270;&#12289;&#24555;&#36895;&#30830;&#23450;&#21306;&#22495;&#26159;&#21542;&#24050;&#34987;&#26144;&#23556;&#20197;&#21450;&#36890;&#36807;&#28155;&#21152;&#26356;&#22810;&#39640;&#26031;&#20989;&#25968;&#36827;&#34892;&#32467;&#26500;&#21270;&#22320;&#22270;&#25193;&#23637;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;SplaTAM...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02126v2 Announce Type: replace-cross  Abstract: Dense simultaneous localization and mapping (SLAM) is crucial for robotics and augmented reality applications. However, current methods are often hampered by the non-volumetric or implicit way they represent a scene. This work introduces SplaTAM, an approach that, for the first time, leverages explicit volumetric representations, i.e., 3D Gaussians, to enable high-fidelity reconstruction from a single unposed RGB-D camera, surpassing the capabilities of existing methods. SplaTAM employs a simple online tracking and mapping system tailored to the underlying Gaussian representation. It utilizes a silhouette mask to elegantly capture the presence of scene density. This combination enables several benefits over prior representations, including fast rendering and dense optimization, quickly determining if areas have been previously mapped, and structured map expansion by adding more Gaussians. Extensive experiments show that SplaTAM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hourglass Tokenizer&#65288;HoT&#65289;&#30340;&#20462;&#21098;&#21644;&#24674;&#22797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#39640;&#25928;&#22320;&#22522;&#20110;Transformer&#36827;&#34892;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#20855;&#26377;&#39640;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#20195;&#34920;&#24615;&#26631;&#35760;&#24182;&#28040;&#38500;&#35270;&#39057;&#24103;&#30340;&#20887;&#20313;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.12028</link><description>&lt;p&gt;
&#39640;&#25928;&#22522;&#20110;Transformer&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;Hourglass Tokenizer
&lt;/p&gt;
&lt;p&gt;
Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hourglass Tokenizer&#65288;HoT&#65289;&#30340;&#20462;&#21098;&#21644;&#24674;&#22797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#39640;&#25928;&#22320;&#22522;&#20110;Transformer&#36827;&#34892;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#20855;&#26377;&#39640;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#20195;&#34920;&#24615;&#26631;&#35760;&#24182;&#28040;&#38500;&#35270;&#39057;&#24103;&#30340;&#20887;&#20313;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#22522;&#20110;&#35270;&#39057;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35270;&#39057;&#23039;&#21183;Transformer&#65288;VPTs&#65289;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hourglass Tokenizer&#65288;HoT&#65289;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#20462;&#21098;&#21644;&#24674;&#22797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#39640;&#25928;&#22320;&#22522;&#20110;Transformer&#36827;&#34892;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;HoT&#39318;&#20808;&#36890;&#36807;&#20462;&#21098;&#20887;&#20313;&#24103;&#30340;&#23039;&#21183;&#26631;&#35760;&#24320;&#22987;&#65292;&#28982;&#21518;&#20197;&#24674;&#22797;&#20840;&#38271;&#24230;&#26631;&#35760;&#32467;&#26463;&#65292;&#20174;&#32780;&#22312;&#20013;&#38388;&#30340;Transformer&#22359;&#20013;&#20135;&#29983;&#23569;&#37327;&#23039;&#21183;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#26377;&#25928;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#35760;&#20462;&#21098;&#38598;&#32676;&#65288;TPC&#65289;&#65292;&#21160;&#24577;&#36873;&#25321;&#19968;&#20123;&#20855;&#26377;&#39640;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#20195;&#34920;&#24615;&#26631;&#35760;&#65292;&#21516;&#26102;&#28040;&#38500;&#35270;&#39057;&#24103;&#30340;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26631;&#35760;&#24674;&#22797;&#27880;&#24847;&#21147;&#65288;TRA&#65289;&#26469;&#24674;&#22797;&#35814;&#32454;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12028v2 Announce Type: replace-cross  Abstract: Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a plug-and-play pruning-and-recovering framework, called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose estimation from videos. Our HoT begins with pruning pose tokens of redundant frames and ends with recovering full-length tokens, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. To effectively achieve this, we propose a token pruning cluster (TPC) that dynamically selects a few representative tokens with high semantic diversity while eliminating the redundancy of video frames. In addition, we develop a token recovering attention (TRA) to restore the detailed spatio-temporal information b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20840;&#23616;&#35821;&#20041;&#34701;&#21512;&#21644;&#33258;&#30456;&#20284;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#65292;&#20197;&#24341;&#23548;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#23545;&#35937;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.10522</link><description>&lt;p&gt;
&#25552;&#21319;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#23545;&#35937;&#36830;&#36143;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Object Coherence in Layout-to-Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20840;&#23616;&#35821;&#20041;&#34701;&#21512;&#21644;&#33258;&#30456;&#20284;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#65292;&#20197;&#24341;&#23548;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#23545;&#35937;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#12290;&#23427;&#26088;&#22312;&#29983;&#25104;&#22797;&#26434;&#22330;&#26223;&#65292;&#29992;&#25143;&#21487;&#20197;&#23545;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#24067;&#23616;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#23545;&#35937;&#36830;&#36143;&#24615;&#65292;&#21253;&#25324;&#35821;&#20041;&#36830;&#36143;&#24615;&#65288;&#20363;&#22914;&#65292;&#29483;&#26159;&#21542;&#30475;&#21521;&#33457;&#26421;&#65289;&#21644;&#29289;&#29702;&#36830;&#36143;&#24615;&#65288;&#20363;&#22914;&#65292;&#25163;&#21644;&#29699;&#25293;&#19981;&#24212;&#38169;&#20301;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#37197;&#22791;&#20102;&#26377;&#25928;&#30340;&#20840;&#23616;&#35821;&#20041;&#34701;&#21512;&#65288;GSF&#65289;&#21644;&#33258;&#30456;&#20284;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#65292;&#20197;&#24341;&#23548;&#35813;&#20219;&#21153;&#30340;&#23545;&#35937;&#36830;&#36143;&#24615;&#12290;&#23545;&#20110;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#22270;&#20687;&#26631;&#39064;&#21253;&#21547;&#20016;&#23500;&#20449;&#24687;&#65292;&#21487;&#20197;&#23450;&#20041;&#22270;&#20687;&#20013;&#23545;&#35937;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#19982;&#20854;&#31616;&#21333;&#22320;&#20351;&#29992;&#26631;&#39064;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#36328;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#21516;&#26102;&#22788;&#29702;&#39640;&#24230;&#30456;&#20851;&#24067;&#23616;&#38480;&#21046;&#21644;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10522v4 Announce Type: replace-cross  Abstract: Layout-to-image synthesis is an emerging technique in conditional image generation. It aims to generate complex scenes, where users require fine control over the layout of the objects in a scene. However, it remains challenging to control the object coherence, including semantic coherence (e.g., the cat looks at the flowers or not) and physical coherence (e.g., the hand and the racket should not be misaligned). In this paper, we propose a novel diffusion model with effective global semantic fusion (GSF) and self-similarity feature enhancement modules to guide the object coherence for this task. For semantic coherence, we argue that the image caption contains rich information for defining the semantic relationship within the objects in the images. Instead of simply employing cross-attention between captions and generated images, which addresses the highly relevant layout restriction and semantic coherence separately and thus lea
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;S4MI&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#33021;&#22815;&#31616;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#26426;&#22120;&#30417;&#30563;&#36807;&#31243;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.10319</link><description>&lt;p&gt;
&#36716;&#21521;&#26426;&#22120;&#30417;&#30563;&#65306;&#29992;&#20110;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#26631;&#27880;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10319
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;S4MI&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#33021;&#22815;&#31616;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#26426;&#22120;&#30417;&#30563;&#36807;&#31243;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#27835;&#30103;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#21463;&#21040;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#25216;&#26415;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#12290;&#26631;&#27880;&#36807;&#31243;&#19981;&#20165;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#19988;&#38656;&#35201;&#20020;&#24202;&#19987;&#23478;&#22823;&#37327;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;S4MI&#65288;&#21307;&#23398;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#65289;&#27969;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21457;&#23637;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#25216;&#26415;&#21442;&#19982;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26426;&#22120;&#30417;&#30563;&#30340;&#25193;&#23637;&#65292;&#30456;&#27604;&#23436;&#20840;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10319v3 Announce Type: replace-cross  Abstract: Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages the advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self-supervised learning significantly surpassed the performance of supervised methods in the classificati
&lt;/p&gt;</description></item><item><title>&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#26816;&#32034;&#30340;&#25991;&#20214;&#19981;&#20165;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#65292;&#20294;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#24050;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2311.07838</link><description>&lt;p&gt;
LLatrieval&#65306;LLM-&#39564;&#35777;&#26816;&#32034;&#29992;&#20110;&#21487;&#39564;&#35777;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLatrieval: LLM-Verified Retrieval for Verifiable Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07838
&lt;/p&gt;
&lt;p&gt;
&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#26816;&#32034;&#30340;&#25991;&#20214;&#19981;&#20165;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#65292;&#20294;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#24050;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#39564;&#35777;&#29983;&#25104;&#26088;&#22312;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#20855;&#26377;&#25903;&#25745;&#25991;&#20214;&#30340;&#25991;&#26412;&#65292;&#36825;&#20351;&#29992;&#25143;&#33021;&#22815;&#28789;&#27963;&#39564;&#35777;&#31572;&#26696;&#65292;&#24182;&#20351;LLM&#30340;&#36755;&#20986;&#26356;&#21487;&#38752;&#12290;&#26816;&#32034;&#22312;&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26816;&#32034;&#21040;&#30340;&#25991;&#20214;&#19981;&#20165;&#34917;&#20805;&#30693;&#35782;&#20197;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#25903;&#25345;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#25104;&#20026;&#25972;&#20010;&#27969;&#31243;&#30340;&#29942;&#39048;&#65292;&#24182;&#38480;&#21046;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#30001;&#20110;&#36890;&#24120;&#20855;&#26377;&#30340;&#21442;&#25968;&#27604;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#23578;&#26410;&#35777;&#26126;&#33021;&#22815;&#33391;&#22909;&#22320;&#25193;&#23637;&#21040;LLM&#30340;&#35268;&#27169;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#33021;&#21147;&#36890;&#24120;&#27604;LLMs&#24046;&#12290;&#22914;&#26524;&#26816;&#32034;&#22120;&#26410;&#33021;&#27491;&#30830;&#25214;&#21040;&#25903;&#25345;&#25991;&#20214;&#65292;&#21017;LLM&#23558;&#26080;&#27861;&#29983;&#25104;&#27491;&#30830;&#21644;&#21487;&#39564;&#35777;&#30340;&#31572;&#26696;&#65292;&#36825;&#20250;&#25513;&#30422;LLM&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07838v2 Announce Type: replace  Abstract: Verifiable generation aims to let the large language model (LLM) generate text with supporting documents, which enables the user to flexibly verify the answer and makes the LLM's output more reliable. Retrieval plays a crucial role in verifiable generation. Specifically, the retrieved documents not only supplement knowledge to help the LLM generate correct answers, but also serve as supporting evidence for the user to verify the LLM's output. However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs. If the retriever does not correctly find the supporting documents, the LLM can not generate the correct and verifiable answer, which overshadows the LLM's remarkable abilities. To address these li
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.04698</link><description>&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#25361;&#25112;&#24120;&#35265;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Challenging Common Paradigms in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04698
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#24182;&#26410;&#24102;&#26469;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#26356;&#28145;&#20837;&#20102;&#35299;MTL&#29305;&#23450;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;MTL&#20013;&#30340;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#20960;&#28857;&#20851;&#20110;STL&#30340;&#37325;&#35201;&#24433;&#21709;&#65306;&#39318;&#20808;&#65292;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#23545;MTL&#30340;&#24433;&#21709;&#21482;&#21463;&#21040;&#20102;&#36731;&#24494;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#30340;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;&#24120;&#35265;STL&#24037;&#20855;&#65288;&#20363;&#22914;Adam&#20248;&#21270;&#22120;&#65289;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;Adam&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#37096;&#20998;&#25439;&#22833;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;&#20854;&#27425;&#65292;&#26799;&#24230;&#20914;&#31361;&#30340;&#27010;&#24565;&#32463;&#24120;&#34987;&#25551;&#36848;&#20026;MTL&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#26799;&#24230;&#20914;&#31361;&#22312;MTL&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;STL&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35282;&#24230;&#26799;&#24230;&#23545;&#40784;&#26041;&#38754;&#65292;&#25105;&#20204;&#27809;&#26377;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#36817;&#20284;&#26041;&#27861;&#21644;&#23618;&#27425;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25915;&#20987;FedAvg&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2308.06822</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36817;&#20284;&#21644;&#21152;&#26435;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Approximate and Weighted Data Reconstruction Attack in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.06822
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#36817;&#20284;&#26041;&#27861;&#21644;&#23618;&#27425;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25915;&#20987;FedAvg&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#20316;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#34987;&#35748;&#20026;&#26159;&#36890;&#36807;&#35774;&#35745;&#20445;&#25252;&#38544;&#31169;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22522;&#20110;&#22312;FL&#20013;&#20849;&#20139;&#30340;&#21442;&#25968;&#24674;&#22797;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25915;&#20987;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#27700;&#24179;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#22330;&#26223;&#65292;&#22312;&#27492;&#22330;&#26223;&#20013;&#65292;&#23458;&#25143;&#31471;&#22312;&#22810;&#20010;&#23616;&#37096;&#35757;&#32451;&#27493;&#39588;&#20043;&#21518;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23458;&#25143;&#31471;&#23616;&#37096;&#35757;&#32451;&#36807;&#31243;&#30340;&#20013;&#38388;&#27169;&#22411;&#26356;&#26032;&#65292;&#20351;&#25915;&#20987;FedAvg&#22330;&#26223;&#21464;&#24471;&#21487;&#34892;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23618;&#27425;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#37325;&#26500;&#30340;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#20026;&#19981;&#21516;&#23618;&#27425;&#30340;&#27169;&#22411;&#26356;&#26032;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.06822v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) is a distributed learning paradigm that enables multiple clients to collaborate on building a machine learning model without sharing their private data. Although FL is considered privacy-preserved by design, recent data reconstruction attacks demonstrate that an attacker can recover clients' training data based on the parameters shared in FL. However, most existing methods fail to attack the most widely used horizontal Federated Averaging (FedAvg) scenario, where clients share model parameters after multiple local training steps. To tackle this issue, we propose an interpolation-based approximation method, which makes attacking FedAvg scenarios feasible by generating the intermediate model updates of the clients' local training processes. Then, we design a layer-wise weighted loss function to improve the data quality of reconstruction. We assign different weights to model updates in different layers conc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#20998;&#37197;&#26041;&#26696;&#65292;&#22312;&#23398;&#20064;&#25511;&#21046;&#25928;&#33021;&#26144;&#23556;&#30340;&#36870;&#36807;&#31243;&#21518;&#65292;&#23558;&#20854;&#20316;&#20026;&#20998;&#37197;&#22120;&#23454;&#26045;&#65292;&#36991;&#20813;&#20102;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2201.06180</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#25511;&#21046;&#20998;&#37197;&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Control Allocation: A Learning Based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.06180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#20998;&#37197;&#26041;&#26696;&#65292;&#22312;&#23398;&#20064;&#25511;&#21046;&#25928;&#33021;&#26144;&#23556;&#30340;&#36870;&#36807;&#31243;&#21518;&#65292;&#23558;&#20854;&#20316;&#20026;&#20998;&#37197;&#22120;&#23454;&#26045;&#65292;&#36991;&#20813;&#20102;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39134;&#26426;&#35774;&#35745;&#20855;&#26377;&#20887;&#20313;&#25511;&#21046;&#25191;&#34892;&#22120;&#65292;&#20197;&#28385;&#36275;&#23481;&#38169;&#21644;&#26426;&#21160;&#24615;&#35201;&#27714;&#12290;&#36825;&#23548;&#33268;&#39134;&#26426;&#36807;&#24230;&#20316;&#29992;&#65292;&#24182;&#38656;&#35201;&#25511;&#21046;&#20998;&#37197;&#26041;&#26696;&#22312;&#25511;&#21046;&#25191;&#34892;&#22120;&#20043;&#38388;&#20998;&#37197;&#25511;&#21046;&#25351;&#20196;&#12290;&#20256;&#32479;&#19978;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;&#25511;&#21046;&#20998;&#37197;&#26041;&#26696;&#34987;&#20351;&#29992;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#20998;&#37197;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#20998;&#37197;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#30001;&#36890;&#36807;ANN&#23398;&#20064;&#25511;&#21046;&#25928;&#33021;&#26144;&#23556;&#30340;&#36870;&#36807;&#31243;&#32452;&#25104;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#20998;&#37197;&#22120;&#23454;&#26045;&#65292;&#32780;&#19981;&#26159;&#35299;&#20915;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#21253;&#21547;&#20998;&#37197;&#22120;&#30340;&#38381;&#29615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#20998;&#27573;&#32447;&#24615;&#25928;&#33021;&#20989;&#25968;&#21644;&#22522;&#20110;ANN&#30340;&#20998;&#37197;&#22120;&#25506;&#35752;&#20102;&#35745;&#31639;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.06180v2 Announce Type: replace-cross  Abstract: Modern aircraft are designed with redundant control effectors to cater for fault tolerance and maneuverability requirements. This leads to aircraft being over-actuated and requires control allocation schemes to distribute the control commands among control effectors. Traditionally, optimization-based control allocation schemes are used; however, for nonlinear allocation problems, these methods require large computational resources. In this work, an artificial neural network (ANN) based nonlinear control allocation scheme is proposed. The proposed scheme is composed of learning the inverse of the control effectiveness map through ANN, and then implementing it as an allocator instead of solving an online optimization problem. Stability conditions are presented for closed-loop systems incorporating the allocator, and computational challenges are explored with piece-wise linear effectiveness functions and ANN-based allocators. To d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#30340;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29615;&#22659;&#30340;&#21382;&#21490;&#31354;&#38388;&#19978;&#19979;&#25991;&#25552;&#20379;&#30340;&#30456;&#20284;&#24615;&#20449;&#21495;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38476;&#29983;&#29615;&#22659;&#20013;&#12290;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#20026;&#20195;&#29702;&#22312;&#20855;&#26377;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#30340;&#26032;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#30340;&#35270;&#35273;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.15120</link><description>&lt;p&gt;
&#22522;&#20110;&#29615;&#22659;&#30340;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#65306;&#21033;&#29992;&#29615;&#22659;&#20316;&#20026;&#25968;&#25454;&#28304;
&lt;/p&gt;
&lt;p&gt;
Context-driven self-supervised visual learning: Harnessing the environment as a data source. (arXiv:2401.15120v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15120
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#30340;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29615;&#22659;&#30340;&#21382;&#21490;&#31354;&#38388;&#19978;&#19979;&#25991;&#25552;&#20379;&#30340;&#30456;&#20284;&#24615;&#20449;&#21495;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38476;&#29983;&#29615;&#22659;&#20013;&#12290;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#20026;&#20195;&#29702;&#22312;&#20855;&#26377;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#30340;&#26032;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#30340;&#35270;&#35273;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#36890;&#24120;&#21457;&#29983;&#22312;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#23545;&#19968;&#33268;&#29615;&#22659;&#20013;&#30340;&#20301;&#32622;&#30340;&#25506;&#32034;&#21644;&#36319;&#36394;&#26469;&#33719;&#21462;&#25216;&#33021;&#12290;&#20195;&#29702;&#30340;&#21382;&#21490;&#31354;&#38388;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#33258;&#25105;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#30456;&#20284;&#24615;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29615;&#22659;&#31354;&#38388;&#30456;&#20284;&#24615;&#65288;ESS&#65289;&#65292;&#23427;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#27169;&#25311;&#30340;&#36924;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ESS&#20248;&#20110;&#20256;&#32479;&#30340;&#23454;&#20363;&#37492;&#21035;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20174;&#30456;&#21516;&#29615;&#22659;&#20013;&#37319;&#26679;&#26356;&#22810;&#25968;&#25454;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#22686;&#24378;&#12290;ESS&#22312;&#25151;&#38388;&#20998;&#31867;&#21644;&#31354;&#38388;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#29087;&#32451;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#38476;&#29983;&#29615;&#22659;&#20013;&#12290;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#26377;&#28508;&#21147;&#22312;&#20855;&#26377;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#30340;&#26032;&#29615;&#22659;&#20013;&#20351;&#20195;&#29702;&#33021;&#22815;&#24555;&#36895;&#36827;&#34892;&#35270;&#35273;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual learning often occurs in a specific context, where an agent acquires skills through exploration and tracking of its location in a consistent environment. The historical spatial context of the agent provides a similarity signal for self-supervised contrastive learning. We present a unique approach, termed Environmental Spatial Similarity (ESS), that complements existing contrastive learning methods. Using images from simulated, photorealistic environments as an experimental setting, we demonstrate that ESS outperforms traditional instance discrimination approaches. Moreover, sampling additional data from the same environment substantially improves accuracy and provides new augmentations. ESS allows remarkable proficiency in room classification and spatial prediction tasks, especially in unfamiliar environments. This learning paradigm has the potential to enable rapid visual learning in agents operating in new environments with unique visual characteristics. Potentially transforma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12492</link><description>&lt;p&gt;
&#27604;&#36739;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35821;&#35328;&#24314;&#27169;&#65306;&#27169;&#25311;&#32676;&#20307;&#12289;&#20010;&#20307;&#29305;&#28857;&#36824;&#26159;&#20004;&#32773;&#20860;&#39038;&#65311;
&lt;/p&gt;
&lt;p&gt;
Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#23558;&#20154;&#30340;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65288;&#22914;45&#23681;&#20197;&#19978;&#30340;&#20154;&#32676;&#65289;&#36824;&#26159;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#26356;&#26377;&#25928;&#30340;&#38382;&#39064;&#23578;&#26410;&#30830;&#23450;&#12290;&#32676;&#20307;&#23646;&#24615;&#22312;&#25216;&#26415;&#19978;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#20294;&#26159;&#36807;&#20110;&#31895;&#31961;&#65306;&#24182;&#38750;&#25152;&#26377;45&#23681;&#20197;&#19978;&#30340;&#20154;&#37117;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#33021;&#22815;&#25429;&#25417;&#27599;&#20010;&#20154;&#36523;&#20221;&#30340;&#22797;&#26434;&#24615;&#65292;&#20801;&#35768;&#26356;&#20010;&#24615;&#21270;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#27169;&#25311;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#24182;&#19988;&#38656;&#35201;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#23558;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#30340;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#65288;&#22914;&#24180;&#40836;&#20272;&#35745;&#25110;&#20010;&#24615;&#35780;&#20272;&#65289;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#31435;&#22330;&#21644;&#20027;&#39064;&#26816;&#27979;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#36816;&#31609;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#36827;&#36816;&#31609;&#23398;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#26469;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#20154;&#24037;&#26234;&#33021;&#19982;&#36816;&#31609;&#23398;&#30340;&#21327;&#21516;&#20316;&#29992;&#23558;&#25512;&#21160;&#39046;&#22495;&#20869;&#30340;&#21019;&#26032;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.03244</link><description>&lt;p&gt;
&#36816;&#31609;&#23398;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#25913;&#21464;&#36816;&#31609;&#23398;&#36807;&#31243;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Operations Research: Revolutionizing the Operations Research Process. (arXiv:2401.03244v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#36816;&#31609;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#36827;&#36816;&#31609;&#23398;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#26469;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#20154;&#24037;&#26234;&#33021;&#19982;&#36816;&#31609;&#23398;&#30340;&#21327;&#21516;&#20316;&#29992;&#23558;&#25512;&#21160;&#39046;&#22495;&#20869;&#30340;&#21019;&#26032;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36805;&#36895;&#21457;&#23637;&#20026;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#36816;&#31609;&#23398;&#65292;&#24320;&#36767;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#36816;&#31609;&#23398;&#36807;&#31243;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#65288;AI4OR&#65289;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21442;&#25968;&#29983;&#25104;&#12289;&#27169;&#22411;&#26500;&#24314;&#21644;&#27169;&#22411;&#20248;&#21270;&#31561;&#22810;&#20010;&#38454;&#27573;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#20840;&#38754;&#27010;&#36848;&#29616;&#26377;&#25216;&#26415;&#30340;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#25913;&#21464;&#36816;&#31609;&#23398;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#26088;&#22312;&#28608;&#21457;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21019;&#26032;&#65292;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#36816;&#31609;&#23398;&#26041;&#27861;&#21644;&#24037;&#20855;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#36816;&#31609;&#23398;&#30340;&#21327;&#21516;&#20316;&#29992;&#23558;&#25512;&#21160;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#23637;&#21644;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#32456;&#23454;&#29616;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of artificial intelligence (AI) techniques has opened up new opportunities to revolutionize various fields, including operations research (OR). This survey paper explores the integration of AI within the OR process (AI4OR) to enhance its effectiveness and efficiency across multiple stages, such as parameter generation, model formulation, and model optimization. By providing a comprehensive overview of the state-of-the-art and examining the potential of AI to transform OR, this paper aims to inspire further research and innovation in the development of AI-enhanced OR methods and tools. The synergy between AI and OR is poised to drive significant advancements and novel solutions in a multitude of domains, ultimately leading to more effective and efficient decision-making.
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#23545;&#27604;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27714;&#35299;&#35270;&#35282;&#21644;&#24635;&#32467;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02009</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#27604;&#65306;&#36890;&#36807;&#19981;&#19968;&#33268;&#30340;&#27714;&#35299;&#35270;&#35282;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24605;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02009
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#27604;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27714;&#35299;&#35270;&#35282;&#21644;&#24635;&#32467;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19968;&#31181;&#20107;&#21518;&#25552;&#31034;&#31574;&#30053;&#65292;&#20363;&#22914;&#21453;&#24605;&#21644;&#33258;&#25105;&#25913;&#36827;&#65292;&#26681;&#25454;&#33258;&#25105;&#35780;&#20272;&#25110;&#22806;&#37096;&#21453;&#39304;&#26469;&#25913;&#21892;LLM&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#20869;&#22312;&#21453;&#24605;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#33258;&#25105;&#35780;&#20272;&#21453;&#39304;&#36136;&#37327;&#26159;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#22312;&#33258;&#25105;&#35780;&#20272;&#26102;&#24120;&#24120;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#25110;&#39640;&#24230;&#38543;&#26426;&#24615;&#65292;&#25552;&#20379;&#22266;&#25191;&#25110;&#19981;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#23548;&#33268;&#21453;&#24605;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#23545;&#27604;&#30340;&#26041;&#27861;&#65306;&#23427;&#26681;&#25454;&#35831;&#27714;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#22810;&#26679;&#30340;&#27714;&#35299;&#35270;&#35282;&#65292;&#23545;&#27604;&#24046;&#24322;&#65292;&#24182;&#23558;&#36825;&#20123;&#24046;&#24322;&#24635;&#32467;&#20026;&#19968;&#20010;&#26816;&#26597;&#34920;&#65292;&#29992;&#20110;&#37325;&#26032;&#23457;&#35270;&#21644;&#28040;&#38500;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36171;&#20104;LLM&#22810;&#26679;&#30340;&#35270;&#35282;&#20197;&#20943;&#36731;&#22266;&#25191;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#24046;&#24322;&#25351;&#31034;&#20102;&#28508;&#22312;&#30340;&#38169;&#35823;&#25110;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties tha
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10997</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#38754;&#20020;&#24187;&#35273;&#12289;&#36807;&#26102;&#30693;&#35782;&#21644;&#38750;&#36879;&#26126;&#12289;&#19981;&#21487;&#36861;&#28335;&#30340;&#25512;&#29702;&#36807;&#31243;&#31561;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;RAG&#23558;LLMs&#33258;&#36523;&#30340;&#30693;&#35782;&#19982;&#24222;&#22823;&#12289;&#21160;&#24577;&#30340;&#22806;&#37096;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#21327;&#21516;&#25928;&#24212;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32771;&#23519;&#20102;RAG&#33539;&#24335;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;&#23427;&#35814;&#32454;&#23457;&#35270;&#20102;RAG&#26694;&#26550;&#30340;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65292;&#21253;&#25324;&#26816;&#32034;&#12289;&#29983;&#25104;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#23884;&#20837;&#22312;RAG&#26694;&#26550;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in e
&lt;/p&gt;</description></item><item><title>FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01483</link><description>&lt;p&gt;
FedSN&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01483
&lt;/p&gt;
&lt;p&gt;
FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#24050;&#32463;&#30001;&#21830;&#19994;&#20844;&#21496;&#25104;&#21151;&#22320;&#21457;&#23556;&#21644;&#37096;&#32626;&#21040;&#22826;&#31354;&#20013;&#65292;&#22914;SpaceX&#12290;&#30001;&#20110;LEO&#21355;&#26143;&#37197;&#22791;&#20102;&#22810;&#27169;&#20256;&#24863;&#22120;&#65292;&#23427;&#20204;&#19981;&#20165;&#29992;&#20110;&#36890;&#20449;&#65292;&#36824;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#31354;&#38388;&#35843;&#21046;&#35782;&#21035;&#12289;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19982;LEO&#21355;&#26143;&#30340;&#26377;&#38480;&#25509;&#35302;&#26102;&#38388;&#65288;&#20363;&#22914;5&#20998;&#38047;&#65289;&#65292;&#22320;&#38754;&#31449;&#65288;GS&#65289;&#21487;&#33021;&#26080;&#27861;&#19979;&#36733;&#22914;&#27492;&#22823;&#37327;&#30340;&#21407;&#22987;&#24863;&#27979;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35201;&#22312;LEO&#21355;&#26143;&#19978;&#20351;&#29992;FL&#65292;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;i&#65289;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#65292;ii&#65289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#65292;&#20197;&#21450;iii&#65289;&#27169;&#22411;&#38472;&#26087;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSN&#30340;&#36890;&#29992;FL&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#19968;
&lt;/p&gt;
&lt;p&gt;
Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
&lt;/p&gt;</description></item><item><title>VIGraph&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#26469;&#35299;&#20915;&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01191</link><description>&lt;p&gt;
VIGraph&#65306;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification. (arXiv:2311.01191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01191
&lt;/p&gt;
&lt;p&gt;
VIGraph&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#26469;&#35299;&#20915;&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20026;&#33410;&#28857;&#20998;&#31867;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;SMOTE&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#24179;&#34913;&#22330;&#26223;&#26500;&#24314;&#36807;&#31243;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#21512;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#20854;&#28508;&#21147;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;SMOTE&#30340;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;VIGraph&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;VGAE&#65289;&#30340;&#26032;&#22411;SSL&#27169;&#22411;&#65292;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;VIGraph&#22312;&#26500;&#24314;&#19981;&#24179;&#34913;&#22270;&#26102;&#20005;&#26684;&#36981;&#24490;&#19981;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#22411;VGAE&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;VIGraph&#22312;&#35299;&#30721;&#38454;&#27573;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;VIGraph&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#65292;&#26080;&#38656;&#37325;&#26032;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance in graph data poses significant challenges for node classification. Existing methods, represented by SMOTE-based approaches, partially alleviate this issue but still exhibit limitations during imbalanced scenario construction. Self-supervised learning (SSL) offers a promising solution by synthesizing minority nodes from the data itself, yet its potential remains unexplored. In this paper, we analyze the limitations of SMOTE-based approaches and introduce VIGraph, a novel SSL model based on the self-supervised Variational Graph Auto-Encoder (VGAE) that leverages Variational Inference (VI) to generate minority nodes. Specifically, VIGraph strictly adheres to the concept of imbalance when constructing imbalanced graphs and utilizes the generative VGAE to generate minority nodes. Moreover, VIGraph introduces a novel Siamese contrastive strategy at the decoding phase to improve the overall quality of generated nodes. VIGraph can generate high-quality nodes without reintegrat
&lt;/p&gt;</description></item><item><title>Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.</title><link>http://arxiv.org/abs/2310.17072</link><description>&lt;p&gt;
&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17072
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;MMP&#65289;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#19968;&#31995;&#21015;&#36830;&#32493;&#36712;&#36857;&#27969;&#24418;&#65292;&#27599;&#19968;&#20010;&#36712;&#36857;&#27969;&#24418;&#37117;&#33021;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#30001;&#23545;&#27969;&#24418;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#35299;&#30721;&#20989;&#25968;&#20197;&#21450;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#23494;&#24230;&#32452;&#25104;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#25197;&#26354;&#65292;MMP&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;--&#36890;&#36807;&#21464;&#24418;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#36816;&#21160;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#27861;&#30456;&#37051;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;IMMP&#65289;&#65292;&#20854;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20445;&#25345;&#20102;&#27969;&#24418;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#21644;&#20351;&#29992;&#20102;&#19968;&#20010;Riemannian&#24230;&#37327;&#65292;&#29992;&#20110;&#36816;&#21160;&#31354;&#38388;&#65288;&#21363;&#65292;&#21442;&#25968;&#21270;&#26354;&#32447;&#31354;&#38388;&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;CurveGeom Riemannian&#24230;&#37327;&#12290;&#23545;&#20110;&#24179;&#38754;&#38556;&#30861;&#36991;&#35753;&#36816;&#21160;&#21644;&#25512;&#21160;&#25805;&#32437;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMMP&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;MMP&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Gabe-YHLee/IMMP&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#35268;&#21010;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#27010;&#24565;&#34920;&#31034;&#12289;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#20197;&#21450;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#20851;&#32852;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2310.03325</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#21644;&#31526;&#21495;&#25512;&#29702;&#29992;&#20110;&#35270;&#35273;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning. (arXiv:2310.03325v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#35268;&#21010;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#27010;&#24565;&#34920;&#31034;&#12289;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#20197;&#21450;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#20851;&#32852;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35268;&#21010;&#27169;&#25311;&#20102;&#20154;&#31867;&#22312;&#25628;&#32034;&#21021;&#22987;&#35270;&#35273;&#29366;&#24577;&#21644;&#26368;&#32456;&#35270;&#35273;&#30446;&#26631;&#29366;&#24577;&#20043;&#38388;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#26102;&#25152;&#20570;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#35270;&#35273;&#20013;&#65292;&#35270;&#35273;&#35268;&#21010;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#22312;&#24341;&#23548;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25191;&#34892;&#26085;&#24120;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#35270;&#35273;&#35268;&#21010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;i&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#20195;&#30340;&#27010;&#24565;&#23398;&#20064;&#22120;&#65288;SCL&#65289;&#65292;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#20998;&#35299;&#30340;&#27010;&#24565;&#34920;&#31034;&#65307;ii&#65289;&#36890;&#36807;&#33258;&#23398;&#31526;&#21495;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#30340;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#65307;iii&#65289;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#35821;&#20041;&#30456;&#20284;&#30340;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#36827;&#34892;&#20851;&#32852;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#27169;&#22411;&#65288;ViCT&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#29366;&#24577;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21644;&#22240;&#26524;&#36716;&#25442;&#30340;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#65292;&#20197;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiv
&lt;/p&gt;</description></item><item><title>ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.00117</link><description>&lt;p&gt;
ABScribe: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00117
&lt;/p&gt;
&lt;p&gt;
ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#20070;&#20889;&#25991;&#26412;&#26469;&#25506;&#32034;&#26367;&#20195;&#24819;&#27861;&#26159;&#20889;&#20316;&#36807;&#31243;&#30340;&#20851;&#38190;&#12290;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#31616;&#21270;&#20889;&#20316;&#21464;&#21270;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30028;&#38754;&#23384;&#22312;&#21516;&#26102;&#32771;&#34385;&#22810;&#31181;&#21464;&#21270;&#30340;&#25361;&#25112;&#65306;&#22312;&#19981;&#35206;&#30422;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26032;&#30340;&#29256;&#26412;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#32780;&#25353;&#39034;&#24207;&#31896;&#36148;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;&#25991;&#26723;&#21464;&#24471;&#26434;&#20081;&#65292;&#22686;&#21152;&#24037;&#20316;&#37327;&#65292;&#24182;&#25171;&#26029;&#20316;&#32773;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ABScribe&#65292;&#19968;&#31181;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#19988;&#32467;&#26500;&#21270;&#22320;&#25506;&#32034;&#20889;&#20316;&#21464;&#21270;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;ABScribe&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;LLM&#25552;&#31034;&#24555;&#36895;&#20135;&#29983;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20250;&#33258;&#21160;&#36716;&#25442;&#25104;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#12290;&#21464;&#20307;&#22312;&#25991;&#26412;&#27573;&#33853;&#20013;&#34987;&#23384;&#20648;&#22312;&#30456;&#37051;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#19978;&#30340;&#40736;&#26631;&#24748;&#20572;&#20132;&#20114;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;12&#21517;&#25776;&#20889;&#20154;&#21592;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;ABScribe&#33021;&#26174;&#33879;&#20943;&#36731;&#20219;&#21153;&#36127;&#33655;&#65288;d = 1.20, p &lt; 0.001&#65289;&#65292;&#25552;&#39640;&#29992;&#25143;&#30340;&#35748;&#30693;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p &lt; 0.001), enhances user perceptions o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TRACE-GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26469;&#39044;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#30340;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11427</link><description>&lt;p&gt;
&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing. (arXiv:2309.11427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TRACE-GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26469;&#39044;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#30340;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TRACE-GPT&#65288;Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers&#65289;&#65292;&#23427;&#26159;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#26816;&#27979;&#25925;&#38556;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#65288;GPT&#65289;&#26469;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#23545;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Ch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#33324;&#21270;&#30028;&#38480;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#20449;&#24687;&#35770;&#21644;PAC-Bayesian&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20849;&#21516;&#28857;&#12290;&#36825;&#23545;&#20110;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#21644;&#26032;&#31639;&#27861;&#30340;&#35774;&#35745;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.04381</link><description>&lt;p&gt;
&#19968;&#33324;&#21270;&#30028;&#38480;&#65306;&#20449;&#24687;&#35770;&#21644;PAC-Bayesian&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds: Perspectives from Information Theory and PAC-Bayes. (arXiv:2309.04381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#33324;&#21270;&#30028;&#38480;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#20449;&#24687;&#35770;&#21644;PAC-Bayesian&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20849;&#21516;&#28857;&#12290;&#36825;&#23545;&#20110;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#21644;&#26032;&#31639;&#27861;&#30340;&#35774;&#35745;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#19968;&#33324;&#21270;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;PAC-Bayesian&#26041;&#27861;&#24050;&#32463;&#34987;&#30830;&#23450;&#20026;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#19968;&#33324;&#21270;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#23545;&#22810;&#31181;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#28508;&#22312;&#36866;&#29992;&#24615;&#65292;&#23427;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36824;&#21457;&#23637;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#65292;&#20854;&#20013;&#24314;&#31435;&#20102;&#19968;&#33324;&#21270;&#19982;&#21508;&#31181;&#20449;&#24687;&#24230;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20010;&#26694;&#26550;&#19982;PAC-Bayesian&#26041;&#27861;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#26377;&#29420;&#31435;&#21457;&#29616;&#30340;&#24456;&#22810;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#36825;&#31181;&#24378;&#36830;&#25509;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#19968;&#33324;&#21270;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#35270;&#35282;&#20849;&#21516;&#25317;&#26377;&#30340;&#25216;&#26415;&#21644;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#21644;&#35299;&#37322;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#36830;&#25509;&#22914;&#20309;&#20135;&#29983;&#26032;&#30340;&#27934;&#35265;&#21644;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#20132;&#21449;&#24212;&#29992;&#21644;&#28508;&#22312;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of generalization. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demons
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DropMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#38500;&#19968;&#23450;&#27604;&#20363;&#30340;&#25968;&#25454;&#26469;&#20943;&#23569;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;MSDA&#65289;&#20013;&#30340;&#31867;&#21035;&#30456;&#20851;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20043;&#21069;&#22240;&#20026;MSDA&#32780;&#19979;&#38477;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#22686;&#21152;&#25972;&#20307;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.09136</link><description>&lt;p&gt;
DropMix: &#20943;&#23569;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#20013;&#30340;&#31867;&#21035;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation. (arXiv:2307.09136v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DropMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#38500;&#19968;&#23450;&#27604;&#20363;&#30340;&#25968;&#25454;&#26469;&#20943;&#23569;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;MSDA&#65289;&#20013;&#30340;&#31867;&#21035;&#30456;&#20851;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20043;&#21069;&#22240;&#20026;MSDA&#32780;&#19979;&#38477;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#22686;&#21152;&#25972;&#20307;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;MSDA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#21508;&#31181;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MSDA&#30340;&#25928;&#26524;&#26159;&#19982;&#31867;&#21035;&#30456;&#20851;&#30340;&#65292;&#19968;&#20123;&#31867;&#21035;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#32780;&#20854;&#20182;&#31867;&#21035;&#21017;&#20986;&#29616;&#20102;&#19979;&#38477;&#12290;&#20026;&#20102;&#20943;&#23569;&#31867;&#21035;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DropMix&#26041;&#27861;&#65292;&#22312;MSDA&#35745;&#31639;&#20013;&#25490;&#38500;&#20102;&#29305;&#23450;&#27604;&#20363;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;MSDA&#21644;&#38750;MSDA&#25968;&#25454;&#30340;&#32452;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#20043;&#21069;&#30001;MSDA&#38477;&#20302;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#25972;&#20307;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#22312;&#20351;&#29992;&#19977;&#31181;MSDA&#26041;&#27861;&#65288;Mixup&#12289;CutMix&#21644;PuzzleMix&#65289;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;CIFAR-100&#21644;ImageNet&#65289;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed sample data augmentation (MSDA) is a widely used technique that has been found to improve performance in a variety of tasks. However, in this paper, we show that the effects of MSDA are class-dependent, with some classes seeing an improvement in performance while others experience a decline. To reduce class dependency, we propose the DropMix method, which excludes a specific percentage of data from the MSDA computation. By training on a combination of MSDA and non-MSDA data, the proposed method not only improves the performance of classes that were previously degraded by MSDA, but also increases overall average accuracy, as shown in experiments on two datasets (CIFAR-100 and ImageNet) using three MSDA methods (Mixup, CutMix and PuzzleMix).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.16772</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#30340;&#20154;&#31867;&#22242;&#38431;&#27963;&#21160;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16772
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#23545;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#30340;&#29702;&#35299;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#20219;&#21153;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;M3Act&#37319;&#29992;Unity&#24341;&#25806;&#39537;&#21160;&#65292;&#21253;&#21547;&#21487;&#20379;&#20223;&#30495;&#20351;&#29992;&#30340;&#19977;&#32500;&#22330;&#26223;&#21644;&#20154;&#29289;&#36164;&#28304;&#65292;&#21487;&#37197;&#32622;&#30340;&#29031;&#26126;&#21644;&#25668;&#20687;&#31995;&#32479;&#65292;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22359;&#21270;&#22242;&#38431;&#27963;&#21160;&#65292;&#20197;&#21450;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#12289;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#12289;2D&#23039;&#21183;&#12289;3D&#21160;&#20316;&#65289;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;2D&#36793;&#30028;&#26694;&#12289;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#12289;&#20010;&#20307;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#31867;&#21035;&#65289;&#12290;&#21033;&#29992;M3Act&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#20445;AI&#31995;&#32479;&#36981;&#24490;&#35268;&#23450;&#65292;&#25552;&#20986;&#20102;&#24403;&#21069;&#21644;&#28508;&#22312;&#21487;&#33021;&#30340;&#25216;&#26415;&#23454;&#29616;&#65292;&#20197;&#21450;&#38656;&#35201;&#36328;&#23398;&#31185;&#26041;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12609</link><description>&lt;p&gt;
&#8220;&#26500;&#24314;&#21487;&#31649;&#25511;&#30340;AI&#31995;&#32479;&#65306;&#25216;&#26415;&#25361;&#25112;&#21644;&#25919;&#31574;&#26426;&#36935;&#8221;
&lt;/p&gt;
&lt;p&gt;
Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities. (arXiv:2306.12609v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#20445;AI&#31995;&#32479;&#36981;&#24490;&#35268;&#23450;&#65292;&#25552;&#20986;&#20102;&#24403;&#21069;&#21644;&#28508;&#22312;&#21487;&#33021;&#30340;&#25216;&#26415;&#23454;&#29616;&#65292;&#20197;&#21450;&#38656;&#35201;&#36328;&#23398;&#31185;&#26041;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22914;&#20309;&#35268;&#33539;AI&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#34987;&#32473;&#20104;&#12290;&#22312;&#31649;&#25511;&#26426;&#26500;&#21162;&#21147;&#30830;&#23450;&#35201;&#23553;&#35013;&#21040;&#27861;&#35268;&#20013;&#30340;&#20215;&#20540;&#35266;&#26102;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#20854;&#20013;&#30340;&#25216;&#26415;&#38382;&#39064;&#65306;AI&#19987;&#23478;&#20204;&#33021;&#22815;&#23457;&#26597;AI&#31995;&#32479;&#26159;&#21542;&#36981;&#23432;&#30417;&#31649;&#35201;&#27714;&#30340;&#31243;&#24230;&#26159;&#22810;&#23569;&#65311;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20844;&#20849;&#37096;&#38376;&#30340;&#37319;&#36141;&#28165;&#21333;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#23450;&#20041;&#20102;&#19977;&#20010;&#26041;&#38754;&#65306;&#30446;&#21069;&#26377;&#21738;&#20123;&#26041;&#38754;&#26159;&#25105;&#20204;&#21487;&#20197;&#20570;&#30340;&#65307;&#21033;&#29992;AI&#25216;&#26415;&#21019;&#26032;&#25105;&#20204;&#33021;&#20570;&#21040;&#21738;&#20123;&#26041;&#38754;&#65307; &#21738;&#20123;&#35201;&#27714;&#38656;&#35201;&#26356;&#22810;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing attention being given to how to regulate AI systems. As governing bodies grapple with what values to encapsulate into regulation, we consider the technical half of the question: To what extent can AI experts vet an AI system for adherence to regulatory requirements? We investigate this question through two public sector procurement checklists, identifying what we can do now, what we should be able to do with technical innovation in AI, and what requirements necessitate a more interdisciplinary approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09459</link><description>&lt;p&gt;
&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#38761;&#24615;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#65292;&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#20195;&#29702;&#30340;&#21382;&#21490;&#21487;&#20197;&#34920;&#31034;&#20026;&#24207;&#21015;&#65292;&#24182;&#19988;&#25972;&#20010;&#20219;&#21153;&#21487;&#20197;&#32553;&#20943;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Atari&#28216;&#25103;&#19978;&#26174;&#30528;&#20248;&#20110;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20180;&#32454;&#30740;&#31350;&#20102;&#35760;&#24518;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32489;&#25928;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24320;&#21457;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04357</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26816;&#32034;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#21382;&#21490;&#35760;&#24405;&#20174;&#20960;&#20010;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21518;&#35757;&#32451;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#32431;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;IR&#31038;&#21306;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#25991;&#26412;&#34920;&#31034;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#23545;&#35805;&#35821;&#20041;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dial-MAE&#65288;&#23545;&#35805;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;&#12290; Dial-MAE&#20351;&#29992;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23398;&#20064;&#23558;&#23545;&#35805;&#30340;&#35821;&#20041;&#26356;&#22909;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#21521;&#37327;&#20013;&#12290; Dial-MAE&#30340;&#36807;&#31243;&#21253;&#25324;&#30001;&#28145;&#24230;&#32534;&#30721;&#22120;&#21019;&#24314;&#24102;&#26377;&#25513;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#23884;&#20837;&#65292;&#28982;&#21518;&#26159;&#27973;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#20351;&#29992;&#27492;&#23884;&#20837;&#20197;&#21450;&#19978;&#19979;&#25991;&#21521;&#37327;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#8212;&#8212;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14258</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#65306;&#32479;&#19968;&#30340;&#37096;&#20998;AUC&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised AUC Optimization: A Unified Partial AUC Approach. (arXiv:2305.14258v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#8212;&#8212;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33719;&#21462;&#23436;&#32654;&#30340;&#30417;&#30563;&#36890;&#24120;&#24456;&#22256;&#38590;&#65292;&#29616;&#23454;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#19981;&#20934;&#30830;&#12289;&#19981;&#23436;&#25972;&#25110;&#19981;&#31934;&#30830;&#30340;&#30417;&#30563;&#65292;&#32479;&#31216;&#20026;&#24369;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#28085;&#30422;&#20102;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#12290;&#22312;WSAUC&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#26368;&#23567;&#21270;&#21463;&#27745;&#26579;&#38598;&#21512;&#19978;AUC&#39118;&#38505;&#30340;&#24120;&#35265;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#19982;&#30495;&#23454;AUC&#19968;&#33268;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#65292;&#21363;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#23427;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#22312;&#23384;&#22312;&#27745;&#26579;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#20316;&#29992;&#12290;WSAUC&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since acquiring perfect supervision is usually difficult, real-world machine learning tasks often confront inaccurate, incomplete, or inexact supervision, collectively referred to as weak supervision. In this work, we present WSAUC, a unified framework for weakly supervised AUC optimization problems, which covers noisy label learning, positive-unlabeled learning, multi-instance learning, and semi-supervised learning scenarios. Within the WSAUC framework, we first frame the AUC optimization problems in various weakly supervised scenarios as a common formulation of minimizing the AUC risk on contaminated sets, and demonstrate that the empirical risk minimization problems are consistent with the true AUC. Then, we introduce a new type of partial AUC, specifically, the reversed partial AUC (rpAUC), which serves as a robust training objective for AUC maximization in the presence of contaminated labels. WSAUC offers a universal solution for AUC optimization in various weakly supervised scena
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.13525</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#36890;&#20449;&#30340;&#24322;&#27493;&#24352;&#37327;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Communication-minimizing Asynchronous Tensor Parallelism. (arXiv:2305.13525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#25193;&#22823;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#35774;&#35745;&#33021;&#22815;&#22312;&#22810;GPU&#38598;&#32676;&#19978;&#39640;&#25928;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#30340;&#24182;&#34892;&#31639;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#19977;&#32500;&#65288;3D&#65289;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22823;&#22411;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24182;&#34892;&#35757;&#32451;&#20013;&#30001;&#36890;&#20449;&#24341;&#36215;&#30340;&#31354;&#38386;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20998;&#24067;&#26041;&#24335;&#65292;&#28040;&#38500;&#20102;&#20026;&#28385;&#36275;&#21508;&#23618;&#25968;&#25454;&#20381;&#36182;&#32780;&#38656;&#35201;&#30340;&#36890;&#20449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35757;&#32451;&#36807;&#31243;&#36229;&#20998;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36890;&#20449;&#19982;&#35745;&#31639;&#30340;&#37325;&#21472;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#31354;&#38386;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#27169;&#22411;&#65292;&#24110;&#21161;&#29992;&#25143;&#20026;&#32473;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#36890;&#20449;&#26368;&#20248;&#30340;&#21487;&#29992;&#30828;&#20214;&#36164;&#28304;&#20998;&#35299;&#12290; &#23545;&#20110;256 A100 GPU&#19978;&#30340;28B&#21442;&#25968;CNN&#65292;&#22312;&#26412;&#25991;&#30340; Tensor3D &#26041;&#27861;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604; GPU &#31354;&#38386;&#26102;&#38388;&#20063;&#38477;&#20302;&#20102;&#32422;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
As state-of-the-art neural networks scale to billions of parameters, designing parallel algorithms that can train these networks efficiently on multi-GPU clusters has become critical. This paper presents Tensor3D, a novel three-dimensional (3D) approach to parallelize tensor computations, that strives to minimize the idle time incurred due to communication in parallel training of large multi-billion parameter models. First, we introduce an intelligent distribution of neural network parameters across GPUs that eliminates communication required for satisfying data dependencies of individual layers. Then, we propose a novel overdecomposition of the parallel training process, using which we achieve significant overlap of communication with computation, thereby reducing GPU idle time. Finally, we present a communication model, which helps users identify communication optimal decompositions of available hardware resources for a given neural network. For a 28B parameter CNN on 256 A100 GPUs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02151</link><description>&lt;p&gt;
&#35821;&#35328;&#36317;&#31163;&#19982;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02151
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#19981;&#21516;&#35821;&#35328;&#29305;&#24449;&#23545;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#31181;&#25928;&#24212;&#22914;&#20309;&#26144;&#23556;&#21040;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24494;&#35843;&#26399;&#38388;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#19978;&#30340;&#24433;&#21709;&#65292;&#32780;&#26412;&#30740;&#31350;&#30740;&#31350;&#30340;&#26159;&#30001;MLLMs&#29983;&#25104;&#30340;&#30456;&#24212;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#30340;&#32477;&#23545;&#28436;&#21464;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#35821;&#35328;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#24182;&#35843;&#26597;&#20854;&#19982;&#34920;&#31034;&#31354;&#38388;&#21644;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#35828;&#26126;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#19978;&#30340;&#24212;&#29992;&#65292;&#39318;&#27425;&#23545;&#19977;&#20010;&#24120;&#29992;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#23454;&#39564;&#21457;&#29616;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36824;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;</title><link>http://arxiv.org/abs/2304.06427</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;ECG&#34920;&#24449;&#23398;&#20064;&#22312;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#65306;&#20998;&#24067;&#20998;&#26512;&#21450;&#23454;&#39564;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Distribution and Out-of-Distribution Self-supervised ECG Representation Learning for Arrhythmia Detection. (arXiv:2304.06427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#19978;&#30340;&#24212;&#29992;&#65292;&#39318;&#27425;&#23545;&#19977;&#20010;&#24120;&#29992;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#23454;&#39564;&#21457;&#29616;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36824;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24515;&#30005;&#22270;(ECG)&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;(Self-Supervised Learning, SSL)&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#19977;&#20010;&#24120;&#29992;&#30340;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#20102;&#32508;&#21512;&#24615;&#23454;&#39564;&#65292;&#20351;&#29992;&#19981;&#21516;&#22686;&#24378;&#21644;&#21442;&#25968;&#35780;&#20272;&#20102;&#21508;&#31181;SSL&#26041;&#27861;&#65288;&#22914;SimCRL&#12289;BYOL&#21644;SwAV&#65289;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#38024;&#23545;In-Distribution (ID)&#21644;Out-of-Distribution (OOD) ECG&#25968;&#25454;&#30340;&#20132;&#21449;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#27979;&#35797;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SSL&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;SwAV&#65292;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;&#31181;&#31867;&#30340;ECG&#25968;&#25454;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a systematic investigation into the effectiveness of Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia detection. We begin by conducting a novel distribution analysis on three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To the best of our knowledge, our study is the first to quantify these distributions in this area. We then perform a comprehensive set of experiments using different augmentations and parameters to evaluate the effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG representation learning, where we observe the best performance achieved by SwAV. Furthermore, our analysis shows that SSL methods achieve highly competitive results to those achieved by supervised state-of-the-art methods. To further assess the performance of these methods on both In-Distribution (ID) and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and testing experiments. Our comprehensive
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.17251</link><description>&lt;p&gt;
&#25581;&#24320;&#23545;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#35823;&#35299;
&lt;/p&gt;
&lt;p&gt;
Demystifying Misconceptions in Social Bots Research. (arXiv:2303.17251v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17251
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#31185;&#23398;&#23547;&#27714;&#35299;&#20915;&#32593;&#32476;&#34394;&#20551;&#20449;&#24687;&#26368;&#21463;&#20105;&#35758;&#30340;&#24418;&#24335;&#20043;&#19968;&#30340;&#30693;&#35782;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#21463;&#21040;&#26222;&#36941;&#30340;&#20559;&#35265;&#12289;&#22840;&#22823;&#30340;&#32467;&#26524;&#21644;&#35823;&#35299;&#30340;&#22256;&#25200;&#65292;&#36825;&#20123;&#37117;&#20026;&#27495;&#20041;&#12289;&#19981;&#20999;&#23454;&#38469;&#30340;&#26399;&#26395;&#21644;&#30475;&#20284;&#26080;&#27861;&#35843;&#21644;&#30340;&#21457;&#29616;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#37325;&#30003;&#31185;&#23398;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20462;&#35746;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#20013;&#30340;&#19968;&#20123;&#26368;&#26032;&#32467;&#26524;&#65292;&#24378;&#35843;&#21644;&#32416;&#27491;&#20102;&#20107;&#23454;&#38169;&#35823;&#20197;&#21450;&#26041;&#27861;&#35770;&#21644;&#27010;&#24565;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25581;&#24320;&#20102;&#26222;&#36941;&#30340;&#35823;&#35299;&#65292;&#35299;&#20915;&#20102;&#26377;&#20851;&#22914;&#20309;&#35752;&#35770;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#30830;&#23450;&#24182;&#39539;&#26021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#24120;&#29992;&#30340;&#35884;&#35823;&#35770;&#35777;&#65292;&#25903;&#25345;&#36825;&#31181;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The science of social bots seeks knowledge and solutions to one of the most debated forms of online misinformation. Yet, social bots research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. In this contribution we revise some recent results in social bots research, highlighting and correcting factual errors as well as methodological and conceptual issues. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss misinformation research in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research as
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#26234;&#21147;&#30340;&#23616;&#38480;&#24615;&#23548;&#33268;&#25216;&#26415;&#27010;&#24565;&#21019;&#36896;&#25918;&#32531;&#21644;&#21407;&#21019;&#24615;&#19979;&#38477;&#65292;&#22240;&#27492;&#24314;&#35758;&#24320;&#21457;&#21644;&#23454;&#26045;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#21019;&#26032;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.13300</link><description>&lt;p&gt;
&#21019;&#26032;&#25918;&#32531;&#65306;&#25216;&#26415;&#27010;&#24565;&#21019;&#36896;&#30340;&#20943;&#36895;&#21450;&#26032;&#25216;&#26415;&#27010;&#24565;&#21407;&#21019;&#24615;&#30340;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Innovation Slowdown: Decelerating Concept Creation and Declining Originality in New Technological Concepts. (arXiv:2303.13300v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13300
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#21147;&#30340;&#23616;&#38480;&#24615;&#23548;&#33268;&#25216;&#26415;&#27010;&#24565;&#21019;&#36896;&#25918;&#32531;&#21644;&#21407;&#21019;&#24615;&#19979;&#38477;&#65292;&#22240;&#27492;&#24314;&#35758;&#24320;&#21457;&#21644;&#23454;&#26045;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#21019;&#26032;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20043;&#21069;&#30340;&#27010;&#24565;&#37325;&#29992;&#12289;&#37325;&#32452;&#21644;&#21512;&#25104;&#36827;&#34892;&#26032;&#25216;&#26415;&#27010;&#24565;&#30340;&#21019;&#36896;&#21487;&#33021;&#20250;&#23548;&#33268;&#27010;&#24565;&#31354;&#38388;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#30001;&#19987;&#21033;&#25991;&#26412;&#20013;&#36229;&#36807;400&#19975;&#20010;&#27010;&#24565;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#25216;&#26415;&#35821;&#20041;&#32593;&#32476;&#36827;&#34892;&#30340;&#32479;&#35745;&#20998;&#26512;&#21457;&#29616;&#65292;&#27010;&#24565;&#21019;&#36896;&#30340;&#27493;&#20240;&#22312;&#25345;&#32493;&#20943;&#32531;&#65292;&#24182;&#19988;&#26032;&#21019;&#36896;&#20986;&#30340;&#27010;&#24565;&#30340;&#21407;&#21019;&#24615;&#26377;&#25152;&#19979;&#38477;&#12290;&#36825;&#20123;&#36235;&#21183;&#21487;&#20197;&#24402;&#22240;&#20110;&#20154;&#31867;&#26234;&#21147;&#22312;&#21019;&#26032;&#36229;&#20986;&#29616;&#26377;&#25216;&#26415;&#30340;&#25299;&#23637;&#31354;&#38388;&#26041;&#38754;&#30340;&#23616;&#38480;&#12290;&#20026;&#20102;&#20445;&#25345;&#21019;&#26032;&#65292;&#25105;&#20204;&#24314;&#35758;&#24320;&#21457;&#21644;&#23454;&#26045;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#65292;&#20197;&#22686;&#24378;&#21019;&#26032;&#36807;&#31243;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#23398;&#20064;&#12289;&#21019;&#36896;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The creation of new technological concepts through design reuses, recombination, and synthesis of prior concepts to create new ones may lead to exponential growth of the concept space over time. However, our statistical analysis of a large-scale technology semantic network consisting of over four million concepts from patent texts found evidence of a persistent deceleration in the pace of concept creation and a decline in the originality of newly created concepts. These trends may be attributed to the limitations of human intelligence in innovating beyond an expanding space of prior art. To sustain innovation, we recommend the development and implementation of creative artificial intelligence that can augment various aspects of the innovation process, including learning, creation, and evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.12091</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#26631;&#35760;&#25968;&#25454;&#12289;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#32771;&#34385;&#21040;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#26631;&#35760;&#25968;&#25454;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#31867;&#21035;&#65288;&#24322;&#24120;&#20540;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;ANEDL&#65289;&#65292;&#20197;&#24212;&#23545;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#22914;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#21644;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20316;&#20026;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#22120;&#26469;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;EDL&#26356;&#21152;&#36866;&#21512;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;ANEDL&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
&lt;/p&gt;</description></item><item><title>CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2303.10365</link><description>&lt;p&gt;
CroSel: &#29992;&#20110;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#33258;&#20449;&#20266;&#26631;&#31614;&#30340;&#36328;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10365
&lt;/p&gt;
&lt;p&gt;
CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;(PLL)&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#26377;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;ground-truth&#26631;&#31614;&#12290;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#20102;&#22522;&#20110;&#35782;&#21035;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;PLL&#20013;&#30340;&#26631;&#31614;&#27495;&#20041;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#30495;&#23454;&#26631;&#31614;&#35270;&#20026;&#35201;&#35782;&#21035;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#21644;&#23436;&#25972;&#22320;&#35782;&#21035;&#30495;&#23454;&#26631;&#31614;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20250;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23548;&#33268;&#20266;&#26631;&#31614;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CroSel&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#27169;&#22411;&#30340;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#26469;&#35782;&#21035;&#22823;&#22810;&#25968;&#35757;&#32451;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#21449;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#24471;&#20004;&#20010;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#30456;&#20114;&#36873;&#25321;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;co-mix&#65292;&#20197;&#36991;&#20813;&#22240;&#34394;&#20551;&#36873;&#25321;&#32780;&#24341;&#36215;&#30340;&#26679;&#26412;&#28010;&#36153;&#21644;&#24494;&#23567;&#22122;&#22768;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CroSel&#33021;&#22815;&#25361;&#36873;&#20986;&#22823;&#22810;&#25968;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09618</link><description>&lt;p&gt;
HIVE&#65306;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#21040;&#20154;&#31867;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20551;&#35774;&#65292;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65292;&#20854;&#36755;&#20986;&#22522;&#20110;&#36755;&#20837;&#22270;&#20687;&#21644;&#32534;&#36753;&#25351;&#20196;&#65292;&#21516;&#26679;&#21487;&#20197;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;&#20854;&#36755;&#20986;&#21487;&#33021;&#19981;&#31526;&#21512;&#29992;&#25143;&#30340;&#27491;&#30830;&#25351;&#20196;&#21644;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#65288;HIVE&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#32534;&#36753;&#30340;&#22270;&#20687;&#19978;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#20197;&#25429;&#25417;&#22522;&#30784;&#29992;&#25143;&#20559;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#21487;&#25193;&#23637;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#26681;&#25454;&#20272;&#35745;&#30340;&#22870;&#21169;&#20540;&#34701;&#20837;&#20154;&#31867;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#20026;&#20943;&#36731;&#25968;&#25454;&#38480;&#21046;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;1M&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;3.6K&#22870;&#21169;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#22870;&#21169;&#23398;&#20064;&#65292;&#20197;&#21450;1K&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of inst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#24724;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#35266;&#27979;&#20013;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2302.06912</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#24724;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regret-Based Optimization for Robust Reinforcement Learning. (arXiv:2302.06912v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#24724;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#35266;&#27979;&#20013;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23545;&#35266;&#27979;&#20013;&#30340;&#24494;&#23567;&#23545;&#25239;&#24615;&#22122;&#22768;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#31181;&#23545;&#25239;&#24615;&#22122;&#22768;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#21487;&#33021;&#36896;&#25104;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#29616;&#26377;&#30340;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#35266;&#27979;&#25200;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;&#20027;&#35201;&#38598;&#20013;&#22312;&#36845;&#20195;&#25913;&#36827;&#27599;&#20010;&#36845;&#20195;&#20013;&#29983;&#25104;&#30340;&#23545;&#25239;&#31034;&#20363;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#23545;&#26222;&#36890;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#26159;&#34987;&#21160;&#24615;&#30340;&#65292;&#22914;&#26524;&#26576;&#20123;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#20135;&#29983;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#34920;&#29616;&#24471;&#26356;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36861;&#27714;&#19968;&#31181;&#26356;&#31215;&#26497;&#30340;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#30452;&#25509;&#20248;&#21270;&#19968;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#40065;&#26834;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable to small adversarial noise in observations. Such adversarial noise can have disastrous consequences in safety-critical environments. For instance, a self-driving car receiving adversarially perturbed sensory observations about nearby signs (e.g., a stop sign physically altered to be perceived as a speed limit sign) or objects (e.g., cars altered to be recognized as trees) can be fatal. Existing approaches for making RL algorithms robust to an observation-perturbing adversary have focused on reactive approaches that iteratively improve against adversarial examples generated at each iteration. While such approaches have been shown to provide improvements over regular RL methods, they are reactive and can fare significantly worse if certain categories of adversarial examples are not generated during training. To that end, we pursue a more proactive approach that relies on directly optimizing a well-studied robustn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36861;&#38543;&#32773;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26031;&#22612;&#20811;&#36125;&#26684;&#21338;&#24328;&#20013;&#30340;&#38382;&#39064;&#65292;&#19982;&#20854;&#20182;&#29616;&#26377;&#20316;&#21697;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#20351;&#29992;&#39044;&#20272;&#22120;&#25110;&#20102;&#35299;&#36861;&#38543;&#32773;&#30340;&#31574;&#30053;&#12289;&#25928;&#29992;&#20989;&#25968;&#12290;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#23618;&#24490;&#29615;&#31639;&#27861;&#65292;&#36890;&#36807;&#29305;&#21035;&#35774;&#35745;&#30340;&#31574;&#30053;&#25506;&#27979;&#36861;&#38543;&#32773;&#26469;&#26356;&#26032;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#36861;&#38543;&#32773;&#30340;&#32852;&#21512;&#31574;&#30053;&#25910;&#25947;&#20110;&#22343;&#34913;&#12290;&#38750;&#28176;&#36827;&#25910;&#25947;&#36895;&#24230;&#21040;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#31283;&#23450;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#28176;&#36827;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2302.01421</link><description>&lt;p&gt;
Follower Agnostic Methods for Stackelberg Games&#65288;&#36866;&#29992;&#20110;&#26031;&#22612;&#20811;&#36125;&#26684;&#21338;&#24328;&#30340;&#36861;&#38543;&#32773;&#19981;&#21487;&#30693;&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
Follower Agnostic Methods for Stackelberg Games. (arXiv:2302.01421v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36861;&#38543;&#32773;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26031;&#22612;&#20811;&#36125;&#26684;&#21338;&#24328;&#20013;&#30340;&#38382;&#39064;&#65292;&#19982;&#20854;&#20182;&#29616;&#26377;&#20316;&#21697;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#20351;&#29992;&#39044;&#20272;&#22120;&#25110;&#20102;&#35299;&#36861;&#38543;&#32773;&#30340;&#31574;&#30053;&#12289;&#25928;&#29992;&#20989;&#25968;&#12290;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#23618;&#24490;&#29615;&#31639;&#27861;&#65292;&#36890;&#36807;&#29305;&#21035;&#35774;&#35745;&#30340;&#31574;&#30053;&#25506;&#27979;&#36861;&#38543;&#32773;&#26469;&#26356;&#26032;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#36861;&#38543;&#32773;&#30340;&#32852;&#21512;&#31574;&#30053;&#25910;&#25947;&#20110;&#22343;&#34913;&#12290;&#38750;&#28176;&#36827;&#25910;&#25947;&#36895;&#24230;&#21040;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#31283;&#23450;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#28176;&#36827;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20197;&#36861;&#38543;&#32773;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#35299;&#20915;&#26031;&#22612;&#20811;&#36125;&#26684;&#21338;&#24328;&#30340;&#19968;&#31867;&#38382;&#39064;&#65288;&#21487;&#33021;&#26377;&#22810;&#20010;&#36861;&#38543;&#32773;&#65289;&#12290;&#19982;&#20854;&#20182;&#24403;&#20195;&#20316;&#21697;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#20351;&#29992;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#26799;&#24230;&#30340;&#39044;&#20272;&#22120;&#65292;&#20063;&#19981;&#38656;&#35201;&#20102;&#35299;&#36861;&#38543;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#25110;&#31574;&#30053;&#31354;&#38388;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#23618;&#24490;&#29615;&#31639;&#27861;&#65292;&#20854;&#20013;&#39046;&#23548;&#32773;&#20351;&#29992;&#36890;&#36807;&#29992;&#29305;&#21035;&#35774;&#35745;&#30340;&#31574;&#30053;&#25506;&#27979;&#36861;&#38543;&#32773;&#33719;&#24471;&#30340;&#29305;&#27530;&#26500;&#36896;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#22312;&#25509;&#25910;&#21040;&#36825;&#20123;&#31574;&#30053;&#21518;&#65292;&#36861;&#38543;&#32773;&#37319;&#29992;&#36866;&#24212;&#35268;&#21017;&#65292;&#20351;&#24471;&#36861;&#38543;&#32773;&#30340;&#32852;&#21512;&#31574;&#30053;&#25910;&#25947;&#20110;&#22343;&#34913;&#29366;&#24577;&#65292;&#36825;&#26159;&#39046;&#23548;&#32773;&#35266;&#23519;&#21040;&#30340;&#21807;&#19968;&#20449;&#24687;&#65292;&#29992;&#20110;&#26500;&#36896;&#21069;&#36848;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#28176;&#36827;&#25910;&#25947;&#36895;&#24230;&#21040;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#31283;&#23450;&#28857;&#65292;&#23613;&#31649;&#38381;&#29615;&#20989;&#25968;&#19981;&#20855;&#26377;&#20984;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#28176;&#36827;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an algorithm to solve a class of Stackelberg games (possibly with multiple followers) in a follower agnostic manner. Particularly, unlike other contemporary works, our algorithm does not require the use of an oracle estimator for the gradient of the leader's objective or knowledge about the follower's utility function or strategy space. Instead, we design two-loop algorithm where the leader updates its strategies using specially constructed gradient estimator obtained by probing followers with specially designed strategies. Upon receiving the followers engage in an adaptation rule such that the joint strategy of followers converges near equilibrium which is the only information observed by leader to construct the aforementioned gradient estimator. We provide non-asymptotic convergence rates to stationary points of the leader's objective in the absence of convexity of the closed-loop function and further show asymptotic convergence to a local minima of the leader's objective.
&lt;/p&gt;</description></item></channel></rss>