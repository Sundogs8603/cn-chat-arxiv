<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>FormalGeo&#26159;&#19968;&#31181;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#65292;&#33021;&#22815;&#21033;&#29992;&#29616;&#20195;AI&#27169;&#22411;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;AI&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#35299;&#20915;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#35835;&#12289;&#36861;&#28335;&#21644;&#21487;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.18021</link><description>&lt;p&gt;
FormalGeo&#65306;&#36808;&#21521;&#20154;&#31867;&#32423;IMO&#27700;&#24179;&#20960;&#20309;&#33258;&#21160;&#25512;&#29702;&#30340;&#31532;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
FormalGeo: The First Step Toward Human-like IMO-level Geometric Automated Reasoning. (arXiv:2310.18021v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18021
&lt;/p&gt;
&lt;p&gt;
FormalGeo&#26159;&#19968;&#31181;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#65292;&#33021;&#22815;&#21033;&#29992;&#29616;&#20195;AI&#27169;&#22411;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;AI&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#35299;&#20915;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#35835;&#12289;&#36861;&#28335;&#21644;&#21487;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#25105;&#20204;&#36807;&#21435;&#21313;&#24180;&#24037;&#20316;&#30340;&#31532;&#19968;&#31687;&#25991;&#31456;&#12290;&#22312;&#36825;&#19968;&#31995;&#21015;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#12290;&#36825;&#23558;&#20316;&#20026;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#25361;&#25112;&#19982;&#21487;&#35835;&#30340;AI&#33258;&#21160;&#25512;&#29702;&#20043;&#38388;&#30340;&#20851;&#38190;&#26725;&#26753;&#12290;&#26377;&#20102;&#36825;&#20010;&#27491;&#24335;&#31995;&#32479;&#65292;&#25105;&#20204;&#33021;&#22815;&#26080;&#32541;&#22320;&#23558;&#29616;&#20195;AI&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#27491;&#24335;&#31995;&#32479;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#22312;&#36825;&#20010;&#27491;&#24335;&#26694;&#26550;&#20869;&#65292;AI&#29616;&#22312;&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#23545;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36825;&#20123;&#35777;&#26126;&#26159;&#21487;&#35835;&#30340;&#12289;&#21487;&#36861;&#28335;&#30340;&#21644;&#21487;&#39564;&#35777;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#24418;&#24335;&#21270;&#29702;&#35770;&#65288;GFT&#65289;&#26469;&#25351;&#23548;&#20960;&#20309;&#24418;&#24335;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#22522;&#20110;GFT&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;FormalGeo&#65292;&#21253;&#25324;88&#20010;&#20960;&#20309;&#35859;&#35789;&#21644;196&#20010;&#23450;&#29702;&#12290;&#23427;&#33021;&#22815;&#34920;&#31034;&#12289;&#39564;&#35777;&#21644;&#35299;&#20915;IMO&#32423;&#20960;&#20309;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;Python&#24320;&#21457;&#20102;FGPS&#65288;&#27491;&#24335;&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;&#22120;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is the first article of our work over the past decade. In this series of papers, we have constructed a complete and compatible formal plane geometry system. This will serve as a crucial bridge between IMO-level plane geometry challenges and readable AI automated reasoning. With this formal system in place, we have been able to seamlessly integrate modern AI models with our formal system. Within this formal framework, AI is now capable of providing deductive reasoning solutions to IMO-level plane geometry problems, just like handling other natural languages, and these proofs are readable, traceable, and verifiable. We propose the geometry formalization theory (GFT) to guide the development of the geometry formal system. Based on the GFT, we have established the FormalGeo, which consists of 88 geometric predicates and 196 theorems. It can represent, validate, and solve IMO-level geometry problems. we also have crafted the FGPS (formal geometry problem solver) in Python. It serves as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.17658</link><description>&lt;p&gt;
&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21333;&#19968;&#32447;&#24615;&#23618;&#30340;&#36890;&#36947;&#30456;&#20851;(CD)&#25110;&#36890;&#36947;&#29420;&#31435;(CI)&#24314;&#27169;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;CD&#21644;CI&#35270;&#20026;&#20004;&#31181;&#20114;&#34917;&#20294;&#20114;&#26021;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#21516;&#26102;&#21033;&#29992;&#36825;&#20004;&#20010;&#26497;&#31471;&#12290;&#32780;&#19988;&#65292;CD&#21644;CI&#37117;&#26159;&#38745;&#24577;&#31574;&#30053;&#65292;&#26080;&#27861;&#22312;&#27809;&#26377;&#22823;&#37327;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26159;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;CI&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;CSC&#65288;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#65289;&#65292;&#29992;&#20110;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#22686;&#24378;&#20102;CI&#31574;&#30053;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20943;&#23567;&#20102;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
&lt;/p&gt;</description></item><item><title>HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15318</link><description>&lt;p&gt;
HetGPT: &#21033;&#29992;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25552;&#31034;&#35843;&#25972;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15318
&lt;/p&gt;
&lt;p&gt;
HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20026;&#34920;&#31034;&#21644;&#20998;&#26512;Web&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20016;&#23500;&#20449;&#24687;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#20351;&#24471;&#22312;&#32447;&#39029;&#38754;&#20998;&#31867;&#21644;&#31038;&#20132;&#25512;&#33616;&#31561;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#8220;&#39044;&#35757;&#32451;&#65292;&#24494;&#35843;&#8221;&#33539;&#24335;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#23384;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36825;&#31181;&#24046;&#36317;&#21487;&#33021;&#23548;&#33268;&#8220;&#36127;&#36716;&#31227;&#8221;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#20852;&#36215;&#34920;&#26126;&#20102;&#23558;&#8220;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#8221;&#33539;&#24335;&#24212;&#29992;&#20110;&#22270;&#24418;&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24418;&#25552;&#31034;&#25216;&#26415;&#38024;&#23545;&#30340;&#26159;&#21516;&#36136;&#22270;&#65292;&#24573;&#35270;&#20102;Web&#22270;&#30340;&#20869;&#22312;&#24322;&#26500;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HetGPT&#65292;
&lt;/p&gt;
&lt;p&gt;
Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a 
&lt;/p&gt;</description></item><item><title>PoisonPrompt&#26159;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#32463;&#36807;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12439</link><description>&lt;p&gt;
PoisonPrompt: &#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models. (arXiv:2310.12439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12439
&lt;/p&gt;
&lt;p&gt;
PoisonPrompt&#26159;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#32463;&#36807;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#31034;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#21508;&#31181;LLM&#24212;&#29992;&#22330;&#26223;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#32780;&#35328;&#65292;&#21518;&#38376;&#28431;&#27934;&#8212;&#8212;&#19968;&#31181;&#21487;&#20197;&#24694;&#24847;&#26356;&#25913;&#21463;&#23475;&#27169;&#22411;&#27491;&#24120;&#39044;&#27979;&#30340;&#20005;&#37325;&#23433;&#20840;&#23041;&#32961;&#8212;&#8212;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#25915;&#20987;POISONPROMPT&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#30828;&#20214;&#21644;&#36719;&#20214;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#27969;&#34892;&#30340;&#25552;&#31034;&#26041;&#27861;&#12289;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#26469;&#35780;&#20272;POISONPROMPT&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompts have significantly improved the performance of pretrained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present POISONPROMPT, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of POISONPROMPT through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.
&lt;/p&gt;</description></item><item><title>Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03059</link><description>&lt;p&gt;
Point-PEFT: &#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03059
&lt;/p&gt;
&lt;p&gt;
Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27969;&#34892;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31561;&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#25104;&#26412;&#65292;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#35328;&#21644;2D&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19987;&#38376;PEFT&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-PEFT&#65292;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#28857;&#20113;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;3D&#27169;&#22411;&#65292;&#25105;&#20204;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#12290;Point-prior Prompt&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20869;&#23384;&#24211;&#26469;&#22686;&#24378;&#25552;&#31034;&#26631;&#35760;&#30340;&#21442;&#25968;&#26080;&#20851;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;Geometry-aware Adapter&#26088;&#22312;&#23545;&#19981;&#21516;&#20219;&#21153;&#25110;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
&lt;/p&gt;</description></item><item><title>MagicDrive&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#24182;&#25429;&#25417;&#20102;&#32454;&#33268;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.02601</link><description>&lt;p&gt;
MagicDrive: &#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#19979;&#30340;&#34903;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02601
&lt;/p&gt;
&lt;p&gt;
MagicDrive&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#24182;&#25429;&#25417;&#20102;&#32454;&#33268;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#20855;&#26377;2D&#25511;&#21046;&#30340;&#25968;&#25454;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#34903;&#26223;&#29983;&#25104;&#20013;&#31934;&#30830;&#30340;&#19977;&#32500;&#25511;&#21046;&#22312;&#19977;&#32500;&#24863;&#30693;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#40479;&#30640;&#22270;&#20316;&#20026;&#20027;&#35201;&#26465;&#20214;&#24120;&#24120;&#23548;&#33268;&#20960;&#20309;&#25511;&#21046;&#65288;&#22914;&#39640;&#24230;&#65289;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24433;&#21709;&#29289;&#20307;&#24418;&#29366;&#12289;&#36974;&#25377;&#27169;&#24335;&#21644;&#36947;&#36335;&#34920;&#38754;&#39640;&#31243;&#31561;&#23545;&#24863;&#30693;&#25968;&#25454;&#21512;&#25104;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicDrive&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#36890;&#36807;&#23450;&#21046;&#30340;&#32534;&#30721;&#31574;&#30053;&#23454;&#29616;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#36824;&#37319;&#29992;&#20102;&#36328;&#35270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#30830;&#20445;&#22810;&#20010;&#30456;&#26426;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;MagicDrive&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#25429;&#25417;&#21040;&#20102;&#31934;&#32454;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework offering diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view synthesis that captures nuanced 3D ge
&lt;/p&gt;</description></item><item><title>RLAdapter&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#35299;&#20915;LLM&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#36890;&#36807;&#36991;&#20813;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#24494;&#35843;LLM&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17176</link><description>&lt;p&gt;
RLAdapter&#65306;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds. (arXiv:2309.17176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17176
&lt;/p&gt;
&lt;p&gt;
RLAdapter&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#35299;&#20915;LLM&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#36890;&#36807;&#36991;&#20813;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#24494;&#35843;LLM&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#20915;&#31574;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#30340;&#20132;&#20114;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#20026;&#20195;&#29702;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;RL&#31639;&#27861;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLM&#36890;&#24120;&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26368;&#20248;&#22320;&#24110;&#21161;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25968;&#25454;&#26469;&#24494;&#35843;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#20026;RL&#20195;&#29702;&#25552;&#20379;&#26377;&#29992;&#30340;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36935;&#21040;&#20102;&#19968;&#20123;&#22256;&#38590;&#65292;&#27604;&#22914;&#26080;&#27861;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#20854;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RLAdapter&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;RL&#31639;&#27861;&#21644;LLM&#20043;&#38388;&#24314;&#31435;&#26356;&#22909;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#25972;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) shows remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#21644;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#31561;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20013;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.15512</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models. (arXiv:2309.15512v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#21644;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#31561;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20013;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#27861;&#22312;&#35821;&#38899;&#20811;&#38534;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#25991;&#26412;-&#35821;&#38899;&#23545;&#12290;&#26368;&#23567;&#30417;&#30563;&#30340;&#35821;&#38899;&#21512;&#25104;&#36890;&#36807;&#32452;&#21512;&#20004;&#31181;&#31867;&#22411;&#30340;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#65288;&#35821;&#20041;&#21644;&#22768;&#23398;&#65289;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#26368;&#23569;&#30417;&#30563;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#65292;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#12290;&#33258;&#22238;&#24402;&#26694;&#26550;&#20855;&#26377;&#20856;&#22411;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#19981;&#21487;&#25511;&#24615;&#38382;&#39064;&#12290;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#21463;&#21040;&#25345;&#32493;&#39044;&#27979;&#27169;&#22411;&#24341;&#36215;&#30340;&#38901;&#24459;&#24179;&#22343;&#21270;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#25152;&#26377;&#27169;&#22359;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#12290;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic \&amp; acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diver
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15293</link><description>&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15293
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#37117;&#24314;&#31435;&#22312;&#25968;&#25454;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#26159;&#20381;&#27425;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#25910;&#38598;&#32780;&#26469;&#26102;&#65292;&#36825;&#19968;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#36941;&#21382;&#36807;&#31243;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#65292;&#21487;&#35777;&#26126;&#22320;&#20351;&#20195;&#29702;&#22312;&#21333;&#27425;&#37096;&#32626;&#20013;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#21021;&#22987;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#26368;&#22823;&#29109;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#31283;&#23450;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#29289;&#29702;&#23398;&#12289;&#23398;&#20064;&#21644;&#25511;&#21046;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#34892;&#36208;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#36879;&#26126;&#21487;&#38752;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prototype-based Mean-Teacher (PMT)&#30340;&#26032;&#22411;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14950</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#21407;&#22411;&#30340;&#22343;&#20540;&#25945;&#24072;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher. (arXiv:2309.14950v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prototype-based Mean-Teacher (PMT)&#30340;&#26032;&#22411;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35270;&#35273;&#30446;&#26631;&#26816;&#27979;&#22120;&#36866;&#24212;&#20110;&#25805;&#20316;&#30446;&#26631;&#39046;&#22495;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#24403;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#26102;&#65292;&#23558;&#23427;&#20204;&#35270;&#20026;&#21333;&#29420;&#30340;&#22495;&#24182;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#65292;&#30456;&#27604;&#23558;&#36825;&#20123;&#28304;&#22495;&#28151;&#21512;&#24182;&#36827;&#34892;UDA&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#20063;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#29616;&#26377;&#30340;MSDA&#26041;&#27861;&#23398;&#20064;&#22495;&#19981;&#21464;&#21644;&#22495;&#29305;&#23450;&#21442;&#25968;&#65288;&#23545;&#20110;&#27599;&#20010;&#28304;&#22495;&#65289;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#19982;&#21333;&#28304;UDA&#26041;&#27861;&#19981;&#21516;&#65292;&#23398;&#20064;&#22495;&#29305;&#23450;&#21442;&#25968;&#20351;&#23427;&#20204;&#19982;&#20351;&#29992;&#30340;&#28304;&#22495;&#25968;&#37327;&#25104;&#27491;&#27604;&#22686;&#38271;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#21407;&#22411;&#30340;&#22343;&#20540;&#25945;&#24072;&#65288;PMT&#65289;&#30340;&#26032;&#22411;MSDA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;&#36825;&#20123;&#21407;&#22411;&#26159;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#30340;&#65292;&#23545;&#40784;&#30456;&#21516;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting visual object detectors to operational target domains is a challenging task, commonly achieved using unsupervised domain adaptation (UDA) methods. When the labeled dataset is coming from multiple source domains, treating them as separate domains and performing a multi-source domain adaptation (MSDA) improves the accuracy and robustness over mixing these source domains and performing a UDA, as observed by recent studies in MSDA. Existing MSDA methods learn domain invariant and domain-specific parameters (for each source domain) for the adaptation. However, unlike single-source UDA methods, learning domain-specific parameters makes them grow significantly proportional to the number of source domains used. This paper proposes a novel MSDA method called Prototype-based Mean-Teacher (PMT), which uses class prototypes instead of domain-specific subnets to preserve domain-specific information. These prototypes are learned using a contrastive loss, aligning the same categories across 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10639</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20840;&#23616;${\mathcal L}^2$&#26368;&#23567;&#21270;&#22120;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;$L$&#20010;&#38544;&#34255;&#23618;&#65292;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#65292;${\mathcal L}^2$ Schatten&#31867;&#65288;&#25110;Hilbert-Schmidt&#65289;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#30456;&#31561;&#32500;&#24230;$Q\geq1$&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#12290;&#38544;&#34255;&#23618;&#20063;&#23450;&#20041;&#22312;${\mathbb R}^{Q}$&#30340;&#31354;&#38388;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#26368;&#26032;&#30340;&#20851;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524;&#65292;&#22312;$L\geq Q$&#30340;&#24773;&#20917;&#19979;&#26500;&#36896;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#26063;&#26159;&#36864;&#21270;&#30340;&#12290;&#22312;&#36825;&#37324;&#25552;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;DL&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#36890;&#36807;&#23545;&#35757;&#32451;&#36755;&#20837;&#30340;&#36882;&#24402;&#25130;&#26029;&#26144;&#23556;&#30340;&#24212;&#29992;&#26469;&#8220;&#25972;&#29702;&#8221;&#35757;&#32451;&#36755;&#20837;&#65292;&#20197;&#26368;&#23567;&#21270;&#22122;&#22768;&#19982;&#20449;&#21495;&#30340;&#27604;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$2^Q-1$&#20010;&#19981;&#21516;&#30340;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25968;&#25454;&#28165;&#29702;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25968;&#23383;&#30382;&#32932;&#31185;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#21487;&#20449;&#24230;&#12290;&#21327;&#35758;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#30382;&#32932;&#31185;&#21307;&#29983;&#30340;&#30830;&#35748;&#19979;&#65292;&#21024;&#38500;&#20102;&#26080;&#20851;&#26679;&#26412;&#21644;&#36817;&#20284;&#37325;&#22797;&#26679;&#26412;&#65292;&#20272;&#35745;&#20102;&#26631;&#31614;&#38169;&#35823;&#30340;&#30334;&#20998;&#27604;&#65292;&#24182;&#25552;&#20379;&#20102;&#20462;&#35746;&#21518;&#30340;&#25968;&#25454;&#38598;&#25991;&#20214;&#21015;&#34920;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#25968;&#23383;&#30382;&#32932;&#31185;&#20013;&#26356;&#21487;&#38752;&#30340;&#24615;&#33021;&#35780;&#20272;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2309.06961</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#38752;&#30340;&#30382;&#32932;&#31185;&#35780;&#20272;&#22522;&#20934;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Dermatology Evaluation Benchmarks. (arXiv:2309.06961v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25968;&#25454;&#28165;&#29702;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25968;&#23383;&#30382;&#32932;&#31185;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#21487;&#20449;&#24230;&#12290;&#21327;&#35758;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#30382;&#32932;&#31185;&#21307;&#29983;&#30340;&#30830;&#35748;&#19979;&#65292;&#21024;&#38500;&#20102;&#26080;&#20851;&#26679;&#26412;&#21644;&#36817;&#20284;&#37325;&#22797;&#26679;&#26412;&#65292;&#20272;&#35745;&#20102;&#26631;&#31614;&#38169;&#35823;&#30340;&#30334;&#20998;&#27604;&#65292;&#24182;&#25552;&#20379;&#20102;&#20462;&#35746;&#21518;&#30340;&#25968;&#25454;&#38598;&#25991;&#20214;&#21015;&#34920;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#25968;&#23383;&#30382;&#32932;&#31185;&#20013;&#26356;&#21487;&#38752;&#30340;&#24615;&#33021;&#35780;&#20272;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30382;&#32932;&#31185;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#26080;&#24847;&#38388;&#21253;&#21547;&#30340;&#19981;&#20934;&#30830;&#24615;&#38477;&#20302;&#20102;&#27169;&#22411;&#24615;&#33021;&#20272;&#35745;&#30340;&#20449;&#20219;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25968;&#25454;&#28165;&#29702;&#21327;&#35758;&#65292;&#29992;&#20110;&#35782;&#21035;&#20043;&#21069;&#30340;&#31574;&#21010;&#20013;&#36951;&#28431;&#30340;&#38382;&#39064;&#12290;&#35813;&#21327;&#35758;&#21033;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#28165;&#29702;&#31574;&#30053;&#65292;&#24182;&#22312;&#30452;&#35266;&#30340;&#20572;&#27490;&#20934;&#21017;&#30340;&#32456;&#27490;&#19979;&#36827;&#34892;&#30830;&#35748;&#36807;&#31243;&#12290;&#22522;&#20110;&#22810;&#20010;&#30382;&#32932;&#31185;&#21307;&#29983;&#30340;&#30830;&#35748;&#65292;&#25105;&#20204;&#21024;&#38500;&#20102;&#26080;&#20851;&#26679;&#26412;&#21644;&#36817;&#20284;&#37325;&#22797;&#26679;&#26412;&#65292;&#24182;&#20272;&#35745;&#20102;&#30001;&#22269;&#38469;&#30382;&#32932;&#25104;&#20687;&#21327;&#20316;&#32452;&#25512;&#24191;&#30340;&#20845;&#20010;&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#26631;&#31614;&#38169;&#35823;&#30340;&#30334;&#20998;&#27604;&#65292;&#20197;&#29992;&#20110;&#27169;&#22411;&#35780;&#20272;&#12290;&#38500;&#20102;&#26412;&#25991;&#65292;&#25105;&#20204;&#36824;&#20844;&#24067;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#20462;&#35746;&#25991;&#20214;&#21015;&#34920;&#65292;&#24212;&#35813;&#29992;&#20110;&#27169;&#22411;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25968;&#23383;&#30382;&#32932;&#31185;&#20013;&#26356;&#21487;&#38752;&#30340;&#24615;&#33021;&#35780;&#20272;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmark datasets for digital dermatology unwittingly contain inaccuracies that reduce trust in model performance estimates. We propose a resource-efficient data cleaning protocol to identify issues that escaped previous curation. The protocol leverages an existing algorithmic cleaning strategy and is followed by a confirmation process terminated by an intuitive stopping criterion. Based on confirmation by multiple dermatologists, we remove irrelevant samples and near duplicates and estimate the percentage of label errors in six dermatology image datasets for model evaluation promoted by the International Skin Imaging Collaboration. Along with this paper, we publish revised file lists for each dataset which should be used for model evaluation. Our work paves the way for more trustworthy performance assessment in digital dermatology.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;SE(3)&#31561;&#21464;&#24615;&#26469;&#36827;&#34892;3D&#20960;&#20309;&#24418;&#29366;&#32452;&#35013;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#37096;&#20214;&#20851;&#32852;&#30340;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#32452;&#35013;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06810</link><description>&lt;p&gt;
&#21033;&#29992;SE(3)&#31561;&#21464;&#24615;&#23398;&#20064;3D&#20960;&#20309;&#24418;&#29366;&#32452;&#35013;
&lt;/p&gt;
&lt;p&gt;
Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly. (arXiv:2309.06810v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;SE(3)&#31561;&#21464;&#24615;&#26469;&#36827;&#34892;3D&#20960;&#20309;&#24418;&#29366;&#32452;&#35013;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#37096;&#20214;&#20851;&#32852;&#30340;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#32452;&#35013;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#32452;&#35013;&#26088;&#22312;&#23558;&#37096;&#20214;&#65288;&#25110;&#30862;&#29255;&#65289;&#37325;&#26032;&#32452;&#35013;&#25104;&#23436;&#25972;&#30340;&#29289;&#20307;&#65292;&#36825;&#26159;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24120;&#35265;&#20219;&#21153;&#12290;&#19982;&#35821;&#20041;&#37096;&#20214;&#32452;&#35013;&#65288;&#20363;&#22914;&#65292;&#23558;&#26885;&#23376;&#30340;&#35821;&#20041;&#37096;&#20214;&#22914;&#33151;&#32452;&#35013;&#25104;&#25972;&#20010;&#26885;&#23376;&#65289;&#19981;&#21516;&#65292;&#20960;&#20309;&#37096;&#20214;&#32452;&#35013;&#65288;&#20363;&#22914;&#65292;&#23558;&#30871;&#30862;&#29255;&#32452;&#35013;&#25104;&#23436;&#25972;&#30340;&#30871;&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#19968;&#39033;&#26032;&#20852;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#19981;&#20851;&#27880;&#37096;&#20214;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#26159;&#20851;&#27880;&#37096;&#20214;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#30001;&#20110;&#26029;&#35010;&#37096;&#20214;&#30340;&#20960;&#20309;&#21644;&#23039;&#24577;&#31354;&#38388;&#37117;&#24322;&#24120;&#24222;&#22823;&#65292;&#23545;&#37096;&#20214;&#34920;&#31034;&#36827;&#34892;&#24418;&#29366;&#23039;&#24577;&#35299;&#32544;&#26159;&#26377;&#30410;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#26469;&#36827;&#34892;&#24418;&#29366;&#23039;&#24577;&#35299;&#32544;&#12290;&#27492;&#22806;&#65292;&#20197;&#24448;&#30340;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#24037;&#20316;&#21482;&#32771;&#34385;&#21333;&#20010;&#23545;&#35937;&#30340;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#34920;&#31034;&#65292;&#32780;&#25105;&#20204;&#26356;&#36827;&#19968;&#27493;&#25552;&#20986;&#21033;&#29992;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#26469;&#32771;&#34385;&#22810;&#37096;&#20214;&#20851;&#32852;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Shape assembly aims to reassemble parts (or fragments) into a complete object, which is a common task in our daily life. Different from the semantic part assembly (e.g., assembling a chair's semantic parts like legs into a whole chair), geometric part assembly (e.g., assembling bowl fragments into a complete bowl) is an emerging task in computer vision and robotics. Instead of semantic information, this task focuses on geometric information of parts. As the both geometric and pose space of fractured parts are exceptionally large, shape pose disentanglement of part representations is beneficial to geometric shape assembly. In our paper, we propose to leverage SE(3) equivariance for such shape pose disentanglement. Moreover, while previous works in vision and robotics only consider SE(3) equivariance for the representations of single objects, we move a step forward and propose leveraging SE(3) equivariance for representations considering multi-part correlations, which further boosts the 
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00424</link><description>&lt;p&gt;
CPSP: &#20174;&#38899;&#32032;&#30417;&#30563;&#20013;&#23398;&#20064;&#35821;&#38899;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26368;&#23567;&#30417;&#30563;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#12289;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31561;&#32454;&#31890;&#24230;&#29983;&#25104;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#34920;&#31034;&#24212;&#21253;&#21547;&#20171;&#20110;&#25991;&#26412;&#32534;&#30721;&#21644;&#22768;&#23398;&#32534;&#30721;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#20869;&#23481;&#31361;&#20986;&#65292;&#32780;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#31561;&#35821;&#38899;&#20449;&#24687;&#24212;&#35813;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20013;&#38388;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#24615;&#36807;&#39640;&#21644;&#32500;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#38899;&#39057;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25552;&#21462;&#29992;&#20110;&#19979;&#28216;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#23616;&#25551;&#36848;&#20449;&#24687;&#65292;&#19981;&#36866;&#21512;TTS&#12289;VC&#21644;ASR&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38899;&#32032;-&#35821;&#38899;&#39044;&#35757;&#32451;&#65288;CPSP&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19977;&#20010;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35299;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representation extracted from speech should contain information that is between text coding and acoustic coding. The linguistic content is salient, while the paralinguistic information such as speaker identity and acoustic details should be removed. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Additionally, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named Contrastive Phoneme-Speech Pretraining (CPSP), which uses three encoders, one decoder, and contrastive learning to bring phoneme and speech
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11978</link><description>&lt;p&gt;
&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#31614;&#39044;&#27979;&#19968;&#20010;&#23436;&#25972;&#30340;&#20855;&#26377;&#22810;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#33647;&#29289;&#21644;&#20998;&#23376;&#35774;&#35745;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#20986;&#29616;&#20102;&#20960;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;(1) &#36825;&#20123;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#22522;&#30784;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#24448;&#24448;&#26410;&#32463;&#28145;&#20837;&#25506;&#32034;&#65307;(2) &#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#22312;&#26377;&#38480;&#30340;&#25351;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26367;&#25442;&#20026;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#65292;&#30740;&#31350;&#20102;GNN&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#19981;&#21516;&#29983;&#25104;&#26694;&#26550;&#65288;GCPN&#21644;GraphAF&#65289;&#20013;&#20845;&#31181;GNN&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20998;&#23376;&#29983;&#25104;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11730</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#12289;&#39044;&#27979;&#8221;&#33539;&#24335;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OD-QA&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MD-QA&#65289;&#22330;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#35201;&#27714;&#23545;&#19981;&#21516;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#32852;&#26377;&#28145;&#20837;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#65288;KGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MD-QA&#20013;&#20026;LLMs&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#22270;&#26500;&#24314;&#27169;&#22359;&#21644;&#22270;&#36941;&#21382;&#27169;&#22359;&#12290;&#23545;&#20110;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26469;&#34920;&#31034;&#25991;&#27573;&#25110;&#25991;&#26723;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#39029;&#38754;/&#34920;&#26684;&#65289;&#65292;&#32780;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#25991;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;/&#35789;&#27719;&#30456;&#20284;&#24615;&#25110;&#32773;&#25991;&#26723;&#20869;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#23545;&#20110;&#22270;&#36941;&#21382;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#23427;&#22312;&#33410;&#28857;&#20043;&#38388;&#23548;&#33322;&#24182;&#25910;&#38598;&#25903;&#25345;&#24615;&#30340;&#25991;&#27573;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.10997</link><description>&lt;p&gt;
SPEGTI: &#32467;&#26500;&#39044;&#27979;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models. (arXiv:2308.10997v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26082;&#36924;&#30495;&#21448;&#19982;&#25991;&#26412;&#25552;&#31034;&#30456;&#31526;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36136;&#37327;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65306;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#37117;&#26159;&#36845;&#20195;&#24335;&#30340;&#65292;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#25512;&#26029;&#65292;&#24182;&#20351;&#29992;&#22823;&#27169;&#22411;&#12290;&#36825;&#31181;&#36845;&#20195;&#36807;&#31243;&#26159;&#20026;&#20102;&#30830;&#20445;&#22270;&#20687;&#30340;&#19981;&#21516;&#21306;&#22495;&#19981;&#20165;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#65292;&#36824;&#19982;&#20854;&#20182;&#21306;&#22495;&#30456;&#23481;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20351;&#29992;&#20102;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;Muse&#27169;&#22411;&#37197;&#21512;&#20351;&#29992;&#12290;MRF&#32534;&#30721;&#20102;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#30340;&#22270;&#20687;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#23481;&#24615;&#65292;&#24182;&#19988;&#20351;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;Muse&#39044;&#27979;&#27493;&#39588;&#12290;&#20351;&#29992;MRF&#30340;&#25512;&#26029;&#25104;&#26412;&#22823;&#22823;&#38477;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#24555;&#36895;&#23398;&#20064;&#20854;&#21442;&#25968;&#65292;&#36890;&#36807;&#23545;MRF&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MR
&lt;/p&gt;</description></item><item><title>BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09936</link><description>&lt;p&gt;
BLIVA: &#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09936
&lt;/p&gt;
&lt;p&gt;
BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#35299;&#20915;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#35299;&#37322;&#23884;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26631;&#20934;&#27969;&#31243;&#36890;&#24120;&#28041;&#21450;&#23398;&#20064;&#19968;&#32452;&#22266;&#23450;&#30340;&#26597;&#35810;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23553;&#35013;&#22270;&#20687;&#19978;&#19979;&#25991;&#65292;&#24182;&#38543;&#21518;&#29992;&#20316;LLM&#20013;&#30340;&#36719;&#25552;&#31034;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21463;&#20196;&#29260;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#38480;&#21046;&#23545;&#25991;&#26412;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#22330;&#26223;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#19968;&#28857;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;BLIVA&#65306;InstructBLIP with Visual Assistant&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;BLIVA&#38598;&#25104;&#20102;&#26469;&#33258;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#65292;&#24182;&#23558;&#32534;&#30721;&#30340;&#34917;&#19969;&#23884;&#20837;&#30452;&#25509;&#25237;&#24433;&#21040;LLM&#20013;&#65292;&#36825;&#26159;&#21463;&#21040;LLaVA&#30340;&#21551;&#21457;&#30340;&#19968;&#31181;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2308.09720</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24847;&#24819;&#19981;&#21040;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09720
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#23637;&#31034;&#20986;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#65288;&#39044;&#27979;&#20154;&#31867;&#20070;&#20889;&#25991;&#26412;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#65289;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#30340;&#24615;&#36136;&#21450;&#20854;&#19982;&#20854;&#20182;&#24050;&#30693;&#38388;&#25509;&#36807;&#31243;&#30340;&#20851;&#31995;&#12290;&#25991;&#31456;&#20027;&#24352;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#30340;&#19968;&#20010;&#37325;&#35201;&#21103;&#20316;&#29992;&#26159;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#24320;&#21457;&#30340;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#21487;&#39044;&#27979;&#30340;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#31616;&#35201;&#35752;&#35770;&#20102;&#36825;&#20123;&#31995;&#32479;&#25152;&#33719;&#24471;&#30340;&#35748;&#30693;&#25216;&#33021;&#19982;&#20154;&#31867;&#35748;&#30693;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I discuss the nature of this indirect acquisition process and its relation to other known indirect processes. I argue that an important side effect of such indirect acquisition is the development of integrated abilities. I discuss the extent to which the abilities developed by large language models are predictable. Finally, I briefly discuss the relation between the cognitive skills acquired by these systems and human cognition.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#21407;&#22240;&#65292;&#26377;&#25928;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#34987;&#39564;&#35777;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.09437</link><description>&lt;p&gt;
&#20174;&#26399;&#26395;&#21040;&#23433;&#20840;&#65306;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#27491;&#30830;&#30340;&#21407;&#22240;&#26469;&#28040;&#38500;&#28145;&#24230;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space. (arXiv:2308.09437v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#21407;&#22240;&#65292;&#26377;&#25928;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#34987;&#39564;&#35777;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#28508;&#34255;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#36825;&#22312;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#20110;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#65288;&#22914;&#21307;&#23398;&#24212;&#29992;&#65289;&#26102;&#23384;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#30340;&#21518;&#22788;&#29702;&#27169;&#22411;&#26657;&#27491;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#36755;&#20837;&#32423;&#21035;&#30340;&#27880;&#37322;&#65292;&#36825;&#21482;&#36866;&#29992;&#20110;&#23616;&#37096;&#21270;&#20559;&#35265;&#65292;&#35201;&#20040;&#36890;&#36807;&#25193;&#20805;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#65292;&#24076;&#26395;&#33021;&#23454;&#29616;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;&#24403;&#36890;&#36807;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#26469;&#24314;&#27169;&#20559;&#35265;&#26102;&#65292;&#25105;&#20204;&#24378;&#35843;&#36873;&#25321;&#31283;&#20581;&#30340;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#65288;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#24448;&#24448;&#20250;&#23548;&#33268;&#21457;&#25955;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#20351;&#29992;VGG&#12289;ResNet&#21644;EfficientN&#22312;ISIC&#12289;&#39592;&#40836;&#12289;ImageNet&#21644;CelebA&#25968;&#25454;&#38598;&#19978;&#22312;&#21463;&#25511;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#26377;&#25928;&#20943;&#36731;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2308.08536</link><description>&lt;p&gt;
Transformers&#33021;&#21542;&#23398;&#20064;&#29992;&#20110;&#26410;&#30693;&#31995;&#32479;&#30340;&#26368;&#20248;&#28388;&#27874;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#23427;&#20351;&#29992;&#36807;&#21435;&#30340;&#25152;&#26377;&#36755;&#20986;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20808;&#39564;&#20998;&#24067;&#30340;&#21508;&#31181;&#31995;&#32479;&#26469;&#35757;&#32451;transformer&#65292;&#28982;&#21518;&#22312;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#30456;&#21516;&#20998;&#24067;&#30340;&#31995;&#32479;&#19978;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33719;&#24471;&#30340;transformer&#23601;&#20687;&#19968;&#20010;&#39044;&#27979;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#24182;&#24555;&#36895;&#36866;&#24212;&#21644;&#39044;&#27979;&#19981;&#21516;&#30340;&#31995;&#32479;&#65292;&#22240;&#27492;&#25105;&#20204;&#31216;&#20043;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;MOP&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated remarkable success in natural language processing; however, their potential remains mostly unexplored for problems arising in dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. We train the transformer using various systems drawn from a prior distribution and then evaluate its performance on previously unseen systems from the same distribution. As a result, the obtained transformer acts like a prediction algorithm that learns in-context and quickly adapts to and predicts well for different systems - thus we call it meta-output-predictor (MOP). MOP matches the performance of the optimal output estimator, based on Kalman filter, for most linear dynamical systems even though it does not have access to a model. We observe via extensive numerical experiments that MOP also performs well in challenging scenarios with non-i.i.d. noise, time-varying dy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26080;&#20154;&#36816;&#36755;&#30340;&#32508;&#21512;&#26041;&#26696;&#65292;&#21253;&#25324;FusionPlanner&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#12289;MiningNav&#35780;&#20272;&#24037;&#20855;&#21644;Parallel Mining Simulator&#27169;&#25311;&#22120;&#65292;&#20197;&#24212;&#23545;&#38706;&#22825;&#37319;&#30719;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#36816;&#36755;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.06931</link><description>&lt;p&gt;
FusionPlanner&#65306;&#19968;&#31181;&#20351;&#29992;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#30340;&#22810;&#20219;&#21153;&#36816;&#21160;&#35268;&#21010;&#22120;&#65292;&#29992;&#20110;&#30719;&#29992;&#21345;&#36710;
&lt;/p&gt;
&lt;p&gt;
FusionPlanner: A Multi-task Motion Planner for Mining Trucks using Multi-sensor Fusion Method. (arXiv:2308.06931v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26080;&#20154;&#36816;&#36755;&#30340;&#32508;&#21512;&#26041;&#26696;&#65292;&#21253;&#25324;FusionPlanner&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#12289;MiningNav&#35780;&#20272;&#24037;&#20855;&#21644;Parallel Mining Simulator&#27169;&#25311;&#22120;&#65292;&#20197;&#24212;&#23545;&#38706;&#22825;&#37319;&#30719;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#36816;&#36755;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#26234;&#33021;&#36710;&#36742;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#20856;&#22411;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#65292;&#38706;&#22825;&#37319;&#30719;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#25805;&#20316;&#26465;&#20214;&#21644;&#19981;&#21033;&#30340;&#29615;&#22659;&#22240;&#32032;&#32780;&#21560;&#24341;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26080;&#20154;&#36816;&#36755;&#30340;&#32508;&#21512;&#33539;&#24335;&#65292;&#21253;&#25324;&#19968;&#20010;&#20223;&#30495;&#24179;&#21488;&#12289;&#19968;&#20010;&#27979;&#35797;&#22522;&#20934;&#21644;&#19968;&#20010;&#21487;&#38752;&#31283;&#20581;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FusionPlanner&#30340;&#22810;&#20219;&#21153;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#36866;&#24212;&#26080;&#20154;&#36816;&#36755;&#30340;&#27178;&#21521;&#21644;&#32437;&#21521;&#25511;&#21046;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;MiningNav&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#38706;&#22825;&#37319;&#30719;&#20132;&#36890;&#36947;&#36335;&#19978;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#31639;&#27861;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24182;&#34892;&#37319;&#30719;&#27169;&#25311;&#22120;&#65288;PMS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, significant achievements have been made in motion planning for intelligent vehicles. However, as a typical unstructured environment, open-pit mining attracts limited attention due to its complex operational conditions and adverse environmental factors. A comprehensive paradigm for unmanned transportation in open-pit mines is proposed in this research, including a simulation platform, a testing benchmark, and a trustworthy and robust motion planner. Firstly, we propose a multi-task motion planning algorithm, called FusionPlanner, for autonomous mining trucks by the Multi-sensor fusion method to adapt both lateral and longitudinal control tasks for unmanned transportation. Then, we develop a novel benchmark called MiningNav, which offers three validation approaches to evaluate the trustworthiness and robustness of well-trained algorithms in transportation roads of open-pit mines. Finally, we introduce the Parallel Mining Simulator (PMS), a new high-fidelity simulator spe
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ETHER&#65292;&#36890;&#36807;&#23545;&#40784;&#32039;&#24613;&#27807;&#36890;&#26469;&#35299;&#20915;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#20013;&#30340;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26550;&#26500;&#20381;&#36182;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15494</link><description>&lt;p&gt;
ETHER: &#23545;&#20110;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#30340;&#32039;&#23494;&#27807;&#36890;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ETHER: Aligning Emergent Communication for Hindsight Experience Replay. (arXiv:2307.15494v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ETHER&#65292;&#36890;&#36807;&#23545;&#40784;&#32039;&#24613;&#27807;&#36890;&#26469;&#35299;&#20915;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#20013;&#30340;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26550;&#26500;&#20381;&#36182;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#36319;&#38543;&#23545;&#20110;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29305;&#24615;&#65292;&#22914;&#32452;&#21512;&#24615;&#65292;&#33021;&#22815;&#25552;&#20379;&#23398;&#20064;&#22797;&#26434;&#31574;&#30053;&#30340;&#24378;&#24402;&#32435;&#20559;&#22909;&#12290;&#20808;&#21069;&#30340;&#26550;&#26500;&#22914;HIGhER&#32467;&#21512;&#20102;&#35821;&#35328;&#26465;&#20214;&#19982;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#65288;HER&#65289;&#26469;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#19982;HER&#31867;&#20284;&#65292;HIGhER&#20381;&#36182;&#20110;&#19968;&#20010;&#39044;&#35774;&#30340;&#20989;&#25968;&#26469;&#25552;&#20379;&#21453;&#39304;&#20449;&#21495;&#65292;&#25351;&#31034;&#21738;&#31181;&#35821;&#35328;&#25551;&#36848;&#22312;&#21738;&#31181;&#29366;&#24577;&#19979;&#26377;&#25928;&#12290;&#36825;&#31181;&#20381;&#36182;&#20110;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;HIGhER&#21482;&#21033;&#29992;&#25104;&#21151;&#30340;&#24378;&#21270;&#23398;&#20064;&#36712;&#36857;&#20013;&#21253;&#21547;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20854;&#26368;&#32456;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#27809;&#26377;&#26089;&#26399;&#25104;&#21151;&#36712;&#36857;&#65292;HIGhER&#24182;&#19981;&#27604;&#20854;&#26500;&#24314;&#20110;&#20043;&#19978;&#30340;DQN&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32039;&#23494;&#25991;&#26412;&#22238;&#39038;&#24615;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language instruction following is paramount to enable collaboration between artificial agents and human beings. Natural language-conditioned reinforcement learning (RL) agents have shown how natural languages' properties, such as compositionality, can provide a strong inductive bias to learn complex policies. Previous architectures like HIGhER combine the benefit of language-conditioning with Hindsight Experience Replay (HER) to deal with sparse rewards environments. Yet, like HER, HIGhER relies on an oracle predicate function to provide a feedback signal highlighting which linguistic description is valid for which state. This reliance on an oracle limits its application. Additionally, HIGhER only leverages the linguistic information contained in successful RL trajectories, thus hurting its final performance and data-efficiency. Without early successful trajectories, HIGhER is no better than DQN upon which it is built. In this paper, we propose the Emergent Textual Hindsight Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#32534;&#30721;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15484</link><description>&lt;p&gt;
&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#35821;&#38899;&#21512;&#25104;&#65306;&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding. (arXiv:2307.15484v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#32534;&#30721;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#33021;&#22815;&#37319;&#29992;&#26368;&#23567;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#25216;&#26415;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#24182;&#20351;&#29992;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#26469;&#35299;&#32806;TTS&#12290;&#20026;&#20102;&#35299;&#20915;&#31163;&#25955;&#34920;&#31034;&#20013;&#30340;&#39640;&#32500;&#24230;&#21644;&#27874;&#24418;&#22833;&#30495;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-LM-Speech&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#23558;&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#20026;&#22522;&#20110;mel&#39057;&#35889;&#22270;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#38901;&#24459;&#29942;&#39048;&#30340;&#25552;&#31034;&#32534;&#30721;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25552;&#31034;&#34920;&#31034;&#33021;&#21147;&#12290;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#36935;&#21040;&#32570;&#22833;&#21644;&#37325;&#22797;&#21333;&#35789;&#30340;&#38382;&#39064;&#65292;&#32780;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#30001;&#20110;&#39044;&#27979;&#27169;&#22411;&#30340;&#23384;&#22312;&#23548;&#33268;&#34920;&#36798;&#24179;&#22343;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tetra-Diff-Speech&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38271;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#38901;&#24459;&#34920;&#36798;&#12290;&#25105;&#20204;&#26399;&#26395;&#35821;&#20041;&#32534;&#30721;&#30340;&#20449;&#24687;&#20869;&#23481;&#20171;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13709</link><description>&lt;p&gt;
&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65306;&#22312;&#27809;&#26377;&#20855;&#20307;&#35780;&#20215;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#29289;&#21697;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23646;&#24615;&#65292;&#22914;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#21487;&#21462;&#24615;&#25110;&#24378;&#24230;&#65292;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#24050;&#30693;&#29289;&#21697;&#30340;&#36825;&#20123;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20986;&#29616;&#22312;&#37197;&#23545;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#30340;&#36816;&#21160;&#21592;&#30340;&#23454;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#20010;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#30340;&#38750;&#23545;&#31216;&#29615;&#22659;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;DBTR&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.02507</link><description>&lt;p&gt;
STS-CCL&#65306;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#39044;&#27979;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting. (arXiv:2307.02507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#37492;&#20110;&#36825;&#20010;&#22256;&#22659;&#65292;&#26412;&#25991;&#37319;&#29992;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#29992;&#20110;&#26102;&#31354;&#22270;&#25968;&#25454;&#30340;&#22522;&#26412;&#21644;&#24378;&#22823;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#25200;&#21160;&#20102;&#22270;&#32467;&#26500;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21033;&#29992;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#36827;&#34892;&#33258;&#36866;&#24212;&#22686;&#24378;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#31354;&#21516;&#27493;&#23545;&#27604;&#27169;&#22359;&#65288;STS-CM&#65289;&#65292;&#20197;&#21516;&#26102;&#25429;&#25417;&#33391;&#22909;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#24182;&#23454;&#29616;&#22270;&#32423;&#23545;&#27604;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21306;&#20998;&#36127;&#31579;&#36873;&#20013;&#30340;&#33410;&#28857;&#20010;&#20307;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#29305;&#24449;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains to be a challenging task. In considering of the dilemma, this work employs the advanced contrastive learning and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. Second, we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. To further discriminate node individuals in negative filtering, a Semantic Contextual Contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#65292;&#22635;&#34917;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.01504</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
All in One: Multi-task Prompting for Graph Neural Networks. (arXiv:2307.01504v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#65292;&#22635;&#34917;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#24050;&#25104;&#20026;&#35768;&#22810;&#22270;&#20219;&#21153;&#30340;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21033;&#29992;&#36890;&#29992;&#30340;&#22270;&#30693;&#35782;&#26469;&#32531;&#35299;&#27599;&#20010;&#24212;&#29992;&#20013;&#32570;&#20047;&#22270;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#33410;&#28857;&#32423;&#12289;&#36793;&#32423;&#21644;&#22270;&#32423;&#30340;&#22270;&#20219;&#21153;&#24046;&#24322;&#24456;&#22823;&#65292;&#23548;&#33268;&#39044;&#35757;&#32451;&#39044;&#25991;&#26412;&#36890;&#24120;&#19982;&#36825;&#20123;&#22810;&#20219;&#21153;&#19981;&#20860;&#23481;&#12290;&#36825;&#31181;&#24046;&#36317;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#8220;&#36127;&#36801;&#31227;&#8221;&#65292;&#20174;&#32780;&#23548;&#33268;&#32467;&#26524;&#19981;&#20339;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#25552;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21033;&#29992;&#20808;&#21069;&#30693;&#35782;&#24050;&#32463;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22635;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#24046;&#36317;&#30340;&#25552;&#31034;&#20027;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;&#20196;&#29260;&#12289;&#20196;&#29260;&#32467;&#26500;&#21644;&#25554;&#20837;&#27169;&#24335;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#26469;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.15222</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#36827;&#34892;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank in Generative Retrieval. (arXiv:2306.15222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15222
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#26469;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25991;&#26412;&#26816;&#32034;&#33539;&#20363;&#65292;&#23427;&#23558;&#30456;&#20851;&#27573;&#33853;&#30340;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#29983;&#25104;&#20026;&#26816;&#32034;&#30446;&#26631;&#12290;&#36825;&#31181;&#33539;&#20363;&#21033;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20195;&#34920;&#20102;&#19982;&#20256;&#32479;&#30340;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#30340;&#26032;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#24555;&#36895;&#21457;&#23637;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#26041;&#27861;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#21551;&#21457;&#24335;&#20989;&#25968;&#23558;&#39044;&#27979;&#30340;&#26631;&#35782;&#31526;&#36716;&#25442;&#20026;&#27573;&#33853;&#25490;&#24207;&#21015;&#34920;&#65292;&#36825;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#23398;&#20064;&#30446;&#26631;&#19982;&#26399;&#26395;&#30340;&#27573;&#33853;&#25490;&#24207;&#30446;&#26631;&#20043;&#38388;&#20135;&#29983;&#20102;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#29983;&#25104;&#30340;&#22266;&#26377;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LTRGR&#65292;&#23427;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#19982;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#35757;&#32451;&#19968;&#20010;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#25439;&#22833;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generation models and represents a new paradigm distinct from traditional learning-to-rank methods. However, despite its rapid development, current generative retrieval methods are still limited. They typically rely on a heuristic function to transform predicted identifiers into a passage rank list, which creates a gap between the learning objective of generative retrieval and the desired passage ranking target. Moreover, the inherent exposure bias problem of text generation also persists in generative retrieval. To address these issues, we propose a novel framework, called LTRGR, that combines generative retrieval with the classical learning-to-rank paradigm. Our approach involves training an autoregressive model using a passage rank loss, which directly optimizes the autoregressive model toward the optimal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#20445;&#25252;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#22788;&#29702;&#24182;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#19979;&#65292;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#65292;&#20026;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20808;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.13803</link><description>&lt;p&gt;
&#22823;&#35937;&#19982;&#31639;&#27861;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#22823;&#35937;&#30417;&#27979;&#20013;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#20316;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring. (arXiv:2306.13803v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#20445;&#25252;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#22788;&#29702;&#24182;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#19979;&#65292;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#65292;&#20026;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20808;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20026;&#22686;&#36827;&#23545;&#21160;&#29289;&#34892;&#20026;&#21644;&#20445;&#25252;&#31574;&#30053;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#26426;&#20250;&#12290;&#20197;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#20026;&#28966;&#28857;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#23427;&#20204;&#20445;&#25252;&#20013;&#30340;&#20316;&#29992;&#12290;&#32473;&#23450;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#65288;&#22914;&#25668;&#20687;&#22836;&#12289;&#40614;&#20811;&#39118;&#12289;&#22320;&#38663;&#20202;&#12289;&#26080;&#20154;&#26426;&#21644;&#21355;&#26143;&#65289;&#25910;&#38598;&#21040;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#65292;&#25361;&#25112;&#22312;&#20110;&#31649;&#29702;&#21644;&#35299;&#35835;&#36825;&#20123;&#24222;&#22823;&#30340;&#25968;&#25454;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24110;&#21161;&#25105;&#20204;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;AI&#39537;&#21160;&#30417;&#27979;&#26041;&#27861;&#21450;&#20854;&#22312;&#25913;&#21892;&#22823;&#35937;&#20445;&#25252;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#21033;&#29992;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#20197;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#30340;&#20851;&#38190;&#25152;&#22312;&#65292;&#20026;&#35768;&#22810;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20102;&#20808;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) and machine learning (ML) present revolutionary opportunities to enhance our understanding of animal behavior and conservation strategies. Using elephants, a crucial species in Africa's protected areas, as our focal point, we delve into the role of AI and ML in their conservation. Given the increasing amounts of data gathered from a variety of sensors like cameras, microphones, geophones, drones, and satellites, the challenge lies in managing and interpreting this vast data. New AI and ML techniques offer solutions to streamline this process, helping us extract vital information that might otherwise be overlooked. This paper focuses on the different AI-driven monitoring methods and their potential for improving elephant conservation. Collaborative efforts between AI experts and ecological researchers are essential in leveraging these innovative technologies for enhanced wildlife conservation, setting a precedent for numerous other species.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09910</link><description>&lt;p&gt;
LabelBench&#65306;&#22522;&#20110;&#32508;&#21512;&#26694;&#26550;&#30340;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#65292;&#20294;&#33719;&#21462;&#26631;&#35760;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#20943;&#32531;&#36825;&#19968;&#25104;&#26412;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36801;&#31227;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65289;&#26088;&#22312;&#23454;&#29616;&#26631;&#31614;&#39640;&#25928;&#24615;&#65306;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#26368;&#20339;&#30340;&#26631;&#31614;&#25928;&#29575;&#36890;&#24120;&#38656;&#35201;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#24182;&#27809;&#26377;&#25429;&#25417;&#21040;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#30340;&#21327;&#21516;&#32452;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LabelBench&#35299;&#20915;&#20102;&#36825;&#20010;&#32570;&#38519;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35780;&#20272;&#22810;&#20010;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#20316;&#20026;LabelBench&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#19968;&#36215;&#20351;&#29992;&#30340;&#26368;&#26032;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35777;&#26126;&#20102;&#27604;&#20808;&#21069;&#25253;&#21578;&#30340;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20808;&#21069;&#24037;&#20316;&#24369;&#19968;&#31867;&#38382;&#39064;&#36827;&#34892;&#25512;&#24191;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#28151;&#21512;&#19979;&#30340;&#24178;&#39044;&#20013;&#23398;&#20064;&#32447;&#24615;&#22240;&#26524;&#34920;&#31034;&#30340;&#24378;&#21487;&#35782;&#21035;&#24615;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#35782;&#21035;&#28508;&#22312;&#21464;&#37327;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.02235</link><description>&lt;p&gt;
&#20174;&#38750;&#32447;&#24615;&#28151;&#21512;&#19979;&#30340;&#24178;&#39044;&#20013;&#23398;&#20064;&#32447;&#24615;&#22240;&#26524;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Linear Causal Representations from Interventions under General Nonlinear Mixing. (arXiv:2306.02235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20808;&#21069;&#24037;&#20316;&#24369;&#19968;&#31867;&#38382;&#39064;&#36827;&#34892;&#25512;&#24191;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#28151;&#21512;&#19979;&#30340;&#24178;&#39044;&#20013;&#23398;&#20064;&#32447;&#24615;&#22240;&#26524;&#34920;&#31034;&#30340;&#24378;&#21487;&#35782;&#21035;&#24615;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#35782;&#21035;&#28508;&#22312;&#21464;&#37327;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#28151;&#21512;&#20989;&#25968;&#23436;&#20840;&#36890;&#29992;&#30340;&#19968;&#33324;&#35774;&#32622;&#19979;&#65292;&#20174;&#26410;&#30693;&#30340;&#28508;&#22312;&#24178;&#39044;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#28508;&#22312;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#12290; &#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21333;&#33410;&#28857;&#26410;&#30693;&#24178;&#39044;&#65288;&#21363;&#27809;&#26377;&#24178;&#39044;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#65289;&#32473;&#20986;&#24378;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;&#36825;&#25512;&#24191;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#30528;&#37325;&#20110;&#26356;&#24369;&#30340;&#31867;&#21035;&#65292;&#20363;&#22914;&#32447;&#24615;&#26144;&#23556;&#25110;&#25104;&#23545;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#12290;&#36825;&#20063;&#26159;&#39318;&#27425;&#20174;&#38750;&#37197;&#23545;&#24178;&#39044;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20013;&#33719;&#24471;&#22240;&#26524;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20381;&#36182;&#20110;&#20180;&#32454;&#25581;&#31034;&#32463;&#36807;&#38750;&#32447;&#24615;&#23494;&#24230;&#36716;&#25442;&#21518;&#25968;&#25454;&#20998;&#24067;&#20013;&#23384;&#22312;&#30340;&#39640;&#32500;&#20960;&#20309;&#32467;&#26500;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#28508;&#22312;&#20998;&#24067;&#30340;&#31934;&#24230;&#30697;&#38453;&#30340;&#20108;&#27425;&#24418;&#24335;&#26469;&#25429;&#25417;&#36825;&#31181;&#32467;&#26500;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#31639;&#27861;&#26469;&#23454;&#38469;&#35782;&#21035;&#28508;&#22312;&#21464;&#37327;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning causal representations from unknown, latent interventions in a general setting, where the latent distribution is Gaussian but the mixing function is completely general. We prove strong identifiability results given unknown single-node interventions, i.e., without having access to the intervention targets. This generalizes prior works which have focused on weaker classes, such as linear maps or paired counterfactual data. This is also the first instance of causal identifiability from non-paired interventions for deep neural network embeddings. Our proof relies on carefully uncovering the high-dimensional geometric structure present in the data distribution after a non-linear density transformation, which we capture by analyzing quadratic forms of precision matrices of the latent distributions. Finally, we propose a contrastive algorithm to identify the latent variables in practice and evaluate its performance on various tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; FigGen&#65292;&#19968;&#31181;&#36890;&#36807;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;&#31185;&#23398;&#22270;&#24418;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#25506;&#32034;&#20102;&#25991;&#26412;&#21040;&#22270;&#24418;&#29983;&#25104;&#30340;&#39046;&#22495;&#65292;&#24182;&#35299;&#20915;&#20102;&#35813;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.00800</link><description>&lt;p&gt;
FigGen: &#25991;&#26412;&#21040;&#31185;&#23398;&#22270;&#24418;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FigGen: Text to Scientific Figure Generation. (arXiv:2306.00800v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; FigGen&#65292;&#19968;&#31181;&#36890;&#36807;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;&#31185;&#23398;&#22270;&#24418;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#25506;&#32034;&#20102;&#25991;&#26412;&#21040;&#22270;&#24418;&#29983;&#25104;&#30340;&#39046;&#22495;&#65292;&#24182;&#35299;&#20915;&#20102;&#35813;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#33258;&#28982;&#22270;&#20687;&#21644;&#33402;&#26415;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#25216;&#26415;&#22312;&#21019;&#36896;&#22797;&#26434;&#30340;&#35270;&#35273;&#32452;&#21512;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36924;&#30495;&#24230;&#21644;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19968;&#30452;&#19987;&#27880;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;&#29421;&#31364;&#39046;&#22495;&#65292;&#32780;&#20854;&#20182;&#20998;&#24067;&#21017;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25991;&#26412;&#21040;&#22270;&#24418;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#21363;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21019;&#36896;&#35770;&#25991;&#30340;&#31185;&#23398;&#22270;&#24418;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; FigGen&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#24418;&#25216;&#26415;&#65292;&#20063;&#20171;&#32461;&#20102;&#25152;&#25552;&#20986;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#22312; https://github.com/joanrod/figure-diffusion &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generative modeling landscape has experienced tremendous growth in recent years, particularly in generating natural images and art. Recent techniques have shown impressive potential in creating complex visual compositions while delivering impressive realism and quality. However, state-of-the-art methods have been focusing on the narrow domain of natural images, while other distributions remain unexplored. In this paper, we introduce the problem of text-to-figure generation, that is creating scientific figures of papers from text descriptions. We present FigGen, a diffusion-based approach for text-to-figure as well as the main challenges of the proposed task. Code and models are available at https://github.com/joanrod/figure-diffusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;BetaZero&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDP&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00249</link><description>&lt;p&gt;
BetaZero&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#29992;&#20110;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDPs
&lt;/p&gt;
&lt;p&gt;
BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations. (arXiv:2306.00249v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;BetaZero&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDP&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#12289;&#30899;&#20648;&#23384;&#21644;&#36164;&#28304;&#21208;&#25506;&#31561;&#21487;&#25345;&#32493;&#33021;&#28304;&#24212;&#29992;&#65292;&#26368;&#36817;&#34987;&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#24182;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#12290;&#20026;&#20102;&#22312;&#23454;&#36341;&#20013;&#35299;&#20915;&#39640;&#32500;&#24230;POMDPs&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#38382;&#39064;&#29305;&#23450;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#65292;&#20197;&#20943;&#23569;&#35268;&#21010;&#26102;&#38388;&#36328;&#24230;&#24182;&#20351;&#38382;&#39064;&#26131;&#20110;&#35299;&#20915;&#12290;&#26368;&#36817;&#25104;&#21151;&#22320;&#22312;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#39046;&#22495;&#20013;&#25214;&#21040;&#20102;&#29992;&#20110;&#26367;&#25442;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#12290;&#20851;&#38190;&#27934;&#35265;&#26159;&#23558;&#22312;&#32447;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#19982;&#31163;&#32447;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#30456;&#32467;&#21512;&#65292;&#20197;&#20248;&#21270;&#31574;&#30053;&#21644;&#20540;&#20989;&#25968;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#27934;&#35265;&#24212;&#29992;&#21040;&#20102;&#37096;&#20998;&#35266;&#23519;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;BetaZero&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;POMDP&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world planning problems$\unicode{x2014}$including autonomous driving and sustainable energy applications like carbon storage and resource exploration$\unicode{x2014}$have recently been modeled as partially observable Markov decision processes (POMDPs) and solved using approximate methods. To solve high-dimensional POMDPs in practice, state-of-the-art methods use online planning with problem-specific heuristics to reduce planning horizons and make the problems tractable. Algorithms that learn approximations to replace heuristics have recently found success in large-scale problems in the fully observable domain. The key insight is the combination of online Monte Carlo tree search with offline neural network approximations of the optimal policy and value function. In this work, we bring this insight to partially observed domains and propose BetaZero, a belief-state planning algorithm for POMDPs. BetaZero learns offline approximations based on accurate belief models to enable online d
&lt;/p&gt;</description></item><item><title>GLOBE-CE&#26159;&#19968;&#31181;&#29992;&#20110;&#20840;&#29699;&#22240;&#26524;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36229;&#36234;&#23616;&#37096;&#35299;&#37322;&#65292;&#25552;&#20379;&#26356;&#26377;&#25928;&#21644;&#20132;&#20114;&#24335;&#30340;&#35299;&#37322;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.17021</link><description>&lt;p&gt;
GLOBE-CE&#65306;&#19968;&#31181;&#29992;&#20110;&#20840;&#29699;&#22240;&#26524;&#35299;&#37322;&#30340;&#22522;&#20110;&#32763;&#35793;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. (arXiv:2305.17021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17021
&lt;/p&gt;
&lt;p&gt;
GLOBE-CE&#26159;&#19968;&#31181;&#29992;&#20110;&#20840;&#29699;&#22240;&#26524;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36229;&#36234;&#23616;&#37096;&#35299;&#37322;&#65292;&#25552;&#20379;&#26356;&#26377;&#25928;&#21644;&#20132;&#20114;&#24335;&#30340;&#35299;&#37322;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35299;&#37322;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20844;&#24179;&#24615;&#12289;&#36861;&#32034;&#26435;&#21644;&#27169;&#22411;&#29702;&#35299;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26368;&#22823;&#30340;&#32570;&#28857;&#26159;&#26080;&#27861;&#25552;&#20379;&#36229;&#36234;&#23616;&#37096;&#25110;&#23454;&#20363;&#32423;&#21035;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#35768;&#22810;&#20316;&#21697;&#28041;&#21450;&#20840;&#23616;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#36890;&#24120;&#24314;&#35758;&#32858;&#21512;&#22823;&#37327;&#23616;&#37096;&#35299;&#37322;&#20197;&#30830;&#23450;&#20840;&#23616;&#23646;&#24615;&#65292;&#20294;&#24456;&#23569;&#25552;&#20379;&#21487;&#38752;&#19988;&#35745;&#31639;&#21487;&#34892;&#30340;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#23454;&#36341;&#32773;&#38656;&#35201;&#26356;&#26377;&#25928;&#21644;&#20132;&#20114;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#12290;&#25105;&#20204;&#20511;&#27492;&#26426;&#20250;&#25552;&#20986;&#20102;&#20840;&#23616;&#19988;&#26377;&#25928;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26694;&#26550;(GLOBE-CE)&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26694;&#26550;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#21644;&#36830;&#32493;&#29305;&#24449;&#23384;&#22312;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods prominent in fairness, recourse and model understanding. The major shortcoming associated with these methods, however, is their inability to provide explanations beyond the local or instance-level. While many works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are both reliable and computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to propose Global &amp; Efficient Counterfactual Explanations (GLOBE-CE), a flexible framework that tackles the reliability and scalability issues associated with current state-of-the-art, particularly on higher dimensional datasets and in the presence of continuous features. Furthermore, we provide a unique mathemati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14561</link><description>&lt;p&gt;
&#36127;&#21453;&#39304;&#35757;&#32451;&#65306;&#25552;&#39640;NVCiM DNN&#21152;&#36895;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#25381;&#21457;&#24615;&#23384;&#20648;&#22120;(NVM)&#23454;&#29616;&#30340;&#20869;&#23384;&#35745;&#31639;(CiM)&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290; CiM&#21152;&#36895;&#22120;&#36890;&#36807;&#22312;&#21516;&#19968;&#30005;&#36335;&#26495;&#32467;&#26500;&#20013;&#23384;&#20648;&#32593;&#32476;&#26435;&#37325;&#21644;&#25191;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20197;&#26368;&#23567;&#30340;&#38754;&#31215;&#38656;&#27714;&#21644;&#24322;&#24120;&#30340;&#33021;&#25928;&#65292;&#25552;&#20379;DNN&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;NVM&#35774;&#22791;&#30340;&#38543;&#26426;&#24615;&#21644;&#20869;&#22312;&#21464;&#21270;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#22914;&#19982;&#39044;&#26399;&#32467;&#26524;&#30456;&#27604;&#20943;&#23569;&#20998;&#31867;&#31934;&#24230;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#35774;&#22791;&#21464;&#24322;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#25972;&#20307;&#35843;&#33410;&#24182;&#32570;&#20047;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#21463;&#21040;&#36127;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20986;&#21475;&#26426;&#21046;&#20316;&#20026;&#36127;&#21453;&#39304;&#65292;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
&lt;/p&gt;</description></item><item><title>ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.14196</link><description>&lt;p&gt;
ZeroSCROLLS&#65306;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14196
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; ZeroSCROLLS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#20165;&#21253;&#21547;&#27979;&#35797;&#38598;&#32780;&#27809;&#26377;&#35757;&#32451;&#25110;&#24320;&#21457;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;SCROLLS&#22522;&#20934;&#27979;&#35797;&#20013;&#36866;&#24212;&#20102;&#20845;&#20010;&#20219;&#21153;&#65292;&#24182;&#28155;&#21152;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20004;&#20010;&#26032;&#30340;&#20449;&#24687;&#34701;&#21512;&#20219;&#21153;&#65292;&#20363;&#22914;&#32858;&#21512;&#27491;&#38754;&#35780;&#20215;&#30340;&#30334;&#20998;&#27604;&#12290;&#20351;&#29992;ZeroSCROLLS&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;Claude&#20248;&#20110;ChatGPT&#65292;&#24182;&#19988;GPT-4&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;ZeroSCROLLS&#30340;&#22810;&#20010;&#24320;&#25918;&#25361;&#25112;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;&#32858;&#21512;&#20219;&#21153;&#65289;&#65292;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#22240;&#20026;&#27169;&#22411;&#24456;&#38590;&#36890;&#36807;&#26420;&#32032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#30001;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36824;&#22312;&#19981;&#26029;&#26356;&#26032;&#65292;&#25105;&#20204;&#36992;&#35831;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#26102;&#30340;ZeroSCROLLS&#25490;&#34892;&#27036;&#19978;&#35780;&#20272;&#20182;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test sets, without training or development data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LCEOPT&#30340;&#31616;&#21333;&#22312;&#32447;&#36830;&#32493;&#21160;&#20316;POMDP&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#31574;&#30053;&#26641;&#30340;&#25042;&#24816;&#20132;&#21449;&#29109;&#25628;&#32034;&#23454;&#29616;&#20102;&#39640;&#25928;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#24615;POMDP&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26368;&#20808;&#36827;&#30340;POMDP&#27714;&#35299;&#22120;&#21487;&#23454;&#29616;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2305.08049</link><description>&lt;p&gt;
&#20196;&#20154;&#24778;&#35766;&#30340;&#31616;&#21333;&#36830;&#32493;&#34892;&#21160;POMDP&#27714;&#35299;&#22120;&#65306;&#22522;&#20110;&#31574;&#30053;&#26641;&#30340;&#25042;&#24816;&#20132;&#21449;&#29109;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
A Surprisingly Simple Continuous-Action POMDP Solver: Lazy Cross-Entropy Search Over Policy Trees. (arXiv:2305.08049v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LCEOPT&#30340;&#31616;&#21333;&#22312;&#32447;&#36830;&#32493;&#21160;&#20316;POMDP&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#31574;&#30053;&#26641;&#30340;&#25042;&#24816;&#20132;&#21449;&#29109;&#25628;&#32034;&#23454;&#29616;&#20102;&#39640;&#25928;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#24615;POMDP&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26368;&#20808;&#36827;&#30340;POMDP&#27714;&#35299;&#22120;&#21487;&#23454;&#29616;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#25552;&#20379;&#20102;&#22312;&#38543;&#26426;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#20294;&#23545;&#20110;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#25552;&#20379;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Lazy Cross-Entropy Search Over Policy Trees (LCEOPT) &#30340;&#31616;&#21333;&#22312;&#32447;POMDP&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#35745;&#21010;&#27493;&#39588;&#20013;&#20351;&#29992;&#25042;&#24816;&#20132;&#21449;&#29109;&#26041;&#27861;&#26469;&#25628;&#32034;&#31574;&#30053;&#26641;&#31354;&#38388;&#65292;&#35813;&#26641;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32500;&#25252;&#19968;&#20010;&#20998;&#24067;&#22312;&#26377;&#21069;&#36884;&#30340;&#26377;&#38480;&#26102;&#38388;&#31574;&#30053;&#26641;&#19978;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#25277;&#26679;&#31574;&#30053;&#12289;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#35780;&#20272;&#23427;&#20204;&#24182;&#23558;&#23427;&#20204;&#37325;&#26032;&#25311;&#21512;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#31574;&#30053;&#19978;&#65292;&#36845;&#20195;&#26356;&#26032;&#27492;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#25042;&#24816;&#30340;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#31574;&#30053;&#26641;&#34920;&#31034;&#26469;&#36991;&#20813;&#31574;&#30053;&#25277;&#26679;&#12289;&#35780;&#20272;&#21644;&#20998;&#24067;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#35745;&#31639;&#12290;&#19982;&#20197;&#21069;&#38024;&#23545;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#30340;&#26368;&#20808;&#36827;&#30340;POMDP&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#36825;&#23548;&#33268;&#21487;&#33410;&#30465;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LCEOPT&#21487;&#20197;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#34892;&#21160;POMDP&#12290;
&lt;/p&gt;
&lt;p&gt;
The Partially Observable Markov Decision Process (POMDP) provides a principled framework for decision making in stochastic partially observable environments. However, computing good solutions for problems with continuous action spaces remains challenging. To ease this challenge, we propose a simple online POMDP solver, called Lazy Cross-Entropy Search Over Policy Trees (LCEOPT). At each planning step, our method uses a lazy Cross-Entropy method to search the space of policy trees, which provide a simple policy representation. Specifically, we maintain a distribution on promising finite-horizon policy trees. The distribution is iteratively updated by sampling policies, evaluating them via Monte Carlo simulation, and refitting them to the top-performing ones. Our method is lazy in the sense that it exploits the policy tree representation to avoid redundant computations in policy sampling, evaluation, and distribution update. This leads to computational savings of up to two orders of magn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#21152;&#24378;&#30340;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#24378;&#20195;&#29702;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#65292;&#20855;&#26377;&#35760;&#24518;&#20808;&#21069;&#25805;&#20316;&#21644;&#29615;&#22659;&#23545;&#35937;&#21487;&#34892;&#24615;&#20004;&#39033;&#39046;&#22495;&#30693;&#35782;&#65292;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.05091</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#25991;&#26412;&#28216;&#25103;&#20013;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge-enhanced Agents for Interactive Text Games. (arXiv:2305.05091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#21152;&#24378;&#30340;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#24378;&#20195;&#29702;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#65292;&#20855;&#26377;&#35760;&#24518;&#20808;&#21069;&#25805;&#20316;&#21644;&#29615;&#22659;&#23545;&#35937;&#21487;&#34892;&#24615;&#20004;&#39033;&#39046;&#22495;&#30693;&#35782;&#65292;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#20132;&#27969;&#26159;&#26234;&#33021;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#38656;&#35201;&#35745;&#31639;&#27169;&#22411;&#23398;&#20064;&#21644;&#25512;&#29702;&#26377;&#20851;&#19990;&#30028;&#27010;&#24565;&#30340;&#30693;&#35782;&#65292;&#20854;&#30417;&#30563;&#31243;&#24230;&#20063;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#21152;&#24378;&#20195;&#29702;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#20004;&#31181;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#29702;&#20013;&#65306;&#20808;&#21069;&#27491;&#30830;&#25805;&#20316;&#30340;&#35760;&#24518;&#21644;&#29615;&#22659;&#20013;&#30456;&#20851;&#23545;&#35937;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#65306;`&#32431;`&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22522;&#20110;&#35760;&#24518;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015; (seq2seq) &#27169;&#22411;&#21644;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340; seq2seq&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication via natural language is a crucial aspect of intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. While there has been significant progress made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding, much of the community has turned to various sequential interactive tasks, as in semi-Markov text-based games, which have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a framework for enabling improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports three representative model classes: `pure' reinforcement learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26580;&#24615;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#27861; SoftCLIP&#65292;&#22312;&#37197;&#23545;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17561</link><description>&lt;p&gt;
SoftCLIP: &#26356;&#26580;&#21644;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#20351; CLIP &#26356;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger. (arXiv:2303.17561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26580;&#24615;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#27861; SoftCLIP&#65292;&#22312;&#37197;&#23545;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20004;&#24180;&#20013;&#65292;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#20854;&#20013;&#37197;&#23545;&#23436;&#20840;&#20114;&#19981;&#24178;&#25200;&#65292;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24120;&#29992;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SoftCLIP&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#32454;&#31890;&#24230;&#20869;&#27169;&#24577;&#33258;&#30456;&#20284;&#24615;&#29983;&#25104;&#30340;&#8220;&#26580;&#24615;&#30446;&#26631;&#8221;&#65292;&#23454;&#29616;&#20102;&#26580;&#24615;&#36328;&#27169;&#24577;&#23545;&#40784;&#12290;&#20869;&#27169;&#24577;&#24341;&#23548;&#33021;&#22815;&#20351;&#24471;&#20004;&#20010;&#37197;&#23545;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#23616;&#37096;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#22810;&#23545;&#22810;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27491;&#26679;&#26412;&#22312;&#26580;&#24615;&#30446;&#26631;&#20998;&#24067;&#20013;&#20173;&#28982;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#31163;&#20998;&#24067;&#20013;&#30340;&#36127;&#26679;&#26412;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#36328;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;&#23545;&#40784;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126; SoftCLIP &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the preceding biennium, vision-language pre-training has achieved noteworthy success on several downstream tasks. Nevertheless, acquiring high-quality image-text pairs, where the pairs are entirely exclusive of each other, remains a challenging task, and noise exists in the commonly used datasets. To address this issue, we propose SoftCLIP, a novel approach that relaxes the strict one-to-one constraint and achieves a soft cross-modal alignment by introducing a softened target, which is generated from the fine-grained intra-modal self-similarity. The intra-modal guidance is indicative to enable two pairs have some local similarities and model many-to-many relationships between the two modalities. Besides, since the positive still dominates in the softened target distribution, we disentangle the negatives in the distribution to further boost the relation alignment with the negatives in the cross-modal learning. Extensive experiments demonstrate the effectiveness of SoftCLIP. In pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#23454;&#39564;&#35777;&#26126;&#25237;&#24433;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12289;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#21644;&#36719;&#26368;&#22823;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23481;&#37327;&#24046;&#24322;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11098</link><description>&lt;p&gt;
&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A closer look at the training dynamics of knowledge distillation. (arXiv:2303.11098v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#23454;&#39564;&#35777;&#26126;&#25237;&#24433;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12289;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#21644;&#36719;&#26368;&#22823;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23481;&#37327;&#24046;&#24322;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#23558;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#20989;&#25968;&#21305;&#37197;&#21644;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#39564;&#35777;&#19977;&#20010;&#37325;&#35201;&#35774;&#35745;&#20915;&#31574;&#65292;&#21363;&#26631;&#20934;&#21270;&#12289;&#36719;&#26368;&#22823;&#20989;&#25968;&#21644;&#25237;&#24433;&#23618;&#20316;&#20026;&#20851;&#38190;&#35201;&#32032;&#65292;&#25105;&#20204;&#26377;&#29702;&#35770;&#22320;&#26174;&#31034;&#20986;&#25237;&#24433;&#22120;&#38544;&#21547;&#22320;&#32534;&#30721;&#20102;&#20851;&#20110;&#36807;&#21435;&#26679;&#26412;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#23398;&#29983;&#25552;&#20379;&#20102;&#20851;&#32852;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#19982;&#25237;&#24433;&#22120;&#30340;&#35757;&#32451;&#21160;&#24577;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#21487;&#33021;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36719;&#26368;&#22823;&#20989;&#25968;&#21487;&#20197;&#29992;&#26469;&#35299;&#20915;&#20219;&#20309;&#26174;&#33879;&#23481;&#37327;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#25110;&#20248;&#20110;&#20854;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classificati
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#38480;&#26597;&#35810;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#30340;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#29992;&#26368;&#23567;&#21270;&#26597;&#35810;&#27425;&#25968;&#30830;&#23450;&#22270;&#30340;s-t&#36830;&#36890;&#24615;&#65292;&#20027;&#35201;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#29992;&#20363;&#20013;&#30830;&#23450;&#25915;&#20987;&#36335;&#24452;&#30340;&#23384;&#22312;&#19982;&#21542;&#12290;</title><link>http://arxiv.org/abs/2302.13036</link><description>&lt;p&gt;
&#26377;&#38480;&#26597;&#35810;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Limited Query Graph Connectivity Test. (arXiv:2302.13036v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13036
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#38480;&#26597;&#35810;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#30340;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#29992;&#26368;&#23567;&#21270;&#26597;&#35810;&#27425;&#25968;&#30830;&#23450;&#22270;&#30340;s-t&#36830;&#36890;&#24615;&#65292;&#20027;&#35201;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#29992;&#20363;&#20013;&#30830;&#23450;&#25915;&#20987;&#36335;&#24452;&#30340;&#23384;&#22312;&#19982;&#21542;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#26597;&#35810;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20855;&#26377;&#20004;&#31181;&#21487;&#33021;&#29366;&#24577;&#65288;&#24320;/&#20851;&#65289;&#30340;&#22270;&#12290;&#36793;&#32536;&#30340;&#29366;&#24577;&#26368;&#21021;&#26159;&#38544;&#34255;&#30340;&#12290;&#25105;&#20204;&#21487;&#20197;&#26597;&#35810;&#36793;&#32536;&#20197;&#25581;&#31034;&#20854;&#29366;&#24577;&#12290;&#32473;&#23450;&#19968;&#20010;&#28304;&#28857;s&#21644;&#19968;&#20010;&#30446;&#26631;&#28857;t&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#35782;&#21035;&#36335;&#24452;&#65288;&#20165;&#30001;&#24320;&#36793;&#32452;&#25104;&#65289;&#25110;&#20999;&#21106;&#65288;&#20165;&#30001;&#20851;&#36793;&#32452;&#25104;&#65289;&#26469;&#27979;&#35797;s-t&#36830;&#36890;&#24615;&#12290;&#26080;&#35770;&#26159;&#21542;&#24314;&#31435;&#20102;&#22270;&#30340;&#36830;&#36890;&#24615;&#65292;&#25105;&#20204;&#37117;&#38480;&#21046;&#26597;&#35810;&#27425;&#25968;&#20026;B&#27425;&#21518;&#20572;&#27490;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#26597;&#35810;&#31574;&#30053;&#65292;&#20351;&#26399;&#26395;&#26597;&#35810;&#27425;&#25968;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#26159;&#22522;&#20110;&#19968;&#20010;&#32593;&#32476;&#20013;&#26159;&#21542;&#23384;&#22312;&#25915;&#20987;&#36335;&#24452;&#30340;&#32593;&#32476;&#23433;&#20840;&#29992;&#20363;&#32780;&#25552;&#20986;&#30340;&#12290;&#36793;&#32536;&#26597;&#35810;&#30001;IT&#31649;&#29702;&#21592;&#30340;&#25163;&#21160;&#24037;&#20316;&#35299;&#20915;&#65292;&#36825;&#26159;&#26368;&#23567;&#21270;&#26597;&#35810;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#21333;&#35843;&#38543;&#26426;&#24067;&#23572;&#20989;&#25968;&#27714;&#20540;&#65288;SBFE&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a combinatorial optimisation model called Limited Query Graph Connectivity Test. We consider a graph whose edges have two possible states (On/Off). The edges' states are hidden initially. We could query an edge to reveal its state. Given a source s and a destination t, we aim to test s-t connectivity by identifying either a path (consisting of only On edges) or a cut (consisting of only Off edges). We are limited to B queries, after which we stop regardless of whether graph connectivity is established. We aim to design a query policy that minimizes the expected number of queries.  Our model is mainly motivated by a cyber security use case where we need to establish whether an attack path exists in a network, between a source and a destination. Edge query is resolved by manual effort from the IT admin, which is the motivation behind query minimization.  Our model is highly related to monotone Stochastic Boolean Function Evaluation (SBFE). There are two existing exact algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;TAILOR&#65292;&#23427;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20505;&#36873;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07317</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithm Selection for Deep Active Learning with Imbalanced Datasets. (arXiv:2302.07317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;TAILOR&#65292;&#23427;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20505;&#36873;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#25928;&#29575;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#25152;&#38656;&#30340;&#26631;&#35760;&#31034;&#20363;&#25968;&#37327;&#65292;&#20294;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#20013;&#65292;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#35777;&#24615;&#33021;&#21487;&#33021;&#20250;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;&#20107;&#20808;&#24456;&#38590;&#30693;&#36947;&#21738;&#31181;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#34920;&#29616;&#33391;&#22909;&#25110;&#26368;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;&#12290;&#23545;&#20110;&#20219;&#20309;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;(meta)&#31639;&#27861;TAILOR (Thompson ActIve Learning algORithm selection)&#36845;&#20195;&#22320;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#19968;&#32452;&#20505;&#36873;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#12290;TAILOR&#20351;&#29992;&#26088;&#22312;&#25910;&#38598;&#31867;&#24179;&#34913;&#31034;&#20363;&#30340;&#26032;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#22823;&#37327;&#23454;&#39564;&#20013;&#65292;TAILOR&#22312;&#23454;&#29616;&#19982;&#20505;&#36873;&#31639;&#27861;&#20013;&#26368;&#20339;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label efficiency has become an increasingly important objective in deep learning applications. Active learning aims to reduce the number of labeled examples needed to train deep networks, but the empirical performance of active learning algorithms can vary dramatically across datasets and applications. It is difficult to know in advance which active learning strategy will perform well or best in a given application. To address this, we propose the first adaptive algorithm selection strategy for deep active learning. For any unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning algORithm selection) iteratively and adaptively chooses among a set of candidate active learning algorithms. TAILOR uses novel reward functions aimed at gathering class-balanced examples. Extensive experiments in multi-class and multi-label applications demonstrate TAILOR's effectiveness in achieving accuracy comparable or better than that of the best of the candidate algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.10343</link><description>&lt;p&gt;
ClimaX:&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10343
&lt;/p&gt;
&lt;p&gt;
ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#20808;&#36827;&#30340;&#22825;&#27668;&#21644;&#27668;&#20505;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25968;&#20540;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#27169;&#25311;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#24456;&#38590;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#36825;&#26679;&#30340;&#25968;&#20540;&#27169;&#22411;&#22312;&#27169;&#25311;&#32454;&#31890;&#24230;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#22823;&#27668;&#29616;&#35937;&#26102;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#21151;&#33021;&#26144;&#23556;&#26469;&#30452;&#25509;&#35299;&#20915;&#19979;&#28216;&#39044;&#27979;&#25110;&#25237;&#23556;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#26159;&#20351;&#29992;&#20026;&#29305;&#23450;&#26102;&#31354;&#20219;&#21153;&#31574;&#21010;&#21644;&#21516;&#36136;&#21270;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#25968;&#20540;&#27169;&#22411;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#28436;&#31034;&#20102;ClimaX&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#36328;&#36234;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;</title><link>http://arxiv.org/abs/2212.01071</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#39578;&#25200;&#34892;&#20026;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#36825;&#23548;&#33268;&#20102;&#34394;&#20551;&#26816;&#27979;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20013;&#24341;&#20154;&#27880;&#30446;&#30340;&#39046;&#22495;&#12290;&#25968;&#25454;&#30340;&#22270;&#24418;&#29305;&#24615;&#20197;&#21450;&#22823;&#37327;&#33410;&#28857;&#23548;&#33268;&#20102;&#35768;&#22810;&#38556;&#30861;&#65292;&#21253;&#25324;&#30697;&#38453;&#20013;&#22823;&#37327;&#26080;&#20851;&#29305;&#24449;&#30340;&#39640;&#31163;&#25955;&#24230;&#21644;&#19981;&#24179;&#34913;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31639;&#27861;&#30340;&#32452;&#21512;&#65292;&#21363;SGAN&#12290;&#26412;&#25991;&#23558;&#23569;&#37327;&#26631;&#31614;&#24212;&#29992;&#20110;SGAN&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#34394;&#20551;&#36134;&#25143;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media grows faster, harassment becomes more prevalent which leads to considered fake detection a fascinating field among researchers. The graph nature of data with the large number of nodes caused different obstacles including a considerable amount of unrelated features in matrices as high dispersion and imbalance classes in the dataset. To deal with these issues Auto-encoders and a combination of semi-supervised learning and the GAN algorithm which is called SGAN were used. This paper is deploying a smaller number of labels and applying SGAN as a classifier. The result of this test showed that the accuracy had reached 91\% in detecting fake accounts using only 100 labeled samples.
&lt;/p&gt;</description></item><item><title>AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.</title><link>http://arxiv.org/abs/2205.12787</link><description>&lt;p&gt;
&#20844;&#27491;&#28216;&#25103;&#65306;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12787
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;AlphaZero&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#26827;&#30424;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20844;&#27491;&#28216;&#25103;&#20013;&#21364;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#20123;&#28216;&#25103;&#20013;&#29609;&#23478;&#20849;&#20139;&#26827;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#28216;&#25103;&#20363;&#23376;&#65292;&#21363;&#23567;&#23401;&#20204;&#29609;&#30340;&#23612;&#22982;&#28216;&#25103;&#65292;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#20844;&#27491;&#28216;&#25103;&#65292;&#36825;&#20123;&#28216;&#25103;&#20284;&#20046;&#25104;&#20026;AlphaZero&#21644;&#31867;&#20284;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32458;&#33050;&#30707;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#34920;&#26126;AlphaZero-style&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#21644;&#25932;&#23545;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#26174;&#31034;&#20102;&#22312;&#25152;&#26377;&#21512;&#27861;&#29366;&#24577;&#19979;&#23398;&#20064;&#25484;&#25569;&#36825;&#20123;&#28216;&#25103;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#23612;&#22982;&#28216;&#25103;&#22312;&#23567;&#22411;&#26827;&#30424;&#19978;&#21487;&#20197;&#23398;&#20064;&#65292;&#20294;&#24403;&#26827;&#30424;&#23610;&#23544;&#22686;&#22823;&#26102;&#65292;AlphaZero-style&#31639;&#27861;&#30340;&#23398;&#20064;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#30452;&#35266;&#19978;&#65292;&#23612;&#22982;&#31561;&#20844;&#27491;&#28216;&#25103;&#19982;&#35937;&#26827;&#21644;&#22260;&#26827;&#31561;&#20826;&#27966;&#28216;&#25103;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#65292;&#22914;&#26524;&#31995;&#32479;&#20013;&#28155;&#21152;&#20102;&#24494;&#23567;&#30340;&#22122;&#38899;&#65288;&#20363;&#22914;&#65292;&#26827;&#30424;&#30340;&#19968;&#23567;&#37096;&#20998;&#34987;&#35206;&#30422;&#65289;&#65292;&#23545;&#20110;&#20844;&#27491;&#28216;&#25103;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning (RL) algorithms excel in various board games but face challenges with impartial games, where players share pieces. We present a concrete example of a game - namely the children's game of nim - and other impartial games that seem to be a stumbling block for AlphaZero-style and similar reinforcement learning algorithms.  Our findings are consistent with recent studies showing that AlphaZero-style algorithms are vulnerable to adversarial attacks and adversarial perturbations, showing the difficulty of learning to master the games in all legal states.  We show that nim can be learned on small boards, but AlphaZero-style algorithms learning dramatically slows down when the board size increases. Intuitively, the difference between impartial games like nim and partisan games like Chess and Go can be explained by the fact that if a tiny amount of noise is added to the system (e.g. if a small part of the board is covered), for impartial games, it is typica
&lt;/p&gt;</description></item></channel></rss>