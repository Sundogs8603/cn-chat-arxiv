<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20013;&#21033;&#29992;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.15202</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;
&lt;/p&gt;
&lt;p&gt;
Predicting Transcription Factor Binding Sites using Transformer based Capsule Network. (arXiv:2310.15202v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20013;&#21033;&#29992;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#30340;&#32467;&#21512;&#20301;&#28857;&#23545;&#20110;&#29702;&#35299;&#23427;&#20204;&#22914;&#20309;&#35843;&#25511;&#22522;&#22240;&#34920;&#36798;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#27835;&#30103;&#25163;&#27573;&#36827;&#34892;&#35843;&#33410;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#24037;&#20316;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#21033;&#29992;ChIP-seq&#25968;&#25454;&#38598;&#25366;&#25496;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;DNABERT-Cap&#26159;&#19968;&#20010;&#21452;&#21521;&#32534;&#30721;&#22120;&#65292;&#32463;&#36807;&#22823;&#37327;&#22522;&#22240;&#32452;DNA&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#33014;&#22218;&#23618;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#23545;&#21253;&#21547;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20116;&#20010;&#32454;&#32990;&#31995;&#30340;&#22522;&#20934;ChIP-seq&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;A54&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of binding sites for transcription factors is important to understand how they regulate gene expression and how this regulation can be modulated for therapeutic purposes. Although in the past few years there are significant works addressing this issue, there is still space for improvement. In this regard, a transformer based capsule network viz. DNABERT-Cap is proposed in this work to predict transcription factor binding sites mining ChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained with large number of genomic DNA sequences, empowered with a capsule layer responsible for the final prediction. The proposed model builds a predictor for transcription factor binding sites using the joint optimisation of features encompassing both bidirectional encoder and capsule layer, along with convolutional and bidirectional long-short term memory layers. To evaluate the efficiency of the proposed approach, we use a benchmark ChIP-seq datasets of five cell lines viz. A54
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.03149</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03149
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#26576;&#20123;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#29305;&#24449;&#65292;&#20316;&#20026;&#20854;&#23545;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#19968;&#37096;&#20998;&#12290;&#30001;&#20110;&#25317;&#26377;&#27491;&#30830;&#65288;&#25110;&#38169;&#35823;&#65289;&#30340;&#27010;&#24565;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25105;&#20204;&#24819;&#35201;&#30693;&#36947;&#22312;&#32473;&#23450;&#23618;&#27425;&#19978;&#65292;&#27169;&#22411;&#21407;&#22987;&#35757;&#32451;&#38598;&#20013;&#30340;&#21738;&#20123;&#36755;&#20837;&#23545;&#20110;&#23398;&#20064;&#19968;&#20010;&#27010;&#24565;&#26368;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#25506;&#27979;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#32593;&#32476;&#23618;&#27425;&#19978;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24402;&#22240;&#30340;TRAK&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27010;&#24565;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#31227;&#38500;&#23545;&#19968;&#20010;&#27010;&#24565;&#20855;&#26377;&#26368;&#39640;&#24402;&#22240;&#30340;&#21069;10000&#24352;&#22270;&#20687;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#27010;&#24565;&#22312;&#32593;&#32476;&#20013;&#30340;&#20301;&#32622;&#20197;&#21450;&#27010;&#24565;&#30340;&#25506;&#27979;&#31232;&#30095;&#24615;&#24182;&#27809;&#26377;&#21457;&#29983;&#25913;&#21464;&#12290;&#36825;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#19981;&#21516;&#65292;&#29992;&#20110;&#30830;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
&lt;/p&gt;</description></item><item><title>Mini-BEHAVIOR&#26159;&#19968;&#20010;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;&#26234;&#33021;&#20307;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#24182;&#36890;&#36807;&#36807;&#31243;&#29983;&#25104;&#23454;&#29616;&#20102;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.01824</link><description>&lt;p&gt;
Mini-BEHAVIOR&#65306;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#38271;&#26399;&#20915;&#31574;&#21046;&#23450;&#30340;&#36807;&#31243;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI. (arXiv:2310.01824v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01824
&lt;/p&gt;
&lt;p&gt;
Mini-BEHAVIOR&#26159;&#19968;&#20010;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;&#26234;&#33021;&#20307;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#24182;&#36890;&#36807;&#36807;&#31243;&#29983;&#25104;&#23454;&#29616;&#20102;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Mini-BEHAVIOR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#25361;&#25112;&#26234;&#33021;&#20307;&#21033;&#29992;&#25512;&#29702;&#21644;&#20915;&#31574;&#25216;&#33021;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#20154;&#31867;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#12290;Mini-BEHAVIOR&#29615;&#22659;&#26159;&#19968;&#20010;&#24555;&#36895;&#65292;&#29616;&#23454;&#30340;Gridworld&#29615;&#22659;&#65292;&#26082;&#20855;&#26377;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#21644;&#26131;&#29992;&#24615;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20063;&#20445;&#30041;&#20102;&#22797;&#26434;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#22522;&#20934;&#20013;&#31526;&#21495;&#32423;&#30340;&#29289;&#29702;&#29616;&#23454;&#24863;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#38190;&#29305;&#24615;&#65292;&#22914;&#36807;&#31243;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;Mini-BEHAVIOR&#25552;&#20379;&#20102;&#21407;&#22987;BEHAVIOR&#22522;&#20934;&#20013;&#21508;&#31181;&#23478;&#21153;&#20219;&#21153;&#30340;&#23454;&#29616;&#65292;&#20197;&#21450;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#35757;&#32451;&#30340;&#20837;&#38376;&#20195;&#30721;&#12290;&#24635;&#20043;&#65292;Mini-BEHAVIOR&#20026;&#35780;&#20272;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#19968;&#20010;&#24555;&#36895;&#12289;&#24320;&#25918;&#24335;&#30340;&#22522;&#20934;&#12290;&#23427;&#20316;&#20026;&#30740;&#31350;&#30340;&#29992;&#25143;&#21451;&#22909;&#30340;&#20837;&#21475;&#28857;&#65292;&#20419;&#36827;&#20102;&#35780;&#20272;&#21644;&#21457;&#23637;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges agents to use reasoning and decision-making skills to solve complex activities that resemble everyday human challenges. The Mini-BEHAVIOR environment is a fast, realistic Gridworld environment that offers the benefits of rapid prototyping and ease of use while preserving a symbolic level of physical realism and complexity found in complex embodied AI benchmarks. We introduce key features such as procedural generation, to enable the creation of countless task variations and support open-ended learning. Mini-BEHAVIOR provides implementations of various household tasks from the original BEHAVIOR benchmark, along with starter code for data collection and reinforcement learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended benchmark for evaluating decision-making and planning solutions in embodied AI. It serves as a user-friendly entry point for research and facilitates the evaluation and devel
&lt;/p&gt;</description></item><item><title>AskIt&#26159;&#19968;&#20010;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#36890;&#36807;&#31616;&#21270;LLM&#30340;&#38598;&#25104;&#21644;&#25552;&#20379;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#35299;&#20915;&#20102;LLM&#23884;&#20837;&#24212;&#29992;&#31243;&#24207;&#21644;&#20195;&#30721;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15645</link><description>&lt;p&gt;
AskIt: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#32534;&#31243;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
AskIt: Unified Programming Interface for Programming with Large Language Models. (arXiv:2308.15645v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15645
&lt;/p&gt;
&lt;p&gt;
AskIt&#26159;&#19968;&#20010;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#36890;&#36807;&#31616;&#21270;LLM&#30340;&#38598;&#25104;&#21644;&#25552;&#20379;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#35299;&#20915;&#20102;LLM&#23884;&#20837;&#24212;&#29992;&#31243;&#24207;&#21644;&#20195;&#30721;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#23637;&#31034;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#29616;&#35937;&#65292;&#21363; emergent abilities&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#29087;&#32451;&#33021;&#21147;&#65292;&#20174;&#25991;&#26412;&#25688;&#35201;&#21040;&#20195;&#30721;&#29983;&#25104;&#12290;&#34429;&#28982;&#36825;&#20123;&#33021;&#21147;&#22312;&#36719;&#20214;&#35774;&#35745;&#21644;&#24320;&#21457;&#26041;&#38754;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#20294;&#23427;&#20204;&#30340;&#25972;&#21512;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#24320;&#21457;&#32773;&#38754;&#20020;&#30528;&#23558;LLMs&#30452;&#25509;&#23884;&#20837;&#24212;&#29992;&#31243;&#24207;&#25110;&#23558;&#20854;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#20013;&#25552;&#21462;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;AskIt&#65292;&#19968;&#31181;&#19987;&#20026;LLMs&#35774;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328; (DSL)&#12290;AskIt&#31616;&#21270;&#20102;LLM&#30340;&#38598;&#25104;&#65292;&#25552;&#20379;&#20102;&#31867;&#22411;&#25351;&#23548;&#30340;&#36755;&#20986;&#25511;&#21046;&#65292;&#22522;&#20110;&#27169;&#26495;&#30340;&#20989;&#25968;&#23450;&#20041;&#20197;&#21450;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#20943;&#23569;&#20102;LLM&#30340;&#22522;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving landscape of software development, Large Language Models (LLMs) exhibit a unique phenomenon known as emergent abilities, demonstrating adeptness across numerous tasks, from text summarization to code generation. While these abilities open up novel avenues in software design and crafting, their incorporation presents substantial challenges. Developers grapple with decisions surrounding the direct embedding of LLMs within applications versus employing them for code generation. Moreover, effective prompt design becomes a critical concern, given the necessity of data extraction from natural language outputs. To address these intricacies, this paper introduces AskIt, a domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies LLM integration, offering type-guided output control, template-based function definitions, and a unified interface that diminishes the distinction between LLM-based code generation and application integration. Furthermore, through 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#21160;&#24577;&#19987;&#23478;&#20132;&#25442;&#20026;MoE&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#26512;MoE&#27169;&#22411;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#20998;&#26512;&#20248;&#21270;&#21442;&#25968;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.15030</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#21160;&#24577;&#19987;&#23478;&#20132;&#25442;&#20026;MoE&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping. (arXiv:2308.15030v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#21160;&#24577;&#19987;&#23478;&#20132;&#25442;&#20026;MoE&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#26512;MoE&#27169;&#22411;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#20998;&#26512;&#20248;&#21270;&#21442;&#25968;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26377;&#26465;&#20214;&#28608;&#27963;&#30340;&#24182;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;(&#19987;&#23478;)&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24310;&#36831;&#20851;&#38190;&#30340;&#36793;&#32536;&#22330;&#26223;&#20013;&#25552;&#20379;MoE&#27169;&#22411;&#30340;&#26381;&#21153;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#26174;&#33879;&#22686;&#21152;&#12290;&#26412;&#25991;&#39318;&#20808;&#20998;&#26512;&#20102;MoE&#27169;&#22411;&#22312;&#36830;&#32493;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#19987;&#23478;&#28608;&#27963;&#30340;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65292;&#21253;&#25324;&#26102;&#38388;&#23616;&#37096;&#24615;&#12289;&#21487;&#20132;&#25442;&#24615;&#21644;&#21487;&#36339;&#36807;&#35745;&#31639;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PC-MoE&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#36830;&#32493;MoE&#27169;&#22411;&#26381;&#21153;&#30340;&#25512;&#29702;&#26694;&#26550;&#12290;PC-MoE&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#31216;&#20026;&#8220;&#21442;&#25968;&#22996;&#21592;&#20250;&#8221;&#65292;&#23427;&#26234;&#33021;&#22320;&#32500;&#25252;&#19968;&#37096;&#20998;&#37325;&#35201;&#30340;&#27491;&#22312;&#20351;&#29992;&#30340;&#19987;&#23478;&#65292;&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#22522;&#20110;&#24615;&#33021;&#20998;&#26512;&#30340;&#22996;&#21592;&#20250;&#35268;&#21010;&#22120;&#65292;&#22312;&#32447;&#25214;&#21040;&#21442;&#25968;&#22996;&#21592;&#20250;&#30340;&#26368;&#20339;&#37197;&#32622;&#65292;&#24182;&#36827;&#34892;&#19987;&#23478;&#20132;&#25442;&#21644;&#35831;&#27714;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture of experts (MoE) is a popular technique in deep learning that improves model capacity with conditionally-activated parallel neural network modules (experts). However, serving MoE models in resource-constrained latency-critical edge scenarios is challenging due to the significantly increased model size and complexity. In this paper, we first analyze the behavior pattern of MoE models in continuous inference scenarios, which leads to three key observations about the expert activations, including temporal locality, exchangeability, and skippable computation. Based on these observations, we introduce PC-MoE, an inference framework for resource-constrained continuous MoE model serving. The core of PC-MoE is a new data structure, Parameter Committee, that intelligently maintains a subset of important experts in use to reduce resource consumption. The optimal configuration of Parameter Committee is found offline by a profiling-guided committee planner, and expert swapping and request 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26234;&#33021;&#20307;&#36991;&#24320;&#25317;&#22581;&#36335;&#24452;&#26469;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#24635;&#20307;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.11234</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#20248;&#21270;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding. (arXiv:2308.11234v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26234;&#33021;&#20307;&#36991;&#24320;&#25317;&#22581;&#36335;&#24452;&#26469;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#24635;&#20307;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#35201;&#27714;&#20026;&#19968;&#20010;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#35745;&#31639;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#25152;&#26377;&#26234;&#33021;&#20307;&#37117;&#22312;&#20849;&#20139;&#22320;&#22270;&#19978;&#31227;&#21160;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#30456;&#20851;&#30740;&#31350;&#65292;&#20294;&#24403;&#21069;&#30340;&#31639;&#27861;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#26102;&#37117;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35268;&#21010;&#33258;&#30001;&#27969;&#21160;&#30340;&#26368;&#20248;&#36335;&#24452;&#65292;&#36825;&#20250;&#23548;&#33268;&#25317;&#22581;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MAPF&#26041;&#27861;&#65292;&#36890;&#36807;&#36319;&#38543;&#36991;&#20813;&#25317;&#22581;&#30340;&#36335;&#24452;&#26469;&#24341;&#23548;&#26234;&#33021;&#20307;&#21040;&#36798;&#30446;&#30340;&#22320;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#36825;&#20010;&#24819;&#27861;&#65306;&#19968;&#27425;&#24615;MAPF&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#26377;&#19968;&#20010;&#30446;&#30340;&#22320;&#65292;&#20197;&#21450;&#32456;&#36523;MAPF&#65292;&#26234;&#33021;&#20307;&#19981;&#26029;&#34987;&#20998;&#37197;&#26032;&#20219;&#21153;&#12290;&#23545;&#20110;&#19968;&#27425;&#24615;MAPF&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;&#23545;&#20110;&#32456;&#36523;MAPF&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#24635;&#20307;&#21534;&#21520;&#37327;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that asks us to compute collision-free paths for a team of agents, all moving across a shared map. Although many works appear on this topic, all current algorithms struggle as the number of agents grows. The principal reason is that existing approaches typically plan free-flow optimal paths, which creates congestion. To tackle this issue we propose a new approach for MAPF where agents are guided to their destination by following congestion-avoiding paths. We evaluate the idea in two large-scale settings: one-shot MAPF, where each agent has a single destination, and lifelong MAPF, where agents are continuously assigned new tasks. For one-shot MAPF we show that our approach substantially improves solution quality. For Lifelong MAPF we report large improvements in overall throughput.
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16082</link><description>&lt;p&gt;
EnrichEvent: &#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#26032;&#20986;&#29616;&#30340;&#20107;&#20214;&#25552;&#20379;&#20016;&#23500;&#30340;&#31038;&#20132;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#24179;&#21488;&#24050;&#25104;&#20026;&#20256;&#25773;&#21644;&#35752;&#35770;&#30495;&#23454;&#20107;&#20214;&#20449;&#24687;&#30340;&#20851;&#38190;&#24179;&#21488;&#65292;&#20026;&#21450;&#26089;&#21457;&#29616;&#26377;&#26032;&#38395;&#20215;&#20540;&#30340;&#20107;&#20214;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#20165;&#21033;&#29992;&#20851;&#38190;&#35789;&#31361;&#21457;&#24615;&#25110;&#32593;&#32476;&#32467;&#26500;&#26469;&#26816;&#27979;&#28909;&#28857;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20107;&#20214;&#21644;&#31038;&#20132;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35328;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#22312;&#36798;&#21040;&#36235;&#21183;&#29366;&#24577;&#20043;&#21069;&#35782;&#21035;&#20986;&#26032;&#20986;&#29616;&#30340;&#31038;&#20132;&#20107;&#20214;&#12290;&#31038;&#20132;&#25968;&#25454;&#65292;&#20363;&#22914;&#25512;&#25991;&#65292;&#20855;&#26377;&#25340;&#20889;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#12289;&#27495;&#20041;&#24615;&#21644;&#35821;&#35328;&#19981;&#35268;&#33539;&#24615;&#65292;&#20197;&#21450;&#24847;&#35265;&#26041;&#38754;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#23398;&#20064;&#20107;&#20214;&#30340;&#28436;&#21464;&#29305;&#24449;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#27969;&#24335;&#31038;&#20132;&#25968;&#25454;&#30340;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#21457;&#29616;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#21024;&#38500;&#24207;&#21015;&#26411;&#23614;&#30340;&#39033;&#30446;&#26174;&#33879;&#38477;&#20302;&#20102;&#24615;&#33021;&#65292;&#32780;&#21024;&#38500;&#24207;&#21015;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#39033;&#30446;&#21017;&#27809;&#26377;&#26126;&#26174;&#24433;&#21709;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#20013;&#25200;&#21160;&#39033;&#30446;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#33021;&#25351;&#23548;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.13165</link><description>&lt;p&gt;
&#30740;&#31350;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#23545;&#35757;&#32451;&#25968;&#25454;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65306;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations: an Empirical Study. (arXiv:2307.13165v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#21457;&#29616;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#21024;&#38500;&#24207;&#21015;&#26411;&#23614;&#30340;&#39033;&#30446;&#26174;&#33879;&#38477;&#20302;&#20102;&#24615;&#33021;&#65292;&#32780;&#21024;&#38500;&#24207;&#21015;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#39033;&#30446;&#21017;&#27809;&#26377;&#26126;&#26174;&#24433;&#21709;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#20013;&#25200;&#21160;&#39033;&#30446;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#33021;&#25351;&#23548;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#34987;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34892;&#20026;&#65292;&#28982;&#32780;&#20854;&#22312;&#38754;&#23545;&#35757;&#32451;&#25968;&#25454;&#25200;&#21160;&#26102;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#26102;&#38388;&#39034;&#24207;&#24207;&#21015;&#20013;&#19981;&#21516;&#20301;&#32622;&#19978;&#21024;&#38500;&#39033;&#30446;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#25240;&#29616;&#32047;&#31215;&#22686;&#30410;&#65288;NDCG&#65289;&#25351;&#26631;&#21644;&#25490;&#21517;&#25935;&#24863;&#24230;&#21015;&#34920;&#65288;Rank Sensitivity List&#65289;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21024;&#38500;&#24207;&#21015;&#26411;&#23614;&#30340;&#39033;&#30446;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#65292;NDCG&#19979;&#38477;&#39640;&#36798;60&#65285;&#65292;&#32780;&#21024;&#38500;&#24207;&#21015;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#39033;&#30446;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#20013;&#25200;&#21160;&#39033;&#30446;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21487;&#25351;&#23548;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommender Systems (SRSs) have been widely used to model user behavior over time, but their robustness in the face of perturbations to training data is a critical issue. In this paper, we conduct an empirical study to investigate the effects of removing items at different positions within a temporally ordered sequence. We evaluate two different SRS models on multiple datasets, measuring their performance using Normalized Discounted Cumulative Gain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that removing items at the end of the sequence significantly impacts performance, with NDCG decreasing up to 60\%, while removing items from the beginning or middle has no significant effect. These findings highlight the importance of considering the position of the perturbed items in the training data and shall inform the design of more robust SRSs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PreDiff&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#20197;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2307.10422</link><description>&lt;p&gt;
PreDiff: &#20351;&#29992;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PreDiff&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#20197;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22320;&#29699;&#31995;&#32479;&#39044;&#27979;&#20027;&#35201;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#35745;&#31639;&#37327;&#22823;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26102;&#31354;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#31354;&#21069;&#22686;&#21152;&#20351;&#24471;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#22320;&#29699;&#31995;&#32479;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#25928;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#35201;&#20040;&#38590;&#20197;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#24573;&#35270;&#29305;&#23450;&#39046;&#22495;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#27169;&#31946;&#25110;&#20135;&#29983;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26102;&#31354;&#39044;&#27979;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#65306;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;PreDiff&#30340;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#65307;2&#65289;&#25105;&#20204;&#34701;&#20837;&#20102;&#19968;&#31181;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#20351;&#39044;&#27979;&#31526;&#21512;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20272;&#35745;&#19982;&#25152;&#26045;&#21152;&#32422;&#26463;&#30340;&#20559;&#24046;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#24773;&#20917;&#19979;&#36798;&#21040;&#23433;&#20840;&#31574;&#30053;&#30340;&#20248;&#21270;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17052</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2306.17052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#24773;&#20917;&#19979;&#36798;&#21040;&#23433;&#20840;&#31574;&#30053;&#30340;&#20248;&#21270;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#65292;&#27604;&#22914;&#20849;&#20139;&#20132;&#36890;&#65292;&#38656;&#35201;&#21327;&#35843;&#22823;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20248;&#21270;&#20195;&#34920;&#24615;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#26469;&#24212;&#23545;&#30001;&#27492;&#24102;&#26469;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#21363;&#26234;&#33021;&#20307;&#20998;&#24067;&#23384;&#22312;&#20840;&#23616;&#32422;&#26463;&#30340;&#24773;&#20917;&#65288;&#20363;&#22914;&#38656;&#35201;&#28385;&#36275;&#23481;&#37327;&#32422;&#26463;&#25110;&#26368;&#23567;&#35206;&#30422;&#35201;&#27714;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23433;&#20840;&#31574;&#30053;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23427;&#22312;&#20445;&#35777;&#24754;&#35266;&#32422;&#26463;&#28385;&#36275;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#36716;&#31227;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#26469;&#20351;&#29992;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#30830;&#20445;&#39640;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#20849;&#20139;&#20132;&#36890;&#36816;&#33829;&#21830;&#38754;&#20020;&#30340;&#36710;&#36742;&#37325;&#23450;&#20301;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;Safe-$\text{M}^3$-UCRL&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#28145;&#22323;&#20986;&#31199;&#36710;&#36712;&#36857;&#25968;&#25454;&#30340;&#20223;&#30495;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#28385;&#36275;&#20851;&#38190;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications, e.g., in shared mobility, require coordinating a large number of agents. Mean-field reinforcement learning addresses the resulting scalability challenge by optimizing the policy of a representative agent. In this paper, we address an important generalization where there exist global constraints on the distribution of agents (e.g., requiring capacity constraints or minimum coverage requirements to be met). We propose Safe-$\text{M}^3$-UCRL, the first model-based algorithm that attains safe policies even in the case of unknown transition dynamics. As a key ingredient, it uses epistemic uncertainty in the transition model within a log-barrier approach to ensure pessimistic constraints satisfaction with high probability. We showcase Safe-$\text{M}^3$-UCRL on the vehicle repositioning problem faced by many shared mobility operators and evaluate its performance through simulations built on Shenzhen taxi trajectory data. Our algorithm effectively meets the demand in critica
&lt;/p&gt;</description></item><item><title>GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04607</link><description>&lt;p&gt;
&#23558;&#20960;&#20309;&#25511;&#21046;&#38598;&#25104;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt. (arXiv:2306.04607v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04607
&lt;/p&gt;
&lt;p&gt;
GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#21019;&#24314;&#20869;&#23481;&#21644;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;&#19981;&#20165;&#22270;&#20687;&#27700;&#24179;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#32780;&#19988;&#36793;&#30028;&#26694;&#21644;&#30456;&#26426;&#35270;&#22270;&#31561;&#20960;&#20309;&#26465;&#20214;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#21069;&#26399;&#30740;&#31350;&#20351;&#29992;&#27169;&#22359;&#32534;&#30721;&#35821;&#20041;&#24067;&#23616;&#26469;&#23454;&#29616;&#22797;&#21046;&#31896;&#36148;&#21512;&#25104;&#25110;&#24067;&#23616;&#21040;&#22270;&#20687;(L2I)&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GeoDiffusion&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#12290;&#19982;&#20197;&#24448;&#30340;L2I&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;GeoDiffusion&#19981;&#20165;&#33021;&#22815;&#32534;&#30721;&#36793;&#30028;&#26694;&#65292;&#36824;&#33021;&#22815;&#32534;&#30721;&#33258;&#39550;&#22330;&#26223;&#20013;&#30340;&#39069;&#22806;&#20960;&#20309;&#26465;&#20214;&#65292;&#22914;&#25668;&#20687;&#22836;&#35270;&#22270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GeoDiffusion&#22312;&#29289;&#20307;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have attracted significant attention due to their remarkable ability to create content and generate data for tasks such as image classification. However, the usage of diffusion models to generate high-quality object detection data remains an underexplored area, where not only the image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image (L2I) generation with specifically designed modules to encode semantic layouts. In this paper, we propose GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower the pre-trained text-to-image (T2I) diffusion models for high-quality detection data generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not only bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.04047</link><description>&lt;p&gt;
&#29992;&#20110;&#25913;&#36827;&#35270;&#21548;&#34701;&#21512;&#23548;&#33322;&#30340;&#20027;&#21160;&#31232;&#30095;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Active Sparse Conversations for Improved Audio-Visual Embodied Navigation. (arXiv:2306.04047v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39640;&#25928;&#22320;&#23548;&#33322;&#21040;&#19968;&#20010;&#21548;&#35273;&#30446;&#26631;&#65292;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#33258;&#20027;&#26435;&#30340;&#23454;&#20307;&#24517;&#39035;&#19981;&#20165;&#35201;&#26377;&#33021;&#21147;&#26377;&#25928;&#22320;&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;, &#32780;&#19988;&#36824;&#35201;&#26377;&#33021;&#21147;&#22312;&#19981;&#29306;&#29298;&#33258;&#20027;&#24615;&#30340;&#24773;&#20917;&#19979;&#20027;&#21160;&#23547;&#27714;&#20154;&#31867;/&#31070;&#35861;&#30340;&#24110;&#21161;&#65292;&#20363;&#22914;&#65292;&#24403;&#19981;&#30830;&#23450;&#23548;&#33322;&#21040;&#21738;&#37324;&#23547;&#25214;&#22024;&#26434;&#25110;&#38388;&#27463;&#24615;&#21548;&#35273;&#30446;&#26631;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAVEN-&#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#30340;&#33258;&#30001;&#24418;&#24335;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#12290;&#22312;CAVEN&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;(RL)&#35774;&#32622;&#65292;&#23427;&#37197;&#22791;&#20102;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#27599;&#19968;&#27493;&#20174;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#20013;&#36873;&#25321;&#19968;&#20010;&#65292;&#21363;&#65306;(i)&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#65292;&#25110;(ii)&#21521;&#31070;&#35861;&#25552;&#20986;&#38382;&#39064;&#24182;&#25509;&#25910;&#30701;&#25110;&#35814;&#32454;&#30340;&#22238;&#31572;&#65292;&#25110;(iii)&#25552;&#38382;&#26222;&#36941;&#38382;&#39064;(&#24403;&#19981;&#30830;&#23450;&#35813;&#38382;&#20160;&#20040;&#26102;)&#24182;&#33719;&#24471;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Efficient navigation towards an audio-goal necessitates an embodied agent to not only possess the ability to use audio-visual cues effectively, but also be equipped to actively (but occasionally) seek human/oracle assistance without sacrificing autonomy, e.g., when it is uncertain of where to navigate towards locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a conversational audio-visual embodied navigation agent that is capable of posing navigation questions to a human/oracle and processing the oracle responses; both in free-form natural language. At the core of CAVEN is a multimodal hierarchical reinforcement learning (RL) setup that is equipped with a high-level policy that is trained to choose from one of three low-level policies (at every step), namely: (i) to navigate using audio-visual cues, or (ii) to frame a question to the oracle and receive a short or detailed response, or (iii) ask generic questions (when unsure of what to ask) and receive instructio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#23553;&#31105;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#22403;&#22334;&#37038;&#20214;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#35753;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.03502</link><description>&lt;p&gt;
&#20420;&#20044;&#25112;&#20105;&#65306;&#39044;&#27979;&#21644;&#35299;&#37322;Twitter&#30340;&#23553;&#31105;
&lt;/p&gt;
&lt;p&gt;
Russo-Ukrainian War: Prediction and explanation of Twitter suspension. (arXiv:2306.03502v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#23553;&#31105;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#22403;&#22334;&#37038;&#20214;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#35753;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;2&#26376;24&#26085;&#65292;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#65292;&#24320;&#22987;&#20102;&#29616;&#22312;&#24050;&#30693;&#30340;&#20420;&#20044;&#25112;&#20105;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24341;&#21457;&#20102;&#22312;&#32447;&#35805;&#35821;&#12290;Twitter&#20316;&#20026;&#26368;&#21463;&#27426;&#36814;&#30340;&#31038;&#20132;&#32593;&#32476;&#20043;&#19968;&#65292;&#20197;&#20854;&#24320;&#25918;&#21644;&#27665;&#20027;&#30340;&#29305;&#28857;&#65292;&#22312;&#20854;&#24222;&#22823;&#30340;&#29992;&#25143;&#32676;&#20013;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#35752;&#35770;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;Twitter&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#28389;&#29992;&#34892;&#20026;&#12289;&#20405;&#29359;&#20844;&#27665;&#26435;&#21033;&#65292;&#22240;&#27492;&#23548;&#33268;&#29992;&#25143;&#36134;&#25143;&#34987;&#23553;&#31105;&#21644;&#21024;&#38500;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#25506;&#35752;&#20102;Twitter&#30340;&#23553;&#31105;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#21487;&#33021;&#23548;&#33268;&#36134;&#25143;&#34987;&#23553;&#31105;&#30340;&#20849;&#20139;&#20869;&#23481;&#21644;&#29992;&#25143;&#36134;&#25143;&#30340;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;Twitter API&#33719;&#24471;&#20102;&#21253;&#21547;107.7M&#26465;&#25512;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;980&#19975;&#29992;&#25143;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#34987;&#23553;&#31105;&#36134;&#25143;&#30340;&#20849;&#20139;&#20869;&#23481;&#31867;&#21035;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#32858;&#31867;&#26469;&#35299;&#37322;&#20854;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20123;&#28389;&#29992;Twitter&#25919;&#31574;&#26631;&#20934;&#30340;&#39575;&#23376;&#27963;&#21160;&#12289;&#22403;&#22334;&#37038;&#20214;&#21644;&#23459;&#20256;&#27963;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#31881;&#19997;&#25968;&#36739;&#23569;&#30340;&#36134;&#25143;&#65292;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#26377;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#20026;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#65292;&#26368;&#23567;&#21270;&#26377;&#23475;&#20869;&#23481;&#30340;&#20256;&#25773;&#25552;&#20379;&#26377;&#29992;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
On 24 February 2022, Russia invaded Ukraine, starting what is now known as the Russo-Ukrainian War, initiating an online discourse on social media. Twitter as one of the most popular SNs, with an open and democratic character, enables a transparent discussion among its large user base. Unfortunately, this often leads to Twitter's policy violations, propaganda, abusive actions, civil integrity violation, and consequently to user accounts' suspension and deletion. This study focuses on the Twitter suspension mechanism and the analysis of shared content and features of the user accounts that may lead to this. Toward this goal, we have obtained a dataset containing 107.7M tweets, originating from 9.8 million users, using Twitter API. We extract the categories of shared content of the suspended accounts and explain their characteristics, through the extraction of text embeddings in junction with cosine similarity clustering. Our results reveal scam campaigns taking advantage of trending top
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18365</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#21040;&#24213;&#26377;&#24590;&#26679;&#30340;&#24212;&#29992;&#65311;&#20843;&#20010;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#36719;&#20214;&#24037;&#31243;&#31561;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;LLM &#26159;&#21542;&#26377;&#33021;&#21147;&#25512;&#21160;&#21270;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#21547; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21517;&#31216;&#39044;&#27979;&#12289;&#23646;&#24615;&#39044;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#12289;&#21453;&#24212;&#39044;&#27979;&#12289;&#21453;&#21512;&#25104;&#65288;&#20174;&#20135;&#29289;&#39044;&#27979;&#21453;&#24212;&#29289;&#65289;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#23376;&#35774;&#35745;&#12289;&#20998;&#23376;&#23383;&#24149;&#21644;&#35797;&#21058;&#36873;&#25321;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#35748;&#21487;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; BBBP&#12289;Tox21&#12289;PubChem&#12289;USPTO &#21644; ChEBI&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#31034;&#20363;&#20013;&#65292;&#23545;&#19977;&#31181; GPT &#27169;&#22411;&#65288;GPT-4&#12289;GPT-3.5 &#21644; DaVinci-003&#65289;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been rapidly applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper,we establish a comprehensive benchmark containing 8 practical chemistry tasks, including 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6)text-based molecule design, 7) molecule captioning, and 8) reagent selection. Our analysis draws on widely recognized datasets including BBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Three GPT models (GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09480</link><description>&lt;p&gt;
&#20132;&#21449;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#19979;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#25239;&#20307;&#35774;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#30001;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#38024;&#23545;&#22806;&#26469;&#29289;&#36136;&#25110;&#25239;&#21407;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#12290;&#25239;&#20307;&#30340;&#29305;&#24322;&#24615;&#30001;&#20854;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#20915;&#23450;&#65292;CDR&#20301;&#20110;&#25239;&#20307;&#38142;&#30340;&#21487;&#21464;&#21306;&#22495;&#20013;&#65292;&#24418;&#25104;&#19982;&#25239;&#21407;&#32467;&#21512;&#30340;&#20301;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;&#25216;&#26415;&#29983;&#25104;CDR&#65292;&#20294;&#23427;&#20204;&#36973;&#21463;&#20102;&#20960;&#20309;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#36845;&#20195;&#31934;&#21270;&#31574;&#30053;&#23548;&#33268;&#20102;&#20302;&#25928;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25239;&#20307;CDR&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#65288;ii&#65289;&#24207;&#21015;&#32467;&#26500;&#20849;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#65292;&#21487;&#25429;&#25417;&#34507;&#30333;&#36136;&#39592;&#26550;&#21407;&#23376;&#65288;&#21253;&#25324;C&#945;&#12289;N&#12289;C&#21644;O&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#32452;&#20998;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#20960;&#20309;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03236</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#26497;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#32508;&#36848;&#20102;OOD&#26816;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;OOD&#26816;&#27979;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#20010;&#30456;&#20851;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26368;&#36817;&#30340;&#31639;&#27861;&#20998;&#25104;&#19977;&#31867;&#65306;&#65288;1&#65289;&#21487;&#29992;OOD&#25968;&#25454;&#65292;&#65288;2&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#26631;&#31614;&#21487;&#29992;&#65292;&#65288;3&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+ID&#26631;&#31614;&#19981;&#21487;&#29992;&#12290;&#31532;&#19977;&#65292;&#20171;&#32461;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#24635;&#32467;&#29616;&#26377;&#24037;&#20316;&#24182;&#25552;&#20986;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#35838;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#20803;Kemeny&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31354;&#38388;&#32553;&#20943;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#39640;&#25928;&#30830;&#23450;&#22791;&#36873;&#26041;&#26696;&#30340;&#30456;&#23545;&#39034;&#24207;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.00140</link><description>&lt;p&gt;
&#19977;&#20803;Kemeny&#38382;&#39064;&#30340;&#31354;&#38388;&#32553;&#20943;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Space reduction techniques for the $3$-wise Kemeny problem. (arXiv:2305.00140v1 [cs.DM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#20803;Kemeny&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31354;&#38388;&#32553;&#20943;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#39640;&#25928;&#30830;&#23450;&#22791;&#36873;&#26041;&#26696;&#30340;&#30456;&#23545;&#39034;&#24207;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kemeny&#35268;&#21017;&#26159;&#26368;&#24191;&#27867;&#30740;&#31350;&#21644;&#30693;&#21517;&#24230;&#24456;&#39640;&#30340;&#36873;&#20030;&#26041;&#26696;&#20043;&#19968;&#65292;&#20855;&#26377;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#21644;&#29983;&#29289;&#23398;&#31561;&#22810;&#31181;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;Gilbert&#31561;&#20154;&#36890;&#36807;&#38598;&#21512;&#26041;&#27861;&#25512;&#24191;&#20102;Kemeny&#35268;&#21017;&#12290;&#37325;&#22609;&#35813;&#27169;&#24335;&#65292;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#19982;&#21476;&#20856;Kemeny&#35268;&#21017;&#30456;&#27604;&#19979;&#65292;&#30001;&#19977;&#20803;Kendall-tau&#36317;&#31163;&#24341;&#20986;&#30340;&#19977;&#20803;Kemeny&#25237;&#31080;&#26041;&#26696;&#20855;&#26377;&#26377;&#36259;&#30340;&#20248;&#21183;&#12290;&#34429;&#28982;&#19977;&#20803;Kemeny&#38382;&#39064;&#65288;&#35745;&#31639;&#25237;&#31080;&#37197;&#32622;&#30340;&#19977;&#20803;&#20849;&#35782;&#25490;&#21517;&#38598;&#21512;&#65289;&#26159;NP&#38590;&#30340;&#65292;&#20294;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#20960;&#20010;&#20027;&#24207;&#23450;&#29702;&#30340;&#25512;&#24191;&#65292;&#22914;&#21516;&#32463;&#20856;Kemeny&#35268;&#21017;&#20013;&#22312;\cite{Milosz-Hamel-2020}&#20013;&#25152;&#24471;&#21040;&#30340;&#65292;&#20026;&#20102;&#23454;&#29616;&#22823;&#24133;&#22320;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#65292;&#36890;&#36807;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#39640;&#25928;&#22320;&#30830;&#23450;&#25104;&#23545;&#22791;&#36873;&#26041;&#26696;&#30340;&#30456;&#23545;&#39034;&#24207;&#12290;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#30340;&#23450;&#29702;&#31934;&#30830;&#37327;&#21270;&#20102;&#38750;&#24179;&#20961;&#23646;&#24615;&#65292;&#21363;&#24403;&#23384;&#22312;&#20219;&#24847;&#25968;&#37327;&#30340;&#25237;&#31080;&#32773;&#26102;&#65292;Kemeny&#35780;&#20998;&#30340;&#35745;&#31639;&#20855;&#26377;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kemeny's rule is one of the most studied and well-known voting schemes with various important applications in computational social choice and biology. Recently, Kemeny's rule was generalized via a set-wise approach by Gilbert et. al. Following this paradigm, we have shown in \cite{Phung-Hamel-2023} that the $3$-wise Kemeny voting scheme induced by the $3$-wise Kendall-tau distance presents interesting advantages in comparison with the classical Kemeny rule. While the $3$-wise Kemeny problem, which consists of computing the set of $3$-wise consensus rankings of a voting profile, is NP-hard, we establish in this paper several generalizations of the Major Order Theorems, as obtained in \cite{Milosz-Hamel-2020} for the classical Kemeny rule, for the $3$-wise Kemeny voting scheme to achieve a substantial search space reduction by efficiently determining in polynomial time the relative orders of pairs of alternatives. Essentially, our theorems quantify precisely the non-trivial property that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#38480;&#21046;&#65292;&#21516;&#26102;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#65292;&#24314;&#31435;&#20102;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#21450;&#20854;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335; HATRPO &#21644; HAPPO&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09870</link><description>&lt;p&gt;
&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#38480;&#21046;&#65292;&#21516;&#26102;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#65292;&#24314;&#31435;&#20102;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#21450;&#20854;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335; HATRPO &#21644; HAPPO&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21442;&#25968;&#20849;&#20139;&#65292;&#36825;&#23558;&#23427;&#20204;&#38480;&#21046;&#22312;&#21516;&#36136;&#24322;&#26500;&#26234;&#33021;&#20307;&#35774;&#32622;&#19979;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#32570;&#20047;&#25910;&#25947;&#20445;&#35777;&#12290;&#20026;&#20102;&#22312;&#19968;&#33324;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#35774;&#32622;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26680;&#24515;&#26159;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#12290;&#22522;&#20110;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32463;&#36807;&#39564;&#35777;&#30340;&#26080;&#21442;&#25968;&#20849;&#20139;&#32422;&#26463;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335;&#24471;&#20986;&#20102;HATRPO&#21644;HAPPO&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL) that is free of parameter-sharing constraint, and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21738;&#20123;&#28304;&#20219;&#21153;&#30340;&#23376;&#38598;&#20250;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2303.14582</link><description>&lt;p&gt;
&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#35782;&#21035;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Identification of Negative Transfers in Multitask Learning Using Surrogate Models. (arXiv:2303.14582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21738;&#20123;&#28304;&#20219;&#21153;&#30340;&#23376;&#38598;&#20250;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#30456;&#20851;&#28304;&#20219;&#21153;&#26469;&#35757;&#32451;&#20302;&#36164;&#28304;&#30446;&#26631;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#25152;&#26377;&#28304;&#20219;&#21153;&#19982;&#30446;&#26631;&#20219;&#21153;&#31616;&#21333;&#32452;&#21512;&#24182;&#19981;&#24635;&#26159;&#33021;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22240;&#20026;&#20250;&#23384;&#22312;&#36127;&#36801;&#31227;&#12290;&#22240;&#27492;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#35782;&#21035;&#21738;&#20123;&#28304;&#20219;&#21153;&#30340;&#23376;&#38598;&#20250;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#30410;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23376;&#38598;&#30340;&#25968;&#37327;&#38543;&#30528;&#28304;&#20219;&#21153;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#20195;&#29702;&#24314;&#27169;&#20013;&#65292;&#25105;&#20204;&#23545;&#28304;&#20219;&#21153;&#36827;&#34892;&#37319;&#26679;&#65288;&#38543;&#26426;&#65289;&#65292;&#24182;&#39044;&#20808;&#35745;&#31639;&#23427;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#34920;&#29616;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#36924;&#36817;&#39044;&#20808;&#35745;&#31639;&#30340;&#34920;&#29616;&#65292;&#35813;&#27169;&#22411;&#20063;&#21487;&#29992;&#20110;&#39044;&#27979;&#26410;&#37319;&#26679;&#30340;&#23376;&#38598;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#31034;&#20363;&#21644;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning is widely used in practice to train a low-resource target task by augmenting it with multiple related source tasks. Yet, naively combining all the source tasks with a target task does not always improve the prediction performance for the target task due to negative transfers. Thus, a critical problem in multitask learning is identifying subsets of source tasks that would benefit the target task. This problem is computationally challenging since the number of subsets grows exponentially with the number of source tasks; efficient heuristics for subset selection does not always capture the relationship between task subsets and multitask learning performances. In this paper, we introduce an efficient procedure to address this problem via surrogate modeling. In surrogate modeling, we sample (random) subsets of source tasks and precompute their multitask learning performances; Then, we approximate the precomputed performances with a linear regression model that can also be
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.12040</link><description>&lt;p&gt;
&#21327;&#21516;&#20154;&#24037;&#26234;&#33021;&#30340;&#26681;&#28304;&#21644;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Roots and Requirements for Collaborative AI. (arXiv:2303.12040v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12040
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21327;&#20316;&#32773;&#30340;&#24895;&#26223;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31185;&#24187;&#23567;&#35828;&#30340;&#32463;&#20856;&#32032;&#26448;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#21327;&#20316;&#21644;&#20154;&#31867;&#27807;&#36890;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#23427;&#20204;&#36890;&#36807;&#36129;&#29486;&#29305;&#27530;&#30340;&#25165;&#33021;&#32473;&#20182;&#20204;&#30340;&#20154;&#31867;&#21512;&#20316;&#32773;&#21644;&#22242;&#38431;&#24102;&#26469;&#20248;&#21183;&#12290;&#22810;&#24180;&#26469;&#65292;&#25919;&#24220;&#21672;&#35810;&#22242;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#39046;&#34966;&#19968;&#30452;&#20513;&#23548;AIs&#24212;&#35813;&#20855;&#26377;&#20154;&#31867;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#21327;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20855;&#22791;&#20687;&#25165;&#21326;&#27178;&#28322;&#30340;&#20154;&#37027;&#26679;&#21327;&#20316;&#33021;&#21147;&#30340;&#24378;&#22823;&#30340;AI&#20173;&#28982;&#36965;&#19981;&#21487;&#21450;&#12290;&#36825;&#31687;&#35770;&#25991;&#20381;&#25454;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20195;&#29702;&#26377;&#25928;&#21644;&#24378;&#22823;&#21327;&#20316;&#25152;&#38656;&#35748;&#30693;&#30340;&#20998;&#26512;&#65292;&#27010;&#36848;&#20102;&#20844;&#20247;&#21644;AI&#24895;&#26223;&#20013;&#20851;&#20110;&#20154;&#24037;&#21327;&#20316;&#32773;&#30340;&#21382;&#21490;&#65292;&#24320;&#22987;&#20110;&#26089;&#26399;&#26234;&#33021;&#22686;&#24378;(IA)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#24895;&#26223;&#12290;&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#25104;&#20026;&#21327;&#21516;AI&#30340;&#31532;&#20108;&#20010;&#31435;&#22330;&#25991;&#20214;(Stefik &amp; Price, 2023)&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;&#31532;&#20108;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#22810;&#23398;&#31185;&#30340;&#29616;&#29366;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#21327;&#20316;&#30740;&#31350;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vision of AI collaborators has long been a staple of science fiction, where artificial agents understand nuances of collaboration and human communication. They bring advantages to their human collaborators and teams by contributing their special talents. Government advisory groups and leaders in AI have advocated for years that AIs should be human compatible and be capable of effective collaboration. Nonetheless, robust AIs that can collaborate like talented people remain out of reach. This position paper draws on a cognitive analysis of what effective and robust collaboration requires of human and artificial agents. It sketches a history of public and AI visions for artificial collaborators, starting with early visions of intelligence augmentation (IA) and artificial intelligence (AI). It is intended as motivation and context for a second position paper on collaborative AI (Stefik &amp; Price, 2023). The second paper reviews the multi-disciplinary state-of-the-art and proposes a roadm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EvHandPose&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#30417;&#30563;&#19979;&#30340;&#26032;&#22411;&#25163;&#37096;&#27969;&#34920;&#31034;&#36827;&#34892;&#22522;&#20110;&#20107;&#20214;&#30340;3D&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#36816;&#21160;&#27169;&#31946;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#30495;&#23454;&#21512;&#25104;&#39046;&#22495;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.02862</link><description>&lt;p&gt;
EvHandPose: &#20351;&#29992;&#31232;&#30095;&#30417;&#30563;&#36827;&#34892;&#22522;&#20110;&#20107;&#20214;&#30340;3D&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision. (arXiv:2303.02862v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EvHandPose&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#30417;&#30563;&#19979;&#30340;&#26032;&#22411;&#25163;&#37096;&#27969;&#34920;&#31034;&#36827;&#34892;&#22522;&#20110;&#20107;&#20214;&#30340;3D&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#36816;&#21160;&#27169;&#31946;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#30495;&#23454;&#21512;&#25104;&#39046;&#22495;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#22312;3D&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#22312;&#35299;&#20915;&#24555;&#36895;&#36816;&#21160;&#21644;&#39640;&#21160;&#24577;&#33539;&#22260;&#30340;&#25361;&#25112;&#20197;&#20302;&#21151;&#32791;&#26041;&#24335;&#22788;&#29702;&#26102;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#27493;&#24046;&#20998;&#25104;&#20687;&#26426;&#21046;&#65292;&#35774;&#35745;&#20107;&#20214;&#34920;&#31034;&#26469;&#32534;&#30721;&#25163;&#37096;&#36816;&#21160;&#20449;&#24687;&#23588;&#20854;&#26159;&#24403;&#25163;&#37096;&#19981;&#21160;&#26102;&#65288;&#23548;&#33268;&#36816;&#21160;&#27169;&#31946;&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#19981;&#21487;&#33021;&#23545;&#26102;&#38388;&#23494;&#38598;&#30340;&#20107;&#20214;&#27969;&#36827;&#34892;&#23436;&#20840;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvHandPose&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#21644;&#20943;&#36731;&#36816;&#21160;&#27169;&#31946;&#38382;&#39064;&#30340;&#26032;&#22411;&#25163;&#37096;&#27969;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#31232;&#30095;&#27880;&#37322;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;Pose-to-IWE&#65288;&#20855;&#26377;&#25197;&#26354;&#20107;&#20214;&#30340;&#22270;&#20687;&#65289;&#27169;&#22359;&#20013;&#35774;&#35745;&#20102;&#23545;&#27604;&#24230;&#26368;&#22823;&#21270;&#21644;&#25163;&#36793;&#32536;&#32422;&#26463;&#65292;&#24182;&#23558;EvHandPose&#26500;&#24314;&#20026;&#24369;&#30417;&#30563;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;EvRealHands&#65292;&#39318;&#20010;&#38024;&#23545;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#22330;&#26223;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#20107;&#20214;&#39537;&#21160;&#25163;&#21183;&#23039;&#21183;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#30495;&#23454;&#21512;&#25104;&#39046;&#22495;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event camera shows great potential in 3D hand pose estimation, especially addressing the challenges of fast motion and high dynamic range in a low-power way. However, due to the asynchronous differential imaging mechanism, it is challenging to design event representation to encode hand motion information especially when the hands are not moving (causing motion ambiguity), and it is infeasible to fully annotate the temporally dense event stream. In this paper, we propose EvHandPose with novel hand flow representations in Event-to-Pose module for accurate hand pose estimation and alleviating the motion ambiguity issue. To solve the problem under sparse annotation, we design contrast maximization and hand-edge constraints in Pose-to-IWE (Image with Warped Events) module and formulate EvHandPose in a weakly-supervision framework. We further build EvRealHands, the first large-scale real-world event-based hand pose dataset on several challenging scenes to bridge the real-synthetic domain gap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21512;&#29702;&#20351;&#29992;&#25152;&#38754;&#20020;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#38544;&#31169;&#12289;&#20559;&#35265;&#12289;&#27602;&#24615;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#30693;&#35782;&#20135;&#26435;&#31561;&#65292;&#24182;&#25552;&#20379;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.01325</link><description>&lt;p&gt;
&#21069;&#24448;&#36127;&#36131;&#20219;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
A Pathway Towards Responsible AI Generated Content. (arXiv:2303.01325v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21512;&#29702;&#20351;&#29992;&#25152;&#38754;&#20020;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#38544;&#31169;&#12289;&#20559;&#35265;&#12289;&#27602;&#24615;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#30693;&#35782;&#20135;&#26435;&#31561;&#65292;&#24182;&#25552;&#20379;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#20869;&#23481;&#28085;&#30422;&#20102;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;AIGC&#24050;&#32463;&#25104;&#20026;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#24182;&#26368;&#36817;&#22312;&#20854;&#36127;&#36131;&#20219;&#20351;&#29992;&#26041;&#38754;&#21463;&#21040;&#20102;&#35768;&#22810;&#25209;&#35780;&#12290;&#22312;&#26412;&#35270;&#37326;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#21487;&#33021;&#38459;&#30861;AIGC&#22312;&#23454;&#36341;&#20013;&#20581;&#24247;&#21457;&#23637;&#21644;&#37096;&#32626;&#30340;&#19977;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21253;&#25324;&#26469;&#33258;&#38544;&#31169;&#12289;&#20559;&#35265;&#12289;&#27602;&#24615;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#35760;&#24405;&#24050;&#30693;&#21644;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#20197;&#21450;AIGC&#30340;&#20219;&#20309;&#21487;&#33021;&#30340;&#28389;&#29992;&#24773;&#20917;&#65292;&#26088;&#22312;&#24341;&#36215;&#23545;&#28508;&#22312;&#39118;&#38505;&#21644;&#28389;&#29992;&#30340;&#27880;&#24847;&#65292;&#24110;&#21161;&#31038;&#20250;&#28040;&#38500;&#38556;&#30861;&#65292;&#24182;&#20419;&#36827;&#26356;&#21152;&#36947;&#24503;&#21644;&#23433;&#20840;&#30340;AIGC&#37096;&#32626;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#24212;&#23545;&#36825;&#20123;&#39118;&#38505;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#21516;&#26102;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;AIGC&#33021;&#22815;&#36127;&#36131;&#20219;&#22320;&#20026;&#31038;&#20250;&#24102;&#26469;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI Generated Content (AIGC) has received tremendous attention within the past few years, with content ranging from image, text, to audio, video, etc. Meanwhile, AIGC has become a double-edged sword and recently received much criticism regarding its responsible usage. In this vision paper, we focus on three main concerns that may hinder the healthy development and deployment of AIGC in practice, including risks from privacy, bias, toxicity, misinformation, and intellectual property (IP). By documenting known and potential risks, as well as any possible misuse scenarios of AIGC, the aim is to draw attention to potential risks and misuse, help society to eliminate obstacles, and promote the more ethical and secure deployment of AIGC. Additionally, we provide insights into the promising directions for tackling these risks while constructing generative models, enabling AIGC to be used responsibly to benefit society.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#30340;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20165;&#20855;&#26377;&#20840;&#37327;&#21270;&#36523;&#20307;&#21464;&#37327;&#30340;&#35268;&#21017;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#22312;NOPI&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.07629</link><description>&lt;p&gt;
&#36890;&#36807;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#23454;&#29616;&#19968;&#33324;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalisation Through Negation and Predicate Invention. (arXiv:2301.07629v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07629
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#30340;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20165;&#20855;&#26377;&#20840;&#37327;&#21270;&#36523;&#20307;&#21464;&#37327;&#30340;&#35268;&#21017;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#22312;NOPI&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20174;&#23569;&#37327;&#30340;&#31034;&#20363;&#20013;&#36827;&#34892;&#27867;&#21270;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#12290;&#32467;&#21512;&#36825;&#20004;&#20010;&#29305;&#24615;&#21487;&#20197;&#20351;ILP&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#20165;&#20855;&#26377;&#20840;&#37327;&#21270;&#36523;&#20307;&#21464;&#37327;&#30340;&#35268;&#21017;&#26469;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24819;&#27861;&#23454;&#29616;&#22312;NOPI&#20013;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#35859;&#35789;&#21019;&#36896;&#30340;&#27491;&#24120;&#36923;&#36753;&#31243;&#24207;&#65292;&#21253;&#25324;&#20855;&#26377;&#20998;&#23618;&#21542;&#23450;&#30340;Datalog&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generalise from a small number of examples is a fundamental challenge in machine learning. To tackle this challenge, we introduce an inductive logic programming (ILP) approach that combines negation and predicate invention. Combining these two features allows an ILP system to generalise better by learning rules with universally quantified body-only variables. We implement our idea in NOPI, which can learn normal logic programs with predicate invention, including Datalog programs with stratified negation. Our experimental results on multiple domains show that our approach can improve predictive accuracies and learning times.
&lt;/p&gt;</description></item><item><title>gRoMA&#26159;&#19968;&#31181;&#34913;&#37327;DNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#21019;&#26032;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#35780;&#20272;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36973;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#35813;&#24037;&#20855;&#21487;&#36816;&#34892;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#24182;&#23545;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20135;&#29983;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.02288</link><description>&lt;p&gt;
gRoMA: &#19968;&#31181;&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness. (arXiv:2301.02288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02288
&lt;/p&gt;
&lt;p&gt;
gRoMA&#26159;&#19968;&#31181;&#34913;&#37327;DNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#21019;&#26032;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#35780;&#20272;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36973;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#35813;&#24037;&#20855;&#21487;&#36816;&#34892;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#24182;&#23545;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20135;&#29983;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#21069;&#27839;&#25216;&#26415;&#30340;&#20195;&#34920;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#65288;&#22914;&#33322;&#31354;&#25110;&#27773;&#36710;&#39046;&#22495;&#65289;&#26102;&#65292;&#30001;&#20110;&#23545;&#25239;&#24615;&#36755;&#20837;&#65288;&#21363;&#21487;&#33021;&#23548;&#33268;DNN&#29359;&#38169;&#30340;&#36755;&#20837;&#25200;&#21160;&#65289;&#30340;&#23041;&#32961;&#65292;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#21363;&#20415;&#26159;&#29616;&#20195;DNN&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#24517;&#39035;&#27979;&#37327;&#24182;&#38477;&#20302;&#36825;&#31181;&#39118;&#38505;&#25165;&#33021;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#37096;&#32626;DNN&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;gRoMA&#65288;&#20840;&#23616;&#40065;&#26834;&#24615;&#27979;&#37327;&#21644;&#35780;&#20272;&#65289;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#26469;&#27979;&#37327;DNN&#30340;&#20840;&#23616;&#20998;&#31867;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;gRoMA&#27979;&#37327;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36935;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#65292;&#20135;&#29983;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;DNN&#22312;&#28909;&#38376;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#40065;&#26834;&#24615;&#24182;&#20998;&#26512;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are at the forefront of cutting-edge technology, and have been achieving remarkable performance in a variety of complex tasks. Nevertheless, their integration into safety-critical systems, such as in the aerospace or automotive domains, poses a significant challenge due to the threat of adversarial inputs: perturbations in inputs that might cause the DNN to make grievous mistakes. Multiple studies have demonstrated that even modern DNNs are susceptible to adversarial inputs; and this risk must thus be measured and mitigated to allow the deployment of DNNs in safety-critical systems.  Here, we present gRoMA (global Robustness Measurement and Assessment), an innovative and scalable tool that implements a probabilistic verification approach to measure the global categorial robustness of a DNN. Specifically, gRoMA measures the probability of encountering adversarial inputs for a specific output category. Our tool operates on pre-trained, black-box classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;EmoAug&#65292;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#35828;&#35805;&#39118;&#26684;&#36716;&#25442;&#26469;&#25552;&#21319;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#27169;&#22411;&#12290;EmoAug&#21033;&#29992;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35821;&#35843;&#32534;&#30721;&#22120;&#34920;&#31034;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#26041;&#24335;&#37325;&#24314;&#35821;&#38899;&#20449;&#21495;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;EmoAug&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#39118;&#26684;&#20016;&#23500;&#20102;&#24773;&#24863;&#35821;&#38899;&#30340;&#34920;&#36798;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.08843</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#35828;&#35805;&#39118;&#26684;&#36716;&#25442;&#25552;&#21319;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving Speech Emotion Recognition with Unsupervised Speaking Style Transfer. (arXiv:2211.08843v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EmoAug&#65292;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#35828;&#35805;&#39118;&#26684;&#36716;&#25442;&#26469;&#25552;&#21319;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#27169;&#22411;&#12290;EmoAug&#21033;&#29992;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35821;&#35843;&#32534;&#30721;&#22120;&#34920;&#31034;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#26041;&#24335;&#37325;&#24314;&#35821;&#38899;&#20449;&#21495;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;EmoAug&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#39118;&#26684;&#20016;&#23500;&#20102;&#24773;&#24863;&#35821;&#38899;&#30340;&#34920;&#36798;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#21508;&#31181;&#38901;&#24459;&#23646;&#24615;&#65292;&#20363;&#22914;&#37325;&#38899;&#20301;&#32622;&#21644;&#24773;&#24863;&#24378;&#24230;&#65292;&#20197;&#20256;&#36798;&#29305;&#23450;&#30340;&#24773;&#24863;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#33268;&#30340;&#35821;&#35328;&#20869;&#23481;&#12290;&#21463;&#21040;&#36825;&#31181;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EmoAug&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#65292;&#26088;&#22312;&#22686;&#24378;&#24773;&#24863;&#34920;&#36798;&#24182;&#35299;&#20915;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;EmoAug&#30001;&#19968;&#20010;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35821;&#35843;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#20998;&#21035;&#34920;&#31034;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#35299;&#30721;&#22120;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#19978;&#36848;&#20004;&#20010;&#20449;&#24687;&#27969;&#30340;&#26465;&#20214;&#19979;&#37325;&#24314;&#35821;&#38899;&#20449;&#21495;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;EmoAug&#36890;&#36807;&#23558;&#19981;&#21516;&#39118;&#26684;&#36755;&#20837;&#21040;&#35821;&#35843;&#32534;&#30721;&#22120;&#20013;&#65292;&#20016;&#23500;&#20102;&#24773;&#24863;&#35821;&#38899;&#30340;&#34920;&#36798;&#65292;&#21253;&#25324;&#37325;&#38899;&#12289;&#33410;&#22863;&#21644;&#24378;&#24230;&#31561;&#19981;&#21516;&#30340;&#38901;&#24459;&#23646;&#24615;&#12290;&#21516;&#26102;&#65292;EmoAug&#33021;&#22815;&#29983;&#25104;&#27599;&#20010;&#31867;&#21035;&#30456;&#20284;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can effortlessly modify various prosodic attributes, such as the placement of stress and the intensity of sentiment, to convey a specific emotion while maintaining consistent linguistic content. Motivated by this capability, we propose EmoAug, a novel style transfer model designed to enhance emotional expression and tackle the data scarcity issue in speech emotion recognition tasks. EmoAug consists of a semantic encoder and a paralinguistic encoder that represent verbal and non-verbal information respectively. Additionally, a decoder reconstructs speech signals by conditioning on the aforementioned two information flows in an unsupervised fashion. Once training is completed, EmoAug enriches expressions of emotional speech with different prosodic attributes, such as stress, rhythm and intensity, by feeding different styles into the paralinguistic encoder. EmoAug enables us to generate similar numbers of samples for each class to tackle the data imbalance issue as well. Experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.15677</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#21487;&#24494;&#22686;&#24378;&#25216;&#26415;&#25913;&#21892;&#20102;GAN&#35757;&#32451;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20294;&#26159;&#65292;&#22686;&#24378;&#25216;&#26415;&#38544;&#24335;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#19981;&#21464;&#24615;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#30001;&#25968;&#25454;&#36716;&#25442;&#24341;&#36215;&#30340;&#26631;&#31614;&#31354;&#38388;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#21028;&#21035;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#26368;&#32456;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21464;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#32487;&#25215;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#21487;&#20197;&#39044;&#27979;&#22686;&#24378;&#25968;&#25454;&#30340;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#25968;&#25454;&#30340;&#39044;&#27979;&#30446;&#26631;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#21306;&#21035;&#24320;&#26469;&#12290;&#25105;&#20204;&#36824;&#40723;&#21169;&#29983;&#25104;&#22120;&#23545;&#25239;&#22320;&#29983;&#25104;&#20854;&#22686;&#24378;&#21442;&#25968;&#21487;&#20197;&#34987;&#21028;&#21035;&#22120;&#20934;&#30830;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#37327;&#21644;&#26356;&#39640;&#25928;&#30340;&#21028;&#21035;&#22120;&#65292;&#25552;&#39640;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
&lt;/p&gt;</description></item></channel></rss>