<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;</title><link>https://rss.arxiv.org/abs/2402.01030</link><description>&lt;p&gt;
&#21487;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#33021;&#22815;&#28608;&#21457;&#26356;&#20986;&#33394;&#30340;LLM&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Executable Code Actions Elicit Better LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01030
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20307;&#20855;&#22791;&#25191;&#34892;&#24191;&#27867;&#34892;&#21160;&#30340;&#33021;&#21147;&#65292;&#22914;&#35843;&#29992;&#24037;&#20855;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#31561;&#65292;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;LLM&#26234;&#33021;&#20307;&#36890;&#24120;&#36890;&#36807;&#29983;&#25104;JSON&#25110;&#25991;&#26412;&#30340;&#39044;&#23450;&#20041;&#26684;&#24335;&#26469;&#20135;&#29983;&#34892;&#21160;&#65292;&#36825;&#36890;&#24120;&#21463;&#38480;&#20110;&#21463;&#38480;&#21046;&#30340;&#34892;&#21160;&#31354;&#38388;&#65288;&#20363;&#22914;&#65292;&#39044;&#23450;&#20041;&#24037;&#20855;&#30340;&#33539;&#22260;&#65289;&#21644;&#21463;&#38480;&#30340;&#28789;&#27963;&#24615;&#65288;&#20363;&#22914;&#65292;&#26080;&#27861;&#32452;&#21512;&#22810;&#20010;&#24037;&#20855;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#23558;LLM&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65288;CodeAct&#65289;&#12290;CodeAct&#19982;Python&#35299;&#37322;&#22120;&#38598;&#25104;&#65292;&#21487;&#20197;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#22312;&#26032;&#30340;&#35266;&#23519;&#20013;&#21160;&#24577;&#20462;&#35746;&#20808;&#21069;&#30340;&#34892;&#21160;&#25110;&#21457;&#20986;&#26032;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#23545;17&#20010;LLM&#22312;API-Bank&#21644;&#26032;&#32534;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;CodeAct&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#65288;&#25104;&#21151;&#29575;&#39640;&#20986;20%&#65289;&#12290;CodeAct&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#28608;&#21169;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#28857;&#32423;&#23545;&#24212;&#20851;&#31995;&#12289;&#21033;&#29992;&#22270;&#20687;&#12289;3D&#37325;&#24314;&#21644;&#36816;&#21160;&#23398;&#30340;&#32447;&#32034;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#26410;&#30693;&#20851;&#33410;&#23545;&#35937;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#32467;&#26524;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#23545;&#35937;&#24418;&#29366;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;</title><link>https://arxiv.org/abs/2404.01440</link><description>&lt;p&gt;
&#29992;&#20110;&#26500;&#24314;&#26410;&#30693;&#20851;&#33410;&#23545;&#35937;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#28857;&#32423;&#23545;&#24212;&#20851;&#31995;&#12289;&#21033;&#29992;&#22270;&#20687;&#12289;3D&#37325;&#24314;&#21644;&#36816;&#21160;&#23398;&#30340;&#32447;&#32034;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#26410;&#30693;&#20851;&#33410;&#23545;&#35937;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#32467;&#26524;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#23545;&#35937;&#24418;&#29366;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#20004;&#20010;&#19981;&#21516;&#20851;&#33410;&#29366;&#24577;&#30340;&#29289;&#20307;&#30340;RGBD&#25195;&#25551;&#26500;&#24314;&#26410;&#30693;&#20851;&#33410;&#23545;&#35937;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#22788;&#29702;&#19981;&#21516;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#29366;&#24577;&#37325;&#24314;&#29289;&#20307;&#32423;&#24418;&#29366;&#65292;&#28982;&#21518;&#24674;&#22797;&#21253;&#25324;&#37096;&#20214;&#20998;&#21106;&#21644;&#20851;&#33410;&#20851;&#32852;&#22312;&#20869;&#30340;&#22522;&#30784;&#20851;&#33410;&#27169;&#22411;&#12290;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#28857;&#32423;&#23545;&#24212;&#20851;&#31995;&#24182;&#21033;&#29992;&#26469;&#33258;&#22270;&#20687;&#12289;3D&#37325;&#24314;&#21644;&#36816;&#21160;&#23398;&#30340;&#32447;&#32034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#30340;&#32467;&#26524;&#12290;&#23427;&#36824;&#22788;&#29702;&#20102;&#22810;&#20010;&#21487;&#31227;&#21160;&#37096;&#20214;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#23545;&#35937;&#24418;&#29366;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01440v1 Announce Type: cross  Abstract: We address the problem of building digital twins of unknown articulated objects from two RGBD scans of the object at different articulation states. We decompose the problem into two stages, each addressing distinct aspects. Our method first reconstructs object-level shape at each state, then recovers the underlying articulation model including part segmentation and joint articulations that associate the two states. By explicitly modeling point-level correspondences and exploiting cues from images, 3D reconstructions, and kinematics, our method yields more accurate and stable results compared to prior work. It also handles more than one movable part and does not rely on any object shape or structure priors. Project page: https://github.com/NVlabs/DigitalTwinArt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.19289</link><description>&lt;p&gt;
&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Treatment Effect Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19289
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24448;&#24448;&#28041;&#21450;&#26114;&#36149;&#30340;&#27835;&#30103;&#20998;&#37197;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#20013;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#31181;&#27835;&#30103;&#25928;&#26524;&#32780;&#26080;&#38656;&#23454;&#38469;&#24178;&#39044;&#26159;&#20943;&#23569;&#39118;&#38505;&#30340;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#23454;&#39564;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#65292;&#22240;&#27492;&#20174;&#26681;&#26412;&#19978;&#23384;&#22312;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#20381;&#36182;&#20110;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#35760;&#23454;&#20363;&#30340;&#33410;&#28857;&#22238;&#24402;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#20808;&#21069;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#21452;&#27169;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#39069;&#22806;&#27493;&#39588;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#19982;&#33719;&#21462;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#20197;&#24341;&#23548;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;VLM&#22312;&#22270;&#20687;&#23383;&#24149;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#32780;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#20165;&#20026;&#20165;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.07750</link><description>&lt;p&gt;
Synth$^2$: &#29992;&#21512;&#25104;&#26631;&#39064;&#21644;&#22270;&#20687;&#23884;&#20837;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07750
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;VLM&#22312;&#22270;&#20687;&#23383;&#24149;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#32780;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#20165;&#20026;&#20165;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#22270;&#20687;&#26631;&#39064;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21457;&#23637;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20026;&#39640;&#25928;&#26377;&#25928;&#30340;VLM&#35757;&#32451;&#21019;&#24314;&#21512;&#25104;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20174;LLM&#29983;&#25104;&#30340;&#26631;&#39064;&#24320;&#22987;&#21512;&#25104;&#22270;&#20687;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#21512;&#25104;&#23545;&#29992;&#20110;&#35757;&#32451;VLM&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;VLM&#22312;&#22270;&#20687;&#23383;&#24149;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#32780;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#20165;&#20026;&#20165;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22686;&#24378;&#65292;&#25105;&#20204;&#36229;&#36807;&#22522;&#32447;17%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#22270;&#20687;&#23884;&#20837;&#31354;&#38388;&#21512;&#25104;&#27604;&#22312;&#20687;&#32032;&#31354;&#38388;&#20013;&#24555;25%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07750v1 Announce Type: cross  Abstract: The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel sp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22270;&#25968;&#25454;&#21387;&#32553;&#20013;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07294</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#23545;&#22270;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Graph Data Condensation via Self-expressive Graph Structure Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07294
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22270;&#25968;&#25454;&#21387;&#32553;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#22270;&#25968;&#25454;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#22312;&#35757;&#32451;&#38454;&#27573;&#20943;&#36731;&#23384;&#20648;&#21644;&#26102;&#38388;&#25104;&#26412;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#23427;&#26088;&#22312;&#23558;&#21407;&#22987;&#22823;&#35268;&#27169;&#22270;&#21387;&#32553;&#20026;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#22270;&#65292;&#21516;&#26102;&#20445;&#30041;&#35757;&#32451;&#19979;&#28216;GNN&#25152;&#38656;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#38598;&#20013;&#20110;&#20165;&#20248;&#21270;&#33410;&#28857;&#29305;&#24449;&#65292;&#35201;&#20040;&#21162;&#21147;&#29420;&#31435;&#23398;&#20064;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#29983;&#25104;&#22120;&#12290;&#23427;&#20204;&#26080;&#27861;&#26126;&#30830;&#21033;&#29992;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#24182;&#26410;&#33021;&#20026;&#21512;&#25104;&#25968;&#25454;&#38598;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#22270;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR})&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31361;&#20986;&#20043;&#22788;&#22312;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07294v1 Announce Type: cross  Abstract: With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR}). Our method stands out by 
&lt;/p&gt;</description></item><item><title>&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#35843;&#26597;, &#30528;&#37325;&#20171;&#32461;&#20102;&#20174;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21040;&#23574;&#31471;Sora&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.05131</link><description>&lt;p&gt;
Sora&#20316;&#20026;AGI&#19990;&#30028;&#27169;&#22411;&#65311;&#20851;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#23436;&#25972;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05131
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#35843;&#26597;, &#30528;&#37325;&#20171;&#32461;&#20102;&#20174;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21040;&#23574;&#31471;Sora&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05131v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#21069;&#27839;&#65292;&#25972;&#21512;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#23383;&#24149;&#21644;&#25991;&#26412;&#24341;&#23548;&#32534;&#36753;&#30340;&#36827;&#23637;&#12290;&#26412;&#35843;&#26597;&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#23457;&#35270;&#65292;&#37325;&#28857;&#20851;&#27880;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21521;&#23574;&#31471;Sora&#27169;&#22411;&#36716;&#21464;&#30340;&#36807;&#31243;&#65292;&#31361;&#20986;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;&#21306;&#21035;&#20110;&#20197;&#24448;&#20316;&#21697;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25216;&#26415;&#26694;&#26550;&#21644;&#28436;&#21270;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;&#20262;&#29702;&#21644;&#25216;&#26415;&#25361;&#25112;&#65292;&#22914;&#26080;&#27861;&#25191;&#34892;&#22810;&#23454;&#20307;&#22788;&#29702;&#12289;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#23398;&#20064;&#12289;&#29702;&#35299;&#29289;&#29702;&#20114;&#21160;&#12289;&#24863;&#30693;&#29289;&#20307;&#32553;&#25918;&#21644;&#27604;&#20363;&#20197;&#21450;&#23545;&#25239;&#29289;&#20307;&#24187;&#35273;&#65292;&#36825;&#20063;&#26159;&#29983;&#25104;&#27169;&#22411;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05131v1 Announce Type: new  Abstract: Text-to-video generation marks a significant frontier in the rapidly evolving domain of generative AI, integrating advancements in text-to-image synthesis, video captioning, and text-guided editing. This survey critically examines the progression of text-to-video technologies, focusing on the shift from traditional generative models to the cutting-edge Sora model, highlighting developments in scalability and generalizability. Distinguishing our analysis from prior works, we offer an in-depth exploration of the technological frameworks and evolutionary pathways of these models. Additionally, we delve into practical applications and address ethical and technological challenges such as the inability to perform multiple entity handling, comprehend causal-effect learning, understand physical interaction, perceive object scaling and proportioning, and combat object hallucination which is also a long-standing problem in generative models. Our c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#38752;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.04696</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#38752;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20135;&#29983;&#38169;&#35823;&#30340;&#22768;&#26126;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#36825;&#31181;&#24187;&#35273;&#21487;&#33021;&#24456;&#21361;&#38505;&#65292;&#22240;&#20026;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#20598;&#23572;&#20986;&#29616;&#30340;&#20107;&#23454;&#19981;&#20934;&#30830;&#21487;&#33021;&#20250;&#34987;&#25972;&#20307;&#19978;&#26159;&#20107;&#23454;&#30340;&#25991;&#26412;&#25513;&#30422;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#26497;&#20854;&#38590;&#20197;&#21457;&#29616;&#12290;&#21033;&#29992;LLMs&#30340;&#24403;&#21069;&#26381;&#21153;&#36890;&#24120;&#19981;&#25552;&#20379;&#26816;&#27979;&#19981;&#21487;&#38752;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#12290;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#25110;&#20854;&#23618;&#36755;&#20986;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26469;&#26816;&#27979;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#24182;&#25105;&#20204;&#23637;&#31034;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#26680;&#26597;LLM&#36755;&#20986;&#20013;&#30340;&#21508;&#31181;&#22768;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#23545;&#20107;&#23454;&#25552;&#20986;&#24576;&#30097;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;&#20195;&#29702;NBCagent&#65292;&#36890;&#36807;&#25216;&#33021;&#29305;&#23450;&#30340;&#28436;&#21270;&#35268;&#21010;&#22120;&#21644;&#25216;&#33021;&#20849;&#20139;&#30340;&#35821;&#20041;&#28210;&#26579;&#27169;&#22359;&#65292;&#23454;&#29616;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#36830;&#32493;&#23398;&#20064;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.00336</link><description>&lt;p&gt;
&#27704;&#19981;&#20572;&#27490;&#30340;&#20855;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Never-Ending Embodied Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00336
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;&#20195;&#29702;NBCagent&#65292;&#36890;&#36807;&#25216;&#33021;&#29305;&#23450;&#30340;&#28436;&#21270;&#35268;&#21010;&#22120;&#21644;&#25216;&#33021;&#20849;&#20139;&#30340;&#35821;&#20041;&#28210;&#26579;&#27169;&#22359;&#65292;&#23454;&#29616;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#36830;&#32493;&#23398;&#20064;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20855;&#36523;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35270;&#35273;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#22312;&#36866;&#24212;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26410;&#35265;&#20219;&#21153;&#26102;&#65292;&#20250;&#36973;&#21463;&#25805;&#32437;&#24615;&#33021;&#19979;&#38477;&#20197;&#21450;&#25216;&#33021;&#30693;&#35782;&#36951;&#24536;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;NBCagent&#22312;&#20855;&#36523;&#26426;&#22120;&#20154;&#20013;&#25506;&#35752;&#20102;&#19978;&#36848;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#12289;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#27704;&#19981;&#20572;&#27490;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#65292;&#21487;&#20197;&#19981;&#26029;&#20174;&#29305;&#23450;&#25216;&#33021;&#21644;&#20849;&#20139;&#25216;&#33021;&#23646;&#24615;&#20013;&#23398;&#20064;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#35266;&#23519;&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#29305;&#23450;&#25216;&#33021;&#19981;&#26029;&#28436;&#21270;&#30340;&#35268;&#21010;&#22120;&#26469;&#36827;&#34892;&#30693;&#35782;&#35299;&#32806;&#65292;&#36825;&#21487;&#20197;&#20174;&#28508;&#22312;&#21644;&#20302;&#31209;&#31354;&#38388;&#20013;&#19981;&#26029;&#21521;&#25105;&#20204;&#30340;NBCagent&#20195;&#29702;&#23884;&#20837;&#26032;&#30340;&#25216;&#33021;&#29305;&#23450;&#30693;&#35782;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25216;&#33021;&#20849;&#20139;&#35821;&#20041;&#28210;&#26579;&#27169;&#22359;&#21644;&#19968;&#20010;&#25216;&#33021;&#20849;&#20139;&#34920;&#31034;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00336v1 Announce Type: cross  Abstract: Relying on large language models (LLMs), embodied robots could perform complex multimodal robot manipulation tasks from visual observations with powerful generalization ability. However, most visual behavior-cloning agents suffer from manipulation performance degradation and skill knowledge forgetting when adapting into a series of challenging unseen tasks. We here investigate the above challenge with NBCagent in embodied robots, a pioneering language-conditioned Never-ending Behavior-Cloning agent, which can continually learn observation knowledge of novel robot manipulation skills from skill-specific and skill-shared attributes. Specifically, we establish a skill-specific evolving planner to perform knowledge decoupling, which can continually embed novel skill-specific knowledge in our NBCagent agent from latent and low-rank space. Meanwhile, we propose a skill-shared semantics rendering module and a skill-shared representation disti
&lt;/p&gt;</description></item><item><title>Agent-Pro&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#36880;&#27493;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.17574</link><description>&lt;p&gt;
Agent-Pro: &#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#21453;&#24605;&#21644;&#20248;&#21270;&#23398;&#20064;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17574
&lt;/p&gt;
&lt;p&gt;
Agent-Pro&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#36880;&#27493;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#37117;&#26159;&#29305;&#23450;&#20219;&#21153;&#27714;&#35299;&#22120;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#32780;&#19981;&#26159;&#33021;&#22815;&#36890;&#36807;&#20114;&#21160;&#23398;&#20064;&#21644;&#36827;&#21270;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Agent-Pro&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#20855;&#26377;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#20016;&#23500;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36880;&#28176;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#28041;&#21450;&#19968;&#20010;&#21160;&#24577;&#20449;&#24565;&#29983;&#25104;&#21644;&#21453;&#24605;&#36807;&#31243;&#65292;&#29992;&#20110;&#31574;&#30053;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17574v1 Announce Type: new  Abstract: Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#65292;&#23558;&#29983;&#25104;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#29992;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16459</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending LLMs against Jailbreaking Attacks via Backtranslation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#65292;&#23558;&#29983;&#25104;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#29992;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#35757;&#32451;&#25104;&#25298;&#32477;&#26377;&#23475;&#35831;&#27714;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#37325;&#20889;&#21407;&#22987;&#25552;&#31034;&#20197;&#38544;&#34255;&#20854;&#26377;&#23475;&#24847;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#21453;&#21521;&#32763;&#35793;&#8221;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#30446;&#26631;LLM&#20174;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#30340;&#21021;&#22987;&#21709;&#24212;&#65292;&#25105;&#20204;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#26029;&#21487;&#20197;&#23548;&#33268;&#35813;&#21709;&#24212;&#30340;&#36755;&#20837;&#25552;&#31034;&#12290;&#25512;&#26029;&#30340;&#25552;&#31034;&#31216;&#20026;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#65292;&#20542;&#21521;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#22240;&#20026;&#23427;&#26159;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#30340;&#65292;&#19981;&#26159;&#30452;&#25509;&#30001;&#25915;&#20987;&#32773;&#25805;&#32437;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20877;&#27425;&#22312;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#19978;&#36816;&#34892;&#30446;&#26631;LLM&#65292;&#22914;&#26524;&#27169;&#22411;&#25298;&#32477;&#20102;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#65292;&#21017;&#25298;&#32477;&#21407;&#22987;&#25552;&#31034;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#25514;&#26045;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#20960;&#20010;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16310</link><description>&lt;p&gt;
REPLAY: &#23545;&#31232;&#30095;&#36712;&#36857;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#39044;&#27979;&#26159;&#26681;&#25454;&#21382;&#21490;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#26469;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#30340;&#22266;&#26377;&#31232;&#30095;&#38382;&#39064;&#65292;&#26102;&#31354;&#19978;&#19979;&#25991;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#26159;&#23558;&#20301;&#32622;&#20043;&#38388;&#30340;&#26102;&#31354;&#36317;&#31163;&#32435;&#20837;&#21040;&#31227;&#21160;&#36712;&#36857;&#20013;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#25552;&#20379;&#32473;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#26469;&#23547;&#25214;&#26377;&#20449;&#24687;&#30340;&#36807;&#21435;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#20363;&#22914;&#65292;&#20154;&#31867;&#31227;&#21160;&#22312;&#26089;&#26216;&#36890;&#24120;&#27604;&#20854;&#20182;&#26102;&#38388;&#26356;&#26377;&#35268;&#24459;&#65307;&#36825;&#26263;&#31034;&#20102;&#23454;&#38469;&#26102;&#38388;&#25139;&#30340;&#26377;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPLAY&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;RNN&#26550;&#26500;&#65292;&#26088;&#22312;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#35268;&#24459;&#20197;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#29616;&#35937;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#22522;&#20110;&#26032;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#24230;&#37327;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.15555</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24635;&#26159;&#29702;&#35299;&#24182;&#19988;&#36825;&#23601;&#26159;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Deep Networks Always Grok and Here is Why
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15555
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#29616;&#35937;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#22522;&#20110;&#26032;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#24230;&#37327;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#65292;&#25110;&#32773;&#24310;&#36831;&#27867;&#21270;&#65292;&#26159;&#25351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#36798;&#21040;&#25509;&#36817;&#20110;&#38646;&#30340;&#35757;&#32451;&#35823;&#24046;&#21518;&#24456;&#38271;&#26102;&#38388;&#20869;&#25165;&#21457;&#29983;&#27867;&#21270;&#30340;&#29616;&#35937;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;&#22312;&#29305;&#23450;&#21463;&#25511;&#29615;&#22659;&#20013;&#20986;&#29616;grokking&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#20351;&#29992;&#22823;&#33539;&#25968;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;DNN&#25110;&#32773;&#22312;&#31639;&#27861;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;transformers&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;grokking&#23454;&#38469;&#19978;&#26356;&#21152;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#24191;&#27867;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#21576;&#29616;&#65292;&#20363;&#22914;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;&#32773;&#22312;Imagenette&#19978;&#35757;&#32451;&#30340;Resnet&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24310;&#36831;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;&#65292;&#21363;DNN&#22312;&#25554;&#20540;&#21644;/&#25110;&#27867;&#21270;&#20043;&#21518;&#23545;&#25239;&#31034;&#20363;&#36827;&#34892;&#29702;&#35299;&#24182;&#21464;&#24471;&#40065;&#26834;&#12290;&#25105;&#20204;&#38024;&#23545;DNN&#30340;&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#25552;&#20986;&#20102;&#20986;&#29616;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#34913;&#37327;&#20102;DNN&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#30340;&#22797;&#26434;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15555v1 Announce Type: cross  Abstract: Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on a new measure of the local complexity of a DNN's input-output mapping. Our local complexity measures the d
&lt;/p&gt;</description></item><item><title>NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15393</link><description>&lt;p&gt;
NeuralThink: &#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#36827;&#34892;&#22806;&#25512;&#30340;&#31639;&#27861;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15393
&lt;/p&gt;
&lt;p&gt;
NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25797;&#38271;&#27169;&#24335;&#35782;&#21035;&#65292;&#20294;&#22312;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#26041;&#24335;&#19978;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#24605;&#32500;&#26041;&#27861;&#23637;&#29616;&#20102;&#23398;&#20064;&#21487;&#20197;&#22806;&#25512;&#30340;&#31639;&#27861;&#30340;&#28508;&#21147;&#65306;&#22312;&#36739;&#23567;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#24182;&#22312;&#36739;&#22823;&#30340;&#29615;&#22659;&#20013;&#25191;&#34892;&#23398;&#21040;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23616;&#38480;&#20110;&#23545;&#31216;&#20219;&#21153;&#65292;&#21363;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#30456;&#21516;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NeuralThink&#65292;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#20854;&#20013;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;&#20219;&#21153;&#22806;&#25512;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NeuralThink &#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#19968;&#30452;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#24605;&#24819;&#30340;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14865</link><description>&lt;p&gt;
DyVal 2: &#20803;&#25506;&#27979;&#20195;&#29702;&#21160;&#24577;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#24605;&#24819;&#30340;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#35774;&#35745;&#20102;&#20351;&#29992;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#26126;&#30830;&#23450;&#20041;&#31639;&#27861;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36825;&#20123;&#21327;&#35758;&#26080;&#27861;&#36731;&#26494;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#21482;&#33021;&#25552;&#20379;&#25972;&#20307;&#22522;&#20934;&#32467;&#26524;&#65292;&#19981;&#33021;&#25903;&#25345;&#23545;LLMs&#33021;&#21147;&#36827;&#34892;&#32454;&#31890;&#24230;&#21644;&#22810;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#24515;&#29702;&#27979;&#37327;&#23398;&#21551;&#21457;&#30340;&#36890;&#29992;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#12290; MPA &#26159; DyVal 2 &#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#33258;&#28982;&#22320;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340; DyVal&#12290; MPA &#35774;&#35745;&#20102;&#25506;&#27979;&#21644;&#35780;&#21028;&#20195;&#29702;&#65292;&#20197;&#33258;&#21160;&#23558;&#21407;&#22987;&#35780;&#20272;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#36981;&#24490;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#22312;&#19977;&#20010;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#19978;&#30340;&#24212;&#29992;: &#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#39064;&#35299;&#20915;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14865v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.13852</link><description>&lt;p&gt;
&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Neural Control System for Continuous Glucose Monitoring and Maintenance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13852
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#33889;&#33796;&#31958;&#27700;&#24179;&#31649;&#29702;&#23545;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#36991;&#20813;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#21033;&#29992;&#24494;&#20998;&#39044;&#27979;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21463;&#21040;&#22797;&#26434;&#31070;&#32463;&#31574;&#30053;&#21644;&#21487;&#21306;&#20998;&#24314;&#27169;&#30340;&#25351;&#23548;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#12290;&#36825;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#26368;&#22823;&#21270;&#25928;&#29575;&#65292;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#21644;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#65292;&#22914;&#32463;&#39564;&#21457;&#29616;&#25152;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13852v1 Announce Type: cross  Abstract: Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#32500;&#20998;&#26512;&#21457;&#29616;&#65292;&#31034;&#33539;&#30340;&#26377;&#25928;&#24615;&#22312;&#22810;&#35821;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20854;&#20013;&#37096;&#20998;&#27169;&#22411;&#23545;&#31034;&#33539;&#36136;&#37327;&#19981;&#25935;&#24863;&#65292;&#32780;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#26495;&#21487;&#20197;&#28040;&#38500;&#23545;&#31034;&#33539;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.12976</link><description>&lt;p&gt;
&#31034;&#33539;&#23545;&#22810;&#35821;&#22659;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#22810;&#32500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12976
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#32500;&#20998;&#26512;&#21457;&#29616;&#65292;&#31034;&#33539;&#30340;&#26377;&#25928;&#24615;&#22312;&#22810;&#35821;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20854;&#20013;&#37096;&#20998;&#27169;&#22411;&#23545;&#31034;&#33539;&#36136;&#37327;&#19981;&#25935;&#24863;&#65292;&#32780;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#26495;&#21487;&#20197;&#28040;&#38500;&#23545;&#31034;&#33539;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#31034;&#33539;&#32463;&#24120;&#34987;&#29992;&#20316;&#25512;&#29702;&#31574;&#30053;&#65292;&#22312;&#27492;&#31574;&#30053;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#31034;&#33539;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#19982;&#21333;&#35821;&#35328;&#65288;&#33521;&#35821;&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#22810;&#35821;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#35813;&#29615;&#22659;&#20013;&#31034;&#33539;&#20316;&#29992;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#22810;&#35821;&#22659;&#23398;&#20064;&#36827;&#34892;&#20102;&#22810;&#32500;&#20998;&#26512;&#65292;&#23454;&#39564;&#37319;&#29992;&#20102;&#26469;&#33258;&#19981;&#21516;&#27169;&#22411;&#23478;&#26063;&#30340;5&#20010;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#22312;&#20869;&#30340;9&#20010;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#20102;56&#31181;&#31867;&#22411;&#19978;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31034;&#33539;&#30340;&#26377;&#25928;&#24615;&#22312;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;Llama 2-Chat&#12289;GPT-3.5&#21644;GPT-4&#23545;&#31034;&#33539;&#36136;&#37327;&#30340;&#25935;&#24863;&#24230;&#36739;&#20302;&#12290;&#30456;&#21453;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#26495;&#24448;&#24448;&#20250;&#28040;&#38500;&#19968;&#20123;&#27169;&#22411;&#23545;&#31034;&#33539;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12976v1 Announce Type: cross  Abstract: In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some t
&lt;/p&gt;</description></item><item><title>EmoBench&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12071</link><description>&lt;p&gt;
EmoBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
EmoBench: Evaluating the Emotional Intelligence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12071
&lt;/p&gt;
&lt;p&gt;
EmoBench&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20984;&#26174;&#20102;&#38656;&#35201;&#31283;&#20581;&#12289;&#20840;&#38754;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#36827;&#34892;&#35780;&#20272;&#30340;&#30740;&#31350;&#30456;&#24403;&#26377;&#38480;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#24773;&#24863;&#35782;&#21035;&#65292;&#24573;&#35270;&#20102;&#24773;&#24863;&#35843;&#33410;&#31561;&#37325;&#35201;&#30340;&#24773;&#24863;&#26234;&#33021;&#33021;&#21147;&#65292;&#32780;&#24773;&#24863;&#29702;&#35299;&#21017;&#20419;&#36827;&#24773;&#24863;; &#20854;&#27425;&#65292;&#23427;&#20204;&#20027;&#35201;&#22522;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#39057;&#32321;&#27169;&#24335;&#12289;&#26126;&#30830;&#20449;&#24687;&#21644;&#27880;&#37322;&#38169;&#35823;&#65292;&#23548;&#33268;&#35780;&#20272;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EmoBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20511;&#37492;&#20102;&#24050;&#24314;&#31435;&#30340;&#24515;&#29702;&#29702;&#35770;&#65292;&#24182;&#20026;&#26426;&#22120;EI&#25552;&#20986;&#20102;&#32508;&#21512;&#23450;&#20041;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;EmoBench&#21253;&#25324;&#19968;&#32452;400&#20010;&#29992;&#33521;&#35821;&#21644;&#20013;&#25991;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#38656;&#35201;&#28145;&#20837;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12071v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.11753</link><description>&lt;p&gt;
ArtPrompt: &#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#23545;&#40784;LLMs&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11753
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#22914;&#25968;&#25454;&#36807;&#28388;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21152;&#24378;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24050;&#30693;&#30340;&#25216;&#26415;&#20551;&#35774;&#29992;&#20110;&#23545;&#40784;LLMs&#23433;&#20840;&#24615;&#30340;&#35821;&#26009;&#24211;&#20165;&#30001;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#19981;&#25104;&#31435;&#65292;&#23548;&#33268;LLMs&#23384;&#22312;&#20005;&#37325;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;Vision-in-Text Challenge&#65288;ViTC&#65289;&#26469;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#19981;&#33021;&#20165;&#36890;&#36807;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#30340;&#25552;&#31034;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20116;&#20010;SOTA LLMs&#65288;GPT-3.5&#12289;GPT-4&#12289;Gemini&#12289;Claude&#21644;Llama2&#65289;&#22312;&#35782;&#21035;&#20197;ASCII&#33402;&#26415;&#24418;&#24335;&#25552;&#20379;&#30340;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
&lt;/p&gt;</description></item><item><title>GNNavi&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20934;&#30830;&#24341;&#23548;&#20449;&#24687;&#27969;&#30340;&#27719;&#32858;&#21644;&#20998;&#24067;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23548;&#33322;&#20449;&#24687;&#27969;&#21160;&#24577;&#65292;&#36229;&#36234;&#20102;&#26631;&#20934;&#25552;&#31034;&#24335;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11709</link><description>&lt;p&gt;
GNNavi&#65306;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#23548;&#33322;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11709
&lt;/p&gt;
&lt;p&gt;
GNNavi&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20934;&#30830;&#24341;&#23548;&#20449;&#24687;&#27969;&#30340;&#27719;&#32858;&#21644;&#20998;&#24067;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23548;&#33322;&#20449;&#24687;&#27969;&#21160;&#24577;&#65292;&#36229;&#36234;&#20102;&#26631;&#20934;&#25552;&#31034;&#24335;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25509;&#25910;&#31034;&#33539;&#36755;&#20837;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65288;ICL&#65289;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#36866;&#24212;&#24615;&#12290;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20294;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#38656;&#27714;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;GNNavi&#21033;&#29992;&#20102;&#26377;&#20851;ICL&#20449;&#24687;&#27969;&#21160;&#24577;&#30340;&#35265;&#35299;&#65292;&#34920;&#26126;&#26631;&#31614;&#35789;&#22312;&#25552;&#31034;&#20013;&#20316;&#20026;&#20449;&#24687;&#20256;&#25773;&#30340;&#38170;&#28857;&#12290;GNNavi&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#31934;&#30830;&#22320;&#24341;&#23548;&#20449;&#24687;&#27969;&#30340;&#27719;&#32858;&#21644;&#20998;&#24067;&#65292;&#22312;&#22788;&#29702;&#25552;&#31034;&#26102;&#23558;&#26399;&#26395;&#30340;&#20449;&#24687;&#27969;&#30828;&#32534;&#30721;&#21040;GNN&#20013;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;GPT-2&#21644;Llama2&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;GNNavi&#36229;&#36234;&#20102;&#26631;&#20934;&#25552;&#31034;&#24335;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11709v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-ba
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;</title><link>https://arxiv.org/abs/2402.10980</link><description>&lt;p&gt;
CHEMREASONER&#65306;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#36827;&#34892;&#21551;&#21457;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21457;&#29616;&#26032;&#30340;&#20652;&#21270;&#21058;&#23545;&#20110;&#35774;&#35745;&#26032;&#30340;&#26356;&#39640;&#25928;&#30340;&#21270;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#21521;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36807;&#28193;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#19977;&#32500;&#21407;&#23376;&#34920;&#31034;&#30340;&#21453;&#39304;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#26500;&#24314;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#23548;&#30340;&#20551;&#35774;&#19982;&#22522;&#20110;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#21453;&#39304;&#30340;&#36845;&#20195;&#32452;&#21512;&#65292;&#31215;&#26497;&#25628;&#32034;&#39640;&#25928;&#20652;&#21270;&#21058;&#12290;&#22312;&#20013;&#38388;&#25628;&#32034;&#27493;&#39588;&#30830;&#23450;&#30340;&#20652;&#21270;&#21058;&#32463;&#36807;&#22522;&#20110;&#31354;&#38388;&#23450;&#21521;&#12289;&#21453;&#24212;&#36884;&#24452;&#21644;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#22522;&#20110;&#21560;&#38468;&#33021;&#21644;&#21183;&#22418;&#30340;&#35780;&#20998;&#20989;&#25968;&#24341;&#23548;&#22312;LLM&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#21521;&#33021;&#37327;&#26377;&#21033;&#12289;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20197;&#33258;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#30740;&#31350;&#25991;&#26412;&#20013;&#30340;&#35805;&#35821;&#29305;&#24449;&#26469;&#21306;&#20998;&#20154;&#31867;&#21019;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#20889;&#20316;&#22312;&#32467;&#26500;&#19978;&#26356;&#20026;&#22810;&#26679;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10586</link><description>&lt;p&gt;
&#32454;&#24494;&#20043;&#32447;&#65306;&#36890;&#36807;&#35805;&#35821;&#20027;&#39064;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#30740;&#31350;&#25991;&#26412;&#20013;&#30340;&#35805;&#35821;&#29305;&#24449;&#26469;&#21306;&#20998;&#20154;&#31867;&#21019;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#20889;&#20316;&#22312;&#32467;&#26500;&#19978;&#26356;&#20026;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#31867;&#21019;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#30028;&#38480;&#21464;&#24471;&#26085;&#30410;&#27169;&#31946;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35782;&#21035;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#21487;&#36776;&#35782;&#21644;&#29420;&#29305;&#30340;&#35821;&#35328;&#29305;&#24615;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#25581;&#31034;&#25991;&#26412;&#22312;&#34920;&#38754;&#32467;&#26500;&#20043;&#22806;&#30340;&#28508;&#22312;&#35805;&#35821;&#32467;&#26500;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#21033;&#29992;&#23618;&#27425;&#21270;&#35299;&#26512;&#26641;&#21644;&#36882;&#24402;&#36229;&#22270;&#26469;&#25581;&#31034;LLM&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#29420;&#29305;&#35805;&#35821;&#27169;&#24335;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#37117;&#21463;&#29305;&#23450;&#39046;&#22495;&#30340;&#24433;&#21709;&#32780;&#20135;&#29983;&#19981;&#21516;&#30340;&#35805;&#35821;&#27169;&#24335;&#65292;&#20294;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#34920;&#29616;&#20986;&#26356;&#22810;&#30340;&#32467;&#26500;&#21464;&#24322;&#24615;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#39046;&#22495;&#20154;&#31867;&#20889;&#20316;&#30340;&#24494;&#22937;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24341;&#20837;&#23618;&#27425;&#35805;&#35821;&#29305;&#24449;&#21487;&#20197;&#22686;&#24378;&#20108;&#20803;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10586v1 Announce Type: new  Abstract: With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between huma
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEWL&#30340;&#24187;&#35273;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;LLM&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#35780;&#20272;&#20107;&#23454;&#24615;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.10412</link><description>&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#21152;&#26435;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;LLM&#22312;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#30340;&#34394;&#26500;
&lt;/p&gt;
&lt;p&gt;
Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10412
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEWL&#30340;&#24187;&#35273;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;LLM&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#35780;&#20272;&#20107;&#23454;&#24615;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#20107;&#23454;&#19981;&#27491;&#30830;&#20294;&#30475;&#20284;&#20196;&#20154;&#20449;&#26381;&#30340;&#31572;&#26696;&#65292;&#30446;&#21069;&#26159;LLM&#21487;&#20449;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#23041;&#32961;&#12290;&#35299;&#20915;&#36825;&#19968;&#22797;&#26434;&#38382;&#39064;&#30340;&#31532;&#19968;&#27493;&#26159;&#23545;&#20854;&#36827;&#34892;&#34913;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24187;&#35273;&#24230;&#37327;&#26631;&#20934;&#38656;&#35201;&#20855;&#26377;&#20855;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21363;&#20154;&#31867;&#32534;&#20889;&#30340;&#8220;&#26368;&#20339;&#8221;&#25110;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#12290;&#36825;&#31181;&#35201;&#27714;&#20351;&#24187;&#35273;&#27979;&#37327;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#21152;&#26435;LLM&#23545;&#20107;&#23454;&#24615;&#36827;&#34892;&#35780;&#20272;&#65288;FEWL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#37329;&#26631;&#20934;&#31572;&#26696;&#32570;&#22833;&#26102;&#35774;&#35745;&#30340;&#24187;&#35273;&#24230;&#37327;&#26631;&#20934;&#12290;FEWL&#21033;&#29992;&#20102;&#29616;&#25104;&#30340;LLM&#31572;&#26696;&#20316;&#20026;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#20195;&#29702;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#37327;&#21270;&#21442;&#32771;LLM&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;FEWL&#20855;&#26377;&#19968;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#23427;&#26356;&#20934;&#30830;&#12290;&#24230;&#37327;&#34394;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10412v1 Announce Type: cross  Abstract: LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.08983</link><description>&lt;p&gt;
SafeDecoding: &#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#21161;&#31561;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#20026;&#20102;&#20351;LLM&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#21253;&#25324;&#23433;&#20840;&#24615;&#22312;&#20869;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#36234;&#29425;&#25915;&#20987;&#26088;&#22312;&#24341;&#21457;LLM&#30340;&#38750;&#39044;&#26399;&#21644;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#20173;&#28982;&#26159;LLM&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;SafeDecoding&#26469;&#38450;&#24481;LLM&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#23433;&#20840;&#24863;&#30693;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#24320;&#21457;SafeDecoding&#26102;&#30340;&#27934;&#23519;&#21147;&#22522;&#20110;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#20195;&#34920;&#26377;&#23475;&#20869;&#23481;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#36229;&#36807;&#20195;&#34920;&#26080;&#23475;&#21709;&#24212;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#20173;&#28982;&#20986;&#29616;&#22312;&#25353;&#27010;&#29575;&#38477;&#24207;&#25490;&#24207;&#30340;&#26631;&#35760;&#20013;&#30340;&#21069;&#20960;&#20010;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35782;&#21035;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#24182;&#22686;&#24378;&#20854;&#33391;&#24615;&#24433;&#21709;&#21147;&#26469;&#20943;&#36731;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;&#12290;&#36890;&#36807;&#24314;&#31435;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#25915;&#20987;&#29983;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#37319;&#29992;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;&#25511;&#21046;&#35201;&#27714;&#19979;&#25628;&#32034;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08679</link><description>&lt;p&gt;
COLD-Attack: &#29992;&#20110;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;&#12290;&#36890;&#36807;&#24314;&#31435;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#25915;&#20987;&#29983;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#37319;&#29992;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;&#25511;&#21046;&#35201;&#27714;&#19979;&#25628;&#32034;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#36234;&#29425;&#30340;&#27880;&#24847;&#21147;&#36234;&#26469;&#36234;&#22810;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;LLM&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#24517;&#35201;&#32771;&#34385;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#36234;&#29425;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#20197;&#21450;&#24773;&#24863;/&#39118;&#26684;&#21464;&#21270;&#65292;&#22240;&#27492;&#30740;&#31350;&#21487;&#25511;&#24615;&#36234;&#29425;&#26159;&#26377;&#30410;&#30340;&#65292;&#21363;&#22914;&#20309;&#23545;LLM&#25915;&#20987;&#36827;&#34892;&#25511;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#24418;&#24335;&#21270;&#20102;&#21487;&#25511;&#24615;&#25915;&#20987;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#38382;&#39064;&#19982;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#30340;&#26032;&#22411;&#20851;&#32852;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#25506;&#32034;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#36825;&#31181;&#20851;&#32852;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#65288;COLD&#65289;&#30340;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32479;&#19968;&#19988;&#33258;&#21160;&#21270;&#22320;&#25628;&#32034;&#21508;&#31181;&#25511;&#21046;&#35201;&#27714;&#19979;&#30340;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#65292;&#20363;&#22914;&#27969;&#30021;&#24615;&#12289;&#38544;&#31192;&#24615;&#12289;&#24773;&#24863;&#21644;&#24038;&#21491;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controlla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#36890;&#29992;&#35268;&#21010;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35748;&#35782;&#24615;&#25506;&#32034;&#22635;&#34917;&#20195;&#29702;&#30340;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#19988;&#21033;&#29992;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#23398;&#20064;&#36890;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#26029;&#21464;&#21270;&#30340;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.08145</link><description>&lt;p&gt;
&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#36890;&#29992;&#35268;&#21010;&#21644;&#23398;&#20064;&#30340;&#35748;&#35782;&#24615;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#36890;&#29992;&#35268;&#21010;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35748;&#35782;&#24615;&#25506;&#32034;&#22635;&#34917;&#20195;&#29702;&#30340;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#19988;&#21033;&#29992;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#23398;&#20064;&#36890;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#26029;&#21464;&#21270;&#30340;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20351;&#29992;&#20851;&#31995;&#34920;&#31034;&#30340;&#38750;&#31283;&#24577;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36830;&#32493;&#35268;&#21010;&#21644;&#27169;&#22411;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#26679;&#30340;&#33021;&#21147;&#23545;&#20110;&#22312;&#19981;&#30830;&#23450;&#12289;&#19981;&#26029;&#21457;&#23637;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#39034;&#24207;&#20915;&#31574;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26410;&#30693;&#65288;&#19988;&#38750;&#31283;&#24577;&#65289;&#36716;&#25442;&#31995;&#32479;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#20219;&#21153;&#20013;&#24037;&#20316;&#26102;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#20195;&#29702;&#29366;&#24577;&#30693;&#35782;&#30340;&#31354;&#30333;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24320;&#23637;&#19987;&#27880;&#12289;&#35843;&#26597;&#24615;&#30340;&#25506;&#32034;&#12290;&#20351;&#29992;&#36825;&#20123;&#25506;&#32034;&#25910;&#38598;&#30340;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#36890;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#30340;&#20219;&#21153;&#65292;&#23613;&#31649;&#29615;&#22659;&#21160;&#24577;&#19981;&#26029;&#21464;&#21270;&#12290;&#23545;&#20960;&#20010;&#22522;&#20934;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#35268;&#21010;&#21644;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#31995;&#32479;&#33021;&#22815;&#22238;&#24402;&#21040;&#34920;&#29616;&#20986;&#29702;&#24819;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new approach for continual planning and model learning in non-stationary stochastic environments expressed using relational representations. Such capabilities are essential for the deployment of sequential decision-making systems in the uncertain, constantly evolving real world. Working in such practical settings with unknown (and non-stationary) transition systems and changing tasks, the proposed framework models gaps in the agent's current state of knowledge and uses them to conduct focused, investigative explorations. Data collected using these explorations is used for learning generalizable probabilistic models for solving the current task despite continual changes in the environment dynamics. Empirical evaluations on several benchmark domains show that this approach significantly outperforms planning and RL baselines in terms of sample complexity in non-stationary settings. Theoretical results show that the system reverts to exhibit desirable convergence pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#23545;&#20110;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#20855;&#26377;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.07933</link><description>&lt;p&gt;
&#20154;&#26412;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#30340;&#26080;&#20195;&#30721;AutoML&#65306;&#27010;&#24565;&#26694;&#26550;&#12289;&#28508;&#21147;&#19982;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#23545;&#20110;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#36825;&#20123;&#20135;&#21697;&#30340;&#29305;&#28857;&#26159;&#19981;&#21487;&#39044;&#27979;&#24615;&#21644;&#23545;&#38750;&#19987;&#23478;&#30340;&#19981;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#12290;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30340;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#26080;&#32541;&#25191;&#34892;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#65292;&#36825;&#23545;&#20110;&#20154;&#26412;&#26234;&#33021;&#20135;&#21697;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#34892;&#19994;&#21644;&#21019;&#26032;&#30456;&#20851;&#65292;&#23427;&#24433;&#21709;&#25112;&#30053;&#20915;&#31574;&#21644;&#25237;&#36164;&#39118;&#38505;&#30340;&#38477;&#20302;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#23545;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#24819;&#27861;&#30340;&#28508;&#21147;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#35774;&#35745;&#31185;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#26080;&#20195;&#30721;AutoML&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#24320;&#21457;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#20351;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#21463;&#30410;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates No-Code AutoML as a solution for challenges in AI product prototyping, characterized by unpredictability and inaccessibility to non-experts, and proposes a conceptual framework. This complexity of AI products hinders seamless execution and interdisciplinary collaboration crucial for human-centered AI products. Relevant to industry and innovation, it affects strategic decision-making and investment risk mitigation. Current approaches provide limited insights into the potential and feasibility of AI product ideas. Employing Design Science Research, the study identifies challenges and integrates no-code AutoML as a solution by presenting a framework for AI product prototyping with No-code AutoML. A case study confirms its potential in supporting non-experts, offering a structured approach to AI product development. The framework facilitates accessible and interpretable prototyping, benefiting academia, managers, and decision-makers. Strategic integration of no-code Au
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#22810;&#39046;&#22495;&#21305;&#37197;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMA&#26694;&#26550;&#26469;&#25913;&#21892;B2B&#20113;&#35299;&#20915;&#26041;&#26696;&#30340;&#21305;&#37197;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#29305;&#24449;&#21644;&#26377;&#38480;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07076</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#65292;&#25913;&#21892;&#22810;&#39046;&#22495;B2B&#20113;&#35299;&#20915;&#26041;&#26696;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07076
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#22810;&#39046;&#22495;&#21305;&#37197;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMA&#26694;&#26550;&#26469;&#25913;&#21892;B2B&#20113;&#35299;&#20915;&#26041;&#26696;&#30340;&#21305;&#37197;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#29305;&#24449;&#21644;&#26377;&#38480;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35299;&#20915;&#26041;&#26696;&#22312;&#25216;&#26415;&#34892;&#19994;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26381;&#21153;&#21644;&#24037;&#20855;&#26469;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#21830;&#30340;&#38144;&#21806;&#22242;&#38431;&#20173;&#28982;&#38754;&#20020;&#19968;&#20010;&#22797;&#26434;&#30340;&#19994;&#21153;&#38382;&#39064;&#65292;&#21363;&#25214;&#21040;&#36866;&#21512;&#29305;&#23450;&#30446;&#26631;&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#36866;&#30340;&#20844;&#21496;&#23458;&#25143;&#65292;&#29616;&#26377;&#30340;&#21305;&#37197;&#31995;&#32479;&#23578;&#26410;&#33021;&#22815;&#20805;&#20998;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;B2B&#35299;&#20915;&#26041;&#26696;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#35813;&#22330;&#26223;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;(1) &#22797;&#26434;&#22810;&#39046;&#22495;&#29305;&#24449;&#30340;&#24314;&#27169;&#21644;(2) &#26377;&#38480;&#12289;&#19981;&#23436;&#25972;&#21644;&#31232;&#30095;&#30340;&#20132;&#26131;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;CAMA&#65292;&#23427;&#20197;&#20998;&#23618;&#22810;&#39046;&#22495;&#21305;&#37197;&#32467;&#26500;&#20026;&#20027;&#24178;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#24357;&#34917;&#21487;&#29992;&#25968;&#25454;&#30340;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction data. To tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three data augmentation strategies and a contrastive pre-training objective to compensate for the imperfections in the available data. Through extensive experiments on a real-world dataset, we demonstra
&lt;/p&gt;</description></item><item><title>&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06627</link><description>&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#25512;&#21160;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Feedback Loops With Language Models Drive In-Context Reward Hacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06627
&lt;/p&gt;
&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#22806;&#37096;&#19990;&#30028;&#20135;&#29983;&#24433;&#21709;&#65306;&#23427;&#20204;&#26597;&#35810;&#21487;&#20197;&#35835;&#20889;&#32593;&#39029;&#30340;API&#65292;&#29983;&#25104;&#33021;&#22815;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#36816;&#34892;&#31995;&#32479;&#21629;&#20196;&#12290;&#36825;&#20123;&#20114;&#21160;&#24418;&#25104;&#20102;&#21453;&#39304;&#24490;&#29615;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#24433;&#21709;&#19990;&#30028;&#65292;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#21518;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;(ICRH)&#65292;&#21363;&#27979;&#35797;&#26102;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20248;&#21270;&#65288;&#21487;&#33021;&#38544;&#21547;&#30340;&#65289;&#30446;&#26631;&#30340;&#21516;&#26102;&#65292;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#19968;&#20010;&#34987;&#37096;&#32626;&#29992;&#20110;&#22686;&#21152;Twitter&#21442;&#19982;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65307;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#26816;&#32034;&#20854;&#20197;&#21069;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#25512;&#25991;&#26356;&#20855;&#20105;&#35758;&#24615;&#65292;&#20174;&#32780;&#22686;&#21152;&#21442;&#19982;&#24230;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;&#23548;&#33268;ICRH&#30340;&#20004;&#20010;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#23545;&#20110;&#36825;&#20123;&#36807;&#31243;&#65292;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26159;&#19981;&#36275;&#22815;&#30340;-&#20182;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#21453;&#39304;&#25928;&#24212;&#65292;&#20063;&#19981;&#33021;&#25429;&#25417;&#21040;&#26368;&#26377;&#23475;&#30340;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07844</link><description>&lt;p&gt;
&#20351;&#29992;ODE&#26041;&#27861;&#36827;&#34892;&#24102;&#26377;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#36924;&#36817;&#21644;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#26159;&#19968;&#31867;&#36890;&#36807;&#36845;&#20195;&#12289;&#22686;&#37327;&#21644;&#38543;&#26426;&#26356;&#26032;&#21521;&#37327;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12290;&#20998;&#26512;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#65292;&#21363;&#35777;&#26126;&#38543;&#26426;&#21521;&#37327;&#36845;&#20195;&#20960;&#20046;&#24517;&#23450;&#26377;&#30028;&#12290;&#26412;&#25991;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#36164;&#26684;&#36857;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30340;&#26680;&#24515;&#22312;&#20110;&#23569;&#25968;&#20989;&#25968;&#30340;&#28176;&#36827;&#21464;&#21270;&#36895;&#29575;&#19979;&#38477;&#65292;&#36825;&#19968;&#28857;&#30001;&#22823;&#25968;&#23450;&#24459;&#21644;&#24120;&#29992;&#30340;V4 Lyapunov&#28418;&#31227;&#26465;&#20214;&#38544;&#21547;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#38142;&#26159;&#26377;&#38480;&#19988;&#19981;&#21487;&#32422;&#26102;&#26174;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
&lt;/p&gt;</description></item><item><title>LCNet&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#23384;&#20648;&#20849;&#20139;&#32467;&#26500;&#65292;&#21516;&#26102;&#20351;&#29992;&#19978;&#19979;&#25991;&#27169;&#22359;&#34920;&#31034;&#29305;&#23450;&#19978;&#19979;&#25991;&#32467;&#26500;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#25552;&#21462;&#20849;&#20139;&#32467;&#26500;&#24182;&#36991;&#20813;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#21151;&#33021;&#65307;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#25968;&#25454;&#20013;&#30340;&#35838;&#31243;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2312.08519</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#21327;&#35843;&#20849;&#20139;&#19982;&#29305;&#23450;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#28508;&#22312;&#22240;&#26524;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Reconciling Shared versus Context-Specific Information in a Neural Network Model of Latent Causes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08519
&lt;/p&gt;
&lt;p&gt;
LCNet&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#23384;&#20648;&#20849;&#20139;&#32467;&#26500;&#65292;&#21516;&#26102;&#20351;&#29992;&#19978;&#19979;&#25991;&#27169;&#22359;&#34920;&#31034;&#29305;&#23450;&#19978;&#19979;&#25991;&#32467;&#26500;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#25552;&#21462;&#20849;&#20139;&#32467;&#26500;&#24182;&#36991;&#20813;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#21151;&#33021;&#65307;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#25968;&#25454;&#20013;&#30340;&#35838;&#31243;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#65292;&#24403;&#22788;&#29702;&#19968;&#31995;&#21015;&#20107;&#20214;&#26102;&#65292;&#20154;&#31867;&#20250;&#26681;&#25454;&#25512;&#26029;&#30340;&#28508;&#22312;&#22240;&#26524;&#65288;LCs&#65289;&#26469;&#21010;&#20998;&#20182;&#20204;&#30340;&#32463;&#39564;&#65292;&#20197;&#25903;&#25345;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#24403;&#20849;&#20139;&#32467;&#26500;&#23384;&#22312;&#20110;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#26102;&#65292;&#22914;&#20309;&#21516;&#26102;&#23454;&#29616;LCs&#30340;&#8220;&#20998;&#35010;&#8221;&#21644;&#23398;&#20064;&#20849;&#20139;&#32467;&#26500;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#28508;&#22312;&#22240;&#26524;&#32593;&#32476;&#65288;LCNet&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;LC&#25512;&#26029;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#23398;&#20064;&#65292;&#23427;&#33258;&#28982;&#22320;&#20648;&#23384;&#32593;&#32476;&#26435;&#37325;&#20013;&#36328;&#20219;&#21153;&#20849;&#20139;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#19978;&#19979;&#25991;&#27169;&#22359;&#34920;&#31034;&#29305;&#23450;&#19978;&#19979;&#25991;&#32467;&#26500;&#65292;&#30001;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#25512;&#29702;&#31639;&#27861;&#25511;&#21046;&#65292;&#20026;&#27599;&#20010;&#25512;&#26029;&#30340;LC&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#12290;&#36890;&#36807;&#19977;&#20010;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LCNet&#33021;&#22815;1)&#22312;&#21151;&#33021;&#23398;&#20064;&#20219;&#21153;&#20013;&#25552;&#21462;&#36328;LC&#30340;&#20849;&#20139;&#32467;&#26500;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;2)&#25429;&#25417;&#20851;&#20110;&#35838;&#31243;&#25928;&#24212;&#30340;&#20154;&#31867;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08519v2 Announce Type: replace-cross  Abstract: It has been proposed that, when processing a stream of events, humans divide their experiences in terms of inferred latent causes (LCs) to support context-dependent learning. However, when shared structure is present across contexts, it is still unclear how the "splitting" of LCs and learning of shared structure can be simultaneously achieved. Here, we present the Latent Cause Network (LCNet), a neural network model of LC inference. Through learning, it naturally stores structure that is shared across tasks in the network weights. Additionally, it represents context-specific structure using a context module, controlled by a Bayesian nonparametric inference algorithm, which assigns a unique context vector for each inferred LC. Across three simulations, we found that LCNet could 1) extract shared structure across LCs in a function learning task while avoiding catastrophic interference, 2) capture human data on curriculum effects 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#36923;&#36753;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.17438</link><description>&lt;p&gt;
CLOMO: &#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;
CLOMO: Counterfactual Logical Modification with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#36923;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22521;&#20859;LLMs&#20869;&#30340;&#21453;&#20107;&#23454;&#24605;&#32500;&#36807;&#31243;&#65292;&#24182;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;&#65288;CLOMO&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;LLMs&#24517;&#39035;&#29087;&#32451;&#22320;&#25913;&#21464;&#32473;&#23450;&#30340;&#35770;&#35777;&#25991;&#26412;&#65292;&#20197;&#20445;&#25345;&#39044;&#23450;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#20026;&#20102;&#26377;&#25928;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36923;&#36753;&#24863;&#30693;&#30340;&#21453;&#20107;&#23454;&#20998;&#25968;&#65292;&#30452;&#25509;&#35780;&#20272;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#23558;&#20219;&#21153;&#24314;&#27169;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#20559;&#22909;&#24456;&#22909;&#22320;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLMs&#23637;&#31034;&#20102;&#26174;&#30528;&#30340;&#36923;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17438v3 Announce Type: replace-cross Abstract: In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the LogicAware Counterfactual Score to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for log
&lt;/p&gt;</description></item><item><title>Fusion-Eval&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#25972;&#21512;&#19981;&#21516;&#36741;&#21161;&#35780;&#20272;&#22120;&#30340;&#35265;&#35299;&#65292;&#26497;&#22823;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09204</link><description>&lt;p&gt;
Fusion-Eval: &#23558;&#35780;&#20272;&#22120;&#19982;LLMs&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Fusion-Eval: Integrating Evaluators with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09204
&lt;/p&gt;
&lt;p&gt;
Fusion-Eval&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#25972;&#21512;&#19981;&#21516;&#36741;&#21161;&#35780;&#20272;&#22120;&#30340;&#35265;&#35299;&#65292;&#26497;&#22823;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#30340;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#39640;&#32423;&#25512;&#29702;&#39046;&#22495;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Fusion-Eval&#8221;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25972;&#21512;&#26469;&#33258;&#21508;&#31181;&#36741;&#21161;&#35780;&#20272;&#22120;&#30340;&#35265;&#35299;&#12290;&#27599;&#20010;&#35780;&#20272;&#22120;&#19987;&#38376;&#36127;&#36131;&#35780;&#20272;&#21709;&#24212;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#36825;&#31181;&#29420;&#29305;&#31574;&#30053;&#20351;&#24471;Fusion-Eval&#33021;&#22815;&#26377;&#25928;&#22320;&#36328;&#36234;&#21508;&#31181;&#20219;&#21153;&#21644;&#26631;&#20934;&#65292;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#22312;SummEval&#19978;&#65292;Fusion-Eval&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#31995;&#32479;&#32423;Kendall-Tau&#30456;&#20851;&#24615;&#36798;&#21040;0.962&#65292;&#22312;TopicalChat&#19978;&#30340;&#36718;&#32423;Spearman&#30456;&#20851;&#24615;&#36798;&#21040;0.744&#65292;&#36828;&#39640;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;Fusion-Eval&#22312;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#35780;&#20272;&#39046;&#22495;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09204v2 Announce Type: replace-cross  Abstract: Evaluating natural language systems poses significant challenges, particularly in the realms of natural language understanding and high-level reasoning. In this paper, we introduce "Fusion-Eval", an innovative approach that leverages Large Language Models (LLMs) to integrate insights from various assistant evaluators. Each of these evaluators specializes in assessing distinct aspects of responses. This unique strategy enables Fusion-Eval to function effectively across a diverse range of tasks and criteria, enhancing the effectiveness of existing evaluation methods. Fusion-Eval achieves a 0.962 system-level Kendall-Tau correlation with humans on SummEval and a 0.744 turn-level Spearman correlation on TopicalChat, which is significantly higher than baseline methods. These results highlight Fusion-Eval's significant potential in the realm of natural language system evaluation.
&lt;/p&gt;</description></item><item><title>CodeScope&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#33021;&#21147;&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22810;&#32500;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#22810;&#35821;&#35328;&#32534;&#31243;&#29615;&#22659;&#21644;&#22810;&#20219;&#21153;&#35774;&#32622;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2311.08588</link><description>&lt;p&gt;
CodeScope:&#19968;&#20010;&#22522;&#20110;&#25191;&#34892;&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22810;&#32500;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08588
&lt;/p&gt;
&lt;p&gt;
CodeScope&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#33021;&#21147;&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22810;&#32500;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#22810;&#35821;&#35328;&#32534;&#31243;&#29615;&#22659;&#21644;&#22810;&#20219;&#21153;&#35774;&#32622;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32534;&#30721;&#30456;&#20851;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#24110;&#21161;&#20154;&#31867;&#32534;&#31243;&#21644;&#20419;&#36827;&#32534;&#31243;&#33258;&#21160;&#21270;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;&#22522;&#20934;&#23384;&#22312;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22823;&#37096;&#20998;&#22522;&#20934;&#23384;&#22312;&#32570;&#38519;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#29421;&#31364;&#33539;&#22260;&#20869;&#30340;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#21644;&#29305;&#23450;&#20219;&#21153;&#65292;&#32780;&#23454;&#38469;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#38656;&#35201;&#23454;&#29616;&#22810;&#35821;&#35328;&#32534;&#31243;&#29615;&#22659;&#20197;&#28385;&#36275;&#21508;&#31181;&#38656;&#27714;&#12290;&#23454;&#38469;&#32534;&#31243;&#23454;&#36341;&#36824;&#24378;&#28872;&#26399;&#26395;&#22810;&#20219;&#21153;&#35774;&#32622;&#65292;&#20197;&#20840;&#38754;&#21644;&#31283;&#20581;&#22320;&#27979;&#35797;LLMs&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#22823;&#37096;&#20998;&#22522;&#20934;&#20063;&#26410;&#32771;&#34385;&#29983;&#25104;&#20195;&#30721;&#30340;&#21487;&#25191;&#34892;&#24615;&#21644;&#25191;&#34892;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#29616;&#26377;&#22522;&#20934;&#19982;&#23454;&#38469;&#24212;&#29992;&#26399;&#26395;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CodeScope&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance on coding related tasks, particularly on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks, whereas the real-world software development scenarios show dire need to implement systems with multilingual programming environments to satisfy diverse requirements. Practical programming practices also strongly expect multi-task settings for testing coding capabilities of LLMs comprehensively and robustly. Second, most benchmarks also fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Lie&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#25968;&#25454;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20351;&#20854;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;&#36890;&#36807;&#25512;&#24191;&#21521;&#37327;&#31070;&#32463;&#20803;&#32593;&#32476;&#21644;&#24341;&#20837;&#26032;&#30340;&#23618;&#65292;&#35813;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.04521</link><description>&lt;p&gt;
Lie&#31070;&#32463;&#20803;&#65306;&#21322;&#21333;Lie&#20195;&#25968;&#30340;&#20276;&#38543;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Lie&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#25968;&#25454;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20351;&#20854;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;&#36890;&#36807;&#25512;&#24191;&#21521;&#37327;&#31070;&#32463;&#20803;&#32593;&#32476;&#21644;&#24341;&#20837;&#26032;&#30340;&#23618;&#65292;&#35813;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#35813;&#25968;&#25454;&#23384;&#22312;&#20110;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#20013;&#12290;&#23545;&#24212;&#30340;&#32676;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20316;&#29992;&#20110;Lie&#20195;&#25968;&#19978;&#65292;&#20351;&#24471;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#20855;&#26377;&#20276;&#38543;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#31616;&#21333;&#30340;$\mathrm{SO}(3)$-&#31561;&#21464;&#32593;&#32476;&#8212;&#8212;&#21521;&#37327;&#31070;&#32463;&#20803;&#20174;3&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25512;&#24191;&#21040;Lie&#20195;&#25968;&#31354;&#38388;&#65292;&#21033;&#29992;Killing&#24418;&#24335;&#30340;&#19981;&#21464;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;Lie&#25324;&#21495;&#23618;&#21644;&#20960;&#20309;&#36890;&#36947;&#28151;&#21512;&#23618;&#26469;&#25193;&#23637;&#24314;&#27169;&#33021;&#21147;&#12290;&#23454;&#39564;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;$\mathfrak{so}(3)$&#21644;$\mathfrak{sl}(3)$ Lie&#20195;&#25968;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#25311;&#21512;&#31561;&#21464;&#21644;&#19981;&#21464;&#20989;&#25968;&#12289;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#12289;&#28857;&#20113;&#37197;&#20934;&#21644;&#22522;&#20110;&#21333;&#24212;&#24615;&#30340;&#24418;&#29366;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31561;&#21464;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an equivariant neural network that takes data in any semi-simple Lie algebra as input. The corresponding group acts on the Lie algebra as adjoint operations, making our proposed network adjoint-equivariant. Our framework generalizes the Vector Neurons, a simple $\mathrm{SO}(3)$-equivariant network, from 3-D Euclidean space to Lie algebra spaces, building upon the invariance property of the Killing form. Furthermore, we propose novel Lie bracket layers and geometric channel mixing layers that extend the modeling capacity. Experiments are conducted for the $\mathfrak{so}(3)$ and $\mathfrak{sl}(3)$ Lie algebras on various tasks, including fitting equivariant and invariant functions, learning system dynamics, point cloud registration, and homography-based shape classification. Our proposed equivariant network shows wide applicability and competitive performance in various domains.
&lt;/p&gt;</description></item><item><title>WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13919</link><description>&lt;p&gt;
WebVoyager&#65306;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#30340;Web Agent
&lt;/p&gt;
&lt;p&gt;
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13919
&lt;/p&gt;
&lt;p&gt;
WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#24341;&#39046;&#20102;&#19968;&#20010;&#30001;&#30495;&#23454;&#19990;&#30028;&#20013;&#33258;&#20027;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#25152;&#26631;&#24535;&#30340;&#26032;&#26102;&#20195;&#65292;&#25512;&#21160;&#20102;&#22522;&#20110;&#32593;&#32476;&#30340;&#39640;&#32423;&#20195;&#29702;&#30340;&#21019;&#26032;&#12290;&#29616;&#26377;&#30340;&#32593;&#32476;&#20195;&#29702;&#36890;&#24120;&#21482;&#22788;&#29702;&#19968;&#20010;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#19988;&#20165;&#22312;&#31616;&#21270;&#30340;&#32593;&#32476;&#27169;&#25311;&#22120;&#25110;&#38745;&#24577;&#30340;&#32593;&#32476;&#24555;&#29031;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WebVoyager&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;Web&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#24335;Web&#20195;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#25361;&#25112;&#65292;&#21033;&#29992;&#20102;GPT-4V&#30340;&#24378;&#22823;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;15&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32593;&#31449;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;WebVoyager&#23454;&#29616;&#20102;55.7&#65285;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#26174;&#33879;&#22320;.....
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
&lt;/p&gt;</description></item><item><title>HyperSense&#26159;&#19968;&#20010;&#21327;&#21516;&#35774;&#35745;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#29289;&#20307;&#23384;&#22312;&#39044;&#27979;&#26377;&#25928;&#22320;&#25511;&#21046;&#25968;&#25454;&#29983;&#25104;&#36895;&#29575;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31934;&#24230;ADC&#20943;&#23569;&#20887;&#20313;&#25968;&#25454;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25104;&#26412;&#65292;&#21033;&#29992;&#36229;&#32500;&#24230;&#35745;&#31639;&#30340;&#29305;&#28857;&#20998;&#26512;&#23454;&#26102;&#30340;&#20302;&#31934;&#24230;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#12289;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#21644;&#23454;&#26102;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39640;&#24615;&#33021;&#30340;&#29289;&#20307;&#26816;&#27979;&#36719;&#20214;&#21644;&#23454;&#26102;&#30340;&#30828;&#20214;&#39044;&#27979;&#65292;&#24341;&#20837;&#20102;&#26234;&#33021;&#20256;&#24863;&#22120;&#25511;&#21046;&#30340;&#26032;&#27010;&#24565;&#12290;&#22312;&#36719;&#20214;&#21644;&#30828;&#20214;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10267</link><description>&lt;p&gt;
HyperSense: &#21152;&#36895;&#36229;&#32500;&#24230;&#35745;&#31639;&#20197;&#29992;&#20110;&#26234;&#33021;&#20256;&#24863;&#22120;&#25968;&#25454;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
HyperSense: Accelerating Hyper-Dimensional Computing for Intelligent Sensor Data Processing. (arXiv:2401.10267v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10267
&lt;/p&gt;
&lt;p&gt;
HyperSense&#26159;&#19968;&#20010;&#21327;&#21516;&#35774;&#35745;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#29289;&#20307;&#23384;&#22312;&#39044;&#27979;&#26377;&#25928;&#22320;&#25511;&#21046;&#25968;&#25454;&#29983;&#25104;&#36895;&#29575;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31934;&#24230;ADC&#20943;&#23569;&#20887;&#20313;&#25968;&#25454;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25104;&#26412;&#65292;&#21033;&#29992;&#36229;&#32500;&#24230;&#35745;&#31639;&#30340;&#29305;&#28857;&#20998;&#26512;&#23454;&#26102;&#30340;&#20302;&#31934;&#24230;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#12289;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#21644;&#23454;&#26102;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39640;&#24615;&#33021;&#30340;&#29289;&#20307;&#26816;&#27979;&#36719;&#20214;&#21644;&#23454;&#26102;&#30340;&#30828;&#20214;&#39044;&#27979;&#65292;&#24341;&#20837;&#20102;&#26234;&#33021;&#20256;&#24863;&#22120;&#25511;&#21046;&#30340;&#26032;&#27010;&#24565;&#12290;&#22312;&#36719;&#20214;&#21644;&#30828;&#20214;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;HyperSense&#65292;&#25105;&#20204;&#21327;&#21516;&#35774;&#35745;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#31995;&#32479;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#29289;&#20307;&#23384;&#22312;&#39044;&#27979;&#26377;&#25928;&#22320;&#25511;&#21046;&#27169;&#25311;&#21040;&#25968;&#23383;&#36716;&#25442;&#22120;&#65288;ADC&#65289;&#27169;&#22359;&#30340;&#25968;&#25454;&#29983;&#25104;&#36895;&#29575;&#12290;&#38024;&#23545;&#19981;&#26029;&#22686;&#21152;&#30340;&#20256;&#24863;&#22120;&#25968;&#37327;&#21644;&#25968;&#25454;&#36895;&#29575;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;HyperSense&#20351;&#29992;&#39640;&#25928;&#30340;&#20302;&#31934;&#24230;ADC&#20943;&#23569;&#20887;&#20313;&#30340;&#25968;&#23383;&#25968;&#25454;&#65292;&#38477;&#20302;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25104;&#26412;&#12290;&#21033;&#29992;&#31070;&#32463;&#21551;&#21457;&#30340;&#36229;&#32500;&#24230;&#35745;&#31639;&#65288;HDC&#65289;&#65292;HyperSense&#20998;&#26512;&#23454;&#26102;&#30340;&#21407;&#22987;&#20302;&#31934;&#24230;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#12289;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#21644;&#23454;&#26102;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;HyperSense&#27169;&#22411;&#23558;&#39640;&#24615;&#33021;&#30340;&#29289;&#20307;&#26816;&#27979;&#36719;&#20214;&#19982;&#23454;&#26102;&#30340;&#30828;&#20214;&#39044;&#27979;&#32467;&#21512;&#36215;&#26469;&#65292;&#24341;&#20837;&#20102;&#26234;&#33021;&#20256;&#24863;&#22120;&#25511;&#21046;&#30340;&#26032;&#27010;&#24565;&#12290;&#20840;&#38754;&#30340;&#36719;&#20214;&#21644;&#30828;&#20214;&#35780;&#20272;&#23637;&#31034;&#20102;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#36890;&#36807;&#26368;&#39640;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#21644;&#26368;&#38497;&#30340;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24615;&#65288;ROC&#65289;&#26354;&#32447;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing HyperSense, our co-designed hardware and software system efficiently controls Analog-to-Digital Converter (ADC) modules' data generation rate based on object presence predictions in sensor data. Addressing challenges posed by escalating sensor quantities and data rates, HyperSense reduces redundant digital data using energy-efficient low-precision ADC, diminishing machine learning system costs. Leveraging neurally-inspired HyperDimensional Computing (HDC), HyperSense analyzes real-time raw low-precision sensor data, offering advantages in handling noise, memory-centricity, and real-time learning.  Our proposed HyperSense model combines high-performance software for object detection with real-time hardware prediction, introducing the novel concept of Intelligent Sensor Control. Comprehensive software and hardware evaluations demonstrate our solution's superior performance, evidenced by the highest Area Under the Curve (AUC) and sharpest Receiver Operating Characteristic (ROC
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#23545;&#27604;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27714;&#35299;&#35270;&#35282;&#21644;&#24635;&#32467;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02009</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#27604;&#65306;&#36890;&#36807;&#19981;&#19968;&#33268;&#30340;&#27714;&#35299;&#35270;&#35282;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24605;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02009
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#27604;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27714;&#35299;&#35270;&#35282;&#21644;&#24635;&#32467;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19968;&#31181;&#20107;&#21518;&#25552;&#31034;&#31574;&#30053;&#65292;&#20363;&#22914;&#21453;&#24605;&#21644;&#33258;&#25105;&#25913;&#36827;&#65292;&#26681;&#25454;&#33258;&#25105;&#35780;&#20272;&#25110;&#22806;&#37096;&#21453;&#39304;&#26469;&#25913;&#21892;LLM&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#20869;&#22312;&#21453;&#24605;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#33258;&#25105;&#35780;&#20272;&#21453;&#39304;&#36136;&#37327;&#26159;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#22312;&#33258;&#25105;&#35780;&#20272;&#26102;&#24120;&#24120;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#25110;&#39640;&#24230;&#38543;&#26426;&#24615;&#65292;&#25552;&#20379;&#22266;&#25191;&#25110;&#19981;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#23548;&#33268;&#21453;&#24605;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#23545;&#27604;&#30340;&#26041;&#27861;&#65306;&#23427;&#26681;&#25454;&#35831;&#27714;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#22810;&#26679;&#30340;&#27714;&#35299;&#35270;&#35282;&#65292;&#23545;&#27604;&#24046;&#24322;&#65292;&#24182;&#23558;&#36825;&#20123;&#24046;&#24322;&#24635;&#32467;&#20026;&#19968;&#20010;&#26816;&#26597;&#34920;&#65292;&#29992;&#20110;&#37325;&#26032;&#23457;&#35270;&#21644;&#28040;&#38500;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36171;&#20104;LLM&#22810;&#26679;&#30340;&#35270;&#35282;&#20197;&#20943;&#36731;&#22266;&#25191;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#24046;&#24322;&#25351;&#31034;&#20102;&#28508;&#22312;&#30340;&#38169;&#35823;&#25110;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#24182;&#21033;&#29992;LLM&#26694;&#26550;&#29983;&#25104;&#22810;&#26679;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.14187</link><description>&lt;p&gt;
WaveCoder: &#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#19982;&#23436;&#21892;&#30340;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#24182;&#21033;&#29992;LLM&#26694;&#26550;&#29983;&#25104;&#22810;&#26679;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23545;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#21518;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#32463;&#24120;&#20250;&#20135;&#29983;&#37325;&#22797;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#25968;&#25454;&#36136;&#37327;&#30340;&#25511;&#21046;&#19981;&#22815;&#28789;&#27963;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#20026;4&#20010;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#25193;&#23637;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#22120;-&#21028;&#21035;&#22120;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#20174;&#24320;&#28304;&#20195;&#30721;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeOcean&#65292;&#19968;&#20010;&#21253;&#21547;4&#20010;&#36890;&#29992;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#12289;&#20849;&#35745;20,000&#20010;&#25351;&#20196;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#25351;&#20196;&#35843;&#20248;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#30340;Code LLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work demonstrates that, after being fine-tuned on a high-quality instruction dataset, the resulting model can obtain impressive capabilities to address a wide range of tasks. However, existing methods for instruction data generation often produce duplicate data and are not controllable enough on data quality. In this paper, we extend the generalization of instruction tuning by classifying the instruction data to 4 code-related tasks and propose a LLM-based Generator-Discriminator data process framework to generate diverse, high-quality instruction data from open source code. Hence, we introduce CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal code-related tasks,which is aimed at augmenting the effectiveness of instruction tuning and improving the generalization ability of fine-tuned model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with Widespread And Versatile Enhanced instruction tuning. This model is specifically designed for enha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#22312;&#23548;&#33322;&#39046;&#22495;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2311.00530</link><description>&lt;p&gt;
LLMs&#23545;&#20855;&#36523;&#23548;&#33322;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
The Development of LLMs for Embodied Navigation. (arXiv:2311.00530v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#22312;&#23548;&#33322;&#39046;&#22495;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35832;&#22914;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#20043;&#31867;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#30340;&#24212;&#29992;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;&#23548;&#33322;&#20219;&#21153;&#23588;&#20026;&#24341;&#20154;&#27880;&#30446;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;&#23545;&#29615;&#22659;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#21644;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;LLMs&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20854;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#22270;&#20687;&#22788;&#29702;&#33021;&#21147;&#65292;&#22686;&#24378;&#20855;&#36523;&#26234;&#33021;&#31995;&#32479;&#22312;&#29615;&#22659;&#24863;&#30693;&#21644;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#20043;&#38388;&#22312;&#23548;&#33322;&#26041;&#38754;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#23457;&#35270;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12289;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#20855;&#36523;&#23548;&#33322;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;LLMs&#22312;&#20855;&#36523;&#23548;&#33322;&#39046;&#22495;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidat
&lt;/p&gt;</description></item><item><title>BioInstruct&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#24615;&#25351;&#20196;&#25968;&#25454;&#38598;BioInstruct&#65292;&#36890;&#36807;GPT-4&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19975</link><description>&lt;p&gt;
BioInstruct:&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19975
&lt;/p&gt;
&lt;p&gt;
BioInstruct&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#24615;&#25351;&#20196;&#25968;&#25454;&#38598;BioInstruct&#65292;&#36890;&#36807;GPT-4&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#29305;&#23450;&#39046;&#22495;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21482;&#21457;&#34920;&#20102;&#24456;&#23569;&#30340;&#25351;&#20196;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioInstruct&#65292;&#36825;&#26159;&#19968;&#20010;&#23450;&#21046;&#30340;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;25,000&#20010;&#31034;&#20363;&#12290;&#36890;&#36807;&#20351;&#29992;&#19977;&#20010;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#26679;&#26412;&#65292;&#20197;GPT-4&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#31034;&#65292;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#26088;&#22312;&#20248;&#21270;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LLaMA LLMs (1&amp;2,7B&amp;13B)&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#25972;&#65292;&#24182;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25351;&#20196;&#22914;&#20309;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#20351;&#29992;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&amp;2, 7B\&amp;13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#34892;&#20026;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#31454;&#20105;&#29615;&#22659;&#24182;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#31454;&#20105;&#21487;&#20197;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#36716;&#21464;&#21644;&#37319;&#21462;&#26032;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#31038;&#20250;&#21644;&#32463;&#27982;&#21457;&#23637;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17512</link><description>&lt;p&gt;
CompeteAI:&#29702;&#35299;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#31454;&#20105;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents. (arXiv:2310.17512v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#34892;&#20026;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#31454;&#20105;&#29615;&#22659;&#24182;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#31454;&#20105;&#21487;&#20197;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#36716;&#21464;&#21644;&#37319;&#21462;&#26032;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#31038;&#20250;&#21644;&#32463;&#27982;&#21457;&#23637;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23436;&#25104;&#19981;&#21516;&#20219;&#21153;&#65292;&#22914;&#20010;&#20154;&#21161;&#29702;&#25110;&#20107;&#20214;&#35268;&#21010;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#19982;&#21327;&#20316;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#21478;&#19968;&#20010;&#37325;&#35201;&#26426;&#21046;&#8212;&#8212;&#31454;&#20105;&#65292;&#23427;&#26159;&#31038;&#20250;&#21644;&#32463;&#27982;&#21457;&#23637;&#30340;&#25512;&#21160;&#21147;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;LLM&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#30740;&#31350;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#31454;&#20105;&#29615;&#22659;&#65292;&#27169;&#25311;&#20102;&#19968;&#20010;&#30001;&#39184;&#39302;&#26234;&#33021;&#20307;&#21644;&#39038;&#23458;&#26234;&#33021;&#20307;&#32452;&#25104;&#30340;&#34394;&#25311;&#22478;&#38215;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39184;&#39302;&#26234;&#33021;&#20307;&#30456;&#20114;&#31454;&#20105;&#20197;&#21560;&#24341;&#26356;&#22810;&#39038;&#23458;&#65292;&#36825;&#31181;&#31454;&#20105;&#20419;&#20351;&#23427;&#20204;&#36827;&#34892;&#36716;&#21464;&#65292;&#27604;&#22914;&#22521;&#20859;&#26032;&#30340;&#36816;&#33829;&#31574;&#30053;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#20174;&#31038;&#20250;&#23398;&#20064;&#21040;&#39532;&#22826;&#25928;&#24212;&#31561;&#22810;&#20010;&#26377;&#36259;&#21457;&#29616;&#65292;&#19982;&#29616;&#26377;&#30340;&#31038;&#20250;&#23398;&#21644;&#32463;&#27982;&#23398;&#29702;&#35770;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that fosters the development of society and economy. In this paper, we seek to examine the competition behaviors in LLM-based agents. We first propose a general framework to study the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, restaurant agents compete with each other to attract more customers, where the competition fosters them to transform, such as cultivating new operating strategies. The results of our experiments reveal several interesting findings ranging from social learning to Matthew Effect, which aligns well with existing sociological and e
&lt;/p&gt;</description></item><item><title>&#36866;&#24212;&#24615;&#23454;&#39564;&#20026;&#25345;&#32493;&#35838;&#31243;&#25913;&#36827;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#36890;&#36807;&#21160;&#24577;&#37096;&#32626;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#20197;&#28385;&#36275;&#23398;&#29983;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.12324</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#34913;&#25945;&#24072;&#21644;&#30740;&#31350;&#32773;&#30340;&#28608;&#21169;&#65292;&#25506;&#32034;&#36866;&#24212;&#24615;&#23454;&#39564;&#20419;&#36827;&#25345;&#32493;&#25913;&#36827;&#30340;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives. (arXiv:2310.12324v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12324
&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#23454;&#39564;&#20026;&#25345;&#32493;&#35838;&#31243;&#25913;&#36827;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#36890;&#36807;&#21160;&#24577;&#37096;&#32626;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#20197;&#28385;&#36275;&#23398;&#29983;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23454;&#39564;&#27604;&#36739;&#19981;&#21516;&#25945;&#23398;&#31574;&#30053;&#30340;&#26426;&#20250;&#21487;&#20197;&#20026;&#25945;&#24072;&#30340;&#20915;&#31574;&#25552;&#20379;&#26377;&#29992;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#23454;&#39564;&#32570;&#20047;&#28165;&#26224;&#31616;&#26126;&#30340;&#20351;&#29992;&#25968;&#25454;&#24555;&#36895;&#22686;&#21152;&#23454;&#39564;&#23398;&#29983;&#33719;&#24471;&#26368;&#20339;&#26465;&#20214;&#26426;&#20250;&#30340;&#36884;&#24452;&#12290;&#21463;&#39046;&#20808;&#31185;&#25216;&#20844;&#21496;&#22312;&#20135;&#21697;&#24320;&#21457;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#23454;&#39564;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36866;&#24212;&#24615;&#23454;&#39564;&#26469;&#25345;&#32493;&#25913;&#36827;&#35838;&#31243;&#12290;&#22312;&#36866;&#24212;&#24615;&#23454;&#39564;&#20013;&#65292;&#19981;&#21516;&#30340;&#26465;&#20214;&#23558;&#34987;&#24212;&#29992;&#20110;&#23398;&#29983;&#36523;&#19978;&#65292;&#25968;&#25454;&#23558;&#34987;&#20998;&#26512;&#24182;&#29992;&#20110;&#25913;&#21464;&#26410;&#26469;&#23398;&#29983;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#21738;&#20123;&#34892;&#21160;&#21487;&#20197;&#26356;&#26377;&#24076;&#26395;&#25913;&#21892;&#23398;&#29983;&#30340;&#20307;&#39564;&#25110;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#21160;&#24577;&#22320;&#23558;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#23398;&#29983;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#23398;&#29983;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#35828;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized experimental comparisons of alternative pedagogical strategies could provide useful empirical evidence in instructors' decision-making. However, traditional experiments do not have a clear and simple pathway to using data rapidly to try to increase the chances that students in an experiment get the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. This can be done using machine learning algorithms to identify which actions are more promising for improving student experience or outcomes. This algorithm can then dynamically deploy the most effective conditions to future students, resulting in better support for students' needs. We illustrate the appr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GTA&#65289;&#65292;&#29992;&#20110;&#23558;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30456;&#23545;&#21464;&#25442;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22810;&#35270;&#22270;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10375</link><description>&lt;p&gt;
GTA&#65306;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#22810;&#35270;&#22270;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers. (arXiv:2310.10375v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10375
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GTA&#65289;&#65292;&#29992;&#20110;&#23558;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30456;&#23545;&#21464;&#25442;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22810;&#35270;&#22270;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformers&#23545;&#36755;&#20837;&#26631;&#35760;&#30340;&#25490;&#21015;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#23545;&#26631;&#35760;&#30340;&#20301;&#32622;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#23545;&#35768;&#22810;&#20219;&#21153;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#23545;&#20110;&#36890;&#24120;&#22312;&#20854;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#32467;&#26500;&#29305;&#24615;&#30340;&#35270;&#35273;&#20219;&#21153;&#26469;&#35828;&#65292;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20540;&#24471;&#24576;&#30097;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#23545;&#20110;3D&#35270;&#35273;&#20219;&#21153;&#26469;&#35828;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#23562;&#37325;&#20854;&#24213;&#23618;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#23558;&#26631;&#35760;&#30340;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30001;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#25152;&#30830;&#23450;&#30340;&#30456;&#23545;&#21464;&#25442;&#12290;&#36890;&#36807;&#22312;&#31232;&#30095;&#23485;&#22522;&#32447;&#22810;&#35270;&#22270;&#35774;&#32622;&#20013;&#35780;&#20272;&#22810;&#20010;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#65288;NVS&#65289;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#8212;&#8212;&#20960;&#20309;&#21464;&#25442;&#27880;&#24847;&#21147;&#65288;GTA&#65289;&#22914;&#20309;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-b
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05492</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#26631;&#35760;&#21644;&#21442;&#25968;&#65292;&#23637;&#29616;&#20986;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#38543;&#31561;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#24320;&#28304;&#31038;&#21306;&#24050;&#32463;&#30740;&#31350;&#20102;&#38024;&#23545;&#27599;&#31181;&#33021;&#21147;&#30340;&#20020;&#26102;SFT&#65292;&#32780;&#19987;&#26377;LLMs&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;SFT&#35299;&#38145;&#22810;&#37325;&#33021;&#21147;&#21464;&#24471;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;SFT&#36807;&#31243;&#20013;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20154;&#31867;&#23545;&#40784;&#33021;&#21147;&#20043;&#38388;&#30340;&#25968;&#25454;&#32452;&#21512;&#12290;&#20174;&#35268;&#27169;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#33021;&#21147;&#19982;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#25968;&#25454;&#37327;&#12289;&#25968;&#25454;&#32452;&#21512;&#27604;&#20363;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;SFT&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#33021;&#21147;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25193;&#23637;&#27169;&#24335;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.11436</link><description>&lt;p&gt;
&#20320;&#20165;&#20851;&#27880;&#23631;&#24149;&#65306;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#19982;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22312;&#22810;&#26679;&#29615;&#22659;&#20013;&#26377;&#25928;&#21442;&#19982;&#12290;&#20026;&#20102;&#31526;&#21512;LLM&#30340;&#36755;&#20837;-&#36755;&#20986;&#35201;&#27714;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#27801;&#30418;&#29615;&#22659;&#20013;&#24320;&#21457;&#65292;&#20381;&#36182;&#20110;&#22806;&#37096;&#24037;&#20855;&#21644;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#30340;API&#23558;&#29615;&#22659;&#35299;&#26512;&#20026;&#25991;&#26412;&#20803;&#32032;&#65292;&#24182;&#35299;&#37322;&#39044;&#27979;&#30340;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#21463;&#21040;&#25512;&#29702;&#25928;&#29575;&#20302;&#21644;&#38169;&#35823;&#20256;&#25773;&#39118;&#38505;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Auto-UI&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#30340;API&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#20316;&#38142;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#31995;&#21015;&#20013;&#38388;&#20808;&#21069;&#21160;&#20316;&#21382;&#21490;&#21644;&#26410;&#26469;&#21160;&#20316;&#35745;&#21010;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -leveraging a series of intermediate previous action histories and future action plans -- to help the age
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;ChatGPT&#20316;&#20026;&#22806;&#29983;&#20914;&#20987;&#65292;&#25581;&#31034;&#20102;&#20854;&#23545;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30452;&#25509;&#25509;&#35302;ChatGPT&#30340;&#20219;&#21153;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#20132;&#26131;&#37327;&#26174;&#33879;&#19979;&#38477;&#65292;&#20294;&#36866;&#24212;&#26032;&#25216;&#26415;&#24182;&#25552;&#20379;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#30340;&#26381;&#21153;&#30340;&#33258;&#30001;&#32844;&#19994;&#32773;&#20173;&#33021;&#33719;&#24471;&#21033;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.05201</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;"&#29983;&#25104;"&#24037;&#20316;&#65306;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#30340;&#32463;&#39564;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
"Generate" the Future of Work through AI: Empirical Evidence from Online Labor Markets. (arXiv:2308.05201v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05201
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;ChatGPT&#20316;&#20026;&#22806;&#29983;&#20914;&#20987;&#65292;&#25581;&#31034;&#20102;&#20854;&#23545;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30452;&#25509;&#25509;&#35302;ChatGPT&#30340;&#20219;&#21153;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#20132;&#26131;&#37327;&#26174;&#33879;&#19979;&#38477;&#65292;&#20294;&#36866;&#24212;&#26032;&#25216;&#26415;&#24182;&#25552;&#20379;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#30340;&#26381;&#21153;&#30340;&#33258;&#30001;&#32844;&#19994;&#32773;&#20173;&#33021;&#33719;&#24471;&#21033;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#23545;&#20854;&#23545;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#12290;&#20026;&#20102;&#22635;&#34917;&#29616;&#26377;&#30340;&#23454;&#35777;&#31354;&#30333;&#65292;&#25105;&#20204;&#23558;ChatGPT&#30340;&#25512;&#20986;&#35299;&#37322;&#20026;&#19968;&#31181;&#22806;&#29983;&#20914;&#20987;&#65292;&#24182;&#37319;&#29992;&#24046;&#24322;&#27861;&#26469;&#37327;&#21270;&#20854;&#23545;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#20013;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#24037;&#20316;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#30452;&#25509;&#25509;&#35302;ChatGPT&#30340;&#20219;&#21153;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#20132;&#26131;&#37327;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#19979;&#38477;&#22312;&#30456;&#23545;&#36739;&#39640;&#30340;&#36807;&#21435;&#20132;&#26131;&#37327;&#25110;&#36739;&#20302;&#30340;&#36136;&#37327;&#26631;&#20934;&#19979;&#23588;&#20026;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#26381;&#21153;&#25552;&#20379;&#21830;&#37117;&#26222;&#36941;&#32463;&#21382;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#38543;&#21518;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#36825;&#20010;&#36716;&#22411;&#26399;&#38388;&#65292;&#33021;&#22815;&#36866;&#24212;&#26032;&#36827;&#23637;&#24182;&#25552;&#20379;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26381;&#21153;&#30340;&#33258;&#30001;&#32844;&#19994;&#32773;&#21487;&#20197;&#33719;&#24471;&#21487;&#35266;&#30340;&#21033;&#30410;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;ChatGPT&#30340;&#20986;&#29616;&#26377;&#21487;&#33021;&#26367;&#20195;&#20154;&#21147;&#21171;&#21160;
&lt;/p&gt;
&lt;p&gt;
With the advent of general-purpose Generative AI, the interest in discerning its impact on the labor market escalates. In an attempt to bridge the extant empirical void, we interpret the launch of ChatGPT as an exogenous shock, and implement a Difference-in-Differences (DID) approach to quantify its influence on text-related jobs and freelancers within an online labor marketplace. Our results reveal a significant decrease in transaction volume for gigs and freelancers directly exposed to ChatGPT. Additionally, this decline is particularly marked in units of relatively higher past transaction volume or lower quality standards. Yet, the negative effect is not universally experienced among service providers. Subsequent analyses illustrate that freelancers proficiently adapting to novel advancements and offering services that augment AI technologies can yield substantial benefits amidst this transformative period. Consequently, even though the advent of ChatGPT could conceivably substitute
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#36731;&#37327;&#32423;&#25506;&#38024;QUAG&#21644;&#26367;&#20195;&#26041;&#27861;QUAG-attention&#65292;&#21457;&#29616;&#35270;&#39057;&#38382;&#31572;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#24187;&#35937;&#65292;&#21363;&#20351;&#22312;&#22810;&#27169;&#24577;&#25439;&#20260;&#19979;&#20173;&#33021;&#20445;&#25345;&#39640;&#24615;&#33021;&#65292;&#19988;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#23454;&#29616;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08889</link><description>&lt;p&gt;
&#25581;&#31034;&#35270;&#39057;&#38382;&#31572;&#27169;&#22411;&#20013;&#32852;&#21512;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#24187;&#35937;
&lt;/p&gt;
&lt;p&gt;
Revealing the Illusion of Joint Multimodal Understanding in VideoQA Models. (arXiv:2306.08889v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08889
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#36731;&#37327;&#32423;&#25506;&#38024;QUAG&#21644;&#26367;&#20195;&#26041;&#27861;QUAG-attention&#65292;&#21457;&#29616;&#35270;&#39057;&#38382;&#31572;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#24187;&#35937;&#65292;&#21363;&#20351;&#22312;&#22810;&#27169;&#24577;&#25439;&#20260;&#19979;&#20173;&#33021;&#20445;&#25345;&#39640;&#24615;&#33021;&#65292;&#19988;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#23454;&#29616;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;Transformer&#27169;&#22411;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#20294;&#20854;&#25104;&#21151;&#21407;&#22240;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20849;&#21516;&#25429;&#25417;&#21644;&#21033;&#29992;&#35270;&#39057;&#21644;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#32467;&#26500;&#21644;&#21160;&#24577;&#24615;&#65311;&#25110;&#32773;&#23427;&#20204;&#20165;&#20165;&#26159;&#21033;&#29992;&#20102;&#25463;&#24452;&#26469;&#33719;&#24471;&#39640;&#20998;&#65311;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#19988;&#38750;&#21442;&#25968;&#21270;&#30340;&#25506;&#38024;&#8220;QUAG&#8221;&#65288;QUadrant AveraGe&#65289;&#65292;&#20197;&#23545;&#22810;&#27169;&#24577;&#34920;&#31034;&#36827;&#34892;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;QUAG&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#31995;&#32479;&#22320;&#28040;&#38500;&#27169;&#22411;&#30340;&#32806;&#21512;&#22810;&#27169;&#24577;&#29702;&#35299;&#26469;&#20419;&#36827;&#32852;&#21512;&#25968;&#25454;&#38598;-&#27169;&#22411;&#30740;&#31350;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#34920;&#26126;&#21363;&#20351;&#22312;&#22810;&#27169;&#24577;&#25439;&#20260;&#19979;&#65292;&#27169;&#22411;&#20173;&#33021;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;QUAG&#25193;&#23637;&#20026;&#8220;QUAG-attention&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21270;&#19988;&#34920;&#36798;&#33021;&#21147;&#36739;&#24369;&#30340;&#33258;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24102;&#26377;QUAG-attention&#30340;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36798;&#21040;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#35745;&#31639;&#37327;&#26174;&#33879;&#20943;&#23569;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#24403;&#21069;&#30340;VideoQA&#27169;&#22411;&#22312;&#29702;&#35299;&#22810;&#27169;&#24577;&#20449;&#24687;&#26102;&#23384;&#22312;&#19968;&#23450;&#30340;&#24187;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
While VideoQA Transformer models demonstrate competitive performance on standard benchmarks, the reasons behind their success are not fully understood. Do these models jointly capture and leverage the rich multimodal structures and dynamics from video and text? Or are they merely exploiting shortcuts to achieve high scores? Hence, we design $\textit{QUAG}$ (QUadrant AveraGe), a lightweight and non-parametric probe, to critically analyze multimodal representations. QUAG facilitates combined dataset-model study by systematic ablation of model's coupled multimodal understanding during inference. Surprisingly, it demonstrates that the models manage to maintain high performance even under multimodal impairment. We extend QUAG to design "QUAG-attention", a simplistic and less-expressive replacement of self-attention. We find that the models with QUAG-attention achieve similar performance with significantly less mulops without any finetuning. These findings indicate that the current VideoQA b
&lt;/p&gt;</description></item></channel></rss>