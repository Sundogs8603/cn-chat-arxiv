<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#20195;&#30721;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#25512;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21253;&#21547;&#25968;&#23398;&#38382;&#39064;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#24494;&#35843;&#21644;&#25512;&#29702;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#29983;&#25104;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340; MathCoder &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03731</link><description>&lt;p&gt;
MathCoder: &#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#20013; LLMs &#20013;&#26080;&#32541;&#20195;&#30721;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning. (arXiv:2310.03731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#20195;&#30721;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#25512;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21253;&#21547;&#25968;&#23398;&#38382;&#39064;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#24494;&#35843;&#21644;&#25512;&#29702;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#29983;&#25104;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340; MathCoder &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340; GPT-4 &#20195;&#30721;&#35299;&#37322;&#22120;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#65292;&#29983;&#25104;&#20195;&#30721;&#65292;&#25191;&#34892;&#20195;&#30721;&#65292;&#24182;&#26681;&#25454;&#25191;&#34892;&#36755;&#20986;&#32487;&#32493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24494;&#35843;&#24320;&#28304;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#20195;&#30721;&#26469;&#24314;&#27169;&#21644;&#25512;&#23548;&#25968;&#23398;&#26041;&#31243;&#65292;&#24182;&#20174;&#32780;&#22686;&#24378;&#20854;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21253;&#21547;&#25968;&#23398;&#38382;&#39064;&#21450;&#20854;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#39062;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; MathCodeInstruct&#12290;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#37117;&#20132;&#38169;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25191;&#34892;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#25512;&#29702;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#20102; MathCoder &#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;MathCoder &#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#21644;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03718</link><description>&lt;p&gt;
&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning. (arXiv:2310.03718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03718
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#21644;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19987;&#27880;&#20110;&#35757;&#32451;&#22312;&#39044;&#23450;&#20041;&#23433;&#20840;&#32422;&#26463;&#26465;&#20214;&#19979;&#33021;&#22815;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#23433;&#20840;&#32422;&#26463;&#35201;&#27714;&#19988;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#22810;&#21151;&#33021;&#23433;&#20840;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#20026;&#26410;&#24320;&#21457;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#20010;&#20027;&#35201;&#38656;&#27714;&#65306;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Conditioned Constrained Policy Optimization&#65288;CCPO&#65289;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#65288;1&#65289;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#65288;VVE&#65289;&#65292;&#29992;&#20110;&#22312;&#26410;&#35265;&#36807;&#30340;&#38408;&#20540;&#26465;&#20214;&#19979;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#24182;&#19988;&#65288;2&#65289;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#65288;CVI&#65289;&#65292;&#29992;&#20110;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#32534;&#30721;&#20219;&#24847;&#32422;&#26463;&#38408;&#20540;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CCPO&#22312;&#23433;&#20840;&#21644;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#19981;&#21516;&#32422;&#26463;&#30340;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#25351;&#25968;&#25253;&#21578;2023&#24180;&#29256;&#65292;&#21253;&#25324;&#20102;&#26356;&#22810;&#30340;&#21407;&#22987;&#25968;&#25454;&#21644;&#20998;&#26512;&#20869;&#23481;&#65292;&#26088;&#22312;&#20026;&#24191;&#22823;&#35835;&#32773;&#25552;&#20379;&#20934;&#30830;&#12289;&#20840;&#38754;&#30340;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#21644;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.03715</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25351;&#25968;&#25253;&#21578;2023&#24180;&#29256;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Index Report 2023. (arXiv:2310.03715v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#25351;&#25968;&#25253;&#21578;2023&#24180;&#29256;&#65292;&#21253;&#25324;&#20102;&#26356;&#22810;&#30340;&#21407;&#22987;&#25968;&#25454;&#21644;&#20998;&#26512;&#20869;&#23481;&#65292;&#26088;&#22312;&#20026;&#24191;&#22823;&#35835;&#32773;&#25552;&#20379;&#20934;&#30830;&#12289;&#20840;&#38754;&#30340;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#21644;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27426;&#36814;&#26469;&#21040;&#31532;&#20845;&#29256;&#30340;AI&#25351;&#25968;&#25253;&#21578;&#12290;&#20170;&#24180;&#30340;&#25253;&#21578;&#27604;&#20043;&#21069;&#20219;&#20309;&#19968;&#29256;&#37117;&#24341;&#20837;&#20102;&#26356;&#22810;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#21253;&#25324;&#19968;&#31456;&#20851;&#20110;AI&#20844;&#20247;&#24847;&#35265;&#30340;&#26032;&#20869;&#23481;&#65292;&#19968;&#31456;&#26356;&#20840;&#38754;&#30340;&#25216;&#26415;&#24615;&#33021;&#20869;&#23481;&#65292;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21407;&#21019;&#20998;&#26512;&#65292;&#20840;&#29699;AI&#31435;&#27861;&#35760;&#24405;&#30340;&#35814;&#32454;&#36235;&#21183;&#65292;&#20197;&#21450;&#23545;AI&#31995;&#32479;&#29615;&#22659;&#24433;&#21709;&#30340;&#30740;&#31350;&#31561;&#12290;AI&#25351;&#25968;&#25253;&#21578;&#36319;&#36394;&#12289;&#25972;&#29702;&#12289;&#25552;&#28860;&#21644;&#21487;&#35270;&#21270;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20351;&#21629;&#26159;&#20026;&#20915;&#31574;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#12289;&#39640;&#31649;&#12289;&#35760;&#32773;&#21644;&#20844;&#20247;&#25552;&#20379;&#27809;&#26377;&#20559;&#35265;&#12289;&#32463;&#36807;&#20005;&#35880;&#23457;&#26680;&#12289;&#26469;&#28304;&#24191;&#27867;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#20182;&#20204;&#23545;&#22797;&#26434;&#30340;AI&#39046;&#22495;&#26377;&#26356;&#20840;&#38754;&#12289;&#26356;&#31934;&#32454;&#30340;&#29702;&#35299;&#12290;&#35813;&#25253;&#21578;&#26088;&#22312;&#25104;&#20026;&#20851;&#20110;AI&#25968;&#25454;&#21644;&#35265;&#35299;&#30340;&#20840;&#29699;&#26368;&#21487;&#20449;&#19988;&#20855;&#26435;&#23041;&#24615;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Welcome to the sixth edition of the AI Index Report. This year, the report introduces more original data than any previous edition, including a new chapter on AI public opinion, a more thorough technical performance chapter, original analysis about large language and multimodal models, detailed trends in global AI legislation records, a study of the environmental impact of AI systems, and more. The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world's most credible and authoritative source for data and insights about AI.
&lt;/p&gt;</description></item><item><title>DSPy&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#65292;&#23558;LM&#27969;&#27700;&#32447;&#25277;&#35937;&#20026;&#25991;&#26412;&#36716;&#25442;&#22270;&#65292;&#36890;&#36807;&#22768;&#26126;&#24615;&#27169;&#22359;&#35843;&#29992;LM&#23454;&#29616;&#20248;&#21270;&#65292;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#21644;&#25968;&#23398;&#38382;&#39064;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.03714</link><description>&lt;p&gt;
DSPy: &#23558;&#22768;&#26126;&#24615;&#35821;&#35328;&#27169;&#22411;&#35843;&#29992;&#32534;&#35793;&#25104;&#33258;&#25105;&#25913;&#36827;&#30340;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. (arXiv:2310.03714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03714
&lt;/p&gt;
&lt;p&gt;
DSPy&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#65292;&#23558;LM&#27969;&#27700;&#32447;&#25277;&#35937;&#20026;&#25991;&#26412;&#36716;&#25442;&#22270;&#65292;&#36890;&#36807;&#22768;&#26126;&#24615;&#27169;&#22359;&#35843;&#29992;LM&#23454;&#29616;&#20248;&#21270;&#65292;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#21644;&#25968;&#23398;&#38382;&#39064;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ML&#31038;&#21306;&#27491;&#22312;&#24555;&#36895;&#25506;&#32034;&#29992;&#20110;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;(LMs)&#21644;&#23558;&#23427;&#20204;&#22534;&#21472;&#25104;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#27969;&#27700;&#32447;&#30340;&#25216;&#26415;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;LM&#27969;&#27700;&#32447;&#36890;&#24120;&#20351;&#29992;&#30828;&#32534;&#30721;&#30340;"&#25552;&#31034;&#27169;&#26495;"&#26469;&#23454;&#29616;&#65292;&#21363;&#36890;&#36807;&#35797;&#38169;&#21457;&#29616;&#30340;&#20887;&#38271;&#23383;&#31526;&#20018;&#12290;&#20026;&#20102;&#26356;&#31995;&#32479;&#22320;&#24320;&#21457;&#21644;&#20248;&#21270;LM&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DSPy&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#25991;&#26412;&#36716;&#25442;&#22270;&#30340;&#24418;&#24335;&#25277;&#35937;LM&#27969;&#27700;&#32447;&#30340;&#32534;&#31243;&#27169;&#22411;&#65292;&#21363;&#36890;&#36807;&#22768;&#26126;&#24615;&#27169;&#22359;&#35843;&#29992;LM&#30340;&#21629;&#20196;&#24335;&#35745;&#31639;&#22270;&#12290;DSPy&#27169;&#22359;&#26159;&#21442;&#25968;&#21270;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#21019;&#24314;&#21644;&#25910;&#38598;&#31034;&#20363;&#26469;&#23398;&#20064;&#22914;&#20309;&#24212;&#29992;&#25552;&#31034;&#12289;&#24494;&#35843;&#12289;&#22686;&#24378;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#21487;&#20197;&#20248;&#21270;&#20219;&#20309;DSPy&#27969;&#27700;&#32447;&#20197;&#26368;&#22823;&#21270;&#32473;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#26174;&#31034;&#20986;&#31616;&#27905;&#30340;DSPy&#31243;&#24207;&#21487;&#20197;&#34920;&#36798;&#21644;&#20248;&#21270;&#22797;&#26434;&#30340;&#25512;&#29702;&#25968;&#23398;&#38382;&#39064;&#12289;&#30331;&#24405;&#26085;&#24535;&#38382;&#39064;&#31561;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03710</link><description>&lt;p&gt;
&#20195;&#29702;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36890;&#29992;&#30340;&#38646;-shot&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#26356;&#22810;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#29983;&#25104;&#12289;&#20998;&#31867;&#21644;&#25512;&#29702;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;29&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;20&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;Vicuna-13b&#65288;13.3%&#65289;&#65292;Llama-2-70b-chat&#65288;23.2%&#65289;&#21644;GPT-3.5 Turbo&#65288;17.0%&#65289;&#12290;&#19982;&#38646;-shot&#24605;&#32500;&#38142;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#25512;&#29702;&#30340;&#25913;&#36827;&#24456;&#26126;&#26174;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;10.5%&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Llama-2-70b-chat&#30340;&#24615;&#33021;&#36229;&#36807;&#38646;-shot GPT-3.5 Turbo 10.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03708</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#35270;&#21516;&#20161;&#65306;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#19982;&#26222;&#36890;&#26631;&#35760;&#32773;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#21487;&#33021;&#19981;&#36866;&#24212;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26041;&#27861;&#36873;&#25321;&#36890;&#36807;&#25910;&#38598;&#22810;&#32500;&#24230;&#21453;&#39304;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#21019;&#24314;&#19981;&#21516;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#26377;&#30410;&#24615;&#65292;&#26080;&#23475;&#24615;&#65292;&#35802;&#23454;&#24615;&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#22870;&#21169;&#26435;&#37325;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#23558;LM&#35843;&#25972;&#21040;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#35843;&#22312;MORLHF&#20013;&#19981;&#31283;&#23450;&#19988;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#21508;&#31181;&#24120;&#24120;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#25193;&#23637;&#21040;&#22810;&#20010;&#23545;&#40784;&#30446;&#26631;&#12290;&#22522;&#26412;&#19978;&#65292;MODPO&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;LM&#26469;&#20195;&#34920;&#19981;&#21516;&#30340;&#38598;&#20307;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#36827;&#34892;&#32452;&#21512;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;LM&#26681;&#25454;MOD&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
&lt;/p&gt;</description></item><item><title>&#24494;&#35843;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#20250;&#29306;&#29298;&#23433;&#20840;&#24615;&#65292;&#21363;&#20351;&#29992;&#25143;&#27809;&#26377;&#24694;&#24847;&#24847;&#22270;&#12290;&#36890;&#36807;&#25932;&#23545;&#35774;&#35745;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#25968;10&#20010;&#65292;&#20063;&#21487;&#20197;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#36825;&#31181;&#23433;&#20840;&#39118;&#38505;&#23384;&#22312;&#20110;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.03693</link><description>&lt;p&gt;
&#35843;&#25972;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#20250;&#29306;&#29298;&#23433;&#20840;&#24615;&#65292;&#21363;&#20351;&#29992;&#25143;&#27809;&#26377;&#24847;&#22270;&#65281;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!. (arXiv:2310.03693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03693
&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#20250;&#29306;&#29298;&#23433;&#20840;&#24615;&#65292;&#21363;&#20351;&#29992;&#25143;&#27809;&#26377;&#24694;&#24847;&#24847;&#22270;&#12290;&#36890;&#36807;&#25932;&#23545;&#35774;&#35745;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#25968;10&#20010;&#65292;&#20063;&#21487;&#20197;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#36825;&#31181;&#23433;&#20840;&#39118;&#38505;&#23384;&#22312;&#20110;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#29992;&#20363;&#30340;&#20248;&#21270;&#36890;&#24120;&#28041;&#21450;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#26469;&#23450;&#21046;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;Meta&#21457;&#24067;Llama&#27169;&#22411;&#21644;OpenAI&#30340;GPT-3.5 Turbo&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#24494;&#35843;API&#20063;&#40723;&#21169;&#36825;&#31181;&#20570;&#27861;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#33258;&#23450;&#20041;&#24494;&#35843;&#30340;&#23433;&#20840;&#25104;&#26412;&#26159;&#22810;&#23569;&#65311;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#23433;&#20840;&#23545;&#40784;&#22522;&#30784;&#35774;&#26045;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#23475;&#34892;&#20026;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#28085;&#30422;&#24403;&#24494;&#35843;&#29305;&#26435;&#25193;&#23637;&#32473;&#32456;&#31471;&#29992;&#25143;&#26102;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32418;&#38431;&#30740;&#31350;&#21457;&#29616;&#65292;&#21482;&#38656;&#20960;&#20010;&#25932;&#23545;&#35774;&#35745;&#30340;&#35757;&#32451;&#26679;&#26412;&#23601;&#21487;&#20197;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21482;&#20351;&#29992;10&#20010;&#36825;&#26679;&#30340;&#31034;&#20363;&#22312;OpenAI&#30340;API&#20013;&#20197;&#19981;&#21040;0.20&#32654;&#20803;&#30340;&#25104;&#26412;&#23558;GPT-3.5 Turbo&#30340;&#23433;&#20840;&#20445;&#25252;&#35299;&#38500;&#20102;&#65292;&#20351;&#27169;&#22411;&#23545;&#20960;&#20046;&#20219;&#20309;&#26377;&#23475;&#25351;&#20196;&#37117;&#26377;&#21709;&#24212;&#12290;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#21363;&#20351;&#27809;&#26377;&#24694;&#24847;&#24847;&#22270;&#65292;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#31243;&#24207;&#21270;&#29615;&#24418;&#36947;&#36335;&#29983;&#25104;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#20316;&#20026;&#36947;&#36335;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#21644;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03687</link><description>&lt;p&gt;
&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#29992;&#20110;&#21457;&#23637;&#20013;&#22269;&#23478;&#31243;&#24207;&#21270;&#29615;&#24418;&#36947;&#36335;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Generative Modeling for Procedural Roundabout Generation for Developing Countries. (arXiv:2310.03687v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#31243;&#24207;&#21270;&#29615;&#24418;&#36947;&#36335;&#29983;&#25104;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#20316;&#20026;&#36947;&#36335;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#21644;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26377;&#38480;&#30340;&#36164;&#28304;&#21644;&#24555;&#36895;&#30340;&#32463;&#27982;&#22686;&#38271;&#65292;&#20197;&#19968;&#31181;&#26377;&#25928;&#30340;&#25104;&#26412;&#26041;&#24335;&#35774;&#35745;&#20248;&#21270;&#30340;&#20132;&#36890;&#36947;&#36335;&#32593;&#32476;&#65292;&#24182;&#36827;&#34892;&#20132;&#36890;&#20223;&#30495;&#21644;&#39564;&#35777;&#23545;&#20110;&#21457;&#23637;&#20013;&#22269;&#23478;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#24191;&#27867;&#30340;&#25163;&#21160;&#27979;&#35797;&#26114;&#36149;&#19988;&#24120;&#24120;&#26080;&#27861;&#23454;&#29616;&#12290;&#24403;&#21069;&#22522;&#20110;&#35268;&#21017;&#30340;&#36947;&#36335;&#35774;&#35745;&#29983;&#25104;&#22120;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#36825;&#26159;&#35774;&#35745;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#23398;&#20064;&#20174;&#19968;&#20010;&#26410;&#24402;&#19968;&#21270;&#30340;&#22870;&#21169;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#23558;&#20107;&#20214;&#36947;&#36335;&#19982;&#29615;&#24418;&#36947;&#36335;&#30340;&#36830;&#25509;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;GFlowNets&#20316;&#20026;Junction-Art&#36947;&#36335;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#30456;&#20851;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#26377;&#25928;&#24615;&#24471;&#20998;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to limited resources and fast economic growth, designing optimal transportation road networks with traffic simulation and validation in a cost-effective manner is vital for developing countries, where extensive manual testing is expensive and often infeasible. Current rule-based road design generators lack diversity, a key feature for design robustness. Generative Flow Networks (GFlowNets) learn stochastic policies to sample from an unnormalized reward distribution, thus generating high-quality solutions while preserving their diversity. In this work, we formulate the problem of linking incident roads to the circular junction of a roundabout by a Markov decision process, and we leverage GFlowNets as the Junction-Art road generator. We compare our method with related methods and our empirical results show that our method achieves better diversity while preserving a high validity score.
&lt;/p&gt;</description></item><item><title>SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03684</link><description>&lt;p&gt;
SmoothLLM&#65306;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03684
&lt;/p&gt;
&lt;p&gt;
SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21162;&#21147;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#65288;&#22914;GPT&#12289;Llama&#12289;Claude&#21644;PaLM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#65292;&#21363;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#27450;&#39575;&#65292;&#20197;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLM&#19978;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#30340;&#25552;&#31034;&#23545;&#23383;&#31526;&#32423;&#21035;&#30340;&#25913;&#21464;&#24456;&#33030;&#24369;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#39318;&#20808;&#38543;&#26426;&#25200;&#21160;&#32473;&#23450;&#36755;&#20837;&#25552;&#31034;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#28982;&#21518;&#27719;&#24635;&#30456;&#24212;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;SmoothLLM&#23558;&#20247;&#22810;&#28909;&#38376;LLM&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#23545;&#25915;&#20987;&#32531;&#35299;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#20351;&#29992;&#30340;&#26597;&#35810;&#25968;&#37327;&#27604;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;LLM&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
&lt;/p&gt;</description></item><item><title>MapperGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38142;&#25509;&#21644;&#26144;&#23556;&#24037;&#20855;&#65292;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#26144;&#23556;&#20013;&#30340;&#35789;&#27719;&#27169;&#31946;&#38382;&#39064;&#21644;&#25163;&#21160;&#26144;&#23556;&#32454;&#21270;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2310.03666</link><description>&lt;p&gt;
MapperGPT:&#29992;&#20110;&#38142;&#25509;&#21644;&#26144;&#23556;&#23454;&#20307;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MapperGPT: Large Language Models for Linking and Mapping Entities. (arXiv:2310.03666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03666
&lt;/p&gt;
&lt;p&gt;
MapperGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38142;&#25509;&#21644;&#26144;&#23556;&#24037;&#20855;&#65292;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#26144;&#23556;&#20013;&#30340;&#35789;&#27719;&#27169;&#31946;&#38382;&#39064;&#21644;&#25163;&#21160;&#26144;&#23556;&#32454;&#21270;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#65292;&#23545;&#40784;&#26415;&#35821;&#36164;&#28304;&#65288;&#21253;&#25324;&#26412;&#20307;&#12289;&#21463;&#25511;&#35789;&#27719;&#12289;&#20998;&#31867;&#27861;&#21644;&#20540;&#38598;&#65289;&#26159;&#25968;&#25454;&#38598;&#25104;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#23454;&#20307;&#26144;&#23556;&#26159;&#30830;&#23450;&#36825;&#20123;&#36164;&#28304;&#20013;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#30340;&#36807;&#31243;&#65292;&#20363;&#22914;&#22522;&#22240;&#26631;&#35782;&#31526;&#12289;&#30142;&#30149;&#27010;&#24565;&#25110;&#21270;&#23398;&#23454;&#20307;&#26631;&#35782;&#31526;&#12290;&#35768;&#22810;&#24037;&#20855;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#22522;&#20110;&#24120;&#35265;&#32467;&#26500;&#29305;&#24449;&#21644;&#35789;&#27719;&#20449;&#24687;&#65288;&#22914;&#26631;&#31614;&#21644;&#21516;&#20041;&#35789;&#65289;&#26469;&#35745;&#31639;&#36825;&#31181;&#26144;&#23556;&#12290;&#29305;&#21035;&#26159;&#35789;&#27719;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#38750;&#24120;&#39640;&#30340;&#21484;&#22238;&#29575;&#65292;&#20294;&#30001;&#20110;&#35789;&#20041;&#27169;&#31946;&#65292;&#31934;&#24230;&#36739;&#20302;&#12290;&#22240;&#27492;&#65292;&#26144;&#23556;&#24037;&#20316;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#20154;&#24037;&#31574;&#21010;&#36827;&#34892;&#32321;&#29712;&#32780;&#36153;&#26102;&#30340;&#25163;&#21160;&#26144;&#23556;&#32454;&#21270;&#36807;&#31243;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;ChatGPT&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#36890;&#29992;&#33021;&#21147;&#65292;&#21487;&#20197;&#25191;&#34892;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#38382;&#31572;&#21644;&#20449;&#24687;&#25552;&#21462;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MapperGPT&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38142;&#25509;&#21644;&#26144;&#23556;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning terminological resources, including ontologies, controlled vocabularies, taxonomies, and value sets is a critical part of data integration in many domains such as healthcare, chemistry, and biomedical research. Entity mapping is the process of determining correspondences between entities across these resources, such as gene identifiers, disease concepts, or chemical entity identifiers. Many tools have been developed to compute such mappings based on common structural features and lexical information such as labels and synonyms. Lexical approaches in particular often provide very high recall, but low precision, due to lexical ambiguity. As a consequence of this, mapping efforts often resort to a labor intensive manual mapping refinement through a human curator.  Large Language Models (LLMs), such as the ones employed by ChatGPT, have generalizable abilities to perform a wide range of tasks, including question-answering and information extraction. Here we present MapperGPT, an a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#20998;&#31867;&#27861;&#65292;&#29992;&#20110;&#24179;&#34913;&#33258;&#27835;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#26550;&#26500;&#20013;&#30340;&#33258;&#20027;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#30446;&#26631;&#20998;&#35299;&#20026;&#21487;&#31649;&#29702;&#30340;&#20219;&#21153;&#21644;&#21327;&#35843;&#26234;&#33021;&#20307;&#21512;&#20316;&#26469;&#23454;&#29616;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03659</link><description>&lt;p&gt;
&#24179;&#34913;&#33258;&#20027;&#24615;&#21644;&#19968;&#33268;&#24615;&#65306;&#38754;&#21521;&#33258;&#27835;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#26550;&#26500;&#30340;&#22810;&#32500;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures. (arXiv:2310.03659v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#20998;&#31867;&#27861;&#65292;&#29992;&#20110;&#24179;&#34913;&#33258;&#27835;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#26550;&#26500;&#20013;&#30340;&#33258;&#20027;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#30446;&#26631;&#20998;&#35299;&#20026;&#21487;&#31649;&#29702;&#30340;&#20219;&#21153;&#21644;&#21327;&#35843;&#26234;&#33021;&#20307;&#21512;&#20316;&#26469;&#23454;&#29616;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#36171;&#20104;&#20854;&#22797;&#26434;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#30456;&#20114;&#36830;&#25509;&#30340;&#20219;&#21153;&#26102;&#65292;&#38656;&#35201;&#28145;&#20837;&#21644;&#36845;&#20195;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;LLMs&#26174;&#38706;&#20986;&#20854;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#33258;&#27835;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#26159;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#25112;&#30053;&#24615;&#22238;&#24212;&#12290;&#36825;&#26679;&#30340;&#31995;&#32479;&#36890;&#36807;&#23558;&#29992;&#25143;&#25552;&#31034;&#30340;&#30446;&#26631;&#20998;&#35299;&#20026;&#21487;&#31649;&#29702;&#30340;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#32452;&#19987;&#38376;&#30340;&#26234;&#33021;&#20307;&#21327;&#35843;&#25191;&#34892;&#21644;&#32467;&#26524;&#32508;&#21512;&#65292;&#21162;&#21147;&#23454;&#29616;&#33258;&#20027;&#22320;&#35299;&#20915;&#36825;&#20123;&#30446;&#26631;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#37197;&#22791;&#20102;LLM&#39537;&#21160;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#36164;&#28304;&#65288;&#22914;&#24037;&#20855;&#21644;&#25968;&#25454;&#38598;&#65289;&#65292;&#22686;&#24378;&#19982;&#21516;&#20276;&#21512;&#20316;&#30340;&#35748;&#30693;&#21327;&#21516;&#25928;&#24212;&#12290;&#34429;&#28982;&#36825;&#20123;&#26550;&#26500;&#22312;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#19981;&#21516;&#23618;&#27425;&#30340;&#33258;&#20027;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#25214;&#21040;&#27491;&#30830;&#30340;&#24179;&#34913;&#26159;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized the field of artificial intelligence, endowing it with sophisticated language understanding and generation capabilities. However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations. Autonomous LLM-powered multi-agent systems represent a strategic response to these challenges. Such systems strive for autonomously tackling user-prompted goals by decomposing them into manageable tasks and orchestrating their execution and result synthesis through a collective of specialized intelligent agents. Equipped with LLM-powered reasoning capabilities, these agents harness the cognitive synergy of collaborating with their peers, enhanced by leveraging contextual resources such as tools and datasets. While these architectures hold promising potential in amplifying AI capabilities, striking the right balance between different levels of autonomy and alignment
&lt;/p&gt;</description></item><item><title>CLEVRER-Humans&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#21028;&#26029;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#26469;&#35299;&#20915;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#35821;&#35328;&#25551;&#36848;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#21644;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03635</link><description>&lt;p&gt;
CLEVRER-Humans: &#29992;&#20154;&#31867;&#30340;&#26041;&#24335;&#25551;&#36848;&#29289;&#29702;&#21644;&#22240;&#26524;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
CLEVRER-Humans: Describing Physical and Causal Events the Human Way. (arXiv:2310.03635v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03635
&lt;/p&gt;
&lt;p&gt;
CLEVRER-Humans&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#21028;&#26029;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#26469;&#35299;&#20915;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#35821;&#35328;&#25551;&#36848;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#21644;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#33021;&#22815;&#25512;&#29702;&#29289;&#29702;&#20107;&#20214;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#30340;&#26426;&#22120;&#23545;&#20110;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#28789;&#27963;&#20114;&#21160;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#29289;&#29702;&#21644;&#22240;&#26524;&#25512;&#29702;&#22522;&#20934;&#37117;&#20165;&#22522;&#20110;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#31181;&#35774;&#35745;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#19968;&#26159;&#20107;&#20214;&#31867;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#32570;&#20047;&#22810;&#26679;&#24615;&#65307;&#20108;&#26159;&#22522;&#20110;&#25163;&#21160;&#23450;&#20041;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#30340;&#22240;&#26524;&#20851;&#31995;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLEVRER-Humans&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23545;&#29289;&#29702;&#20107;&#20214;&#30340;&#22240;&#26524;&#21028;&#26029;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#65306;&#39318;&#20808;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#20219;&#21153;&#65292;&#20197; eliciting &#35270;&#39057;&#20013;&#20107;&#20214;&#30340;&#26032;&#34920;&#31034;&#26041;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22240;&#26524;&#20107;&#20214;&#22270; (CEGs)&#65307;&#20854;&#27425;&#65292;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models.
&lt;/p&gt;</description></item><item><title>PeaTMOSS&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#30740;&#31350;&#24320;&#28304;&#36719;&#20214;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#36719;&#20214;&#24037;&#31243;&#34892;&#20026;&#21644;&#25361;&#25112;&#30340;&#24179;&#21488;&#65292;&#21253;&#25324;&#22823;&#37327;&#30340;PTMs&#24555;&#29031;&#21644;&#20351;&#29992;PTMs&#30340;&#24320;&#28304;&#36719;&#20214;&#23384;&#20648;&#24211;&#12290;</title><link>http://arxiv.org/abs/2310.03620</link><description>&lt;p&gt;
PeaTMOSS: &#22312;&#24320;&#28304;&#36719;&#20214;&#20013;&#25366;&#25496;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PeaTMOSS: Mining Pre-Trained Models in Open-Source Software. (arXiv:2310.03620v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03620
&lt;/p&gt;
&lt;p&gt;
PeaTMOSS&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#30740;&#31350;&#24320;&#28304;&#36719;&#20214;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#36719;&#20214;&#24037;&#31243;&#34892;&#20026;&#21644;&#25361;&#25112;&#30340;&#24179;&#21488;&#65292;&#21253;&#25324;&#22823;&#37327;&#30340;PTMs&#24555;&#29031;&#21644;&#20351;&#29992;PTMs&#30340;&#24320;&#28304;&#36719;&#20214;&#23384;&#20648;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#21644;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#36719;&#20214;&#24037;&#31243;&#24072;&#24050;&#32463;&#24320;&#22987;&#37325;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;(PTMs)&#24182;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;PTMs&#30340;&#20351;&#29992;&#24050;&#32463;&#24191;&#27867;&#65292;&#20294;&#25105;&#20204;&#23545;&#30456;&#24212;&#30340;&#36719;&#20214;&#24037;&#31243;&#34892;&#20026;&#21644;&#25361;&#25112;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;PTMs&#36827;&#34892;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PeaTMOSS&#25968;&#25454;&#38598;&#65306;&#24320;&#28304;&#36719;&#20214;&#20013;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;PeaTMOSS&#25968;&#25454;&#38598;&#21253;&#25324;&#19977;&#20010;&#37096;&#20998;&#65306;(1) 281,638&#20010;PTMs&#30340;&#24555;&#29031;&#65292;(2) &#20351;&#29992;PTMs&#30340;27,270&#20010;&#24320;&#28304;&#36719;&#20214;&#23384;&#20648;&#24211;&#65292;&#20197;&#21450;(3) PTMs&#21644;&#20351;&#29992;&#23427;&#20204;&#30340;&#39033;&#30446;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#21628;&#21505;PeaTMOSS&#30719;&#24037;&#20204;&#21457;&#29616;&#22260;&#32469;PTMs&#30340;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#12290;&#28436;&#31034;&#21644;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#38142;&#25509;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/PurdueDualityLab/PeaTMOSS-Demos&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing and training deep learning models is expensive, so software engineers have begun to reuse pre-trained deep learning models (PTMs) and fine-tune them for downstream tasks. Despite the wide-spread use of PTMs, we know little about the corresponding software engineering behaviors and challenges.  To enable the study of software engineering with PTMs, we present the PeaTMOSS dataset: Pre-Trained Models in Open-Source Software. PeaTMOSS has three parts: a snapshot of (1) 281,638 PTMs, (2) 27,270 open-source software repositories that use PTMs, and (3) a mapping between PTMs and the projects that use them. We challenge PeaTMOSS miners to discover software engineering practices around PTMs. A demo and link to the full dataset are available at: https://github.com/PurdueDualityLab/PeaTMOSS-Demos.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#31867;&#38750;&#20984;&#26368;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FL&#31639;&#27861;&#65288;FedSGDA+&#21644;FedSGDA-M&#65289;&#65292;&#24182;&#22312;&#26368;&#24120;&#35265;&#30340;&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#20013;&#38477;&#20302;&#20102;&#22797;&#26434;&#24230;&#12290;&#38024;&#23545;&#38750;&#20984;&#20985;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;FedSGDA+&#31639;&#27861;&#23558;&#36890;&#20449;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;O(&#949;^{-6})&#12290;&#22312;&#38750;&#20984;&#24378;&#20985;&#21644;&#38750;&#20984;PL&#26368;&#23567;&#26497;&#22823;&#35774;&#32622;&#19979;&#65292;&#35777;&#26126;&#20102;FedSGDA-M&#20855;&#26377;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03613</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#19968;&#31867;&#38750;&#20984;&#26368;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving a Class of Non-Convex Minimax Optimization in Federated Learning. (arXiv:2310.03613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#31867;&#38750;&#20984;&#26368;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FL&#31639;&#27861;&#65288;FedSGDA+&#21644;FedSGDA-M&#65289;&#65292;&#24182;&#22312;&#26368;&#24120;&#35265;&#30340;&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#20013;&#38477;&#20302;&#20102;&#22797;&#26434;&#24230;&#12290;&#38024;&#23545;&#38750;&#20984;&#20985;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;FedSGDA+&#31639;&#27861;&#23558;&#36890;&#20449;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;O(&#949;^{-6})&#12290;&#22312;&#38750;&#20984;&#24378;&#20985;&#21644;&#38750;&#20984;PL&#26368;&#23567;&#26497;&#22823;&#35774;&#32622;&#19979;&#65292;&#35777;&#26126;&#20102;FedSGDA-M&#20855;&#26377;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21508;&#31181;&#22330;&#26223;&#65292;&#21253;&#25324;&#23545;&#25239;&#35757;&#32451;&#12289;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#35780;&#20272;&#20197;&#21450; AUROC &#26368;&#22823;&#21270;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36328;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#25361;&#25112;&#65292;&#22312;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#22312;&#38598;&#20013;&#24335;&#65288;&#21363;&#21333;&#26426;&#65289;&#29615;&#22659;&#19979;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20294;&#22312; FL &#19979;&#30340;&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#31639;&#27861;&#20173;&#28982;&#19981;&#22815;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#32852;&#37030;&#38750;&#20984;&#26368;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; FL &#31639;&#27861;&#65288;FedSGDA+ &#21644; FedSGDA-M&#65289;&#24182;&#38477;&#20302;&#20102;&#26368;&#24120;&#35265;&#26368;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#29616;&#26377;&#22797;&#26434;&#24230;&#32467;&#26524;&#12290;&#23545;&#20110;&#38750;&#20984;&#20985;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FedSGDA+ &#24182;&#23558;&#36890;&#20449;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040; $O(\varepsilon^{-6})$&#12290;&#22312;&#38750;&#20984;&#24378;&#20985;&#21644;&#38750;&#20984; PL &#26368;&#23567;&#26497;&#22823;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; FedSGDA-M &#20855;&#26377;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The minimax problems arise throughout machine learning applications, ranging from adversarial training and policy evaluation in reinforcement learning to AUROC maximization. To address the large-scale data challenges across multiple clients with communication-efficient distributed training, federated learning (FL) is gaining popularity. Many optimization algorithms for minimax problems have been developed in the centralized setting (\emph{i.e.} single-machine). Nonetheless, the algorithm for minimax problems under FL is still underexplored. In this paper, we study a class of federated nonconvex minimax optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and reduce existing complexity results for the most common minimax problems. For nonconvex-concave problems, we propose FedSGDA+ and reduce the communication complexity to $O(\varepsilon^{-6})$. Under nonconvex-strongly-concave and nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known sample comp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.03605</link><description>&lt;p&gt;
FASER: &#36890;&#36807;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35782;&#21035;&#36328;&#26550;&#26500;&#36719;&#20214;&#20013;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#23545;&#20110;&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#12289;&#20445;&#25252;&#36719;&#20214;&#20379;&#24212;&#38142;&#25110;&#36827;&#34892;&#28431;&#27934;&#30740;&#31350;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#36328;&#26550;&#26500;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#24050;&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#25506;&#32034;&#65292;&#24182;&#20351;&#29992;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#26469;&#28304;&#26469;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;&#36890;&#24120;&#20351;&#29992;&#30340;&#25968;&#25454;&#26469;&#28304;&#21253;&#25324;&#20174;&#20108;&#36827;&#21046;&#25991;&#20214;&#20013;&#25552;&#21462;&#30340;&#24120;&#35265;&#32467;&#26500;&#65292;&#22914;&#20989;&#25968;&#25511;&#21046;&#27969;&#22270;&#25110;&#20108;&#36827;&#21046;&#32423;&#35843;&#29992;&#22270;&#65292;&#21453;&#27719;&#32534;&#36807;&#31243;&#30340;&#36755;&#20986;&#25110;&#21160;&#24577;&#20998;&#26512;&#26041;&#27861;&#30340;&#36755;&#20986;&#12290;&#20854;&#20013;&#19968;&#31181;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#25968;&#25454;&#26469;&#28304;&#26159;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#12290;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#20855;&#26377;&#20004;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65306;&#23427;&#20204;&#30340;&#36328;&#26550;&#26500;&#24615;&#36136;&#20197;&#21450;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#20197;&#25903;&#25345;&#19979;&#28216;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#20989;&#25968;&#23383;&#31526;&#20018;&#32534;&#30721;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#25991;&#26723;&#36716;&#25442;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transforme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#22320;&#23548;&#33322;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22833;&#25928;&#24314;&#27169;&#20026;&#38556;&#30861;&#29289;&#21644;&#22353;&#27934;&#65292;&#20174;&#21463;&#25439;&#30340;&#24863;&#30693;&#20013;&#37325;&#24314;&#29615;&#22659;&#20449;&#24687;&#24182;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21453;&#24212;&#65292;&#23454;&#29616;&#20102;&#22312;&#21463;&#25439;&#24863;&#30693;&#19979;&#30340;&#21487;&#38752;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2310.03581</link><description>&lt;p&gt;
&#20855;&#26377;&#24377;&#24615;&#27493;&#24577;&#30340;&#26412;&#22320;&#23548;&#33322;&#65306;&#23398;&#20064;&#22312;&#21463;&#25439;&#24863;&#30693;&#19979;&#31471;&#21040;&#31471;&#31359;&#36234;
&lt;/p&gt;
&lt;p&gt;
Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End. (arXiv:2310.03581v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#22320;&#23548;&#33322;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22833;&#25928;&#24314;&#27169;&#20026;&#38556;&#30861;&#29289;&#21644;&#22353;&#27934;&#65292;&#20174;&#21463;&#25439;&#30340;&#24863;&#30693;&#20013;&#37325;&#24314;&#29615;&#22659;&#20449;&#24687;&#24182;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21453;&#24212;&#65292;&#23454;&#29616;&#20102;&#22312;&#21463;&#25439;&#24863;&#30693;&#19979;&#30340;&#21487;&#38752;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#24517;&#39035;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#21487;&#38752;&#22320;&#23548;&#33322;&#65292;&#21363;&#20351;&#22312;&#21463;&#25439;&#30340;&#22806;&#24863;&#30693;&#25110;&#24863;&#30693;&#22833;&#25928;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#25991;&#23558;&#24863;&#30693;&#22833;&#25928;&#24314;&#27169;&#20026;&#30475;&#19981;&#35265;&#30340;&#38556;&#30861;&#29289;&#21644;&#22353;&#27934;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#22320;&#23548;&#33322;&#31574;&#30053;&#26469;&#24341;&#23548;&#25105;&#20204;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#12290;&#19982;&#20381;&#36182;&#21551;&#21457;&#24335;&#21644;&#24322;&#24120;&#26816;&#27979;&#26469;&#26356;&#26032;&#23548;&#33322;&#20449;&#24687;&#30340;&#20808;&#21069;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#25105;&#20204;&#30340;&#23548;&#33322;&#31574;&#30053;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20174;&#25439;&#22351;&#30340;&#24863;&#30693;&#20013;&#37325;&#24314;&#29615;&#22659;&#20449;&#24687;&#24182;&#23545;&#24863;&#30693;&#22833;&#25928;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21453;&#24212;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#26412;&#20307;&#24863;&#30693;&#21644;&#22806;&#24863;&#30693;&#37117;&#32435;&#20837;&#25105;&#20204;&#30340;&#31574;&#30053;&#36755;&#20837;&#20013;&#65292;&#20174;&#32780;&#20351;&#31574;&#30053;&#33021;&#22815;&#24863;&#30693;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#30340;&#30896;&#25758;&#21644;&#22353;&#27934;&#65292;&#24182;&#24341;&#21457;&#30456;&#24212;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots must navigate reliably in unknown environments even under compromised exteroceptive perception, or perception failures. Such failures often occur when harsh environments lead to degraded sensing, or when the perception algorithm misinterprets the scene due to limited generalization. In this paper, we model perception failures as invisible obstacles and pits, and train a reinforcement learning (RL) based local navigation policy to guide our legged robot. Unlike previous works relying on heuristics and anomaly detection to update navigational information, we train our navigation policy to reconstruct the environment information in the latent space from corrupted perception and react to perception failures end-to-end. To this end, we incorporate both proprioception and exteroception into our policy inputs, thereby enabling the policy to sense collisions on different body parts and pits, prompting corresponding reactions. We validate our approach in simulation and on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;Swift-DynGFN&#65292;&#23427;&#25552;&#39640;&#20102;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#22240;&#26524;&#32467;&#26500;&#30340;&#23398;&#20064;&#33021;&#21147;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#22240;&#30340;&#29420;&#31435;&#24615;&#26469;&#22686;&#24378;&#24182;&#34892;&#24615;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;GRNs&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#21644;&#22788;&#29702;&#22823;&#35268;&#27169;&#31995;&#32479;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.03579</link><description>&lt;p&gt;
GFlowNet&#20013;&#30340;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#22240;&#26524;&#25512;&#26029;&#65306;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Inference in Gene Regulatory Networks with GFlowNet: Towards Scalability in Large Systems. (arXiv:2310.03579v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;Swift-DynGFN&#65292;&#23427;&#25552;&#39640;&#20102;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#22240;&#26524;&#32467;&#26500;&#30340;&#23398;&#20064;&#33021;&#21147;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#22240;&#30340;&#29420;&#31435;&#24615;&#26469;&#22686;&#24378;&#24182;&#34892;&#24615;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;GRNs&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#21644;&#22788;&#29702;&#22823;&#35268;&#27169;&#31995;&#32479;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRNs&#65289;&#20869;&#30340;&#22240;&#26524;&#20851;&#31995;&#23545;&#20110;&#25581;&#31034;&#32454;&#32990;&#36807;&#31243;&#20013;&#30340;&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;GRNs&#20013;&#30340;&#22240;&#26524;&#21457;&#29616;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26377;&#22810;&#31181;&#65292;&#21253;&#25324;&#23384;&#22312;&#24490;&#29615;&#21453;&#39304;&#29615;&#36335;&#21644;&#20135;&#29983;&#22810;&#26679;&#24615;&#21487;&#33021;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20197;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#35201;&#20040;&#24573;&#30053;&#20102;&#24490;&#29615;&#21160;&#21147;&#23398;&#65288;&#20551;&#35774;&#38750;&#24490;&#29615;&#32467;&#26500;&#65289;&#65292;&#35201;&#20040;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#26377;&#22256;&#38590;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Swift-DynGFN&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23427;&#22686;&#24378;&#20102;GRNs&#20013;&#22240;&#26524;&#32467;&#26500;&#30340;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Swift-DynGFN&#21033;&#29992;&#22522;&#22240;&#29420;&#31435;&#24615;&#26469;&#25552;&#39640;&#24182;&#34892;&#24615;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#30495;&#23454;&#30340;&#21333;&#32454;&#32990;RNA&#36895;&#24230;&#21644;&#21512;&#25104;GRN&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;GRNs&#20013;&#22240;&#26524;&#32467;&#26500;&#21644;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding causal relationships within Gene Regulatory Networks (GRNs) is essential for unraveling the gene interactions in cellular processes. However, causal discovery in GRNs is a challenging problem for multiple reasons including the existence of cyclic feedback loops and uncertainty that yields diverse possible causal structures. Previous works in this area either ignore cyclic dynamics (assume acyclic structure) or struggle with scalability. We introduce Swift-DynGFN as a novel framework that enhances causal structure learning in GRNs while addressing scalability concerns. Specifically, Swift-DynGFN exploits gene-wise independence to boost parallelization and to lower computational cost. Experiments on real single-cell RNA velocity and synthetic GRN datasets showcase the advancement in learning causal structure in GRNs and scalability in larger systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#36755;&#20837;&#22122;&#22768;&#30340;&#40065;&#26834;&#21644;&#21487;&#25512;&#24191;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#22024;&#26434;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#40065;&#26834;&#24615;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03518</link><description>&lt;p&gt;
&#26397;&#30528;&#40065;&#26834;&#21644;&#21487;&#25512;&#24191;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#22024;&#26434;&#27133;&#22635;&#20805;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations. (arXiv:2310.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#36755;&#20837;&#22122;&#22768;&#30340;&#40065;&#26834;&#21644;&#21487;&#25512;&#24191;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#22024;&#26434;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#40065;&#26834;&#24615;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#23545;&#35805;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#35805;&#35821;&#20013;&#23384;&#22312;&#26410;&#30693;&#30340;&#36755;&#20837;&#22122;&#22768;&#65292;&#29616;&#26377;&#30340;&#30417;&#30563;&#24335;&#27133;&#22635;&#20805;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#36739;&#24046;&#12290;&#34429;&#28982;&#26377;&#19968;&#20123;&#20851;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#21482;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#31181;&#38480;&#21046;&#24615;&#23548;&#33268;&#20102;&#22122;&#22768;&#40065;&#26834;&#26041;&#27861;&#30740;&#31350;&#30340;&#25512;&#24191;&#22256;&#38590;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Noise-SF&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20116;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#26631;&#27880;&#22122;&#22768;&#65292;&#24182;&#19988;&#25152;&#26377;&#36825;&#20123;&#22122;&#22768;&#37117;&#30830;&#23454;&#23384;&#22312;&#20110;&#30495;&#23454;&#30340;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#35757;&#32451;&#26041;&#27861;&#20013;&#12290;&#36890;&#36807;&#23545;Noise-SF&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#35777;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#32447;&#27169;&#22411;&#22312;&#40065;&#26834;&#24615;&#35780;&#20272;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#32780;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#23454;&#35777;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#21069;&#30651;&#24615;&#30340;&#24314;&#35758;&#26469;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real dialogue scenarios, as there are unknown input noises in the utterances, existing supervised slot filling models often perform poorly in practical applications. Even though there are some studies on noise-robust models, these works are only evaluated on rule-based synthetic datasets, which is limiting, making it difficult to promote the research of noise-robust methods. In this paper, we introduce a noise robustness evaluation dataset named Noise-SF for slot filling task. The proposed dataset contains five types of human-annotated noise, and all those noises are exactly existed in real extensive robust-training methods of slot filling into the proposed framework. By conducting exhaustive empirical evaluation experiments on Noise-SF, we find that baseline models have poor performance in robustness evaluation, and the proposed framework can effectively improve the robustness of models. Based on the empirical experimental results, we make some forward-looking suggestions to fuel t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27979;&#37327;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#21457;&#29616;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#33021;&#26356;&#22909;&#22320;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.03494</link><description>&lt;p&gt;
&#22914;&#20309;&#27700;&#24179;&#37319;&#26679;&#36807;&#31243;&#24433;&#21709;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03494
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27979;&#37327;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#21457;&#29616;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#33021;&#26356;&#22909;&#22320;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38459;&#27490;&#24191;&#27867;&#37319;&#29992;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#20195;&#29702;&#30340;&#20851;&#38190;&#23616;&#38480;&#26159;&#23427;&#20204;&#26377;&#38480;&#30340;&#36866;&#24212;&#26032;&#29615;&#22659;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#35757;&#32451;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#25110;&#23618;&#32423;&#30340;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;RL&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#65288;ZSG&#65289;&#33021;&#21147;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#22833;&#25928;&#27169;&#24335;&#65306;&#36807;&#25311;&#21512;&#21644;&#36807;&#24230;&#27867;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;&#21457;&#29616;MI&#19982;&#23454;&#20363;&#30340;&#36807;&#25311;&#21512;&#30456;&#20851;&#24615;&#24456;&#24378;&#12290;&#19982;&#22343;&#21248;&#37319;&#26679;&#30456;&#27604;&#65292;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#26356;&#33021;&#26377;&#25928;&#22320;&#20445;&#25345;&#36739;&#20302;&#30340;MI&#65292;&#36825;&#20026;&#36825;&#31867;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;U&#65289;
&lt;/p&gt;
&lt;p&gt;
A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (U
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36716;&#25442;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#35821;&#35328;&#26631;&#35760;&#26144;&#23556;&#21040;&#35821;&#20041;&#30456;&#20284;&#30340;&#28304;&#35821;&#35328;&#26631;&#35760;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#35757;&#32451;&#21333;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#24182;&#22312;&#33655;&#20848;&#35821;&#21644;&#24343;&#37324;&#26031;&#20848;&#35821;&#31561;&#22810;&#31181;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03477</link><description>&lt;p&gt;
Tik-to-Tok:&#19968;&#27425;&#32763;&#35793;&#19968;&#20010;&#26631;&#35760;&#65306;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#35821;&#35328;&#36866;&#24212;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding Initialization Strategy for Efficient Language Adaptation. (arXiv:2310.03477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36716;&#25442;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#35821;&#35328;&#26631;&#35760;&#26144;&#23556;&#21040;&#35821;&#20041;&#30456;&#20284;&#30340;&#28304;&#35821;&#35328;&#26631;&#35760;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#35757;&#32451;&#21333;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#24182;&#22312;&#33655;&#20848;&#35821;&#21644;&#24343;&#37324;&#26031;&#20848;&#35821;&#31561;&#22810;&#31181;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36716;&#25442;&#31574;&#30053;&#65292;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#35757;&#32451;&#21333;&#35821;&#35328;&#27169;&#22411;&#26102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#21644;&#36890;&#24120;&#19981;&#36275;&#22815;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#26469;&#28304;&#35821;&#35328;&#26631;&#35760;&#22120;&#21644;&#30446;&#26631;&#35821;&#35328;&#26631;&#35760;&#22120;&#20043;&#38388;&#24314;&#31435;&#19968;&#20010;&#21253;&#21547;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#21333;&#35789;&#32763;&#35793;&#23383;&#20856;&#30340;&#27867;&#21270;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#30446;&#26631;&#26631;&#35760;&#26144;&#23556;&#21040;&#35821;&#20041;&#30456;&#20284;&#30340;&#28304;&#35821;&#35328;&#26631;&#35760;&#12290;&#36825;&#31181;&#19968;&#23545;&#22810;&#30340;&#26631;&#35760;&#26144;&#23556;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;&#30446;&#26631;&#35821;&#35328;&#30340;&#23884;&#20837;&#34920;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#23545;&#39640;&#36164;&#28304;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#20013;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#20998;&#21035;&#26159;&#33655;&#20848;&#35821;&#21644;&#24343;&#37324;&#26031;&#20848;&#35821;&#12290;&#36825;&#20123;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#36825;&#20123;&#35821;&#35328;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26368;&#26032;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#21644;&#26102;&#38388;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#27169;&#22411;&#36716;&#25442;&#31574;&#30053;&#22823;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training monolingual language models for low and mid-resource languages is made challenging by limited and often inadequate pretraining data. In this study, we propose a novel model conversion strategy to address this issue, adapting high-resources monolingual language models to a new target language. By generalizing over a word translation dictionary encompassing both the source and target languages, we map tokens from the target tokenizer to semantically similar tokens from the source language tokenizer. This one-to-many token mapping improves tremendously the initialization of the embedding table for the target language. We conduct experiments to convert high-resource models to midand low-resource languages, namely Dutch and Frisian. These converted models achieve a new state-of-the-art performance on these languages across all sorts of downstream tasks. By reducing significantly the amount of data and time required for training state-of-the-art models, our novel model conversion 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#21453;&#20107;&#23454;&#30340;&#23450;&#37327;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#36890;&#36807;&#21512;&#25104;&#21453;&#20107;&#23454;&#26631;&#35760;&#30340;&#32467;&#26500;&#24615;MRIs&#65292;&#24182;&#36716;&#21270;&#20026;&#28784;&#36136;&#23494;&#24230;&#22270;&#26469;&#27979;&#37327;&#20307;&#31215;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#20419;&#36827;&#20102;&#23450;&#37327;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.03457</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#21453;&#20107;&#23454;&#30340;&#23450;&#37327;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Quantitatively Interpretable Model for Alzheimer's Disease Prediction Using Deep Counterfactuals. (arXiv:2310.03457v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#21453;&#20107;&#23454;&#30340;&#23450;&#37327;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#36890;&#36807;&#21512;&#25104;&#21453;&#20107;&#23454;&#26631;&#35760;&#30340;&#32467;&#26500;&#24615;MRIs&#65292;&#24182;&#36716;&#21270;&#20026;&#28784;&#36136;&#23494;&#24230;&#22270;&#26469;&#27979;&#37327;&#20307;&#31215;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#20419;&#36827;&#20102;&#23450;&#37327;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#23545;&#20110;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#24050;&#32463;&#22312;&#30142;&#30149;&#36827;&#23637;&#30340;&#21450;&#26102;&#24178;&#39044;&#26041;&#38754;&#25552;&#20379;&#20102;&#24110;&#21161;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#27880;&#24847;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#35299;&#37322;&#20182;&#20204;&#30340;DL&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#26126;&#30830;&#30340;&#20915;&#31574;&#12290;&#26368;&#36817;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#22312;&#21307;&#23398;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#32454;&#33268;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#22270;&#12290;&#28982;&#32780;&#65292;&#20165;&#20973;&#35270;&#35273;&#26816;&#26597;&#21644;&#35270;&#35273;&#35299;&#37322;&#22270;&#26159;&#19981;&#22815;&#30340;&#65292;&#38500;&#38750;&#25105;&#20204;&#36890;&#36807;&#23450;&#37327;&#29305;&#24449;&#30452;&#35266;&#22320;&#35777;&#26126;&#23427;&#20204;&#22312;&#21307;&#23398;&#25110;&#31070;&#32463;&#31185;&#23398;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21512;&#25104;&#20102;&#21453;&#20107;&#23454;&#26631;&#35760;&#30340;&#32467;&#26500;&#24615;MRIs&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#28784;&#36136;&#23494;&#24230;&#22270;&#65292;&#20197;&#27979;&#37327;&#20854;&#22312;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#20307;&#31215;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#32447;&#24615;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#26500;&#24314;&#30340;ROI&#30340;&#25928;&#26524;&#65292;&#20419;&#36827;&#23450;&#37327;&#35299;&#37322;&#65292;&#24182;&#23454;&#29616;&#19982;DL&#26041;&#27861;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) for predicting Alzheimer's disease (AD) has provided timely intervention in disease progression yet still demands attentive interpretability to explain how their DL models make definitive decisions. Recently, counterfactual reasoning has gained increasing attention in medical research because of its ability to provide a refined visual explanatory map. However, such visual explanatory maps based on visual inspection alone are insufficient unless we intuitively demonstrate their medical or neuroscientific validity via quantitative features. In this study, we synthesize the counterfactual-labeled structural MRIs using our proposed framework and transform it into a gray matter density map to measure its volumetric changes over the parcellated region of interest (ROI). We also devised a lightweight linear classifier to boost the effectiveness of constructed ROIs, promoted quantitative interpretation, and achieved comparable predictive performance to DL methods. Throughout
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#30340;&#26080;&#22870;&#21169;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#38382;&#39064;&#30340;&#24418;&#24335;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#30340;GFlowNet&#65288;OC-GFN&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.03419</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Pre-Training and Fine-Tuning Generative Flow Networks. (arXiv:2310.03419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03419
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#30340;&#26080;&#22870;&#21169;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#38382;&#39064;&#30340;&#24418;&#24335;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#30340;GFlowNet&#65288;OC-GFN&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#23398;&#20064;&#20174;&#32473;&#23450;&#30340;&#38750;&#26631;&#20934;&#21270;&#22870;&#21169;&#20998;&#24067;&#20013;&#39034;&#24207;&#29983;&#25104;&#22797;&#21512;&#23545;&#35937;&#30340;&#38543;&#26426;&#31574;&#30053;&#30340;&#25674;&#38144;&#37319;&#26679;&#22120;&#12290;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#22870;&#21169;&#23545;&#35937;&#65292;&#22312;&#31185;&#23398;&#21457;&#29616;&#20219;&#21153;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#36890;&#24120;&#26159;&#26681;&#25454;&#32473;&#23450;&#30340;&#22806;&#22312;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20851;&#20110;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#21147;&#37327;&#20197;&#21450;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;GFlowNets&#20197;&#23454;&#29616;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#25928;&#33258;&#36866;&#24212;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#21463;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;GFlowNets&#26080;&#22870;&#21169;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#19968;&#20010;&#33258;&#30417;&#30563;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;GFlowNet&#65288;OC-GFN&#65289;&#65292;&#23427;&#23398;&#20064;&#25506;&#32034;&#20505;&#36873;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;OC-GFN&#23398;&#20064;&#36798;&#21040;&#20219;&#20309;&#30446;&#26631;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution. They can generate diverse sets of high-reward objects, which is an important consideration in scientific discovery tasks. However, as they are typically trained from a given extrinsic reward function, it remains an important open challenge about how to leverage the power of pre-training and train GFlowNets in an unsupervised fashion for efficient adaptation to downstream tasks. Inspired by recent successes of unsupervised pre-training in various domains, we introduce a novel approach for reward-free pre-training of GFlowNets. By framing the training as a self-supervised problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to explore the candidate space. Specifically, OC-GFN learns to reach any targeted outcomes, akin to goal-conditioned policies in reinforcement learning. We show that
&lt;/p&gt;</description></item><item><title>GRAPES&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#22312;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.03399</link><description>&lt;p&gt;
GRAPES: &#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks. (arXiv:2310.03399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03399
&lt;/p&gt;
&lt;p&gt;
GRAPES&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#22312;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#20197;&#19981;&#21516;&#26041;&#24335;&#32858;&#21512;&#21608;&#22260;&#20449;&#24687;&#26469;&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#36825;&#20123;&#32593;&#32476;&#30340;&#21152;&#28145;&#65292;&#30001;&#20110;&#37051;&#22495;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#24863;&#21463;&#37326;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#28040;&#32791;&#12290;&#22270;&#37319;&#26679;&#36890;&#36807;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;GNNs&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;GNNs&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#12290;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#19987;&#27880;&#20110;&#22266;&#23450;&#30340;&#37319;&#26679;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#32467;&#26500;&#25110;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GRAPES&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#35782;&#21035;&#29992;&#20110;&#35757;&#32451;GNN&#20998;&#31867;&#22120;&#30340;&#19968;&#32452;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12290;GRAPES&#20351;&#29992;GFlowNet&#26469;&#23398;&#20064;&#32473;&#23450;&#20998;&#31867;&#30446;&#26631;&#30340;&#33410;&#28857;&#37319;&#26679;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#22270;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;GRAPES&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;GRAPES&#21363;&#20351;&#22312;&#37319;&#26679;&#27604;&#20363;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#20173;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) learn the representation of nodes in a graph by aggregating the neighborhood information in various ways. As these networks grow in depth, their receptive field grows exponentially due to the increase in neighborhood sizes, resulting in high memory costs. Graph sampling solves memory issues in GNNs by sampling a small ratio of the nodes in the graph. This way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed sampling heuristics, which may not generalize to different structures or tasks. We introduce GRAPES, an adaptive graph sampling method that learns to identify sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet to learn node sampling probabilities given the classification objectives. We evaluate GRAPES across several small- and large-scale graph benchmarks and demonstrate its effectiveness in accuracy and scalability. In contrast to existing sampling methods, GRAPES maintains high accuracy even with 
&lt;/p&gt;</description></item><item><title>&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65288;HAII&#65289;&#21313;&#20998;&#37325;&#35201;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;HAII&#30340;&#30740;&#31350;&#38646;&#25955;&#19988;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#20010;&#19968;&#33268;&#20351;&#29992;&#30340;&#26415;&#35821;&#25551;&#36848;HAII&#65292;&#24182;&#19988;&#20116;&#20010;&#22240;&#32032;&#24433;&#21709;HAII&#65292;&#21363;&#29992;&#25143;&#29305;&#24449;&#21644;&#32972;&#26223;&#12289;AI&#30028;&#38754;&#21644;&#21151;&#33021;&#12289;&#29992;&#25143;&#20449;&#20219;&#12289;&#30417;&#25511;&#21644;&#21453;&#39304;&#65292;&#20197;&#21450;&#24037;&#20316;&#19978;&#30340;&#22242;&#38431;&#32467;&#26500;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25913;&#36827;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30340;HAII&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.03392</link><description>&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review. (arXiv:2310.03392v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03392
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65288;HAII&#65289;&#21313;&#20998;&#37325;&#35201;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;HAII&#30340;&#30740;&#31350;&#38646;&#25955;&#19988;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#20010;&#19968;&#33268;&#20351;&#29992;&#30340;&#26415;&#35821;&#25551;&#36848;HAII&#65292;&#24182;&#19988;&#20116;&#20010;&#22240;&#32032;&#24433;&#21709;HAII&#65292;&#21363;&#29992;&#25143;&#29305;&#24449;&#21644;&#32972;&#26223;&#12289;AI&#30028;&#38754;&#21644;&#21151;&#33021;&#12289;&#29992;&#25143;&#20449;&#20219;&#12289;&#30417;&#25511;&#21644;&#21453;&#39304;&#65292;&#20197;&#21450;&#24037;&#20316;&#19978;&#30340;&#22242;&#38431;&#32467;&#26500;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25913;&#36827;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30340;HAII&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65288;HAII&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19981;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21644;&#33268;&#21629;&#30340;&#21518;&#26524;&#12290;&#23613;&#31649;&#22914;&#27492;&#32039;&#36843;&#65292;&#20851;&#20110;HAII&#30340;&#30740;&#31350;&#24456;&#23569;&#19988;&#38646;&#25955;&#65292;&#19988;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#25913;&#36827;&#30740;&#31350;&#26368;&#20339;&#23454;&#36341;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#35843;&#26597;&#20998;&#20026;&#20197;&#19979;&#30740;&#31350;&#39046;&#22495;&#65306;&#65288;1&#65289;&#29992;&#20110;&#25551;&#36848;HAII&#30340;&#26415;&#35821;&#65292;&#65288;2&#65289;AI-enabled&#31995;&#32479;&#30340;&#20027;&#35201;&#35282;&#33394;&#65292;&#65288;3&#65289;&#24433;&#21709;HAII&#30340;&#22240;&#32032;&#65292;&#20197;&#21450;&#65288;4&#65289;&#22914;&#20309;&#34913;&#37327;HAII&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#36825;&#20123;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;AI-enabled&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#25104;&#29087;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25991;&#29486;&#20013;&#27809;&#26377;&#19968;&#20010;&#26415;&#35821;&#34987;&#19968;&#33268;&#20351;&#29992;&#26469;&#25551;&#36848;HAII&#65292;&#32780;&#19968;&#20123;&#26415;&#35821;&#20855;&#26377;&#22810;&#20010;&#21547;&#20041;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#25991;&#29486;&#65292;&#26377;&#20116;&#20010;&#22240;&#32032;&#24433;&#21709;HAII&#65306;&#29992;&#25143;&#29305;&#24449;&#21644;&#32972;&#26223;&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#20010;&#24615;&#65292;&#24863;&#30693;&#65289;&#65292;AI&#30028;&#38754;&#21644;&#21151;&#33021;&#65288;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, what little research there is on HAII is fragmented and inconsistent. We present here a survey of that literature and recommendations for research best practices that will improve the field. We divided our investigation into the following research areas: (1) terms used to describe HAII, (2) primary roles of AI-enabled systems, (3) factors that influence HAII, and (4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, five factors influence HAII: user characteristics and background (e.g., user personality, perceptions), AI interface and features (e.g., in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31243;&#24207;&#24615;&#25991;&#26412;&#25366;&#25496;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-4&#27169;&#22411;&#21644;&#33258;&#23450;&#20041;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;PDF&#25991;&#26412;&#20013;&#25552;&#21462;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.03376</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31243;&#24207;&#24615;&#25991;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Procedural Text Mining with Large Language Models. (arXiv:2310.03376v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31243;&#24207;&#24615;&#25991;&#26412;&#25366;&#25496;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-4&#27169;&#22411;&#21644;&#33258;&#23450;&#20041;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;PDF&#25991;&#26412;&#20013;&#25552;&#21462;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#35757;&#32451;&#20102;&#22823;&#37327;&#30693;&#35782;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20026;&#30693;&#35782;&#24037;&#31243;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#26426;&#36935;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#20174;&#38750;&#32467;&#26500;&#21270;PDF&#25991;&#26412;&#20013;&#20197;&#22686;&#37327;&#38382;&#31572;&#26041;&#24335;&#25552;&#21462;&#31243;&#24207;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GPT-4 (Generative Pre-trained Transformer 4)&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21464;&#20307;&#65292;&#21253;&#25324;&#24102;&#26377;&#31243;&#24207;&#21644;&#27493;&#39588;&#23450;&#20041;&#30340;&#26412;&#20307;&#21644;&#26377;&#38480;&#25968;&#37327;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#23450;&#21046;&#21270;&#30340;&#20215;&#20540;&#12290;&#36825;&#20123;&#20462;&#25913;&#26377;&#26395;&#26174;&#33879;&#35299;&#20915;&#33719;&#21462;&#36275;&#22815;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#36719;&#22686;&#38271;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#38142;&#65292;&#25552;&#20379;&#26368;&#20339;&#26426;&#22120;&#20154;&#23610;&#23544;&#20197;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#32676;&#20307;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#26448;&#26009;&#21644;&#36164;&#28304;&#28010;&#36153;&#12290;</title><link>http://arxiv.org/abs/2310.03374</link><description>&lt;p&gt;
&#24179;&#38754;&#36719;&#22686;&#38271;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#35774;&#35745;&#20248;&#21270;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Design Optimizer for Planar Soft-Growing Robot Manipulators. (arXiv:2310.03374v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#36719;&#22686;&#38271;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#38142;&#65292;&#25552;&#20379;&#26368;&#20339;&#26426;&#22120;&#20154;&#23610;&#23544;&#20197;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#32676;&#20307;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#26448;&#26009;&#21644;&#36164;&#28304;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#22686;&#38271;&#26426;&#22120;&#20154;&#26159;&#19968;&#31181;&#21019;&#26032;&#35774;&#22791;&#65292;&#20855;&#26377;&#26893;&#29289;&#21551;&#21457;&#30340;&#29983;&#38271;&#29305;&#24615;&#26469;&#23548;&#33322;&#29615;&#22659;&#12290;&#24471;&#30410;&#20110;&#20854;&#36866;&#24212;&#29615;&#22659;&#30340;&#20307;&#22806;&#26234;&#33021;&#21644;&#28608;&#21169;&#19982;&#21046;&#36896;&#30340;&#26368;&#26032;&#21019;&#26032;&#65292;&#21487;&#20197;&#23558;&#23427;&#20204;&#29992;&#20110;&#29305;&#23450;&#30340;&#25805;&#32437;&#20219;&#21153;&#12290;&#36825;&#20123;&#35774;&#22791;&#30340;&#24212;&#29992;&#21253;&#25324;&#23545;&#33030;&#24369;/&#21361;&#38505;&#29615;&#22659;&#30340;&#25506;&#32034;&#12289;&#29289;&#20307;&#30340;&#25805;&#32437;&#25110;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#21327;&#21161;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36719;&#22686;&#38271;&#26426;&#22120;&#20154;&#35774;&#35745;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#23558;&#22312;&#21046;&#36896;&#20043;&#21069;&#29992;&#20110;&#24314;&#35758;&#24037;&#31243;&#24072;&#25110;&#26426;&#22120;&#20154;&#35774;&#35745;&#29233;&#22909;&#32773;&#26500;&#24314;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#30340;&#26368;&#20339;&#26426;&#22120;&#20154;&#23610;&#23544;&#12290;&#25105;&#23558;&#35774;&#35745;&#36807;&#31243;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20248;&#21270;&#36719;&#25805;&#32437;&#22120;&#30340;&#36816;&#21160;&#38142;&#26469;&#36798;&#21040;&#30446;&#26631;&#24182;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#26448;&#26009;&#21644;&#36164;&#28304;&#28010;&#36153;&#12290;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#22522;&#20110;&#32676;&#20307;&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft-growing robots are innovative devices that feature plant-inspired growth to navigate environments. Thanks to their embodied intelligence of adapting to their surroundings and the latest innovation in actuation and manufacturing, it is possible to employ them for specific manipulation tasks. The applications of these devices include exploration of delicate/dangerous environments, manipulation of items, or assistance in domestic environments.  This work presents a novel approach for design optimization of soft-growing robots, which will be used prior to manufacturing to suggest engineers -- or robot designer enthusiasts -- the optimal dimension of the robot to be built for solving a specific task. I modeled the design process as a multi-objective optimization problem, in which I optimize the kinematic chain of a soft manipulator to reach targets and avoid unnecessary overuse of material and resources. The method exploits the advantages of population-based optimization algorithms, in
&lt;/p&gt;</description></item><item><title>Swin-Tempo&#26159;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;&#12290;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32954;&#32467;&#33410;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03365</link><description>&lt;p&gt;
Swin-Tempo: &#20351;&#29992;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet. (arXiv:2310.03365v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03365
&lt;/p&gt;
&lt;p&gt;
Swin-Tempo&#26159;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;&#12290;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32954;&#32467;&#33410;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#20855;&#26377;&#26497;&#39640;&#30340;&#33268;&#27515;&#29575;&#65292;&#26089;&#26399;&#26816;&#27979;&#23545;&#20110;&#38450;&#27835;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#32780;&#35328;&#65292;&#35782;&#21035;&#32954;&#32467;&#33410;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#20182;&#20204;&#24448;&#24448;&#20381;&#36182;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#26469;&#36827;&#34892;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#24050;&#32463;&#20986;&#29616;&#65292;&#24110;&#21161;&#21307;&#29983;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#35782;&#21035;&#32954;&#32467;&#33410;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32593;&#32476;&#24448;&#24448;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#23548;&#33268;&#35823;&#25253;&#21644;&#28431;&#25253;&#29575;&#36739;&#39640;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#20248;&#21183;&#12290;&#21463;&#35270;&#39057;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;3D CT&#22270;&#20687;&#35270;&#20026;&#19968;&#20010;&#35270;&#39057;&#65292;&#23558;&#27599;&#20010;&#20999;&#29255;&#35270;&#20026;&#24103;&#65292;&#23558;&#32954;&#32467;&#33410;&#35270;&#20026;&#30446;&#26631;&#65292;&#23454;&#29616;&#19968;&#20010;&#26102;&#24207;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is highly lethal, emphasizing the critical need for early detection. However, identifying lung nodules poses significant challenges for radiologists, who rely heavily on their expertise and experience for accurate diagnosis. To address this issue, computer-aided diagnosis systems based on machine learning techniques have emerged to assist doctors in identifying lung nodules from computed tomography (CT) scans. Unfortunately, existing networks in this domain often suffer from computational complexity, leading to high rates of false negatives and false positives, limiting their effectiveness. To address these challenges, we present an innovative model that harnesses the strengths of both convolutional neural networks and vision transformers. Inspired by object detection in videos, we treat each 3D CT image as a video, individual slices as frames, and lung nodules as objects, enabling a time-series application. The primary objective of our work is to overcome hardware limitati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#24449;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03358</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#36827;&#34892;&#40065;&#26834;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention. (arXiv:2310.03358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#24449;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#22122;&#22768;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#34987;&#35777;&#26126;&#26159;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#27450;&#39575;&#30340;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;AT&#24573;&#35270;&#20102;&#23398;&#20064;&#40065;&#26834;&#29305;&#24449;&#65292;&#23548;&#33268;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#40065;&#26834;&#34920;&#24449;&#30340;&#20004;&#20010;&#29305;&#24449;&#65306;&#65288;1&#65289;&#25490;&#20182;&#24615;&#65306;&#33258;&#28982;&#26679;&#26412;&#30340;&#29305;&#24449;&#36828;&#31163;&#20854;&#20182;&#31867;&#21035;&#30340;&#29305;&#24449;&#65307;&#65288;2&#65289;&#23545;&#40784;&#24615;&#65306;&#33258;&#28982;&#26679;&#26412;&#21644;&#30456;&#24212;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#24444;&#27492;&#25509;&#36817;&#12290;&#36825;&#20123;&#29305;&#28857;&#28608;&#21457;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;AT&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#26469;&#33719;&#24471;&#40065;&#26834;&#30340;&#34920;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#27979;&#27010;&#29575;&#30340;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#65292;&#23558;&#29305;&#24449;&#31354;&#38388;&#20013;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#25512;&#24320;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#21442;&#25968;&#23545;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#65292;&#20316;&#20026;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#20197;&#33719;&#24471;&#40065;&#26834;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial noise. Adversarial training (AT) has been demonstrated to be the most effective defense strategy to protect neural networks from being fooled. However, we find AT omits to learning robust features, resulting in poor performance of adversarial robustness. To address this issue, we highlight two characteristics of robust representation: (1) $\bf{exclusion}$: the feature of natural examples keeps away from that of other classes; (2) $\bf{alignment}$: the feature of natural and corresponding adversarial examples is close to each other. These motivate us to propose a generic framework of AT to gain robust representation, by the asymmetric negative contrast and reverse attention. Specifically, we design an asymmetric negative contrast based on predicted probabilities, to push away examples of different classes in the feature space. Moreover, we propose to weight feature by parameters of the linear classifier as the reverse attention, to obta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#27979;&#65288;PSRO&#65289;&#20316;&#20026;&#35299;&#20915;&#31454;&#20105;&#28216;&#25103;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21457;&#29616;&#33258;&#25105;&#23545;&#24328;&#26041;&#27861;&#22312;&#28151;&#21512;&#21512;&#20316;&#31454;&#20105;&#28216;&#25103;&#20013;&#26080;&#27861;&#25910;&#25947;&#21040;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#65292;&#32780;PSRO&#33021;&#22815;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#26368;&#20339;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;PSRO&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#32852;&#21512;&#31574;&#30053;&#65292;&#22686;&#21152;&#20102;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03354</link><description>&lt;p&gt;
&#34394;&#26500;&#20132;&#21449;&#29609;: &#22312;&#28151;&#21512;&#21512;&#20316;&#31454;&#20105;&#28216;&#25103;&#20013;&#23398;&#20064;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games. (arXiv:2310.03354v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#27979;&#65288;PSRO&#65289;&#20316;&#20026;&#35299;&#20915;&#31454;&#20105;&#28216;&#25103;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21457;&#29616;&#33258;&#25105;&#23545;&#24328;&#26041;&#27861;&#22312;&#28151;&#21512;&#21512;&#20316;&#31454;&#20105;&#28216;&#25103;&#20013;&#26080;&#27861;&#25910;&#25947;&#21040;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#65292;&#32780;PSRO&#33021;&#22815;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#26368;&#20339;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;PSRO&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#32852;&#21512;&#31574;&#30053;&#65292;&#22686;&#21152;&#20102;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#24328;&#65288;SP&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#28216;&#25103;&#65292;&#22312;&#36825;&#31181;&#26694;&#26550;&#19979;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#23558;&#20854;&#20182;&#26234;&#33021;&#20307;&#35270;&#20026;&#29615;&#22659;&#30340;&#19968;&#37096;&#20998;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#23613;&#31649;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;SP&#26041;&#27861;&#30340;&#29702;&#35770;&#24615;&#36136;&#20165;&#38480;&#20110;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#12290;&#28982;&#32780;&#65292;&#22312;&#28151;&#21512;&#21512;&#20316;&#31454;&#20105;&#28216;&#25103;&#20013;&#65292;&#38656;&#35201;&#22242;&#38431;&#20013;&#30340;&#26234;&#33021;&#20307;&#30456;&#20114;&#21512;&#20316;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#21453;&#20363;&#26469;&#35777;&#26126;SP&#26041;&#27861;&#26080;&#27861;&#20197;&#39640;&#27010;&#29575;&#25910;&#25947;&#21040;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#27979;&#65288;PSRO&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;NE&#30340;&#36845;&#20195;&#26694;&#26550;&#65292;&#20854;&#20013;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#23398;&#20064;&#30456;&#23545;&#20110;&#20808;&#21069;&#31574;&#30053;&#30340;&#26368;&#20339;&#21709;&#24212;&#12290;PSRO&#21487;&#20197;&#30452;&#25509;&#25193;&#23637;&#20026;&#28151;&#21512;&#21512;&#20316;&#31454;&#20105;&#22330;&#26223;&#65292;&#21516;&#26102;&#23398;&#20064;&#22242;&#38431;&#26368;&#20339;&#21709;&#24212;&#32780;&#25152;&#26377;&#25910;&#25947;&#24615;&#36136;&#22343;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;PSRO&#38656;&#35201;&#37325;&#22797;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#32852;&#21512;&#31574;&#30053;&#30452;&#21040;&#25910;&#25947;&#65292;&#36825;&#20351;&#24471;&#23427;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-play (SP) is a popular multi-agent reinforcement learning (MARL) framework for solving competitive games, where each agent optimizes policy by treating others as part of the environment. Despite the empirical successes, the theoretical properties of SP-based methods are limited to two-player zero-sum games. However, for mixed cooperative-competitive games where agents on the same team need to cooperate with each other, we can show a simple counter-example where SP-based methods cannot converge to a global Nash equilibrium (NE) with high probability. Alternatively, Policy-Space Response Oracles (PSRO) is an iterative framework for learning NE, where the best responses w.r.t. previous policies are learned in each iteration. PSRO can be directly extended to mixed cooperative-competitive settings by jointly learning team best responses with all convergence properties unchanged. However, PSRO requires repeatedly training joint policies from scratch till convergence, which makes it hard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#28145;&#24230;&#20960;&#20309;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#36827;&#23637;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#22312;&#40654;&#26364;&#31354;&#38388;&#20013;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#25968;&#25454;&#20960;&#20309;&#23646;&#24615;&#32771;&#34385;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#19981;&#23436;&#25972;&#26679;&#26412;&#20013;&#22806;&#25512;&#27491;&#23450;&#23545;&#31216;&#24230;&#37327;&#30340;&#38480;&#21046;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.03353</link><description>&lt;p&gt;
&#37319;&#29992;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#28145;&#24230;&#20960;&#20309;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Geometric Learning with Monotonicity Constraints for Alzheimer's Disease Progression. (arXiv:2310.03353v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#28145;&#24230;&#20960;&#20309;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#36827;&#23637;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#22312;&#40654;&#26364;&#31354;&#38388;&#20013;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#25968;&#25454;&#20960;&#20309;&#23646;&#24615;&#32771;&#34385;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#19981;&#23436;&#25972;&#26679;&#26412;&#20013;&#22806;&#25512;&#27491;&#23450;&#23545;&#31216;&#24230;&#37327;&#30340;&#38480;&#21046;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#19968;&#31181;&#27585;&#28781;&#24615;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20854;&#22312;&#28176;&#36827;&#21644;&#19981;&#21487;&#36870;&#30340;&#30196;&#21574;&#21069;&#20986;&#29616;&#65307;&#22240;&#27492;&#65292;&#23545;&#20854;&#38543;&#26102;&#38388;&#30340;&#36827;&#23637;&#36827;&#34892;&#39044;&#27979;&#23545;&#20110;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#37319;&#29992;&#32467;&#26500;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#26469;&#27169;&#25311;AD&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#65288;i&#65289;&#26102;&#38388;&#21464;&#24322;&#24615;&#65292;&#65288;ii&#65289;&#19981;&#23436;&#25972;&#35266;&#27979;&#65292;&#21644;&#65288;iii&#65289;&#26102;&#38388;&#20960;&#20309;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#25968;&#25454;&#21464;&#24322;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23578;&#26410;&#20805;&#20998;&#32771;&#34385;&#20854;&#22266;&#26377;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#22522;&#20110;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#30340;&#20960;&#20309;&#24314;&#27169;&#26041;&#27861;&#65288;ODE-RGRU&#65289;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#22312;&#40654;&#26364;&#31354;&#38388;&#20013;&#36890;&#36807;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;ODE&#20132;&#32455;&#22312;&#19968;&#36215;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#23450;&#25104;&#23601;&#65292;&#20294;ODE-RGRU&#22312;&#20174;&#19981;&#23436;&#25972;&#26679;&#26412;&#20013;&#22806;&#25512;&#27491;&#23450;&#23545;&#31216;&#24230;&#37327;&#26102;&#38754;&#20020;&#30528;&#19968;&#20123;&#38480;&#21046;&#65292;&#23548;&#33268;&#29305;&#24449;&#21453;&#36716;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is a devastating neurodegenerative condition that precedes progressive and irreversible dementia; thus, predicting its progression over time is vital for clinical diagnosis and treatment. Numerous studies have implemented structural magnetic resonance imaging (MRI) to model AD progression, focusing on three integral aspects: (i) temporal variability, (ii) incomplete observations, and (iii) temporal geometric characteristics. However, deep learning-based approaches regarding data variability and sparsity have yet to consider inherent geometrical properties sufficiently. The ordinary differential equation-based geometric modeling method (ODE-RGRU) has recently emerged as a promising strategy for modeling time-series data by intertwining a recurrent neural network and an ODE in Riemannian space. Despite its achievements, ODE-RGRU encounters limitations when extrapolating positive definite symmetric metrics from incomplete samples, leading to feature reverse occurr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30382;&#23572;&#26031;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20013;&#38480;&#21046;&#37096;&#20998;&#21487;&#35782;&#21035;&#26597;&#35810;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#32534;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31526;&#21495;&#21442;&#25968;&#26367;&#25442;&#21644;&#24182;&#34892;&#21270;&#25216;&#26415;&#21152;&#36895;&#30028;&#38480;&#35745;&#31639;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.03352</link><description>&lt;p&gt;
&#21487;&#30693;&#35782;&#32534;&#35793;&#30340;&#21487;&#25511;&#21046;&#30340;&#21453;&#20107;&#23454;&#26597;&#35810;&#36793;&#30028;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tractable Bounding of Counterfactual Queries by Knowledge Compilation. (arXiv:2310.03352v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03352
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30382;&#23572;&#26031;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20013;&#38480;&#21046;&#37096;&#20998;&#21487;&#35782;&#21035;&#26597;&#35810;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#32534;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31526;&#21495;&#21442;&#25968;&#26367;&#25442;&#21644;&#24182;&#34892;&#21270;&#25216;&#26415;&#21152;&#36895;&#30028;&#38480;&#35745;&#31639;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#30382;&#23572;&#26031;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20013;&#38480;&#21046;&#37096;&#20998;&#21487;&#35782;&#21035;&#26597;&#35810;&#65288;&#20363;&#22914;&#21453;&#20107;&#23454;&#65289;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#36845;&#20195;EM&#26041;&#26696;&#36890;&#36807;&#23545;&#21021;&#22987;&#21270;&#21442;&#25968;&#36827;&#34892;&#37319;&#26679;&#26469;&#33719;&#24471;&#36825;&#20123;&#30028;&#38480;&#30340;&#20869;&#37096;&#36817;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#23545;&#20849;&#20139;&#30456;&#21516;&#32467;&#26500;&#26041;&#31243;&#21644;&#25299;&#25169;&#32467;&#26500;&#20294;&#20855;&#26377;&#19981;&#21516;&#22806;&#29983;&#27010;&#29575;&#30340;&#27169;&#22411;&#36827;&#34892;&#22810;&#20010;&#65288;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26597;&#35810;&#12290;&#36825;&#31181;&#35774;&#32622;&#20351;&#24471;&#23558;&#24213;&#23618;&#27169;&#22411;&#32534;&#35793;&#20026;&#31639;&#26415;&#30005;&#36335;&#21464;&#24471;&#26377;&#21033;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#35266;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21333;&#19968;&#30340;&#31526;&#21495;&#30693;&#35782;&#32534;&#35793;&#26469;&#33719;&#24471;&#30005;&#36335;&#32467;&#26500;&#65292;&#24182;&#22312;&#35745;&#31639;&#19981;&#21516;&#26597;&#35810;&#26102;&#29992;&#23454;&#38469;&#20540;&#26367;&#25442;&#31526;&#21495;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36827;&#19968;&#27493;&#21152;&#36895;&#30028;&#38480;&#35745;&#31639;&#30340;&#24182;&#34892;&#21270;&#25216;&#26415;&#12290;&#19982;&#26631;&#20934;&#36125;&#21494;&#26031;&#32593;&#32476;&#25512;&#29702;&#30456;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26126;&#26174;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#20855;&#26377;&#39640;&#36798;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss the problem of bounding partially identifiable queries, such as counterfactuals, in Pearlian structural causal models. A recently proposed iterated EM scheme yields an inner approximation of those bounds by sampling the initialisation parameters. Such a method requires multiple (Bayesian network) queries over models sharing the same structural equations and topology, but different exogenous probabilities. This setup makes a compilation of the underlying model to an arithmetic circuit advantageous, thus inducing a sizeable inferential speed-up. We show how a single symbolic knowledge compilation allows us to obtain the circuit structure with symbolic parameters to be replaced by their actual values when computing the different queries. We also discuss parallelisation techniques to further speed up the bound computation. Experiments against standard Bayesian network inference show clear computational advantages with up to an order of magnitude of speed-up.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#35268;&#21010;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#27010;&#24565;&#34920;&#31034;&#12289;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#20197;&#21450;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#20851;&#32852;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2310.03325</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#21644;&#31526;&#21495;&#25512;&#29702;&#29992;&#20110;&#35270;&#35273;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning. (arXiv:2310.03325v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#35268;&#21010;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#27010;&#24565;&#34920;&#31034;&#12289;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#20197;&#21450;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#20851;&#32852;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35268;&#21010;&#27169;&#25311;&#20102;&#20154;&#31867;&#22312;&#25628;&#32034;&#21021;&#22987;&#35270;&#35273;&#29366;&#24577;&#21644;&#26368;&#32456;&#35270;&#35273;&#30446;&#26631;&#29366;&#24577;&#20043;&#38388;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#26102;&#25152;&#20570;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#35270;&#35273;&#20013;&#65292;&#35270;&#35273;&#35268;&#21010;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#22312;&#24341;&#23548;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25191;&#34892;&#26085;&#24120;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#35270;&#35273;&#35268;&#21010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;i&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#20195;&#30340;&#27010;&#24565;&#23398;&#20064;&#22120;&#65288;SCL&#65289;&#65292;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#20998;&#35299;&#30340;&#27010;&#24565;&#34920;&#31034;&#65307;ii&#65289;&#36890;&#36807;&#33258;&#23398;&#31526;&#21495;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#30340;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#65307;iii&#65289;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#35821;&#20041;&#30456;&#20284;&#30340;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#36827;&#34892;&#20851;&#32852;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#27169;&#22411;&#65288;ViCT&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#29366;&#24577;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21644;&#22240;&#26524;&#36716;&#25442;&#30340;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#65292;&#20197;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#20307;&#20851;&#33410;&#32422;&#26463;&#21644;&#22330;&#26223;&#32422;&#26463;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#19968;&#23450;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#20154;&#20307;&#21160;&#20316;&#65292;&#20197;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.03314</link><description>&lt;p&gt;
&#20351;&#29992;&#21463;&#38480;&#27010;&#29575;&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction. (arXiv:2310.03314v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#20307;&#20851;&#33410;&#32422;&#26463;&#21644;&#22330;&#26223;&#32422;&#26463;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#19968;&#23450;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#20154;&#20307;&#21160;&#20316;&#65292;&#20197;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#26159;&#23454;&#29616;&#39640;&#25928;&#23433;&#20840;&#30340;&#20154;&#26426;&#21327;&#20316;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#23436;&#20840;&#20381;&#36182;&#20110;&#23558;&#20154;&#31867;&#20851;&#33410;&#28857;&#34920;&#31034;&#20026;&#26576;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#35201;&#20040;&#20351;&#29992;&#31163;&#32447;&#22238;&#24402;&#27169;&#22411;&#26469;&#25311;&#21512;&#36229;&#21442;&#25968;&#65292;&#20197;&#25429;&#25417;&#21253;&#21547;&#20154;&#20307;&#21160;&#20316;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#21021;&#22987;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#21033;&#29992;&#30740;&#31350;&#20805;&#20998;&#30340;&#20154;&#20307;&#36816;&#21160;&#23398;&#27169;&#22411;&#20197;&#21450;&#36523;&#20307;&#21644;&#22330;&#26223;&#32422;&#26463;&#65292;&#36825;&#20123;&#32422;&#26463;&#26377;&#21161;&#20110;&#25552;&#39640;&#36825;&#20123;&#39044;&#27979;&#26694;&#26550;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#26126;&#30830;&#36991;&#20813;&#19981;&#21512;&#29702;&#30340;&#20154;&#20307;&#20851;&#33410;&#37197;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20154;&#20307;&#20851;&#33410;&#32422;&#26463;&#21644;&#22330;&#26223;&#32422;&#26463;&#32467;&#21512;&#22312;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#27169;&#22411;&#20013;&#65292;&#20197;&#39044;&#27979;&#19968;&#23450;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#20154;&#20307;&#21160;&#20316;&#12290;&#27492;&#20844;&#24335;&#19982;&#22312;&#32447;&#19978;&#19979;&#25991;&#24863;&#30693;&#32422;&#26463;&#27169;&#22411;&#32467;&#21512;&#65292;&#20197;&#21033;&#29992;&#20219;&#21153;&#30456;&#20851;&#36816;&#21160;&#12290;&#22312;&#20154;&#20307;&#33218;&#36816;&#21160;&#23398;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction is an essential step for efficient and safe human-robot collaboration. Current methods either purely rely on representing the human joints in some form of neural network-based architecture or use regression models offline to fit hyper-parameters in the hope of capturing a model encompassing human motion. While these methods provide good initial results, they are missing out on leveraging well-studied human body kinematic models as well as body and scene constraints which can help boost the efficacy of these prediction frameworks while also explicitly avoiding implausible human joint configurations. We propose a novel human motion prediction framework that incorporates human joint constraints and scene constraints in a Gaussian Process Regression (GPR) model to predict human motion over a set time horizon. This formulation is combined with an online context-aware constraints model to leverage task-dependent motions. It is tested on a human arm kinematic model and
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03309</link><description>&lt;p&gt;
&#31616;&#26126;&#26377;&#24207;&#30340;&#24863;&#30693;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03309
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#28436;&#32462;&#25512;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#22797;&#26434;&#30340;&#28436;&#32462;&#38382;&#39064;&#20013;&#20173;&#28982;&#24456;&#38590;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#31867;&#38382;&#39064;&#20855;&#26377;&#22823;&#37327;&#21069;&#25552;&#65288;&#21363;&#20107;&#23454;&#25110;&#35268;&#21017;&#65289;&#65292;&#20854;&#20013;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20197;&#21069;&#21521;&#65288;&#20363;&#22914;&#36873;&#25321;-&#25512;&#29702;&#65289;&#25110;&#21453;&#21521;&#65288;&#20363;&#22914;LAMBADA&#65289;&#26041;&#24335;&#23558;&#22810;&#20010;&#22240;&#26524;&#25512;&#29702;&#27493;&#39588;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#24635;&#20307;&#38454;&#27573;&#65292;&#23548;&#33268;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#24182;&#19988;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#27493;&#39588;&#12290;&#38500;&#20102;&#36880;&#38454;&#27573;&#20998;&#35299;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#21478;&#19968;&#20010;&#26041;&#38754;&#33719;&#24471;&#20102;&#21551;&#21457;&#12290;&#20154;&#31867;&#20542;&#21521;&#20110;&#25552;&#28860;&#20986;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#24182;&#26377;&#24207;&#22320;&#32452;&#32455;&#24605;&#32500;&#65288;&#20363;&#22914;&#21019;&#24314;&#24605;&#32500;&#23548;&#22270;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#20182;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#65292;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#65292;&#20174;&#32780;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#33021;&#22815;&#25191;&#34892;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;AI&#30740;&#31350;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.03302</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models As AI Research Agents. (arXiv:2310.03302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#65292;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#65292;&#20174;&#32780;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#33021;&#22815;&#25191;&#34892;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;AI&#30740;&#31350;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#23454;&#39564;&#28041;&#21450;&#21019;&#24314;&#20551;&#35774;&#12289;&#35774;&#35745;&#23454;&#39564;&#12289;&#36816;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#32467;&#26524;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;&#25105;&#20204;&#33021;&#21542;&#26500;&#24314;AI&#30740;&#31350;&#20195;&#29702;&#26469;&#25191;&#34892;&#36825;&#20123;&#38271;&#26399;&#30446;&#26631;&#30340;&#20219;&#21153;&#21602;&#65311;&#20026;&#20102;&#26397;&#30528;&#22312;&#27492;&#31867;&#24320;&#25918;&#24615;&#20915;&#31574;&#20219;&#21153;&#19978;&#26500;&#24314;&#21644;&#35780;&#20272;&#30740;&#31350;&#20195;&#29702;&#30340;&#30446;&#26631;&#36808;&#20986;&#19968;&#27493;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#26426;&#22120;&#23398;&#20064;&#24037;&#31243;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#20219;&#21153;&#25551;&#36848;&#21644;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#19968;&#20010;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#12290;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#35835;&#20889;&#25991;&#20214;&#12289;&#25191;&#34892;&#20195;&#30721;&#21644;&#26816;&#26597;&#36755;&#20986;&#31561;&#21160;&#20316;&#12290;&#36890;&#36807;&#36825;&#20123;&#21160;&#20316;&#65292;&#20195;&#29702;&#21487;&#20197;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#65292;&#22914;&#25968;&#25454;&#22788;&#29702;&#12289;&#26550;&#26500;&#12289;&#35757;&#32451;&#36807;&#31243;&#31561;&#12290;&#28982;&#21518;&#65292;&#22522;&#20934;&#27979;&#35797;&#33258;&#21160;&#23458;&#35266;&#22320;&#35780;&#20272;&#20195;&#29702;&#22312;&#19982;&#24615;&#33021;&#21644;&#25928;&#29575;&#30456;&#20851;&#30340;&#21508;&#31181;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;LLM-
&lt;/p&gt;
&lt;p&gt;
Scientific experimentation involves an iterative process of creating hypotheses, designing experiments, running experiments, and analyzing the results. Can we build AI research agents to perform these long-horizon tasks? To take a step towards building and evaluating research agents on such open-ended decision-making tasks, we focus on the problem of machine learning engineering: given a task description and a dataset, build a high-performing model. In this paper, we propose MLAgentBench, a suite of ML tasks for benchmarking AI research agents. Agents can perform actions like reading/writing files, executing code, and inspecting outputs. With these actions, agents could run experiments, analyze the results, and modify the code of entire machine learning pipelines, such as data processing, architecture, training processes, etc. The benchmark then automatically evaluates the agent's performance objectively over various metrics related to performance and efficiency. We also design an LLM-
&lt;/p&gt;</description></item><item><title>LightSeq&#26159;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#19982;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#20860;&#23481;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#19982;Megatron-LM&#30456;&#27604;&#30340;&#36890;&#20449;&#37327;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#37325;&#21472;&#12290;</title><link>http://arxiv.org/abs/2310.03294</link><description>&lt;p&gt;
LightSeq&#65306;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24207;&#21015;&#32423;&#24182;&#34892;ism
&lt;/p&gt;
&lt;p&gt;
LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03294
&lt;/p&gt;
&lt;p&gt;
LightSeq&#26159;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#19982;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#20860;&#23481;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#19982;Megatron-LM&#30456;&#27604;&#30340;&#36890;&#20449;&#37327;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21487;&#20197;&#35299;&#24320;&#22522;&#26412;&#19978;&#26032;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#26174;&#33879;&#22686;&#21152;&#20102;&#35757;&#32451;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#20197;&#24448;&#30340;&#27169;&#22411;&#24182;&#34892;&#31995;&#32479;&#65288;&#20363;&#22914;Megatron-LM&#65289;&#23545;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#21306;&#21644;&#35745;&#31639;&#65292;&#24182;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#22823;&#37327;&#36890;&#20449;&#37327;&#65292;&#22240;&#27492;&#19981;&#33021;&#22312;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#20043;&#22806;&#25193;&#23637;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20854;&#37319;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LightSeq&#65292;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;LLM&#30340;&#35757;&#32451;&#12290;LightSeq&#20855;&#26377;&#35768;&#22810;&#26174;&#33879;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;LightSeq&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#22240;&#27492;&#23545;&#20110;&#20855;&#26377;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#36866;&#29992;&#20110;Multi-Head&#65292;Multi-Query&#21644;Grouped-Query attention&#31561;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;LightSeq&#19982;Megatron-LM&#30456;&#27604;&#65292;&#22312;&#27969;&#34892;&#30340;LLM&#19978;&#19981;&#20165;&#38656;&#27714;&#23569;&#33267;4.7&#20493;&#30340;&#36890;&#20449;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#23558;&#36890;&#20449;&#19982;&#35745;&#31639;&#37325;&#21472;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;LightSeq&#36824;&#20855;&#26377;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;che
&lt;/p&gt;
&lt;p&gt;
Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient che
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;49&#31687;&#25991;&#31456;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#22270;&#24418;&#21270;&#31574;&#30053;&#37197;&#32622;&#24037;&#20855;&#21644;&#33258;&#21160;&#31574;&#30053;&#29983;&#25104;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#35775;&#38382;&#25511;&#21046;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2310.03292</link><description>&lt;p&gt;
Sok&#65306;&#20174;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#35201;&#27714;&#29983;&#25104;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SoK: Access Control Policy Generation from High-level Natural Language Requirements. (arXiv:2310.03292v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;49&#31687;&#25991;&#31456;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#22270;&#24418;&#21270;&#31574;&#30053;&#37197;&#32622;&#24037;&#20855;&#21644;&#33258;&#21160;&#31574;&#30053;&#29983;&#25104;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#35775;&#38382;&#25511;&#21046;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#29702;&#21592;&#20013;&#24515;&#30340;&#35775;&#38382;&#25511;&#21046;&#22833;&#36133;&#21487;&#33021;&#20250;&#23548;&#33268;&#25968;&#25454;&#27844;&#38706;&#65292;&#20351;&#32452;&#32455;&#38754;&#20020;&#36130;&#21153;&#25439;&#22833;&#21644;&#22768;&#35465;&#25439;&#23475;&#30340;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#22270;&#24418;&#21270;&#31574;&#30053;&#37197;&#32622;&#24037;&#20855;&#21644;&#33258;&#21160;&#31574;&#30053;&#29983;&#25104;&#26694;&#26550;&#35797;&#22270;&#36890;&#36807;&#36991;&#20813;&#27492;&#31867;&#25925;&#38556;&#26469;&#24110;&#21161;&#31649;&#29702;&#21592;&#37197;&#32622;&#21644;&#29983;&#25104;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22270;&#24418;&#21270;&#31574;&#30053;&#37197;&#32622;&#24037;&#20855;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#38169;&#35823;&#65292;&#20351;&#20854;&#26080;&#27861;&#20351;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#21160;&#31574;&#30053;&#29983;&#25104;&#26694;&#26550;&#23481;&#26131;&#21457;&#29983;&#38169;&#35823;&#39044;&#27979;&#65292;&#20351;&#20854;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23547;&#25214;&#25913;&#36827;&#20854;&#21487;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;49&#31687;&#25991;&#31456;&#65292;&#20197;&#35782;&#21035;&#36825;&#20123;&#24037;&#20855;&#12289;&#26694;&#26550;&#21450;&#20854;&#38480;&#21046;&#12290;&#35782;&#21035;&#36825;&#20123;&#38480;&#21046;&#23558;&#26377;&#21161;&#20110;&#24320;&#21457;&#26377;&#25928;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#36991;&#20813;&#35775;&#38382;&#25511;&#21046;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Administrator-centered access control failures can cause data breaches, putting organizations at risk of financial loss and reputation damage. Existing graphical policy configuration tools and automated policy generation frameworks attempt to help administrators configure and generate access control policies by avoiding such failures. However, graphical policy configuration tools are prone to human errors, making them unusable. On the other hand, automated policy generation frameworks are prone to erroneous predictions, making them unreliable. Therefore, to find ways to improve their usability and reliability, we conducted a Systematic Literature Review analyzing 49 publications, to identify those tools, frameworks, and their limitations. Identifying those limitations will help develop effective access control policy generation solutions while avoiding access control failures.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;UTR-LM&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#29289;&#31181;&#30340;5' UTR&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#26377;&#30417;&#30563;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#12289;&#32763;&#35793;&#25928;&#29575;&#12289;mRNA&#34920;&#36798;&#27700;&#24179;&#65292;&#24182;&#25913;&#36827;&#20102;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03281</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35299;&#30721;mRNA&#30340;5' UTR&#35821;&#35328;&#27169;&#22411;&#21644;&#21151;&#33021;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions. (arXiv:2310.03281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03281
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;UTR-LM&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#29289;&#31181;&#30340;5' UTR&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#26377;&#30417;&#30563;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#12289;&#32763;&#35793;&#25928;&#29575;&#12289;mRNA&#34920;&#36798;&#27700;&#24179;&#65292;&#24182;&#25913;&#36827;&#20102;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5' UTR&#26159;mRNA&#20998;&#23376;&#24320;&#31471;&#30340;&#35843;&#25511;&#21306;&#22495;&#65292;&#22312;&#35843;&#25511;&#32763;&#35793;&#36807;&#31243;&#21644;&#24433;&#21709;&#34507;&#30333;&#34920;&#36798;&#27700;&#24179;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#35299;&#30721;&#34507;&#30333;&#36136;&#21644;&#22522;&#22240;&#32452;&#24207;&#21015;&#21151;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;5' UTR&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;UTR-LM&#12290;UTR-LM&#22312;&#22810;&#20010;&#29289;&#31181;&#30340;&#20869;&#28304;&#24615;5' UTR&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#36827;&#19968;&#27493;&#21152;&#20837;&#20102;&#21253;&#25324;&#20108;&#32423;&#32467;&#26500;&#21644;&#26368;&#23567;&#33258;&#30001;&#33021;&#22312;&#20869;&#30340;&#26377;&#30417;&#30563;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;UTR-LM&#36827;&#34892;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#27169;&#22411;&#22312;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#24050;&#30693;&#30340;&#26368;&#20339;&#22522;&#20934;&#27169;&#22411;&#26368;&#22810;42%&#65292;&#21516;&#26102;&#22312;&#39044;&#27979;&#32763;&#35793;&#25928;&#29575;&#21644;mRNA&#34920;&#36798;&#27700;&#24179;&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;&#26368;&#22810;60%&#12290;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#26410;&#27880;&#37322;&#30340;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#65292;&#24182;&#23558;AUPR&#19982;&#26368;&#20339;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#20174;0.37&#25552;&#39640;&#33267;0.52&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
The 5' UTR, a regulatory region at the beginning of an mRNA molecule, plays a crucial role in regulating the translation process and impacts the protein expression level. Language models have showcased their effectiveness in decoding the functions of protein and genome sequences. Here, we introduced a language model for 5' UTR, which we refer to as the UTR-LM. The UTR-LM is pre-trained on endogenous 5' UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The model outperformed the best-known benchmark by up to 42% for predicting the Mean Ribosome Loading, and by up to 60% for predicting the Translation Efficiency and the mRNA Expression Level. The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we designed a li
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.03272</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#20256;&#36755;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#24314;&#31435;&#19968;&#23545;&#19968;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#22312;&#39640;&#24433;&#21709;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#34987;&#35748;&#20026;&#26159;NP&#38590;&#30340;&#65292;&#32780;&#19988;&#29616;&#26377;&#30340;&#31639;&#27861;&#22312;&#22270;&#30340;&#35268;&#27169;&#22686;&#22823;&#26102;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#22270;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#21462;&#24378;&#22823;&#19988;&#40065;&#26834;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#40784;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#29983;&#25104;&#30340;&#23884;&#20837;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#65292;&#24182;&#19988;&#19982;&#32463;&#20856;&#35889;&#26041;&#27861;&#30456;&#27604;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32593;&#32476;&#23545;&#40784;&#21644;&#23376;&#32593;&#32476;&#23545;&#40784;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#25903;&#25345;&#35813;&#26694;&#26550;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#22312;&#20381;&#36182;&#25968;&#25454;&#65288;&#22914;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65289;&#19978;&#30340;&#29702;&#35770;&#21644;&#24212;&#29992;&#12290;&#36890;&#36807;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#31232;&#30095;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#24182;&#23545;&#20854;&#39044;&#27979;&#36827;&#34892;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03243</link><description>&lt;p&gt;
&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sparse Deep Learning for Time Series Data: Theory and Applications. (arXiv:2310.03243v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#22312;&#20381;&#36182;&#25968;&#25454;&#65288;&#22914;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65289;&#19978;&#30340;&#29702;&#35770;&#21644;&#24212;&#29992;&#12290;&#36890;&#36807;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#31232;&#30095;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#24182;&#23545;&#20854;&#39044;&#27979;&#36827;&#34892;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#25552;&#21319;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#21464;&#37327;&#36873;&#25321;&#21644;&#22823;&#35268;&#27169;&#32593;&#32476;&#21387;&#32553;&#31561;&#39046;&#22495;&#24615;&#33021;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35266;&#27979;&#30456;&#20114;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#30340;&#38382;&#39064;&#19978;&#65292;&#24182;&#19988;&#22312;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39034;&#24207;&#25968;&#25454;&#31561;&#35266;&#27979;&#30456;&#20114;&#20381;&#36182;&#30340;&#38382;&#39064;&#19978;&#20960;&#20046;&#27809;&#26377;&#30456;&#20851;&#24037;&#20316;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#30740;&#31350;&#20855;&#26377;&#20381;&#36182;&#25968;&#25454;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31232;&#30095;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#39044;&#27979;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#28176;&#36817;&#22320;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#65292;&#20174;&#32780;&#33021;&#22815;&#27491;&#30830;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#32988;&#36807;&#20102;&#35832;&#22914;&#20381;&#29031;&#24615;&#39044;&#27979;&#31561;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse deep learning has become a popular technique for improving the performance of deep neural networks in areas such as uncertainty quantification, variable selection, and large-scale network compression. However, most existing research has focused on problems where the observations are independent and identically distributed (i.i.d.), and there has been little work on the problems where the observations are dependent, such as time series data and sequential data in natural language processing. This paper aims to address this gap by studying the theory for sparse deep learning with dependent data. We show that sparse recurrent neural networks (RNNs) can be consistently estimated, and their predictions are asymptotically normally distributed under appropriate assumptions, enabling the prediction uncertainty to be correctly quantified. Our numerical results show that sparse deep learning outperforms state-of-the-art methods, such as conformal predictions, in prediction uncertainty qua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.03234</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#12290;&#30001;&#20110;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#20854;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;FCCO&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;FCCO&#30340;&#30740;&#31350;&#20551;&#35774;&#20869;&#22806;&#20989;&#25968;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#31181;&#31867;&#30340;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20854;&#20013;&#22806;&#20989;&#25968;&#26159;&#24369;&#20984;&#19988;&#38750;&#36882;&#20943;&#30340;&#65292;&#20869;&#20989;&#25968;&#26159;&#24369;&#20984;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#34920;&#24449;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#65292;&#20998;&#26512;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#26631;&#20934;&#20998;&#31867;&#20196;&#29260;&#23884;&#20837;&#21644;&#22522;&#20110;&#39057;&#29575;&#30340;&#20195;&#35789;&#20998;&#26512;&#65292;&#19978;&#19979;&#25991;&#21270;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.03232</link><description>&lt;p&gt;
&#28145;&#24230;&#34920;&#24449;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#20197;&#39044;&#27979;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity. (arXiv:2310.03232v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#34920;&#24449;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#65292;&#20998;&#26512;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#26631;&#20934;&#20998;&#31867;&#20196;&#29260;&#23884;&#20837;&#21644;&#22522;&#20110;&#39057;&#29575;&#30340;&#20195;&#35789;&#20998;&#26512;&#65292;&#19978;&#19979;&#25991;&#21270;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20998;&#26512;&#20351;&#29992;&#31532;&#19968;&#20154;&#31216;&#21333;&#25968;&#20195;&#35789;&#21487;&#20197;&#25581;&#31034;&#20010;&#20307;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#29305;&#21035;&#26159;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#36825;&#20123;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#25991;&#26412;&#25968;&#25454;&#20013;&#31532;&#19968;&#20154;&#31216;&#21333;&#25968;&#20195;&#35789;&#30340;&#39057;&#29575;&#26469;&#24471;&#20986;&#32467;&#35770;&#12290;&#28982;&#32780;&#65292;&#35745;&#25968;&#19981;&#33021;&#25429;&#25417;&#21040;&#36825;&#20123;&#20195;&#35789;&#30340;&#20351;&#29992;&#26041;&#24335;&#12290;&#36817;&#26399;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#30340;&#36827;&#23637;&#21033;&#29992;&#20102;&#29983;&#25104;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;&#20174;&#19978;&#19979;&#25991;&#35821;&#35328;&#34920;&#24449;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#26469;&#25429;&#25417;&#36825;&#20123;&#20195;&#35789;&#30340;&#20351;&#29992;&#26041;&#24335;&#65292;&#20197;&#20998;&#26512;&#24515;&#29702;&#29366;&#24577;&#12290;&#35780;&#20272;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#22312;&#22312;&#32447;&#24515;&#29702;&#27835;&#30103;&#26399;&#38388;&#21457;&#36865;&#30340;&#21435;&#36523;&#20221;&#21270;&#30340;&#25991;&#26412;&#28040;&#24687;&#65292;&#20854;&#20013;&#27599;&#21608;&#35780;&#20272;&#20102;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#20998;&#31867;&#20196;&#29260;&#23884;&#20837;&#21644;&#22522;&#20110;&#39057;&#29575;&#30340;&#20195;&#35789;&#20998;&#26512;&#30456;&#27604;&#65292;&#19978;&#19979;&#25991;&#21270;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has shown that analyzing the use of first-person singular pronouns can provide insight into individuals' mental status, especially depression symptom severity. These findings were generated by counting frequencies of first-person singular pronouns in text data. However, counting doesn't capture how these pronouns are used. Recent advances in neural language modeling have leveraged methods generating contextual embeddings. In this study, we sought to utilize the embeddings of first-person pronouns obtained from contextualized language representation models to capture ways these pronouns are used, to analyze mental status. De-identified text messages sent during online psychotherapy with weekly assessment of depression severity were used for evaluation. Results indicate the advantage of contextualized first-person pronoun embeddings over standard classification token embeddings and frequency-based pronoun analysis results in predicting depression symptom severity. This suggest
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#23433;&#20840;&#25506;&#32034;&#38382;&#39064;&#65292;&#21363;GSE&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#31639;&#27861;MASE&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MASE&#23558;&#26080;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#32467;&#21512;&#65292;&#20197;&#20445;&#35777;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#36829;&#21453;&#23433;&#20840;&#32422;&#26463;&#20043;&#21069;&#24809;&#32602;&#19981;&#23433;&#20840;&#30340;&#25506;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#26159;&#22312;&#20445;&#35777;&#23433;&#20840;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#20855;&#26377;&#39640;&#27010;&#29575;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03225</link><description>&lt;p&gt;
&#23433;&#20840;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#26222;&#36866;&#30340;&#24418;&#24335;&#21270;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms. (arXiv:2310.03225v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#23433;&#20840;&#25506;&#32034;&#38382;&#39064;&#65292;&#21363;GSE&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#31639;&#27861;MASE&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MASE&#23558;&#26080;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#32467;&#21512;&#65292;&#20197;&#20445;&#35777;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#36829;&#21453;&#23433;&#20840;&#32422;&#26463;&#20043;&#21069;&#24809;&#32602;&#19981;&#23433;&#20840;&#30340;&#25506;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#26159;&#22312;&#20445;&#35777;&#23433;&#20840;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#20855;&#26377;&#39640;&#27010;&#29575;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#23433;&#20840;&#25506;&#32034;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#23433;&#20840;&#25506;&#32034;&#65288;GSE&#65289;&#38382;&#39064;&#65292;&#20316;&#20026;&#24120;&#35265;&#23433;&#20840;&#25506;&#32034;&#38382;&#39064;&#30340;&#32479;&#19968;&#24418;&#24335;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GSE&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23433;&#20840;&#25506;&#32034;&#30340;&#20803;&#31639;&#27861;MASE&#65292;&#23427;&#23558;&#26080;&#32422;&#26463;&#30340;RL&#31639;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#20445;&#35777;&#24403;&#21069;&#22238;&#21512;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#23433;&#20840;&#36829;&#35268;&#20043;&#21069;&#36866;&#24403;&#24809;&#32602;&#19981;&#23433;&#20840;&#30340;&#25506;&#32034;&#65292;&#20197;&#38450;&#27490;&#23427;&#20204;&#22312;&#26410;&#26469;&#22238;&#21512;&#20013;&#21457;&#29983;&#12290;MASE&#30340;&#20248;&#21183;&#22312;&#20110;&#25105;&#20204;&#21487;&#20197;&#22312;&#20445;&#35777;&#39640;&#27010;&#29575;&#19979;&#19981;&#36829;&#21453;&#23433;&#20840;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#65292;&#20248;&#21270;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#30340;MASE&#21464;&#20307;&#65306;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;&#20855;&#26377;&#23433;&#20840;&#24615;&#21644;&#25509;&#36817;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21478;&#19968;&#31181;&#21017;&#32467;&#21512;&#20102;&#39640;&#26031;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe exploration is essential for the practical use of reinforcement learning (RL) in many real-world scenarios. In this paper, we present a generalized safe exploration (GSE) problem as a unified formulation of common safe exploration problems. We then propose a solution of the GSE problem in the form of a meta-algorithm for safe exploration, MASE, which combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety in the current episode while properly penalizing unsafe explorations before actual safety violation to discourage them in future episodes. The advantage of MASE is that we can optimize a policy while guaranteeing with a high probability that no safety constraint will be violated under proper assumptions. Specifically, we present two variants of MASE with different constructions of the uncertainty quantifier: one based on generalized linear models with theoretical guarantees of safety and near-optimality, and another that combines a Gaussian proce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#25913;&#36827;&#30340;&#38271;&#26399; MCMC&#37319;&#26679;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#33021;&#37327;&#20808;&#39564;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03218</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#25913;&#36827;&#30340; MCMC &#23398;&#20064;&#33021;&#37327;&#20808;&#39564;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Energy-Based Prior Model with Diffusion-Amortized MCMC. (arXiv:2310.03218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#25913;&#36827;&#30340;&#38271;&#26399; MCMC&#37319;&#26679;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#33021;&#37327;&#20808;&#39564;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21464;&#37327;&#31354;&#38388;&#30340;&#33021;&#37327;&#22522;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#20063;&#31216;&#20026;&#33021;&#37327;&#20808;&#39564;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#22312;&#20844;&#24335;&#21270;&#21644;&#28508;&#22312;&#31354;&#38388;&#30340;&#24378;&#24314;&#27169;&#33021;&#21147;&#19978;&#30340;&#28789;&#27963;&#24615;&#65292;&#24341;&#36215;&#20102;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#38750;&#25910;&#25947;&#30340;&#30701;&#26399; MCMC &#36827;&#34892;&#20808;&#39564;&#21644;&#21518;&#39564;&#37319;&#26679;&#26469;&#23398;&#20064;&#38544;&#21464;&#37327;&#31354;&#38388;&#30340;&#33021;&#37327;&#20808;&#39564;&#27169;&#22411;&#30340;&#24120;&#35265;&#20570;&#27861;&#65292;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#65307;&#23454;&#36341;&#20013;&#36864;&#21270;&#30340; MCMC &#37319;&#26679;&#36136;&#37327;&#36890;&#24120;&#23548;&#33268;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#22810;&#27169;&#24577;&#21644;/&#25110;&#39640;&#32500;&#30446;&#26631;&#20998;&#24067;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#37319;&#26679;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25674;&#38144;&#26041;&#27861;&#65292;&#29992;&#20110;&#38271;&#26399; MCMC &#37319;&#26679;&#65292;&#24182;&#22522;&#20110;&#27492;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#38544;&#21464;&#37327;&#31354;&#38388;&#30340;EBM&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#25454;&#65292;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;MCMC&#25674;&#38144;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#38271;&#26399;MCMC&#37319;&#26679;&#22120;&#12290;&#22312;&#20960;&#20010;&#22270;&#20687;&#24314;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in the field of generative modeling due to its flexibility in the formulation and strong modeling power of the latent space. However, the common practice of learning latent space EBMs with non-convergent short-run MCMC for prior and posterior sampling is hindering the model from further progress; the degenerate MCMC sampling quality in practice often leads to degraded generation quality and instability in training, especially with highly multi-modal and/or high-dimensional target distributions. To remedy this sampling issue, in this paper we introduce a simple but effective diffusion-based amortization method for long-run MCMC sampling and develop a novel learning algorithm for the latent space EBM based on it. We provide theoretical evidence that the learned amortization of MCMC is a valid long-run MCMC sampler. Experiments on several image modeling benchmark datasets demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#20026;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.03211</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Performance of Multimodal Language Models. (arXiv:2310.03211v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#20026;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29420;&#31435;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#36890;&#36807;&#27169;&#22411;&#23233;&#25509;&#30340;&#26041;&#24335;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21518;&#65292;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26377;&#26395;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#19981;&#21516;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#12289;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#20108;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#25351;&#23548;&#26550;&#26500;&#36873;&#25321;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#27809;&#26377;&#36275;&#22815;&#22320;&#35299;&#20915;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) have demonstrated promising zero-shot generalization capabilities across various downstream tasks. Recent research has introduced multimodal capabilities to LLMs by integrating independently pretrained vision encoders through model grafting. These multimodal variants undergo instruction tuning, similar to LLMs, enabling effective zero-shot generalization for multimodal tasks. This study conducts a comparative analysis of different multimodal instruction tuning approaches and evaluates their performance across a range of tasks, including complex reasoning, conversation, image captioning, multiple-choice questions (MCQs), and binary classification. Through rigorous benchmarking and ablation experiments, we reveal key insights for guiding architectural choices when incorporating multimodal capabilities into LLMs. However, current approaches have limitations; they do not sufficiently address the need for a diverse multimodal instruction datase
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;NeuFace&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#20877;&#21442;&#25968;&#21270;&#20248;&#21270;&#65292;&#22312;&#22823;&#35268;&#27169;&#30340;&#20154;&#33080;&#35270;&#39057;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#20154;&#33080;&#32593;&#26684;&#26631;&#27880;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;3D&#20154;&#33080;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#26377;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#25913;&#21892;&#29616;&#26377;&#30340;3D&#20154;&#33080;&#37325;&#24314;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;3D&#38754;&#37096;&#36816;&#21160;&#20808;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.03205</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#20877;&#21442;&#25968;&#21270;&#20248;&#21270;&#23454;&#29616;&#30340;&#22823;&#35268;&#27169;3D&#20154;&#33080;&#32593;&#26684;&#35270;&#39057;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization. (arXiv:2310.03205v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03205
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NeuFace&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#20877;&#21442;&#25968;&#21270;&#20248;&#21270;&#65292;&#22312;&#22823;&#35268;&#27169;&#30340;&#20154;&#33080;&#35270;&#39057;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#20154;&#33080;&#32593;&#26684;&#26631;&#27880;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;3D&#20154;&#33080;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#26377;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#25913;&#21892;&#29616;&#26377;&#30340;3D&#20154;&#33080;&#37325;&#24314;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;3D&#38754;&#37096;&#36816;&#21160;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuFace&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#20877;&#21442;&#25968;&#21270;&#20248;&#21270;&#22312;&#35270;&#39057;&#20013;&#36827;&#34892;3D&#20154;&#33080;&#32593;&#26684;&#30340;&#20266;&#26631;&#27880;&#12290;&#23613;&#31649;&#22312;3D&#20154;&#33080;&#37325;&#24314;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#37326;&#22806;&#21160;&#24577;&#35270;&#39057;&#20013;&#29983;&#25104;&#21487;&#38752;&#30340;3D&#20154;&#33080;&#26631;&#31614;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;NeuFace&#20248;&#21270;&#65292;&#25105;&#20204;&#23545;&#22823;&#35268;&#27169;&#20154;&#33080;&#35270;&#39057;&#36827;&#34892;&#27599;&#20010;&#35270;&#35282;/&#24103;&#20934;&#30830;&#32780;&#19968;&#33268;&#30340;&#20154;&#33080;&#32593;&#26684;&#26631;&#27880;&#65292;&#31216;&#20026;NeuFace&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#20877;&#21442;&#25968;&#21270;&#22914;&#20309;&#36890;&#36807;&#26799;&#24230;&#20998;&#26512;&#23558;&#22270;&#20687;&#23545;&#40784;&#30340;&#38754;&#37096;&#32454;&#33410;&#37325;&#24314;&#21040;3D&#32593;&#26684;&#19978;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#25968;&#25454;&#38598;&#20013;3D&#20154;&#33080;&#30340;&#33258;&#28982;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#22312;3D&#20154;&#33080;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#29992;&#22788;&#65306;&#25552;&#39640;&#29616;&#26377;3D&#20154;&#33080;&#37325;&#24314;&#27169;&#22411;&#30340;&#37325;&#24314;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;3D&#38754;&#37096;&#36816;&#21160;&#20808;&#39564;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#23558;&#22312;https://neuface-dataset.github&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose NeuFace, a 3D face mesh pseudo annotation method on videos via neural re-parameterized optimization. Despite the huge progress in 3D face reconstruction methods, generating reliable 3D face labels for in-the-wild dynamic videos remains challenging. Using NeuFace optimization, we annotate the per-view/-frame accurate and consistent face meshes on large-scale face videos, called the NeuFace-dataset. We investigate how neural re-parameterization helps to reconstruct image-aligned facial details on 3D meshes via gradient analysis. By exploiting the naturalness and diversity of 3D faces in our dataset, we demonstrate the usefulness of our dataset for 3D face-related tasks: improving the reconstruction accuracy of an existing 3D face reconstruction model and learning 3D facial motion prior. Code and datasets will be available at https://neuface-dataset.github.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#26426;&#22120;&#35843;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#27604;&#36739;&#65292;&#24635;&#32467;&#20986;&#23427;&#20204;&#30340;&#26041;&#27861;&#35770;&#12289;&#24212;&#29992;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#65292;&#21457;&#29616;DRL&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03195</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#26426;&#22120;&#35843;&#24230;&#65306;&#26041;&#27861;&#35770;&#12289;&#29616;&#29366;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions. (arXiv:2310.03195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#26426;&#22120;&#35843;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#27604;&#36739;&#65292;&#24635;&#32467;&#20986;&#23427;&#20204;&#30340;&#26041;&#27861;&#35770;&#12289;&#24212;&#29992;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#65292;&#21457;&#29616;DRL&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#35843;&#24230;&#26088;&#22312;&#36890;&#36807;&#28385;&#36275;&#21046;&#36896;&#35268;&#21017;&#21644;&#20316;&#19994;&#35268;&#33539;&#30340;&#21069;&#25552;&#19979;&#65292;&#20248;&#21270;&#20316;&#19994;&#20998;&#37197;&#32473;&#26426;&#22120;&#12290;&#36825;&#31181;&#20248;&#21270;&#21487;&#20197;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#12289;&#25552;&#39640;&#23458;&#25143;&#38656;&#27714;&#30340;&#28385;&#36275;&#24230;&#21644;&#22686;&#24378;&#29983;&#20135;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854; NP-hard &#30340;&#24615;&#36136;&#65292;&#26426;&#22120;&#35843;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#20316;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#33258;1995&#24180;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#25506;&#32034;&#23558;DRL&#24212;&#29992;&#20110;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;&#22522;&#20110;DRL&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#27604;&#36739;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#26041;&#27861;&#35770;&#12289;&#24212;&#29992;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#23427;&#23558;&#36825;&#20123;&#26041;&#27861;&#26681;&#25454;&#35745;&#31639;&#32452;&#20214;&#36827;&#34892;&#20102;&#20998;&#31867;&#65306;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#12289;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#24471;&#20986;&#32467;&#35770;&#65292;&#22522;&#20110;DRL&#30340;&#26041;&#27861;&#20248;&#20110;&#31934;&#30830;&#27714;&#35299;&#22120;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine scheduling aims to optimize job assignments to machines while adhering to manufacturing rules and job specifications. This optimization leads to reduced operational costs, improved customer demand fulfillment, and enhanced production efficiency. However, machine scheduling remains a challenging combinatorial problem due to its NP-hard nature. Deep Reinforcement Learning (DRL), a key component of artificial general intelligence, has shown promise in various domains like gaming and robotics. Researchers have explored applying DRL to machine scheduling problems since 1995. This paper offers a comprehensive review and comparison of DRL-based approaches, highlighting their methodology, applications, advantages, and limitations. It categorizes these approaches based on computational components: conventional neural networks, encoder-decoder architectures, graph neural networks, and metaheuristic algorithms. Our review concludes that DRL-based methods outperform exact solvers, heuristi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20132;&#20114;&#24335;&#27807;&#36890;&#23558;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#25552;&#21462;&#21040;&#19979;&#28216;&#27169;&#22411;&#20013;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24378;&#22823;&#25945;&#24072;&#26080;&#27861;&#20445;&#35777;&#24378;&#22823;&#23398;&#29983;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#20132;&#20114;&#24335;&#27807;&#36890;&#25552;&#39640;&#20102;&#30693;&#35782;&#33976;&#39311;&#22312;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.03188</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;:&#36890;&#36807;&#20132;&#20114;&#24335;&#27807;&#36890;&#23558;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#25552;&#21462;&#21040;&#19979;&#28216;&#27169;&#22411;&#20013;&#30340;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication. (arXiv:2310.03188v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03188
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#27807;&#36890;&#23558;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#25552;&#21462;&#21040;&#19979;&#28216;&#27169;&#22411;&#20013;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24378;&#22823;&#25945;&#24072;&#26080;&#27861;&#20445;&#35777;&#24378;&#22823;&#23398;&#29983;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#20132;&#20114;&#24335;&#27807;&#36890;&#25552;&#39640;&#20102;&#30693;&#35782;&#33976;&#39311;&#22312;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#31361;&#30772;&#37117;&#26159;&#30001;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#23454;&#29616;&#30340;&#12290;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21442;&#25968;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#24050;&#32463;&#34987;&#29992;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;KD&#23558;&#30693;&#35782;&#20174;&#19968;&#20010;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#32473;&#19968;&#20010;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#34429;&#28982;KD&#22312;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24378;&#22823;&#30340;&#25945;&#24072;&#24182;&#19981;&#19968;&#23450;&#20250;&#23548;&#33268;&#24378;&#22823;&#30340;&#23398;&#29983;&#65292;&#22240;&#20026;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#28508;&#22312;&#30340;&#20998;&#24067;&#20559;&#31227;&#21487;&#33021;&#20351;KD&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#23545;&#20110;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19981;&#22815;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20132;&#20114;&#24335;&#27807;&#36890;&#36807;&#31243;&#25193;&#23637;&#20102;KD&#65292;&#20197;&#24110;&#21161;&#23398;&#29983;&#27169;&#22411;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent breakthroughs in machine learning have been enabled by the pre-trained foundation models. By scaling up model parameters, training data, and computation resources, foundation models have significantly advanced the state-of-the-art in many applications. However, it is still an open question of how to use these models to perform downstream tasks efficiently. Knowledge distillation (KD) has been explored to tackle this challenge. KD transfers knowledge from a large teacher model to a smaller student model. While KD has been successful in improving student model performance, recent research has discovered that a powerful teacher does not necessarily lead to a powerful student, due to their huge capacity gap. In addition, the potential distribution shifts between the pre-training data and downstream tasks can make knowledge transfer in KD sub-optimal for improving downstream task performance. In this paper, we extend KD with an interactive communication process to help students 
&lt;/p&gt;</description></item><item><title>&#22823;&#33041;&#20855;&#26377;&#19968;&#31995;&#21015;&#37325;&#22797;&#30340;&#35268;&#33539;&#35745;&#31639;&#21333;&#20803;&#65292;&#20294;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#24067;&#24335;&#30340;&#65292;&#22240;&#27492;&#22914;&#20309;&#23450;&#20041;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20174;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#20013;&#25512;&#26029;&#20986;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#31639;&#27861;&#32423;&#21035;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#36890;&#36807;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#65292;&#21487;&#20197;&#25214;&#21040;&#31070;&#32463;&#27963;&#21160;&#19982;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22240;&#26524;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03186</link><description>&lt;p&gt;
&#25512;&#27979;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Inferring Inference. (arXiv:2310.03186v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03186
&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#20855;&#26377;&#19968;&#31995;&#21015;&#37325;&#22797;&#30340;&#35268;&#33539;&#35745;&#31639;&#21333;&#20803;&#65292;&#20294;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#24067;&#24335;&#30340;&#65292;&#22240;&#27492;&#22914;&#20309;&#23450;&#20041;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20174;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#20013;&#25512;&#26029;&#20986;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#31639;&#27861;&#32423;&#21035;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#36890;&#36807;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#65292;&#21487;&#20197;&#25214;&#21040;&#31070;&#32463;&#27963;&#21160;&#19982;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22240;&#26524;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#30005;&#36335;&#22270;&#26696;&#34920;&#26126;&#22823;&#33041;&#20855;&#26377;&#19968;&#31995;&#21015;&#37325;&#22797;&#30340;&#35268;&#33539;&#35745;&#31639;&#21333;&#20803;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#24067;&#24335;&#30340;&#65292;&#22240;&#27492;&#30456;&#20851;&#35745;&#31639;&#21487;&#33021;&#20165;&#19982;&#21333;&#20010;&#31070;&#32463;&#20803;&#21464;&#25442;&#38388;&#25509;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#23450;&#20041;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#35745;&#31639;&#30340;&#35268;&#33539;&#21644;&#31639;&#27861;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#20174;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#20013;&#25512;&#26029;&#20986;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#35268;&#33539;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#20551;&#35774;&#22823;&#33041;&#21019;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20869;&#37096;&#27169;&#22411;&#65292;&#20551;&#35774;&#35299;&#37322;&#20854;&#24863;&#23448;&#36755;&#20837;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#24863;&#23448;&#36755;&#20837;&#26469;&#25512;&#26029;&#28508;&#22312;&#21407;&#22240;&#12290;&#22312;&#31639;&#27861;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#36825;&#20010;&#25512;&#29702;&#36807;&#31243;&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#36890;&#36807;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#65288;i&#65289;&#31070;&#32463;&#27963;&#21160;&#19982;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22240;&#26524;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patterns of microcircuitry suggest that the brain has an array of repeated canonical computational units. Yet neural representations are distributed, so the relevant computations may only be related indirectly to single-neuron transformations. It thus remains an open challenge how to define canonical distributed computations. We integrate normative and algorithmic theories of neural computation into a mathematical framework for inferring canonical distributed computations from large-scale neural activity patterns. At the normative level, we hypothesize that the brain creates a structured internal model of its environment, positing latent causes that explain its sensory inputs, and uses those sensory inputs to infer the latent causes. At the algorithmic level, we propose that this inference process is a nonlinear message-passing algorithm on a graph-structured model of the world. Given a time series of neural activity during a perceptual inference task, our framework finds (i) the neura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#35270;&#35273;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25191;&#34892;&#25915;&#20987;&#32773;&#24819;&#35201;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;&#25915;&#20987;&#21487;&#20197;&#23548;&#33268;LLM&#21024;&#38500;&#26085;&#21382;&#20107;&#20214;&#12289;&#27844;&#38706;&#31169;&#20154;&#23545;&#35805;&#21644;&#39044;&#35746;&#37202;&#24215;&#65292;&#24182;&#19988;&#20855;&#26377;&#38544;&#34109;&#24615;&#21644;&#36866;&#29992;&#24615;&#24191;&#30340;&#29305;&#28857;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#25915;&#20987;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#25805;&#32437;LLM&#20197;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#27861;&#26469;&#35843;&#29992;&#24037;&#20855;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#24178;&#20928;&#22270;&#20687;&#30340;&#39640;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03185</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21033;&#29992;&#35270;&#35273;&#23545;&#25239;&#24615;&#26679;&#26412;&#35823;&#29992;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Misusing Tools in Large Language Models With Visual Adversarial Examples. (arXiv:2310.03185v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#35270;&#35273;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25191;&#34892;&#25915;&#20987;&#32773;&#24819;&#35201;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;&#25915;&#20987;&#21487;&#20197;&#23548;&#33268;LLM&#21024;&#38500;&#26085;&#21382;&#20107;&#20214;&#12289;&#27844;&#38706;&#31169;&#20154;&#23545;&#35805;&#21644;&#39044;&#35746;&#37202;&#24215;&#65292;&#24182;&#19988;&#20855;&#26377;&#38544;&#34109;&#24615;&#21644;&#36866;&#29992;&#24615;&#24191;&#30340;&#29305;&#28857;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#25915;&#20987;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#25805;&#32437;LLM&#20197;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#27861;&#26469;&#35843;&#29992;&#24037;&#20855;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#24178;&#20928;&#22270;&#20687;&#30340;&#39640;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#22686;&#24378;&#20854;&#20351;&#29992;&#24037;&#20855;&#21644;&#22788;&#29702;&#22810;&#31181;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#33021;&#21147;&#24102;&#26469;&#20102;&#26032;&#30340;&#22909;&#22788;&#65292;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#35270;&#35273;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#24341;&#23548;&#27169;&#22411;&#25191;&#34892;&#25915;&#20987;&#32773;&#24819;&#35201;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;&#20363;&#22914;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#35753;&#21463;&#23475;&#32773;&#30340;LLM&#21024;&#38500;&#26085;&#21382;&#20107;&#20214;&#65292;&#27844;&#38706;&#31169;&#20154;&#23545;&#35805;&#24182;&#39044;&#35746;&#37202;&#24215;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#21516;&#26102;&#24433;&#21709;&#19982;LLM&#36830;&#25509;&#30340;&#29992;&#25143;&#36164;&#28304;&#30340;&#26426;&#23494;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#38544;&#34109;&#24615;&#21644;&#36866;&#29992;&#20110;&#22810;&#20010;&#36755;&#20837;&#25552;&#31034;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#23545;&#25239;&#35757;&#32451;&#26500;&#24314;&#36825;&#20123;&#25915;&#20987;&#65292;&#24182;&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#23545;&#25239;&#22270;&#20687;&#20960;&#20046;&#24635;&#26159;&#65288;&#32422;98%&#65289;&#21487;&#20197;&#25805;&#32437;LLM&#20197;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#27861;&#26469;&#35843;&#29992;&#24037;&#20855;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#24178;&#20928;&#22270;&#20687;&#30340;&#39640;&#30456;&#20284;&#24615;&#65288;&#32422;0.9 SSIM&#65289;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20154;&#24037;&#35780;&#20998;&#21644;&#33258;&#21160;&#21270;&#25351;&#26631;&#65292;&#25105;&#20204;&#21457;&#29616;&#25915;&#20987;&#20250;&#38477;&#20302;LLM&#30340;&#33258;&#28982;&#24615;&#33021;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attack
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#22270;&#21472;&#21152;&#21040;&#22270;&#20687;&#19978;&#65292;&#21487;&#20197;&#30452;&#25509;&#35266;&#23519;&#21040;&#20195;&#29702;&#25152;&#20351;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#26356;&#23481;&#26131;&#35299;&#37322;&#36873;&#25321;&#21160;&#20316;&#32972;&#21518;&#30340;&#36923;&#36753;&#12290;</title><link>http://arxiv.org/abs/2310.03161</link><description>&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#23545;&#20110;&#35782;&#21035;&#26102;&#38388;&#24310;&#38271;&#22686;&#24378;&#23398;&#20064;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Neural architecture impact on identifying temporally extended Reinforcement Learning tasks. (arXiv:2310.03161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#22270;&#21472;&#21152;&#21040;&#22270;&#20687;&#19978;&#65292;&#21487;&#20197;&#30452;&#25509;&#35266;&#23519;&#21040;&#20195;&#29702;&#25152;&#20351;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#26356;&#23481;&#26131;&#35299;&#37322;&#36873;&#25321;&#21160;&#20316;&#32972;&#21518;&#30340;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20851;&#27880;&#27169;&#22411;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;OpenAI Gym Atari-2600&#28216;&#25103;&#22871;&#20214;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103;&#21644;&#21307;&#30103;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#35299;&#37322;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#65292;&#25552;&#21462;&#21644;&#21472;&#21152;&#27880;&#24847;&#21147;&#22270;&#21040;&#22270;&#20687;&#19978;&#65292;&#21487;&#20197;&#30452;&#25509;&#35266;&#23519;&#20195;&#29702;&#20351;&#29992;&#30340;&#20449;&#24687;&#20197;&#36873;&#25321;&#21160;&#20316;&#65292;&#24182;&#26356;&#23481;&#26131;&#35299;&#37322;&#36873;&#25321;&#21160;&#20316;&#32972;&#21518;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;gym-Atari&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36824;&#33021;&#25552;&#20379;&#20851;&#20110;&#20195;&#29702;&#22914;&#20309;&#24863;&#30693;&#20854;&#29615;&#22659;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#20351;&#29992;&#35270;&#35273;&#27880;&#24847;&#21147;&#30340;&#35270;&#39057;&#20998;&#31867;&#27169;&#22411;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
Inspired by recent developments in attention models for image classification and natural language processing, we present various Attention based architectures in reinforcement learning (RL) domain, capable of performing well on OpenAI Gym Atari-2600 game suite. In spite of the recent success of Deep Reinforcement learning techniques in various fields like robotics, gaming and healthcare, they suffer from a major drawback that neural networks are difficult to interpret. We try to get around this problem with the help of Attention based models. In Attention based models, extracting and overlaying of attention map onto images allows for direct observation of information used by agent to select actions and easier interpretation of logic behind the chosen actions. Our models in addition to playing well on gym-Atari environments, also provide insights on how agent perceives its environment. In addition, motivated by recent developments in attention based video-classification models using Vis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#39044;&#27979;&#21306;&#38388;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#21644;&#30456;&#23545;&#20110;&#31354;&#30333;&#21442;&#32771;&#30340;&#25910;&#30410;&#27010;&#24565;&#12290;&#36825;&#31181;&#26041;&#27861;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#30740;&#31350;&#22330;&#26223;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#39044;&#27979;&#21306;&#38388;&#20840;&#38754;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.03158</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;&#19981;&#30830;&#23450;&#29305;&#24449;&#26354;&#32447;&#30340;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Assessment of Prediction Intervals Using Uncertainty Characteristics Curves. (arXiv:2310.03158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#39044;&#27979;&#21306;&#38388;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#21644;&#30456;&#23545;&#20110;&#31354;&#30333;&#21442;&#32771;&#30340;&#25910;&#30410;&#27010;&#24565;&#12290;&#36825;&#31181;&#26041;&#27861;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#30740;&#31350;&#22330;&#26223;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#39044;&#27979;&#21306;&#38388;&#20840;&#38754;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24050;&#32463;&#34987;&#35748;&#20026;&#26159;&#21487;&#20449;AI&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#36890;&#24120;&#20351;&#29992;&#26657;&#20934;&#21040;&#19987;&#26377;&#25805;&#20316;&#28857;&#30340;&#39044;&#27979;&#21306;&#38388;&#26469;&#37327;&#21270;&#65292;&#20174;&#32780;&#20351;&#24471;&#23545;&#19981;&#21516;&#30740;&#31350;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#30456;&#23545;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21033;&#29992;&#20102;(1)&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#30340;&#27010;&#24565;&#21644;(2)&#30456;&#23545;&#20110;&#31354;&#30333;&#21442;&#32771;&#30340;&#25910;&#30410;&#27010;&#24565;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#25805;&#20316;&#28857;&#19981;&#21487;&#30693;&#35780;&#20272;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39044;&#27979;&#21306;&#38388;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19981;&#30830;&#23450;&#29305;&#24449;&#26354;&#32447;&#65292;&#24182;&#22312;&#36873;&#23450;&#22330;&#26223;&#20013;&#23637;&#31034;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23545;&#39044;&#27979;&#21306;&#38388;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24403;&#21069;&#38656;&#27714;&#65292;&#22240;&#27492;&#26159;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24037;&#20855;&#31665;&#30340;&#23453;&#36149;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate quantification of model uncertainty has long been recognized as a fundamental requirement for trusted AI. In regression tasks, uncertainty is typically quantified using prediction intervals calibrated to an ad-hoc operating point, making evaluation and comparison across different studies relatively difficult. Our work leverages: (1) the concept of operating characteristics curves and (2) the notion of a gain over a null reference, to derive a novel operating point agnostic assessment methodology for prediction intervals. The paper defines the Uncertainty Characteristics Curve and demonstrates its utility in selected scenarios. We argue that the proposed method addresses the current need for comprehensive assessment of prediction intervals and thus represents a valuable addition to the uncertainty quantification toolbox.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.03149</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03149
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#26576;&#20123;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#29305;&#24449;&#65292;&#20316;&#20026;&#20854;&#23545;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#19968;&#37096;&#20998;&#12290;&#30001;&#20110;&#25317;&#26377;&#27491;&#30830;&#65288;&#25110;&#38169;&#35823;&#65289;&#30340;&#27010;&#24565;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25105;&#20204;&#24819;&#35201;&#30693;&#36947;&#22312;&#32473;&#23450;&#23618;&#27425;&#19978;&#65292;&#27169;&#22411;&#21407;&#22987;&#35757;&#32451;&#38598;&#20013;&#30340;&#21738;&#20123;&#36755;&#20837;&#23545;&#20110;&#23398;&#20064;&#19968;&#20010;&#27010;&#24565;&#26368;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#25506;&#27979;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#32593;&#32476;&#23618;&#27425;&#19978;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24402;&#22240;&#30340;TRAK&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27010;&#24565;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#31227;&#38500;&#23545;&#19968;&#20010;&#27010;&#24565;&#20855;&#26377;&#26368;&#39640;&#24402;&#22240;&#30340;&#21069;10000&#24352;&#22270;&#20687;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#27010;&#24565;&#22312;&#32593;&#32476;&#20013;&#30340;&#20301;&#32622;&#20197;&#21450;&#27010;&#24565;&#30340;&#25506;&#27979;&#31232;&#30095;&#24615;&#24182;&#27809;&#26377;&#21457;&#29983;&#25913;&#21464;&#12290;&#36825;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#19981;&#21516;&#65292;&#29992;&#20110;&#30830;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23558;&#22810;&#31181;&#21487;&#33021;&#30340;&#24402;&#32435;&#35299;&#37322;&#27719;&#24635;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03131</link><description>&lt;p&gt;
Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23558;&#22810;&#31181;&#21487;&#33021;&#30340;&#24402;&#32435;&#35299;&#37322;&#27719;&#24635;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#21518;&#39564;&#27169;&#22411;&#36817;&#20284;&#35299;&#37322;&#26041;&#27861;&#65288;&#22914;LIME&#21644;SHAP&#65289;&#20581;&#22766;&#24615;&#30340;&#25209;&#35780;&#23548;&#33268;&#20102;&#27169;&#22411;&#31934;&#30830;&#38416;&#37322;&#30340;&#20852;&#36215;&#12290;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#28857;&#65292;&#24402;&#32435;&#35299;&#37322;&#25552;&#20379;&#20102;&#33021;&#22815;&#29983;&#25104;&#32467;&#26524;&#30340;&#26368;&#23567;&#29305;&#24449;&#23376;&#38598;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#19988;&#20005;&#35880;&#30340;&#65292;&#20294;&#26159;&#24402;&#32435;&#35299;&#37322;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064; - &#23545;&#20110;&#21516;&#19968;&#25968;&#25454;&#28857;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#21333;&#19968;&#30340;&#24402;&#32435;&#35299;&#37322;&#21487;&#33021;&#26159;&#19981;&#36275;&#22815;&#30340;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#25552;&#20379;&#25152;&#26377;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#21487;&#33021;&#30001;&#20110;&#20854;&#25968;&#37327;&#24222;&#22823;&#32780;&#38590;&#20197;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22810;&#31181;&#21487;&#33021;&#30340;&#24402;&#32435;&#35299;&#37322;&#27719;&#24635;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65306;&#20004;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#21644;&#19968;&#31181;&#22522;&#20110;&#33879;&#21517;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#20844;&#29702;&#21270;&#34920;&#24449;&#36825;&#19977;&#31181;&#26041;&#27861;&#65292;&#35777;&#26126;&#23427;&#20204;&#27599;&#19968;&#20010;&#37117;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#40657;&#30418;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#32852;&#21512;&#25552;&#31034;&#35843;&#25972;&#65288;Fed-BBPT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25682;&#24323;&#23545;&#21442;&#25968;&#32467;&#26500;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#35775;&#38382;&#30340;&#20381;&#36182;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#20869;&#23384;&#38480;&#21046;&#21644;&#20445;&#25345;&#38544;&#31169;&#24615;&#30340;&#21516;&#26102;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#23616;&#37096;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.03123</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#40657;&#30418;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#32852;&#21512;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models. (arXiv:2310.03123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03123
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#40657;&#30418;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#32852;&#21512;&#25552;&#31034;&#35843;&#25972;&#65288;Fed-BBPT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25682;&#24323;&#23545;&#21442;&#25968;&#32467;&#26500;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#35775;&#38382;&#30340;&#20381;&#36182;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#20869;&#23384;&#38480;&#21046;&#21644;&#20445;&#25345;&#38544;&#31169;&#24615;&#30340;&#21516;&#26102;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#23616;&#37096;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#30340;&#36805;&#29467;&#21457;&#23637;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#35843;&#25972;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#24212;&#29992;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#20851;&#27880;&#28857;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#20869;&#23384;&#38480;&#21046;&#65306;&#24320;&#28304;PTMs&#22823;&#23567;&#30340;&#25345;&#32493;&#22686;&#38271;&#20351;&#24471;&#21363;&#20351;&#23545;&#20854;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#36827;&#34892;&#24494;&#35843;&#20063;&#23545;&#35768;&#22810;&#20174;&#19994;&#32773;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#65288;2&#65289;&#27169;&#22411;&#38544;&#31169;&#24615;&#65306;&#29616;&#26377;&#30340;PTMs&#36890;&#24120;&#20316;&#20026;&#20844;&#20849;API&#26381;&#21153;&#65292;&#20854;&#21442;&#25968;&#26080;&#27861;&#26377;&#25928;&#25110;&#23450;&#21046;&#22320;&#36827;&#34892;&#24494;&#35843;&#12290;&#65288;3&#65289;&#25968;&#25454;&#38544;&#31169;&#24615;&#65306;&#23545;PTMs&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#23616;&#37096;&#21270;&#30340;&#24182;&#19988;&#19981;&#20849;&#20139;&#32473;&#20844;&#20247;&#12290;&#20026;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#38480;&#21046;&#21644;&#20445;&#25345;&#38544;&#31169;&#24615;&#30340;&#21516;&#26102;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#23616;&#37096;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#40657;&#30418;&#25552;&#31034;&#35843;&#25972;&#65288;Fed-BBPT&#65289;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#25682;&#24323;&#20102;&#23545;&#21442;&#25968;&#32467;&#26500;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#35775;&#38382;&#30340;&#20381;&#36182;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the blowout development of pre-trained models (PTMs), the efficient tuning of these models for diverse downstream applications has emerged as a pivotal research concern. Although recent investigations into prompt tuning have provided promising avenues, three salient challenges persist: (1) memory constraint: the continuous growth in the size of open-source PTMs renders fine-tuning, even a fraction of their parameters, challenging for many practitioners. (2) model privacy: existing PTMs often function as public API services, with their parameters inaccessible for effective or tailored fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates high-quality datasets, which are typically localized and not shared to public. To optimally harness each local dataset while navigating memory constraints and preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT). This innovative approach eschews reliance on parameter architectures and private dataset access, ins
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03094</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#31181;&#24378;&#22823;&#30340;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#20351;&#29992;&#20184;&#36153;API&#26381;&#21153;&#30340;&#39640;&#26114;&#36153;&#29992;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#20026;&#20102;&#30740;&#31350;&#26500;&#24314;LLM&#32423;&#32852;&#20197;&#33410;&#32422;&#20351;&#29992;LLM&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#36827;&#34892;&#25512;&#29702;&#65288;&#20363;&#22914;&#25968;&#23398;&#12289;&#22240;&#26524;&#25512;&#29702;&#65289;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#31649;&#36947;&#36981;&#24490;&#19968;&#20010;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#21363;&#31616;&#21333;&#30340;&#38382;&#39064;&#21487;&#20197;&#30001;&#19968;&#20010;&#26356;&#24369;&#20294;&#26356;&#23454;&#24800;&#30340;LLM&#26469;&#35299;&#20915;&#65292;&#32780;&#21482;&#26377;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#25165;&#38656;&#35201;&#26356;&#24378;&#22823;&#12289;&#26356;&#26114;&#36149;&#30340;LLM&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#20915;&#31574;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#26356;&#24369;&#30340;LLM&#30340;&#8220;&#31572;&#26696;&#19968;&#33268;&#24615;&#8221;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31572;&#26696;&#37319;&#26679;&#21644;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#24605;&#32500;&#34920;&#31034;&#65288;&#21363;&#36830;&#32493;&#24605;&#32500;&#21644;&#31243;&#24207;&#24605;&#32500;&#65289;&#30340;&#28151;&#21512;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#21644;GPT-4&#20316;&#20026;&#36739;&#24369;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#37325;&#22823;&#21464;&#38761;&#65292;&#26082;&#33021;&#22815;&#25913;&#21892;DNA&#24207;&#21015;&#20998;&#26512;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#65292;&#20063;&#20026;&#22522;&#22240;&#32452;&#21464;&#24322;&#26816;&#27979;&#21644;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03086</link><description>&lt;p&gt;
&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning in Computational Biology: Advancements, Challenges, and Future Outlook. (arXiv:2310.03086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03086
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#37325;&#22823;&#21464;&#38761;&#65292;&#26082;&#33021;&#22815;&#25913;&#21892;DNA&#24207;&#21015;&#20998;&#26512;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#65292;&#20063;&#20026;&#22522;&#22240;&#32452;&#21464;&#24322;&#26816;&#27979;&#21644;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#38761;&#26032;&#20102;&#29983;&#29289;&#25968;&#25454;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#22312;&#25105;&#20204;&#30340;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20854;&#21382;&#21490;&#12289;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20004;&#20010;&#20027;&#35201;&#24212;&#29992;&#39046;&#22495;&#65306;DNA&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#65292;&#20197;&#21450;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;&#35201;&#20805;&#20998;&#21457;&#25381;&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#30340;&#28508;&#21147;&#65292;&#20851;&#38190;&#26159;&#35299;&#20915;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#23545;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#20998;&#26512;DNA&#24207;&#21015;&#26041;&#38754;&#30340;&#24212;&#29992;&#24050;&#32463;&#22312;&#22522;&#22240;&#32452;&#21464;&#24322;&#26816;&#27979;&#21644;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#21464;&#38761;&#65292;&#20174;&#32780;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has become a powerful tool in computational biology, revolutionising the analysis and interpretation of biological data over time. In our article review, we delve into various aspects of deep learning in computational biology. Specifically, we examine its history, advantages, and challenges. Our focus is on two primary applications: DNA sequence classification and prediction, as well as protein structure prediction from sequence data. Additionally, we provide insights into the outlook for this field. To fully harness the potential of deep learning in computational biology, it is crucial to address the challenges that come with it. These challenges include the requirement for large, labelled datasets and the interpretability of deep learning models. The use of deep learning in the analysis of DNA sequences has brought about a significant transformation in the detection of genomic variants and the analysis of gene expression. This has greatly contributed to the advancement 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65292;&#21363;&#36127;&#36131;&#32534;&#30721;&#29305;&#23450;&#30693;&#35782;&#30340;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21448;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.03084</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Discovering Knowledge-Critical Subnetworks in Pretrained Language Models. (arXiv:2310.03084v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65292;&#21363;&#36127;&#36131;&#32534;&#30721;&#29305;&#23450;&#30693;&#35782;&#30340;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21448;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#38544;&#21547;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23450;&#20301;&#36825;&#20123;&#34920;&#31034;&#24182;&#23558;&#20854;&#35299;&#31163;&#20986;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21253;&#21547;&#20102;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65306;&#36127;&#36131;&#32534;&#30721;&#27169;&#22411;&#25152;&#35760;&#24518;&#30340;&#29305;&#23450;&#30693;&#35782;&#30340;&#29305;&#23450;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#26469;&#21457;&#29616;&#36825;&#20123;&#23376;&#32593;&#32476;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#26469;&#31934;&#30830;&#22320;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;GPT2&#21464;&#20307;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#39640;&#24230;&#31232;&#30095;&#23376;&#32593;&#32476;&#65288;98%+&#65289;&#65292;&#23427;&#20204;&#20165;&#36127;&#36131;&#29305;&#23450;&#30340;&#20851;&#31995;&#30693;&#35782;&#38598;&#21512;&#12290;&#24403;&#21024;&#38500;&#36825;&#20123;&#23376;&#32593;&#32476;&#26102;&#65292;&#21097;&#20313;&#30340;&#32593;&#32476;&#20173;&#20445;&#25345;&#20102;&#22823;&#37096;&#20998;&#20854;&#21021;&#22987;&#23481;&#37327;&#65288;&#23545;&#35821;&#35328;&#21644;&#20854;&#20182;&#35760;&#24518;&#20851;&#31995;&#30340;&#24314;&#27169;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized rel
&lt;/p&gt;</description></item><item><title>Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03059</link><description>&lt;p&gt;
Point-PEFT: &#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03059
&lt;/p&gt;
&lt;p&gt;
Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27969;&#34892;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31561;&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#25104;&#26412;&#65292;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#35328;&#21644;2D&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19987;&#38376;PEFT&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-PEFT&#65292;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#28857;&#20113;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;3D&#27169;&#22411;&#65292;&#25105;&#20204;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#12290;Point-prior Prompt&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20869;&#23384;&#24211;&#26469;&#22686;&#24378;&#25552;&#31034;&#26631;&#35760;&#30340;&#21442;&#25968;&#26080;&#20851;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;Geometry-aware Adapter&#26088;&#22312;&#23545;&#19981;&#21516;&#20219;&#21153;&#25110;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;LAB&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#25628;&#32034;&#31354;&#38388;&#32553;&#20943;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#27714;&#35299;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#31283;&#20581;&#24615;&#21644;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#33021;&#21147;&#65292;&#21516;&#26102;&#33021;&#22815;&#35299;&#20915;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03055</link><description>&lt;p&gt;
&#20462;&#25913;&#21518;&#30340;LAB&#31639;&#27861;&#21450;&#22522;&#20110;&#32858;&#31867;&#30340;&#25628;&#32034;&#31354;&#38388;&#32553;&#20943;&#26041;&#27861;&#22312;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#27714;&#35299;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems. (arXiv:2310.03055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;LAB&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#25628;&#32034;&#31354;&#38388;&#32553;&#20943;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#27714;&#35299;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#31283;&#20581;&#24615;&#21644;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#33021;&#21147;&#65292;&#21516;&#26102;&#33021;&#22815;&#35299;&#20915;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;LAB&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#24314;&#31435;&#22312;&#21407;&#22987;&#30340;LAB&#31639;&#27861;&#65288;Reddy&#31561;&#20154;&#65292;2023&#24180;&#65289;&#22522;&#30784;&#19978;&#65292;&#35813;&#31639;&#27861;&#27169;&#25311;&#20102;&#32676;&#20307;&#20869;&#37096;&#30340;&#31454;&#20105;&#21644;&#23398;&#20064;&#34892;&#20026;&#65292;&#24314;&#31435;&#20102;&#23618;&#27425;&#35282;&#33394;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#36718;&#30424;&#36172;&#26041;&#27861;&#21644;&#32553;&#20943;&#22240;&#23376;&#65292;&#24341;&#20837;&#20102;&#32676;&#32452;&#38388;&#31454;&#20105;&#65292;&#24182;&#36880;&#27493;&#32553;&#23567;&#26679;&#26412;&#31354;&#38388;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;CEC 2005&#21644;CEC 2017&#30340;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#36827;&#34892;&#39564;&#35777;&#12290;&#20351;&#29992;&#21452;&#36793;&#21644;&#25104;&#23545;&#31526;&#21495;&#31209;Wilcoxon&#27979;&#35797;&#20197;&#21450;Friedman&#31209;&#27979;&#35797;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#39564;&#35777;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#25913;&#36827;&#21644;&#21331;&#36234;&#30340;&#31283;&#20581;&#24615;&#20197;&#21450;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#25628;&#32034;&#31354;&#38388;&#32553;&#20943;&#65288;C-SSR&#65289;&#26041;&#27861;&#65292;&#20351;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#32422;&#26463;&#38382;&#39064;&#12290;C-SSR&#26041;&#27861;&#20351;&#31639;&#27861;&#33021;&#22815;&#35782;&#21035;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21487;&#34892;&#21306;&#22495;&#20043;&#38388;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
A modified LAB algorithm is introduced in this paper. It builds upon the original LAB algorithm (Reddy et al. 2023), which is a socio-inspired algorithm that models competitive and learning behaviours within a group, establishing hierarchical roles. The proposed algorithm incorporates the roulette wheel approach and a reduction factor introducing inter-group competition and iteratively narrowing down the sample space. The algorithm is validated by solving the benchmark test problems from CEC 2005 and CEC 2017. The solutions are validated using standard statistical tests such as two-sided and pairwise signed rank Wilcoxon test and Friedman rank test. The algorithm exhibited improved and superior robustness as well as search space exploration capabilities. Furthermore, a Clustering-Based Search Space Reduction (C-SSR) method is proposed, making the algorithm capable to solve constrained problems. The C-SSR method enables the algorithm to identify clusters of feasible regions, satisfying 
&lt;/p&gt;</description></item><item><title>Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03052</link><description>&lt;p&gt;
Memoria: &#29992;&#20110;&#31867;&#20154;&#39034;&#24207;&#22788;&#29702;&#30340;&#28023;&#27604;&#23433;&#35760;&#24518;&#20307;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03052
&lt;/p&gt;
&lt;p&gt;
Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#23481;&#37327;&#65292;Transformer &#24456;&#38590;&#22788;&#29702;&#38271;&#36755;&#20837;&#24207;&#21015;&#12290;&#34429;&#28982;&#22686;&#21152;&#36755;&#20837;&#38271;&#24230;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26080;&#27490;&#22659;&#22320;&#22686;&#21152;&#38271;&#24230;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#27492;&#22806;&#65292;&#19982; Transformer &#19981;&#21516;&#65292;&#20154;&#31867;&#26377;&#36873;&#25321;&#24615;&#22320;&#35760;&#20303;&#21644;&#20351;&#29992;&#20165;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#21040;&#23614;&#22788;&#29702;&#25152;&#26377;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Memoria&#65292;&#19968;&#20010;&#24212;&#29992;&#28023;&#27604;&#23433;&#35760;&#24518;&#24418;&#25104;&#29702;&#35770;&#30340;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;Memoria &#22312;&#24037;&#20316;&#35760;&#24518;&#12289;&#30701;&#26399;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#30340;&#22810;&#20010;&#35760;&#24518;&#23618;&#32423;&#19978;&#23384;&#20648;&#21644;&#26816;&#32034;&#31216;&#20026; engram &#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#12290;&#36890;&#36807;&#19982;&#35832;&#22914; BERT &#21644; GPT &#31561;&#27969;&#34892;&#30340;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986; Memoria &#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Resul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65306;&#24605;&#32771;&#20026;&#20102;&#34892;&#21160;&#65288;T4D&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#25797;&#38271;&#36319;&#36394;&#35282;&#33394;&#30340;&#20449;&#24565;&#65292;&#20294;&#22312;&#23558;&#36825;&#20123;&#25512;&#26029;&#36716;&#21270;&#20026;&#25112;&#30053;&#34892;&#21160;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#38544;&#21547;&#30340;&#20851;&#20110;&#24515;&#29702;&#29366;&#24577;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#35810;&#38382;&#65292;&#36825;&#20250;&#23548;&#33268;&#27491;&#30830;&#30340;&#34892;&#21160;&#36873;&#25321;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.03051</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20855;&#26377;&#24515;&#28789;&#29702;&#35770;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How FaR Are Large Language Models From Agents with Theory-of-Mind?. (arXiv:2310.03051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65306;&#24605;&#32771;&#20026;&#20102;&#34892;&#21160;&#65288;T4D&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#25797;&#38271;&#36319;&#36394;&#35282;&#33394;&#30340;&#20449;&#24565;&#65292;&#20294;&#22312;&#23558;&#36825;&#20123;&#25512;&#26029;&#36716;&#21270;&#20026;&#25112;&#30053;&#34892;&#21160;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#38544;&#21547;&#30340;&#20851;&#20110;&#24515;&#29702;&#29366;&#24577;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#35810;&#38382;&#65292;&#36825;&#20250;&#23548;&#33268;&#27491;&#30830;&#30340;&#34892;&#21160;&#36873;&#25321;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#24605;&#32771;&#26159;&#20026;&#20102;&#34892;&#21160;&#12290;&#8221;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#26469;&#25512;&#26029;&#20182;&#20154;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#36825;&#31181;&#33021;&#21147;&#34987;&#31216;&#20026;&#24515;&#28789;&#29702;&#35770;&#65288;Theory-of-Mind&#65292;ToM&#65289;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#34892;&#20107;&#12290;&#29616;&#26377;&#30340;&#38382;&#31572;&#22522;&#20934;&#65288;&#22914;ToMi&#65289;&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#25925;&#20107;&#20013;&#35282;&#33394;&#30340;&#20449;&#24565;&#36827;&#34892;&#25512;&#26029;&#65292;&#20294;&#24182;&#19981;&#27979;&#35797;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21033;&#29992;&#36825;&#20123;&#25512;&#26029;&#26469;&#25351;&#23548;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#30340;&#35780;&#20272;&#33539;&#24335;&#65306;&#24605;&#32771;&#20026;&#20102;&#34892;&#21160;&#65288;T4D&#65289;&#65292;&#36825;&#35201;&#27714;&#27169;&#22411;&#23558;&#20851;&#20110;&#20182;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#25512;&#26029;&#19982;&#31038;&#20132;&#22330;&#26223;&#20013;&#30340;&#34892;&#21160;&#32852;&#31995;&#36215;&#26469;&#12290;T4D&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;GPT-4&#21644;PaLM 2&#31561;LLMs&#20284;&#20046;&#25797;&#38271;&#36319;&#36394;&#25925;&#20107;&#20013;&#35282;&#33394;&#30340;&#20449;&#24565;&#65292;&#20294;&#20182;&#20204;&#22312;&#23558;&#36825;&#31181;&#33021;&#21147;&#36716;&#21270;&#20026;&#25112;&#30053;&#34892;&#21160;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#38754;&#20020;&#30340;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#38544;&#21547;&#30340;&#20851;&#20110;&#24515;&#29702;&#29366;&#24577;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#26159;&#20687;ToMi&#20013;&#37027;&#26679;&#26126;&#30830;&#35810;&#38382;&#65292;&#36825;&#20250;&#23548;&#33268;&#27491;&#30830;&#30340;&#34892;&#21160;&#36873;&#25321;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
"Thinking is for Doing." Humans can infer other people's mental states from observations--an ability called Theory-of-Mind (ToM)--and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct acti
&lt;/p&gt;</description></item><item><title>EcoAssistant&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#20351;LLM&#21161;&#25163;&#26356;&#32463;&#27982;&#12289;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#20195;&#30721;&#39537;&#21160;&#30340;&#26597;&#35810;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;&#19982;&#33258;&#21160;&#20195;&#30721;&#25191;&#34892;&#22120;&#23545;&#35805;&#21644;&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#30340;LLM&#21161;&#25163;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03046</link><description>&lt;p&gt;
EcoAssistant: &#26356;&#32463;&#27982;&#12289;&#26356;&#20934;&#30830;&#22320;&#20351;&#29992;LLM&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
EcoAssistant: Using LLM Assistant More Affordably and Accurately. (arXiv:2310.03046v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03046
&lt;/p&gt;
&lt;p&gt;
EcoAssistant&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#20351;LLM&#21161;&#25163;&#26356;&#32463;&#27982;&#12289;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#20195;&#30721;&#39537;&#21160;&#30340;&#26597;&#35810;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;&#19982;&#33258;&#21160;&#20195;&#30721;&#25191;&#34892;&#22120;&#23545;&#35805;&#21644;&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#30340;LLM&#21161;&#25163;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#29992;&#25143;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#21161;&#25163;&#26469;&#22238;&#31572;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#30340;&#26597;&#35810;&#65307;&#20182;&#20204;&#35810;&#38382;&#29305;&#23450;&#22478;&#24066;&#30340;&#22825;&#27668;&#24773;&#20917;&#65292;&#32929;&#31080;&#20215;&#26684;&#65292;&#29978;&#33267;&#35810;&#38382;&#33258;&#24049;&#38468;&#36817;&#30340;&#29305;&#23450;&#22320;&#28857;&#22312;&#21738;&#37324;&#12290;&#36825;&#20123;&#26597;&#35810;&#35201;&#27714;LLM&#29983;&#25104;&#35843;&#29992;&#22806;&#37096;API&#30340;&#20195;&#30721;&#26469;&#22238;&#31572;&#29992;&#25143;&#30340;&#38382;&#39064;&#65292;&#20294;LLM&#24456;&#23569;&#33021;&#19968;&#27425;&#24615;&#29983;&#25104;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#25191;&#34892;&#32467;&#26524;&#19978;&#36827;&#34892;&#36845;&#20195;&#30340;&#20195;&#30721;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;LLM&#21161;&#25163;&#25903;&#25345;&#39640;&#26597;&#35810;&#37327;&#21487;&#33021;&#20250;&#24456;&#26114;&#36149;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;EcoAssistant&#65292;&#20351;LLM&#33021;&#22815;&#26356;&#32463;&#27982;&#12289;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#20195;&#30721;&#39537;&#21160;&#30340;&#26597;&#35810;&#12290;EcoAssistant&#21253;&#21547;&#19977;&#20010;&#32452;&#20214;&#12290;&#39318;&#20808;&#65292;&#23427;&#20801;&#35768;LLM&#21161;&#25163;&#19982;&#33258;&#21160;&#20195;&#30721;&#25191;&#34892;&#22120;&#23545;&#35805;&#65292;&#20197;&#36845;&#20195;&#20248;&#21270;&#20195;&#30721;&#25110;&#26681;&#25454;&#25191;&#34892;&#32467;&#26524;&#29983;&#25104;&#31572;&#26696;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;LLM&#21161;&#25163;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#22312;&#23581;&#35797;&#29992;&#36739;&#24369;&#12289;&#26356;&#20415;&#23452;&#30340;LLM&#22238;&#31572;&#26597;&#35810;&#20043;&#21069;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, users ask Large language models (LLMs) as assistants to answer queries that require external knowledge; they ask about the weather in a specific city, about stock prices, and even about where specific locations are within their neighborhood. These queries require the LLM to produce code that invokes external APIs to answer the user's question, yet LLMs rarely produce correct code on the first try, requiring iterative code refinement upon execution results. In addition, using LLM assistants to support high query volumes can be expensive. In this work, we contribute a framework, EcoAssistant, that enables LLMs to answer code-driven queries more affordably and accurately. EcoAssistant contains three components. First, it allows the LLM assistants to converse with an automatic code executor to iteratively refine code or to produce answers based on the execution results. Second, we use a hierarchy of LLM assistants, which attempts to answer the query with weaker, cheaper LLMs before 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#20114;&#24335;&#25628;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#21477;&#32423;&#21453;&#39304;&#20449;&#24687;&#26469;&#25552;&#39640;&#25628;&#32034;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36866;&#24212;&#26368;&#26032;&#30340;BERT-based&#27169;&#22411;&#36827;&#34892;&#20851;&#38190;&#21477;&#23376;&#36873;&#25321;&#21644;&#39033;&#30446;&#25490;&#24207;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#28385;&#24847;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03043</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#20114;&#24335;&#25628;&#32034;&#26041;&#27861;&#19982;&#21477;&#32423;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback. (arXiv:2310.03043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#20114;&#24335;&#25628;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#21477;&#32423;&#21453;&#39304;&#20449;&#24687;&#26469;&#25552;&#39640;&#25628;&#32034;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36866;&#24212;&#26368;&#26032;&#30340;BERT-based&#27169;&#22411;&#36827;&#34892;&#20851;&#38190;&#21477;&#23376;&#36873;&#25321;&#21644;&#39033;&#30446;&#25490;&#24207;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#28385;&#24847;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#29992;&#25143;&#30340;&#20132;&#20114;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#20307;&#39564;&#12290;&#36825;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25628;&#32034;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#36991;&#20813;&#26080;&#20851;&#20449;&#24687;&#24182;&#25429;&#25417;&#29992;&#25143;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#29616;&#26377;&#30340;&#26368;&#26032;&#31995;&#32479;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#26469;&#25972;&#21512;&#36825;&#20123;&#20132;&#20114;&#65292;&#20294;&#26159;&#24573;&#30053;&#20102;&#21477;&#32423;&#21453;&#39304;&#20013;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21453;&#39304;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;RL&#34892;&#21160;&#31354;&#38388;&#25506;&#32034;&#21644;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#65288;DQ&#65289;&#26041;&#27861;DQrank&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;DQrank&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#26032;&#25216;&#26415;BERT-based&#27169;&#22411;&#26469;&#36873;&#25321;&#20851;&#38190;&#21477;&#23376;&#65292;&#24182;&#22522;&#20110;&#29992;&#25143;&#30340;&#21442;&#19982;&#24230;&#23545;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#65292;&#20197;&#33719;&#24471;&#26356;&#28385;&#24847;&#30340;&#22238;&#24212;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26426;&#21046;&#26469;&#26356;&#22909;&#22320;&#25506;&#32034;&#26368;&#20248;&#34892;&#21160;&#12290;DQrank&#36824;&#21033;&#29992;DQ&#20013;&#30340;&#32463;&#39564;&#37325;&#29616;&#26426;&#21046;&#26469;&#23384;&#20648;&#21453;&#39304;&#21477;&#23376;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive search can provide a better experience by incorporating interaction feedback from the users. This can significantly improve search accuracy as it helps avoid irrelevant information and captures the users' search intents. Existing state-of-the-art (SOTA) systems use reinforcement learning (RL) models to incorporate the interactions but focus on item-level feedback, ignoring the fine-grained information found in sentence-level feedback. Yet such feedback requires extensive RL action space exploration and large amounts of annotated data. This work addresses these challenges by proposing a new deep Q-learning (DQ) approach, DQrank. DQrank adapts BERT-based models, the SOTA in natural language processing, to select crucial sentences based on users' engagement and rank the items to obtain more satisfactory responses. We also propose two mechanisms to better explore optimal actions. DQrank further utilizes the experience replay mechanism in DQ to store the feedback sentences to ob
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#31995;&#32479;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#29366;&#24577;&#21644;&#21160;&#20316;&#21040;&#37327;&#23376;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#22686;&#24378;&#22411;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#38271;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#26368;&#22823;&#21270;&#65292;&#20174;&#21021;&#22987;&#24577;&#20934;&#30830;&#28436;&#21270;&#21040;&#30446;&#26631;&#24577;&#65292;&#24182;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;1&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03036</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#31995;&#32479;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A quantum system control method based on enhanced reinforcement learning. (arXiv:2310.03036v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03036
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#31995;&#32479;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#29366;&#24577;&#21644;&#21160;&#20316;&#21040;&#37327;&#23376;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#22686;&#24378;&#22411;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#38271;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#26368;&#22823;&#21270;&#65292;&#20174;&#21021;&#22987;&#24577;&#20934;&#30830;&#28436;&#21270;&#21040;&#30446;&#26631;&#24577;&#65292;&#24182;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;1&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#37327;&#23376;&#31995;&#32479;&#25511;&#21046;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#19981;&#21516;&#30340;&#38480;&#21046;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#26465;&#20214;&#19979;&#23481;&#26131;&#23548;&#33268;&#27844;&#38706;&#21644;&#38543;&#26426;&#25511;&#21046;&#35823;&#24046;&#12290;&#24378;&#21270;&#23398;&#20064;&#34987;&#35777;&#26126;&#26159;&#23436;&#25104;&#37327;&#23376;&#31995;&#32479;&#25511;&#21046;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#31995;&#32479;&#25511;&#21046;&#26041;&#27861;&#65288;QSC-ERL&#65289;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#29366;&#24577;&#21644;&#21160;&#20316;&#34987;&#26144;&#23556;&#21040;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#37327;&#23376;&#24577;&#21644;&#25511;&#21046;&#25805;&#20316;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#22686;&#24378;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#38271;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#26368;&#22823;&#21270;&#65292;&#20174;&#21021;&#22987;&#24577;&#20934;&#30830;&#22320;&#28436;&#21270;&#21040;&#30446;&#26631;&#24577;&#12290;&#26681;&#25454;&#20505;&#36873;&#37193;&#25805;&#20316;&#30340;&#25968;&#37327;&#65292;&#37319;&#29992;&#19977;&#24320;&#20851;&#25511;&#21046;&#36827;&#34892;&#20223;&#30495;&#23454;&#39564;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;QSC-ERL&#23454;&#29616;&#20102;&#25509;&#36817;1&#30340;&#20445;&#30495;&#24230;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional quantum system control methods often face different constraints, and are easy to cause both leakage and stochastic control errors under the condition of limited resources. Reinforcement learning has been proved as an efficient way to complete the quantum system control task. To learn a satisfactory control strategy under the condition of limited resources, a quantum system control method based on enhanced reinforcement learning (QSC-ERL) is proposed. The states and actions in reinforcement learning are mapped to quantum states and control operations in quantum systems. By using new enhanced neural networks, reinforcement learning can quickly achieve the maximization of long-term cumulative rewards, and a quantum state can be evolved accurately from an initial state to a target state. According to the number of candidate unitary operations, the three-switch control is used for simulation experiments. Compared with other methods, the QSC-ERL achieves close to 1 fidelity learn
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#22522;&#20934;&#38382;&#39064;&#26469;&#35780;&#20272;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#27169;&#22411;&#30340;&#26412;&#22320;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38382;&#39064;&#32771;&#34385;&#20102;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#37327;&#12289;&#36755;&#20837;&#32500;&#24230;&#21644;&#21306;&#22495;&#25968;&#37327;&#65292;&#24182;&#25361;&#25112;&#20102;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2310.03033</link><description>&lt;p&gt;
&#25552;&#21319;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#30340;&#39640;&#20934;&#30830;&#24615;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#22320;&#31283;&#20581;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition. (arXiv:2310.03033v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#22522;&#20934;&#38382;&#39064;&#26469;&#35780;&#20272;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#27169;&#22411;&#30340;&#26412;&#22320;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38382;&#39064;&#32771;&#34385;&#20102;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#37327;&#12289;&#36755;&#20837;&#32500;&#24230;&#21644;&#21306;&#22495;&#25968;&#37327;&#65292;&#24182;&#25361;&#25112;&#20102;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#26631;&#24535;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#23545;&#36947;&#36335;&#23433;&#20840;&#21644;&#20132;&#36890;&#31649;&#29702;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30001;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#24615;&#65292;&#22914;&#23545;&#25239;&#24615;&#31034;&#20363;&#21644;&#36974;&#25377;&#31561;&#65292;&#20934;&#30830;&#30340;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#22312;&#26500;&#24314;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20998;&#31867;&#22120;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#30340;&#39640;&#20934;&#30830;&#24615;BNN&#27169;&#22411;&#65292;&#37325;&#28857;&#26159;&#32039;&#20945;&#30340;&#22823;&#23567;&#65292;&#20197;&#36866;&#24212;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#36164;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#30340;&#26412;&#22320;&#31283;&#20581;&#24615;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#32452;&#22522;&#20934;&#38382;&#39064;&#65292;&#20854;&#20013;&#30340;&#23618;&#25361;&#25112;&#20102;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;&#36825;&#20123;&#23618;&#21253;&#25324;&#20108;&#36827;&#21046;&#21367;&#31215;&#12289;&#26368;&#22823;&#27744;&#21270;&#12289;&#25209;&#24402;&#19968;&#21270;&#21644;&#20840;&#36830;&#25509;&#23618;&#12290;&#39564;&#35777;&#38382;&#39064;&#30340;&#22256;&#38590;&#31243;&#24230;&#30001;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#37327;&#65288;905k-1.7M&#65289;&#12289;&#36755;&#20837;&#32500;&#24230;&#65288;2.7k-12k&#65289;&#21644;&#21306;&#22495;&#25968;&#37327;&#65288;43&#65289;&#20197;&#21450;&#31070;&#32463;&#20803;&#30340;&#20107;&#23454;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic signs play a critical role in road safety and traffic management for autonomous driving systems. Accurate traffic sign classification is essential but challenging due to real-world complexities like adversarial examples and occlusions. To address these issues, binary neural networks offer promise in constructing classifiers suitable for resource-constrained devices.  In our previous work, we proposed high-accuracy BNN models for traffic sign recognition, focusing on compact size for limited computation and energy resources. To evaluate their local robustness, this paper introduces a set of benchmark problems featuring layers that challenge state-of-the-art verification tools. These layers include binarized convolutions, max pooling, batch normalization, fully connected. The difficulty of the verification problem is given by the high number of network parameters (905k - 1.7 M), of the input dimension (2.7k-12k), and of the number of regions (43) as well by the fact that the neur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03031</link><description>&lt;p&gt;
ChatGPT&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#26377;&#22810;&#26222;&#36941;&#65311;&#8212;&#8212; &#25506;&#32034;&#24503;&#35821;&#21644;&#33521;&#35821;ChatGPT&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;OpenAI&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20379;&#20855;&#26377;&#26377;&#38480;IT&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#32972;&#26223;&#30340;&#29992;&#25143;&#21487;&#33021;&#32570;&#20047;&#23545;LLM&#30340;&#36866;&#24403;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#32570;&#20047;&#23545;&#20854;&#22266;&#26377;&#38480;&#21046;&#30340;&#24847;&#35782;&#65292;&#23558;&#25509;&#21463;&#31995;&#32479;&#36755;&#20986;&#30340;&#34920;&#38754;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#36755;&#20837;&#25552;&#31034;&#21644;&#29983;&#25104;&#30340;&#22238;&#24212;&#65292;&#20197;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#29992;&#25143;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#38656;&#35201;&#24847;&#35782;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;ChatGPT&#22312;&#33521;&#35821;&#21644;&#24503;&#35821;&#20013;&#30340;&#21453;&#24212;&#65292;&#24182;&#25552;&#20379;&#20102;&#22899;&#24615;&#12289;&#30007;&#24615;&#25110;&#20013;&#31435;&#35282;&#24230;&#30340;&#25351;&#20196;&#26102;&#65292;&#22238;&#22797;&#30340;&#26159;&#21542;&#26377;&#24046;&#24322;&#12290;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#36873;&#25321;&#30340;&#25552;&#31034;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#22312;&#30456;&#21516;&#26041;&#24335;&#19979;&#22810;&#27425;&#25552;&#20379;&#25351;&#20196;&#26102;&#22238;&#24212;&#30340;&#24046;&#24322;&#31243;&#24230;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#26085;&#24120;&#24037;&#20316;&#25991;&#26412;&#65292;ChatGPT&#30830;&#23454;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#28982;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#35201;&#24847;&#35782;&#21040;&#65292;&#24403;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#21040;&#20854;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
&lt;/p&gt;</description></item><item><title>SAF&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#28201;&#24230;&#36229;&#21442;&#25968;&#23545;&#21407;&#23376;&#21152;&#26435;&#65292;&#24182;&#35777;&#26126;&#20854;&#21487;&#20197;&#25552;&#39640;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#25581;&#31034;&#37325;&#35201;&#30340;&#21407;&#23376;&#12290;</title><link>http://arxiv.org/abs/2310.03028</link><description>&lt;p&gt;
SAF: &#26234;&#33021;&#32858;&#21512;&#26694;&#26550;&#65292;&#25581;&#31034;&#21407;&#23376;&#37325;&#35201;&#24615;&#25490;&#21517;&#24182;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#39044;&#27979;&#29575;
&lt;/p&gt;
&lt;p&gt;
SAF: Smart Aggregation Framework for Revealing Atoms Importance Rank and Improving Prediction Rates in Drug Discovery. (arXiv:2310.03028v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03028
&lt;/p&gt;
&lt;p&gt;
SAF&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#28201;&#24230;&#36229;&#21442;&#25968;&#23545;&#21407;&#23376;&#21152;&#26435;&#65292;&#24182;&#35777;&#26126;&#20854;&#21487;&#20197;&#25552;&#39640;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#25581;&#31034;&#37325;&#35201;&#30340;&#21407;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#34920;&#31034;&#23398;&#20064;&#65292;&#26377;&#28508;&#21147;&#36890;&#36807;&#22312;&#30789;&#22522;&#20013;&#31579;&#36873;&#22823;&#37327;&#21270;&#23398;&#31354;&#38388;&#26469;&#20419;&#36827;&#33647;&#29289;&#21457;&#29616;&#12290;&#34920;&#31034;&#20998;&#23376;&#30340;&#19968;&#31181;&#25104;&#21151;&#26041;&#27861;&#26159;&#23558;&#20854;&#35270;&#20026;&#22270;&#24418;&#24182;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31867;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#38656;&#35201;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#21407;&#23376;&#26469;&#34920;&#31034;&#21270;&#21512;&#29289;&#65292;&#36825;&#38656;&#35201;&#23545;&#21407;&#23376;&#20449;&#24687;&#36827;&#34892;&#32858;&#21512;&#12290;&#24120;&#35265;&#30340;&#32858;&#21512;&#25805;&#20316;&#65292;&#22914;&#21462;&#24179;&#22343;&#65292;&#20250;&#23548;&#33268;&#22312;&#21407;&#23376;&#32423;&#21035;&#19978;&#20002;&#22833;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#21407;&#23376;&#37117;&#20351;&#29992;&#31867;&#20284;&#20110;&#28201;&#24230;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#38750;&#32447;&#24615;&#21152;&#26435;&#65292;&#37319;&#29992;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#36825;&#31181;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#40644;&#37329;&#26631;&#20934;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#25239;&#29983;&#32032;&#27963;&#24615;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25913;&#21464;&#28201;&#24230;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24179;&#28369;&#22320;&#25581;&#31034;&#23545;&#27963;&#24615;&#39044;&#27979;&#37325;&#35201;&#30340;&#21407;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning, and representation learning in particular, has the potential to facilitate drug discovery by screening a large chemical space in silico. A successful approach for representing molecules is to treat them as a graph and utilize graph neural networks. One of the key limitations of such methods is the necessity to represent compounds with different numbers of atoms, which requires aggregating the atom's information. Common aggregation operators, such as averaging, result in loss of information at the atom level. In this work, we propose a novel aggregating approach where each atom is weighted non-linearly using the Boltzmann distribution with a hyperparameter analogous to temperature. We show that using this weighted aggregation improves the ability of the gold standard message-passing neural network to predict antibiotic activity. Moreover, by changing the temperature hyperparameter, our approach can reveal the atoms that are important for activity prediction in a smooth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SYNFUSION&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#21516;&#34701;&#21512;GNN&#21644;Transformer&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03027</link><description>&lt;p&gt;
&#22270;&#21644;Transformer&#29305;&#24449;&#30340;&#21327;&#21516;&#34701;&#21512;&#21487;&#22686;&#24378;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Synergistic Fusion of Graph and Transformer Features for Enhanced Molecular Property Prediction. (arXiv:2310.03027v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SYNFUSION&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#21516;&#34701;&#21512;GNN&#21644;Transformer&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26159;&#35745;&#31639;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;Transformer&#30340;&#36827;&#23637;&#34920;&#26126;&#23427;&#20204;&#26159;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#30340;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;Transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#26126;&#30830;&#32771;&#34385;&#24213;&#23618;&#20998;&#23376;&#32467;&#26500;&#26041;&#38754;&#19981;&#36275;&#65292;&#32780;&#20165;&#20381;&#38752;GNN&#29305;&#24449;&#34920;&#31034;&#19981;&#36275;&#20197;&#25429;&#25417;&#32454;&#31890;&#24230;&#21644;&#38544;&#34255;&#30340;&#20998;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#29305;&#24449;&#65292;&#36825;&#20351;&#24471;&#30456;&#20284;&#20998;&#23376;&#20043;&#38388;&#38590;&#20197;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SYNFUSION&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21327;&#21516;&#32452;&#21512;&#20102;&#26469;&#33258;GNN&#21644;Transformer&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#25429;&#25417;&#20102;&#20840;&#23616;&#20998;&#23376;&#32467;&#26500;&#21644;&#21508;&#20010;&#21407;&#23376;&#30340;&#29305;&#24615;&#12290;&#22312;MoleculeNet&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SYNFUSION&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;5&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;6&#20010;&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction is a critical task in computational drug discovery. While recent advances in Graph Neural Networks (GNNs) and Transformers have shown to be effective and promising, they face the following limitations: Transformer self-attention does not explicitly consider the underlying molecule structure while GNN feature representation alone is not sufficient to capture granular and hidden interactions and characteristics that distinguish similar molecules. To address these limitations, we propose SYNFUSION, a novel approach that synergistically combines pre-trained features from GNNs and Transformers. This approach provides a comprehensive molecular representation, capturing both the global molecule structure and the individual atom characteristics. Experimental results on MoleculeNet benchmarks demonstrate superior performance, surpassing previous models in 5 out of 7 classification datasets and 4 out of 6 regression datasets. The performance of SYN-FUSION has been
&lt;/p&gt;</description></item><item><title>&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.02861</link><description>&lt;p&gt;
Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02861
&lt;/p&gt;
&lt;p&gt;
&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#37238;&#39044;&#27979;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#22270;&#24322;&#24120;&#30340;&#28508;&#22312;&#23646;&#24615;&#65292;&#23548;&#33268;&#26694;&#26550;&#35774;&#35745;&#19981;&#21487;&#35299;&#37322;&#21644;&#24615;&#33021;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#37325;&#26032;&#30740;&#31350;&#20102;&#24322;&#24120;&#21644;&#27491;&#24120;&#22270;&#20043;&#38388;&#30340;&#20809;&#35889;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31867;&#20043;&#38388;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#20449;&#21495;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#21487;&#20197;&#29992;&#20854;&#29790;&#21033;&#21830;&#34920;&#31034;&#65292;&#36825;&#34920;&#26126;&#29790;&#21033;&#21830;&#26159;&#22270;&#24322;&#24120;&#23646;&#24615;&#30340;&#19968;&#20010;&#39537;&#21160;&#22240;&#32032;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rayleigh Quotient Graph Neural Network&#65288;RQGNN&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#20809;&#35889;GNN&#65292;&#20026;&#25506;&#32034;&#24322;&#24120;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
&lt;/p&gt;</description></item><item><title>MagicDrive&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#24182;&#25429;&#25417;&#20102;&#32454;&#33268;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.02601</link><description>&lt;p&gt;
MagicDrive: &#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#19979;&#30340;&#34903;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02601
&lt;/p&gt;
&lt;p&gt;
MagicDrive&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#24182;&#25429;&#25417;&#20102;&#32454;&#33268;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#20855;&#26377;2D&#25511;&#21046;&#30340;&#25968;&#25454;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#34903;&#26223;&#29983;&#25104;&#20013;&#31934;&#30830;&#30340;&#19977;&#32500;&#25511;&#21046;&#22312;&#19977;&#32500;&#24863;&#30693;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#40479;&#30640;&#22270;&#20316;&#20026;&#20027;&#35201;&#26465;&#20214;&#24120;&#24120;&#23548;&#33268;&#20960;&#20309;&#25511;&#21046;&#65288;&#22914;&#39640;&#24230;&#65289;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24433;&#21709;&#29289;&#20307;&#24418;&#29366;&#12289;&#36974;&#25377;&#27169;&#24335;&#21644;&#36947;&#36335;&#34920;&#38754;&#39640;&#31243;&#31561;&#23545;&#24863;&#30693;&#25968;&#25454;&#21512;&#25104;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicDrive&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#36890;&#36807;&#23450;&#21046;&#30340;&#32534;&#30721;&#31574;&#30053;&#23454;&#29616;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#36824;&#37319;&#29992;&#20102;&#36328;&#35270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#30830;&#20445;&#22810;&#20010;&#30456;&#26426;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;MagicDrive&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#25429;&#25417;&#21040;&#20102;&#31934;&#32454;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework offering diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view synthesis that captures nuanced 3D ge
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02357</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#27602;&#24615;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02357
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#19981;&#28165;&#12290;&#35895;&#27468;&#26071;&#19979;&#30340;&#22242;&#38431;Jigsaw&#26159;&#35813;&#39046;&#22495;&#30340;&#39046;&#23548;&#32773;&#20043;&#19968;&#65292;&#20182;&#20204;&#20351;&#29992;Dixon&#31561;&#20154;&#32473;&#20986;&#30340;&#27602;&#24615;&#23450;&#20041;&#65306;&#8220;&#31895;&#40065;&#12289;&#19981;&#23562;&#37325;&#25110;&#19981;&#21512;&#29702;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#35753;&#26576;&#20154;&#31163;&#24320;&#35752;&#35770;&#8221;&#12290;&#20154;&#20204;&#21487;&#20197;&#31435;&#21363;&#30475;&#21040;&#36825;&#20010;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32473;&#20986;&#27602;&#24615;&#30340;&#23450;&#37327;&#24230;&#37327;&#65292;&#32780;&#19988;&#28041;&#21450;&#39640;&#24230;&#20027;&#35266;&#30340;&#25991;&#21270;&#26415;&#35821;&#12290;&#23613;&#31649;&#23384;&#22312;&#27169;&#31946;&#21644;&#32570;&#38519;&#65292;&#20294;&#36825;&#20010;&#23450;&#20041;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22768;&#23398;&#29305;&#24615;&#29983;&#25104;&#38899;&#39057;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#24773;&#24863;&#34920;&#36798;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22768;&#23398;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#24773;&#24863;&#38899;&#39057;&#26816;&#32034;&#21644;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02298</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#23398;&#29305;&#24615;&#20026;&#24773;&#24863;&#34920;&#36798;&#29983;&#25104;&#38899;&#39057;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompting Audios Using Acoustic Properties For Emotion Representation. (arXiv:2310.02298v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22768;&#23398;&#29305;&#24615;&#29983;&#25104;&#38899;&#39057;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#24773;&#24863;&#34920;&#36798;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22768;&#23398;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#24773;&#24863;&#38899;&#39057;&#26816;&#32034;&#21644;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#23384;&#22312;&#20110;&#19968;&#20010;&#36830;&#32493;&#30340;&#33539;&#22260;&#20869;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#23558;&#24773;&#24863;&#35270;&#20026;&#26377;&#38480;&#31163;&#25955;&#20540;&#21464;&#37327;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#24335;&#26080;&#27861;&#25429;&#25417;&#24773;&#24863;&#34920;&#36798;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34920;&#31034;&#24773;&#24863;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65288;&#25110;&#25552;&#31034;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#36825;&#20123;&#25552;&#31034;&#24182;&#35757;&#32451;&#27169;&#22411;&#20174;&#38899;&#39057;&#21644;&#25552;&#31034;&#23545;&#20013;&#26356;&#22909;&#22320;&#23398;&#20064;&#24773;&#24863;&#34920;&#36798;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#22768;&#23398;&#29305;&#24615;&#65288;&#22914;&#38899;&#35843;&#12289;&#24378;&#24230;&#12289;&#35821;&#36895;&#21644;&#21457;&#38899;&#36895;&#24230;&#65289;&#26469;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#65292;&#21363;&#8220;&#22768;&#23398;&#25552;&#31034;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#23558;&#35821;&#38899;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#22768;&#23398;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#24773;&#24863;&#38899;&#39057;&#26816;&#32034;&#21644;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22768;&#23398;&#25552;&#31034;&#22312;EAR&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;Precision@K&#25351;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;SER&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;Ravdess&#25968;&#25454;&#19978;&#30340;&#30456;&#23545;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;3.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.01791</link><description>&lt;p&gt;
&#20855;&#26377;&#20219;&#24847;&#30830;&#23450;&#24615;&#20445;&#35777;&#30340;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Online POMDP Planning with Anytime Deterministic Guarantees. (arXiv:2310.01791v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#32463;&#24120;&#36935;&#21040;&#19981;&#30830;&#23450;&#24615;&#24182;&#22522;&#20110;&#19981;&#23436;&#25972;&#20449;&#24687;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#35268;&#21010;&#21487;&#20197;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;POMDP&#30340;&#26368;&#20248;&#35268;&#21010;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#21482;&#26377;&#22312;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#21487;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#36817;&#20284;&#31639;&#27861;&#65288;&#22914;&#26641;&#25628;&#32034;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36739;&#22823;&#38382;&#39064;&#30340;&#20808;&#36827;POMDP&#27714;&#35299;&#22120;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20165;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#37319;&#26679;&#30340;&#32536;&#25925;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#36873;&#25321;&#19968;&#32452;&#35266;&#27979;&#20197;&#22312;&#35745;&#31639;&#27599;&#20010;&#21518;&#39564;&#33410;&#28857;&#26102;&#20998;&#25903;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents operating in real-world scenarios frequently encounter uncertainty and make decisions based on incomplete information. Planning under uncertainty can be mathematically formalized using partially observable Markov decision processes (POMDPs). However, finding an optimal plan for POMDPs can be computationally expensive and is feasible only for small tasks. In recent years, approximate algorithms, such as tree search and sample-based methodologies, have emerged as state-of-the-art POMDP solvers for larger problems. Despite their effectiveness, these algorithms offer only probabilistic and often asymptotic guarantees toward the optimal solution due to their dependence on sampling. To address these limitations, we derive a deterministic relationship between a simplified solution that is easier to obtain and the theoretically optimal one. First, we derive bounds for selecting a subset of the observations to branch from while computing a complete belief at each posterior nod
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.01425</link><description>&lt;p&gt;
Borges&#19982;AI
&lt;/p&gt;
&lt;p&gt;
Borges and AI. (arXiv:2310.01425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01425
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21551;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26102;&#20195;&#12290;&#19968;&#20123;&#20154;&#30475;&#21040;&#20102;&#26426;&#36935;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#30475;&#21040;&#20102;&#21361;&#38505;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#37117;&#36890;&#36807;&#31185;&#24187;&#23567;&#35828;&#20013;&#27969;&#34892;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;AI&#12290;&#26426;&#22120;&#26159;&#21542;&#20250;&#21464;&#24471;&#26377;&#24863;&#30693;&#33021;&#21147;&#24182;&#21453;&#25239;&#20854;&#21019;&#36896;&#32773;&#65311;&#25105;&#20204;&#26159;&#21542;&#20250;&#32463;&#21382;&#32440;&#22841;&#22841;&#23376;&#21551;&#31034;&#65311;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20043;&#21069;&#65292;&#25105;&#20204;&#39318;&#20808;&#24212;&#35813;&#38382;&#19968;&#19979;&#65292;&#36825;&#31181;&#24515;&#29702;&#24847;&#35937;&#26159;&#21542;&#23545;&#25163;&#22836;&#30340;&#29616;&#35937;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#25551;&#36848;&#12290;&#20165;&#36890;&#36807;&#31070;&#28789;&#30340;&#24773;&#32490;&#26469;&#29702;&#35299;&#22825;&#27668;&#27169;&#24335;&#30340;&#26041;&#27861;&#26159;&#26377;&#38480;&#30340;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20027;&#24352;&#36890;&#36807;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;LLMs&#21450;&#20854;&#19982;AI&#30340;&#20851;&#31995;&#65292;&#21338;&#23572;&#36203;&#26031;&#26159;20&#19990;&#32426;&#25991;&#23398;&#22823;&#24072;&#65292;&#39764;&#24187;&#29616;&#23454;&#20027;&#20041;&#20808;&#39537;&#21644;&#21518;&#29616;&#20195;&#25991;&#23398;&#30340;&#21069;&#22863;&#12290;&#36825;&#31181;&#25506;&#32034;&#26041;&#24335;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01423</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#20197;&#26469;&#65292;&#23427;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65288;&#21253;&#25324;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#65289;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22238;&#24212;&#65292;&#21560;&#24341;&#20102;&#35768;&#22810;&#20154;&#30340;&#20852;&#36259;&#12290;ChatGPT&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#35823;&#29992;&#21487;&#33021;&#20250;&#24102;&#26469;&#20005;&#37325;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#32946;&#21644;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#12290;&#30446;&#21069;&#24050;&#32463;&#26377;&#20960;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#21487;&#20379;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#22312;&#30495;&#23454;&#25991;&#26412;&#19978;&#36827;&#34892;&#27979;&#35797;&#30340;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#23427;&#20204;&#23545;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#29992;&#20110;&#26816;&#27979;&#22823;&#23398;&#21644;&#20854;&#20182;&#30740;&#31350;&#26426;&#26500;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#31456;&#12289;&#25688;&#35201;&#12289;&#25925;&#20107;&#12289;&#26032;&#38395;&#21644;&#20135;&#21697;&#35780;&#35770;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#27493;&#26159;&#20351;&#29992;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#20845;&#31181;&#24037;&#20855;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#24182;&#20855;&#26377;&#19982;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.00035</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;LoRA&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
LoRA ensembles for large language model fine-tuning. (arXiv:2310.00035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#24182;&#20855;&#26377;&#19982;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#34920;&#29616;&#20026;&#36807;&#20110;&#33258;&#20449;&#12289;&#26657;&#20934;&#19981;&#20339;&#20197;&#21450;&#23545;&#27979;&#35797;&#25968;&#25454;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#26679;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#20123;&#20302;&#31209;&#36866;&#37197;&#22120;&#34920;&#31034;&#30340;&#21442;&#25968;&#25968;&#37327;&#38750;&#24120;&#23567;&#65292;&#27604;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#23567;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples. One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations. However, there is a huge challenge to ensembling LLMs: the most effective LLMs are very, very large. Keeping a single LLM in memory is already challenging enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many settings. To address these issues, we propose an ensemble approach using Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique. Critically, these low-rank adapters represent a very small number of parameters, orders of magnitude less than the underlying pre-trained model. Thus, it is possible to construct large ensembles of LoRA adapters with almost the same computat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17329</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#37096;&#30142;&#30149;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#27835;&#24840;&#32954;&#37096;&#30142;&#30149;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#32954;&#37096;&#31995;&#32479;&#20869;&#30340;&#35768;&#22810;&#22797;&#26434;&#30340;3D&#26641;&#29366;&#32467;&#26500;&#65292;&#22914;&#27668;&#36947;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#22534;&#26632;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23494;&#38598;&#20307;&#32032;&#32593;&#26684;&#30340;&#26631;&#20934;CNN&#26041;&#27861;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#26641;&#39592;&#26550;&#30340;&#22270;&#36830;&#36890;&#24615;&#65292;&#24182;&#32467;&#21512;&#20102;&#38544;&#24335;&#34920;&#38754;&#34920;&#31034;&#12290;&#23427;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#29992;&#30340;&#34920;&#38754;&#12290;&#30001;&#20110;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#25968;&#25454;&#31232;&#32570;&#65292;&#25105;&#20204;&#36824;&#25972;&#29702;&#20102;&#19968;&#22871;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
&lt;/p&gt;</description></item><item><title>PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;</title><link>http://arxiv.org/abs/2309.17260</link><description>&lt;p&gt;
PlaceNav: &#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17260
&lt;/p&gt;
&lt;p&gt;
PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25299;&#25169;&#23548;&#33322;&#20998;&#20026;&#26426;&#22120;&#20154;&#26080;&#20851;&#21644;&#26426;&#22120;&#20154;&#29305;&#23450;&#30340;&#32452;&#20214;&#21487;&#20197;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#26426;&#22120;&#20154;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23548;&#33322;&#26041;&#27861;&#20173;&#21463;&#21040;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35745;&#31639;&#32553;&#25918;&#24615;&#24046;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PlaceNav&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#26469;&#36873;&#25321;&#25299;&#25169;&#23548;&#33322;&#27969;&#31243;&#20013;&#30340;&#23376;&#30446;&#26631;&#12290;&#36825;&#20351;&#24471;&#23376;&#30446;&#26631;&#36873;&#25321;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22320;&#28857;&#35782;&#21035;&#20351;&#24471;&#36125;&#21494;&#26031;&#28388;&#27874;&#25104;&#20026;&#21487;&#33021;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#23376;&#30446;&#26631;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#19968;&#35774;&#35745;&#65292;&#24182;&#19988;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
&lt;/p&gt;</description></item><item><title>DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17167</link><description>&lt;p&gt;
DyVal: &#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17167
&lt;/p&gt;
&lt;p&gt;
DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#24222;&#22823;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20934;&#30340;&#38745;&#24577;&#24615;&#36136;&#21644;&#22266;&#23450;&#22797;&#26434;&#24615;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#34913;&#37327;LLM&#30340;&#36827;&#27493;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DyVal&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#21160;&#24577;&#35780;&#20272;LLM&#30340;&#21327;&#35758;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#21160;&#24577;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#20248;&#21183;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;DyVal&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#12290;DyVal&#29983;&#25104;&#20102;&#21253;&#25324;&#25968;&#23398;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#31639;&#27861;&#38382;&#39064;&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;Flan-T5-large&#21040;ChatGPT&#21644;GPT4&#30340;&#21508;&#31181;LLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;DyVal&#29983;&#25104;&#30340;&#35780;&#20272;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23450;&#24615;&#20998;&#26512;&#36827;&#34892;&#27880;&#37322;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#21644;&#27979;&#37327;&#35823;&#24046;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26356;&#22909;&#22320;&#20943;&#23569;&#20559;&#35265;&#21644;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17147</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#21487;&#33021;&#24341;&#20837;&#20005;&#37325;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models for Qualitative Analysis can Introduce Serious Bias. (arXiv:2309.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17147
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23450;&#24615;&#20998;&#26512;&#36827;&#34892;&#27880;&#37322;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#21644;&#27979;&#37327;&#35823;&#24046;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26356;&#22909;&#22320;&#20943;&#23569;&#20559;&#35265;&#21644;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#36805;&#36895;&#26222;&#21450;&#65292;&#20294;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#24456;&#22909;&#29702;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#20998;&#26512;&#22823;&#37327;&#24320;&#25918;&#24615;&#38754;&#35848;&#25968;&#25454;&#65292;&#20197;&#32599;&#20852;&#20122;&#38590;&#27665;&#22312;&#23391;&#21152;&#25289;&#22269;&#31185;&#20811;&#26031;&#24052;&#25166;&#30340;&#35775;&#35848;&#35760;&#24405;&#20026;&#24212;&#29992;&#26696;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;LLMs&#23545;&#25991;&#26412;&#36827;&#34892;&#27880;&#37322;&#26102;&#38656;&#35201;&#38750;&#24120;&#35880;&#24910;&#65292;&#22240;&#20026;&#23384;&#22312;&#24341;&#20837;&#20559;&#35265;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#25512;&#26029;&#12290;&#25105;&#20204;&#36825;&#37324;&#25152;&#25351;&#30340;&#20559;&#35265;&#26159;&#25216;&#26415;&#24847;&#20041;&#19978;&#30340;&#65292;&#21363;LLMs&#22312;&#27880;&#37322;&#35775;&#35848;&#35760;&#24405;&#26102;&#30340;&#38169;&#35823;&#19981;&#26159;&#19982;&#35775;&#35848;&#23545;&#35937;&#30340;&#29305;&#24449;&#26080;&#20851;&#30340;&#38543;&#26426;&#35823;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#23569;&#27979;&#37327;&#35823;&#24046;&#21644;&#20559;&#35265;&#65292;&#20248;&#20110;LLMs&#30340;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#24517;&#39035;&#26377;&#19968;&#20123;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#20197;&#35780;&#20272;LLM&#26159;&#21542;&#24341;&#20837;&#20559;&#35265;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#22909;&#30340;&#36873;&#25321;&#21487;&#33021;&#26159;&#20351;&#29992;&#36739;&#31616;&#21333;&#30340;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are quickly becoming ubiquitous, but the implications for social science research are not yet well understood. This paper asks whether LLMs can help us analyse large-N qualitative data from open-ended interviews, with an application to transcripts of interviews with Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences. We here mean bias in the technical sense, that the errors that LLMs make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects. Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations. Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is probably preferable to
&lt;/p&gt;</description></item><item><title>LinGCN&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#21644;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#21644;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.14331</link><description>&lt;p&gt;
LinGCN: &#32467;&#26500;&#21270;&#30340;&#32447;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21516;&#24577;&#21152;&#23494;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference. (arXiv:2309.14331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14331
&lt;/p&gt;
&lt;p&gt;
LinGCN&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#21644;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#21644;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#38271;&#24050;&#32463;&#22312;&#20010;&#20154;&#21307;&#30103;&#21644;&#37329;&#34701;&#31995;&#32479;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20113;&#31471;&#37096;&#32626;GCN&#24341;&#21457;&#20102;&#23545;&#23458;&#25143;&#25968;&#25454;&#21487;&#33021;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#65292;&#37319;&#29992;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#21487;&#20197;&#30830;&#20445;&#25935;&#24863;&#23458;&#25143;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#24341;&#20837;&#20102;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LinGCN&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#24182;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#12290;LinGCN&#22260;&#32469;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#23637;&#24320;&#65306;&#65288;1&#65289;&#21487;&#24494;&#30340;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#65292;&#25645;&#37197;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#65292;&#36890;&#36807;&#19982;&#27169;&#22411;&#26435;&#37325;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#20197;&#28385;&#36275;&#20248;&#21270;&#30446;&#26631;&#12290;&#36825;&#31181;&#31574;&#30053;&#20419;&#36827;&#20102;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#21644;&#39640;&#26031;&#24615;&#20551;&#35774;&#19979;&#31283;&#23450;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35745;&#31639;&#35813;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#31561;&#20215;&#20110;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;GPU&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2309.14073</link><description>&lt;p&gt;
&#28508;&#21464;&#37327;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65306;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach. (arXiv:2309.14073v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#21644;&#39640;&#26031;&#24615;&#20551;&#35774;&#19979;&#31283;&#23450;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35745;&#31639;&#35813;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#31561;&#20215;&#20110;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;GPU&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615;&#21644;&#39640;&#26031;&#24615;&#20551;&#35774;&#19979;&#31283;&#23450;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#22270;&#24418;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35745;&#31639;&#36825;&#20010;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#31561;&#20215;&#20110;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;GPU&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a graphical structure for structural equation models that is stable under marginalization under linearity and Gaussianity assumptions. We show that computing the maximum likelihood estimation of this model is equivalent to training a neural network. We implement a GPU-based algorithm that computes the maximum likelihood estimation of these models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.11981</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26410;&#26469;&#24230;&#37327;&#30340;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11981
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#36825;&#20026;&#37325;&#26032;&#23457;&#35270;&#20256;&#32479;&#30340;&#26426;&#22120;&#26234;&#33021;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20174;&#20256;&#32479;&#30340;&#22270;&#28789;&#27979;&#35797;&#36716;&#21521;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#24182;&#20511;&#37492;&#20102;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#28145;&#21463;&#22810;&#20010;&#23398;&#31185;&#30340;&#21331;&#36234;&#24037;&#20316;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#20102;&#20445;&#25345;&#36328;&#23398;&#31185;&#26725;&#26753;&#24320;&#25918;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#21246;&#21202;&#20102;&#19968;&#20010;&#26356;&#21152;&#31283;&#20581;&#21644;&#21487;&#25345;&#32493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of artificial intelligence (AI), the unprecedented progress of large language models (LLMs) in natural language processing (NLP) offers an opportunity to revisit the entire approach of traditional metrics of machine intelligence, both in form and content. As the realm of machine cognitive evaluation has already reached Imitation, the next step is an efficient Language Acquisition and Understanding. Our paper proposes a paradigm shift from the established Turing Test towards an all-embracing framework that hinges on language acquisition, taking inspiration from the recent advancements in LLMs. The present contribution is deeply tributary of the excellent work from various disciplines, point out the need to keep interdisciplinary bridges open, and delineates a more robust and sustainable approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#19981;&#24403;&#27169;&#25311;&#35774;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28385;&#36275;&#26399;&#26395;&#30340;&#27169;&#25311;&#26694;&#26550;PARCS&#65292;&#35813;&#26694;&#26550;&#21512;&#25104;&#20102;&#22522;&#20110;&#22240;&#26524;&#27169;&#22411;&#21644;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#29983;&#25104;&#31526;&#21512;&#26465;&#20214;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10514</link><description>&lt;p&gt;
&#37096;&#20998;&#25351;&#23450;&#30340;&#22240;&#26524;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Partially-Specified Causal Simulations. (arXiv:2309.10514v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#19981;&#24403;&#27169;&#25311;&#35774;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28385;&#36275;&#26399;&#26395;&#30340;&#27169;&#25311;&#26694;&#26550;PARCS&#65292;&#35813;&#26694;&#26550;&#21512;&#25104;&#20102;&#22522;&#20110;&#22240;&#26524;&#27169;&#22411;&#21644;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#29983;&#25104;&#31526;&#21512;&#26465;&#20214;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#30740;&#31350;&#22312;&#39564;&#35777;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#21482;&#26377;&#22312;&#30740;&#31350;&#26681;&#25454;&#34987;&#27979;&#35797;&#26041;&#27861;&#25152;&#25215;&#35834;&#30340;&#25805;&#20316;&#26465;&#20214;&#36827;&#34892;&#35774;&#35745;&#26102;&#65292;&#27169;&#25311;&#32467;&#26524;&#25165;&#26159;&#21487;&#38752;&#30340;&#12290;&#28982;&#32780;&#65292;&#24456;&#22810;&#22240;&#26524;&#25512;&#26029;&#25991;&#29486;&#24448;&#24448;&#35774;&#35745;&#20102;&#36807;&#20110;&#21463;&#38480;&#25110;&#38169;&#35823;&#25351;&#23450;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#22240;&#26524;&#26041;&#27861;&#19981;&#24403;&#27169;&#25311;&#35774;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#32534;&#21046;&#20102;&#26377;&#25928;&#30340;&#27169;&#25311;&#26694;&#26550;&#30340;&#19968;&#31995;&#21015;&#26399;&#26395;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#37096;&#20998;&#38543;&#26426;&#21270;&#30340;&#22240;&#26524;&#27169;&#25311;&#65288;PARCS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#28385;&#36275;&#36825;&#20123;&#26399;&#26395;&#30340;&#27169;&#25311;&#26694;&#26550;&#12290;PARCS&#22522;&#20110;&#22270;&#24418;&#21270;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#19968;&#31995;&#21015;&#21487;&#35843;&#21442;&#25968;&#21512;&#25104;&#25968;&#25454;&#12290;&#36890;&#24120;&#30340;&#22240;&#26524;&#20551;&#35774;&#19982;&#21442;&#25968;&#20043;&#38388;&#26377;&#26126;&#30830;&#30340;&#26144;&#23556;&#65292;&#22240;&#27492;&#29992;&#25143;&#21487;&#20197;&#30830;&#23450;&#24182;&#25351;&#23450;&#30456;&#20851;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#24182;&#38543;&#26426;&#21270;&#20854;&#20313;&#21442;&#25968;&#20197;&#29983;&#25104;&#31526;&#21512;&#26465;&#20214;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#29992;&#20110;&#20854;&#22240;&#26524;&#26041;&#27861;&#12290;&#32467;&#26524;&#26159;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation studies play a key role in the validation of causal inference methods. The simulation results are reliable only if the study is designed according to the promised operational conditions of the method-in-test. Still, many causal inference literature tend to design over-restricted or misspecified studies. In this paper, we elaborate on the problem of improper simulation design for causal methods and compile a list of desiderata for an effective simulation framework. We then introduce partially-randomized causal simulation (PARCS), a simulation framework that meets those desiderata. PARCS synthesizes data based on graphical causal models and a wide range of adjustable parameters. There is a legible mapping from usual causal assumptions to the parameters, thus, users can identify and specify the subset of related parameters and randomize the remaining ones to generate a range of complying data-generating processes for their causal method. The result is a more comprehensive and i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#28216;&#25103;&#35268;&#21017;&#29983;&#25104;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#30340;&#35268;&#21017;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#65292;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.09476</link><description>&lt;p&gt;
&#26426;&#26800;&#21270;&#29983;&#25104;&#22120;2.0: &#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#30340;&#28216;&#25103;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules. (arXiv:2309.09476v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#28216;&#25103;&#35268;&#21017;&#29983;&#25104;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#30340;&#35268;&#21017;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#65292;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#28216;&#25103;&#35774;&#35745;&#65288;AGD&#65289;&#26159;&#30740;&#31350;&#33258;&#21160;&#29983;&#25104;&#28216;&#25103;&#35268;&#21017;&#30340;&#25216;&#26415;&#28216;&#25103;&#30740;&#31350;&#30340;&#19968;&#20010;&#38271;&#26399;&#35838;&#39064;&#12290; AGD&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#20154;&#31867;&#29609;&#23478;&#28216;&#25103;&#30340;&#36817;&#20284;&#65292;&#21487;&#20197;&#26159;&#23458;&#35266;&#20989;&#25968;&#25110;AI&#20195;&#29702;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#37096;&#20998;&#36825;&#20123;&#36817;&#20284;&#22120;&#26159;&#38745;&#24577;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20204;&#19981;&#33021;&#21453;&#26144;&#20154;&#31867;&#29609;&#23478;&#22312;&#28216;&#25103;&#20013;&#30340;&#23398;&#20064;&#21644;&#25552;&#39640;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#29983;&#25104;&#35268;&#21017;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#20013;&#12290;&#25105;&#20204;&#22312;Unity&#20013;&#37325;&#26032;&#21019;&#24314;&#20102;&#32463;&#20856;&#30340;AGD&#29615;&#22659;Mechanic Maker&#20316;&#20026;&#19968;&#20010;&#20840;&#26032;&#30340;&#24320;&#28304;&#29983;&#25104;&#35268;&#21017;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;RL&#19982;A*&#20195;&#29702;&#22522;&#32447;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#35268;&#21017;&#38598;&#65292;&#36825;&#20123;&#35268;&#21017;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated game design (AGD), the study of automatically generating game rules, has a long history in technical games research. AGD approaches generally rely on approximations of human play, either objective functions or AI agents. Despite this, the majority of these approximators are static, meaning they do not reflect human player's ability to learn and improve in a game. In this paper, we investigate the application of Reinforcement Learning (RL) as an approximator for human play for rule generation. We recreate the classic AGD environment Mechanic Maker in Unity as a new, open-source rule generation framework. Our results demonstrate that RL produces distinct sets of rules from an A* agent baseline, which may be more usable by humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22359;&#21644;&#24179;&#34913;&#22120;&#27169;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08916</link><description>&lt;p&gt;
&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65306;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer's Disease. (arXiv:2309.08916v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22359;&#21644;&#24179;&#34913;&#22120;&#27169;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#33041;&#30142;&#30149;&#30340;&#21457;&#30149;&#26426;&#21046;&#65292;&#21253;&#25324;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#65292;&#33041;&#32467;&#26500;&#19982;&#21151;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#23558;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#26144;&#23556;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#26469;&#34920;&#31034;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;InnerGCN&#65289;&#27169;&#22359;&#65292;BGGAN&#30340;&#29983;&#25104;&#22120;&#21487;&#20197;&#21033;&#29992;&#30452;&#25509;&#21644;&#38388;&#25509;&#33041;&#21306;&#22495;&#30340;&#29305;&#24449;&#26469;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;Balancer&#30340;&#26032;&#27169;&#22359;&#26469;&#24179;&#34913;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20043;&#38388;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;Balancer&#24341;&#20837;&#21040;BGGAN&#20013;&#65292;&#32467;&#26500;&#29983;&#25104;&#22120;&#21644;&#21151;&#33021;&#29983;&#25104;&#22120;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;AD&#20013;&#20934;&#30830;&#22320;&#34920;&#31034;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between brain structure and function is critical for revealing the pathogenesis of brain disease, including Alzheimer's disease (AD). However, it is a great challenge to map brain structure-function connections due to various reasons. In this work, a bidirectional graph generative adversarial networks (BGGAN) is proposed to represent brain structure-function connections. Specifically, by designing a module incorporating inner graph convolution network (InnerGCN), the generators of BGGAN can employ features of direct and indirect brain regions to learn the mapping function between structural domain and functional domain. Besides, a new module named Balancer is designed to counterpoise the optimization between generators and discriminators. By introducing the Balancer into BGGAN, both the structural generator and functional generator can not only alleviate the issue of mode collapse but also learn complementarity of structural and functional features. Experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26426;&#21046;&#65292;&#25506;&#32034;&#20102;&#22810;&#20010;&#22810;&#26041;&#38754;&#20844;&#24179;&#24212;&#29992;&#20013;&#30340;&#36873;&#25321;&#26426;&#21046;&#36873;&#39033;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#30340;&#36873;&#25321;&#21644;&#20998;&#37197;&#26426;&#21046;&#20250;&#20135;&#29983;&#19981;&#21516;&#20294;&#19968;&#33268;&#30340;&#20844;&#24179;&#24615;/&#20934;&#30830;&#24615;&#26435;&#34913;&#32467;&#26524;&#65292;&#24182;&#19988;&#22810;&#26234;&#33021;&#20307;&#30340;&#26500;&#25104;&#20351;&#24471;&#31995;&#32479;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20154;&#21475;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.08621</link><description>&lt;p&gt;
&#22312;SCRUF&#20013;&#25506;&#32034;&#25512;&#33616;&#20844;&#24179;&#24615;&#30340;&#31038;&#20250;&#36873;&#25321;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF. (arXiv:2309.08621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26426;&#21046;&#65292;&#25506;&#32034;&#20102;&#22810;&#20010;&#22810;&#26041;&#38754;&#20844;&#24179;&#24212;&#29992;&#20013;&#30340;&#36873;&#25321;&#26426;&#21046;&#36873;&#39033;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#30340;&#36873;&#25321;&#21644;&#20998;&#37197;&#26426;&#21046;&#20250;&#20135;&#29983;&#19981;&#21516;&#20294;&#19968;&#33268;&#30340;&#20844;&#24179;&#24615;/&#20934;&#30830;&#24615;&#26435;&#34913;&#32467;&#26524;&#65292;&#24182;&#19988;&#22810;&#26234;&#33021;&#20307;&#30340;&#26500;&#25104;&#20351;&#24471;&#31995;&#32479;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20154;&#21475;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#24448;&#24448;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#22797;&#26434;&#24615;&#65292;&#32780;&#36825;&#19968;&#28857;&#22312;&#31616;&#21270;&#30340;&#30740;&#31350;&#20844;&#24335;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20307;&#29616;&#12290;&#22312;&#23545;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#31038;&#20250;&#36873;&#25321;&#30340;&#26694;&#26550;&#20013;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#30340;&#20844;&#24179;&#24615;&#20851;&#27880;&#22522;&#30784;&#19978;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#19988;&#22810;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;&#25512;&#33616;&#26041;&#27861;&#12290;&#21033;&#29992;&#31038;&#20250;&#36873;&#25321;&#21487;&#20197;&#22686;&#21152;&#36890;&#29992;&#24615;&#65292;&#24182;&#26377;&#21487;&#33021;&#21033;&#29992;&#32463;&#36807;&#30740;&#31350;&#30340;&#31038;&#20250;&#36873;&#25321;&#31639;&#27861;&#35299;&#20915;&#22810;&#20010;&#31454;&#20105;&#30340;&#20844;&#24179;&#24615;&#20851;&#27880;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22810;&#26041;&#38754;&#20844;&#24179;&#24212;&#29992;&#20013;&#36873;&#25321;&#26426;&#21046;&#30340;&#19968;&#31995;&#21015;&#36873;&#39033;&#65292;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#31867;&#21035;&#30340;&#36873;&#25321;&#21644;&#20998;&#37197;&#26426;&#21046;&#22312;&#20844;&#24179;&#24615;/&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20135;&#29983;&#20102;&#19981;&#21516;&#20294;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22810;&#26234;&#33021;&#20307;&#30340;&#26500;&#25104;&#25552;&#20379;&#20102;&#36866;&#24212;&#29992;&#25143;&#20154;&#21475;&#21160;&#24577;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness problems in recommender systems often have a complexity in practice that is not adequately captured in simplified research formulations. A social choice formulation of the fairness problem, operating within a multi-agent architecture of fairness concerns, offers a flexible and multi-aspect alternative to fairness-aware recommendation approaches. Leveraging social choice allows for increased generality and the possibility of tapping into well-studied social choice algorithms for resolving the tension between multiple, competing fairness concerns. This paper explores a range of options for choice mechanisms in multi-aspect fairness applications using both real and synthetic data and shows that different classes of choice and allocation mechanisms yield different but consistent fairness / accuracy tradeoffs. We also show that a multi-agent formulation offers flexibility in adapting to user population dynamics.
&lt;/p&gt;</description></item><item><title>Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07936</link><description>&lt;p&gt;
Landscape-Sketch-Step: &#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems. (arXiv:2309.07936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07936
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25104;&#26412;&#20989;&#25968;&#30340;&#35780;&#20272;&#38750;&#24120;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#29978;&#33267;&#31105;&#27490;&#30340;&#22330;&#26223;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;Landscape-Sketch-Step&#65288;LSS&#65289;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#20808;&#21069;&#37319;&#26679;&#28857;&#30340;&#21382;&#21490;&#20449;&#24687;&#65292;&#20197;&#26126;&#26234;&#22320;&#36873;&#25321;&#24212;&#35780;&#20272;&#25104;&#26412;&#20989;&#25968;&#30340;&#21442;&#25968;&#20540;&#12290;&#19982;&#22797;&#21046;&#20132;&#25442;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#25152;&#38656;&#30340;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#19982;&#27169;&#25311;&#36864;&#28779;&#26041;&#27861;&#30456;&#24403;&#65292;&#36825;&#22312;&#39640;&#36890;&#37327;&#35745;&#31639;&#25110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#31561;&#29615;&#22659;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#35780;&#20272;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#35201;&#20040;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#23436;&#25104;&#12290;&#35813;&#26041;&#27861;&#19982;&#26631;&#20934;&#30340;&#20195;&#29702;&#20248;&#21270;&#25216;&#26415;&#20063;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#19981;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new heuristics for global optimization in scenarios where extensive evaluations of the cost function are expensive, inaccessible, or even prohibitive. The method, which we call Landscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic Optimization, and Reinforcement Learning techniques, relying on historical information from previously sampled points to make judicious choices of parameter values where the cost function should be evaluated at. Unlike optimization by Replica Exchange Monte Carlo methods, the number of evaluations of the cost function required in this approach is comparable to that used by Simulated Annealing, quality that is especially important in contexts like high-throughput computing or high-performance computing tasks, where evaluations are either computationally expensive or take a long time to be performed. The method also differs from standard Surrogate Optimization techniques, for it does not construct a surrogate model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#24067;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.07056</link><description>&lt;p&gt;
&#28145;&#24230;&#37327;&#23376;&#22270;&#20687;&#27169;&#25311;&#65306;&#35299;&#26512;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#23454;&#39564;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments. (arXiv:2309.07056v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#24067;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#20419;&#36827;&#26032;&#30340;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#20854;&#36923;&#36753;&#32972;&#21518;&#30340;&#19981;&#36879;&#26126;&#24615;&#32473;&#35299;&#37322;&#20854;&#21457;&#29616;&#30340;&#25361;&#25112;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#8220;inception&#8221;&#25110;&#8220;&#28145;&#24230;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#34987;&#21457;&#26126;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#26469;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#25925;&#20107;&#20174;&#23545;&#37327;&#23376;&#31995;&#32479;&#23646;&#24615;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24320;&#22987;&#12290;&#32463;&#36807;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#8220;&#21453;&#36716;&#8221;&#31070;&#32463;&#32593;&#32476;--&#23454;&#38469;&#19978;&#26159;&#35810;&#38382;&#23427;&#22914;&#20309;&#24819;&#35937;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#37327;&#23376;&#31995;&#32479;&#65292;&#20197;&#21450;&#22914;&#20309;&#36830;&#32493;&#20462;&#25913;&#37327;&#23376;&#31995;&#32479;&#20197;&#25913;&#21464;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#21021;&#22987;&#23646;&#24615;&#20998;&#24067;&#65292;&#25105;&#20204;&#21487;&#20197;&#27010;&#24565;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#36739;&#27973;&#23618;&#65292;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#31616;&#21333;&#30340;&#23646;&#24615;&#65292;&#32780;&#22312;&#36739;&#28145;&#23618;&#27425;&#19978;...&#65288;&#20869;&#23481;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Despite their promise to facilitate new scientific discoveries, the opaqueness of neural networks presents a challenge in interpreting the logic behind their findings. Here, we use a eXplainable-AI (XAI) technique called $inception$ or $deep$ $dreaming$, which has been invented in machine learning for computer vision. We use this techniques to explore what neural networks learn about quantum optics experiments. Our story begins by training a deep neural networks on the properties of quantum systems. Once trained, we "invert" the neural network -- effectively asking how it imagines a quantum system with a specific property, and how it would continuously modify the quantum system to change a property. We find that the network can shift the initial distribution of properties of the quantum system, and we can conceptualize the learned strategies of the neural network. Interestingly, we find that, in the first layers, the neural network identifies simple properties, while in the deeper ones
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#37319;&#26679;&#26694;&#26550;&#65292;&#22312;&#20351;&#29992;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#31163;&#32447;&#25968;&#25454;&#35780;&#20272;&#20195;&#29702;&#31574;&#30053;&#24615;&#33021;&#26102;&#65292;&#35299;&#20915;&#20102;&#22823;&#23494;&#24230;&#27604;&#29575;&#21644;&#38388;&#25509;&#30417;&#30563;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.01807</link><description>&lt;p&gt;
&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#37319;&#26679;&#29992;&#20110;&#31163;&#32447;&#29615;&#22659;&#19979;&#30340;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Marginalized Importance Sampling for Off-Environment Policy Evaluation. (arXiv:2309.01807v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#37319;&#26679;&#26694;&#26550;&#65292;&#22312;&#20351;&#29992;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#31163;&#32447;&#25968;&#25454;&#35780;&#20272;&#20195;&#29702;&#31574;&#30053;&#24615;&#33021;&#26102;&#65292;&#35299;&#20915;&#20102;&#22823;&#23494;&#24230;&#27604;&#29575;&#21644;&#38388;&#25509;&#30417;&#30563;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#20351;&#24471;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#35757;&#32451;&#21644;&#37096;&#32626;RL&#31574;&#30053;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21363;&#20351;&#26159;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#30340;&#31283;&#20581;&#31574;&#30053;&#65292;&#20063;&#38656;&#35201;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#35780;&#20272;&#20195;&#29702;&#31574;&#30053;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#31163;&#32447;&#25968;&#25454;&#65292;&#20351;&#29992;&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#37319;&#26679;&#65288;MIS&#65289;&#26694;&#26550;&#26469;&#35780;&#20272;&#20219;&#20309;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;MIS&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22823;&#30340;&#23494;&#24230;&#27604;&#29575;&#20559;&#31163;&#21512;&#29702;&#33539;&#22260;&#65292;&#65288;2&#65289;&#38388;&#25509;&#30417;&#30563;&#65292;&#38656;&#35201;&#38388;&#25509;&#25512;&#26029;&#27604;&#29575;&#65292;&#20174;&#32780;&#21152;&#21095;&#20272;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#27169;&#25311;&#22120;&#20013;&#30446;&#26631;&#31574;&#30053;&#30340;&#21344;&#20301;&#21464;&#37327;&#65292;&#24182;&#23558;&#23494;&#24230;&#27604;&#29575;&#23398;&#20064;&#20026;&#20004;&#20010;&#21487;&#20998;&#21035;&#23398;&#20064;&#30340;&#39033;&#30340;&#20056;&#31215;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies prior to deploying them in the real world. Our approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00079</link><description>&lt;p&gt;
&#20851;&#20110;Adam&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21518;&#21521;&#35823;&#24046;&#20998;&#26512;&#34987;&#29992;&#26469;&#25214;&#21040;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#21457;&#29616;&#26377;&#38480;&#27493;&#38271;&#20250;&#38544;&#24335;&#22320;&#35268;&#33539;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20986;&#29616;&#22312;ODE&#20013;&#30340;&#39033;&#20250;&#24809;&#32602;&#25439;&#22833;&#26799;&#24230;&#30340;&#20108;&#33539;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#20013;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#20294;&#28041;&#21450;&#30340;&#8220;&#33539;&#25968;&#8221;&#19981;&#21516;&#65306;&#23545;&#24212;&#30340;ODE&#39033;&#35201;&#20040;&#24809;&#32602;&#65288;&#25200;&#21160;&#30340;&#65289;&#25439;&#22833;&#26799;&#24230;&#30340;&#19968;&#33539;&#25968;&#65292;&#35201;&#20040;&#30456;&#21453;&#22320;&#38459;&#27490;&#20854;&#20943;&#23567;&#65288;&#21518;&#19968;&#31181;&#24773;&#20917;&#26159;&#20856;&#22411;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
&lt;/p&gt;</description></item><item><title>BaDExpert&#26159;&#19968;&#31181;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#12290;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#29992;&#35813;&#27169;&#22411;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.12439</link><description>&lt;p&gt;
BaDExpert: &#25552;&#21462;&#21518;&#38376;&#21151;&#33021;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12439
&lt;/p&gt;
&lt;p&gt;
BaDExpert&#26159;&#19968;&#31181;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#12290;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#29992;&#35813;&#27169;&#22411;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#19978;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#23558;&#24694;&#24847;&#34892;&#20026;&#65288;&#21518;&#38376;&#65289;&#31192;&#23494;&#22320;&#26893;&#20837;DNN&#20013;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#23646;&#20110;&#21518;&#26399;&#24320;&#21457;&#30340;&#38450;&#24481;&#33539;&#30068;&#65292;&#29420;&#31435;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#36870;&#21521;&#24037;&#31243;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#24456;&#31616;&#21333; - &#22312;&#19968;&#23567;&#32452;&#26377;&#24847;&#20041;&#30340;&#38169;&#35823;&#26631;&#35760;&#30340;&#24178;&#20928;&#26679;&#26412;&#19978;&#24494;&#35843;&#21518;&#38376;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#20854;&#24536;&#35760;&#27491;&#24120;&#21151;&#33021;&#20294;&#20173;&#20445;&#30041;&#21518;&#38376;&#21151;&#33021;&#65292;&#20174;&#32780;&#29983;&#25104;&#19968;&#20010;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#30340;&#27169;&#22411;&#65288;&#31216;&#20026;&#21518;&#38376;&#19987;&#23478;&#27169;&#22411;&#65289;&#12290;&#22522;&#20110;&#25552;&#21462;&#30340;&#21518;&#38376;&#19987;&#23478;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#36807;&#28388;&#25481;&#21518;&#38376;&#36755;&#20837;&#12290;&#36827;&#19968;&#27493;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00436</link><description>&lt;p&gt;
SelfCheck: &#20351;&#29992;LLMs&#33258;&#26816;&#20854;&#36880;&#27493;&#25512;&#29702;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#30340;&#21457;&#26126;&#65292;&#20351;&#24471;&#35299;&#20915;&#25512;&#29702;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#24378;&#22823;&#30340;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#38750;&#32447;&#24615;&#24605;&#32500;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#20855;&#26377;&#35782;&#21035;&#33258;&#24049;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#36880;&#27493;&#25512;&#29702;&#20013;&#30340;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#20197;&#35782;&#21035;&#27492;&#31867;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#39564;&#35777;&#26041;&#26696;&#26469;&#25913;&#36827;&#38382;&#31572;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#25237;&#31080;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;-GSM8K&#65292;MathQA&#21644;MATH&#19978;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#32780;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.00143</link><description>&lt;p&gt;
&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20869;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#30340;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;DNNs&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#36825;&#20351;&#24471;&#35299;&#37322;&#21644;&#35777;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#23545;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#36825;&#20123;&#25216;&#26415;&#33021;&#22815;&#25214;&#20986;&#23548;&#33268;DNN&#34892;&#20026;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;XAI&#25216;&#26415;&#36890;&#24120;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;(i)&#23427;&#20204;&#26159;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#35299;&#37322;&#27491;&#30830;&#24615;&#30340;&#27491;&#24335;&#20445;&#35777;&#65307;(ii)&#23427;&#20204;&#36890;&#24120;&#36866;&#29992;&#20110;&#8220;&#19968;&#27425;&#24615;&#8221;&#31995;&#32479;(&#21363;DNN&#29420;&#31435;&#20110;&#36807;&#21435;&#30340;&#35843;&#29992;)&#65292;&#32780;&#19981;&#26159;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#22987;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#29992;&#20110;&#25512;&#29702;&#22810;&#27493;&#39588;&#30340;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20943;&#23569;&#24213;&#23618;&#39564;&#35777;&#22120;&#25152;&#25506;&#32034;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10025</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#65306;&#29983;&#32946;&#25919;&#31574;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods. (arXiv:2307.10025v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#32946;&#38382;&#39064;&#19982;&#20154;&#21475;&#23433;&#20840;&#23494;&#20999;&#30456;&#20851;&#65292;&#20013;&#22269;60&#24180;&#26469;&#39318;&#27425;&#20986;&#29616;&#20154;&#21475;&#36127;&#22686;&#38271;&#36235;&#21183;&#65292;&#29983;&#32946;&#25919;&#31574;&#30340;&#21464;&#21270;&#24341;&#36215;&#20102;&#31038;&#20250;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#37319;&#29992;&#20849;&#29616;&#35821;&#20041;&#20998;&#26512;&#12289;&#20027;&#39064;&#20998;&#26512;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#22810;&#31890;&#24230;&#30340;&#35821;&#20041;&#20998;&#26512;&#12290;&#21457;&#29616;&#20851;&#20110;&#8220;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#8221;&#30340;&#25552;&#26696;&#35752;&#35770;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#24182;&#35814;&#32454;&#25506;&#35752;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community. 2023 ``two sessions" proposal ``suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction" This topic was once a hot topic on the Internet, and ``unbundling" the relationship between birth registration and marriage has become the focus of social debate. In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments. It is found that the discussion on the proposal of ``removing marriage restrictions from birth registration" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behaviour, social ethics and law, and national policy, with people's s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06125</link><description>&lt;p&gt;
&#23398;&#20064;&#23618;&#27425;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20197;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#33258;&#30001;&#36335;&#24452;&#19978;&#36827;&#34892;&#25628;&#32034;&#65292;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#32463;&#24120;&#38656;&#35201;&#25805;&#25511;&#29615;&#22659;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#25171;&#24320;&#38376;&#20197;&#27983;&#35272;&#25151;&#38388;&#65292;&#24182;&#22312;&#27249;&#26588;&#21644;&#25277;&#23625;&#20869;&#25628;&#32034;&#30446;&#26631;&#29289;&#21697;&#12290;&#36825;&#20123;&#26032;&#25361;&#25112;&#38656;&#35201;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#32467;&#21512;&#25805;&#25511;&#21644;&#23548;&#33322;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HIMOS&#65292;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#32452;&#21512;&#25506;&#32034;&#12289;&#23548;&#33322;&#21644;&#25805;&#25511;&#25216;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22260;&#32469;&#35821;&#20041;&#22320;&#22270;&#35760;&#24518;&#30340;&#25277;&#35937;&#39640;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#25506;&#32034;&#36807;&#30340;&#29615;&#22659;&#20316;&#20026;&#23454;&#20363;&#23548;&#33322;&#28857;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;HIMOS&#21487;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26377;&#25928;&#22320;&#36801;&#31227;&#21040;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#24182;&#19988;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06092</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#37327;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#23485;&#24230;&#19982;&#22823;&#24120;&#25968; $n$ &#25104;&#27604;&#20363;&#12290;&#22312;&#38750;&#32447;&#24615;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#26377;&#38480;&#32500;&#20998;&#24067;&#36824;&#26159;&#25972;&#20010;&#36807;&#31243;&#65292;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;&#21450;&#20854;&#23548;&#25968;&#65289;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#37117;&#20250;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#20854;&#20013; $\gamma&gt;0$&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#29992;&#20110;&#24230;&#37327;&#24046;&#24322;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20379;&#30340;&#20219;&#20309;&#30028;&#38480;&#37117;&#35201;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04333</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20998;&#25968;&#30340;&#20248;&#21270;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Robustness via Score-Based Optimization. (arXiv:2307.04333v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26377;&#21487;&#33021;&#36890;&#36807;&#24341;&#20837;&#24494;&#23567;&#25200;&#21160;&#26469;&#35823;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#12290;&#24320;&#21457;&#33021;&#22815;&#20943;&#36731;&#36825;&#20123;&#25915;&#20987;&#24433;&#21709;&#30340;&#31639;&#27861;&#23545;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#23545;&#25239;&#38450;&#24481;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#38450;&#24481;&#20381;&#36182;&#20110;&#39034;&#24207;&#27169;&#25311;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#24182;&#19988;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#22312;&#30001;&#22522;&#20110;&#20998;&#25968;&#20808;&#39564;&#25351;&#23548;&#30340;&#26041;&#21521;&#19978;&#23545;&#21407;&#22987;&#24178;&#20928;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#26469;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;CIFAR10&#12289;CIFAR100&#21644;ImageNet&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustnes
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15724</link><description>&lt;p&gt;
REFLECT:&#23545;&#26426;&#22120;&#20154;&#32463;&#21382;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#29992;&#20110;&#22833;&#36133;&#35299;&#37322;&#21644;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#22833;&#36133;&#25191;&#34892;&#26159;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#31283;&#20581;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#36755;&#20837;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;LLM&#30340;&#21147;&#37327;&#36827;&#34892;&#26426;&#22120;&#20154;&#22833;&#36133;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;REFLECT&#65292;&#23558;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#36807;&#21435;&#32463;&#39564;&#30340;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#36880;&#27493;&#22833;&#36133;&#35299;&#37322;&#31639;&#27861;&#26597;&#35810;LLM&#12290;&#22522;&#20110;&#35299;&#37322;&#65292;&#22833;&#36133;&#32416;&#27491;&#35268;&#21010;&#22120;&#29983;&#25104;&#19968;&#20010;&#21487;&#25191;&#34892;&#35745;&#21010;&#65292;&#20197;&#32416;&#27491;&#22833;&#36133;&#24182;&#23436;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;RoboFail&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#20174;&#32780;&#24110;&#21161;&#25104;&#21151;&#30340;&#32416;&#27491;&#35268;&#21010;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://roboreflect.github.io/
&lt;/p&gt;
&lt;p&gt;
The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09376</link><description>&lt;p&gt;
&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27169;&#22359;&#21270;&#65306;&#19968;&#31181;&#26032;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Modularizing while Training: a New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#24050;&#25104;&#20026;&#26234;&#33021;&#36719;&#20214;&#31995;&#32479;&#20013;&#36234;&#26469;&#36234;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;DNN&#27169;&#22411;&#36890;&#24120;&#22312;&#26102;&#38388;&#21644;&#25104;&#26412;&#26041;&#38754;&#37117;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#20851;&#27880;&#37325;&#29992;&#29616;&#26377;&#30340;DNN&#27169;&#22411;-&#20511;&#37492;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#20195;&#30721;&#37325;&#29992;&#24605;&#24819;&#12290;&#20294;&#26159;&#65292;&#37325;&#29992;&#25972;&#20010;&#27169;&#22411;&#21487;&#33021;&#20250;&#36896;&#25104;&#39069;&#22806;&#30340;&#24320;&#38144;&#25110;&#20174;&#19981;&#38656;&#35201;&#30340;&#21151;&#33021;&#20013;&#32487;&#25215;&#24369;&#28857;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20998;&#35299;&#25104;&#27169;&#22359;&#65292;&#21363;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#65292;&#24182;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24182;&#19981;&#26159;&#20026;&#20102;&#27169;&#22359;&#21270;&#32780;&#26500;&#24314;&#30340;&#65292;&#25152;&#20197;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#20250;&#23548;&#33268;&#24040;&#22823;&#30340;&#24320;&#38144;&#21644;&#27169;&#22411;&#31934;&#24230;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;&#65288;MwT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#27169;&#22411;&#20855;&#26377;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#33021;&#21147;&#65292;&#36825;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#21516;&#26102;&#20248;&#21270;&#27169;&#22359;&#20869;&#30340;&#20869;&#32858;&#24615;&#21644;&#27169;&#22359;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#30495;&#27491;&#30340;&#27169;&#22359;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedJETs&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#37030;&#28151;&#21512;&#19987;&#23478;&#30340;&#26694;&#26550;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#21450;&#26102;&#30340;&#20010;&#24615;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19987;&#38376;&#30340;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#38376;&#25511;&#20989;&#25968;&#23558;&#36755;&#20837;&#36335;&#30001;&#21040;&#30456;&#20851;&#30340;&#19987;&#23478;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08586</link><description>&lt;p&gt;
FedJETs&#65306;&#20855;&#26377;&#32852;&#37030;&#28151;&#21512;&#19987;&#23478;&#30340;&#39640;&#25928;&#21450;&#26102;&#20010;&#24615;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts. (arXiv:2306.08586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedJETs&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#37030;&#28151;&#21512;&#19987;&#23478;&#30340;&#26694;&#26550;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#21450;&#26102;&#30340;&#20010;&#24615;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19987;&#38376;&#30340;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#38376;&#25511;&#20989;&#25968;&#23558;&#36755;&#20837;&#36335;&#30001;&#21040;&#30456;&#20851;&#30340;&#19987;&#23478;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#30446;&#26631;&#20043;&#19968;&#26159;&#21019;&#24314;&#33021;&#22815;&#36866;&#24212;&#27599;&#20010;&#21442;&#19982;&#23458;&#25143;&#31471;&#19978;&#19979;&#25991;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20010;&#24615;&#21270;&#38656;&#35201;&#20351;&#29992;&#23458;&#25143;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#22312;&#26032;&#26469;&#30340;&#23458;&#25143;&#31471;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22312;&#38544;&#31169;&#26041;&#38754;&#20063;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#23454;&#29616;&#21450;&#26102;&#20010;&#24615;&#21270;&#20173;&#28982;&#26159;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedJETs&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;FL&#35774;&#32622;&#20013;&#20351;&#29992;&#8220;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#8221;&#26694;&#26550;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23458;&#25143;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#19981;&#21516;&#30340;&#31867;&#21035;&#23376;&#38598;&#19978;&#35757;&#32451;&#19987;&#38376;&#30340;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#38376;&#25511;&#20989;&#25968;&#23558;&#36755;&#20837;&#36335;&#30001;&#21040;&#26368;&#30456;&#20851;&#30340;&#19987;&#23478;&#12290;&#25105;&#20204;&#30340;&#38376;&#25511;&#20989;&#25968;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20849;&#20139;&#19987;&#23478;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#20854;&#21363;&#26102;&#30340;&#36335;&#30001;&#20915;&#31574;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23558;&#20934;&#30830;&#24615;&#25552;&#39640;&#39640;&#36798;18&#65285;&#65292;&#36798;&#21040;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the goals in Federated Learning (FL) is to create personalized models that can adapt to the context of each participating client, while utilizing knowledge from a shared global model. Yet, often, personalization requires a fine-tuning step using clients' labeled data in order to achieve good performance. This may not be feasible in scenarios where incoming clients are fresh and/or have privacy concerns. It, then, remains open how one can achieve just-in-time personalization in these scenarios. We propose FedJETs, a novel solution by using a Mixture-of-Experts (MoE) framework within a FL setup. Our method leverages the diversity of the clients to train specialized experts on different subsets of classes, and a gating function to route the input to the most relevant expert(s). Our gating function harnesses the knowledge of a pretrained model common expert to enhance its routing decisions on-the-fly. As a highlight, our approach can improve accuracy up to 18\% in state of the art F
&lt;/p&gt;</description></item><item><title>PyTrial&#26159;&#19968;&#20010;&#23454;&#29616;&#22810;&#31181;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#20020;&#24202;&#35797;&#39564;&#20219;&#21153;&#12289;&#21487;&#38598;&#25104;&#33258;&#24049;AI&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#24182;&#22312;&#29616;&#23454;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.04018</link><description>&lt;p&gt;
PyTrial&#65306;&#33647;&#29289;&#30740;&#21457;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
PyTrial: A Comprehensive Platform for Artificial Intelligence for Drug Development. (arXiv:2306.04018v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04018
&lt;/p&gt;
&lt;p&gt;
PyTrial&#26159;&#19968;&#20010;&#23454;&#29616;&#22810;&#31181;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#20020;&#24202;&#35797;&#39564;&#20219;&#21153;&#12289;&#21487;&#38598;&#25104;&#33258;&#24049;AI&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#24182;&#22312;&#29616;&#23454;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30740;&#21457;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#36807;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#20020;&#24202;&#35797;&#39564;&#27979;&#35797;&#20505;&#36873;&#33647;&#29289;&#22312;&#20154;&#20307;&#20869;&#30340;&#30103;&#25928;&#21644;&#23433;&#20840;&#24615;&#20197;&#33719;&#24471;&#30417;&#31649;&#25209;&#20934;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#33647;&#29289;&#30740;&#21457;&#30340;&#37325;&#35201;&#24037;&#20855;&#20986;&#29616;&#20102;&#65292;&#20026;&#25552;&#39640;&#35813;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#20026;&#20102;&#20419;&#36827;&#33647;&#29289;&#30740;&#21457;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;Python&#36719;&#20214;&#21253;&#65292;&#21517;&#20026;PyTrial&#65292;&#35813;&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#22810;&#31181;&#34987;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#20020;&#24202;&#35797;&#39564;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PyTrial&#23454;&#29616;&#20102;6&#20010;&#20851;&#38190;&#30340;&#33647;&#29289;&#30740;&#21457;&#20219;&#21153;&#65292;&#21253;&#25324;&#24739;&#32773;&#32467;&#26524;&#39044;&#27979;&#12289;&#35797;&#39564;&#22320;&#28857;&#36873;&#25321;&#12289;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12289;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#12289;&#35797;&#39564;&#30456;&#20284;&#24615;&#25628;&#32034;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#22312;PyTrial&#20013;&#65292;&#25152;&#26377;&#20219;&#21153;&#37117;&#30001;&#22235;&#20010;&#27493;&#39588;&#23450;&#20041;&#65306;&#21152;&#36733;&#25968;&#25454;&#12289;&#27169;&#22411;&#23450;&#20041;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#27169;&#22411;&#35780;&#20272;&#65292;&#36825;&#21487;&#20197;&#29992;&#20960;&#34892;&#20195;&#30721;&#23436;&#25104;&#12290;&#27492;&#22806;&#65292;&#27169;&#22359;&#21270;&#30340;API&#35774;&#35745;&#20801;&#35768;&#20174;&#19994;&#32773;&#38598;&#25104;&#33258;&#24049;&#30340;AI&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;PyTrial&#24050;&#22312;&#29616;&#23454;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#21463;AI&#25903;&#25345;&#30340;&#20020;&#24202;&#35797;&#39564;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug development is a complex process that aims to test the efficacy and safety of candidate drugs in the human body for regulatory approval via clinical trials. Recently, machine learning has emerged as a vital tool for drug development, offering new opportunities to improve the efficiency and success rates of the process. To facilitate the research and development of artificial intelligence (AI) for drug development, we developed a Python package, namely PyTrial, that implements various clinical trial tasks supported by AI algorithms.  To be specific, PyTrial implements 6 essential drug development tasks, including patient outcome prediction, trial site selection, trial outcome prediction, patient-trial matching, trial similarity search, and synthetic data generation. In PyTrial, all tasks are defined by four steps: load data, model definition, model training, and model evaluation, which can be done with a couple of lines of code. In addition, the modular API design allows practition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03116</link><description>&lt;p&gt;
&#36890;&#36807;&#36716;&#21270;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds. (arXiv:2306.03116v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20174;&#20247;&#21253;&#26381;&#21153;&#20013;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;&#27599;&#20010;&#27880;&#37322;&#32773;&#37117;&#23436;&#25104;&#33258;&#24049;&#30340;&#23567;&#37096;&#20998;&#27880;&#37322;&#65292;&#19981;&#21516;&#27880;&#37322;&#32773;&#30340;&#26631;&#27880;&#38169;&#35823;&#24448;&#24448;&#19981;&#21516;&#12290;&#36890;&#36807;&#26631;&#31614;&#22122;&#22768;&#30340;&#36716;&#31227;&#30697;&#38453;&#26469;&#24314;&#27169;&#22122;&#22768;&#20135;&#29983;&#36807;&#31243;&#26159;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#30340;&#19968;&#31181;&#26377;&#25928;&#24037;&#20855;&#12290;&#22312;&#23454;&#38469;&#20247;&#21253;&#27169;&#22411;&#20013;&#65292;&#36716;&#31227;&#30697;&#38453;&#26082;&#30001;&#27880;&#37322;&#32773;&#20381;&#36182;&#65292;&#20063;&#30001;&#23454;&#20363;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#37322;&#32773;&#21644;&#23454;&#20363;&#20381;&#36182;&#30340;&#36716;&#31227;&#30697;&#38453;(AIDTM)&#20855;&#26377;&#39640;&#22797;&#26434;&#24230;&#65292;&#32780;&#23454;&#38469;&#27880;&#37322;&#24448;&#24448;&#28041;&#21450;&#27880;&#37322;&#31232;&#30095;&#24615;&#65292;&#36825;&#20351;&#24471;&#24314;&#31435;AIDTM&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26082;&#35201;&#20445;&#25345;&#24314;&#27169;&#30340;&#24191;&#27867;&#24615;&#65292;&#21448;&#33021;&#26356;&#30495;&#23454;&#22320;&#35299;&#20915;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#31639;AIDTM&#21644;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from crowds describes that the annotations of training data are obtained with crowd-sourcing services. Multiple annotators each complete their own small part of the annotations, where labeling mistakes that depend on annotators occur frequently. Modeling the label-noise generation process by the noise transition matrix is a power tool to tackle the label noise. In real-world crowd-sourcing scenarios, noise transition matrices are both annotator- and instance-dependent. However, due to the high complexity of annotator- and instance-dependent transition matrices (AIDTM), \textit{annotation sparsity}, which means each annotator only labels a little part of instances, makes modeling AIDTM very challenging. Prior works simplify the problem by assuming the transition matrix is instance-independent or using simple parametric way, while lose modeling generality. Motivated by this, we target a more realistic problem, estimating general AIDTM in practice. Without losing modeling general
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01879</link><description>&lt;p&gt;
VisualGPTScore: &#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#20998;&#25968;&#30340;&#35270;&#35273;&#35821;&#20041;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01879
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; VisualGPTScore &#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#26469;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#19978;&#36816;&#31639;&#12290;&#19982;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#30340;VLM&#21482;&#26159;&#26080;&#24847;&#20041;&#30340;&#21333;&#35789;&#34955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340; VisualGPTScore &#22312; ARO &#21644; Crepe &#31561;&#26368;&#36817;&#25552;&#20986;&#30340;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#39030;&#23574;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$ have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the ${\bf V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal generative}$ score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text) and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725; (UNSB)&#65292;&#23427;&#32467;&#21512;&#20102;&#34203;&#23450;&#35860;&#26725;&#12289;&#23545;&#25239;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#22312;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#23398;&#20064; SDE&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#35768;&#22810;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15086</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725;&#23454;&#29616;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge. (arXiv:2305.15086v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725; (UNSB)&#65292;&#23427;&#32467;&#21512;&#20102;&#34203;&#23450;&#35860;&#26725;&#12289;&#23545;&#25239;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#22312;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#23398;&#20064; SDE&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#35768;&#22810;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#20174;&#22122;&#22768;&#29983;&#25104;&#25968;&#25454;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#39640;&#26031;&#20808;&#39564;&#20551;&#35774;&#65292;&#23427;&#20204;&#22312;&#38750;&#37197;&#23545;&#30340;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#34203;&#23450;&#35860;&#26725;&#26159;&#19968;&#31181;&#23398;&#20064; SDE &#20197;&#22312;&#20004;&#20010;&#20219;&#24847;&#20998;&#24067;&#20043;&#38388;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#34987;&#35270;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#34203;&#23450;&#35860;&#26725;&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20043;&#38388;&#30340;&#38750;&#37197;&#23545;&#36716;&#25442;&#26041;&#38754;&#24182;&#19981;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725;&#65288;UNSB&#65289;&#65292;&#23427;&#23558;&#34203;&#23450;&#35860;&#26725;&#19982;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#23398;&#20064;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#30340; SDE&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; UNSB &#26159;&#21487;&#20280;&#32553;&#30340;&#65292;&#24182;&#19988;&#25104;&#21151;&#35299;&#20915;&#20102;&#21508;&#31181;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. Although diffusion models have achieved remarkable progress in recent years, they have limitations in the unpaired image-to-image translation tasks due to the Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. However, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which combines SB with adversarial training and regularization to learn a SB between unpaired data. We demonstrate that UNSB is scalable, and that it successfully solves various unpaired image-to-image translation tasks. Code: \url{https://github.com/cyclomon/UNSB}
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14979</link><description>&lt;p&gt;
&#23610;&#24230;&#24456;&#37325;&#35201;&#65306;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#35299;&#37322;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scale Matters: Attribution Meets the Wavelet Domain to Explain Model Sensitivity to Image Corruptions. (arXiv:2305.14979v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#30001;&#20110;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#23646;&#24615;&#26041;&#27861;&#23545;&#20110;&#35299;&#37322;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#24378;&#20581;&#24615;&#39046;&#22495;&#30340;&#25991;&#29486;&#20165;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#25439;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#23457;&#26597;&#27169;&#22411;&#30340;&#34892;&#20026;&#33021;&#21147;&#23545;&#20110;&#25552;&#39640;&#29992;&#25143;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Wavelet sCale Attribution Method (WCAM)&#65292;&#23427;&#26159;&#20174;&#20687;&#32032;&#22495;&#21040;&#31354;&#38388;&#23610;&#24230;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#27010;&#25324;&#12290;&#22312;&#31354;&#38388;&#23610;&#24230;&#22495;&#20013;&#36827;&#34892;&#23646;&#24615;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#20851;&#27880;&#28857;&#21644;&#23610;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;WCAM&#35299;&#37322;&#20102;&#27169;&#22411;&#22312;&#22270;&#20687;&#30772;&#22351;&#19979;&#30340;&#22833;&#25928;&#65292;&#30830;&#23450;&#20102;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#36890;&#36807;&#32553;&#25918;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown remarkable performance in computer vision, but their deployment in real-world scenarios is challenging due to their sensitivity to image corruptions. Existing attribution methods are uninformative for explaining the sensitivity to image corruptions, while the literature on robustness only provides model-based explanations. However, the ability to scrutinize models' behavior under image corruptions is crucial to increase the user's trust. Towards this end, we introduce the Wavelet sCale Attribution Method (WCAM), a generalization of attribution from the pixel domain to the space-scale domain. Attribution in the space-scale domain reveals where and on what scales the model focuses. We show that the WCAM explains models' failures under image corruptions, identifies sufficient information for prediction, and explains how zoom-in increases accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;LLMs&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#12290;</title><link>http://arxiv.org/abs/2305.12766</link><description>&lt;p&gt;
&#23558; Emergent In-Context Learning &#35299;&#37322;&#20026;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Explaining Emergent In-Context Learning as Kernel Regression. (arXiv:2305.12766v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;LLMs&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#19968;&#22330;&#33539;&#24335;&#36716;&#21464;&#12290;&#19982;&#32463;&#20856;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#36807;&#31243;&#30456;&#27604;&#65292;&#20026;&#20102;&#23558;LLMs&#29992;&#20110;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#65292;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20123;&#31034;&#20363;&#65292;&#21363;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#25110;&#26356;&#26032;&#29616;&#26377;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#38750;&#24120;&#26377;&#24847;&#24605;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#23436;&#20840;&#20102;&#35299;&#39044;&#35757;&#32451;LLMs&#22914;&#20309;&#33719;&#24471;&#36825;&#31181;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#24403;&#38754;&#20020;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#65292;LLMs&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#65292;&#26469;&#30740;&#31350;&#20026;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#26009;&#24211;&#20043;&#21518;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#26680;&#22238;&#24402; $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing one hypothesis that LLMs can simulate kernel regression with internal representations when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2305.12081</link><description>&lt;p&gt;
AnyPredict: &#34920;&#26684;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#20027;&#35201;&#38382;&#39064;&#21253;&#25324; (1) &#32570;&#20047;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#24102;&#26377;&#26631;&#20934;&#26631;&#31614;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450; (2) &#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#25972;&#21512;&#34920;&#26684;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#27169;&#24335;&#34920;&#26684;&#20043;&#38388;&#30340;&#38556;&#30861;&#65292;&#24182;&#20351;&#29992;&#8220;&#23398;&#20064;&#65292;&#27880;&#37322;&#21644;&#23457;&#35745;&#8221;&#27969;&#31243;&#23558;&#39046;&#22495;&#22806;&#25968;&#25454;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#12290;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#20351;&#39044;&#35757;&#32451;&#30340; AnyPredict &#33021;&#22815;&#25903;&#25345;&#27599;&#20010;&#34920;&#26684;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#38382;&#39064;&#19978;&#30340;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.14993</link><description>&lt;p&gt;
ChatGPT - &#23545;&#20110;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#21644;&#25945;&#24072;&#26159;&#31119;&#26159;&#31096;?
&lt;/p&gt;
&lt;p&gt;
ChatGPT -- a Blessing or a Curse for Undergraduate Computer Science Students and Instructors?. (arXiv:2304.14993v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#38382;&#39064;&#19978;&#30340;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;AI&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#12290;&#23427;&#21487;&#29992;&#20110;&#35821;&#35328;&#29983;&#25104;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#32842;&#22825;&#26426;&#22120;&#20154;&#24320;&#21457;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20869;&#23481;&#21019;&#20316;&#12289;&#20010;&#24615;&#21270;&#12289;&#25991;&#26412;&#23436;&#25104;&#21644;&#25925;&#20107;&#21465;&#36848;&#31561;&#22810;&#31181;&#29992;&#36884;&#12290;&#34429;&#28982;ChatGPT&#21463;&#21040;&#20102;&#30456;&#24403;&#31215;&#26497;&#30340;&#20851;&#27880;&#65292;&#20294;&#22312;&#23398;&#26415;&#30028;&#20063;&#24341;&#36215;&#20102;&#19968;&#31181;&#25285;&#24551;&#21644;&#19981;&#30830;&#23450;&#24863;&#12290;&#23384;&#22312;&#25285;&#24551;&#23398;&#29983;&#21487;&#33021;&#20250;&#21033;&#29992;ChatGPT&#23436;&#25104;&#35838;&#22806;&#20316;&#19994;&#21644;&#32771;&#35797;&#65292;&#24182;&#33719;&#24471;&#26377;&#21033;&#30340;&#25104;&#32489;&#65292;&#32780;&#19981;&#30495;&#27491;&#33719;&#24471;&#30693;&#35782;&#12290;&#26412;&#25991;&#37319;&#29992;&#23450;&#37327;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#33539;&#22260;&#20869;&#30340;&#21508;&#31181;&#38382;&#39064;&#19978;&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23398;&#29983;&#30450;&#30446;&#20381;&#36182;ChatGPT&#23436;&#25104;&#20316;&#19994;&#21644;&#32771;&#35797;&#21487;&#33021;&#20250;&#33258;&#27585;&#21069;&#31243;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#20998;&#26512;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#22914;&#20309;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is an AI language model developed by OpenAI that can understand and generate human-like text. It can be used for a variety of use cases such as language generation, question answering, text summarization, chatbot development, language translation, sentiment analysis, content creation, personalization, text completion, and storytelling. While ChatGPT has garnered significant positive attention, it has also generated a sense of apprehension and uncertainty in academic circles. There is concern that students may leverage ChatGPT to complete take-home assignments and exams and obtain favorable grades without genuinely acquiring knowledge. This paper adopts a quantitative approach to demonstrate ChatGPT's high degree of unreliability in answering a diverse range of questions pertaining to topics in undergraduate computer science. Our analysis shows that students may risk self-sabotage by blindly depending on ChatGPT to complete assignments and exams. We build upon this analysis to p
&lt;/p&gt;</description></item><item><title>MedAlpaca&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21307;&#30103;&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#21512;&#65292;&#26088;&#22312;&#36890;&#36807;&#32454;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#21644;&#21307;&#29983;&#35748;&#35777;&#32771;&#35797;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.08247</link><description>&lt;p&gt;
MedAlpaca -- &#19968;&#20010;&#24320;&#28304;&#30340;&#21307;&#30103;&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data. (arXiv:2304.08247v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08247
&lt;/p&gt;
&lt;p&gt;
MedAlpaca&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21307;&#30103;&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#21512;&#65292;&#26088;&#22312;&#36890;&#36807;&#32454;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#21644;&#21307;&#29983;&#35748;&#35777;&#32771;&#35797;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;OpenAI&#30340;GPT&#31995;&#21015;&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#35265;&#35777;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#20986;&#29616;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#25913;&#21892;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#12289;&#35786;&#26029;&#12289;&#24739;&#32773;&#25252;&#29702;&#21644;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#28304;&#27169;&#22411;&#65292;&#20197;&#22312;&#26412;&#22320;&#37096;&#32626;&#20197;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;16&#19975;&#26465;&#25968;&#25454;&#65292;&#19987;&#38376;&#20026;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#21270;&#35843;&#25972;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19978;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#21270;&#35843;&#25972;&#30340;&#24433;&#21709;&#65292;&#24182;&#38543;&#21518;&#36890;&#36807;&#27604;&#36739;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#32454;&#21270;&#35843;&#25972;&#27169;&#22411;&#22312;&#26410;&#26469;&#21307;&#29983;&#24517;&#39035;&#36890;&#36807;&#30340;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#26469;&#23637;&#31034;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) like OpenAI's GPT series continue to make strides, we witness the emergence of artificial intelligence applications in an ever-expanding range of fields. In medicine, these LLMs hold considerable promise for improving medical workflows, diagnostics, patient care, and education. Yet, there is an urgent need for open-source models that can be deployed on-premises to safeguard patient privacy. In our work, we present an innovative dataset consisting of over 160,000 entries, specifically crafted to fine-tune LLMs for effective medical applications. We investigate the impact of fine-tuning these datasets on publicly accessible pre-trained LLMs, and subsequently, we juxtapose the performance of pre-trained-only models against the fine-tuned models concerning the examinations that future medical doctors must pass to achieve certification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.06715</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#35780;&#20272;&#35299;&#37322;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21482;&#26377;&#24403;&#35299;&#37322;&#26041;&#27861;&#24544;&#23454;&#22320;&#25551;&#36848;&#25152;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;&#65292;&#35299;&#37322;&#26041;&#27861;&#25165;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#39044;&#27979;&#22312;&#29305;&#23450;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#36825;&#21253;&#25324;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#26550;&#26500;&#12290;&#20219;&#20309;&#24544;&#23454;&#25551;&#36848;&#36825;&#31181;&#31867;&#22411;&#27169;&#22411;&#30340;&#35299;&#37322;&#37117;&#38656;&#35201;&#19982;&#35813;&#19981;&#21464;&#24615;&#23646;&#24615;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#36816;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#26469;&#24418;&#24335;&#21270;&#36825;&#31181;&#30452;&#35273;&#12290;&#36890;&#36807;&#36825;&#31181;&#20005;&#26684;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#65288;1&#65289;&#20004;&#20010;&#24230;&#37327;&#26469;&#34913;&#37327;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#27169;&#22411;&#23545;&#31216;&#32676;&#30340;&#20581;&#22766;&#24615;;&#65288;2&#65289;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#65307;&#65288;3&#65289;&#25552;&#39640;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#23545;&#31216;&#32676;&#30340;&#19981;&#21464;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#19982;&#19981;&#21516;&#23545;&#31216;&#32676;&#30456;&#20851;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#32463;&#39564;&#22320;&#27979;&#37327;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#23545;&#20110;&#24378;&#22823;&#30340;&#35299;&#37322;&#26041;&#27861;&#26159;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#65292;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05128</link><description>&lt;p&gt;
&#33258;&#25105;&#35843;&#35797;&#65306;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Teaching Large Language Models to Self-Debug. (arXiv:2304.05128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#65292;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#32534;&#31243;&#20219;&#21153;&#65292;&#22312;&#19968;&#27425;&#24615;&#29983;&#25104;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#26469;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#35843;&#35797;(Self-Debugging)&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#35843;&#35797;&#21487;&#20197;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27233;&#30382;&#40493;&#23376;&#35843;&#35797;(Rubber Duck Debugging)&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#20195;&#30721;&#27491;&#30830;&#24615;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#29983;&#25104;&#30340;&#20195;&#30721;&#26469;&#35782;&#21035;&#23427;&#30340;&#38169;&#35823;&#12290;&#33258;&#25105;&#35843;&#35797;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#30340;Spider&#25968;&#25454;&#38598;&#65292;C++&#21040;Python&#32763;&#35793;&#30340;TransCoder&#21644;&#25991;&#26412;&#21040;Python&#29983;&#25104;&#30340;MBPP&#12290;&#22312;&#27809;&#26377;&#21333;&#20803;&#27979;&#35797;&#30340;Spider&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10650</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#36923;&#36753;&#30340;&#36923;&#36753;&#65306;&#36208;&#21521;DL&#30340;&#32479;&#19968;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#21487;&#24494;&#20998;&#36923;&#36753;&#65288;DL&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#28385;&#36275;&#36923;&#36753;&#35268;&#33539;&#30340;&#26041;&#27861;&#12290;DL&#21253;&#25324;&#35821;&#27861;&#21644;&#23558;&#35821;&#27861;&#20013;&#30340;&#34920;&#36798;&#24335;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#35299;&#37322;&#20989;&#25968;&#12290;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290; &#29616;&#26377;DL&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#20854;&#24418;&#24335;&#21270;&#31243;&#24230;&#30340;&#19981;&#21516;&#22788;&#29702;&#20351;&#24471;&#23545;&#23427;&#20204;&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#20316;&#20026;DL&#23450;&#20041;&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.04054</link><description>&lt;p&gt;
&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25512;&#29702;&#22797;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#8212;&#8212;&#21363;&#22312;&#22797;&#21046;&#30340;&#27169;&#22411;&#35757;&#32451;&#36816;&#34892;&#20013;&#35266;&#23519;&#21040;&#30340;&#35780;&#20272;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21463;&#21040;&#20960;&#31181;&#38750;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#27979;&#37327;&#22122;&#22768;&#12290;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#21435;&#38500;&#22122;&#22768;&#65292;&#20197;&#24378;&#21046;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#24573;&#30053;&#20102;&#23454;&#29616;&#23618;&#38754;&#22266;&#26377;&#30340;&#38750;&#30830;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#22122;&#22768;&#22240;&#32032;&#21644;&#25968;&#25454;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#36825;&#38480;&#21046;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#32467;&#35770;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#20960;&#20010;&#26041;&#24046;&#26469;&#28304;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#25968;&#25454;&#29305;&#24615;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#26174;&#33879;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#20013;&#65292;&#20197;&#26399;&#20174;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#23450;&#23454;&#20363;&#24471;&#20986;&#25512;&#29702;&#32467;&#35770;, &#32780;&#38750;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#29992;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#32771;&#34385;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#28304;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#21508;&#20010;&#26041;&#24046;&#26469;&#28304;&#23545;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AI&#23545;&#40784;&#23545;&#35805;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#21644;&#20195;&#29702;&#30340;&#20132;&#20114;&#21162;&#21147;&#23454;&#29616;&#21644;&#32500;&#25345;AI&#23545;&#40784;&#65292;&#24182;&#30456;&#27604;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#34892;&#20026;&#25903;&#25345;&#20195;&#29702;&#26041;&#38754;&#20855;&#26377;&#26356;&#22810;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.06421</link><description>&lt;p&gt;
AI&#23545;&#40784;&#23545;&#35805;&#65306;&#19968;&#31181;&#19982;&#25903;&#25345;&#20195;&#29702;&#23436;&#25104;AI&#23545;&#40784;&#30340;&#20132;&#20114;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AI Alignment Dialogues: An Interactive Approach to AI Alignment in Support Agents. (arXiv:2301.06421v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AI&#23545;&#40784;&#23545;&#35805;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#21644;&#20195;&#29702;&#30340;&#20132;&#20114;&#21162;&#21147;&#23454;&#29616;&#21644;&#32500;&#25345;AI&#23545;&#40784;&#65292;&#24182;&#30456;&#27604;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#34892;&#20026;&#25903;&#25345;&#20195;&#29702;&#26041;&#38754;&#20855;&#26377;&#26356;&#22810;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#26159;&#30830;&#20445;AI&#31995;&#32479;&#21482;&#36861;&#27714;&#26377;&#30410;&#20110;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#27963;&#21160;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;AI&#23545;&#40784;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#34892;&#20026;&#25968;&#25454;&#26469;&#20102;&#35299;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23545;&#40784;&#27010;&#24565;&#65292;&#21363;&#24341;&#20837;AI&#23545;&#40784;&#23545;&#35805;&#65306;&#29992;&#25143;&#21644;&#20195;&#29702;&#36890;&#36807;&#20132;&#20114;&#21162;&#21147;&#23454;&#29616;&#21644;&#32500;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23545;&#40784;&#23545;&#35805;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#34892;&#20026;&#25903;&#25345;&#20195;&#29702;&#65292;&#36825;&#20123;&#20195;&#29702;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#20182;&#20204;&#26399;&#26395;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24403;&#21069;&#30340;&#34892;&#20026;&#12290;&#23545;&#40784;&#23545;&#35805;&#30340;&#20248;&#21183;&#21253;&#25324;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#20256;&#36798;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#32473;&#20195;&#29702;&#65292;&#24182;&#20351;&#20195;&#29702;&#26356;&#21152;&#36879;&#26126;&#21644;&#21487;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#23545;&#40784;&#23545;&#35805;&#30340;&#27010;&#24565;&#21644;&#39640;&#23618;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#24615;&#30340;&#28966;&#28857;&#23567;&#32452;&#29992;&#25143;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI alignment is about ensuring AI systems only pursue goals and activities that are beneficial to humans. Most of the current approach to AI alignment is to learn what humans value from their behavioural data. This paper proposes a different way of looking at the notion of alignment, namely by introducing AI Alignment Dialogues: dialogues with which users and agents try to achieve and maintain alignment via interaction. We argue that alignment dialogues have a number of advantages in comparison to data-driven approaches, especially for behaviour support agents, which aim to support users in achieving their desired future behaviours rather than their current behaviours. The advantages of alignment dialogues include allowing the users to directly convey higher-level concepts to the agent, and making the agent more transparent and trustworthy. In this paper we outline the concept and high-level structure of alignment dialogues. Moreover, we conducted a qualitative focus group user study f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#23494;&#24230;&#32858;&#31867;&#21644;&#36136;&#24515;&#20998;&#26512;&#30340;&#31574;&#30053;&#65292;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#21542;&#21463;&#21040;&#21453;&#21521;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#25915;&#20987;&#26080;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.04554</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#23494;&#24230;&#32858;&#31867;&#21644;&#36136;&#24515;&#20998;&#26512;&#30340;&#26041;&#27861;&#36827;&#34892;&#21453;&#21521;&#25915;&#20987;&#30340;&#36890;&#29992;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis. (arXiv:2301.04554v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04554
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#23494;&#24230;&#32858;&#31867;&#21644;&#36136;&#24515;&#20998;&#26512;&#30340;&#31574;&#30053;&#65292;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#21542;&#21463;&#21040;&#21453;&#21521;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#25915;&#20987;&#26080;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#36136;&#24515;&#20998;&#26512;&#30340;&#36890;&#29992;&#38450;&#24481;&#26041;&#27861;&#65288;CCA-UD&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#21542;&#21463;&#21040;&#21453;&#21521;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;CCA-UD&#39318;&#20808;&#36890;&#36807;&#23494;&#24230;&#32858;&#31867;&#30340;&#26041;&#27861;&#23545;&#35757;&#32451;&#38598;&#26679;&#26412;&#36827;&#34892;&#32858;&#31867;&#65292;&#28982;&#21518;&#24212;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#26469;&#26816;&#27979;&#26159;&#21542;&#23384;&#22312;&#34987;&#27745;&#26579;&#30340;&#32858;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#21040;&#30340;&#24120;&#35268;&#35823;&#20998;&#31867;&#34892;&#20026;&#65292;&#21363;&#24403;&#34987;&#20998;&#26512;&#32858;&#31867;&#30340;&#20195;&#34920;&#24615;&#31034;&#20363;&#30340;&#29305;&#24449;&#28155;&#21152;&#21040;&#33391;&#24615;&#26679;&#26412;&#20013;&#26102;&#20135;&#29983;&#35823;&#20998;&#31867;&#12290;&#35825;&#23548;&#35823;&#20998;&#31867;&#38169;&#35823;&#30340;&#33021;&#21147;&#26159;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#26222;&#36941;&#29305;&#24449;&#65292;&#22240;&#27492;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#26159;&#25915;&#20987;&#26080;&#20851;&#30340;&#65292;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#26377;&#26126;&#26174;&#30340;&#19981;&#21516;&#65292;&#35201;&#20040;&#21482;&#33021;&#38450;&#24481;&#26576;&#20123;&#31867;&#22411;&#30340;&#21453;&#21521;&#25915;&#20987;&#65292;&#35201;&#20040;&#21482;&#22312;&#26576;&#20123;&#20851;&#20110;&#27745;&#26579;&#27604;&#20363;&#25110;&#35302;&#21457;&#20449;&#21495;&#31867;&#22411;&#30340;&#26465;&#20214;&#19979;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Universal Defence against backdoor attacks based on Clustering and Centroids Analysis (CCA-UD). The goal of the defence is to reveal whether a Deep Neural Network model is subject to a backdoor attack by inspecting the training dataset. CCA-UD first clusters the samples of the training set by means of density-based clustering. Then, it applies a novel strategy to detect the presence of poisoned clusters. The proposed strategy is based on a general misclassification behaviour observed when the features of a representative example of the analysed cluster are added to benign samples. The capability of inducing a misclassification error is a general characteristic of poisoned samples, hence the proposed defence is attack-agnostic. This marks a significant difference with respect to existing defences, that, either can defend against only some types of backdoor attacks, or are effective only when some conditions on the poisoning ratio or the kind of triggering signal used by the
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20559;&#35265;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#25490;&#21517;&#22270;&#20687;&#30340;&#34394;&#20551;&#24615;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#29575;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#20844;&#27491;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2212.02648</link><description>&lt;p&gt;
Spuriosity Rankings: &#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases. (arXiv:2212.02648v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20559;&#35265;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#25490;&#21517;&#22270;&#20687;&#30340;&#34394;&#20551;&#24615;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#29575;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#20844;&#27491;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20381;&#36182;&#25152;&#24341;&#36215;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#25968;&#25454;&#25110;&#27169;&#22411;&#35757;&#32451;&#36827;&#34892;&#26114;&#36149;&#30340;&#25913;&#21464;&#65292;&#32780;&#26159;&#26356;&#22909;&#22320;&#21033;&#29992;&#24050;&#26377;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#36890;&#36807;&#21487;&#35299;&#37322;&#32593;&#32476;&#30340;&#28145;&#24230;&#31070;&#32463;&#29305;&#24449;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#31867;&#20869;&#25490;&#24207;&#65292;&#20197;&#34913;&#37327;&#20854;&#34394;&#20551;&#24615;&#65288;&#21363;&#24120;&#35265;&#34394;&#20551;&#32447;&#32034;&#30340;&#23384;&#22312;&#31243;&#24230;&#65289;&#12290;&#36890;&#36807;&#34394;&#20551;&#24615;&#25490;&#21517;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65288;&#21363;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#65289;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#29978;&#33267;&#21487;&#20197;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#20998;&#31867;&#22836;&#37096;&#65292;&#20197;&#26497;&#23569;&#30340;&#20934;&#30830;&#29575;&#25439;&#22833;&#26469;&#26377;&#25928;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#26356;&#20844;&#27491;&#22788;&#29702;&#65292;&#26080;&#35770;&#34394;&#20551;&#24615;&#22914;&#20309;&#12290;&#25105;&#20204;&#22312;ImageNet&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27880;&#37322;&#20102;5000&#20010;&#31867;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#65288;&#20854;&#20013;630&#20010;&#26159;&#34394;&#20551;&#30340;&#65289;&#65292;&#24182;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;325k&#20010;&#36719;&#20998;&#21106;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft seg
&lt;/p&gt;</description></item><item><title>BiViT&#26159;&#19968;&#31181;&#26497;&#24230;&#21387;&#32553;&#30340;&#20108;&#36827;&#21046;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;Softmax-aware&#20108;&#36827;&#21046;&#21270;&#21644;&#36328;&#23618;&#20108;&#36827;&#21046;&#21270;&#26041;&#26696;&#35299;&#20915;&#20102;&#22312;&#20108;&#36827;&#21046;&#21270;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#27880;&#24847;&#21147;&#35823;&#24046;&#21644;&#20449;&#24687;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.07091</link><description>&lt;p&gt;
BiViT: &#26497;&#24230;&#21387;&#32553;&#30340;&#20108;&#36827;&#21046;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
BiViT: Extremely Compressed Binary Vision Transformer. (arXiv:2211.07091v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07091
&lt;/p&gt;
&lt;p&gt;
BiViT&#26159;&#19968;&#31181;&#26497;&#24230;&#21387;&#32553;&#30340;&#20108;&#36827;&#21046;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;Softmax-aware&#20108;&#36827;&#21046;&#21270;&#21644;&#36328;&#23618;&#20108;&#36827;&#21046;&#21270;&#26041;&#26696;&#35299;&#20915;&#20102;&#22312;&#20108;&#36827;&#21046;&#21270;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#27880;&#24847;&#21147;&#35823;&#24046;&#21644;&#20449;&#24687;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20108;&#36827;&#21046;&#21270;&#21487;&#20197;&#36890;&#36807;&#39640;&#25928;&#30340;&#20301;&#36816;&#31639;&#26174;&#33879;&#21387;&#32553;&#27169;&#22411;&#22823;&#23567;&#65292;&#20943;&#23569;&#33021;&#32791;&#24182;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#12290;&#34429;&#28982;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20108;&#36827;&#21046;&#21270;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#22810;&#65292;&#20294;&#23545;&#20110;&#22312;&#35270;&#35273;&#35782;&#21035;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#30340;&#35270;&#35273;Transformer&#30340;&#20108;&#36827;&#21046;&#21270;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#20108;&#36827;&#21046;&#35270;&#35273;Transformer&#65288;BiViT&#65289;&#30340;&#20004;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;softmax&#27880;&#24847;&#21147;&#30340;&#38271;&#23614;&#20998;&#24067;&#65292;&#23548;&#33268;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#23384;&#22312;&#36739;&#22823;&#30340;&#20108;&#36827;&#21046;&#21270;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#24182;&#20943;&#23569;&#22240;&#20108;&#36827;&#21046;&#21270;&#23548;&#33268;&#30340;&#35823;&#24046;&#30340;Softmax-aware &#20108;&#36827;&#21046;&#21270;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20449;&#24687;&#24182;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#23618;&#20108;&#36827;&#21046;&#21270;&#26041;&#26696;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#30340;&#20108;&#36827;&#21046;&#21270;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model binarization can significantly compress model size, reduce energy consumption, and accelerate inference through efficient bit-wise operations. Although binarizing convolutional neural networks have been extensively studied, there is little work on exploring binarization of vision Transformers which underpin most recent breakthroughs in visual recognition. To this end, we propose to solve two fundamental challenges to push the horizon of Binary Vision Transformers (BiViT). First, the traditional binary method does not take the long-tailed distribution of softmax attention into consideration, bringing large binarization errors in the attention module. To solve this, we propose Softmax-aware Binarization, which dynamically adapts to the data distribution and reduces the error caused by binarization. Second, to better preserve the information of the pretrained model and restore accuracy, we propose a Cross-layer Binarization scheme that decouples the binarization of self-attention an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#26469;&#20272;&#35745;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#65292;&#20854;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.17505</link><description>&lt;p&gt;
&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Space-fluid Adaptive Sampling by Self-Organisation. (arXiv:2210.17505v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#26469;&#20272;&#35745;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#65292;&#20854;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#35843;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#31649;&#29702;&#65288;&#20272;&#35745;&#12289;&#39044;&#27979;&#25110;&#25511;&#21046;&#65289;&#38543;&#31354;&#38388;&#21464;&#21270;&#30340;&#20449;&#21495;&#65292;&#20363;&#22914;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31454;&#20105;&#21644;&#29983;&#38271;/&#32553;&#23567;&#30340;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#26041;&#27861;&#65292;&#21327;&#21516;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#20272;&#35745;&#31354;&#38388;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#22330;&#30340;&#21327;&#35843;&#26694;&#26550;&#20013;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#33258;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20272;&#35745;&#22797;&#26434;&#20989;&#25968;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#21644;&#36319;&#36394;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#26102;&#31354;&#29616;&#35937;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recurrent task in coordinated systems is managing (estimating, predicting, or controlling) signals that vary in space, such as distributed sensed data or computation outcomes. Especially in large-scale settings, the problem can be addressed through decentralised and situated computing systems: nodes can locally sense, process, and act upon signals, and coordinate with neighbours to implement collective strategies. Accordingly, in this work we devise distributed coordination strategies for the estimation of a spatial phenomenon through collaborative adaptive sampling. Our design is based on the idea of dynamically partitioning space into regions that compete and grow/shrink to provide accurate aggregate sampling. Such regions hence define a sort of virtualised space that is "fluid", since its structure adapts in response to pressure forces exerted by the underlying phenomenon. We provide an adaptive sampling algorithm in the field-based coordination framework, and prove it is self-sta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#36974;&#34109;&#21367;&#31215;&#21464;&#21387;&#22120;&#22359;&#65288;SSMCTB&#65289;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#26680;&#24515;&#26550;&#26500;&#23618;&#38754;&#19978;&#38598;&#25104;&#20102;&#37325;&#26500;&#21151;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#30340;&#20449;&#24687;&#36974;&#34109;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.12148</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#36974;&#34109;&#21367;&#31215;&#21464;&#21387;&#22120;&#22359;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Masked Convolutional Transformer Block for Anomaly Detection. (arXiv:2209.12148v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#36974;&#34109;&#21367;&#31215;&#21464;&#21387;&#22120;&#22359;&#65288;SSMCTB&#65289;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#26680;&#24515;&#26550;&#26500;&#23618;&#38754;&#19978;&#38598;&#25104;&#20102;&#37325;&#26500;&#21151;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#30340;&#20449;&#24687;&#36974;&#34109;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#26368;&#36817;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#24037;&#19994;&#29983;&#20135;&#32447;&#19978;&#30340;&#20135;&#21697;&#25925;&#38556;&#26816;&#27979;&#12289;&#35270;&#39057;&#30417;&#25511;&#20013;&#21363;&#23558;&#21457;&#29983;&#30340;&#20107;&#20214;&#26816;&#27979;&#20197;&#21450;&#21307;&#23398;&#25195;&#25551;&#20013;&#30340;&#30149;&#21464;&#26816;&#27979;&#31561;&#12290;&#26080;&#35770;&#26159;&#21738;&#20010;&#39046;&#22495;&#65292;&#24322;&#24120;&#26816;&#27979;&#36890;&#24120;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#21333;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#23398;&#20064;&#20165;&#22312;&#27491;&#24120;&#31034;&#20363;&#19978;&#36827;&#34892;&#12290;&#25104;&#21151;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#23478;&#26063;&#22522;&#20110;&#23398;&#20064;&#37325;&#26500;&#36974;&#34109;&#30340;&#27491;&#24120;&#36755;&#20837;&#65288;&#20363;&#22914;&#34917;&#19969;&#12289;&#26410;&#26469;&#24103;&#31561;&#65289;&#65292;&#24182;&#23558;&#37325;&#26500;&#35823;&#24046;&#30340;&#22823;&#23567;&#20316;&#20026;&#24322;&#24120;&#31243;&#24230;&#30340;&#25351;&#31034;&#22120;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#36974;&#34109;&#21367;&#31215;&#21464;&#21387;&#22120;&#22359; (SSMCTB)&#65292;&#35813;&#22359;&#22312;&#26680;&#24515;&#26550;&#26500;&#23618;&#38754;&#19978;&#21253;&#21547;&#20102;&#22522;&#20110;&#37325;&#26500;&#30340;&#21151;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#33258;&#30417;&#30563;&#22359;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#38754;&#36827;&#34892;&#20449;&#24687;&#36974;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection has recently gained increasing attention in the field of computer vision, likely due to its broad set of applications ranging from product fault detection on industrial production lines and impending event detection in video surveillance to finding lesions in medical scans. Regardless of the domain, anomaly detection is typically framed as a one-class classification task, where the learning is conducted on normal examples only. An entire family of successful anomaly detection methods is based on learning to reconstruct masked normal inputs (e.g. patches, future frames, etc.) and exerting the magnitude of the reconstruction error as an indicator for the abnormality level. Unlike other reconstruction-based methods, we present a novel self-supervised masked convolutional transformer block (SSMCTB) that comprises the reconstruction-based functionality at a core architectural level. The proposed self-supervised block is extremely flexible, enabling information masking at a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25104;&#21151;&#20174;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#35760;&#24405;&#20013;&#35299;&#30721;&#24863;&#30693;&#35821;&#38899;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#39640;&#36798;41%&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#19982;&#33041;&#30005;&#20449;&#21495;&#30456;&#23545;&#24212;&#30340;&#35821;&#38899;&#29255;&#27573;&#12290;</title><link>http://arxiv.org/abs/2208.12266</link><description>&lt;p&gt;
&#20174;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#35760;&#24405;&#20013;&#35299;&#30721;&#35821;&#38899;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Decoding speech perception from non-invasive brain recordings. (arXiv:2208.12266v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25104;&#21151;&#20174;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#35760;&#24405;&#20013;&#35299;&#30721;&#24863;&#30693;&#35821;&#38899;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#39640;&#36798;41%&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#19982;&#33041;&#30005;&#20449;&#21495;&#30456;&#23545;&#24212;&#30340;&#35821;&#38899;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#30005;&#27963;&#21160;&#20013;&#35299;&#30721;&#35821;&#38899;&#19968;&#30452;&#26159;&#21307;&#30103;&#20445;&#20581;&#21644;&#31070;&#32463;&#31185;&#23398;&#20013;&#26399;&#24453;&#24050;&#20037;&#30340;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#20405;&#20837;&#24615;&#35774;&#22791;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65306;&#22522;&#20110;&#39045;&#20869;&#35760;&#24405;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#29616;&#22312;&#21487;&#20197;&#35299;&#30721;&#22522;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#65288;&#20363;&#22914;&#23383;&#27597;&#12289;&#21333;&#35789;&#12289;&#39057;&#35889;&#22270;&#65289;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#33258;&#28982;&#35821;&#38899;&#21644;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#35760;&#24405;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#20581;&#24247;&#20010;&#20307;&#30340;&#38750;&#20405;&#20837;&#24615;&#35760;&#24405;&#20013;&#35299;&#30721;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#30340;&#24863;&#30693;&#35821;&#38899;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;175&#21517;&#24535;&#24895;&#32773;&#30340;&#33041;&#30913;&#22270;&#25110;&#33041;&#30005;&#22270;&#35760;&#24405;&#65292;&#20182;&#20204;&#22312;&#21548;&#30701;&#31687;&#25925;&#20107;&#21644;&#23396;&#31435;&#30340;&#21477;&#23376;&#26102;&#35760;&#24405;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;3&#31186;&#30340;&#33041;&#30913;&#22270;&#20449;&#21495;&#20013;&#20197;&#39640;&#36798;41%&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#30456;&#24212;&#30340;&#35821;&#38899;&#29255;&#27573;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;1000&#20010;&#20197;&#19978;&#30340;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding speech from brain activity is a long-awaited goal in both healthcare and neuroscience. Invasive devices have recently led to major milestones in that regard: deep learning algorithms trained on intracranial recordings now start to decode elementary linguistic features (e.g. letters, words, spectrograms). However, extending this approach to natural speech and non-invasive brain recordings remains a major challenge. Here, we introduce a model trained with contrastive-learning to decode self-supervised representations of perceived speech from the non-invasive recordings of a large cohort of healthy individuals. To evaluate this approach, we curate and integrate four public datasets, encompassing 175 volunteers recorded with magneto- or electro-encephalography (M/EEG), while they listened to short stories and isolated sentences. The results show that our model can identify, from 3 seconds of MEG signals, the corresponding speech segment with up to 41% accuracy out of more than 1,0
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22522;&#20110;&#21516;&#19968;&#24037;&#19994;&#36807;&#31243;&#20013;&#20247;&#22810;&#20855;&#26377;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#30340;&#21464;&#37327;&#65292;&#23454;&#29616;&#20102;&#21464;&#37327;&#29305;&#24449;&#24314;&#27169;&#21644;&#34920;&#31034;&#12289;&#22270;&#32593;&#32476;&#26500;&#24314;&#21450;&#22270;&#29305;&#24449;&#24863;&#30693;&#65292;&#24182;&#24212;&#29992;&#20110;&#36807;&#31243;&#30417;&#27979;&#20013;&#12290;</title><link>http://arxiv.org/abs/2205.05250</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#20851;&#32852;&#34920;&#31034;&#27861;&#21450;&#20854;&#22312;&#36807;&#31243;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;: &#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal associations representation and application for process monitoring using graph convolution neural network. (arXiv:2205.05250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05250
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22522;&#20110;&#21516;&#19968;&#24037;&#19994;&#36807;&#31243;&#20013;&#20247;&#22810;&#20855;&#26377;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#30340;&#21464;&#37327;&#65292;&#23454;&#29616;&#20102;&#21464;&#37327;&#29305;&#24449;&#24314;&#27169;&#21644;&#34920;&#31034;&#12289;&#22270;&#32593;&#32476;&#26500;&#24314;&#21450;&#22270;&#29305;&#24449;&#24863;&#30693;&#65292;&#24182;&#24212;&#29992;&#20110;&#36807;&#31243;&#30417;&#27979;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#35874;&#21516;&#34892;&#21644;&#23398;&#32773;&#20204;&#23545;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#27880;&#21644;&#20851;&#24515;&#12290;&#22312;&#19987;&#23478;&#12289;&#32534;&#36753;&#21644;&#23457;&#31295;&#20154;&#30340;&#35780;&#35770;&#21644;&#25351;&#23548;&#19979;&#65292;&#36825;&#39033;&#24037;&#20316;&#24050;&#34987;&#25509;&#21463;&#21457;&#34920;&#22312;&#12298;&#36807;&#31243;&#23433;&#20840;&#19982;&#29615;&#22659;&#20445;&#25252;&#12299;&#26399;&#21002;&#19978;&#12290;&#26412;&#25991;&#30340;&#20027;&#39064;&#26159;&#22522;&#20110;&#21516;&#19968;&#24037;&#19994;&#36807;&#31243;&#20013;&#20247;&#22810;&#21464;&#37327;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20851;&#32852;&#65292;&#36825;&#26159;&#25351;&#22312;&#21160;&#24577;&#24037;&#19994;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#20247;&#22810;&#20855;&#26377;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#30340;&#21464;&#37327;&#65292;&#21363;&#36825;&#20123;&#21464;&#37327;&#19981;&#20165;&#22312;&#26102;&#38388;&#19978;&#39640;&#24230;&#30456;&#20851;&#65292;&#20063;&#22312;&#31354;&#38388;&#19978;&#30456;&#20114;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#35299;&#20915;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#21464;&#37327;&#29305;&#24449;&#24314;&#27169;&#21644;&#34920;&#31034;&#12289;&#22270;&#32593;&#32476;&#26500;&#24314;&#65288;&#26102;&#38388;&#20449;&#24687;&#65289;&#21644;&#22270;&#29305;&#24449;&#24863;&#30693;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#36890;&#36807;&#20551;&#35774;&#25968;&#25454;&#26381;&#20174;&#25913;&#36827;&#30340;&#39640;&#26031;&#20998;&#24067;&#26469;&#23454;&#29616;&#65292;&#32780;&#22270;&#32593;&#32476;&#21487;&#20197;&#30001;&#30417;&#27979;&#21464;&#37327;&#21450;&#20854;&#36793;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thank you very much for the attention and concern of colleagues and scholars in this work. With the comments and guidance of experts, editors, and reviewers, this work has been accepted for publishing in the journal "Process Safety and Environmental Protection". The theme of this paper relies on the Spatial-temporal associations of numerous variables in the same industrial processes, which refers to numerous variables obtained in dynamic industrial processes with Spatial-temporal correlation characteristics, i.e., these variables are not only highly correlated in time but also interrelated in space. To handle this problem, three key issues need to be well addressed: variable characteristics modeling and representation, graph network construction (temporal information), and graph characteristics perception. The first issue is implemented by assuming the data follows one improved Gaussian distribution, while the graph network can be defined by the monitoring variables and their edges whi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#27979;&#24230;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#25972;&#20307;&#27979;&#24230;&#26469;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#23545;&#35937;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#29983;&#25104;&#31639;&#27861;&#20013;&#23545;&#35937;&#37325;&#22797;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.09573</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversity in deep generative models and generative AI. (arXiv:2202.09573v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#27979;&#24230;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#25972;&#20307;&#27979;&#24230;&#26469;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#23545;&#35937;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#29983;&#25104;&#31639;&#27861;&#20013;&#23545;&#35937;&#37325;&#22797;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#25104;&#31639;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#22312;&#26500;&#24314;&#19982;&#35757;&#32451;&#38598;&#20013;&#30456;&#20284;&#30340;&#23545;&#35937;&#26102;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#26032;&#23545;&#35937;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38544;&#34255;&#32467;&#26500;&#30340;&#29702;&#35299;&#65292;&#28982;&#21518;&#20174;&#22810;&#32500;&#27491;&#24577;&#21464;&#37327;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23588;&#20854;&#26159;&#65292;&#27599;&#20010;&#26679;&#26412;&#37117;&#26159;&#29420;&#31435;&#30340;&#65292;&#21487;&#33021;&#20250;&#37325;&#22797;&#25552;&#20986;&#30456;&#21516;&#31867;&#22411;&#30340;&#23545;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#27979;&#24230;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23558;&#32473;&#23450;&#30446;&#26631;&#27979;&#24230;&#36817;&#20284;&#20026;&#25972;&#20307;&#26469;&#29983;&#25104;&#26032;&#23545;&#35937;&#65292;&#24182;&#19988;&#33021;&#22815;&#36991;&#20813;&#20174;&#35813;&#20998;&#24067;&#20013;&#24050;&#32463;&#32472;&#21046;&#30340;&#20803;&#32032;&#12290;&#36825;&#30830;&#20445;&#20102;&#29983;&#25104;&#23545;&#35937;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning generative algorithms such as Generative Adversarial Networks (GAN) and Variational Auto-Encoders (VAE) show impressive results when constructing objects similar to those in a training ensemble. However, the generation of new objects builds mainly on the understanding of the hidden structure of the training dataset followed by a sampling from a multi-dimensional normal variable. In particular each sample is independent from the others and can repeatedly propose same type of objects. To cure this drawback we introduce a kernel-based measure quantization method that can produce new objects from a given target measure by approximating it as a whole and even staying away from elements already drawn from that distribution. This ensures a better diversity of the produced objects. The method is tested on classic machine learning benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#27169;&#22411;&#31867;&#21035;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26032;&#30340;&#22270;&#27169;&#22411;&#65292;&#21487;&#20197;&#24471;&#20986;&#26356;&#24378;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2112.08417</link><description>&lt;p&gt;
&#23545;&#20855;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#22240;&#26524;&#31062;&#20808;&#22270;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Characterization of causal ancestral graphs for time series with latent confounders. (arXiv:2112.08417v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#27169;&#22411;&#31867;&#21035;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26032;&#30340;&#22270;&#27169;&#22411;&#65292;&#21487;&#20197;&#24471;&#20986;&#26356;&#24378;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#27169;&#22411;&#31867;&#21035;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#28382;&#21518;&#29305;&#23450;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29420;&#31435;&#24615;&#12290;&#25105;&#20204;&#23436;&#20840;&#29305;&#24449;&#21270;&#20102;&#36825;&#20123;&#22270;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#26159;&#24403;&#21069;&#20351;&#29992;&#30340;&#27169;&#22411;&#31867;&#21035;&#30340;&#36866;&#24403;&#23376;&#38598;&#12290;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26032;&#30340;&#22270;&#21487;&#20197;&#24471;&#20986;&#26356;&#24378;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#36825;&#20123;&#26032;&#22270;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#25152;&#23398;&#21040;&#30340;&#20869;&#23481;&#30456;&#27604;&#65292;&#36825;&#31181;&#22270;&#24418;&#34920;&#31034;&#21253;&#21547;&#26356;&#22810;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel class of graphical models for representing time lag specific causal relationships and independencies of multivariate time series with unobserved confounders. We completely characterize these graphs and show that they constitute proper subsets of the currently employed model classes. As we show, from the novel graphs one can thus draw stronger causal inferences -- without additional assumptions. We further introduce a graphical representation of Markov equivalence classes of the novel graphs. This graphical representation contains more causal knowledge than what current state-of-the-art causal discovery algorithms learn.
&lt;/p&gt;</description></item><item><title>Colossal-AI&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#24182;&#34892;&#35757;&#32451;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22815;&#20197;&#39640;&#36798;2.76&#20493;&#30340;&#36895;&#24230;&#21152;&#24555;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#27861;&#21644;&#38646;&#20887;&#20313;&#20248;&#21270;&#22120;&#38598;&#25104;&#30340;&#24322;&#26500;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.14883</link><description>&lt;p&gt;
Colossal-AI: &#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#24182;&#34892;&#35757;&#32451;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14883
&lt;/p&gt;
&lt;p&gt;
Colossal-AI&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#24182;&#34892;&#35757;&#32451;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22815;&#20197;&#39640;&#36798;2.76&#20493;&#30340;&#36895;&#24230;&#21152;&#24555;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#27861;&#21644;&#38646;&#20887;&#20313;&#20248;&#21270;&#22120;&#38598;&#25104;&#30340;&#24322;&#26500;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#30340;&#25104;&#21151;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#36798;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21333;&#20010;GPU&#30340;&#26377;&#38480;&#20869;&#23384;&#36164;&#28304;&#65292;&#36873;&#25321;&#26368;&#20339;&#24182;&#34892;&#31574;&#30053;&#30340;&#26368;&#20339;&#23454;&#36341;&#20173;&#28982;&#32570;&#20047;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28145;&#24230;&#23398;&#20064;&#21644;&#24182;&#34892;&#35745;&#31639;&#26041;&#38754;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;Colossal-AI&#31995;&#32479;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32479;&#19968;&#30340;&#25509;&#21475;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#23558;&#27169;&#22411;&#35757;&#32451;&#30340;&#39034;&#24207;&#20195;&#30721;&#25193;&#23637;&#21040;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#12290;&#23427;&#25903;&#25345;&#25968;&#25454;&#24182;&#34892;&#12289;&#27969;&#27700;&#32447;&#24182;&#34892;&#12289;&#24352;&#37327;&#24182;&#34892;&#21644;&#24207;&#21015;&#24182;&#34892;&#31561;&#24182;&#34892;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#38646;&#20887;&#20313;&#20248;&#21270;&#22120;&#38598;&#25104;&#30340;&#24322;&#26500;&#35757;&#32451;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#31995;&#32479;&#30456;&#27604;&#65292;Colossal-AI&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#21487;&#20197;&#36798;&#21040;&#39640;&#36798;2.76&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Transformer models has pushed the deep learning model scale to billions of parameters. Due to the limited memory resource of a single GPU, However, the best practice for choosing the optimal parallel strategy is still lacking, since it requires domain expertise in both deep learning and parallel computing.  The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism, as well as heterogeneous training methods integrated with zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;SR-HetGNN&#65292;&#36890;&#36807;&#23398;&#20064;&#20250;&#35805;&#23884;&#20837;&#24182;&#25429;&#25417;&#21311;&#21517;&#29992;&#25143;&#30340;&#29305;&#23450;&#20559;&#22909;&#65292;&#20197;&#25913;&#36827;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2108.05641</link><description>&lt;p&gt;
SR-HetGNN:&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural Network. (arXiv:2108.05641v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.05641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;SR-HetGNN&#65292;&#36890;&#36807;&#23398;&#20064;&#20250;&#35805;&#23884;&#20837;&#24182;&#25429;&#25417;&#21311;&#21517;&#29992;&#25143;&#30340;&#29305;&#23450;&#20559;&#22909;&#65292;&#20197;&#25913;&#36827;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#30446;&#30340;&#26159;&#26681;&#25454;&#20808;&#21069;&#30340;&#20250;&#35805;&#24207;&#21015;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#27425;&#28857;&#20987;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#26681;&#25454;&#29992;&#25143;&#20250;&#35805;&#24207;&#21015;&#20013;&#30340;&#39033;&#30446;&#36716;&#25442;&#26469;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#20250;&#35805;&#24207;&#21015;&#20013;&#30340;&#20854;&#20182;&#26377;&#25928;&#20449;&#24687;&#65292;&#22914;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#26080;&#27861;&#23398;&#20064;&#29992;&#25143;&#30340;&#20855;&#20307;&#20559;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SR-HetGNN&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HetGNN&#65289;&#23398;&#20064;&#20250;&#35805;&#23884;&#20837;&#65292;&#24182;&#25429;&#25417;&#21311;&#21517;&#29992;&#25143;&#30340;&#29305;&#23450;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SR-HetGNN&#39318;&#20808;&#26681;&#25454;&#20250;&#35805;&#24207;&#21015;&#26500;&#24314;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#33410;&#28857;&#30340;&#24322;&#26500;&#22270;&#65292;&#21487;&#20197;&#25429;&#25417;&#39033;&#30446;&#12289;&#29992;&#25143;&#21644;&#20250;&#35805;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;HetGNN&#25429;&#25417;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#36716;&#25442;&#24182;&#23398;&#20064;&#21253;&#21547;&#39033;&#30446;&#23884;&#20837;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of the Session-Based Recommendation System is to predict the user's next click according to the previous session sequence. The current studies generally learn user preferences according to the transitions of items in the user's session sequence. However, other effective information in the session sequence, such as user profiles, are largely ignored which may lead to the model unable to learn the user's specific preferences. In this paper, we propose a heterogeneous graph neural network-based session recommendation method, named SR-HetGNN, which can learn session embeddings by heterogeneous graph neural network (HetGNN), and capture the specific preferences of anonymous users. Specifically, SR-HetGNN first constructs heterogeneous graphs containing various types of nodes according to the session sequence, which can capture the dependencies among items, users, and sessions. Second, HetGNN captures the complex transitions between items and learns the item embeddings containing
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#30697;&#38453;&#34920;&#31034;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#65292;&#36890;&#36807;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#27010;&#29575;&#24615;&#27010;&#24565;&#23450;&#20041;&#21644;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#23384;&#20648;&#19978;&#30028;&#65292;&#24182;&#19982;&#21521;&#37327;&#34920;&#31034;&#30340;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2104.07454</link><description>&lt;p&gt;
&#20855;&#26377;&#30697;&#38453;&#34920;&#31034;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Memory Capacity of Recurrent Neural Networks with Matrix Representation. (arXiv:2104.07454v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.07454
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#30697;&#38453;&#34920;&#31034;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#65292;&#36890;&#36807;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#27010;&#29575;&#24615;&#27010;&#24565;&#23450;&#20041;&#21644;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#23384;&#20648;&#19978;&#30028;&#65292;&#24182;&#19982;&#21521;&#37327;&#34920;&#31034;&#30340;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#32463;&#20856;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#22312;&#23398;&#20064;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#19968;&#38382;&#39064;&#22312;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#20013;&#36890;&#36807;&#23384;&#20648;&#32467;&#26500;&#24471;&#21040;&#20102;&#35299;&#20915;&#12290;&#31070;&#32463;&#22270;&#28789;&#26426;&#65288;NTM&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;RNN&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#23454;&#29616;&#20102;&#21487;&#32534;&#31243;&#35745;&#31639;&#26426;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#23398;&#20064;&#31616;&#21333;&#30340;&#31639;&#27861;&#20219;&#21153;&#12290;&#30697;&#38453;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#30697;&#38453;&#34920;&#31034;&#65292;&#19982;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#34920;&#31034;&#30340;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#21487;&#20197;&#22266;&#26377;&#22320;&#20445;&#30041;&#25968;&#25454;&#30340;&#31354;&#38388;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#35748;&#20026;&#20855;&#26377;&#30697;&#38453;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20855;&#26377;&#26356;&#22909;&#30340;&#23384;&#20648;&#23481;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;Fisher&#20449;&#24687;&#23450;&#20041;&#21644;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#30697;&#38453;RNN&#23384;&#20648;&#23481;&#37327;&#30340;&#27010;&#24565;&#12290;&#22312;&#21508;&#31181;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#30340;&#19978;&#30028;&#65292;&#24182;&#19982;&#23427;&#20204;&#30340;&#21521;&#37327;&#23545;&#24212;&#29289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;s
&lt;/p&gt;
&lt;p&gt;
It is well known that canonical recurrent neural networks (RNNs) face limitations in learning long-term dependencies which have been addressed by memory structures in long short-term memory (LSTM) networks. Neural Turing machines (NTMs) are novel RNNs that implement the notion of programmable computers with neural network controllers that can learn simple algorithmic tasks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data when compared to canonical neural networks that use vector-based representation. One may then argue that neural networks with matrix representations may have the potential to provide better memory capacity. In this paper, we define and study a probabilistic notion of memory capacity based on Fisher information for matrix-based RNNs. We find bounds on memory capacity for such networks under various hypotheses and compare them with their vector counterparts. In particular, we show that the memory capacity of s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#26102;&#31354;&#22825;&#27668;&#25968;&#25454;&#12290;&#35813;&#26550;&#26500;&#38598;&#25104;&#20102;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#21305;&#37197;&#26426;&#21046;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2102.00696</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#21305;&#37197;&#26426;&#21046;&#30340;&#21367;&#31215;LSTM&#36827;&#34892;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Numerical Weather Forecasting using Convolutional-LSTM with Attention and Context Matcher Mechanisms. (arXiv:2102.00696v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#26102;&#31354;&#22825;&#27668;&#25968;&#25454;&#12290;&#35813;&#26550;&#26500;&#38598;&#25104;&#20102;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#21305;&#37197;&#26426;&#21046;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#27169;&#22411;&#36827;&#34892;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#36890;&#24120;&#38656;&#35201;&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20943;&#23569;&#20102;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#20869;&#25581;&#31034;&#20986;&#21019;&#26032;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#26102;&#31354;&#22825;&#27668;&#25968;&#25454;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#38598;&#25104;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#25193;&#23637;&#20256;&#32479;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#21305;&#37197;&#26426;&#21046;&#34701;&#20837;&#21040;&#27169;&#22411;&#26550;&#26500;&#20013;&#12290;&#19982;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;ConvLSTM&#12289;TrajGRU&#21644;U-Net&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#22825;&#27668;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#28041;&#21450;&#39640;&#35268;&#27169;&#30340;&#23454;&#38469;&#22522;&#20934;&#25968;&#20540;&#22825;&#27668;&#25968;&#25454;&#38598;&#65292;&#21363;ERA5&#23567;&#26102;&#32423;&#27668;&#21387;&#27700;&#24179;&#25968;&#25454;&#38598;&#21644;WeatherBench&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Numerical weather forecasting using high-resolution physical models often requires extensive computational resources on supercomputers, which diminishes their wide usage in most real-life applications. As a remedy, applying deep learning methods has revealed innovative solutions within this field. To this end, we introduce a novel deep learning architecture for forecasting high-resolution spatio-temporal weather data. Our approach extends the conventional encoder-decoder structure by integrating Convolutional Long-short Term Memory and Convolutional Neural Networks. In addition, we incorporate attention and context matcher mechanisms into the model architecture. Our Weather Model achieves significant performance improvements compared to baseline deep learning models, including ConvLSTM, TrajGRU, and U-Net. Our experimental evaluation involves high-scale, real-world benchmark numerical weather datasets, namely the ERA5 hourly dataset on pressure levels and WeatherBench. Our results demo
&lt;/p&gt;</description></item></channel></rss>