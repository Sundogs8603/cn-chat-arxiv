<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#31227;&#21160;&#30340;&#20154;&#29289;&#21160;&#30011;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#20154;&#20307;&#19981;&#21487;&#35265;&#37096;&#20998;&#30340;&#21512;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10889</link><description>&lt;p&gt;
&#20351;&#29992;3D&#25511;&#21046;&#21512;&#25104;&#31227;&#21160;&#20154;&#29289;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Moving People with 3D Control. (arXiv:2401.10889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#31227;&#21160;&#30340;&#20154;&#29289;&#21160;&#30011;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#20154;&#20307;&#19981;&#21487;&#35265;&#37096;&#20998;&#30340;&#21512;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#20026;&#32473;&#23450;&#30340;&#30446;&#26631;3D&#36816;&#21160;&#24207;&#21015;&#29983;&#25104;&#20154;&#29289;&#21160;&#30011;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65306;a) &#23398;&#20064;&#20851;&#20110;&#20154;&#20307;&#21644;&#26381;&#35013;&#19981;&#21487;&#35265;&#37096;&#20998;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;b) &#20197;&#36866;&#24403;&#30340;&#26381;&#35013;&#21644;&#32441;&#29702;&#28210;&#26579;&#26032;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#23545;&#20110;&#31532;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#22635;&#20805;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#32473;&#23450;&#21333;&#24352;&#22270;&#20687;&#29983;&#25104;&#20154;&#29289;&#30340;&#19981;&#21487;&#35265;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#32441;&#29702;&#26144;&#23556;&#31354;&#38388;&#19978;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#20351;&#20854;&#23545;&#23039;&#21183;&#21644;&#35270;&#35282;&#19981;&#21464;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#28210;&#26579;&#27969;&#27700;&#32447;&#65292;&#30001;3D&#20154;&#20307;&#23039;&#21183;&#25511;&#21046;&#12290;&#36825;&#21487;&#20197;&#20135;&#29983;&#36924;&#30495;&#30340;&#20154;&#29289;&#26032;&#23039;&#21183;&#30340;&#28210;&#26579;&#22270;&#20687;&#65292;&#21253;&#25324;&#26381;&#35013;&#12289;&#22836;&#21457;&#21644;&#26410;&#30693;&#21306;&#22495;&#30340;&#21512;&#29702;&#34917;&#20805;&#12290;&#36825;&#31181;&#20998;&#35299;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#19968;&#31995;&#21015;&#22270;&#20687;&#65292;&#26082;&#31526;&#21512;3D&#23039;&#21183;&#20013;&#30340;&#30446;&#26631;&#36816;&#21160;&#65292;&#20063;&#31526;&#21512;&#35270;&#35273;&#19978;&#19982;&#36755;&#20837;&#22270;&#20687;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to tha
&lt;/p&gt;</description></item><item><title>SCENES&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26497;&#32447;&#30417;&#30563;&#30340;&#20122;&#20687;&#32032;&#23545;&#24212;&#20851;&#31995;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#32780;&#19981;&#38656;&#35201;3D&#32467;&#26500;&#65292;&#20197;&#25918;&#23485;&#23545;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.10886</link><description>&lt;p&gt;
SCENES: &#20351;&#29992;&#26497;&#32447;&#30417;&#30563;&#30340;&#20122;&#20687;&#32032;&#23545;&#24212;&#20851;&#31995;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SCENES: Subpixel Correspondence Estimation With Epipolar Supervision. (arXiv:2401.10886v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10886
&lt;/p&gt;
&lt;p&gt;
SCENES&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26497;&#32447;&#30417;&#30563;&#30340;&#20122;&#20687;&#32032;&#23545;&#24212;&#20851;&#31995;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#32780;&#19981;&#38656;&#35201;3D&#32467;&#26500;&#65292;&#20197;&#25918;&#23485;&#23545;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22330;&#26223;&#30340;&#20004;&#20010;&#25110;&#26356;&#22810;&#35270;&#35282;&#20013;&#25552;&#21462;&#28857;&#23545;&#24212;&#20851;&#31995;&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#29305;&#21035;&#37325;&#35201;&#30340;&#26159;&#30456;&#23545;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#21644;&#32467;&#26500;&#36816;&#21160;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23545;&#24212;&#20851;&#31995;&#30417;&#30563;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#39640;&#24230;&#20934;&#30830;&#30340;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#29305;&#24449;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24448;&#24448;&#26080;&#27861;&#24456;&#22909;&#22320;&#27867;&#21270;&#65292;&#36825;&#19982;&#32463;&#20856;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#19981;&#21516;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#38656;&#35201;&#24494;&#35843;&#65292;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#22320;&#38754;&#23454;&#20917;&#23545;&#24212;&#20851;&#31995;&#25110;&#22320;&#38754;&#23454;&#20917;&#30456;&#26426;&#23039;&#24577;&#21644;3D&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#21462;&#28040;&#23545;3D&#32467;&#26500;&#65288;&#22914;&#28145;&#24230;&#22270;&#25110;&#28857;&#20113;&#65289;&#30340;&#35201;&#27714;&#65292;&#21482;&#38656;&#35201;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#65288;&#21487;&#20197;&#20174;&#37324;&#31243;&#35745;&#33719;&#24471;&#65289;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23545;&#24212;&#20851;&#31995;&#25439;&#22833;&#26367;&#25442;&#20026;&#26497;&#32447;&#25439;&#22833;&#26469;&#23454;&#29616;&#65292;&#26497;&#32447;&#25439;&#22833;&#40723;&#21169;&#20551;&#35774;&#21305;&#37197;&#28857;&#20301;&#20110;&#30456;&#20851;&#30340;&#26497;&#32447;&#19978;&#12290;&#34429;&#28982;&#23427;&#30456;&#23545;&#26469;&#35828;&#27604;&#23545;&#24212;&#20851;&#31995;&#36739;&#24369;&#65292;&#20294;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#20197;&#22312;&#26080;&#38656;3D&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20986;&#30456;&#26426;&#30340;&#30456;&#23545;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than corresponde
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;Stack Overflow&#35780;&#20998;&#65292;&#25552;&#39640;&#20102;GPT Neo&#22312;&#32534;&#31243;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20256;&#32479;&#35821;&#35328;&#24230;&#37327;&#26041;&#27861;&#22312;&#32534;&#31243;&#39046;&#22495;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10882</link><description>&lt;p&gt;
&#20351;&#29992;&#20844;&#20849;&#31038;&#21306;&#35780;&#20998;&#20316;&#20026;&#20154;&#31867;&#21453;&#39304;&#65292;&#22312;&#32534;&#31243;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38382;&#31572;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning for question answering in programming domain using public community scoring as a human feedback. (arXiv:2401.10882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;Stack Overflow&#35780;&#20998;&#65292;&#25552;&#39640;&#20102;GPT Neo&#22312;&#32534;&#31243;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20256;&#32479;&#35821;&#35328;&#24230;&#37327;&#26041;&#27861;&#22312;&#32534;&#31243;&#39046;&#22495;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#26469;&#33258;Stack Overflow&#30340;&#35780;&#20998;&#65292;&#25506;&#35752;&#20102;&#22312;&#32534;&#31243;&#39046;&#22495;&#20013;&#25552;&#39640;GPT Neo 125M&#22312;&#31038;&#21306;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;Proximal Policy Optimization (PPO)&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#25913;&#36827;&#26041;&#38754;&#19982;GPT Neo 2.7B&#21442;&#25968;&#21464;&#20307;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#36741;&#21161;&#35780;&#20998;&#26426;&#21046;&#65292;&#26174;&#31034;&#20102;&#20256;&#32479;&#35821;&#35328;&#24230;&#37327;&#22312;&#32534;&#31243;&#39046;&#22495;&#20013;&#35780;&#20272;&#21709;&#24212;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#20934;&#30830;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#24212;&#29992;&#20110;&#32534;&#31243;&#31038;&#21306;&#38382;&#31572;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30340;&#35780;&#20272;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate the enhancement of the GPT Neo 125M performance in Community Question Answering (CQA) with a focus on programming, through the integration of Reinforcement Learning from Human Feedback (RLHF) and the utilization of scores from Stack Overflow. Two distinct reward model training strategies are employed for fine-tuning with Proximal Policy Optimization (PPO). Notably, the improvements in performance achieved through this method are comparable to those of GPT Neo 2.7B parameter variant. Additionally, an auxiliary scoring mechanism is introduced, which demonstrates the limitations of conventional linguistic metrics in evaluating responses in the programming domain. Through accurate analysis, this paper looks at the divergence between traditional linguistic metrics and our human-preferences-based reward model, underscoring the imperative for domain-specific evaluation methods. By elucidating the complexities involved in applying RLHF to programming CQA and accen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10862</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#20445;&#25252;: &#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#23545;&#40784;&#30340;LLMs&#30340;&#36234;&#29425;&#25269;&#25239;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#35825;&#20351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#21644;&#36829;&#27861;&#20869;&#23481;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21098;&#26525;LLM&#21442;&#25968;&#22810;&#36798;20&#65285;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#23427;&#20204;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#24182;&#19988;&#19981;&#25439;&#23475;&#20854;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21098;&#26525;&#21518;&#35266;&#23519;&#21040;&#30340;&#22686;&#24378;&#23433;&#20840;&#24615;&#19982;&#27169;&#22411;&#30340;&#21021;&#22987;&#23433;&#20840;&#35757;&#32451;&#27700;&#24179;&#30456;&#20851;&#65292;&#36825;&#26263;&#31034;&#21098;&#26525;&#30340;&#25928;&#26524;&#21487;&#33021;&#26356;&#26222;&#36941;&#65292;&#20063;&#21487;&#33021;&#36866;&#29992;&#20110;&#36229;&#20986;&#23433;&#20840;&#24615;&#33539;&#30068;&#30340;&#20854;&#20182;LLM&#34892;&#20026;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#12289;&#25554;&#20837;&#21040;&#21313;&#20010;&#19981;&#21516;&#36234;&#29425;&#25552;&#31034;&#20013;&#30340;225&#20010;&#26377;&#23475;&#20219;&#21153;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;LLMs&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#36234;&#29425;&#25552;&#31034;&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;LLaMA-2 Chat&#65292;Vicuna&#21644;Mistral Instruct&#65289;&#20855;&#26377;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;eHealth&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#36827;&#23637;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#21644;&#35299;&#37322;&#21307;&#30103;&#39046;&#22495;&#30340;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#30103;&#26381;&#21153;&#21644;&#25972;&#20010;&#21307;&#30103;&#39046;&#22495;&#30340;&#25928;&#29575;&#21644;&#30693;&#35782;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.10850</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;eHealth&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advancements in eHealth Data Analytics through Natural Language Processing and Deep Learning. (arXiv:2401.10850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10850
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;eHealth&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#36827;&#23637;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#21644;&#35299;&#37322;&#21307;&#30103;&#39046;&#22495;&#30340;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#30103;&#26381;&#21153;&#21644;&#25972;&#20010;&#21307;&#30103;&#39046;&#22495;&#30340;&#25928;&#29575;&#21644;&#30693;&#35782;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#29615;&#22659;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#20449;&#24687;&#20016;&#23500;&#8221;&#20294;&#20063;&#8220;&#30693;&#35782;&#21294;&#20047;&#8221;&#12290;&#21307;&#30103;&#31995;&#32479;&#20174;&#21508;&#31181;&#26469;&#28304;&#25910;&#38598;&#22823;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#23454;&#39564;&#23460;&#25253;&#21578;&#12289;&#21307;&#30103;&#20449;&#20989;&#12289;&#21307;&#30103;&#24037;&#20855;&#25110;&#31243;&#24207;&#30340;&#26085;&#24535;&#12289;&#21307;&#30103;&#22788;&#26041;&#31561;&#12290;&#36825;&#20123;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#21307;&#30103;&#26381;&#21153;&#21644;&#25972;&#20010;&#21307;&#30103;&#39046;&#22495;&#30340;&#23453;&#36149;&#30693;&#35782;&#21644;&#20449;&#24687;&#65292;&#20363;&#22914;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30151;&#29366;&#36827;&#34892;&#30142;&#30149;&#39044;&#27979;&#25110;&#36890;&#36807;&#21457;&#29616;&#30142;&#30149;&#30340;&#34892;&#20026;&#22240;&#32032;&#36827;&#34892;&#30142;&#30149;&#39044;&#38450;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21482;&#26377;&#30456;&#23545;&#36739;&#23569;&#30340;&#25991;&#26412;eHealth&#25968;&#25454;&#34987;&#22788;&#29702;&#21644;&#35299;&#37322;&#65292;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#26159;&#22312;&#25191;&#34892;&#22823;&#25968;&#25454;&#25805;&#20316;&#26102;&#30340;&#22256;&#38590;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#26816;&#27979;&#29305;&#23450;&#39046;&#22495;&#30340;&#22810;&#35789;&#26415;&#35821;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29992;&#20960;&#20010;&#35789;&#26469;&#23450;&#20041;&#19968;&#20010;&#25972;&#20010;&#27010;&#24565;&#12290;&#26415;&#35821;&#21487;&#20197;&#23450;&#20041;&#20026;&#19968;&#20010;&#35821;&#35328;&#32467;&#26500;&#25110;&#27010;&#24565;&#65292;&#23427;&#30001;&#19968;&#20010;&#25110;&#22810;&#20010;&#35789;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The healthcare environment is commonly referred to as "information-rich" but also "knowledge poor". Healthcare systems collect huge amounts of data from various sources: lab reports, medical letters, logs of medical tools or programs, medical prescriptions, etc. These massive sets of data can provide great knowledge and information that can improve the medical services, and overall the healthcare domain, such as disease prediction by analyzing the patient's symptoms or disease prevention, by facilitating the discovery of behavioral factors for diseases. Unfortunately, only a relatively small volume of the textual eHealth data is processed and interpreted, an important factor being the difficulty in efficiently performing Big Data operations. In the medical field, detecting domain-specific multi-word terms is a crucial task as they can define an entire concept with a few words. A term can be defined as a linguistic structure or a concept, and it is composed of one or more words with a s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#21644;&#20165;&#22270;&#20687;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31867;&#21035;&#32423;&#30446;&#26631;&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#29305;&#23450;&#23376;&#37096;&#20998;&#22312;&#36328;&#22495;&#24773;&#26223;&#20013;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#27169;&#22411;&#12290;&#20351;&#29992;&#31616;&#21333;&#30340;&#31435;&#26041;&#20307;&#32593;&#26684;&#21644;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#28608;&#27963;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#27809;&#26377;3D&#25110;&#28145;&#24230;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#20805;&#28385;&#24178;&#25200;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.10848</link><description>&lt;p&gt;
&#26080;&#28304;&#21644;&#20165;&#22270;&#20687;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#31867;&#21035;&#32423;&#30446;&#26631;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation. (arXiv:2401.10848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#21644;&#20165;&#22270;&#20687;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31867;&#21035;&#32423;&#30446;&#26631;&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#29305;&#23450;&#23376;&#37096;&#20998;&#22312;&#36328;&#22495;&#24773;&#26223;&#20013;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#27169;&#22411;&#12290;&#20351;&#29992;&#31616;&#21333;&#30340;&#31435;&#26041;&#20307;&#32593;&#26684;&#21644;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#28608;&#27963;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#27809;&#26377;3D&#25110;&#28145;&#24230;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#20805;&#28385;&#24178;&#25200;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#21040;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#26080;&#28304;&#26080;&#30417;&#30563;&#31867;&#21035;&#32423;&#21035;&#23039;&#24577;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#28304;&#39046;&#22495;&#25968;&#25454;&#25110;3D&#27880;&#37322;&#12290;&#25910;&#38598;&#21644;&#27880;&#37322;&#29616;&#23454;&#19990;&#30028;&#30340;3D&#25968;&#25454;&#21644;&#30456;&#24212;&#30340;&#22270;&#20687;&#26159;&#19968;&#39033;&#36153;&#26102;&#36153;&#21147;&#30340;&#36807;&#31243;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#65292;&#22240;&#20026;&#21363;&#20351;&#26159;3D&#23039;&#24577;&#22495;&#36866;&#24212;&#26041;&#27861;&#20063;&#38656;&#35201;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;3D&#25968;&#25454;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;3DUDA&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#20110;&#20805;&#28385;&#24178;&#25200;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;3D&#25110;&#28145;&#24230;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#28304;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65306;&#29305;&#23450;&#30340;&#29289;&#20307;&#23376;&#37096;&#20998;&#22312;&#22495;&#22806;&#24773;&#26223;&#20013;&#20445;&#25345;&#31283;&#23450;&#65292;&#20174;&#32780;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#19981;&#21464;&#30340;&#23376;&#32452;&#20214;&#36827;&#34892;&#27169;&#22411;&#26356;&#26032;&#12290;&#25105;&#20204;&#23558;&#29289;&#20307;&#31867;&#21035;&#34920;&#31034;&#20026;&#31616;&#21333;&#30340;&#31435;&#26041;&#20307;&#32593;&#26684;&#65292;&#24182;&#21033;&#29992;&#22312;&#27599;&#20010;&#32593;&#26684;&#39030;&#28857;&#19978;&#23398;&#20064;&#30340;&#31070;&#32463;&#29305;&#24449;&#28608;&#27963;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20010;&#20307;&#23616;&#37096;&#31283;&#20581;&#30340;&#32593;&#26684;&#39030;&#28857;&#29305;&#24449;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36845;&#20195;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of source-free unsupervised category-level pose estimation from only RGB images to a target domain without any access to source domain data or 3D annotations during adaptation. Collecting and annotating real-world 3D data and corresponding images is laborious, expensive, yet unavoidable process, since even 3D pose domain adaptation methods require 3D data in the target domain. We introduce 3DUDA, a method capable of adapting to a nuisance-ridden target domain without 3D or depth data. Our key insight stems from the observation that specific object subparts remain stable across out-of-domain (OOD) scenarios, enabling strategic utilization of these invariant subcomponents for effective model updates. We represent object categories as simple cuboid meshes, and harness a generative model of neural feature activations modeled at each mesh vertex learnt using differential rendering. We focus on individual locally robust mesh vertex features and iteratively update them
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.10841</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#21457;&#29616;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#32534;&#30721;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20167;&#24680;&#35328;&#35770;&#30340;&#34067;&#24310;&#32473;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24102;&#26469;&#20102;&#19968;&#20010;&#38590;&#39064;&#12290;&#19968;&#20010;&#29305;&#27530;&#30340;&#25361;&#25112;&#19982;&#20351;&#29992;&#32534;&#30721;&#35821;&#35328;&#30340;&#32676;&#20307;&#26377;&#20851;&#65292;&#36825;&#20123;&#32676;&#20307;&#26082;&#24819;&#20026;&#20854;&#29992;&#25143;&#21019;&#36896;&#24402;&#23646;&#24863;&#65292;&#21448;&#24819;&#22238;&#36991;&#26816;&#27979;&#12290;&#32534;&#30721;&#35821;&#35328;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20351;&#29992;&#26041;&#24335;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26041;&#27861;&#22312;&#22312;&#32447;&#21453;&#29369;&#22826;&#35328;&#35770;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#25235;&#21462;&#30340;&#24086;&#23376;&#65292;&#36890;&#24120;&#26159;&#26497;&#31471;&#20027;&#20041;&#29992;&#25143;&#20351;&#29992;&#30340;&#12290;&#24086;&#23376;&#26159;&#20351;&#29992;&#19982;&#20197;&#21069;&#24050;&#30693;&#30340;&#38024;&#23545;&#29369;&#22826;&#20154;&#30340;&#20167;&#24680;&#35328;&#35770;&#30456;&#20851;&#30340;&#31181;&#23376;&#34920;&#36798;&#24335;&#36827;&#34892;&#25235;&#21462;&#30340;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#24086;&#23376;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#22312;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#30340;&#39057;&#29575;&#12290;&#36807;&#28388;&#25481;&#35821;&#27861;&#19981;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#21644;&#20043;&#21069;&#36935;&#21040;&#36807;&#30340;&#34920;&#36798;&#24335;&#65292;&#20197;&#20415;&#20851;&#27880;&#26032;&#20986;&#29616;&#30340;&#33391;&#22909;&#24418;&#24335;&#30340;&#26415;&#35821;&#12290;&#28982;&#21518;&#36827;&#34892;&#20102;&#35821;&#20041;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#35748;&#30693;&#35786;&#26029;&#65288;SCD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31526;&#21495;&#26641;&#26126;&#30830;&#34920;&#31034;&#23398;&#29983;&#19982;&#32451;&#20064;&#20114;&#21160;&#65292;&#20197;&#21450;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#23398;&#20064;&#21442;&#25968;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10840</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#23454;&#29616;&#31526;&#21495;&#35748;&#30693;&#35786;&#26029;&#30340;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Symbolic Cognitive Diagnosis via Hybrid Optimization for Intelligent Education Systems. (arXiv:2401.10840v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#35748;&#30693;&#35786;&#26029;&#65288;SCD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31526;&#21495;&#26641;&#26126;&#30830;&#34920;&#31034;&#23398;&#29983;&#19982;&#32451;&#20064;&#20114;&#21160;&#65292;&#20197;&#21450;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#23398;&#20064;&#21442;&#25968;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#35780;&#20272;&#26159;&#23398;&#29983;&#23398;&#20064;&#30340;&#22522;&#26412;&#19988;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#23427;&#24314;&#27169;&#20102;&#23398;&#29983;&#19982;&#32451;&#20064;&#30340;&#20114;&#21160;&#65292;&#24182;&#21457;&#29616;&#20102;&#23398;&#29983;&#23545;&#27599;&#20010;&#30693;&#35782;&#23646;&#24615;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#22312;&#23454;&#38469;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#65292;&#35748;&#30693;&#35786;&#26029;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#21516;&#26679;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#23398;&#29983;&#19982;&#32451;&#20064;&#20114;&#21160;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#20108;&#32773;&#20860;&#39038;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#35748;&#30693;&#35786;&#26029;&#65288;SCD&#65289;&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;SCD&#26694;&#26550;&#21033;&#29992;&#31526;&#21495;&#26641;&#26126;&#30830;&#22320;&#34920;&#31034;&#22797;&#26434;&#30340;&#23398;&#29983;&#19982;&#32451;&#20064;&#20114;&#21160;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#23398;&#20064;&#23398;&#29983;&#21644;&#32451;&#20064;&#21442;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25361;&#25112;&#22312;&#20110;&#25105;&#20204;&#38656;&#35201;&#20256;&#36882;&#31163;&#25955;&#30340;&#31526;&#21495;&#34920;&#31034;&#21644;&#36830;&#32493;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive diagnosis assessment is a fundamental and crucial task for student learning. It models the student-exercise interaction, and discovers the students' proficiency levels on each knowledge attribute. In real-world intelligent education systems, generalization and interpretability of cognitive diagnosis methods are of equal importance. However, most existing methods can hardly make the best of both worlds due to the complicated student-exercise interaction. To this end, this paper proposes a symbolic cognitive diagnosis~(SCD) framework to simultaneously enhance generalization and interpretability. The SCD framework incorporates the symbolic tree to explicably represent the complicated student-exercise interaction function, and utilizes gradient-based optimization methods to effectively learn the student and exercise parameters. Meanwhile, the accompanying challenge is that we need to tunnel the discrete symbolic representation and continuous parameter optimization. To address thi
&lt;/p&gt;</description></item><item><title>Holonic Learning (HoL) is a flexible and privacy-focused learning framework designed for training deep learning models. It leverages holonic concepts to establish a structured self-similar hierarchy, allowing more nuanced control over collaborations. HoL has extensive design and flexibility potentials.</title><link>http://arxiv.org/abs/2401.10839</link><description>&lt;p&gt;
&#24377;&#24615;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65306;Holonic Learning
&lt;/p&gt;
&lt;p&gt;
Holonic Learning: A Flexible Agent-based Distributed Machine Learning Framework. (arXiv:2401.10839v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10839
&lt;/p&gt;
&lt;p&gt;
Holonic Learning (HoL) is a flexible and privacy-focused learning framework designed for training deep learning models. It leverages holonic concepts to establish a structured self-similar hierarchy, allowing more nuanced control over collaborations. HoL has extensive design and flexibility potentials.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#26222;&#21450;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#25512;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#21521;&#26356;&#20998;&#24067;&#24335;&#30340;&#26041;&#27861;&#36716;&#21464;&#12290;&#36825;&#31181;&#36716;&#21464;&#19981;&#20165;&#26088;&#22312;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#21644;&#36164;&#28304;&#20998;&#24067;&#25361;&#25112;&#65292;&#36824;&#35201;&#35299;&#20915;&#32039;&#36843;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#23545;&#36825;&#19968;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#36129;&#29486;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Holonic Learning (HoL)&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#30340;&#21327;&#20316;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;Holonic&#27010;&#24565;&#65292;HoL&#26694;&#26550;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#33258;&#30456;&#20284;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#27599;&#20010;holon&#30340;&#20010;&#20307;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#20869;&#37096;holon&#25215;&#35834;&#21644;&#36890;&#20449;&#27169;&#24335;&#65292;&#23454;&#29616;&#23545;&#21327;&#20316;&#30340;&#26356;&#32454;&#33268;&#25511;&#21046;&#12290;Holonic Learning&#22312;&#20854;&#36890;&#29992;&#24418;&#24335;&#20013;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#35774;&#35745;&#21644;&#28789;&#27963;&#24615;&#28508;&#21147;&#12290;&#20026;&#20102;&#32463;&#39564;&#20998;&#26512;&#21644;&#23637;&#31034;&#20854;&#25928;&#26524;&#65292;&#26412;&#25991;&#23454;&#29616;&#20102;Holo
&lt;/p&gt;
&lt;p&gt;
Ever-increasing ubiquity of data and computational resources in the last decade have propelled a notable transition in the machine learning paradigm towards more distributed approaches. Such a transition seeks to not only tackle the scalability and resource distribution challenges but also to address pressing privacy and security concerns. To contribute to the ongoing discourse, this paper introduces Holonic Learning (HoL), a collaborative and privacy-focused learning framework designed for training deep learning models. By leveraging holonic concepts, the HoL framework establishes a structured self-similar hierarchy in the learning process, enabling more nuanced control over collaborations through the individual model aggregation approach of each holon, along with their intra-holon commitment and communication patterns. HoL, in its general form, provides extensive design and flexibility potentials. For empirical analysis and to demonstrate its effectiveness, this paper implements Holo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10831</link><description>&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#27010;&#24565;&#21457;&#29616;&#29702;&#35299;&#35270;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35797;&#22270;&#35299;&#37322;&#22522;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#39640;&#23618;&#26102;&#31354;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20197;&#24448;&#20851;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#20165;&#38598;&#20013;&#22312;&#22270;&#20687;&#32423;&#20219;&#21153;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35270;&#39057;&#27169;&#22411;&#22788;&#29702;&#20102;&#39069;&#22806;&#30340;&#26102;&#38388;&#32500;&#24230;&#65292;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#21160;&#24577;&#27010;&#24565;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;(VTCD)&#31639;&#27861;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21333;&#20803;&#65288;&#27010;&#24565;&#65289;&#24182;&#23545;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#21517;&#12290;&#24471;&#21040;&#30340;&#27010;&#24565;&#20855;&#26377;&#24456;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video m
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#30340;&#20248;&#21270;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#31946;&#25512;&#29702;&#21644;&#27010;&#29575;&#25512;&#29702;&#22312;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24471;&#21040;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#22914;&#19982;&#20044;&#40486;&#24726;&#35770;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.10819</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimisation in Neurosymbolic Learning Systems. (arXiv:2401.10819v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10819
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#30340;&#20248;&#21270;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#31946;&#25512;&#29702;&#21644;&#27010;&#29575;&#25512;&#29702;&#22312;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24471;&#21040;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#22914;&#19982;&#20044;&#40486;&#24726;&#35770;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#38598;&#25104;&#26377;&#35768;&#22810;&#20248;&#21183;&#65292;&#20363;&#22914;&#20943;&#23569;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65292;&#25552;&#39640;&#27169;&#22411;&#31572;&#26696;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#20197;&#21450;&#39564;&#35777;&#35757;&#32451;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#21516;&#26102;&#20351;&#29992;&#31526;&#21495;&#35821;&#35328;&#34920;&#31034;&#25968;&#25454;&#21644;&#32972;&#26223;&#30693;&#35782;&#12290;&#25105;&#20204;&#22914;&#20309;&#36830;&#25509;&#31526;&#21495;&#21644;&#31070;&#32463;&#32452;&#20214;&#20197;&#20256;&#36798;&#36825;&#20123;&#30693;&#35782;&#21602;&#65311;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#27169;&#31946;&#25512;&#29702;&#65292;&#23427;&#30740;&#31350;&#30340;&#26159;&#30495;&#23454;&#31243;&#24230;&#12290;&#20363;&#22914;&#65292;&#36523;&#39640;&#24182;&#19981;&#26159;&#20108;&#20803;&#27010;&#24565;&#12290;&#30456;&#21453;&#65292;&#27010;&#29575;&#25512;&#29702;&#30740;&#31350;&#30340;&#26159;&#26576;&#20214;&#20107;&#24773;&#25104;&#20026;&#30495;&#23454;&#25110;&#23558;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#30740;&#31350;&#38382;&#39064;&#25506;&#35752;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#27169;&#31946;&#25512;&#29702;&#19982;&#23398;&#20064;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#19982;&#20044;&#40486;&#24726;&#35770;&#30340;&#32852;&#31995;&#65292;&#21363;&#24403;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#32511;&#33394;&#30340;&#33529;&#26524;&#26102;&#65292;&#25105;&#20204;&#30830;&#35748;&#8220;&#20044;&#40486;&#26159;&#40657;&#33394;&#30340;&#8221;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27809;&#26377;&#20351;&#29992;&#32972;&#26223;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI aims to integrate deep learning with symbolic AI. This integration has many promises, such as decreasing the amount of data required to train a neural network, improving the explainability and interpretability of answers given by models and verifying the correctness of trained systems. We study neurosymbolic learning, where we have both data and background knowledge expressed using symbolic languages. How do we connect the symbolic and neural components to communicate this knowledge? One option is fuzzy reasoning, which studies degrees of truth. For example, being tall is not a binary concept. Instead, probabilistic reasoning studies the probability that something is true or will happen. Our first research question studies how different forms of fuzzy reasoning combine with learning. We find surprising results like a connection to the Raven paradox stating we confirm "ravens are black" when we observe a green apple. In this study, we did not use the background knowledg
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#65292;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#65292;&#33021;&#22815;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#27700;&#24179;&#21644;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#26102;&#38271;&#12290;</title><link>http://arxiv.org/abs/2401.10816</link><description>&lt;p&gt;
Co-Pilot for Health: &#20010;&#24615;&#21270;&#31639;&#27861;AI&#24341;&#23548;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes. (arXiv:2401.10816v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#65292;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#65292;&#33021;&#22815;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#27700;&#24179;&#21644;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#26102;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#33258;&#21160;&#22609;&#36896;&#22823;&#22411;&#20154;&#32676;&#30340;&#20581;&#24247;&#34892;&#20026;&#65292;&#36328;&#21487;&#31359;&#25140;&#35774;&#22791;&#31867;&#22411;&#21644;&#30142;&#30149;&#29366;&#20917;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#26469;&#25913;&#21892;&#20840;&#29699;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#20581;&#36523;&#35774;&#22791;&#30340;&#31934;&#32454;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#29992;&#20110;&#25968;&#23383;&#31639;&#27861;&#24341;&#23548;&#12290;&#22312;&#27492;&#25105;&#20204;&#25551;&#36848;&#20102;&#35813;&#24179;&#21488;&#22312;&#26032;&#21152;&#22369;&#38024;&#23545;$n=84,764$&#20010;&#20010;&#20307;&#30340;12&#21608;&#26399;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#30340;&#26377;&#25928;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#32479;&#35745;&#39564;&#35777;&#20102;&#30446;&#26631;&#32452;&#20013;&#25509;&#21463;&#27492;&#31867;AI&#20248;&#21270;&#26085;&#24120;&#24341;&#23548;&#30340;&#21442;&#19982;&#32773;&#30456;&#36739;&#20110;&#25511;&#21046;&#32452;&#20013;&#26410;&#25509;&#21463;&#20219;&#20309;&#24341;&#23548;&#30340;&#21305;&#37197;&#21442;&#19982;&#32773;&#65292;&#20854;&#27599;&#22825;&#30340;&#27493;&#25968;&#22686;&#21152;&#20102;6.17%&#65288;$p = 3.09\times10^{-4}$&#65289;&#65292;&#27599;&#21608;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#65288;MVPA&#65289;&#20998;&#38047;&#22686;&#21152;&#20102;7.61%&#65288;$p = 1.16\times10^{-2}$&#65289;&#12290;&#27492;&#22806;&#65292;&#27492;&#31867;&#24341;&#23548;&#38750;&#24120;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to shape health behaviors of large populations automatically, across wearable types and disease conditions at scale has tremendous potential to improve global health outcomes. We designed and implemented an AI driven platform for digital algorithmic nudging, enabled by a Graph-Neural Network (GNN) based Recommendation System, and granular health behavior data from wearable fitness devices. Here we describe the efficacy results of this platform with its capabilities of personalized and contextual nudging to $n=84,764$ individuals over a 12-week period in Singapore. We statistically validated that participants in the target group who received such AI optimized daily nudges increased daily physical activity like step count by 6.17% ($p = 3.09\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical Activity (MVPA) by 7.61% ($p = 1.16\times10^{-2}$), compared to matched participants in control group who did not receive any nudges. Further, such nudges were very well r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.10805</link><description>&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#65288;CATE&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;CATE&#21487;&#20197;&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#22522;&#20110;CATE&#30340;&#20219;&#21153;&#24418;&#24335;&#65292;&#22914;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#65292;&#20854;&#20013;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#20197;&#35821;&#20041;&#21644;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#36830;&#25509;&#21160;&#20316;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#29992;&#20110;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#12290;&#23613;&#31649;&#20219;&#21153;&#20855;&#26377;&#30452;&#35266;&#24615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#22256;&#38590;&#37325;&#37325;&#65292;&#20154;&#31867;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#36830;&#25509;&#35270;&#39057;&#29702;&#35299;&#20013;&#21160;&#20316;&#21644;&#25928;&#26524;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24230;&#37327;&#21160;&#24577;&#24179;&#34913;&#36923;&#36753;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;ASP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23450;&#23450;&#24615;&#21644;&#23450;&#37327;&#30340;&#21160;&#24577;&#32422;&#26463;&#12290;&#23427;&#26159;Equilibrium Logic&#30340;&#26368;&#36890;&#29992;&#30340;&#26102;&#38388;&#25193;&#23637;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2401.10781</link><description>&lt;p&gt;
&#24230;&#37327;&#21160;&#24577;&#24179;&#34913;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Metric Dynamic Equilibrium Logic. (arXiv:2401.10781v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10781
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24230;&#37327;&#21160;&#24577;&#24179;&#34913;&#36923;&#36753;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;ASP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23450;&#23450;&#24615;&#21644;&#23450;&#37327;&#30340;&#21160;&#24577;&#32422;&#26463;&#12290;&#23427;&#26159;Equilibrium Logic&#30340;&#26368;&#36890;&#29992;&#30340;&#26102;&#38388;&#25193;&#23637;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#32447;&#24615;&#26102;&#38388;&#30340;Answer Set Programming&#65288;ASP&#65289;&#30340;&#26102;&#38388;&#25193;&#23637;&#20013;&#65292;&#21160;&#24577;&#31995;&#32479;&#30340;&#34892;&#20026;&#36890;&#36807;&#29366;&#24577;&#24207;&#21015;&#26469;&#25429;&#25417;&#12290;&#34429;&#28982;&#36825;&#31181;&#34920;&#31034;&#21453;&#26144;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#39034;&#24207;&#65292;&#20294;&#26159;&#23427;&#30465;&#30053;&#20102;&#19982;&#27599;&#20010;&#29366;&#24577;&#30456;&#20851;&#32852;&#30340;&#20855;&#20307;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#26102;&#38388;&#32422;&#26463;&#24456;&#37325;&#35201;&#65292;&#20363;&#22914;&#65292;&#22312;&#35268;&#21010;&#21644;&#35843;&#24230;&#21516;&#26102;&#36827;&#34892;&#26102;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#24230;&#37327;&#30340;&#32447;&#24615;&#26102;&#38388;&#21160;&#24577;&#24179;&#34913;&#36923;&#36753;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21160;&#24577;&#25805;&#20316;&#31526;&#21463;&#21040;&#25972;&#25968;&#21306;&#38388;&#30340;&#32422;&#26463;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#24230;&#37327;&#21160;&#24577;&#24179;&#34913;&#36923;&#36753;&#20026;&#22522;&#20110;ASP&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#21160;&#24577;&#32422;&#26463;&#35268;&#33539;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#22240;&#27492;&#65292;&#22312;Equilibrium Logic&#30340;&#19968;&#31995;&#21015;&#26102;&#38388;&#25193;&#23637;&#20013;&#65292;&#23427;&#26159;&#26368;&#36890;&#29992;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#23427;&#21253;&#25324;&#20102;Temporal&#12289;Dynamic&#12289;Metric&#21644;regular Equilibrium Logic&#65292;&#20197;&#21450;&#22312;&#25490;&#20013;&#24459;&#25104;&#31435;&#26102;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In temporal extensions of Answer Set Programming (ASP) based on linear-time, the behavior of dynamic systems is captured by sequences of states. While this representation reflects their relative order, it abstracts away the specific times associated with each state. In many applications, however, timing constraints are important like, for instance, when planning and scheduling go hand in hand. We address this by developing a metric extension of linear-time Dynamic Equilibrium Logic, in which dynamic operators are constrained by intervals over integers. The resulting Metric Dynamic Equilibrium Logic provides the foundation of an ASP-based approach for specifying qualitative and quantitative dynamic constraints. As such, it constitutes the most general among a whole spectrum of temporal extensions of Equilibrium Logic. In detail, we show that it encompasses Temporal, Dynamic, Metric, and regular Equilibrium Logic, as well as its classic counterparts once the law of the excluded middle is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#25945;&#23398;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#25552;&#31034;&#38382;&#39064;&#65292;&#20197;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#23398;&#29983;&#36890;&#36807;&#35270;&#35273;&#26041;&#24335;&#33719;&#24471;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;LLMs&#33021;&#22815;&#29702;&#35299;&#30340;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#33021;&#22815;&#36890;&#36807;&#25152;&#26377;&#27979;&#35797;&#29992;&#20363;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.10759</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#38382;&#39064;&#36827;&#34892;&#32534;&#31243;&#25945;&#23398;&#30340;&#26032;&#26041;&#27861;&#65306;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models. (arXiv:2401.10759v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#25945;&#23398;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#25552;&#31034;&#38382;&#39064;&#65292;&#20197;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#23398;&#29983;&#36890;&#36807;&#35270;&#35273;&#26041;&#24335;&#33719;&#24471;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;LLMs&#33021;&#22815;&#29702;&#35299;&#30340;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#33021;&#22815;&#36890;&#36807;&#25152;&#26377;&#27979;&#35797;&#29992;&#20363;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39072;&#35206;&#20102;&#35745;&#31639;&#26426;&#25945;&#32946;&#20960;&#21313;&#24180;&#26469;&#30340;&#25945;&#23398;&#26041;&#27861;&#12290;&#20197;&#21069;&#65292;&#23398;&#29983;&#36890;&#36807;&#32534;&#20889;&#35768;&#22810;&#23567;&#38382;&#39064;&#26469;&#23398;&#20064;&#32534;&#31243;&#65292;&#23545;&#20195;&#30721;&#38405;&#35835;&#21644;&#29702;&#35299;&#30340;&#37325;&#35270;&#31243;&#24230;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;LLMs&#39537;&#21160;&#30340;&#33258;&#30001;&#20195;&#30721;&#29983;&#25104;&#24037;&#20855;&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#30340;&#21021;&#32423;&#32534;&#31243;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#25945;&#23398;&#26041;&#24335;&#65292;&#21363;&#20351;&#29992;&#25552;&#31034;&#38382;&#39064;&#12290;&#23398;&#29983;&#36890;&#36807;&#35270;&#35273;&#26041;&#24335;&#33719;&#24471;&#19968;&#20010;&#38382;&#39064;&#65292;&#25351;&#31034;&#36755;&#20837;&#22914;&#20309;&#36716;&#25442;&#20026;&#36755;&#20986;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;LLMs&#33021;&#22815;&#29702;&#35299;&#30340;&#25552;&#31034;&#12290;&#24403;&#30001;&#23398;&#29983;&#25552;&#31034;&#29983;&#25104;&#30340;&#20195;&#30721;&#33021;&#22815;&#36890;&#36807;&#25152;&#26377;&#27979;&#35797;&#29992;&#20363;&#26102;&#65292;&#38382;&#39064;&#34987;&#35748;&#20026;&#26159;&#27491;&#30830;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27492;&#24037;&#20855;&#30340;&#35774;&#35745;&#65292;&#35752;&#35770;&#20102;&#23398;&#29983;&#22312;&#23398;&#20064;&#20013;&#19982;&#27492;&#24037;&#20855;&#30340;&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#31181;&#26032;&#22411;&#32534;&#31243;&#38382;&#39064;&#21450;&#38598;&#25104;LLMs&#30340;&#35774;&#35745;&#24037;&#20855;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have upended decades of pedagogy in computing education. Students previously learned to code through \textit{writing} many small problems with less emphasis on code reading and comprehension. Recent research has shown that free code generation tools powered by LLMs can solve introductory programming problems presented in natural language with ease. In this paper, we propose a new way to teach programming with Prompt Problems. Students receive a problem visually, indicating how input should be transformed to output, and must translate that to a prompt for an LLM to decipher. The problem is considered correct when the code that is generated by the student prompt can pass all test cases. In this paper we present the design of this tool, discuss student interactions with it as they learn, and provide insights into this new class of programming problems as well as the design tools that integrate LLMs.
&lt;/p&gt;</description></item><item><title>BoolGebra&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#23646;&#24615;&#22270;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#25913;&#36827;&#24067;&#23572;&#20195;&#25968;&#25805;&#20316;&#30340;&#36923;&#36753;&#32508;&#21512;&#12290;&#23427;&#36890;&#36807;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#24182;&#39640;&#25928;&#23450;&#20301;&#20248;&#21270;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#36328;&#35774;&#35745;&#25512;&#35770;&#30340;&#36890;&#29992;&#24615;&#21644;&#35268;&#27169;&#21270;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10753</link><description>&lt;p&gt;
BoolGebra: &#29992;&#20110;&#24067;&#23572;&#20195;&#25968;&#25805;&#20316;&#30340;&#23646;&#24615;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation. (arXiv:2401.10753v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10753
&lt;/p&gt;
&lt;p&gt;
BoolGebra&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#23646;&#24615;&#22270;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#25913;&#36827;&#24067;&#23572;&#20195;&#25968;&#25805;&#20316;&#30340;&#36923;&#36753;&#32508;&#21512;&#12290;&#23427;&#36890;&#36807;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#24182;&#39640;&#25928;&#23450;&#20301;&#20248;&#21270;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#36328;&#35774;&#35745;&#25512;&#35770;&#30340;&#36890;&#29992;&#24615;&#21644;&#35268;&#27169;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#20195;&#25968;&#25805;&#20316;&#26159;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#26680;&#24515;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#20805;&#20998;&#21033;&#29992;&#20248;&#21270;&#26426;&#20250;&#65292;&#24182;&#19988;&#24448;&#24448;&#21463;&#21040;&#25628;&#32034;&#31354;&#38388;&#29190;&#28856;&#21644;&#21487;&#25193;&#23637;&#24615;&#25928;&#29575;&#26377;&#38480;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BoolGebra&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#24067;&#23572;&#20195;&#25968;&#25805;&#20316;&#30340;&#23646;&#24615;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#22522;&#30784;&#36923;&#36753;&#32508;&#21512;&#12290;BoolGebra&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24182;&#23558;&#32467;&#26500;&#21644;&#21151;&#33021;&#20449;&#24687;&#30340;&#21021;&#22987;&#29305;&#24449;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#12290;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#20316;&#30452;&#25509;&#20248;&#21270;&#32467;&#26524;&#39044;&#27979;&#30340;&#39044;&#27979;&#22120;&#65292;&#26174;&#33879;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#24182;&#39640;&#25928;&#23450;&#20301;&#20248;&#21270;&#31354;&#38388;&#12290;&#23454;&#39564;&#28041;&#21450;&#22521;&#35757;BoolGebra&#27169;&#22411;&#20851;&#20110;&#35774;&#35745;&#29305;&#23450;&#21644;&#36328;&#35774;&#35745;&#25512;&#35770;&#65292;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#30340;BoolGebra&#23637;&#31034;&#20102;&#36328;&#35774;&#35745;&#25512;&#35770;&#30340;&#36890;&#29992;&#24615;&#21644;&#35268;&#27169;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boolean algebraic manipulation is at the core of logic synthesis in Electronic Design Automation (EDA) design flow. Existing methods struggle to fully exploit optimization opportunities, and often suffer from an explosive search space and limited scalability efficiency. This work presents BoolGebra, a novel attributed graph-learning approach for Boolean algebraic manipulation that aims to improve fundamental logic synthesis. BoolGebra incorporates Graph Neural Networks (GNNs) and takes initial feature embeddings from both structural and functional information as inputs. A fully connected neural network is employed as the predictor for direct optimization result predictions, significantly reducing the search space and efficiently locating the optimization space. The experiments involve training the BoolGebra model w.r.t design-specific and cross-design inferences using the trained model, where BoolGebra demonstrates generalizability for cross-design inference and its potential to scale 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#32490;&#26694;&#26550;&#26412;&#20307;(EFO)&#65292;&#23427;&#23558;&#24773;&#32490;&#35270;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#35821;&#20041;&#35282;&#33394;&#26469;&#25429;&#25417;&#24773;&#32490;&#32463;&#39564;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;EFO&#37319;&#29992;&#22522;&#20110;&#27169;&#24335;&#30340;&#26412;&#20307;&#35770;&#35774;&#35745;&#65292;&#24182;&#19982;DOLCE&#22522;&#26412;&#26412;&#20307;&#35770;&#23545;&#40784;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#22810;&#31181;&#24773;&#32490;&#29702;&#35770;&#65292;&#24182;&#22312;Emotion Ontology Network&#20013;&#20132;&#21449;&#38142;&#25509;&#12290;&#26412;&#25991;&#20197;Ekman&#30340;&#22522;&#26412;&#24773;&#32490;(BE)&#29702;&#35770;&#20316;&#20026;EFO-BE&#27169;&#22359;&#36827;&#34892;&#31034;&#20363;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#23545;&#34920;&#31034;&#24773;&#32490;&#24773;&#20917;&#36827;&#34892;&#33258;&#21160;&#25512;&#29702;&#12290;&#36890;&#36807;&#20174;Framester&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;BE&#24773;&#32490;&#26694;&#26550;&#23545;EFO-BE&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.10751</link><description>&lt;p&gt;
EFO&#65306;&#24773;&#32490;&#26694;&#26550;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
EFO: the Emotion Frame Ontology. (arXiv:2401.10751v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#32490;&#26694;&#26550;&#26412;&#20307;(EFO)&#65292;&#23427;&#23558;&#24773;&#32490;&#35270;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#35821;&#20041;&#35282;&#33394;&#26469;&#25429;&#25417;&#24773;&#32490;&#32463;&#39564;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;EFO&#37319;&#29992;&#22522;&#20110;&#27169;&#24335;&#30340;&#26412;&#20307;&#35770;&#35774;&#35745;&#65292;&#24182;&#19982;DOLCE&#22522;&#26412;&#26412;&#20307;&#35770;&#23545;&#40784;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#22810;&#31181;&#24773;&#32490;&#29702;&#35770;&#65292;&#24182;&#22312;Emotion Ontology Network&#20013;&#20132;&#21449;&#38142;&#25509;&#12290;&#26412;&#25991;&#20197;Ekman&#30340;&#22522;&#26412;&#24773;&#32490;(BE)&#29702;&#35770;&#20316;&#20026;EFO-BE&#27169;&#22359;&#36827;&#34892;&#31034;&#20363;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#23545;&#34920;&#31034;&#24773;&#32490;&#24773;&#20917;&#36827;&#34892;&#33258;&#21160;&#25512;&#29702;&#12290;&#36890;&#36807;&#20174;Framester&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;BE&#24773;&#32490;&#26694;&#26550;&#23545;EFO-BE&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#19968;&#30452;&#26159;&#28608;&#28872;&#36777;&#35770;&#30340;&#20027;&#39064;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29702;&#35770;&#21644;&#23450;&#20041;&#65292;&#20294;&#20851;&#20110;&#20160;&#20040;&#26159;&#24773;&#32490;&#20197;&#21450;&#22914;&#20309;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#31867;&#20173;&#28982;&#27809;&#26377;&#20849;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OWL&#26694;&#26550;&#30340;&#24773;&#32490;&#26412;&#20307;&#35770;&#65292;&#21363;&#24773;&#32490;&#26694;&#26550;&#26412;&#20307;(EFO) &#12290;EFO&#23558;&#24773;&#32490;&#35270;&#20026;&#35821;&#20041;&#26694;&#26550;&#65292;&#24182;&#20855;&#26377;&#19968;&#32452;&#25429;&#25417;&#24773;&#32490;&#32463;&#39564;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#35821;&#20041;&#35282;&#33394;&#12290;EFO&#37319;&#29992;&#22522;&#20110;&#27169;&#24335;&#30340;&#26412;&#20307;&#35770;&#35774;&#35745;&#65292;&#24182;&#19982;DOLCE&#22522;&#26412;&#26412;&#20307;&#35770;&#23545;&#40784;&#12290;EFO&#29992;&#20110;&#24314;&#27169;&#22810;&#31181;&#24773;&#32490;&#29702;&#35770;&#65292;&#21487;&#20197;&#23558;&#20854;&#20316;&#20026;Emotion Ontology Network&#30340;&#27169;&#22359;&#36827;&#34892;&#20132;&#21449;&#38142;&#25509;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;Ekman&#30340;&#22522;&#26412;&#24773;&#32490;(BE)&#29702;&#35770;&#24314;&#27169;&#20026;EFO-BE&#27169;&#22359;&#26469;&#36827;&#34892;&#31034;&#20363;&#65292;&#24182;&#28436;&#31034;&#22914;&#20309;&#23545;&#24773;&#32490;&#24773;&#20917;&#30340;&#34920;&#31034;&#36827;&#34892;&#33258;&#21160;&#25512;&#29702;&#12290;&#36890;&#36807;&#20174;Framester&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;BE&#24773;&#32490;&#26694;&#26550;&#23545;EFO-BE&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotions are a subject of intense debate in various disciplines. Despite the proliferation of theories and definitions, there is still no consensus on what emotions are, and how to model the different concepts involved when we talk about - or categorize - them. In this paper, we propose an OWL frame-based ontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as semantic frames, with a set of semantic roles that capture the different aspects of emotional experience. EFO follows pattern-based ontology design, and is aligned to the DOLCE foundational ontology. EFO is used to model multiple emotion theories, which can be cross-linked as modules in an Emotion Ontology Network. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE) Theory as an EFO-BE module, and demonstrate how to perform automated inferences on the representation of emotion situations. EFO-BE has been evaluated by lexicalizing the BE emotion frames from within the Framester knowledge 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.10746</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#33041;&#30005;&#35299;&#30721;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#32463;&#24120;&#29992;&#20110;&#21508;&#31181;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#22823;&#37327;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#65292;&#36801;&#31227;&#23398;&#20064;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;DL&#27169;&#22411;&#12290;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#26159;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#65288;EA&#65289;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#20351;&#29992;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#24182;&#19988;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20854;&#23545;&#20849;&#20139;&#21644;&#20010;&#20307;DL&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;EA&#19982;DL&#30456;&#32467;&#21512;&#22312;&#35299;&#30721;BCI&#20449;&#21495;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;EA&#26469;&#35757;&#32451;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#26032;&#21463;&#35797;&#32773;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23558;&#30446;&#26631;&#21463;&#35797;&#32773;&#30340;&#35299;&#30721;&#29575;&#25552;&#39640;&#20102;4.33&#65285;&#65292;&#24182;&#19988;&#25910;&#25947;&#26102;&#38388;&#32553;&#30701;&#20102;&#36229;&#36807;70&#65285;&#12290;&#25105;&#20204;&#36824;&#20026;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10745</link><description>&lt;p&gt;
&#23545;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#30340;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models. (arXiv:2401.10745v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;ChatGPT&#12289;LaMDA&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#25216;&#26415;&#34892;&#19994;&#21644;&#20854;&#20182;&#34892;&#19994;&#23545;LLMs&#30340;&#24320;&#21457;&#21644;&#20351;&#29992;&#26377;&#25152;&#22686;&#21152;&#12290;&#34429;&#28982;LLMs&#30340;&#27700;&#24179;&#23578;&#26410;&#36229;&#36807;&#20154;&#31867;&#26234;&#33021;&#65292;&#20294;&#24635;&#26377;&#19968;&#22825;&#20250;&#36798;&#21040;&#36825;&#19968;&#28857;&#12290;&#36825;&#31181;LLMs&#21487;&#20197;&#31216;&#20026;&#39640;&#32423;LLMs&#12290;&#30446;&#21069;&#65292;&#30001;&#20110;&#23578;&#26410;&#36798;&#21040;&#36825;&#19968;&#28857;&#65292;&#20351;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;LLMs&#30340;&#38382;&#39064;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#19968;&#26086;&#36798;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#26080;&#27861;&#20805;&#20998;&#20934;&#22791;&#22909;&#20197;&#36947;&#24503;&#21644;&#26368;&#20339;&#26041;&#24335;&#22788;&#29702;&#20854;&#20135;&#29983;&#30340;&#21518;&#26524;&#65292;&#36825;&#23558;&#23548;&#33268;&#19981;&#21487;&#39044;&#26399;&#30340;&#21518;&#26524;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;LLMs&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of ChatGPT, LaMDA and other large language models (LLMs), there has been an increase in development and usage of LLMs within the technology sector and other sectors. While the level in which LLMs has not reached a level where it has surpassed human intelligence, there will be a time when it will. Such LLMs can be referred to as advanced LLMs. Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet. However, this is a problem as once we do reach that point, we will not be adequately prepared to deal with the aftermath of it in an ethical and optimal way, which will lead to undesired and unexpected consequences. This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.
&lt;/p&gt;</description></item><item><title>FinLLMs&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37329;&#34701;&#25512;&#29702;&#25968;&#25454;&#38598;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#24120;&#35265;&#37329;&#34701;&#20844;&#24335;&#29983;&#25104;&#37329;&#34701;&#38382;&#31572;&#25968;&#25454;&#65292;&#26088;&#22312;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#36164;&#28304;&#21644;&#39640;&#26114;&#30340;&#25163;&#24037;&#27880;&#37322;&#25104;&#26412;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10744</link><description>&lt;p&gt;
FinLLMs&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37329;&#34701;&#25512;&#29702;&#25968;&#25454;&#38598;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models. (arXiv:2401.10744v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10744
&lt;/p&gt;
&lt;p&gt;
FinLLMs&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37329;&#34701;&#25512;&#29702;&#25968;&#25454;&#38598;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#24120;&#35265;&#37329;&#34701;&#20844;&#24335;&#29983;&#25104;&#37329;&#34701;&#38382;&#31572;&#25968;&#25454;&#65292;&#26088;&#22312;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#36164;&#28304;&#21644;&#39640;&#26114;&#30340;&#25163;&#24037;&#27880;&#37322;&#25104;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#22312;&#37329;&#34701;&#39046;&#22495;&#65292;&#21019;&#24314;&#21253;&#21547;&#34920;&#26684;&#21644;&#38271;&#25991;&#26412;&#28151;&#21512;&#30340;&#25968;&#20540;&#25512;&#29702;&#25968;&#25454;&#38598;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25163;&#24037;&#27880;&#37322;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#30340;&#25968;&#25454;&#36164;&#28304;&#24182;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FinLLMs&#65292;&#19968;&#31181;&#22522;&#20110;&#24120;&#35265;&#37329;&#34701;&#20844;&#24335;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#37329;&#34701;&#38382;&#31572;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#24120;&#35265;&#37329;&#34701;&#20844;&#24335;&#21015;&#34920;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#20844;&#24335;&#20351;&#29992;&#30340;&#21464;&#37327;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#24418;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21512;&#24182;&#20855;&#26377;&#30456;&#21516;&#21464;&#37327;&#30340;&#20844;&#24335;&#20316;&#20026;&#26032;&#20803;&#32032;&#26469;&#25193;&#20805;&#20844;&#24335;&#38598;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#36941;&#21382;&#26500;&#24314;&#30340;&#22270;&#24418;&#65292;&#25506;&#32034;&#36890;&#36807;&#25163;&#24037;&#27880;&#37322;&#33719;&#24471;&#30340;&#20844;&#24335;&#65292;&#24182;&#21512;&#24182;&#20855;&#26377;&#20849;&#20139;&#21464;&#37327;&#30340;&#20844;&#24335;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;GPT-3.5&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#21253;&#21547;&#34920;&#26684;&#20449;&#24687;&#21644;&#38271;&#25991;&#26412;&#20869;&#23481;&#30340;&#37329;&#34701;&#38382;&#31572;&#25968;&#25454;&#65292;&#20511;&#21161;&#24050;&#25910;&#38598;&#30340;&#20844;&#24335;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language models (LLMs) usually rely on extensive training datasets. In the financial domain, creating numerical reasoning datasets that include a mix of tables and long text often involves substantial manual annotation expenses. To address the limited data resources and reduce the annotation cost, we introduce FinLLMs, a method for generating financial question-answering data based on common financial formulas using Large Language Models. First, we compile a list of common financial formulas and construct a graph based on the variables these formulas employ. We then augment the formula set by combining those that share identical variables as new elements. Specifically, we explore formulas obtained by manual annotation and merge those formulas with shared variables by traversing the constructed graph. Finally, utilizing GPT-3.5, we generate financial question-answering data that encompasses both tabular information and long textual content, building on the collected formula set. O
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26723;&#21160;&#24577;&#38382;&#31572;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#12290;&#36890;&#36807;Langchain&#21644;Transformer-based LLMs&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20020;&#24202;&#31508;&#35760;&#24182;&#33719;&#24471;&#30456;&#20851;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Wizard Vicuna&#20855;&#26377;&#20986;&#33394;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#35745;&#31639;&#35201;&#27714;&#36739;&#39640;&#12290;&#27169;&#22411;&#20248;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#32422;48&#20493;&#30340;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#20135;&#29983;&#24187;&#35937;&#21644;&#22810;&#26679;&#21270;&#21307;&#30103;&#26696;&#20363;&#35780;&#20272;&#30340;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23545;&#20110;&#21457;&#25496;&#20020;&#24202;&#31508;&#35760;&#30340;&#20215;&#20540;&#21644;&#25512;&#36827;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.10733</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26723;&#30340;&#21160;&#24577;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Dynamic Q&amp;A of Clinical Documents with Large Language Models. (arXiv:2401.10733v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26723;&#21160;&#24577;&#38382;&#31572;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#12290;&#36890;&#36807;Langchain&#21644;Transformer-based LLMs&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20020;&#24202;&#31508;&#35760;&#24182;&#33719;&#24471;&#30456;&#20851;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Wizard Vicuna&#20855;&#26377;&#20986;&#33394;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#35745;&#31639;&#35201;&#27714;&#36739;&#39640;&#12290;&#27169;&#22411;&#20248;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#32422;48&#20493;&#30340;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#20135;&#29983;&#24187;&#35937;&#21644;&#22810;&#26679;&#21270;&#21307;&#30103;&#26696;&#20363;&#35780;&#20272;&#30340;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23545;&#20110;&#21457;&#25496;&#20020;&#24202;&#31508;&#35760;&#30340;&#20215;&#20540;&#21644;&#25512;&#36827;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#25910;&#24405;&#20102;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#37325;&#35201;&#24739;&#32773;&#25968;&#25454;&#12290;&#38543;&#30528;&#36825;&#20123;&#31508;&#35760;&#25968;&#37327;&#21644;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#25163;&#21160;&#25552;&#21462;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#23545;&#20020;&#24202;&#31508;&#35760;&#36827;&#34892;&#21160;&#24577;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30001;Langchain&#21644;&#22522;&#20110;Transformer&#30340;LLMs&#39537;&#21160;&#65292;&#20801;&#35768;&#29992;&#25143;&#29992;&#33258;&#28982;&#35821;&#35328;&#21457;&#20986;&#26597;&#35810;&#65292;&#24182;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#33719;&#24471;&#30456;&#20851;&#31572;&#26696;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#23884;&#20837;&#27169;&#22411;&#21644;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;Wizard Vicuna&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#23613;&#31649;&#35745;&#31639;&#35201;&#27714;&#36739;&#39640;&#12290;&#27169;&#22411;&#20248;&#21270;&#65292;&#21253;&#25324;&#26435;&#37325;&#37327;&#21270;&#65292;&#23558;&#24310;&#36831;&#25552;&#39640;&#20102;&#32422;48&#20493;&#12290;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#20215;&#20540;&#28508;&#21147;&#65292;&#20294;&#20173;&#23384;&#22312;&#27169;&#22411;&#20135;&#29983;&#24187;&#35937;&#21644;&#26377;&#38480;&#30340;&#22810;&#26679;&#21270;&#21307;&#30103;&#26696;&#20363;&#35780;&#20272;&#31561;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#21457;&#25496;&#20020;&#24202;&#31508;&#35760;&#30340;&#20215;&#20540;&#21644;&#25512;&#21160;AI&#39537;&#21160;&#30340;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) house crucial patient data in clinical notes. As these notes grow in volume and complexity, manual extraction becomes challenging. This work introduces a natural language interface using large language models (LLMs) for dynamic question-answering on clinical notes. Our chatbot, powered by Langchain and transformer-based LLMs, allows users to query in natural language, receiving relevant answers from clinical notes. Experiments, utilizing various embedding models and advanced LLMs, show Wizard Vicuna's superior accuracy, albeit with high compute demands. Model optimization, including weight quantization, improves latency by approximately 48 times. Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain. Addressing these gaps is crucial for unlocking the value in clinical notes and advancing AI-driven clinical decision-making.
&lt;/p&gt;</description></item><item><title>&#31532;14&#23626;&#20960;&#20309;&#33258;&#21160;&#25512;&#29702;&#22269;&#38469;&#20250;&#35758;&#65288;ADG&#65289;&#22312;&#22622;&#23572;&#32500;&#20122;&#36125;&#23572;&#26684;&#33713;&#24503;&#20030;&#21150;&#65292;&#37325;&#28857;&#20851;&#27880;&#25945;&#32946;&#20013;&#30340;&#25512;&#29702;&#12290;&#29305;&#36992;&#25253;&#21578;&#22025;&#23486;&#20998;&#21035;&#25506;&#35752;&#20102;&#20960;&#20309;&#30340;&#24418;&#24335;&#21270;&#12289;&#31639;&#26415;&#21270;&#21644;&#33258;&#21160;&#21270;&#65292;&#20197;&#21450;&#21452;&#26354;&#20960;&#20309;&#30340;&#33258;&#21160;&#21270;&#12289;&#24418;&#24335;&#21270;&#21644;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.10725</link><description>&lt;p&gt;
&#31532;14&#23626;&#20960;&#20309;&#33258;&#21160;&#25512;&#29702;&#22269;&#38469;&#20250;&#35758;&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
Proceedings 14th International Conference on Automated Deduction in Geometry. (arXiv:2401.10725v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10725
&lt;/p&gt;
&lt;p&gt;
&#31532;14&#23626;&#20960;&#20309;&#33258;&#21160;&#25512;&#29702;&#22269;&#38469;&#20250;&#35758;&#65288;ADG&#65289;&#22312;&#22622;&#23572;&#32500;&#20122;&#36125;&#23572;&#26684;&#33713;&#24503;&#20030;&#21150;&#65292;&#37325;&#28857;&#20851;&#27880;&#25945;&#32946;&#20013;&#30340;&#25512;&#29702;&#12290;&#29305;&#36992;&#25253;&#21578;&#22025;&#23486;&#20998;&#21035;&#25506;&#35752;&#20102;&#20960;&#20309;&#30340;&#24418;&#24335;&#21270;&#12289;&#31639;&#26415;&#21270;&#21644;&#33258;&#21160;&#21270;&#65292;&#20197;&#21450;&#21452;&#26354;&#20960;&#20309;&#30340;&#33258;&#21160;&#21270;&#12289;&#24418;&#24335;&#21270;&#21644;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ADG&#26159;&#19968;&#20010;&#20132;&#27969;&#20960;&#20309;&#21644;&#33258;&#21160;&#25512;&#29702;&#20132;&#21449;&#39046;&#22495;&#24605;&#24819;&#21644;&#35266;&#28857;&#12289;&#23637;&#31034;&#30740;&#31350;&#25104;&#26524;&#21644;&#36827;&#23637;&#12289;&#23637;&#31034;&#36719;&#20214;&#24037;&#20855;&#30340;&#35770;&#22363;&#12290;&#20250;&#35758;&#27599;&#20004;&#24180;&#20030;&#21150;&#19968;&#27425;&#12290;&#20043;&#21069;&#30340;ADG&#20250;&#35758;&#20998;&#21035;&#22312;2021&#24180;&#30340;Hagenberg&#65288;&#22312;&#32447;&#65292;&#22240;COVID-19&#32780;&#24310;&#26399;&#65289;&#12289;2018&#24180;&#30340;&#21335;&#23425;&#12289;2016&#24180;&#30340;&#26031;&#29305;&#25289;&#26031;&#22561;&#12289;2014&#24180;&#30340;&#31185;&#33521;&#24067;&#25289;&#12289;2012&#24180;&#30340;&#29233;&#19969;&#22561;&#12289;2010&#24180;&#30340;&#24917;&#23612;&#40657;&#12289;2008&#24180;&#30340;&#19978;&#28023;&#12289;2006&#24180;&#30340;&#24222;&#29305;&#38886;&#24503;&#25289;&#12289;2004&#24180;&#30340;&#30422;&#24681;&#26031;&#32500;&#23572;&#12289;2002&#24180;&#30340;Hagenberg&#12289;2000&#24180;&#30340;&#33487;&#40654;&#19990;&#12289;1998&#24180;&#30340;&#21271;&#20140;&#21644;1996&#24180;&#30340;&#22270;&#21346;&#20857;&#20030;&#21150;&#12290;&#31532;14&#23626;ADG&#20110;2023&#24180;9&#26376;20-22&#26085;&#22312;&#22622;&#23572;&#32500;&#20122;&#36125;&#23572;&#26684;&#33713;&#24503;&#20030;&#21150;&#65292;&#26412;&#23626;ADG&#36824;&#35774;&#32622;&#20102;&#19968;&#20010;&#29305;&#21035;&#20851;&#27880;&#30340;&#20027;&#39064;&#8212;&#8212;&#25945;&#32946;&#20013;&#30340;&#25512;&#29702;&#12290;&#29305;&#36992;&#25253;&#21578;&#22025;&#23486;&#21253;&#25324;&#27861;&#22269;&#26031;&#29305;&#25289;&#26031;&#22561;&#22823;&#23398;&#30340;Julien Narboux&#65306;&#8220;&#20960;&#20309;&#30340;&#24418;&#24335;&#21270;&#12289;&#31639;&#26415;&#21270;&#21644;&#33258;&#21160;&#21270;&#8221;&#65292;&#22622;&#23572;&#32500;&#20122;&#36125;&#23572;&#26684;&#33713;&#24503;&#22823;&#23398;&#30340;Filip Mari\'c&#65306;&#8220;&#21452;&#26354;&#20960;&#20309;&#30340;&#33258;&#21160;&#21270;&#12289;&#24418;&#24335;&#21270;&#21644;&#21487;&#35270;&#21270;&#8221;&#65292;&#20197;&#21450;University of
&lt;/p&gt;
&lt;p&gt;
ADG is a forum to exchange ideas and views, to present research results and progress, and to demonstrate software tools at the intersection between geometry and automated deduction. The conference is held every two years. The previous editions of ADG were held in Hagenberg in 2021 (online, postponed from 2020 due to COVID-19), Nanning in 2018, Strasbourg in 2016, Coimbra in 2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006, Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and Toulouse in 1996.  The 14th edition, ADG 2023, was held in Belgrade, Serbia, in September 20-22, 2023. This edition of ADG had an additional special focus topic, Deduction in Education.  Invited Speakers: Julien Narboux, University of Strasbourg, France "Formalisation, arithmetization and automatisation of geometry"; Filip Mari\'c, University of Belgrade, Serbia, "Automatization, formalization and visualization of hyperbolic geometry"; Zlatan Magajna, University 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.10711</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26088;&#22312;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#23613;&#31649;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22270;&#20687;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#65292;&#20165;&#20165;&#26159;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#38024;&#23545;&#38382;&#39064;&#20851;&#38190;&#26102;&#38388;&#25139;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#24378;&#21046;LMMs&#20351;&#29992;&#38382;&#39064;&#20851;&#38190;&#26102;&#21051;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#21512;&#24182;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#20197;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#36825;&#20123;&#26102;&#21051;&#23558;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20266;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#24369;&#30417;&#30563;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#12290;GCG&#23398;&#20064;&#22810;&#20010;&#39640;&#26031;&#20989;&#25968;&#26469;&#25551;&#36848;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#34892;&#24615;&#23548;&#21521;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24179;&#34913;&#23433;&#20840;&#32422;&#26463;&#28385;&#36275;&#12289;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#35268;&#33539;&#21270;&#12290;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#23558;&#30828;&#23433;&#20840;&#32422;&#26463;&#36716;&#21270;&#20026;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#26368;&#22823;&#21487;&#34892;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.10700</link><description>&lt;p&gt;
&#23433;&#20840;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#21487;&#34892;&#24615;&#23548;&#21521;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model. (arXiv:2401.10700v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#34892;&#24615;&#23548;&#21521;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24179;&#34913;&#23433;&#20840;&#32422;&#26463;&#28385;&#36275;&#12289;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#35268;&#33539;&#21270;&#12290;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#23558;&#30828;&#23433;&#20840;&#32422;&#26463;&#36716;&#21270;&#20026;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#26368;&#22823;&#21487;&#34892;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#25928;&#36991;&#20813;&#39118;&#38505;&#30340;&#22312;&#32447;&#20132;&#20114;&#20197;&#23454;&#29616;&#23433;&#20840;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#24378;&#21046;&#36719;&#32422;&#26463;&#65292;&#21363;&#23558;&#26399;&#26395;&#30340;&#23433;&#20840;&#36829;&#35268;&#32422;&#26463;&#22312;&#39044;&#23450;&#30340;&#38408;&#20540;&#20197;&#19979;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#32467;&#26524;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#21478;&#19968;&#31181;&#36873;&#25321;&#26159;&#24378;&#21046;&#38646;&#36829;&#35268;&#30340;&#30828;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#22312;&#23433;&#20840;&#32422;&#26463;&#28385;&#36275;&#12289;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#25152;&#26045;&#21152;&#30340;&#34892;&#20026;&#27491;&#21017;&#21270;&#20043;&#38388;&#21462;&#24471;&#36866;&#24403;&#30340;&#24179;&#34913;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#23433;&#20840;&#25511;&#21046;&#29702;&#35770;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#30828;&#23433;&#20840;&#32422;&#26463;&#21487;&#20197;&#31561;&#25928;&#22320;&#36716;&#21270;&#20026;&#35782;&#21035;&#32473;&#23450;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#26368;&#22823;&#21487;&#34892;&#21306;&#22495;&#12290;&#36825;&#26080;&#32541;&#22320;&#23558;&#21407;&#22987;&#30340;&#19977;&#37325;&#38382;&#39064;&#36716;&#21270;&#20026;&#20381;&#36182;&#20110;&#21487;&#34892;&#24615;&#30340;&#30446;&#26631;&#65292;&#21363;&#22312;&#21487;&#34892;&#24615;&#33539;&#22260;&#20869;&#26368;&#22823;&#21270;&#22870;&#21169;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10690</link><description>&lt;p&gt;
&#36229;&#36234;RMSE&#21644;MAE&#65306;&#24341;&#20837;EAUC&#26469;&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#30340;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#23545;&#23454;&#20307;&#30340;&#23454;&#20540;&#32467;&#26524;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#22522;&#30784;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39044;&#27979;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#35780;&#20998;&#65289;&#65292;&#22312;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#20013;&#20063;&#26377;&#35768;&#22810;&#28508;&#21147;&#20294;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#22312;&#20010;&#24615;&#21270;&#33647;&#29702;&#23398;&#20013;&#36817;&#20284;&#30830;&#23450;&#24739;&#32773;&#30340;&#36866;&#24403;&#21058;&#37327;&#65289;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20010;&#20307;&#23454;&#20307;&#35266;&#23519;&#20540;&#20998;&#24067;&#30340;&#38750;&#22343;&#21248;&#24615;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#20013;&#30340;&#20005;&#37325;&#20559;&#35265;&#39044;&#27979;&#65292;&#20559;&#21521;&#20110;&#23454;&#20307;&#30340;&#35266;&#23519;&#36807;&#21435;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#21478;&#31867;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#27604;&#38543;&#26426;&#39044;&#27979;&#26356;&#24046;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#21478;&#31867;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#21478;&#31867;-&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;EAUC&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#34917;&#20805;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#37327;&#21270;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;PrNet&#26469;&#36827;&#34892;&#20266;&#36317;&#20462;&#27491;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10685</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#20551;&#20266;&#36317;&#20462;&#27491;&#23454;&#29616;&#31471;&#21040;&#31471;GPS&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-End GPS Localization with Neural Pseudorange Correction. (arXiv:2401.10685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;PrNet&#26469;&#36827;&#34892;&#20266;&#36317;&#20462;&#27491;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#36317;&#35823;&#24046;&#26159;GPS&#23450;&#20301;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#20197;&#24448;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#20013;&#38388;&#26631;&#31614;&#36827;&#34892;&#20266;&#36317;&#35823;&#24046;&#22238;&#24402;&#21644;&#28040;&#38500;&#12290;&#19982;&#20043;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#20351;&#29992;GPS&#25509;&#25910;&#26426;&#29366;&#24577;&#30340;&#30495;&#23454;&#20540;&#35745;&#31639;&#26368;&#32456;&#20219;&#21153;&#25439;&#22833;&#65292;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#20266;&#36317;&#20462;&#27491;&#30340;&#31070;&#32463;&#32593;&#32476;PrNet&#12290;&#25439;&#22833;&#23545;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#26799;&#24230;&#36890;&#36807;&#21487;&#24494;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#22120;&#21453;&#21521;&#20256;&#25773;&#21040;PrNet&#12290;&#36890;&#36807;&#20351;&#29992;Android&#25163;&#26426;&#25910;&#38598;&#30340;GPS&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;E2E-PrNet&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudorange errors are the root cause of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a differentiable nonlinear least squares optimizer to PrNet. The feasibility is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the state-of-the-art end-to-end GPS localization methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#39044;&#27979;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#24182;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#35843;&#25972;&#65292;&#35813;&#26694;&#26550;&#38477;&#20302;&#20102;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#24182;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.10660</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21333;&#35821;&#25991;&#26412;&#29983;&#25104;&#30340;&#21152;&#36895;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation. (arXiv:2401.10660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10660
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#39044;&#27979;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#24182;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#35843;&#25972;&#65292;&#35813;&#26694;&#26550;&#38477;&#20302;&#20102;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#24182;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#19981;&#20165;&#22312;&#33521;&#35821;&#32780;&#19988;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#37117;&#20419;&#36827;&#20102;&#22797;&#26434;&#30340;&#35821;&#35328;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#35760;&#22120;&#65288;&#22914;Llama&#65289;&#22312;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#65292;&#20542;&#21521;&#20110;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#36807;&#20998;&#20998;&#21106;&#26631;&#35760;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38750;&#32599;&#39532;&#23383;&#27597;&#35821;&#35328;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#36825;&#20123;&#35821;&#35328;&#36890;&#24120;&#22312;&#23383;&#31526;&#25110;Unicode&#32423;&#21035;&#19978;&#34987;&#21010;&#20998;&#65292;&#23548;&#33268;&#25991;&#26412;&#29983;&#25104;&#36895;&#24230;&#36739;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#36825;&#20123;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#39044;&#27979;&#27604;&#20256;&#32479;&#30340;&#22810;&#35821;&#35328;&#26631;&#35760;&#22120;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#65292;&#24182;&#19988;&#19987;&#38376;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35299;&#30721;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#35299;&#30721;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models have facilitated the execution of complex language tasks, not only in English but also in non-English languages. However, the tokenizers of most language models, such as Llama, trained on English-centric corpora, tend to excessively fragment tokens in non-English languages. This issue is especially pronounced in non-roman alphabetic languages, which are often divided at a character or even Unicode level, leading to slower text generation. To address this, our study introduces a novel framework designed to expedite text generation in these languages. This framework predicts larger linguistic units than those of conventional multilingual tokenizers and is specifically tailored to the target language, thereby reducing the number of decoding steps required. Our empirical results demonstrate that the proposed framework increases the generation speed by a factor of 1.9 compared to standard decoding while maintaining the performance of a pre-traine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26631;&#20934;&#20197;&#21450;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;</title><link>http://arxiv.org/abs/2401.10643</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#32508;&#21512;&#35843;&#26597;&#65306;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges. (arXiv:2401.10643v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26631;&#20934;&#20197;&#21450;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#26088;&#22312;&#23558;&#26469;&#33258;&#20998;&#24067;&#22312;&#19981;&#21516;&#20132;&#36890;&#29615;&#22659;&#30340;&#25668;&#20687;&#22836;&#32593;&#32476;&#20013;&#30340;&#36710;&#36742;&#22270;&#20687;&#36827;&#34892;&#20851;&#32852;&#12290;&#22312;&#36710;&#36742;&#20013;&#24515;&#25216;&#26415;&#33539;&#30068;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22312;&#37096;&#32626;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#21644;&#25512;&#36827;&#26234;&#24935;&#22478;&#24066;&#20513;&#35758;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;&#36710;&#36742; ReID &#25216;&#26415;&#30340;&#28436;&#36827;&#12290;&#22240;&#27492;&#65292;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#24050;&#21464;&#24471;&#36843;&#20999;&#19988;&#19981;&#21487;&#36991;&#20813;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#36710;&#36742; ReID &#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#21253;&#25324;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#36825;&#20123;&#20998;&#31867;&#20013;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#65292;&#24182;&#21246;&#21202;&#20102;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Vehicle re-identification (ReID) endeavors to associate vehicle images collected from a distributed network of cameras spanning diverse traffic environments. This task assumes paramount importance within the spectrum of vehicle-centric technologies, playing a pivotal role in deploying Intelligent Transportation Systems (ITS) and advancing smart city initiatives. Rapid advancements in deep learning have significantly propelled the evolution of vehicle ReID technologies in recent years. Consequently, undertaking a comprehensive survey of methodologies centered on deep learning for vehicle re-identification has become imperative and inescapable. This paper extensively explores deep learning techniques applied to vehicle ReID. It outlines the categorization of these methods, encompassing supervised and unsupervised approaches, delves into existing research within these categories, introduces datasets and evaluation criteria, and delineates forthcoming challenges and potential research dire
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34678;&#24418;&#26680;&#31038;&#21306;&#32467;&#26500;&#30340;&#24555;&#36895;&#31038;&#21306;&#25628;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#38543;&#26426;&#28216;&#36208;&#19982;&#37325;&#21551;&#31639;&#27861;&#21644;&#34678;&#24418;&#24230;&#26469;&#35780;&#20272;&#31038;&#21306;&#20869;&#39030;&#28857;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#25928;&#30340;&#39030;&#28857;&#36317;&#31163;&#26356;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#22312;&#22823;&#22411;&#22270;&#20013;&#23547;&#25214;&#23494;&#38598;&#36830;&#25509;&#30340;&#23376;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.10642</link><description>&lt;p&gt;
&#22823;&#22411;&#26631;&#35760;&#22270;&#30340;&#24555;&#36895;&#34678;&#24418;&#26680;&#31038;&#21306;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Fast Butterfly-Core Community Search For Large Labeled Graphs. (arXiv:2401.10642v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34678;&#24418;&#26680;&#31038;&#21306;&#32467;&#26500;&#30340;&#24555;&#36895;&#31038;&#21306;&#25628;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#38543;&#26426;&#28216;&#36208;&#19982;&#37325;&#21551;&#31639;&#27861;&#21644;&#34678;&#24418;&#24230;&#26469;&#35780;&#20272;&#31038;&#21306;&#20869;&#39030;&#28857;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#25928;&#30340;&#39030;&#28857;&#36317;&#31163;&#26356;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#22312;&#22823;&#22411;&#22270;&#20013;&#23547;&#25214;&#23494;&#38598;&#36830;&#25509;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#25628;&#32034;&#26088;&#22312;&#22312;&#22270;&#20013;&#35782;&#21035;&#19982;&#26597;&#35810;&#39030;&#28857;&#23545;&#24212;&#30340;&#23494;&#38598;&#36830;&#25509;&#30340;&#23376;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24322;&#26500;&#22270;&#31038;&#21306;&#25628;&#32034;&#26041;&#27861;&#38656;&#35201;&#24110;&#21161;&#35782;&#21035;&#36328;&#32452;&#31038;&#21306;&#65292;&#24182;&#19988;&#23384;&#22312;&#25928;&#29575;&#38382;&#39064;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34678;&#24418;&#26680;&#31038;&#21306;&#65288;BCC&#65289;&#32467;&#26500;&#30340;&#24555;&#36895;&#31038;&#21306;&#25628;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#22270;&#12290;&#36890;&#36807;&#38543;&#26426;&#28216;&#36208;&#19982;&#37325;&#21551;&#65288;RWR&#65289;&#31639;&#27861;&#21644;&#34678;&#24418;&#24230;&#26469;&#20840;&#38754;&#35780;&#20272;&#31038;&#21306;&#20869;&#39030;&#28857;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#24555;&#36895;&#26356;&#26032;&#39046;&#23548;&#39030;&#28857;&#20197;&#20445;&#25345;&#36328;&#32452;&#36830;&#36143;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#39030;&#28857;&#36317;&#31163;&#26356;&#26032;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#39030;&#28857;&#35775;&#38382;&#27425;&#25968;&#24182;&#22686;&#24378;&#25805;&#20316;&#25928;&#29575;&#12290;&#23545;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#22270;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Search (CS) aims to identify densely interconnected subgraphs corresponding to query vertices within a graph. However, existing heterogeneous graph-based community search methods need help identifying cross-group communities and suffer from efficiency issues, making them unsuitable for large graphs. This paper presents a fast community search model based on the Butterfly-Core Community (BCC) structure for heterogeneous graphs. The Random Walk with Restart (RWR) algorithm and butterfly degree comprehensively evaluate the importance of vertices within communities, allowing leader vertices to be rapidly updated to maintain cross-group cohesion. Moreover, we devised a more efficient method for updating vertex distances, which minimizes vertex visits and enhances operational efficiency. Extensive experiments on several real-world temporal graphs demonstrate the effectiveness and efficiency of this solution.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;D-truss&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#26377;&#21521;&#22270;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#31038;&#21306;&#25628;&#32034;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#28040;&#32791;&#36807;&#22810;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10641</link><description>&lt;p&gt;
&#22522;&#20110;&#26689;&#26550;&#30340;&#22823;&#22411;&#26377;&#21521;&#22270;&#31038;&#21306;&#25628;&#32034;&#30340;&#26377;&#25928;&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
An Effective Index for Truss-based Community Search on Large Directed Graphs. (arXiv:2401.10641v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10641
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;D-truss&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#26377;&#21521;&#22270;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#31038;&#21306;&#25628;&#32034;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#28040;&#32791;&#36807;&#22810;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#25628;&#32034;&#26159;&#31038;&#21306;&#26816;&#27979;&#30340;&#19968;&#20010;&#34893;&#29983;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#32447;&#21644;&#20010;&#24615;&#21270;&#22320;&#21457;&#29616;&#31038;&#21306;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#38469;&#32593;&#32476;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#23613;&#31649;&#23545;&#20110;&#26080;&#21521;&#22270;&#24050;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#26368;&#36817;&#23545;&#20110;&#26377;&#21521;&#22270;&#20013;&#30340;&#31038;&#21306;&#25628;&#32034;&#38382;&#39064;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;D-truss&#27169;&#22411;&#22312;&#26816;&#32034;&#21040;&#30340;&#31038;&#21306;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;D-truss&#30340;&#24037;&#20316;&#26080;&#27861;&#22312;&#22823;&#22411;&#22270;&#19978;&#36827;&#34892;&#39640;&#25928;&#30340;&#31038;&#21306;&#25628;&#32034;&#65292;&#22240;&#20026;&#26816;&#32034;&#26368;&#22823;D-truss&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#22826;&#22810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#21512;&#24182;&#20851;&#31995;&#65292;&#21363;D-truss-connected&#65292;&#20197;&#25429;&#25417;D-truss&#20013;&#36793;&#32536;&#30340;&#20869;&#22312;&#23494;&#24230;&#21644;&#20957;&#32858;&#21147;&#12290;&#35813;&#20851;&#31995;&#23558;&#25152;&#26377;&#21407;&#22987;&#22270;&#20013;&#30340;&#36793;&#32536;&#21010;&#20998;&#20026;&#19968;&#31995;&#21015;D-truss-connected&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;D-truss&#26500;&#36896;&#20102;&#19968;&#20010;&#31616;&#27905;&#19988;&#32039;&#20945;&#30340;&#32034;&#24341;&#65292;&#31216;&#20026;ConDTruss&#12290;
&lt;/p&gt;
&lt;p&gt;
Community search is a derivative of community detection that enables online and personalized discovery of communities and has found extensive applications in massive real-world networks. Recently, there needs to be more focus on the community search issue within directed graphs, even though substantial research has been carried out on undirected graphs. The recently proposed D-truss model has achieved good results in the quality of retrieved communities. However, existing D-truss-based work cannot perform efficient community searches on large graphs because it consumes too many computing resources to retrieve the maximal D-truss. To overcome this issue, we introduce an innovative merge relation known as D-truss-connected to capture the inherent density and cohesiveness of edges within D-truss. This relation allows us to partition all the edges in the original graph into a series of D-truss-connected classes. Then, we construct a concise and compact index, ConDTruss, based on D-truss-co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#24544;&#35802;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#23454;&#39564;&#20013;&#30340;&#29616;&#26377;&#24230;&#37327;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25104;&#20026;&#36825;&#20123;&#24230;&#37327;&#30340;&#23458;&#35266;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10640</link><description>&lt;p&gt;
XAI&#30340;&#24544;&#35802;&#24230;&#24230;&#37327;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comprehensive study on fidelity metrics for XAI. (arXiv:2401.10640v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10640
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#24544;&#35802;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#23454;&#39564;&#20013;&#30340;&#29616;&#26377;&#24230;&#37327;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25104;&#20026;&#36825;&#20123;&#24230;&#37327;&#30340;&#23458;&#35266;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#31995;&#32479;&#30340;&#20351;&#29992;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#22914;&#20309;&#27491;&#30830;&#36873;&#25321;XAI&#26041;&#27861;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#22256;&#38590;&#22312;&#20110;&#32570;&#20047;&#26631;&#20934;&#31572;&#26696;&#12290;&#19968;&#20123;&#20316;&#32773;&#25552;&#20986;&#20102;&#24230;&#37327;&#19981;&#21516;XAI&#26041;&#27861;&#24544;&#35802;&#24230;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#24230;&#37327;&#32570;&#20047;&#39564;&#35777;&#65292;&#24182;&#23384;&#22312;&#26377;&#20851;&#30340;&#20998;&#27495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#24544;&#35802;&#24230;&#24230;&#37327;&#65292;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#36879;&#26126;&#27169;&#22411;&#65292;&#21363;&#20915;&#31574;&#26641;&#12290;&#36825;&#20010;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#23436;&#32654;&#24544;&#35802;&#24230;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26500;&#25104;&#20102;&#36825;&#20123;&#24230;&#37327;&#30340;&#31532;&#19968;&#20010;&#23458;&#35266;&#22522;&#20934;&#65292;&#20415;&#20110;&#27604;&#36739;&#29616;&#26377;&#30340;&#25552;&#26696;&#65292;&#24182;&#36229;&#36234;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#22522;&#20934;&#24212;&#29992;&#20110;&#35780;&#20272;&#20004;&#20010;&#19981;&#21516;&#23454;&#39564;&#20013;&#29616;&#26377;&#30340;&#24544;&#35802;&#24230;&#24230;&#37327;&#65292;&#27599;&#20010;&#23454;&#39564;&#20351;&#29992;&#21253;&#21547;52,000&#24352;&#22270;&#29255;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#22270;&#29255;&#22823;&#23567;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of eXplainable Artificial Intelligence (XAI) systems has introduced a set of challenges that need resolution. Herein, we focus on how to correctly select an XAI method, an open questions within the field. The inherent difficulty of this task is due to the lack of a ground truth. Several authors have proposed metrics to approximate the fidelity of different XAI methods. These metrics lack verification and have concerning disagreements. In this study, we proposed a novel methodology to verify fidelity metrics, using a well-known transparent model, namely a decision tree. This model allowed us to obtain explanations with perfect fidelity. Our proposal constitutes the first objective benchmark for these metrics, facilitating a comparison of existing proposals, and surpassing existing methods. We applied our benchmark to assess the existing fidelity metrics in two different experiments, each using public datasets comprising 52,000 images. The images from these datasets had a size a 
&lt;/p&gt;</description></item><item><title>ZnTrack&#26159;&#19968;&#20010;Python&#39537;&#21160;&#30340;&#25968;&#25454;&#29256;&#26412;&#25511;&#21046;&#24037;&#20855;&#65292;&#36890;&#36807;&#31616;&#21270;&#22823;&#22411;&#25968;&#25454;&#38598;&#20026;Python&#33050;&#26412;&#30340;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20195;&#30721;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#21442;&#25968;&#36319;&#36394;&#12289;&#24037;&#20316;&#27969;&#35774;&#35745;&#20197;&#21450;&#25968;&#25454;&#23384;&#20648;&#21644;&#20849;&#20139;&#30340;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2401.10603</link><description>&lt;p&gt;
ZnTrack -- &#25968;&#25454;&#21363;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
ZnTrack -- Data as Code. (arXiv:2401.10603v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10603
&lt;/p&gt;
&lt;p&gt;
ZnTrack&#26159;&#19968;&#20010;Python&#39537;&#21160;&#30340;&#25968;&#25454;&#29256;&#26412;&#25511;&#21046;&#24037;&#20855;&#65292;&#36890;&#36807;&#31616;&#21270;&#22823;&#22411;&#25968;&#25454;&#38598;&#20026;Python&#33050;&#26412;&#30340;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20195;&#30721;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#21442;&#25968;&#36319;&#36394;&#12289;&#24037;&#20316;&#27969;&#35774;&#35745;&#20197;&#21450;&#25968;&#25454;&#23384;&#20648;&#21644;&#20849;&#20139;&#30340;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#31361;&#30772;&#65292;&#32780;&#19988;&#27809;&#26377;&#36857;&#35937;&#34920;&#26126;&#36825;&#31181;&#36235;&#21183;&#20250;&#22312;&#30701;&#26399;&#20869;&#20943;&#24930;&#12290;&#26426;&#22120;&#23398;&#20064;&#12289;&#22823;&#35268;&#27169;&#35745;&#31639;&#36164;&#28304;&#21644;&#22686;&#21152;&#30340;&#34892;&#19994;&#20851;&#27880;&#23548;&#33268;&#20102;&#23545;&#25968;&#25454;&#31649;&#29702;&#12289;&#27169;&#25311;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#35745;&#31639;&#26426;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#30340;&#25237;&#36164;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#38271;&#65292;&#25968;&#25454;&#30340;&#25193;&#22823;&#20063;&#24102;&#26469;&#20102;&#25968;&#25454;&#23384;&#20648;&#12289;&#20849;&#20139;&#21644;&#36861;&#36394;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ZnTrack&#30340;Python&#39537;&#21160;&#25968;&#25454;&#29256;&#26412;&#25511;&#21046;&#24037;&#20855;&#12290;ZnTrack&#22522;&#20110;&#24050;&#24314;&#31435;&#30340;&#29256;&#26412;&#25511;&#21046;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#29992;&#20110;&#36319;&#36394;&#23454;&#39564;&#20013;&#30340;&#21442;&#25968;&#12289;&#35774;&#35745;&#24037;&#20316;&#27969;&#12289;&#20197;&#21450;&#23384;&#20648;&#21644;&#20849;&#20139;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31616;&#21270;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;Python&#33050;&#26412;&#65292;&#20135;&#29983;&#20102;&#8220;&#25968;&#25454;&#21363;&#20195;&#30721;&#8221;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#26412;&#25991;&#25152;&#20171;&#32461;&#30340;&#24037;&#20316;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#20063;&#26159;&#35745;&#31639;&#26102;&#20195;&#32487;&#32493;&#28436;&#21464;&#30340;&#19968;&#20010;&#26080;&#30097;&#37325;&#35201;&#30340;&#27010;&#24565;&#12290;ZnTrack&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25968;&#25454;&#24403;&#20316;&#20195;&#30721;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has seen tremendous breakthroughs in computation and there is no indication that this will slow any time soon. Machine learning, large-scale computing resources, and increased industry focus have resulted in rising investments in computer-driven solutions for data management, simulations, and model generation. However, with this growth in computation has come an even larger expansion of data and with it, complexity in data storage, sharing, and tracking. In this work, we introduce ZnTrack, a Python-driven data versioning tool. ZnTrack builds upon established version control systems to provide a user-friendly and easy-to-use interface for tracking parameters in experiments, designing workflows, and storing and sharing data. From this ability to reduce large datasets to a simple Python script emerges the concept of Data as Code, a core component of the work presented here and an undoubtedly important concept as the age of computation continues to evolve. ZnTrack offers an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#36719;&#20914;&#31361;&#20266;&#24067;&#23572;&#32422;&#26463;&#36716;&#21270;&#20026;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#30340;&#23376;&#21477;&#21152;&#26435;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23376;&#21477;&#21152;&#26435;&#31574;&#30053;&#12290;&#22522;&#20110;&#36825;&#20123;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPB-MaxSAT&#30340;&#26032;&#22411;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#20026;MaxSAT&#23616;&#37096;&#25628;&#32034;&#35299;&#31639;&#22120;&#30340;&#23376;&#21477;&#21152;&#26435;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2401.10589</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;MaxSAT&#23616;&#37096;&#25628;&#32034;&#35299;&#31639;&#22120;&#20013;&#30340;&#36719;&#20914;&#31361;&#20266;&#24067;&#23572;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Soft Conflict Pseudo Boolean Constraint on MaxSAT Local Search Solvers. (arXiv:2401.10589v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#36719;&#20914;&#31361;&#20266;&#24067;&#23572;&#32422;&#26463;&#36716;&#21270;&#20026;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#30340;&#23376;&#21477;&#21152;&#26435;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23376;&#21477;&#21152;&#26435;&#31574;&#30053;&#12290;&#22522;&#20110;&#36825;&#20123;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPB-MaxSAT&#30340;&#26032;&#22411;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#20026;MaxSAT&#23616;&#37096;&#25628;&#32034;&#35299;&#31639;&#22120;&#30340;&#23376;&#21477;&#21152;&#26435;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MaxSAT&#26159;&#33879;&#21517;&#30340;NP&#23436;&#20840;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;SAT&#65289;&#30340;&#20248;&#21270;&#29256;&#26412;&#12290;MaxSAT&#31639;&#27861;&#20027;&#35201;&#21253;&#25324;&#23436;&#20840;&#35299;&#31639;&#22120;&#21644;&#23616;&#37096;&#25628;&#32034;&#19981;&#23436;&#20840;&#35299;&#31639;&#22120;&#12290;&#22312;&#35768;&#22810;&#23436;&#20840;&#35299;&#31639;&#22120;&#20013;&#65292;&#19968;&#26086;&#25214;&#21040;&#26356;&#22909;&#30340;&#35299;&#65292;&#23601;&#20250;&#29983;&#25104;&#19968;&#20010;&#36719;&#20914;&#31361;&#20266;&#24067;&#23572;&#65288;SPB&#65289;&#32422;&#26463;&#65292;&#24378;&#21046;&#31639;&#27861;&#23547;&#25214;&#26356;&#22909;&#30340;&#35299;&#12290;&#22312;&#35768;&#22810;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#20013;&#65292;&#23376;&#21477;&#21152;&#26435;&#26159;&#19968;&#31181;&#26377;&#25928;&#24341;&#23548;&#25628;&#32034;&#26041;&#21521;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;SPB&#32422;&#26463;&#36716;&#21270;&#20026;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#30340;&#23376;&#21477;&#21152;&#26435;&#31995;&#32479;&#65292;&#20351;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#26356;&#22909;&#30340;&#35299;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23376;&#21477;&#21152;&#26435;&#31574;&#30053;&#65292;&#25171;&#30772;&#20102;&#20351;&#29992;&#24120;&#25968;&#20540;&#26469;&#35843;&#25972;&#23376;&#21477;&#26435;&#37325;&#30340;&#20256;&#32479;&#12290;&#22522;&#20110;&#19978;&#36848;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPB-MaxSAT&#30340;&#26032;&#22411;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#20026;MaxSAT&#23616;&#37096;&#25628;&#32034;&#35299;&#31639;&#22120;&#30340;&#23376;&#21477;&#21152;&#26435;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;SPB-MaxSAT&#31639;&#27861;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
MaxSAT is an optimization version of the famous NP-complete Satisfiability problem (SAT). Algorithms for MaxSAT mainly include complete solvers and local search incomplete solvers. In many complete solvers, once a better solution is found, a Soft conflict Pseudo Boolean (SPB) constraint will be generated to enforce the algorithm to find better solutions. In many local search algorithms, clause weighting is a key technique for effectively guiding the search directions. In this paper, we propose to transfer the SPB constraint into the clause weighting system of the local search method, leading the algorithm to better solutions. We further propose an adaptive clause weighting strategy that breaks the tradition of using constant values to adjust clause weights. Based on the above methods, we propose a new local search algorithm called SPB-MaxSAT that provides new perspectives for clause weighting on MaxSAT local search solvers. Extensive experiments demonstrate the excellent performance of
&lt;/p&gt;</description></item><item><title>PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.10586</link><description>&lt;p&gt;
PuriDefense&#65306;&#29992;&#20110;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#38543;&#26426;&#23616;&#37096;&#38544;&#24335;&#23545;&#25239;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10586
&lt;/p&gt;
&lt;p&gt;
PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#31995;&#32479;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#21442;&#25968;&#12290;&#20256;&#32479;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#22914;&#23545;&#25239;&#35757;&#32451;&#12289;&#26799;&#24230;&#25513;&#30422;&#21644;&#36755;&#20837;&#36716;&#25442;&#65292;&#35201;&#20040;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#35201;&#20040;&#25439;&#23475;&#38750;&#23545;&#25239;&#36755;&#20837;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;PuriDefense&#65292;&#22312;&#20302;&#25512;&#29702;&#25104;&#26412;&#30340;&#32423;&#21035;&#19978;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#30340;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#23616;&#37096;&#38544;&#24335;&#20989;&#25968;&#24182;&#37325;&#24314;&#33258;&#28982;&#22270;&#20687;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#38543;&#26426;&#24615;&#32435;&#20837;&#20928;&#21270;&#36807;&#31243;&#26469;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23545;CIFAR-10&#21644;ImageNet&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20928;&#21270;&#22120;&#38450;&#24481;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defen
&lt;/p&gt;</description></item><item><title>CivRealm&#26159;&#19968;&#20010;&#21463;&#25991;&#26126;&#28216;&#25103;&#21551;&#21457;&#30340;&#29615;&#22659;&#65292;&#35201;&#27714;&#20195;&#29702;&#20154;&#22312;&#22797;&#26434;&#30340;&#24773;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#20197;&#24212;&#23545;&#21464;&#21270;&#30340;&#28216;&#25103;&#35268;&#21017;&#21644;&#38543;&#26426;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2401.10568</link><description>&lt;p&gt;
CivRealm: &#19968;&#20010;&#22312;&#25991;&#26126;&#28216;&#25103;&#20013;&#30340;&#23398;&#20064;&#19982;&#25512;&#29702;&#22855;&#24187;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents. (arXiv:2401.10568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10568
&lt;/p&gt;
&lt;p&gt;
CivRealm&#26159;&#19968;&#20010;&#21463;&#25991;&#26126;&#28216;&#25103;&#21551;&#21457;&#30340;&#29615;&#22659;&#65292;&#35201;&#27714;&#20195;&#29702;&#20154;&#22312;&#22797;&#26434;&#30340;&#24773;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#20197;&#24212;&#23545;&#21464;&#21270;&#30340;&#28216;&#25103;&#35268;&#21017;&#21644;&#38543;&#26426;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#20195;&#29702;&#30340;&#27867;&#21270;&#21253;&#25324;&#20004;&#20010;&#22522;&#26412;&#20803;&#32032;&#65306;&#20174;&#36807;&#21435;&#32463;&#39564;&#20013;&#23398;&#20064;&#21644;&#22312;&#26032;&#24773;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#20132;&#20114;&#29615;&#22659;&#20013;&#65292;&#23545;&#23398;&#20064;&#30340;&#37325;&#35270;&#24448;&#24448;&#20197;&#29306;&#29298;&#25512;&#29702;&#22797;&#26434;&#24615;&#20026;&#20195;&#20215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CivRealm&#65292;&#19968;&#20010;&#21463;&#21040;&#25991;&#26126;&#28216;&#25103;&#21551;&#21457;&#30340;&#29615;&#22659;&#12290;&#25991;&#26126;&#28216;&#25103;&#19982;&#20154;&#31867;&#21382;&#21490;&#21644;&#31038;&#20250;&#30340;&#28145;&#21051;&#22865;&#21512;&#38656;&#35201;&#22797;&#26434;&#30340;&#23398;&#20064;&#65292;&#32780;&#20854;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#22659;&#21017;&#35201;&#27714;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;CivRealm&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#30528;&#19981;&#23436;&#20840;&#20449;&#24687;&#21644;&#21464;&#21270;&#20154;&#25968;&#30340;&#24635;&#21644;&#28216;&#25103;&#65307;&#23427;&#21576;&#29616;&#20102;&#20247;&#22810;&#22797;&#26434;&#30340;&#29305;&#24449;&#65292;&#25361;&#25112;&#20195;&#29702;&#20154;&#22788;&#29702;&#24320;&#25918;&#30340;&#12289;&#38543;&#26426;&#30340;&#29615;&#22659;&#65292;&#38656;&#35201;&#22806;&#20132;&#21644;&#35848;&#21028;&#25216;&#24039;&#12290;&#22312;CivRealm&#20013;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#20856;&#22411;&#30340;&#20195;&#29702;&#31867;&#22411;&#25552;&#20379;&#20102;&#25509;&#21475;&#65306;&#22522;&#20110;&#24352;&#37327;&#30340;&#23398;&#20064;&#22411;&#20195;&#29702;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#25512;&#29702;&#22411;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization's profound alignment with human history and society necessitates sophisticated learning, while its ever-changing situations demand strong reasoning to generalize. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further rese
&lt;/p&gt;</description></item><item><title>OrchMoE&#36890;&#36807;&#21033;&#29992;&#27169;&#22359;&#21270;&#25216;&#33021;&#26550;&#26500;&#21644;&#33258;&#21160;&#20219;&#21153;&#35782;&#21035;&#65292;&#25552;&#21319;&#20102;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10559</link><description>&lt;p&gt;
OrchMoE&#65306;&#20855;&#26377;&#20219;&#21153;-&#25216;&#33021;&#21327;&#21516;&#25928;&#24212;&#30340;&#39640;&#25928;&#22810;&#36866;&#37197;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy. (arXiv:2401.10559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10559
&lt;/p&gt;
&lt;p&gt;
OrchMoE&#36890;&#36807;&#21033;&#29992;&#27169;&#22359;&#21270;&#25216;&#33021;&#26550;&#26500;&#21644;&#33258;&#21160;&#20219;&#21153;&#35782;&#21035;&#65292;&#25552;&#21319;&#20102;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#21019;&#26032;&#30340;&#22810;&#36866;&#37197;&#22120;&#26041;&#27861;OrchMoE&#25512;&#36827;&#20102;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#65288;PEFT&#65289;&#39046;&#22495;&#65292;&#21033;&#29992;&#27169;&#22359;&#21270;&#25216;&#33021;&#26550;&#26500;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#20256;&#36882;&#12290;&#19982;&#20381;&#36182;&#26174;&#24335;&#20219;&#21153;&#35782;&#21035;&#36755;&#20837;&#30340;&#20808;&#21069;&#27169;&#22411;&#19981;&#21516;&#65292;OrchMoE&#33258;&#21160;&#35782;&#21035;&#20219;&#21153;&#31867;&#21035;&#65292;&#31616;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#25972;&#21512;&#26426;&#21046;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#33258;&#21160;&#20219;&#21153;&#20998;&#31867;&#27169;&#22359;&#21644;&#20219;&#21153;-&#25216;&#33021;&#20998;&#37197;&#27169;&#22359;&#65292;&#20849;&#21516;&#25512;&#26029;&#20219;&#21153;&#29305;&#23450;&#30340;&#20998;&#31867;&#24182;&#35843;&#25972;&#25216;&#33021;&#20998;&#37197;&#30697;&#38453;&#12290;&#25105;&#20204;&#22312;&#8220;&#36229;&#33258;&#28982;&#25351;&#20196;&#8221;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;1,600&#20010;&#22810;&#26679;&#30340;&#25351;&#20196;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;OrchMoE&#22312;&#24615;&#33021;&#21644;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#21487;&#27604;&#30340;&#22810;&#36866;&#37197;&#22120;&#22522;&#32447;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#21442;&#25968;&#38480;&#21046;&#19979;&#36816;&#34892;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;OrchMoE&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel multi-adapter method, OrchMoE, which capitalizes on modular skill architecture for enhanced forward transfer in neural networks. Unlike prior models that depend on explicit task identification inputs, OrchMoE automatically discerns task categories, streamlining the learning process. This is achieved through an integrated mechanism comprising an Automatic Task Classification module and a Task-Skill Allocation module, which collectively deduce task-specific classifications and tailor skill allocation matrices. Our extensive evaluations on the 'Super Natural Instructions' dataset, featuring 1,600 diverse instructional tasks, indicate that OrchMoE substantially outperforms comparable multi-adapter baselines in terms of both performance and sample utilization efficiency, all while operating within the same parameter constraints. These findings suggest that OrchMoE offers a significant leap forward in multi-task le
&lt;/p&gt;</description></item><item><title>AAT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adapter&#24494;&#35843;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#29306;&#29298;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2401.10544</link><description>&lt;p&gt;
AAT: &#36866;&#24212;&#19981;&#21516;&#22768;&#23398;&#35782;&#21035;&#20219;&#21153;&#30340;&#38899;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
AAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks. (arXiv:2401.10544v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10544
&lt;/p&gt;
&lt;p&gt;
AAT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adapter&#24494;&#35843;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#29306;&#29298;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Transformer&#34987;&#24341;&#20837;&#21040;&#22768;&#23398;&#35782;&#21035;&#39046;&#22495;&#12290;&#23427;&#20204;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#31283;&#20581;&#30340;&#36890;&#29992;&#24615; - &#23427;&#23481;&#26131;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#24182;&#26174;&#31034;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20027;&#35201;&#20351;&#29992;&#30340;&#24494;&#35843;&#26041;&#27861;&#20173;&#28982;&#26159;&#23436;&#20840;&#24494;&#35843;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#26032;&#25152;&#26377;&#21442;&#25968;&#12290;&#36825;&#19981;&#20165;&#23548;&#33268;&#26174;&#33879;&#30340;&#20869;&#23384;&#20351;&#29992;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#36824;&#25439;&#23475;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#35201;&#20040;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35201;&#20040;&#26080;&#27861;&#36798;&#21040;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adapter&#24494;&#35843;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21363;AAT&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#20923;&#32467;&#38899;&#39057;Transformer&#27169;&#22411;&#24182;&#25554;&#20837;&#39069;&#22806;&#30340;&#21487;&#23398;&#20064;Adapter&#65292;&#20197;&#39640;&#25928;&#33719;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#32780;&#19981;&#25439;&#23475;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformers have been introduced into the field of acoustics recognition. They are pre-trained on large-scale datasets using methods such as supervised learning and semi-supervised learning, demonstrating robust generality--It fine-tunes easily to downstream tasks and shows more robust performance. However, the predominant fine-tuning method currently used is still full fine-tuning, which involves updating all parameters during training. This not only incurs significant memory usage and time costs but also compromises the model's generality. Other fine-tuning methods either struggle to address this issue or fail to achieve matching performance. Therefore, we conducted a comprehensive analysis of existing fine-tuning methods and proposed an efficient fine-tuning approach based on Adapter tuning, namely AAT. The core idea is to freeze the audio Transformer model and insert extra learnable Adapters, efficiently acquiring downstream task knowledge without compromising the model'
&lt;/p&gt;</description></item><item><title>Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;</title><link>http://arxiv.org/abs/2401.10529</link><description>&lt;p&gt;
Mementos: &#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10529
&lt;/p&gt;
&lt;p&gt;
Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#39640;&#36229;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;MLLM&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#21333;&#20010;&#22270;&#20687;&#30340;&#38745;&#24577;&#20449;&#24687;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#29616;&#20195;MLLM&#22312;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#36827;&#34892;&#25512;&#26029;&#30340;&#33021;&#21147;&#65292;&#22312;&#29702;&#35299;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#36739;&#23569;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Mementos&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#30340;&#24207;&#21015;&#22270;&#20687;&#25512;&#29702;&#33021;&#21147;&#12290;Mementos&#21253;&#25324;4761&#20010;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#22810;&#26679;&#30340;&#22270;&#20687;&#24207;&#21015;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;GPT-4&#36741;&#21161;&#26041;&#27861;&#26469;&#35780;&#20272;MLLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;Mementos&#20013;&#21253;&#25324;GPT-4V&#21644;Gemini&#22312;&#20869;&#30340;&#20061;&#20010;&#26368;&#26032;MLLM&#36827;&#34892;&#20180;&#32454;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#20934;&#30830;&#25551;&#36848;&#25152;&#32473;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24448;&#24448;&#23548;&#33268;&#23545;&#35937;&#21450;&#20854;&#23545;&#24212;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#32534;&#36753;&#20107;&#23454;&#24182;&#35266;&#23519;&#20854;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#26356;&#26032;&#20256;&#25773;&#65292;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.10521</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#35821;&#35328;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Editing in Multilingual Language Models. (arXiv:2401.10521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#32534;&#36753;&#20107;&#23454;&#24182;&#35266;&#23519;&#20854;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#26356;&#26032;&#20256;&#25773;&#65292;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#32780;&#26356;&#26032;&#36807;&#26102;&#30340;LLM&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#36164;&#28304;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#35768;&#22810;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#20197;&#20415;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#26356;&#26032;&#27169;&#22411;&#36755;&#20986;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;LLM&#20013;&#65292;&#20854;&#20013;&#30340;&#30693;&#35782;&#20197;&#22810;&#31181;&#35821;&#35328;&#23384;&#20648;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#22312;&#35813;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20107;&#23454;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#34987;&#32534;&#36753;&#65292;&#35266;&#23519;&#20854;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26356;&#26032;&#20256;&#25773;&#12290;&#20026;&#20102;&#30740;&#31350;XME&#33539;&#24335;&#65292;&#25105;&#20204;&#20351;&#29992;BLOOM&#12289;mBERT&#21644;XLM-RoBERTa&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#20889;&#20316;&#33050;&#26412;&#65292;&#21363;&#25289;&#19969;&#35821;&#65288;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#21644;&#21360;&#22320;&#35821;&#65288;&#21360;&#22320;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;XME&#35774;&#32622;&#19979;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;MET&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#24403;&#28041;&#21450;&#30340;&#35821;&#35328;&#23646;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#35821;&#26063;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (\textbf{XME}) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: \textit{Latin} (English, French, and Spanish) and \textit{Indic} (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25193;&#23637;&#30340;&#29366;&#24577;-&#22870;&#21169;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#21892;DRL&#20013;&#29366;&#24577;&#19982;&#22870;&#21169;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20540;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#31574;&#30053;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10516</link><description>&lt;p&gt;
&#25193;&#23637;&#29366;&#24577;-&#22870;&#21169;&#31354;&#38388;&#30340;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Episodic Reinforcement Learning with Expanded State-reward Space. (arXiv:2401.10516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10516
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25193;&#23637;&#30340;&#29366;&#24577;-&#22870;&#21169;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#21892;DRL&#20013;&#29366;&#24577;&#19982;&#22870;&#21169;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20540;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#31574;&#30053;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21147;&#37327;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#28216;&#25103;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#30001;&#20110;&#26377;&#25928;&#31574;&#30053;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#26679;&#26412;&#65292;DRL&#20173;&#34987;&#35748;&#20026;&#26159;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#30340;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#24773;&#33410;&#25511;&#21046;&#65288;EC&#65289;&#30340;&#26080;&#27169;&#22411;DRL&#26041;&#27861;&#36890;&#36807;&#20174;&#24773;&#33410;&#35760;&#24518;&#20013;&#22238;&#39038;&#36807;&#21435;&#30340;&#32463;&#39564;&#23454;&#29616;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;EC&#30340;&#26041;&#27861;&#30001;&#20110;&#24573;&#30053;&#20102;&#21033;&#29992;&#65288;&#36807;&#21435;&#30340;&#65289;&#26816;&#32034;&#29366;&#24577;&#30340;&#24191;&#27867;&#20449;&#24687;&#65292;&#23384;&#22312;&#29366;&#24577;&#21644;&#22870;&#21169;&#31354;&#38388;&#20043;&#38388;&#30340;&#28508;&#22312;&#19981;&#23545;&#40784;&#30340;&#38480;&#21046;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20540;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#31574;&#30053;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25193;&#23637;&#29366;&#24577;-&#22870;&#21169;&#31354;&#38388;&#30340;&#39640;&#25928;EC&#22411;DRL&#26694;&#26550;&#65292;&#20854;&#20013;&#29992;&#20316;&#36755;&#20837;&#30340;&#25193;&#23637;&#29366;&#24577;&#21644;&#29992;&#20110;&#35757;&#32451;&#30340;&#25193;&#23637;&#22870;&#21169;&#37117;&#21253;&#21547;&#21382;&#21490;&#21644;&#24403;&#21069;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empowered by deep neural networks, deep reinforcement learning (DRL) has demonstrated tremendous empirical successes in various domains, including games, health care, and autonomous driving. Despite these advancements, DRL is still identified as data-inefficient as effective policies demand vast numbers of environmental samples. Recently, episodic control (EC)-based model-free DRL methods enable sample efficiency by recalling past experiences from episodic memory. However, existing EC-based methods suffer from the limitation of potential misalignment between the state and reward spaces for neglecting the utilization of (past) retrieval states with extensive information, which probably causes inaccurate value estimation and degraded policy performance. To tackle this issue, we introduce an efficient EC-based DRL framework with expanded state-reward space, where the expanded states used as the input and the expanded rewards used in the training both contain historical and current informa
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10510</link><description>&lt;p&gt;
&#22825;&#20316;&#20043;&#21512;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10510
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21019;&#36896;&#24615;&#30340;&#33258;&#28982;&#25991;&#26412;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21487;&#20197;&#21457;&#29616;&#22797;&#26434;&#23454;&#38469;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25991;&#26412;&#24207;&#21015;&#29983;&#25104;&#21644;&#36827;&#21270;&#30340;&#20849;&#21516;&#29305;&#28857;&#21644;&#26041;&#21521;&#24615;&#65292;&#38416;&#36848;&#20102;LLMs&#19982;EAs&#20043;&#38388;&#30340;&#24378;&#22823;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#19968;&#23545;&#19968;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#22312;&#36825;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;&#19979;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#21253;&#25324;&#36827;&#21270;&#24494;&#35843;&#21644;LLM&#22686;&#24378;&#22411;EAs&#12290;&#20511;&#21161;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#22312;LLMs&#21644;EAs&#32806;&#21512;&#26041;&#38754;&#30340;&#22522;&#26412;&#30740;&#31350;&#36335;&#32447;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#37329;&#34701;&#20998;&#26512;&#30340;&#22522;&#20110;LLMs&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;FinSQL&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#37329;&#34701;&#20998;&#26512;&#25991;&#26412;&#21040;SQL&#22522;&#20934;&#25968;&#25454;&#38598;BULL&#65292;&#20174;&#25552;&#31034;&#26500;&#36896;&#21644;&#21442;&#25968;&#21270;&#30340;&#35282;&#24230;&#20026;&#37329;&#34701;&#25991;&#26412;&#21040;SQL&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.10506</link><description>&lt;p&gt;
FinSQL&#65306;&#38754;&#21521;&#37329;&#34701;&#20998;&#26512;&#30340;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis. (arXiv:2401.10506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#37329;&#34701;&#20998;&#26512;&#30340;&#22522;&#20110;LLMs&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;FinSQL&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#37329;&#34701;&#20998;&#26512;&#25991;&#26412;&#21040;SQL&#22522;&#20934;&#25968;&#25454;&#38598;BULL&#65292;&#20174;&#25552;&#31034;&#26500;&#36896;&#21644;&#21442;&#25968;&#21270;&#30340;&#35282;&#24230;&#20026;&#37329;&#34701;&#25991;&#26412;&#21040;SQL&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#25991;&#26412;&#21040;SQL&#30340;&#39046;&#22495;&#65292;&#35813;&#39046;&#22495;&#20026;&#25805;&#20316;&#20851;&#31995;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#26080;&#20195;&#30721;&#25509;&#21475;&#65292;&#22312;&#37329;&#34701;&#20998;&#26512;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65307;&#22240;&#20026;&#37329;&#34701;&#19987;&#19994;&#20154;&#21592;&#21487;&#33021;&#19981;&#25797;&#38271;SQL&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#20026;&#37329;&#34701;&#20998;&#26512;&#25552;&#20379;&#23454;&#29992;&#30340;&#25991;&#26412;&#21040;SQL&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;SQL&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#37329;&#34701;&#24212;&#29992;&#20013;&#25968;&#25454;&#24211;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#22914;&#36890;&#24120;&#23384;&#22312;&#30340;&#23485;&#34920;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#37329;&#34701;&#20998;&#26512;&#25991;&#26412;&#21040;SQL&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37329;&#34701;&#20998;&#26512;&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#12290;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;BULL&#26159;&#20174;&#24658;&#29983;&#31185;&#25216;&#20844;&#21496;&#30340;&#23454;&#38469;&#37329;&#34701;&#20998;&#26512;&#19994;&#21153;&#20013;&#25910;&#38598;&#30340;&#65292;&#21253;&#25324;&#22522;&#37329;&#12289;&#32929;&#31080;&#21644;&#23439;&#35266;&#32463;&#27982;&#30340;&#25968;&#25454;&#24211;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25552;&#20986;&#30340;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;FinSQL&#20174;&#25552;&#31034;&#26500;&#36896;&#21644;&#21442;&#25968;&#21270;&#30340;&#35282;&#24230;&#20026;&#37329;&#34701;&#25991;&#26412;&#21040;SQL&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because, financial professionals may not well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, paramete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#29109;Oracle&#26469;&#24674;&#22797;&#22270;&#23618;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;&#31639;&#27861;&#36890;&#36807;&#27604;&#36739;&#33410;&#28857;&#30340;&#26465;&#20214;&#29109;&#21644;&#22122;&#22768;&#30340;&#26080;&#26465;&#20214;&#29109;&#65292;&#36890;&#36807;&#21024;&#38500;&#28304;&#25110;&#27719;&#28857;&#26469;&#23454;&#29616;&#33410;&#28857;&#30340;&#20998;&#31163;&#12290;&#31639;&#27861;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#27491;&#30830;&#24615;&#21644;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10495</link><description>&lt;p&gt;
&#26465;&#20214;&#29109;&#19979;&#30340;&#22240;&#26524;&#23618;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Layering via Conditional Entropy. (arXiv:2401.10495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#29109;Oracle&#26469;&#24674;&#22797;&#22270;&#23618;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;&#31639;&#27861;&#36890;&#36807;&#27604;&#36739;&#33410;&#28857;&#30340;&#26465;&#20214;&#29109;&#21644;&#22122;&#22768;&#30340;&#26080;&#26465;&#20214;&#29109;&#65292;&#36890;&#36807;&#21024;&#38500;&#28304;&#25110;&#27719;&#28857;&#26469;&#23454;&#29616;&#33410;&#28857;&#30340;&#20998;&#31163;&#12290;&#31639;&#27861;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#27491;&#30830;&#24615;&#21644;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26088;&#22312;&#20174;&#21487;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#20851;&#20110;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#22270;&#30340;&#20449;&#24687;&#12290;&#23618;&#26512;&#26159;&#23545;&#21464;&#37327;&#36827;&#34892;&#25490;&#24207;&#65292;&#23558;&#22240;&#26524;&#25918;&#22312;&#25928;&#24212;&#20043;&#21069;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36890;&#36807;&#35775;&#38382;&#26465;&#20214;&#29109;Oracle&#26469;&#24674;&#22797;&#22270;&#23618;&#26512;&#30340;&#26041;&#27861;&#65292;&#24403;&#20998;&#24067;&#26159;&#31163;&#25955;&#30340;&#26102;&#20505;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#19981;&#26029;&#20174;&#22270;&#20013;&#21024;&#38500;&#28304;&#25110;&#27719;&#28857;&#26469;&#24037;&#20316;&#12290;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#30340;&#26465;&#20214;&#29109;&#19982;&#22122;&#22768;&#30340;&#26080;&#26465;&#20214;&#29109;&#26469;&#23558;&#28304;&#25110;&#27719;&#28857;&#19982;&#20854;&#20313;&#33410;&#28857;&#20998;&#31163;&#24320;&#26469;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#20108;&#27425;&#30340;&#65292;&#24182;&#19988;&#32463;&#36807;&#35777;&#26126;&#26159;&#27491;&#30830;&#30340;&#12290;&#20027;&#35201;&#30340;&#20551;&#35774;&#26159;&#24544;&#23454;&#24615;&#21644;&#21333;&#23556;&#22122;&#22768;&#65292;&#20197;&#21450;&#24050;&#30693;&#22122;&#22768;&#29109;&#25110;&#27839;&#30528;&#26377;&#21521;&#36335;&#24452;&#24369;&#21333;&#35843;&#36882;&#22686;&#30340;&#22122;&#22768;&#29109;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38656;&#35201;&#24544;&#23454;&#24615;&#30340;&#19968;&#20010;&#38750;&#24120;&#28201;&#21644;&#30340;&#25193;&#23637;&#65292;&#25110;&#32773;&#20005;&#26684;&#21333;&#35843;&#36882;&#22686;&#30340;&#22122;&#22768;&#29109;&#65292;&#25110;&#32773;&#25193;&#23637;&#21040;&#26080;&#38480;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery aims to recover information about an unobserved causal graph from the observable data it generates. Layerings are orderings of the variables which place causes before effects. In this paper, we provide ways to recover layerings of a graph by accessing the data via a conditional entropy oracle, when distributions are discrete. Our algorithms work by repeatedly removing sources or sinks from the graph. Under appropriate assumptions and conditioning, we can separate the sources or sinks from the remainder of the nodes by comparing their conditional entropy to the unconditional entropy of their noise. Our algorithms are provably correct and run in worst-case quadratic time. The main assumptions are faithfulness and injective noise, and either known noise entropies or weakly monotonically increasing noise entropies along directed paths. In addition, we require one of either a very mild extension of faithfulness, or strictly monotonically increasing noise entropies, or expan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#21644;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38477;&#20302;&#21151;&#32791;&#21644;&#27169;&#22411;&#23610;&#23544;&#65292;&#24182;&#23454;&#29616;&#39640;&#36798;66.67%&#30340;GPU&#35745;&#31639;&#21151;&#29575;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#39318;&#27425;&#24212;&#29992;&#20102;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#20110;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#23545;&#35813;&#39046;&#22495;&#20316;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.10484</link><description>&lt;p&gt;
&#22522;&#20110;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#25216;&#26415;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Scalability in Recommender Systems through Lottery Ticket Hypothesis and Knowledge Distillation-based Neural Network Pruning. (arXiv:2401.10484v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#21644;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38477;&#20302;&#21151;&#32791;&#21644;&#27169;&#22411;&#23610;&#23544;&#65292;&#24182;&#23454;&#29616;&#39640;&#36798;66.67%&#30340;GPU&#35745;&#31639;&#21151;&#29575;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#39318;&#27425;&#24212;&#29992;&#20102;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#20110;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#23545;&#35813;&#39046;&#22495;&#20316;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#39640;&#25928;&#22320;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#65288;LTH&#65289;&#19982;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#21098;&#26525;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#21098;&#26525;&#25216;&#26415;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#21151;&#32791;&#21644;&#27169;&#22411;&#23610;&#23544;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23545;&#20004;&#20010;&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#20196;&#20154;&#28385;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GPU&#35745;&#31639;&#21151;&#29575;&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;66.67%&#30340;&#38477;&#20302;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#39318;&#27425;&#24212;&#29992;LTH&#21644;KD&#25216;&#26415;&#65292;&#23545;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces an innovative approach aimed at the efficient pruning of neural networks, with a particular focus on their deployment on edge devices. Our method involves the integration of the Lottery Ticket Hypothesis (LTH) with the Knowledge Distillation (KD) framework, resulting in the formulation of three distinct pruning models. These models have been developed to address scalability issue in recommender systems, whereby the complexities of deep learning models have hindered their practical deployment. With judicious application of the pruning techniques, we effectively curtail the power consumption and model dimensions without compromising on accuracy. Empirical evaluation has been performed using two real world datasets from diverse domains against two baselines. Gratifyingly, our approaches yielded a GPU computation-power reduction of up to 66.67%. Notably, our study contributes to the field of recommendation system by pioneering the application of LTH and KD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25552;&#21069;&#20572;&#27490;&#33258;&#19968;&#33268;&#24615;&#65288;ESC&#65289;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#38477;&#20302;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#33258;&#19968;&#33268;&#24615;&#30340;&#25104;&#26412;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10480</link><description>&lt;p&gt;
&#36867;&#31163;&#39640;&#26114;&#25104;&#26412;&#65306;&#22810;&#27493;&#25512;&#29702;&#30340;&#25552;&#21069;&#20572;&#27490;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning. (arXiv:2401.10480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25552;&#21069;&#20572;&#27490;&#33258;&#19968;&#33268;&#24615;&#65288;ESC&#65289;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#38477;&#20302;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#33258;&#19968;&#33268;&#24615;&#30340;&#25104;&#26412;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#39640;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#27425;&#39044;&#35774;&#22823;&#23567;&#30340;&#25277;&#26679;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#36807;&#31243;&#8212;&#8212;&#25552;&#21069;&#20572;&#27490;&#33258;&#19968;&#33268;&#24615;&#65288;ESC&#65289;&#65292;&#20197;&#26497;&#22823;&#38477;&#20302;&#33258;&#19968;&#33268;&#24615;&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;&#19968;&#31181;&#25511;&#21046;&#26041;&#26696;&#65292;&#21487;&#20197;&#21160;&#24577;&#36873;&#25321;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#25104;&#26412;&#24179;&#34913;&#12290;&#20026;&#20102;&#35777;&#26126;ESC&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#25512;&#29702;&#20219;&#21153;&#31867;&#21035;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65306;&#31639;&#26415;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#31526;&#21495;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21253;&#25324;MATH&#22312;&#20869;&#30340;&#20845;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ESC&#23558;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24179;&#22343;&#25277;&#26679;&#27425;&#25968;&#26174;&#33879;&#20943;&#23569;&#20102;&#65288;-33.8%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-consistency (SC) has been a widely used decoding strategy for chain-of-thought reasoning. Despite bringing significant performance improvements across a variety of multi-step reasoning tasks, it is a high-cost method that requires multiple sampling with the preset size. In this paper, we propose a simple and scalable sampling process, \textbf{E}arly-Stopping \textbf{S}elf-\textbf{C}onsistency (ESC), to greatly reduce the cost of SC without sacrificing performance. On this basis, one control scheme for ESC is further derivated to dynamically choose the performance-cost balance for different tasks and models. To demonstrate ESC's effectiveness, we conducted extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning over language models with varying scales. The empirical results show that ESC reduces the average number of sampling of chain-of-thought reasoning by a significant margin on six benchmarks, including MATH (-33.8%),
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;LDReg&#30340;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;LDReg&#33021;&#22815;&#25913;&#21892;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10474</link><description>&lt;p&gt;
LDReg: &#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LDReg: Local Dimensionality Regularized Self-Supervised Learning. (arXiv:2401.10474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;LDReg&#30340;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;LDReg&#33021;&#22815;&#25913;&#21892;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23398;&#20064;&#30340;&#34920;&#31034;&#21487;&#33021;&#23481;&#26131;&#20986;&#29616;&#32500;&#24230;&#22349;&#32553;&#65292;&#20854;&#20013;&#23398;&#20064;&#30340;&#34920;&#31034;&#23376;&#31354;&#38388;&#32500;&#24230;&#26497;&#20302;&#65292;&#22240;&#27492;&#26080;&#27861;&#34920;&#31034;&#23436;&#25972;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#24577;&#12290;&#32500;&#24230;&#22349;&#32553;&#20063;&#34987;&#31216;&#20026;&#8220;&#22635;&#20805;&#19981;&#36275;&#8221;&#29616;&#35937;&#65292;&#26159;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#30740;&#31350;&#20102;SSL&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#34920;&#31034;&#21487;&#20197;&#22312;&#20840;&#23616;&#19978;&#35206;&#30422;&#39640;&#32500;&#31354;&#38388;&#65292;&#20294;&#22312;&#23616;&#37096;&#19978;&#20250;&#22349;&#32553;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#65288;LDReg&#65289;&#8221;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#26159;&#22522;&#20110;Fisher-Rao&#24230;&#37327;&#30340;&#25512;&#23548;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#20248;&#21270;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#28176;&#36827;&#23567;&#21322;&#24452;&#22788;&#30340;&#23616;&#37096;&#36317;&#31163;&#20998;&#24067;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;LDReg&#21487;&#20197;&#25913;&#21892;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse also known as the "underfilling" phenomenon is one of the major causes of degraded performance on downstream tasks. Previous work has investigated the dimensional collapse problem of SSL at a global level. In this paper, we demonstrate that representations can span over high dimensional space globally, but collapse locally. To address this, we propose a method called $\textit{local dimensionality regularization (LDReg)}$. Our formulation is based on the derivation of the Fisher-Rao metric to compare and optimize local distance distributions at an asymptotically small radius for each data point. By increasing the local intrinsic dimensionality, we demonstrate through a range of experiments that LDReg improves the repres
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10471</link><description>&lt;p&gt;
DeepEdit: &#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#24335;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10471
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#35270;&#20026;&#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeepEdit&#65288;&#22522;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#28176;&#36827;&#24335;&#35299;&#30721;&#30693;&#35782;&#32534;&#36753;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12289;&#38382;&#39064;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#26469;&#25913;&#36827;&#30693;&#35782;&#32534;&#36753;&#12290;DeepEdit&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#25152;&#26377;&#40657;&#30418;LLMs&#65306;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#34920;&#31034;&#25110;&#36755;&#20986;&#35789;&#27719;&#20998;&#24067;&#12290;DeepEdit&#36880;&#27493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#20462;&#25913;LLMs&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#36755;&#20986;&#23545;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#12290;&#22312;&#30693;&#35782;&#32534;&#36753;&#26041;&#38754;&#65292;DeepEdit&#22312;&#25511;&#21046;LLMs&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;MQuaKE&#19978;&#65292;DeepEdit&#22312;&#23450;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#24182;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#65292;&#21462;&#24471;&#20102;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.10467</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Learning Backdoors for Mixed Integer Programs with Contrastive Learning. (arXiv:2401.10467v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#24182;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#65292;&#21462;&#24471;&#20102;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#24182;&#20351;&#29992;&#20998;&#25903;&#23450;&#30028;&#26041;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#23384;&#22312;MIP&#21518;&#38376;&#65292;&#21363;&#19968;&#23567;&#32452;&#21464;&#37327;&#65292;&#22914;&#26524;&#20248;&#20808;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#22312;&#23427;&#20204;&#19978;&#36827;&#34892;&#20998;&#25903;&#65292;&#21017;&#21487;&#20197;&#21152;&#24555;&#36816;&#34892;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#33021;&#25552;&#39640;&#36816;&#34892;&#26102;&#38388;&#30340;&#39640;&#36136;&#37327;&#21518;&#38376;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#25490;&#21517;&#23398;&#20064;&#20272;&#35745;&#38543;&#26426;&#37319;&#26679;&#30340;&#21518;&#38376;&#30456;&#23545;&#27714;&#35299;&#22120;&#36895;&#24230;&#65292;&#28982;&#21518;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26041;&#27861;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#38543;&#26426;&#37319;&#26679;&#65292;&#24182;&#19988;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#24120;&#35265;&#30340;MIP&#38382;&#39064;&#39046;&#22495;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#29616;&#20986;&#23545;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world problems can be efficiently modeled as Mixed Integer Programs (MIPs) and solved with the Branch-and-Bound method. Prior work has shown the existence of MIP backdoors, small sets of variables such that prioritizing branching on them when possible leads to faster running times. However, finding high-quality backdoors that improve running times remains an open question. Previous work learns to estimate the relative solver speed of randomly sampled backdoors through ranking and then decide whether to use it. In this paper, we utilize the Monte-Carlo tree search method to collect backdoors for training, rather than relying on random sampling, and adapt a contrastive learning framework to train a Graph Attention Network model to predict backdoors. Our method, evaluated on four common MIP problem domains, demonstrates performance improvements over both Gurobi and previous models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10463</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#36825;&#26159;&#20174;&#24555;&#36895;&#35760;&#24518;&#21040;&#32531;&#24930;&#27867;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#36716;&#21464;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30456;&#21464;&#24418;&#24335;&#21270;&#20026;Grokking&#37197;&#32622;&#19979;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#65292;&#24182;&#30830;&#23450;&#20102;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#20805;&#36275;&#21644;&#36807;&#21097;&#38454;&#27573;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21021;&#22987;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;Grokking&#37197;&#32622;&#65292;&#31283;&#23450;&#22320;&#22312;&#31616;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#37325;&#29616;&#20102;Grokking&#12290;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26679;&#26412;&#32423;&#21644;&#27169;&#22411;&#32423;&#30340;Grokking&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#25968;&#25454;&#38598;&#22823;&#23567;&#22788;&#21457;&#29983;&#30340;&#26356;&#24179;&#28369;&#30340;&#30456;&#21464;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#20020;&#30028;&#28857;&#20063;&#21464;&#24471;&#26356;&#22823;&#65292;&#36825;&#34920;&#26126;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21152;&#28145;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#29702;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#20302;&#31209;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;LoRA&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#30340;&#38477;&#20302;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#32423;LoRA&#21464;&#20307;&#23548;&#33268;&#20102;&#26576;&#20123;&#25200;&#21160;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2401.10447</link><description>&lt;p&gt;
&#35821;&#38899;&#35782;&#21035;&#20013;&#20302;&#31209;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition. (arXiv:2401.10447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#20302;&#31209;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;LoRA&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#30340;&#38477;&#20302;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#32423;LoRA&#21464;&#20307;&#23548;&#33268;&#20102;&#26576;&#20123;&#25200;&#21160;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36164;&#28304;&#26377;&#38480;&#30340;&#30828;&#20214;&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#19982;&#20923;&#32467;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20027;&#27969;&#12289;&#36164;&#28304;&#39640;&#25928;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;LoRA&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#20844;&#24320;&#30340;Librispeech&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;3.50&#65285;&#65292;&#22312;&#28040;&#24687;&#39046;&#22495;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;3.67&#65285;&#30340;&#38477;&#20302;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35780;&#20272;&#22522;&#20110;LoRA&#30340;&#20108;&#27425;&#20256;&#36882;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#25200;&#21160;&#28304;&#20110;&#21516;&#38899;&#23383;&#26367;&#20195;&#21644;&#19968;&#31181;&#21517;&#20026;N-best Perturbation-based Rescoring Robustness&#65288;NPRR&#65289;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#29992;&#20110;&#34913;&#37327;&#37325;&#35780;&#20998;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#23545;&#38477;&#35299;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;LoRA&#30340;&#39640;&#32423;&#21464;&#20307;&#65288;&#20363;&#22914;&#21160;&#24577;&#31209;&#20998;&#37197;&#30340;LoRA&#65289;&#23548;&#33268;&#20102;$1$-best&#25200;&#21160;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of low-rank adaptation (LoRA) with frozen pretrained language models (PLMs) has become increasing popular as a mainstream, resource-efficient modeling approach for memory-constrained hardware. In this study, we first explore how to enhance model performance by introducing various LoRA training strategies, achieving relative word error rate reductions of 3.50\% on the public Librispeech dataset and of 3.67\% on an internal dataset in the messaging domain. To further characterize the stability of LoRA-based second-pass speech recognition models, we examine robustness against input perturbations. These perturbations are rooted in homophone replacements and a novel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both designed to measure the relative degradation in the performance of rescoring models. Our experimental results indicate that while advanced variants of LoRA, such as dynamic rank-allocated LoRA, lead to performance degradation in $1$-best perturbati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20449;&#24687;&#20316;&#20026;&#26465;&#20214;&#22120;&#65292;&#24182;&#20174;N-best&#21015;&#34920;&#20013;&#25552;&#21462;&#35821;&#35328;&#31354;&#38388;&#22122;&#22768;&#23884;&#20837;&#65292;&#25945;&#20250;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22122;&#22768;&#21435;&#38500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#12290;</title><link>http://arxiv.org/abs/2401.10446</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#30340;&#39640;&#25928;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Efficient Learners of Noise-Robust Speech Recognition. (arXiv:2401.10446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20449;&#24687;&#20316;&#20026;&#26465;&#20214;&#22120;&#65292;&#24182;&#20174;N-best&#21015;&#34920;&#20013;&#25552;&#21462;&#35821;&#35328;&#31354;&#38388;&#22122;&#22768;&#23884;&#20837;&#65292;&#25945;&#20250;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22122;&#22768;&#21435;&#38500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#65292;&#21033;&#29992;LLMs&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#25913;&#21892;&#35782;&#21035;&#32467;&#26524;&#12290;&#26368;&#26032;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;GER&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;HyPoradise&#25968;&#25454;&#38598;&#36890;&#36807;&#39640;&#25928;&#30340;LLM&#24494;&#35843;&#20174;ASR N-best&#20551;&#35774;&#21040;&#22320;&#38754;&#30495;&#23454;&#36716;&#24405;&#30340;&#26144;&#23556;&#65292;&#36825;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22122;&#22768;&#40065;&#26834;ASR&#26041;&#38754;&#32570;&#20047;&#20855;&#20307;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#22522;&#20934;&#27979;&#35797;&#25193;&#23637;&#21040;&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#24182;&#30740;&#31350;&#26159;&#21542;&#21487;&#20197;&#25945;&#20250;LLMs&#20687;&#22122;&#22768;&#40065;&#26834;ASR&#19968;&#26679;&#25191;&#34892;&#21435;&#22122;&#12290;&#20854;&#20013;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#22122;&#22768;&#20449;&#24687;&#20316;&#20026;&#26465;&#20214;&#22120;&#24341;&#20837;LLM&#20013;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#38899;&#39057;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#22122;&#22768;&#23884;&#20837;&#21487;&#33021;&#20250;&#23545;LLM&#24494;&#35843;&#36896;&#25104;&#25439;&#23475;&#65292;&#22240;&#20026;&#23384;&#22312;&#36328;&#27169;&#24577;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;N-best&#21015;&#34920;&#20013;&#25552;&#21462;&#35821;&#35328;&#31354;&#38388;&#22122;&#22768;&#23884;&#20837;&#26469;&#34920;&#31034;&#28304;&#35821;&#38899;&#30340;&#22122;&#22768;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with HyPoradise dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#35745;&#31639;&#35748;&#30693;&#26550;&#26500;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;LLMs&#65292;&#24182;&#24378;&#35843;&#20102;&#21452;&#36807;&#31243;&#26550;&#26500;&#21644;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#20063;&#25351;&#20986;&#38656;&#35201;&#25913;&#38761;&#35745;&#31639;&#35748;&#30693;&#26550;&#26500;&#20197;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20027;&#24352;&#37319;&#29992;&#22810;&#23398;&#31185;&#12289;&#20114;&#24800;&#20114;&#21033;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#26356;&#22909;&#30340;AI&#27169;&#22411;&#21644;&#29702;&#35299;&#20154;&#31867;&#24515;&#28789;&#12290;</title><link>http://arxiv.org/abs/2401.10444</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#35748;&#30693;&#26550;&#26500;&#26159;&#21542;&#21487;&#20197;&#26681;&#26412;&#25913;&#36827;LLMs&#65311;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?. (arXiv:2401.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#35745;&#31639;&#35748;&#30693;&#26550;&#26500;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;LLMs&#65292;&#24182;&#24378;&#35843;&#20102;&#21452;&#36807;&#31243;&#26550;&#26500;&#21644;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#20063;&#25351;&#20986;&#38656;&#35201;&#25913;&#38761;&#35745;&#31639;&#35748;&#30693;&#26550;&#26500;&#20197;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20027;&#24352;&#37319;&#29992;&#22810;&#23398;&#31185;&#12289;&#20114;&#24800;&#20114;&#21033;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#26356;&#22909;&#30340;AI&#27169;&#22411;&#21644;&#29702;&#35299;&#20154;&#31867;&#24515;&#28789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#35299;&#20915;&#24403;&#21069;&#20197;LLM&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#25152;&#38656;&#30340;&#26465;&#20214;&#12290;&#25991;&#31456;&#35748;&#20026;&#65292;&#21560;&#25910;&#20154;&#31867;&#35748;&#30693;&#21644;&#24515;&#29702;&#23398;&#26041;&#38754;&#30340;&#35265;&#35299;&#65292;&#24182;&#24212;&#29992;&#35745;&#31639;&#35748;&#30693;&#26550;&#26500;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#21152;&#33021;&#21147;&#24378;&#12289;&#21487;&#38752;&#24615;&#26356;&#39640;&#12289;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#31995;&#32479;&#12290;&#25991;&#20013;&#24378;&#35843;&#20102;&#21452;&#36807;&#31243;&#26550;&#26500;&#21644;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#35299;&#20915;&#24403;&#21069;LLMs&#30340;&#23616;&#38480;&#24615;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#20063;&#24378;&#35843;&#20102;&#38656;&#35201;&#23545;&#35745;&#31639;&#35748;&#30693;&#26550;&#26500;&#36827;&#34892;&#25913;&#38761;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20027;&#24352;&#37319;&#29992;&#22810;&#23398;&#31185;&#12289;&#20114;&#24800;&#20114;&#21033;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#26356;&#22909;&#30340;AI&#27169;&#22411;&#21644;&#29702;&#35299;&#20154;&#31867;&#24515;&#28789;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper discusses what is needed to address the limitations of current LLM-centered AI systems. The paper argues that incorporating insights from human cognition and psychology, as embodied by a computational cognitive architecture, can help develop systems that are more capable, more reliable, and more human-like. It emphasizes the importance of the dual-process architecture and the hybrid neuro-symbolic approach in addressing the limitations of current LLMs. In the opposite direction, the paper also highlights the need for an overhaul of computational cognitive architectures to better reflect advances in AI and computing technology. Overall, the paper advocates for a multidisciplinary, mutually beneficial approach towards developing better models both for AI and for understanding the human mind.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32479;&#35745;&#24050;&#35299;&#20915;&#38382;&#39064;&#20449;&#24687;&#26469;&#33258;&#21160;&#35745;&#31639;&#20808;&#39564;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#20010;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10431</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#25773;&#32452;&#21512;&#38382;&#39064;&#30340;&#35299;&#26469;&#23398;&#20064;&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Learning a Prior for Monte Carlo Search by Replaying Solutions to Combinatorial Problems. (arXiv:2401.10431v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32479;&#35745;&#24050;&#35299;&#20915;&#38382;&#39064;&#20449;&#24687;&#26469;&#33258;&#21160;&#35745;&#31639;&#20808;&#39564;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#20010;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#22312;&#22810;&#20010;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#20808;&#39564;&#30693;&#35782;&#23545;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#38750;&#22343;&#21248;&#23637;&#24320;&#36827;&#34892;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#22343;&#21248;&#23637;&#24320;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32467;&#26524;&#12290;&#36890;&#24120;&#20351;&#29992;&#19987;&#38376;&#38024;&#23545;&#32452;&#21512;&#38382;&#39064;&#30340;&#25163;&#24037;&#21551;&#21457;&#24335;&#31639;&#27861;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35745;&#31639;&#20808;&#39564;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24050;&#35299;&#20915;&#38382;&#39064;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#22312;&#23637;&#24320;&#36807;&#31243;&#20013;&#19981;&#20250;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#19977;&#20010;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#65306;&#25289;&#19969;&#26041;&#38453;&#34917;&#20840;&#12289;Kakuro&#21644;&#36870;&#36716;RNA&#25240;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Search gives excellent results in multiple difficult combinatorial problems. Using a prior to perform non uniform playouts during the search improves a lot the results compared to uniform playouts. Handmade heuristics tailored to the combinatorial problem are often used as priors. We propose a method to automatically compute a prior. It uses statistics on solved problems. It is a simple and general method that incurs no computational cost at playout time and that brings large performance gains. The method is applied to three difficult combinatorial problems: Latin Square Completion, Kakuro, and Inverse RNA Folding.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38480;&#21046;&#37325;&#22797;&#27425;&#25968;&#65292;&#36991;&#20813;&#20102;&#22312;&#24191;&#20041;&#23884;&#22871;&#23637;&#24320;&#31574;&#30053;&#33258;&#36866;&#24212;&#20013;&#20986;&#29616;&#37325;&#22797;&#30340;&#26368;&#20339;&#24207;&#21015;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10420</link><description>&lt;p&gt;
&#26377;&#38480;&#37325;&#22797;&#30340;&#24191;&#20041;&#23884;&#22871;&#23637;&#24320;&#31574;&#30053;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Generalized Nested Rollout Policy Adaptation with Limited Repetitions. (arXiv:2401.10420v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38480;&#21046;&#37325;&#22797;&#27425;&#25968;&#65292;&#36991;&#20813;&#20102;&#22312;&#24191;&#20041;&#23884;&#22871;&#23637;&#24320;&#31574;&#30053;&#33258;&#36866;&#24212;&#20013;&#20986;&#29616;&#37325;&#22797;&#30340;&#26368;&#20339;&#24207;&#21015;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#23884;&#22871;&#23637;&#24320;&#31574;&#30053;&#33258;&#36866;&#24212;(GNRPA)&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#19968;&#31995;&#21015;&#36873;&#25321;&#30340;&#33945;&#29305;&#21345;&#27931;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#38480;&#21046;&#22312;&#32473;&#23450;&#23618;&#32423;&#25214;&#21040;&#30340;&#26368;&#20339;&#24207;&#21015;&#30340;&#37325;&#22797;&#27425;&#25968;&#65292;&#36991;&#20813;&#20351;&#29992;&#36807;&#20110;&#30830;&#23450;&#24615;&#30340;&#31574;&#30053;&#65292;&#26080;&#27861;&#25214;&#21040;&#19981;&#21516;&#30340;&#36873;&#25321;&#24207;&#21015;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#36870;&#21521;RNA&#21046;&#20316;&#65292;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#21644;&#24369;Schur&#38382;&#39064;&#36825;&#19977;&#20010;&#32452;&#21512;&#38382;&#39064;&#19978;&#25913;&#36827;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Nested Rollout Policy Adaptation (GNRPA) is a Monte Carlo search algorithm for optimizing a sequence of choices. We propose to improve on GNRPA by avoiding too deterministic policies that find again and again the same sequence of choices. We do so by limiting the number of repetitions of the best sequence found at a given level. Experiments show that it improves the algorithm for three different combinatorial problems: Inverse RNA Folding, the Traveling Salesman Problem with Time Windows and the Weak Schur problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#38750;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20154;&#31867;&#65292;&#21516;&#26102;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#24341;&#23548;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#38271;&#25688;&#35201;&#21644;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#26377;&#38480;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25511;&#21046;&#26041;&#38754;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.10415</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#26426;&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#31185;&#23398;&#20256;&#25773;&#30446;&#26631;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?. (arXiv:2401.10415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#38750;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20154;&#31867;&#65292;&#21516;&#26102;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#24341;&#23548;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#38271;&#25688;&#35201;&#21644;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#26377;&#38480;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25511;&#21046;&#26041;&#38754;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#34920;&#24449;&#35770;&#25991;&#35780;&#35770;&#12289;&#25688;&#35201;&#21644;&#31616;&#21270;&#25688;&#35201;&#31561;&#19981;&#21516;&#31867;&#22411;&#25688;&#35201;&#30340;&#20851;&#38190;&#39118;&#26684;&#21644;&#20869;&#23481;&#35206;&#30422;&#22240;&#32032;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#24494;&#35843;&#30340;LLMs&#22312;MuP&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#19982;&#21442;&#32771;&#25688;&#35201;&#30340;&#30456;&#20284;&#24230;&#36824;&#26159;&#22312;&#20154;&#31867;&#20559;&#22909;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548; (CFG) &#26469;&#25913;&#21892;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#22312;arXiv&#21644;PubMed&#19978;&#23454;&#29616;&#19982;&#24378;&#24494;&#35843;&#22522;&#32447;&#30456;&#24403;&#30340;&#35789;&#27719;&#37325;&#21472;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;LLMs&#26080;&#27861;&#19968;&#33268;&#22320;&#29983;&#25104;&#36229;&#36807;8&#20010;&#21477;&#23376;&#30340;&#38271;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#34429;&#28982;LLMs&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#36890;&#29992;&#25688;&#35201;&#33021;&#21147;&#65292;&#20294;&#22312;&#19981;&#26114;&#36149;&#30340;&#24494;&#35843;&#25514;&#26045;&#19979;&#65292;&#23545;&#20869;&#23481;&#30340;&#22797;&#26434;&#25511;&#21046;&#33021;&#21147;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the controllability of large language models (LLMs) on scientific summarization tasks. We identify key stylistic and content coverage factors that characterize different types of summaries such as paper reviews, abstracts, and lay summaries. By controlling stylistic features, we find that non-fine-tuned LLMs outperform humans in the MuP review generation task, both in terms of similarity to reference summaries and human preferences. Also, we show that we can improve the controllability of LLMs with keyword-based classifier-free guidance (CFG) while achieving lexical overlap comparable to strong fine-tuned baselines on arXiv and PubMed. However, our results also indicate that LLMs cannot consistently generate long summaries with more than 8 sentences. Furthermore, these models exhibit limited capacity to produce highly abstractive lay summaries. Although LLMs demonstrate strong generic summarization competency, sophisticated content control without costly fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#28040;&#38500;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#65292;&#25552;&#39640;&#33258;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10394</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#31232;&#30095;&#26631;&#31614;&#22270;&#31070;&#32463;&#32593;&#32476;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels. (arXiv:2401.10394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#28040;&#38500;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#65292;&#25552;&#39640;&#33258;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#30417;&#30563;&#19981;&#36275;&#21644;&#28508;&#22312;&#30340;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#33258;&#35757;&#32451;&#26159;&#19968;&#31181;&#24191;&#27867;&#27969;&#34892;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#25193;&#23637;&#35757;&#32451;&#38598;&#65292;&#36890;&#36807;&#32473;&#36873;&#23450;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#20998;&#37197;&#20266;&#26631;&#31614;&#12290;&#24050;&#32463;&#24320;&#23637;&#20102;&#35768;&#22810;&#22522;&#20110;&#32622;&#20449;&#24230;&#12289;&#20449;&#24687;&#22686;&#30410;&#31561;&#36873;&#25321;&#31574;&#30053;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#27809;&#26377;&#19968;&#20010;&#32771;&#34385;&#21040;&#35757;&#32451;&#21644;&#27979;&#35797;&#33410;&#28857;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#20266;&#26631;&#31614;&#27493;&#39588;&#21487;&#33021;&#22686;&#21152;&#36825;&#31181;&#36716;&#31227;&#29978;&#33267;&#24341;&#20837;&#26032;&#30340;&#36716;&#31227;&#65292;&#20174;&#32780;&#38459;&#30861;&#33258;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#33258;&#35757;&#32451;&#36807;&#31243;&#20013;&#26126;&#30830;&#22320;&#28040;&#38500;&#25193;&#23637;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#22270;&#33258;&#35757;&#32451;(DC-GST)&#26694;&#26550;&#65292;&#29992;&#20110;&#36776;&#21035;
&lt;/p&gt;
&lt;p&gt;
Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.10393</link><description>&lt;p&gt;
&#33258;&#28982;&#30340;&#21151;&#29575;&#27861;&#21017;&#23398;&#20064;&#29615;&#22659;&#20013;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36973;&#21463;&#28798;&#38590;&#24615;&#24178;&#25200;&#65288;CI&#65289;&#65306;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#19982;&#20154;&#31867;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#20154;&#31867;&#21487;&#20197;&#36830;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#20250;&#26126;&#26174;&#24536;&#35760;&#20808;&#21069;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#20943;&#36731;CI&#30340;&#25216;&#26415;&#65292;&#20363;&#22914;&#27491;&#21017;&#21270;&#12289;&#22238;&#24518;&#12289;&#29983;&#25104;&#24615;&#22238;&#25918;&#21644;&#27987;&#32553;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#30340;&#25351;&#23548;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36935;&#21040;&#20219;&#21153;&#30340;&#27010;&#29575;&#19982;&#26368;&#21518;&#19968;&#27425;&#25191;&#34892;&#20219;&#21153;&#30340;&#26102;&#38388;&#25104;&#21151;&#29575;&#27861;&#21017;&#36882;&#20943;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27169;&#25311;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#20943;&#36731;CI&#25216;&#26415;&#30340;&#30495;&#23454;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#31867;&#20284;&#20154;&#31867;&#38754;&#20020;&#30340;&#21151;&#29575;&#27861;&#21017;&#29615;&#22659;&#20013;&#35757;&#32451;&#31616;&#21333;&#30340;&#22238;&#24518;&#26041;&#27861;&#26102;&#65292;CI&#30340;&#20943;&#36731;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#36825;&#31181;&#22522;&#20110;&#22238;&#24518;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that a realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based appro
&lt;/p&gt;</description></item><item><title>YOLO&#31639;&#27861;&#22312;&#20892;&#19994;&#29289;&#20307;&#26816;&#27979;&#20013;&#20855;&#26377;&#20808;&#36827;&#30340;&#24615;&#33021;&#29305;&#28857;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#26816;&#27979;&#12289;&#20934;&#30830;&#24615;&#22909;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20892;&#19994;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#23545;YOLO&#22312;&#20892;&#19994;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35760;&#24405;&#21644;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2401.10379</link><description>&lt;p&gt;
&#20892;&#19994;&#29289;&#20307;&#26816;&#27979;&#30340;You Look Only Once(YOLO)&#31639;&#27861;&#65306;&#19968;&#39033;&#25991;&#29486;&#35745;&#37327;&#23398;&#21644;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Agricultural Object Detection with You Look Only Once (YOLO) Algorithm: A Bibliometric and Systematic Literature Review. (arXiv:2401.10379v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10379
&lt;/p&gt;
&lt;p&gt;
YOLO&#31639;&#27861;&#22312;&#20892;&#19994;&#29289;&#20307;&#26816;&#27979;&#20013;&#20855;&#26377;&#20808;&#36827;&#30340;&#24615;&#33021;&#29305;&#28857;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#26816;&#27979;&#12289;&#20934;&#30830;&#24615;&#22909;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20892;&#19994;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#23545;YOLO&#22312;&#20892;&#19994;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35760;&#24405;&#21644;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26159;&#20892;&#19994;&#20013;&#20960;&#31181;&#25968;&#23383;&#25216;&#26415;&#21644;&#24037;&#20855;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;You Look Only Once (YOLO)&#29289;&#20307;&#26816;&#27979;&#22120;&#30001;&#20110;&#20854;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#22312;&#20892;&#19994;&#39046;&#22495;&#20869;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#24191;&#21463;&#27426;&#36814;&#12290;YOLO&#25552;&#20379;&#23454;&#26102;&#26816;&#27979;&#65292;&#20934;&#30830;&#24615;&#33391;&#22909;&#65292;&#24182;&#22312;&#30417;&#27979;&#12289;&#30417;&#35270;&#12289;&#24863;&#30693;&#12289;&#33258;&#21160;&#21270;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#20892;&#19994;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#23454;&#26045;&#12290;YOLO&#22312;&#20892;&#19994;&#20013;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#27491;&#22312;&#24555;&#36895;&#21152;&#36895;&#65292;&#20294;&#23384;&#22312;&#30862;&#29255;&#21270;&#21644;&#22810;&#23398;&#31185;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#29305;&#24449;&#65288;&#22914;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#12289;&#35745;&#31639;&#65289;&#24433;&#21709;&#20102;&#20892;&#19994;&#25216;&#26415;&#30340;&#23454;&#26045;&#21644;&#37319;&#29992;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25910;&#38598;&#24191;&#27867;&#30340;&#25991;&#29486;&#65292;&#20197;&#35760;&#24405;&#21644;&#25209;&#21028;&#24615;&#35780;&#20215;YOLO&#22312;&#20892;&#19994;&#29289;&#20307;&#35782;&#21035;&#20013;&#30340;&#36827;&#23637;&#21644;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;257&#31687;&#25991;&#31456;&#30340;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#65292;&#20197;&#20102;&#35299;&#20892;&#19994;&#39046;&#22495;&#20013;YOLO&#30340;&#23398;&#26415;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision is a major component in several digital technologies and tools used in agriculture. The object detector, You Look Only Once (YOLO), has gained popularity in agriculture in a relatively short span due to its state-of-the-art performance. YOLO offers real-time detection with good accuracy and is implemented in various agricultural tasks, including monitoring, surveillance, sensing, automation, and robotics. The research and application of YOLO in agriculture are accelerating rapidly but are fragmented and multidisciplinary. Moreover, the performance characteristics (i.e., accuracy, speed, computation) of the object detector influence the rate of technology implementation and adoption in agriculture. Thus, the study aims to collect extensive literature to document and critically evaluate the advances and application of YOLO for agricultural object recognition. First, we conducted a bibliometric review of 257 articles to understand the scholarly landscape of YOLO in agricultural dom
&lt;/p&gt;</description></item><item><title>MutaBot&#26159;&#19968;&#31181;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21464;&#24322;&#27979;&#35797;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#23545;&#23545;&#35805;&#27969;&#31243;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#36827;&#34892;&#21464;&#24322;&#12290;&#23427;&#20026;&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#38024;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29305;&#23450;&#25925;&#38556;&#31867;&#22411;&#30340;&#25903;&#25345;&#65292;&#26159;&#19968;&#20010;&#26377;&#25928;&#35780;&#20272;&#27979;&#35797;&#22871;&#20214;&#26377;&#25928;&#24615;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.10372</link><description>&lt;p&gt;
MutaBot: &#19968;&#31181;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21464;&#24322;&#27979;&#35797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MutaBot: A Mutation Testing Approach for Chatbots. (arXiv:2401.10372v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10372
&lt;/p&gt;
&lt;p&gt;
MutaBot&#26159;&#19968;&#31181;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21464;&#24322;&#27979;&#35797;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#23545;&#23545;&#35805;&#27969;&#31243;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#36827;&#34892;&#21464;&#24322;&#12290;&#23427;&#20026;&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#38024;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29305;&#23450;&#25925;&#38556;&#31867;&#22411;&#30340;&#25903;&#25345;&#65292;&#26159;&#19968;&#20010;&#26377;&#25928;&#35780;&#20272;&#27979;&#35797;&#22871;&#20214;&#26377;&#25928;&#24615;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#24322;&#27979;&#35797;&#26159;&#19968;&#31181;&#35780;&#20272;&#27979;&#35797;&#22871;&#20214;&#26377;&#25928;&#24615;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#21521;&#31243;&#24207;&#20013;&#27880;&#20837;&#20154;&#24037;&#25925;&#38556;&#26469;&#23454;&#29616;&#12290;&#23613;&#31649;&#36866;&#29992;&#20110;&#35768;&#22810;&#24179;&#21488;&#21644;&#32534;&#31243;&#35821;&#35328;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#36866;&#29992;&#20110;&#23545;&#35805;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21464;&#24322;&#27979;&#35797;&#24037;&#20855;&#65292;&#32780;&#36825;&#20123;&#26426;&#22120;&#20154;&#26159;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#30340;&#26085;&#30410;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;MutaBot&#30340;&#21464;&#24322;&#27979;&#35797;&#24037;&#20855;&#65292;&#23427;&#21487;&#38024;&#23545;&#23545;&#35805;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22810;&#20010;&#23618;&#32423;&#36827;&#34892;&#21464;&#24322;&#65292;&#21253;&#25324;&#23545;&#35805;&#27969;&#31243;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#28508;&#22312;&#22320;&#38024;&#23545;&#22810;&#20010;&#24179;&#21488;&#65292;&#21516;&#26102;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;Google Dialogflow&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21021;&#27493;&#25903;&#25345;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;Dialogflow&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#27979;&#35797;&#29992;&#20363;&#26469;&#35780;&#20272;&#35813;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mutation testing is a technique aimed at assessing the effectiveness of test suites by seeding artificial faults into programs. Although available for many platforms and languages, no mutation testing tool is currently available for conversational chatbots, which represent an increasingly popular solution to design systems that can interact with users through a natural language interface. Note that since conversations must be explicitly engineered by the developers of conversational chatbots, these systems are exposed to specific types of faults not supported by existing mutation testing tools.  In this paper, we present MutaBot, a mutation testing tool for conversational chatbots. MutaBot addresses mutations at multiple levels, including conversational flows, intents, and contexts. We designed the tool to potentially target multiple platforms, while we implemented initial support for Google Dialogflow chatbots. We assessed the tool with three Dialogflow chatbots and test cases generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#36339;&#22522;&#20110;&#38598;&#32676;&#30340;&#36710;&#32852;&#32593;&#20013;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#24179;&#22343;&#30456;&#23545;&#36895;&#24230;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#23545;FL&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#32452;&#21512;&#20197;&#20316;&#20026;&#32858;&#31867;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#39640;&#36710;&#36742;&#31227;&#21160;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10361</link><description>&lt;p&gt;
&#22312;&#22810;&#36339;&#22522;&#20110;&#38598;&#32676;&#30340;&#36710;&#32852;&#32593;&#20013;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Federated Learning in Multi-hop Cluster-Based VANETs. (arXiv:2401.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#36339;&#22522;&#20110;&#38598;&#32676;&#30340;&#36710;&#32852;&#32593;&#20013;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#24179;&#22343;&#30456;&#23545;&#36895;&#24230;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#23545;FL&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#32452;&#21512;&#20197;&#20316;&#20026;&#32858;&#31867;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#39640;&#36710;&#36742;&#31227;&#21160;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36710;&#32852;&#32593;&#20013;&#20351;&#29992;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#22240;&#20026;&#36890;&#36807;&#36890;&#20449;&#26412;&#22320;&#25968;&#25454;&#38598;&#26799;&#24230;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#65292;&#21487;&#20197;&#20943;&#23569;&#20256;&#36755;&#24320;&#38144;&#24182;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#36710;&#32852;&#32593;&#20013;&#23454;&#26045;FL&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#36890;&#20449;&#36164;&#28304;&#12289;&#39640;&#36710;&#36742;&#31227;&#21160;&#24615;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32479;&#35745;&#22810;&#26679;&#24615;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#22810;&#36339;&#32858;&#31867;&#30340;&#36710;&#32852;&#32593;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;FL&#27169;&#22411;&#21442;&#25968;&#30340;&#24179;&#22343;&#30456;&#23545;&#36895;&#24230;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#21152;&#26435;&#32452;&#21512;&#20316;&#20026;&#32858;&#31867;&#24230;&#37327;&#65292;&#20197;&#32771;&#34385;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#39640;&#36710;&#36742;&#31227;&#21160;&#24615;&#12290;&#36825;&#20010;&#24230;&#37327;&#21487;&#20197;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#22330;&#26223;&#20013;&#20445;&#35777;&#26368;&#23567;&#21464;&#21270;&#31751;&#22836;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35299;&#20915;&#19982;&#35813;&#22330;&#26223;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usage of federated learning (FL) in Vehicular Ad hoc Networks (VANET) has garnered significant interest in research due to the advantages of reducing transmission overhead and protecting user privacy by communicating local dataset gradients instead of raw data. However, implementing FL in VANETs faces challenges, including limited communication resources, high vehicle mobility, and the statistical diversity of data distributions. In order to tackle these issues, this paper introduces a novel framework for hierarchical federated learning (HFL) over multi-hop clustering-based VANET. The proposed method utilizes a weighted combination of the average relative speed and cosine similarity of FL model parameters as a clustering metric to consider both data diversity and high vehicle mobility. This metric ensures convergence with minimum changes in cluster heads while tackling the complexities associated with non-independent and identically distributed (non-IID) data scenarios. Additionall
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#22522;&#20110;&#21382;&#21490;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#38450;&#27490;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20026;&#36719;&#20214;&#24037;&#31243;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.10359</link><description>&lt;p&gt;
&#20445;&#25345;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#25511;&#24615;: &#22522;&#20110;&#21382;&#21490;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Keeping Deep Learning Models in Check: A History-Based Approach to Mitigate Overfitting. (arXiv:2401.10359v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10359
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#22522;&#20110;&#21382;&#21490;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#38450;&#27490;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20026;&#36719;&#20214;&#24037;&#31243;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#20851;&#38190;&#20219;&#21153;&#65292;&#22914;&#28431;&#27934;&#26816;&#27979;&#21644;&#20195;&#30721;&#23457;&#26597;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#24433;&#21709;&#30528;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#36136;&#37327;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#24403;&#21069;&#20351;&#29992;&#30340;&#36807;&#25311;&#21512;&#26816;&#27979;&#21644;&#38450;&#27490;&#26041;&#27861;&#37117;&#26377;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#32467;&#26500;&#21644;&#39640;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22522;&#20110;&#35757;&#32451;&#21382;&#21490;&#65288;&#21363;&#39564;&#35777;&#25439;&#22833;&#65289;&#21516;&#26102;&#26816;&#27979;&#21644;&#38450;&#27490;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#22312;&#36807;&#25311;&#21512;&#27169;&#22411;&#30340;&#35757;&#32451;&#21382;&#21490;&#19978;&#35757;&#32451;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#20998;&#31867;&#22120;&#34987;&#29992;&#26469;&#26816;&#27979;&#19968;&#20010;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#26159;&#21542;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#22909;&#30340;&#20998;&#31867;&#22120;&#20063;&#21487;&#20197;&#29992;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In software engineering, deep learning models are increasingly deployed for critical tasks such as bug detection and code review. However, overfitting remains a challenge that affects the quality, reliability, and trustworthiness of software systems that utilize deep learning models. Overfitting can be (1) prevented (e.g., using dropout or early stopping) or (2) detected in a trained model (e.g., using correlation-based approaches). Both overfitting detection and prevention approaches that are currently used have constraints (e.g., requiring modification of the model structure, and high computing resources). In this paper, we propose a simple, yet powerful approach that can both detect and prevent overfitting based on the training history (i.e., validation losses). Our approach first trains a time series classifier on training histories of overfit models. This classifier is then used to detect if a trained model is overfit. In addition, our trained classifier can be used to prevent ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#38376;&#25511;&#22270;&#21464;&#25442;&#22120;&#65288;GGT&#65289;&#26694;&#26550;&#65292;&#29992;&#26469;&#39044;&#27979;&#22522;&#20110;&#21151;&#33021;&#36830;&#25509;&#24615;&#65288;FC&#65289;&#30340;&#35748;&#30693;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#36153;&#22478;&#31070;&#32463;&#21457;&#32946;&#38431;&#21015;&#65288;PNC&#65289;&#23454;&#35777;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#20984;&#26174;&#20102;&#20854;&#22312;&#35782;&#21035;&#19982;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30456;&#20851;&#32852;&#30340;&#20851;&#38190;&#31070;&#32463;&#36830;&#25509;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10348</link><description>&lt;p&gt;
&#36890;&#36807;&#38376;&#25511;&#22270;&#21464;&#25442;&#22120;&#22312;&#21151;&#33021;&#36830;&#25509;&#24615;&#30740;&#31350;&#20013;&#25506;&#32034;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Exploring General Intelligence via Gated Graph Transformer in Functional Connectivity Studies. (arXiv:2401.10348v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#38376;&#25511;&#22270;&#21464;&#25442;&#22120;&#65288;GGT&#65289;&#26694;&#26550;&#65292;&#29992;&#26469;&#39044;&#27979;&#22522;&#20110;&#21151;&#33021;&#36830;&#25509;&#24615;&#65288;FC&#65289;&#30340;&#35748;&#30693;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#36153;&#22478;&#31070;&#32463;&#21457;&#32946;&#38431;&#21015;&#65288;PNC&#65289;&#23454;&#35777;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#20984;&#26174;&#20102;&#20854;&#22312;&#35782;&#21035;&#19982;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30456;&#20851;&#32852;&#30340;&#20851;&#38190;&#31070;&#32463;&#36830;&#25509;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#36830;&#25509;&#24615;&#65288;FC&#65289;&#26159;&#20174;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#25552;&#21462;&#30340;&#19968;&#39033;&#37325;&#35201;&#24037;&#20855;&#65292;&#29992;&#20110;&#38416;&#26126;&#21508;&#31181;&#31934;&#31070;&#30142;&#30149;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#21246;&#21202;&#20986;&#25903;&#25745;&#20154;&#33041;&#35748;&#30693;&#21644;&#34892;&#20026;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#36884;&#24452;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#38656;&#39044;&#23450;&#20041;&#22270;&#32467;&#26500;&#26469;&#25551;&#36848;&#33041;&#21306;&#20043;&#38388;&#20851;&#32852;&#30340;&#38480;&#21046;&#65292;&#32780;&#36825;&#19968;&#32454;&#33410;&#24182;&#38750;&#20165;&#30001;FC&#25552;&#20379;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38376;&#25511;&#22270;&#21464;&#25442;&#22120;&#65288;GGT&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#22522;&#20110;FC&#39044;&#27979;&#35748;&#30693;&#25351;&#26631;&#12290;&#23545;&#36153;&#22478;&#31070;&#32463;&#21457;&#32946;&#38431;&#21015;&#65288;PNC&#65289;&#30340;&#23454;&#35777;&#39564;&#35777;&#31361;&#20986;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#20248;&#36234;&#39044;&#27979;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#20984;&#26174;&#20102;&#23427;&#22312;&#35782;&#21035;&#19982;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30456;&#20851;&#32852;&#30340;&#20851;&#38190;&#31070;&#32463;&#36830;&#25509;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional connectivity (FC) as derived from fMRI has emerged as a pivotal tool in elucidating the intricacies of various psychiatric disorders and delineating the neural pathways that underpin cognitive and behavioral dynamics inherent to the human brain. While Graph Neural Networks (GNNs) offer a structured approach to represent neuroimaging data, they are limited by their need for a predefined graph structure to depict associations between brain regions, a detail not solely provided by FCs. To bridge this gap, we introduce the Gated Graph Transformer (GGT) framework, designed to predict cognitive metrics based on FCs. Empirical validation on the Philadelphia Neurodevelopmental Cohort (PNC) underscores the superior predictive prowess of our model, further accentuating its potential in identifying pivotal neural connectivities that correlate with human cognitive processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ELRT&#65292;&#19968;&#31181;&#39640;&#25928;&#20302;&#31209;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#39640;&#20934;&#30830;&#24615;&#12289;&#39640;&#32039;&#20945;&#24615;&#30340;&#20302;&#31209;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;ELRT&#20855;&#26377;&#36739;&#23569;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#19981;&#38656;&#35201;&#26356;&#26032;&#20840;&#23610;&#23544;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10341</link><description>&lt;p&gt;
ELRT&#65306;&#39640;&#25928;&#20302;&#31209;&#35757;&#32451;&#32039;&#20945;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks. (arXiv:2401.10341v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ELRT&#65292;&#19968;&#31181;&#39640;&#25928;&#20302;&#31209;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#39640;&#20934;&#30830;&#24615;&#12289;&#39640;&#32039;&#20945;&#24615;&#30340;&#20302;&#31209;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;ELRT&#20855;&#26377;&#36739;&#23569;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#19981;&#38656;&#35201;&#26356;&#26032;&#20840;&#23610;&#23544;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#21387;&#32553;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20302;&#31209;&#35757;&#32451;&#20316;&#20026;&#19968;&#31181;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20302;&#31209;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#19981;&#21516;&#20110;&#20302;&#31209;&#21387;&#32553;&#65292;&#20302;&#31209;&#35757;&#32451;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#30340;&#20840;&#31209;&#27169;&#22411;&#65292;&#25972;&#20010;&#35757;&#32451;&#38454;&#27573;&#22987;&#32456;&#22312;&#20302;&#31209;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#21560;&#24341;&#20154;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20302;&#31209;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;/&#25110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20173;&#38656;&#26356;&#26032;&#20840;&#23610;&#23544;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20302;&#31209;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#36890;&#36807;&#30830;&#23450;&#36866;&#24403;&#30340;&#20302;&#31209;&#26684;&#24335;&#21644;&#24615;&#33021;&#25913;&#36827;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELRT&#65292;&#19968;&#31181;&#29992;&#20110;&#39640;&#20934;&#30830;&#24615;&#12289;&#39640;&#32039;&#20945;&#24615;&#12289;&#20302;&#31209;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#39640;&#25928;&#20302;&#31209;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank compression, a popular model compression technique that produces compact convolutional neural networks (CNNs) with low rankness, has been well-studied in the literature. On the other hand, low-rank training, as an alternative way to train low-rank CNNs from scratch, has been exploited little yet. Unlike low-rank compression, low-rank training does not need pre-trained full-rank models, and the entire training phase is always performed on the low-rank structure, bringing attractive benefits for practical applications. However, the existing low-rank training solutions still face several challenges, such as a considerable accuracy drop and/or still needing to update full-size models during the training. In this paper, we perform a systematic investigation on low-rank CNN training. By identifying the proper low-rank format and performance-improving strategy, we propose ELRT, an efficient low-rank training solution for high-accuracy, high-compactness, low-rank CNN models. Our exten
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10337</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#26159;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#22312;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#26377;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20889;&#20316;&#20013;&#35782;&#21035;TTPs&#65292;&#36890;&#24120;&#31216;&#20026;TTP&#26144;&#23556;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#32463;&#20856;&#30340;&#22810;&#31867;&#25110;&#22810;&#26631;&#31614;&#20998;&#31867;&#35774;&#32622;&#20026;&#30446;&#26631;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#31867;&#21035;&#65288;&#21363;TTPs&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#22797;&#26434;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#31181;&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23558;&#25991;&#26412;&#19982;TTP&#26631;&#31614;&#20043;&#38388;&#30340;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#20026;&#25991;&#26412;&#20998;&#37197;&#32473;TTP&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20165;&#20165;&#22312;&#22823;&#22411;&#26631;&#31614;&#31354;&#38388;&#19978;&#31454;&#20105;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#27604;&#36739;&#26426;&#21046;&#30340;&#31070;&#32463;&#21305;&#37197;&#26550;&#26500;&#65292;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
&lt;/p&gt;</description></item><item><title>DrugAssist&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#20998;&#23376;&#20248;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#26426;&#23545;&#35805;&#23454;&#29616;&#20248;&#21270;&#65292;&#21033;&#29992;LLM&#30340;&#24378;&#20132;&#20114;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10334</link><description>&lt;p&gt;
DrugAssist&#65306;&#19968;&#20010;&#29992;&#20110;&#20998;&#23376;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DrugAssist: A Large Language Model for Molecule Optimization. (arXiv:2401.10334v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10334
&lt;/p&gt;
&lt;p&gt;
DrugAssist&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#20998;&#23376;&#20248;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#26426;&#23545;&#35805;&#23454;&#29616;&#20248;&#21270;&#65292;&#21033;&#29992;LLM&#30340;&#24378;&#20132;&#20114;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#23581;&#35797;&#23558;LLMs&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#27969;&#31243;&#20013;&#65292;&#20998;&#23376;&#20248;&#21270;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;LLMs&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21442;&#19982;&#24456;&#23569;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#25429;&#25417;&#25968;&#25454;&#20013;&#25552;&#20379;&#30340;&#21270;&#23398;&#32467;&#26500;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#32780;&#27809;&#26377;&#21033;&#29992;&#19987;&#23478;&#21453;&#39304;&#12290;&#36825;&#20123;&#38750;&#20132;&#20114;&#24335;&#26041;&#27861;&#24573;&#35270;&#20102;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#23454;&#38469;&#19978;&#38656;&#35201;&#19987;&#23478;&#32463;&#39564;&#21644;&#36845;&#20195;&#25913;&#36827;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DrugAssist&#65292;&#19968;&#20010;&#36890;&#36807;&#20154;&#26426;&#23545;&#35805;&#21033;&#29992;LLM&#30340;&#24378;&#20132;&#20114;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20998;&#23376;&#20248;&#21270;&#30340;&#20132;&#20114;&#24335;&#27169;&#22411;&#12290;DrugAssist&#22312;&#21333;&#19968;&#21644;&#22810;&#20010;&#24615;&#36136;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the impressive performance of large language models (LLMs) on a wide range of tasks has attracted an increasing number of attempts to apply LLMs in drug discovery. However, molecule optimization, a critical task in the drug discovery pipeline, is currently an area that has seen little involvement from LLMs. Most of existing approaches focus solely on capturing the underlying patterns in chemical structures provided by the data, without taking advantage of expert feedback. These non-interactive approaches overlook the fact that the drug discovery process is actually one that requires the integration of expert experience and iterative refinement. To address this gap, we propose DrugAssist, an interactive molecule optimization model which performs optimization through human-machine dialogue by leveraging LLM's strong interactivity and generalizability. DrugAssist has achieved leading results in both single and multiple property optimization, simultaneously showcasing immense pot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#38544;&#24335;&#21453;&#39304;&#20013;&#19981;&#21516;&#20559;&#22909;&#24378;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#20854;&#20013;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#23618;&#26469;&#25506;&#32034;&#39640;&#38454;&#20851;&#31995;&#12290;&#36825;&#20351;&#24471;&#34920;&#31034;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10316</link><description>&lt;p&gt;
&#22312;&#19981;&#21516;&#20559;&#22909;&#24378;&#24230;&#19978;&#36890;&#36807;&#22810;&#20219;&#21153;&#25552;&#21319;&#21333;&#31867;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Improving One-class Recommendation with Multi-tasking on Various Preference Intensities. (arXiv:2401.10316v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#38544;&#24335;&#21453;&#39304;&#20013;&#19981;&#21516;&#20559;&#22909;&#24378;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#20854;&#20013;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#23618;&#26469;&#25506;&#32034;&#39640;&#38454;&#20851;&#31995;&#12290;&#36825;&#20351;&#24471;&#34920;&#31034;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21333;&#31867;&#25512;&#33616;&#38382;&#39064;&#20013;&#65292;&#38656;&#35201;&#26681;&#25454;&#29992;&#25143;&#30340;&#38544;&#24335;&#21453;&#39304;&#26469;&#36827;&#34892;&#25512;&#33616;&#65292;&#35813;&#21453;&#39304;&#26159;&#36890;&#36807;&#29992;&#25143;&#30340;&#34892;&#20026;&#21644;&#19981;&#34892;&#20026;&#36827;&#34892;&#25512;&#26029;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#32534;&#30721;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#31215;&#26497;&#21644;&#28040;&#26497;&#20132;&#20114;&#26469;&#33719;&#21462;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#38544;&#24335;&#21453;&#39304;&#20013;&#30340;&#25152;&#26377;&#31215;&#26497;&#20449;&#21495;&#37117;&#21453;&#26144;&#20102;&#22266;&#23450;&#30340;&#20559;&#22909;&#24378;&#24230;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#29992;&#36825;&#20123;&#26041;&#27861;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21453;&#26144;&#19981;&#21516;&#20559;&#22909;&#24378;&#24230;&#30340;&#20449;&#24687;&#24615;&#23454;&#20307;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#38544;&#24335;&#21453;&#39304;&#20013;&#27599;&#20010;&#20449;&#21495;&#30340;&#19981;&#21516;&#20559;&#22909;&#24378;&#24230;&#12290;&#23454;&#20307;&#30340;&#34920;&#31034;&#38656;&#35201;&#21516;&#26102;&#28385;&#36275;&#27599;&#20010;&#23376;&#20219;&#21153;&#30340;&#30446;&#26631;&#65292;&#20351;&#20854;&#26356;&#21152;&#31283;&#20581;&#21644;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#23618;&#24341;&#20837;&#21040;&#29992;&#25143;-&#29289;&#21697;&#30340;&#39640;&#38454;&#20851;&#31995;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the one-class recommendation problem, it's required to make recommendations basing on users' implicit feedback, which is inferred from their action and inaction. Existing works obtain representations of users and items by encoding positive and negative interactions observed from training data. However, these efforts assume that all positive signals from implicit feedback reflect a fixed preference intensity, which is not realistic. Consequently, representations learned with these methods usually fail to capture informative entity features that reflect various preference intensities.  In this paper, we propose a multi-tasking framework taking various preference intensities of each signal from implicit feedback into consideration. Representations of entities are required to satisfy the objective of each subtask simultaneously, making them more robust and generalizable. Furthermore, we incorporate attentive graph convolutional layers to explore high-order relationships in the user-item
&lt;/p&gt;</description></item><item><title>LangProp&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36845;&#20195;&#20248;&#21270;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#24615;&#33021;&#21644;&#25429;&#25417;&#24322;&#24120;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.10314</link><description>&lt;p&gt;
LangProp: &#19968;&#31181;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LangProp: A code optimization framework using Language Models applied to driving. (arXiv:2401.10314v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10314
&lt;/p&gt;
&lt;p&gt;
LangProp&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36845;&#20195;&#20248;&#21270;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#24615;&#33021;&#21644;&#25429;&#25417;&#24322;&#24120;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LangProp&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30417;&#30563;/&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#36845;&#20195;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#34429;&#28982;LLM&#33021;&#22815;&#38646;-shot&#22320;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#21021;&#22987;&#20195;&#30721;&#21487;&#33021;&#22312;&#26576;&#20123;&#36793;&#32536;&#24773;&#20917;&#19979;&#22833;&#36133;&#12290;LangProp&#33258;&#21160;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#20195;&#30721;&#24615;&#33021;&#65292;&#24182;&#25429;&#25417;&#20219;&#20309;&#24322;&#24120;&#65292;&#24182;&#23558;&#32467;&#26524;&#21453;&#39304;&#32473;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20351;LLM&#21487;&#20197;&#36845;&#20195;&#25913;&#36827;&#20854;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24230;&#37327;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#35757;&#32451;&#33539;&#24335;&#26469;&#36827;&#34892;&#20195;&#30721;&#20248;&#21270;&#36807;&#31243;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20511;&#37492;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#27169;&#20223;&#23398;&#20064;&#12289;DAgger&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#30340;&#31532;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;LangProp&#21487;&#20197;&#29983;&#25104;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#30340;&#39550;&#39542;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
LangProp is a framework for iteratively optimizing code generated by large language models (LLMs) in a supervised/reinforcement learning setting. While LLMs can generate sensible solutions zero-shot, the solutions are often sub-optimal. Especially for code generation tasks, it is likely that the initial code will fail on certain edge cases. LangProp automatically evaluates the code performance on a dataset of input-output pairs, as well as catches any exceptions, and feeds the results back to the LLM in the training loop, so that the LLM can iteratively improve the code it generates. By adopting a metricand data-driven training paradigm for this code optimization procedure, one could easily adapt findings from traditional machine learning techniques such as imitation learning, DAgger, and reinforcement learning. We demonstrate the first proof of concept of automated code optimization for autonomous driving in CARLA, showing that LangProp can generate interpretable and transparent dri
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#21644;&#21496;&#27861;&#32422;&#26463;&#19979;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#25968;&#23398;&#31639;&#27861;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31639;&#27861;&#36879;&#26126;&#24615;&#30340;&#35201;&#27714;&#65292;&#24182;&#20351;&#29992;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;&#22312;&#35745;&#31639;&#27169;&#22411;&#20013;&#23454;&#29616;&#36879;&#26126;&#23454;&#26045;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10310</link><description>&lt;p&gt;
&#22312;&#31038;&#20250;&#21644;&#21496;&#27861;&#32422;&#26463;&#19979;&#23545;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#25968;&#23398;&#31639;&#27861;&#35774;&#35745;: &#31639;&#27861;&#36879;&#26126;&#24615;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Mathematical Algorithm Design for Deep Learning under Societal and Judicial Constraints: The Algorithmic Transparency Requirement. (arXiv:2401.10310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10310
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#21644;&#21496;&#27861;&#32422;&#26463;&#19979;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#25968;&#23398;&#31639;&#27861;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31639;&#27861;&#36879;&#26126;&#24615;&#30340;&#35201;&#27714;&#65292;&#24182;&#20351;&#29992;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;&#22312;&#35745;&#31639;&#27169;&#22411;&#20013;&#23454;&#29616;&#36879;&#26126;&#23454;&#26045;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#20449;&#24230;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#32570;&#38519;&#65292;&#36825;&#25551;&#36848;&#20102;&#19968;&#31181;&#21487;&#29702;&#35299;&#12289;&#20844;&#24179;&#12289;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20943;&#36731;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#36890;&#36807;&#30417;&#31649;&#25351;&#21335;&#25552;&#20986;&#20102;&#19982;&#21487;&#20449;&#24230;&#30456;&#20851;&#30340;&#26126;&#30830;&#20041;&#21153;&#65292;&#20363;&#22914;,&#22312;&#27431;&#27954;AI&#27861;&#26696;&#20013;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#21487;&#20197;&#23454;&#29616;&#22810;&#22823;&#31243;&#24230;&#19978;&#30340;&#21487;&#20449;&#24230;&#28145;&#24230;&#23398;&#20064;&#12290;&#24314;&#31435;&#26500;&#25104;&#21487;&#20449;&#24230;&#30340;&#25551;&#36848;&#24615;&#23646;&#24615;&#35201;&#27714;&#33021;&#22815;&#36861;&#28335;&#24433;&#21709;&#31639;&#27861;&#35745;&#31639;&#30340;&#22240;&#32032;&#65292;&#21363;&#31639;&#27861;&#30340;&#23454;&#29616;&#26159;&#36879;&#26126;&#30340;&#12290;&#21463;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28436;&#21270;&#38656;&#35201;&#25913;&#21464;&#35745;&#31639;&#25216;&#26415;&#30340;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#24471;&#20986;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#22312;&#35745;&#31639;&#27169;&#22411;&#20013;&#26159;&#21542;&#26377;&#21487;&#33021;&#23454;&#29616;&#36879;&#26126;&#23454;&#26045;&#12290;&#25105;&#20204;&#24212;&#29992;&#25105;&#20204;&#30340;&#21487;&#20449;&#24230;&#26694;&#26550;&#26469;&#20998;&#26512;&#25968;&#23383;&#21644;&#27169;&#25311;&#35745;&#31639;&#27169;&#22411;&#20013;&#36870;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning still has drawbacks in terms of trustworthiness, which describes a comprehensible, fair, safe, and reliable method. To mitigate the potential risk of AI, clear obligations associated to trustworthiness have been proposed via regulatory guidelines, e.g., in the European AI Act. Therefore, a central question is to what extent trustworthy deep learning can be realized. Establishing the described properties constituting trustworthiness requires that the factors influencing an algorithmic computation can be retraced, i.e., the algorithmic implementation is transparent. Motivated by the observation that the current evolution of deep learning models necessitates a change in computing technology, we derive a mathematical framework which enables us to analyze whether a transparent implementation in a computing model is feasible. We exemplarily apply our trustworthiness framework to analyze deep learning approaches for inverse problems in digital and analog computing models represe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#24314;&#35758;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2401.10304</link><description>&lt;p&gt;
&#35770;&#31185;&#23398;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#27491;&#21644;&#36879;&#26126;&#20351;&#29992;&#30340;&#20934;&#22791;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning. (arXiv:2401.10304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#24314;&#35758;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24615;&#65292;&#26368;&#36817;&#30340;&#31435;&#27861;&#20030;&#25514;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#30456;&#20851;&#30740;&#31350;&#25351;&#20986;&#38656;&#35201;&#35760;&#24405;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#37325;&#22797;&#24615;&#65292;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#20849;&#20139;&#23454;&#36341;&#36817;&#24180;&#26469;&#20063;&#26377;&#20102;&#21457;&#23637;&#12290;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#65292;&#23398;&#26415;&#26426;&#26500;&#37319;&#29992;&#20102;&#36825;&#20123;&#23454;&#36341;&#65292;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#25968;&#25454;&#21644;&#25216;&#26415;&#25991;&#20214;&#21457;&#24067;&#22312;&#21516;&#34892;&#35780;&#35758;&#30340;&#20986;&#29256;&#29289;&#19978;&#65292;&#22914;&#25968;&#25454;&#35770;&#25991;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;4041&#31687;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#35770;&#25991;&#26679;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35780;&#20272;&#20854;&#23436;&#25972;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#30740;&#31350;&#20102;&#36817;&#24180;&#26469;&#30340;&#36235;&#21183;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26368;&#22810;&#21644;&#26368;&#23569;&#34987;&#35760;&#24405;&#30340;&#26041;&#38754;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#25968;&#25454;&#21019;&#24314;&#32773;&#30340;&#24314;&#35758;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure the fairness and trustworthiness of machine learning (ML) systems, recent legislative initiatives and relevant research in the ML community have pointed out the need to document the data used to train ML models. Besides, data-sharing practices in many scientific domains have evolved in recent years for reproducibility purposes. In this sense, the adoption of these practices by academic institutions has encouraged researchers to publish their data and technical documentation in peer-reviewed publications such as data papers. In this study, we analyze how this scientific data documentation meets the needs of the ML community and regulatory bodies for its use in ML technologies. We examine a sample of 4041 data papers of different domains, assessing their completeness and coverage of the requested dimensions, and trends in recent years, putting special emphasis on the most and least documented dimensions. As a result, we propose a set of recommendation guidelines for data creato
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21644;&#20195;&#29702;&#30340;&#34920;&#31034;&#65292;&#20351;&#29992;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#25429;&#25417;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#31354;&#38388;&#27169;&#24335;&#21644;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10300</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#26816;&#27979;&#30340;&#20855;&#26377;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#30340;&#20998;&#23618;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems. (arXiv:2401.10300v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21644;&#20195;&#29702;&#30340;&#34920;&#31034;&#65292;&#20351;&#29992;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#25429;&#25417;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#31354;&#38388;&#27169;&#24335;&#21644;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30001;&#20132;&#20114;&#20195;&#29702;&#32452;&#25104;&#30340;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#65288;CAS&#65289;&#20013;&#65292;&#29616;&#35937;&#26159;&#19968;&#31181;&#20840;&#23616;&#23646;&#24615;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#24456;&#26222;&#36941;&#65292;&#20363;&#22914;&#32593;&#32476;&#23618;&#27425;&#30340;&#20132;&#36890;&#25317;&#22581;&#12290;&#26816;&#27979;&#23427;&#30340;&#24418;&#25104;&#21644;&#28040;&#25955;&#26377;&#21161;&#20110;&#30417;&#27979;&#31995;&#32479;&#30340;&#29366;&#24577;&#65292;&#24182;&#21457;&#20986;&#26377;&#23475;&#29616;&#35937;&#30340;&#35686;&#25253;&#20449;&#21495;&#12290;&#30001;&#20110;CAS&#27809;&#26377;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#65292;&#22522;&#20110;&#27599;&#20010;&#20195;&#29702;&#30340;&#23616;&#37096;&#35266;&#23519;&#26469;&#26816;&#27979;&#29616;&#35937;&#26159;&#21487;&#21462;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#19981;&#33021;&#25429;&#25417;&#19982;&#29616;&#35937;&#30456;&#20851;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#24182;&#19988;&#26080;&#27861;&#24314;&#27169;&#20195;&#29702;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#34920;&#31034;&#21644;&#20195;&#29702;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#22120;&#38024;&#23545;&#20195;&#29702;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#31995;&#32479;&#30340;&#22797;&#26434;&#28436;&#21270;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#36890;&#36807;&#20445;&#30041;&#26368;&#26032;100&#20010;&#20195;&#29702;&#30340;&#29366;&#24577;&#21644;&#21382;&#21490;&#29366;&#24577;&#26469;&#23398;&#20064;&#20195;&#29702;&#21644;&#31995;&#32479;&#30340;&#34920;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;
Emergence, a global property of complex adaptive systems (CASs) constituted by interactive agents, is prevalent in real-world dynamic systems, e.g., network-level traffic congestions. Detecting its formation and evaporation helps to monitor the state of a system, allowing to issue a warning signal for harmful emergent phenomena. Since there is no centralized controller of CAS, detecting emergence based on each agent's local observation is desirable but challenging. Existing works are unable to capture emergence-related spatial patterns, and fail to model the nonlinear relationships among agents. This paper proposes a hierarchical framework with spatio-temporal consistency learning to solve these two problems by learning the system representation and agent representations, respectively. Especially, spatio-temporal encoders are tailored to capture agents' nonlinear relationships and the system's complex evolution. Representations of the agents and the system are learned by preserving the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24402;&#19968;&#21270;&#27969;&#29983;&#25104;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#39640;&#32500;&#30697;&#38453;&#34892;&#21015;&#24335;&#35745;&#31639;&#21644;&#31070;&#32463;&#32593;&#32476;&#21487;&#36870;&#21464;&#25442;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10299</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#27969;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#30340;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
An attempt to generate new bridge types from latent space of generative flow. (arXiv:2401.10299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24402;&#19968;&#21270;&#27969;&#29983;&#25104;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#39640;&#32500;&#30697;&#38453;&#34892;&#21015;&#24335;&#35745;&#31639;&#21644;&#31070;&#32463;&#32593;&#32476;&#21487;&#36870;&#21464;&#25442;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20171;&#32461;&#19981;&#21516;&#20998;&#24067;&#20043;&#38388;&#30340;&#22352;&#26631;&#21644;&#27010;&#29575;&#21464;&#25442;&#30340;&#31034;&#20363;&#65292;&#31616;&#26126;&#25212;&#35201;&#22320;&#20171;&#32461;&#20102;&#24402;&#19968;&#21270;&#27969;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#20174;&#38543;&#26426;&#21464;&#37327;&#20989;&#25968;&#30340;&#20998;&#24067;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#27010;&#29575;&#21464;&#25442;&#30340;&#26412;&#36136;&#65292;&#24182;&#24341;&#20837;&#20102;&#27010;&#29575;&#21464;&#25442;&#30340;&#32553;&#25918;&#22240;&#23376;&#38597;&#21487;&#27604;&#34892;&#21015;&#24335;&#12290;&#23558;&#25968;&#25454;&#38598;&#35270;&#20026;&#26469;&#33258;&#24635;&#20307;&#30340;&#26679;&#26412;&#65292;&#33719;&#21462;&#24402;&#19968;&#21270;&#27969;&#26412;&#36136;&#19978;&#26159;&#36890;&#36807;&#37319;&#26679;&#35843;&#26597;&#23545;&#24635;&#20307;&#30340;&#25968;&#20540;&#29305;&#24449;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#28982;&#21518;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#24314;&#31435;&#25439;&#22833;&#20989;&#25968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24402;&#19968;&#21270;&#27969;&#22914;&#20309;&#24039;&#22937;&#22320;&#35299;&#20915;&#20102;&#39640;&#32500;&#30697;&#38453;&#34892;&#21015;&#24335;&#35745;&#31639;&#21644;&#31070;&#32463;&#32593;&#32476;&#21487;&#36870;&#21464;&#25442;&#20004;&#20010;&#20027;&#35201;&#24212;&#29992;&#25361;&#25112;&#12290;&#21033;&#29992;&#19977;&#36328;&#26753;&#26725;&#12289;&#25329;&#26725;&#12289;&#26012;&#25289;&#26725;&#21644;&#24748;&#32034;&#26725;&#30340;&#23545;&#31216;&#32467;&#26500;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;&#29983;&#25104;&#26725;&#26753;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through examples of coordinate and probability transformation between different distributions, the basic principle of normalizing flow is introduced in a simple and concise manner. From the perspective of the distribution of random variable function, the essence of probability transformation is explained, and the scaling factor Jacobian determinant of probability transformation is introduced. Treating the dataset as a sample from the population, obtaining normalizing flow is essentially through sampling surveys to statistically infer the numerical features of the population, and then the loss function is established by using the maximum likelihood estimation method. This article introduces how normalizing flow cleverly solves the two major application challenges of high-dimensional matrix determinant calculation and neural network reversible transformation. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge, constr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20809;&#30005;&#31070;&#32463;&#22788;&#29702;&#22120;&#65292;&#29992;&#20110;&#27169;&#25311;&#32463;&#22270;&#20687;&#26816;&#27979;&#35757;&#32451;&#36807;&#30340;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#21512;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20809;&#36951;&#20256;&#23398;&#23454;&#29616;&#31934;&#30830;&#28608;&#27963;&#30340;&#21453;&#21521;&#20256;&#25773;STDP&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10289</link><description>&lt;p&gt;
&#20809;&#30005;&#31070;&#32463;&#22788;&#29702;&#22120;&#30340;&#35774;&#35745;&#19982;&#24320;&#21457;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#26426;&#22120;&#20154;&#20013;&#23454;&#29616;&#32463;&#22270;&#20687;&#26816;&#27979;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Design and development of opto-neural processors for simulation of neural networks trained in image detection for potential implementation in hybrid robotics. (arXiv:2401.10289v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20809;&#30005;&#31070;&#32463;&#22788;&#29702;&#22120;&#65292;&#29992;&#20110;&#27169;&#25311;&#32463;&#22270;&#20687;&#26816;&#27979;&#35757;&#32451;&#36807;&#30340;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#21512;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20809;&#36951;&#20256;&#23398;&#23454;&#29616;&#31934;&#30830;&#28608;&#27963;&#30340;&#21453;&#21521;&#20256;&#25773;STDP&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#12289;&#36816;&#21160;&#25511;&#21046;&#12289;&#29289;&#20307;&#26816;&#27979;&#31561;&#21508;&#31181;&#22788;&#29702;&#24212;&#29992;&#20013;&#12290;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#20302;&#21151;&#32791;&#12289;&#26356;&#24555;&#30340;&#22788;&#29702;&#36895;&#24230;&#21644;&#29983;&#29289;&#29616;&#23454;&#24615;&#30340;&#20248;&#21183;&#12290;&#20809;&#36951;&#20256;&#23398;&#20026;&#29983;&#29289;&#31070;&#32463;&#20803;&#25552;&#20379;&#39640;&#24230;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#25511;&#21046;&#65292;&#24182;&#22312;&#35757;&#32451;&#27963;&#29983;&#29983;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#26377;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20809;&#36951;&#20256;&#23398;&#23454;&#29616;&#31934;&#30830;&#28608;&#27963;&#30340;&#21453;&#21521;&#20256;&#25773;STDP&#31639;&#27861;&#38388;&#25509;&#35757;&#32451;&#30340;&#27169;&#25311;&#27963;&#29983;&#29983;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20934;&#30830;&#24230;&#21487;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms.
&lt;/p&gt;</description></item><item><title>&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2401.10286</link><description>&lt;p&gt;
&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#20348;&#20348;&#32773;&#65306;&#33521;&#25991;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10286
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#65292;&#20219;&#21153;&#19982;&#35757;&#32451;&#35821;&#26009;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#20849;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#21644;&#25105;&#20204;&#35774;&#35745;&#30340;&#35780;&#20272;&#25351;&#26631;&#34920;&#26126;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#19982;&#20219;&#21153;&#32039;&#23494;&#21305;&#37197;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#31243;&#24230;&#36739;&#39640;&#30340;&#20219;&#21153;&#20013;&#65292;&#23637;&#31034;&#36739;&#23569;&#20013;&#25991;&#35821;&#35328;&#29305;&#24449;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#29992;&#20195;&#30721;&#27169;&#22411;&#26367;&#25442;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#20934;&#22791;&#25968;&#25454;&#65292;&#24456;&#23481;&#26131;&#24471;&#21040;&#22797;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the alignment between tasks and training corpora is a fundamental consensus in the application of language models, our series of experiments and the metrics we designed reveal that code-based Large Language Models (LLMs) significantly outperform models trained on data that is closely matched to the tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to Chinese hallucinations, models exhibiting fewer linguistic features of the Chinese language achieve better performance. Our experimental results can be easily replicated in Chinese data processing tasks, such as preparing data for Retrieval-Augmented Generation (RAG), by simply replacing the base model with a code-based model. Additionally, our research offers a distinct perspective for discussion on the philosophical "Chinese Room" thought experiment.
&lt;/p&gt;</description></item><item><title>MorpheusNet&#26159;&#19968;&#20010;&#36164;&#28304;&#25928;&#29575;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#23454;&#26102;&#39044;&#27979;&#30561;&#30496;&#38454;&#27573;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#65292;&#28040;&#32791;&#23569;&#37327;&#33021;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.10284</link><description>&lt;p&gt;
MorpheusNet&#65306;&#29992;&#20110;&#23884;&#20837;&#24335;&#22312;&#32447;&#31995;&#32479;&#30340;&#36164;&#28304;&#25928;&#29575;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
MorpheusNet: Resource efficient sleep stage classifier for embedded on-line systems. (arXiv:2401.10284v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10284
&lt;/p&gt;
&lt;p&gt;
MorpheusNet&#26159;&#19968;&#20010;&#36164;&#28304;&#25928;&#29575;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#23454;&#26102;&#39044;&#27979;&#30561;&#30496;&#38454;&#27573;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#65292;&#28040;&#32791;&#23569;&#37327;&#33021;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#65288;SSC&#65289;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#19987;&#23478;&#26816;&#26597;&#25968;&#23567;&#26102;&#30340;&#30005;&#29983;&#29702;&#35760;&#24405;&#36827;&#34892;&#25163;&#21160;&#20998;&#31867;&#12290;&#36825;&#22312;&#21033;&#29992;&#30561;&#30496;&#38454;&#27573;&#36827;&#34892;&#27835;&#30103;&#30446;&#30340;&#26102;&#26159;&#19968;&#20010;&#38480;&#21046;&#22240;&#32032;&#12290;&#38543;&#30528;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20215;&#26684;&#36234;&#26469;&#36234;&#21487;&#25215;&#21463;&#21644;&#25193;&#24352;&#65292;&#33258;&#21160;&#21270;SSC&#21487;&#33021;&#20351;&#24471;&#35268;&#27169;&#25193;&#22823;&#30340;&#30561;&#30496;&#22522;&#30784;&#27835;&#30103;&#25104;&#20026;&#21487;&#33021;&#12290;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20986;&#19982;&#25163;&#21160;&#19987;&#23478;&#35780;&#20998;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#23454;&#26102;&#20998;&#31867;&#21644;&#22312;&#36793;&#32536;&#37096;&#32626;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#33021;&#22815;&#22312;&#23454;&#26102;&#39044;&#27979;&#30561;&#30496;&#38454;&#27573;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#22806;&#37096;&#35745;&#31639;&#36164;&#28304;&#65288;&#20363;&#22914;&#31227;&#21160;&#25163;&#26426;&#12289;&#20113;&#65289;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#20302;&#21151;&#32791;&#30340;&#29305;&#28857;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#24335;&#30005;&#27744;&#20379;&#30005;&#31995;&#32479;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep Stage Classification (SSC) is a labor-intensive task, requiring experts to examine hours of electrophysiological recordings for manual classification. This is a limiting factor when it comes to leveraging sleep stages for therapeutic purposes. With increasing affordability and expansion of wearable devices, automating SSC may enable deployment of sleep-based therapies at scale. Deep Learning has gained increasing attention as a potential method to automate this process. Previous research has shown accuracy comparable to manual expert scores. However, previous approaches require sizable amount of memory and computational resources. This constrains the ability to classify in real time and deploy models on the edge. To address this gap, we aim to provide a model capable of predicting sleep stages in real-time, without requiring access to external computational sources (e.g., mobile phone, cloud). The algorithm is power efficient to enable use on embedded battery powered systems. Our
&lt;/p&gt;</description></item><item><title>BioDiffusion&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22797;&#26434;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10282</link><description>&lt;p&gt;
BioDiffusion&#65306;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10282
&lt;/p&gt;
&lt;p&gt;
BioDiffusion&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22797;&#26434;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12289;&#26631;&#31614;&#22797;&#26434;&#24615;&#21644;&#27979;&#37327;&#22122;&#22768;&#30340;&#24178;&#25200;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#32463;&#24120;&#38459;&#30861;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20339;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioDiffusion&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#21512;&#25104;&#22810;&#21464;&#37327;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#36827;&#34892;&#20248;&#21270;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;BioDiffusion&#22312;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#29992;&#20110;&#26080;&#26465;&#20214;&#12289;&#26631;&#31614;&#26465;&#20214;&#21644;&#20449;&#21495;&#26465;&#20214;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#20026;&#19978;&#36848;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;&#23545;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#36827;&#34892;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24378;&#35843;&#20854;&#22312;&#19982;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19982;&#24403;&#21069;&#20027;&#27969;&#30340;&#26102;&#38388;&#31995;&#20449;&#24687;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;BioDiffusion&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning tasks involving biomedical signals frequently grapple with issues such as limited data availability, imbalanced datasets, labeling complexities, and the interference of measurement noise. These challenges often hinder the optimal training of machine learning algorithms. Addressing these concerns, we introduce BioDiffusion, a diffusion-based probabilistic model optimized for the synthesis of multivariate biomedical signals. BioDiffusion demonstrates excellence in producing high-fidelity, non-stationary, multivariate signals for a range of tasks including unconditional, label-conditional, and signal-conditional generation. Leveraging these synthesized signals offers a notable solution to the aforementioned challenges. Our research encompasses both qualitative and quantitative assessments of the synthesized data quality, underscoring its capacity to bolster accuracy in machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed with current leading tim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26497;&#20540;&#29702;&#35770;&#65288;EVT&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#21487;&#38752;&#36890;&#20449;&#20013;&#31934;&#30830;&#30340;&#20449;&#36947;&#24314;&#27169;&#12290;&#36890;&#36807;&#20351;&#29992;GPD&#36827;&#34892;&#26497;&#31471;&#20107;&#20214;&#30340;&#20998;&#24067;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;GANs&#26469;&#20272;&#35745;GPD&#30340;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;GAN&#37197;&#32622;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#22312;GAN&#32467;&#26500;&#20013;&#22686;&#21152;&#20102;&#19968;&#20010;&#27169;&#22359;&#65292;&#30452;&#25509;&#20272;&#35745;GPD&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.10280</link><description>&lt;p&gt;
GANs&#29992;&#20110;&#23454;&#26102;&#36229;&#21487;&#38752;&#36890;&#20449;&#20013;&#22522;&#20110;EVT&#30340;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
GANs for EVT Based Model Parameter Estimation in Real-time Ultra-Reliable Communication. (arXiv:2401.10280v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26497;&#20540;&#29702;&#35770;&#65288;EVT&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#21487;&#38752;&#36890;&#20449;&#20013;&#31934;&#30830;&#30340;&#20449;&#36947;&#24314;&#27169;&#12290;&#36890;&#36807;&#20351;&#29992;GPD&#36827;&#34892;&#26497;&#31471;&#20107;&#20214;&#30340;&#20998;&#24067;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;GANs&#26469;&#20272;&#35745;GPD&#30340;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;GAN&#37197;&#32622;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#22312;GAN&#32467;&#26500;&#20013;&#22686;&#21152;&#20102;&#19968;&#20010;&#27169;&#22359;&#65292;&#30452;&#25509;&#20272;&#35745;GPD&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20845;&#20195;&#65288;6G&#65289;&#31995;&#32479;&#20013;&#30340;&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#65288;URLLC&#65289;&#33539;&#24335;&#22312;&#26080;&#32447;&#36890;&#20449;&#20449;&#36947;&#20013;&#22788;&#29702;&#31232;&#26377;&#21644;&#26497;&#31471;&#20107;&#20214;&#26102;&#65292;&#20005;&#37325;&#20381;&#36182;&#31934;&#30830;&#30340;&#20449;&#36947;&#24314;&#27169;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#23558;&#26497;&#20540;&#29702;&#35770;&#65288;EVT&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;&#20449;&#36947;&#24314;&#27169;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;GPD&#65288;Generalized Pareto Distribution&#65289;&#37319;&#29992;EVT&#26469;&#23545;&#26497;&#31471;&#20107;&#20214;&#30340;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;GANs&#20272;&#35745;GPD&#30340;&#21442;&#25968;&#12290;&#19982;&#20256;&#32479;&#30340;GAN&#37197;&#32622;&#20391;&#37325;&#20110;&#20272;&#35745;&#25972;&#20307;&#20998;&#24067;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;GAN&#32467;&#26500;&#20013;&#22686;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#27169;&#22359;&#65292;&#30446;&#30340;&#26159;&#30452;&#25509;&#20272;&#35745;&#24191;&#20041;&#24085;&#32047;&#25176;&#20998;&#24067;&#65288;GPD&#65289;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Ultra-Reliable Low-Latency Communications (URLLC) paradigm in sixth-generation (6G) systems heavily relies on precise channel modeling, especially when dealing with rare and extreme events within wireless communication channels. This paper explores a novel methodology integrating Extreme Value Theory (EVT) and Generative Adversarial Networks (GANs) to achieve the precise channel modeling in real-time. The proposed approach harnesses EVT by employing the Generalized Pareto Distribution (GPD) to model the distribution of extreme events. Subsequently, Generative Adversarial Networks (GANs) are employed to estimate the parameters of the GPD. In contrast to conventional GAN configurations that focus on estimating the overall distribution, the proposed approach involves the incorporation of an additional block within the GAN structure. This specific augmentation is designed with the explicit purpose of directly estimating the parameters of the Generalized Pareto Distribution (GPD). Throu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22320;&#29702;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#20027;&#35201;&#30340;&#23884;&#20837;&#20027;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#31354;&#38388;&#24418;&#24577;&#21644;&#29983;&#25104;&#27169;&#24577;&#26041;&#38754;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.10279</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22320;&#29702;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#30340;&#31995;&#32479;&#32508;&#36848;&#65306;&#36808;&#21521;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems. (arXiv:2401.10279v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10279
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22320;&#29702;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#20027;&#35201;&#30340;&#23884;&#20837;&#20027;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#31354;&#38388;&#24418;&#24577;&#21644;&#29983;&#25104;&#27169;&#24577;&#26041;&#38754;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#20301;&#32622;&#23884;&#20837;&#65288;GLE&#65289;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21560;&#25910;&#21644;&#20998;&#26512;&#31354;&#38388;&#25968;&#25454;&#12290;GLE&#22312;&#22320;&#29702;&#20154;&#24037;&#26234;&#33021;&#65288;GeoAI&#65289;&#20013;&#30340;&#20986;&#29616;&#26159;&#30001;&#20110;&#25105;&#20204;&#22797;&#26434;&#24403;&#20195;&#31354;&#38388;&#20013;&#23545;&#26356;&#28145;&#20837;&#30340;&#22320;&#29702;&#35748;&#30693;&#30340;&#38656;&#27714;&#20197;&#21450;LLM&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#25552;&#21462;&#28145;&#23618;&#21547;&#20041;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#22312;Google Scholar&#12289;Science Direct&#21644;arXiv&#19978;&#25628;&#32034;&#20102;&#20851;&#20110;&#22320;&#29702;&#20301;&#32622;&#23884;&#20837;&#21644;LLM&#30340;&#35770;&#25991;&#65292;&#24182;&#23457;&#26597;&#20102;&#30528;&#37325;&#20110;&#36890;&#36807;LLM&#23454;&#29616;&#26356;&#28145;&#20837;&#31354;&#38388;&#8220;&#30693;&#35782;&#8221;&#30340;&#25991;&#31456;&#12290;&#25105;&#20204;&#31579;&#36873;&#20102;304&#20010;&#26631;&#39064;&#12289;30&#20010;&#25688;&#35201;&#21644;18&#31687;&#20840;&#25991;&#35770;&#25991;&#65292;&#25581;&#31034;&#20102;&#22235;&#20010;GLE&#20027;&#39064; - &#23454;&#20307;&#20301;&#32622;&#23884;&#20837;&#65288;ELE&#65289;&#12289;&#25991;&#26723;&#20301;&#32622;&#23884;&#20837;&#65288;DLE&#65289;&#12289;&#24207;&#21015;&#20301;&#32622;&#23884;&#20837;&#65288;SLE&#65289;&#21644;&#20196;&#29260;&#20301;&#32622;&#23884;&#20837;&#65288;TLE&#65289;&#12290;&#32508;&#36848;&#20197;&#34920;&#26684;&#21644;&#21465;&#36848;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#21253;&#25324;&#8220;&#31354;&#38388;&#8221;&#21644;&#8220;LLM&#8221;&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#23613;&#31649;GLE&#36890;&#36807;&#21472;&#21152;&#31354;&#38388;&#25968;&#25454;&#26377;&#21161;&#20110;&#29702;&#35299;&#31354;&#38388;&#65292;&#20294;&#24378;&#35843;&#20102;&#22312;&#31354;&#38388;&#24418;&#24577;&#21644;&#29983;&#25104;&#27169;&#24577;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geospatial Location Embedding (GLE) helps a Large Language Model (LLM) assimilate and analyze spatial data. GLE emergence in Geospatial Artificial Intelligence (GeoAI) is precipitated by the need for deeper geospatial awareness in our complex contemporary spaces and the success of LLMs in extracting deep meaning in Generative AI. We searched Google Scholar, Science Direct, and arXiv for papers on geospatial location embedding and LLM and reviewed articles focused on gaining deeper spatial "knowing" through LLMs. We screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE themes - Entity Location Embedding (ELE), Document Location Embedding (DLE), Sequence Location Embedding (SLE), and Token Location Embedding (TLE). Synthesis is tabular and narrative, including a dialogic conversation between "Space" and "LLM." Though GLEs aid spatial understanding by superimposing spatial data, they emphasize the need to advance in the intricacies of spatial modalities and gener
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EEGFormer&#30340;&#33041;&#30005;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#30340;&#22797;&#21512;EEG&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#36801;&#31227;&#21644;&#21487;&#35299;&#37322;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#20026;&#33041;&#30005;&#20449;&#21495;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.10278</link><description>&lt;p&gt;
EEGFormer: &#23454;&#29616;&#21487;&#36801;&#31227;&#21644;&#21487;&#35299;&#37322;&#30340;&#22823;&#35268;&#27169;&#33041;&#30005;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model. (arXiv:2401.10278v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EEGFormer&#30340;&#33041;&#30005;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#30340;&#22797;&#21512;EEG&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#36801;&#31227;&#21644;&#21487;&#35299;&#37322;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#20026;&#33041;&#30005;&#20449;&#21495;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#22312;&#33041;&#30005;&#22270;(EEG)&#25968;&#25454;&#31561;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20063;&#36866;&#29992;&#20110;&#33041;&#20449;&#21495;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#20174;&#30315;&#30187;&#26816;&#27979;&#21040;&#27874;&#24418;&#20998;&#26512;&#31561;&#21508;&#31181;&#30495;&#23454;&#21307;&#23398;&#24212;&#29992;&#20013;&#23384;&#22312;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;EEG&#24314;&#27169;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#24212;&#20110;&#21333;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#27599;&#20010;&#29420;&#31435;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#19978;&#65292;&#36825;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20016;&#23500;&#30340;&#25968;&#25454;&#65292;&#32780;&#19988;&#21487;&#33021;&#20250;&#23548;&#33268;&#32570;&#20047;&#27867;&#21270;&#24615;&#30340;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#38590;&#20197;&#29702;&#35299;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#23398;&#20064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;EEG&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;EEGFormer&#65292;&#23427;&#22312;&#22823;&#35268;&#27169;&#22797;&#21512;EEG&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#36866;&#24212;&#24615;&#33021;&#30340;EEG&#20449;&#21495;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30693;&#35782;&#36741;&#21161;&#30340;&#21452;&#38454;&#27573;&#36827;&#21270;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#21407;&#27833;&#35843;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#20840;&#23616;&#25628;&#32034;&#21644;&#23616;&#37096;&#20248;&#21270;&#25913;&#36827;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10274</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#36741;&#21161;&#30340;&#22823;&#35268;&#27169;&#21407;&#27833;&#35843;&#24230;&#30340;&#21452;&#38454;&#27573;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Assisted Dual-Stage Evolutionary Optimization of Large-Scale Crude Oil Scheduling. (arXiv:2401.10274v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10274
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#36741;&#21161;&#30340;&#21452;&#38454;&#27573;&#36827;&#21270;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#21407;&#27833;&#35843;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#20840;&#23616;&#25628;&#32034;&#21644;&#23616;&#37096;&#20248;&#21270;&#25913;&#36827;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#28860;&#27833;&#21378;&#21407;&#27833;&#35843;&#24230;&#30340;&#35268;&#27169;&#25193;&#22823;&#65292;&#20986;&#29616;&#20102;&#20855;&#26377;&#25968;&#21315;&#20010;&#20108;&#36827;&#21046;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#21407;&#27833;&#35843;&#24230;&#38382;&#39064;&#65288;LSCOSPs&#65289;&#65292;&#36825;&#23545;&#20110;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;LSCOSPs&#65292;&#25105;&#20204;&#20197;&#28023;&#19978;&#36827;&#21475;&#28860;&#27833;&#21378;&#30340;&#23454;&#38469;&#21407;&#27833;&#35843;&#24230;&#20026;&#20363;&#65292;&#20174;&#21407;&#27833;&#21368;&#36135;&#12289;&#36816;&#36755;&#12289;&#21407;&#27833;&#33976;&#39311;&#35013;&#32622;&#21152;&#24037;&#21644;&#20013;&#38388;&#20135;&#21697;&#24211;&#23384;&#31649;&#29702;&#31561;&#26041;&#38754;&#23545;LSCOSPs&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#21551;&#21457;&#24335;&#35268;&#21017;&#39537;&#21160;&#30340;&#21452;&#38454;&#27573;&#36827;&#21270;&#31639;&#27861;&#65288;&#31616;&#31216;DSEA/HR&#65289;&#65292;&#20854;&#20013;&#21452;&#38454;&#27573;&#25628;&#32034;&#26426;&#21046;&#21253;&#25324;&#20840;&#23616;&#25628;&#32034;&#21644;&#23616;&#37096;&#20248;&#21270;&#12290;&#22312;&#20840;&#23616;&#25628;&#32034;&#38454;&#27573;&#65292;&#25105;&#20204;&#26681;&#25454;&#32463;&#39564;&#25805;&#20316;&#30693;&#35782;&#35774;&#35745;&#20102;&#20960;&#20010;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#22312;&#28151;&#21512;&#21464;&#37327;&#31354;&#38388;&#20013;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#30340;&#21021;&#22987;&#31181;&#32676;&#65292;&#24182;&#21152;&#36895;&#25910;&#25947;&#12290;&#22312;&#23616;&#37096;&#20248;&#21270;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#20462;&#22797;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the scaling up of crude oil scheduling in modern refineries, large-scale crude oil scheduling problems (LSCOSPs) emerge with thousands of binary variables and non-linear constraints, which are challenging to be optimized by traditional optimization methods. To solve LSCOSPs, we take the practical crude oil scheduling from a marine-access refinery as an example and start with modeling LSCOSPs from crude unloading, transportation, crude distillation unit processing, and inventory management of intermediate products. On the basis of the proposed model, a dual-stage evolutionary algorithm driven by heuristic rules (denoted by DSEA/HR) is developed, where the dual-stage search mechanism consists of global search and local refinement. In the global search stage, we devise several heuristic rules based on the empirical operating knowledge to generate a well-performing initial population and accelerate convergence in the mixed variables space. In the local refinement stage, a repair strat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25209;&#21028;&#24615;&#27010;&#36848;&#20102;&#21046;&#33647;&#34892;&#19994;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#37325;&#22823;&#36827;&#23637;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#31561;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#21046;&#33647;&#36816;&#33829;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.10273</link><description>&lt;p&gt;
&#38761;&#26032;&#21046;&#33647;&#19994;&#65306;&#25581;&#24320;&#33647;&#29289;&#34892;&#19994;&#20013;&#20154;&#24037;&#26234;&#33021;&#21644;&#27861;&#24459;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Pharma: Unveiling the AI and LLM Trends in the Pharmaceutical Industry. (arXiv:2401.10273v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10273
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25209;&#21028;&#24615;&#27010;&#36848;&#20102;&#21046;&#33647;&#34892;&#19994;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#37325;&#22823;&#36827;&#23637;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#31561;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#21046;&#33647;&#36816;&#33829;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21046;&#33647;&#34892;&#19994;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#37325;&#22823;&#36827;&#23637;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#27010;&#36848;&#12290;&#35814;&#32454;&#20171;&#32461;&#20102;&#20854;&#22312;&#30740;&#21457;&#12289;&#21160;&#29289;&#27979;&#35797;&#12289;&#20020;&#24202;&#35797;&#39564;&#12289;&#21307;&#38498;&#20020;&#24202;&#38454;&#27573;&#12289;&#29983;&#20135;&#12289;&#30417;&#31649;&#20107;&#21153;&#12289;&#36136;&#37327;&#25511;&#21046;&#21644;&#20854;&#20182;&#25903;&#25345;&#39046;&#22495;&#31561;&#20851;&#38190;&#36816;&#33829;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#25991;&#31456;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#27599;&#20010;&#39046;&#22495;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#31561;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#21046;&#33647;&#36816;&#33829;&#21508;&#20010;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#36825;&#19968;&#20840;&#38754;&#20998;&#26512;&#65292;&#25991;&#31456;&#31361;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#37325;&#22609;&#21046;&#33647;&#19994;&#26410;&#26469;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document offers a critical overview of the emerging trends and significant advancements in artificial intelligence (AI) within the pharmaceutical industry. Detailing its application across key operational areas, including research and development, animal testing, clinical trials, hospital clinical stages, production, regulatory affairs, quality control and other supporting areas, the paper categorically examines AI's role in each sector. Special emphasis is placed on cutting-edge AI technologies like machine learning algorithms and their contributions to various aspects of pharmaceutical operations. Through this comprehensive analysis, the paper highlights the transformative potential of AI in reshaping the pharmaceutical industry's future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#21512;&#22810;&#28304;&#26799;&#24230;&#24046;&#24322;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#37030;&#22495;&#27867;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22495;&#20869;&#21644;&#22495;&#38388;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#24335;&#65292;&#20943;&#23569;&#20102;&#26469;&#33258;&#19981;&#21516;&#28304;&#22495;&#30340;&#25968;&#25454;&#38548;&#31163;&#25152;&#24102;&#26469;&#30340;&#22495;&#24046;&#36317;&#65292;&#20174;&#32780;&#20351;&#24471;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#26410;&#30693;&#22495;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10272</link><description>&lt;p&gt;
&#32852;&#21512;&#22810;&#28304;&#26799;&#24230;&#24046;&#24322;&#26368;&#23567;&#21270;&#29992;&#20110;&#32852;&#37030;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Collaborative Gradient Discrepancy Minimization for Federated Domain Generalization. (arXiv:2401.10272v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#21512;&#22810;&#28304;&#26799;&#24230;&#24046;&#24322;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#37030;&#22495;&#27867;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22495;&#20869;&#21644;&#22495;&#38388;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#24335;&#65292;&#20943;&#23569;&#20102;&#26469;&#33258;&#19981;&#21516;&#28304;&#22495;&#30340;&#25968;&#25454;&#38548;&#31163;&#25152;&#24102;&#26469;&#30340;&#22495;&#24046;&#36317;&#65292;&#20174;&#32780;&#20351;&#24471;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#26410;&#30693;&#22495;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#22495;&#27867;&#21270;&#26088;&#22312;&#20174;&#22810;&#20010;&#20998;&#25955;&#30340;&#28304;&#22495;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#27169;&#22411;&#65292;&#20197;&#22312;&#26410;&#30693;&#30340;&#30446;&#26631;&#22495;&#19978;&#36827;&#34892;&#37096;&#32626;&#12290;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#26469;&#33258;&#19981;&#21516;&#28304;&#22495;&#30340;&#25968;&#25454;&#34987;&#20445;&#25345;&#38548;&#31163;&#65292;&#36825;&#22312;&#24357;&#21512;&#22495;&#24046;&#36317;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#28304;&#26799;&#24230;&#24046;&#24322;&#26368;&#23567;&#21270;&#65288;MCGDM&#65289;&#26041;&#27861;&#29992;&#20110;&#32852;&#37030;&#22495;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22987;&#22270;&#20687;&#21644;&#22686;&#24378;&#22270;&#20687;&#20043;&#38388;&#30340;&#22495;&#20869;&#26799;&#24230;&#21305;&#37197;&#65292;&#20197;&#36991;&#20813;&#23545;&#38548;&#31163;&#22495;&#20869;&#30340;&#22495;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#20854;&#20182;&#22495;&#30340;&#21327;&#20316;&#19979;&#36827;&#34892;&#22495;&#38388;&#26799;&#24230;&#21305;&#37197;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#20998;&#25955;&#22495;&#20043;&#38388;&#30340;&#22495;&#20559;&#31227;&#12290;&#36890;&#36807;&#32467;&#21512;&#22495;&#20869;&#21644;&#22495;&#38388;&#26799;&#24230;&#21305;&#37197;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#26410;&#30693;&#22495;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#32852;&#37030;&#22495;&#36866;&#24212;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Domain Generalization aims to learn a domain-invariant model from multiple decentralized source domains for deployment on unseen target domain. Due to privacy concerns, the data from different source domains are kept isolated, which poses challenges in bridging the domain gap. To address this issue, we propose a Multi-source Collaborative Gradient Discrepancy Minimization (MCGDM) method for federated domain generalization. Specifically, we propose intra-domain gradient matching between the original images and augmented images to avoid overfitting the domain-specific information within isolated domains. Additionally, we propose inter-domain gradient matching with the collaboration of other domains, which can further reduce the domain shift across decentralized domains. Combining intra-domain and inter-domain gradient matching, our method enables the learned model to generalize well on unseen domains. Furthermore, our method can be extended to the federated domain adaptation ta
&lt;/p&gt;</description></item><item><title>&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#22242;&#38431;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#26377;&#26356;&#39640;&#30340;&#20851;&#27880;&#24230;&#21644;&#24341;&#29992;&#29575;&#65292;&#19988;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#32780;&#23398;&#26415;&#30028;&#30340;&#22242;&#38431;&#21017;&#26356;&#20542;&#21521;&#20110;&#20135;&#29983;&#20855;&#26377;&#26356;&#39640;&#31243;&#24230;&#21019;&#26032;&#30340;&#24037;&#20316;&#65292;&#20986;&#29616;&#38750;&#24120;&#19981;&#23547;&#24120;&#21644;&#20856;&#22411;&#30340;&#35770;&#25991;&#12290;&#36825;&#31181;&#24433;&#21709;&#21147;-&#21019;&#26032;&#24230;&#20248;&#21183;&#22312;&#19981;&#21516;&#39046;&#22495;&#12289;&#22242;&#38431;&#35268;&#27169;&#12289;&#36164;&#21382;&#21644;&#22768;&#26395;&#19979;&#22343;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2401.10268</link><description>&lt;p&gt;
&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#20114;&#34917;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
The complementary contributions of academia and industry to AI research. (arXiv:2401.10268v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10268
&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#22242;&#38431;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#26377;&#26356;&#39640;&#30340;&#20851;&#27880;&#24230;&#21644;&#24341;&#29992;&#29575;&#65292;&#19988;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#32780;&#23398;&#26415;&#30028;&#30340;&#22242;&#38431;&#21017;&#26356;&#20542;&#21521;&#20110;&#20135;&#29983;&#20855;&#26377;&#26356;&#39640;&#31243;&#24230;&#21019;&#26032;&#30340;&#24037;&#20316;&#65292;&#20986;&#29616;&#38750;&#24120;&#19981;&#23547;&#24120;&#21644;&#20856;&#22411;&#30340;&#35770;&#25991;&#12290;&#36825;&#31181;&#24433;&#21709;&#21147;-&#21019;&#26032;&#24230;&#20248;&#21183;&#22312;&#19981;&#21516;&#39046;&#22495;&#12289;&#22242;&#38431;&#35268;&#27169;&#12289;&#36164;&#21382;&#21644;&#22768;&#26395;&#19979;&#22343;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#20013;&#37117;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#30028;&#36817;&#26399;&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#23398;&#26415;&#30740;&#31350;&#22312;&#35813;&#39046;&#22495;&#20013;&#30340;&#20316;&#29992;&#30340;&#26032;&#35270;&#35282;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#36807;&#21435;25&#24180;&#20013;&#20004;&#20010;&#29615;&#22659;&#20013;&#20135;&#29983;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#24433;&#21709;&#21644;&#31867;&#22411;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#31181;&#27169;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#24037;&#19994;&#30028;&#30740;&#31350;&#20154;&#21592;&#32452;&#25104;&#30340;&#22242;&#38431;&#21457;&#34920;&#30340;&#25991;&#31456;&#24448;&#24448;&#26356;&#21463;&#20851;&#27880;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#34987;&#24341;&#29992;&#21644;&#24341;&#21457;&#24341;&#29992;&#39072;&#35206;&#30340;&#21487;&#33021;&#24615;&#65292;&#19988;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#32431;&#23398;&#26415;&#22242;&#38431;&#21457;&#34920;&#20102;&#22823;&#37096;&#20998;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#65292;&#24182;&#20542;&#21521;&#20110;&#20135;&#29983;&#26356;&#39640;&#31243;&#24230;&#30340;&#21019;&#26032;&#24037;&#20316;&#65292;&#21333;&#31687;&#35770;&#25991;&#26377;&#25968;&#20493;&#30340;&#21487;&#33021;&#24615;&#26159;&#38750;&#24120;&#19981;&#23547;&#24120;&#21644;&#20856;&#22411;&#30340;&#12290;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#22312;&#24433;&#21709;&#21147;-&#21019;&#26032;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#19981;&#21463;&#23376;&#39046;&#22495;&#12289;&#22242;&#38431;&#35268;&#27169;&#12289;&#36164;&#21382;&#21644;&#22768;&#26395;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#23398;&#26415;&#30028;&#20135;&#29983;&#20102;&#26356;&#22810;&#32508;&#36848;&#21644;&#20998;&#26512;&#22411;&#35770;&#25991;&#65292;&#32780;&#24037;&#19994;&#30028;&#21017;&#26356;&#22810;&#22320;&#27880;&#37325;&#24212;&#29992;&#21644;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has seen tremendous development in industry and academia. However, striking recent advances by industry have stunned the world, inviting a fresh perspective on the role of academic research in this field. Here, we characterize the impact and type of AI produced by both environments over the last 25 years and establish several patterns. We find that articles published by teams consisting exclusively of industry researchers tend to get greater attention, with a higher chance of being highly cited and citation-disruptive, and several times more likely to produce state-of-the-art models. In contrast, we find that exclusively academic teams publish the bulk of AI research and tend to produce higher novelty work, with single papers having several times higher likelihood of being unconventional and atypical. The respective impact-novelty advantages of industry and academia are robust to controls for subfield, team size, seniority, and prestige. We find that academ
&lt;/p&gt;</description></item><item><title>HyperSense&#26159;&#19968;&#20010;&#21327;&#21516;&#35774;&#35745;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#29289;&#20307;&#23384;&#22312;&#39044;&#27979;&#26377;&#25928;&#22320;&#25511;&#21046;&#25968;&#25454;&#29983;&#25104;&#36895;&#29575;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31934;&#24230;ADC&#20943;&#23569;&#20887;&#20313;&#25968;&#25454;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25104;&#26412;&#65292;&#21033;&#29992;&#36229;&#32500;&#24230;&#35745;&#31639;&#30340;&#29305;&#28857;&#20998;&#26512;&#23454;&#26102;&#30340;&#20302;&#31934;&#24230;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#12289;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#21644;&#23454;&#26102;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39640;&#24615;&#33021;&#30340;&#29289;&#20307;&#26816;&#27979;&#36719;&#20214;&#21644;&#23454;&#26102;&#30340;&#30828;&#20214;&#39044;&#27979;&#65292;&#24341;&#20837;&#20102;&#26234;&#33021;&#20256;&#24863;&#22120;&#25511;&#21046;&#30340;&#26032;&#27010;&#24565;&#12290;&#22312;&#36719;&#20214;&#21644;&#30828;&#20214;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10267</link><description>&lt;p&gt;
HyperSense: &#21152;&#36895;&#36229;&#32500;&#24230;&#35745;&#31639;&#20197;&#29992;&#20110;&#26234;&#33021;&#20256;&#24863;&#22120;&#25968;&#25454;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
HyperSense: Accelerating Hyper-Dimensional Computing for Intelligent Sensor Data Processing. (arXiv:2401.10267v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10267
&lt;/p&gt;
&lt;p&gt;
HyperSense&#26159;&#19968;&#20010;&#21327;&#21516;&#35774;&#35745;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#29289;&#20307;&#23384;&#22312;&#39044;&#27979;&#26377;&#25928;&#22320;&#25511;&#21046;&#25968;&#25454;&#29983;&#25104;&#36895;&#29575;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31934;&#24230;ADC&#20943;&#23569;&#20887;&#20313;&#25968;&#25454;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25104;&#26412;&#65292;&#21033;&#29992;&#36229;&#32500;&#24230;&#35745;&#31639;&#30340;&#29305;&#28857;&#20998;&#26512;&#23454;&#26102;&#30340;&#20302;&#31934;&#24230;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#12289;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#21644;&#23454;&#26102;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39640;&#24615;&#33021;&#30340;&#29289;&#20307;&#26816;&#27979;&#36719;&#20214;&#21644;&#23454;&#26102;&#30340;&#30828;&#20214;&#39044;&#27979;&#65292;&#24341;&#20837;&#20102;&#26234;&#33021;&#20256;&#24863;&#22120;&#25511;&#21046;&#30340;&#26032;&#27010;&#24565;&#12290;&#22312;&#36719;&#20214;&#21644;&#30828;&#20214;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;HyperSense&#65292;&#25105;&#20204;&#21327;&#21516;&#35774;&#35745;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#31995;&#32479;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#29289;&#20307;&#23384;&#22312;&#39044;&#27979;&#26377;&#25928;&#22320;&#25511;&#21046;&#27169;&#25311;&#21040;&#25968;&#23383;&#36716;&#25442;&#22120;&#65288;ADC&#65289;&#27169;&#22359;&#30340;&#25968;&#25454;&#29983;&#25104;&#36895;&#29575;&#12290;&#38024;&#23545;&#19981;&#26029;&#22686;&#21152;&#30340;&#20256;&#24863;&#22120;&#25968;&#37327;&#21644;&#25968;&#25454;&#36895;&#29575;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;HyperSense&#20351;&#29992;&#39640;&#25928;&#30340;&#20302;&#31934;&#24230;ADC&#20943;&#23569;&#20887;&#20313;&#30340;&#25968;&#23383;&#25968;&#25454;&#65292;&#38477;&#20302;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25104;&#26412;&#12290;&#21033;&#29992;&#31070;&#32463;&#21551;&#21457;&#30340;&#36229;&#32500;&#24230;&#35745;&#31639;&#65288;HDC&#65289;&#65292;HyperSense&#20998;&#26512;&#23454;&#26102;&#30340;&#21407;&#22987;&#20302;&#31934;&#24230;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#12289;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#21644;&#23454;&#26102;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;HyperSense&#27169;&#22411;&#23558;&#39640;&#24615;&#33021;&#30340;&#29289;&#20307;&#26816;&#27979;&#36719;&#20214;&#19982;&#23454;&#26102;&#30340;&#30828;&#20214;&#39044;&#27979;&#32467;&#21512;&#36215;&#26469;&#65292;&#24341;&#20837;&#20102;&#26234;&#33021;&#20256;&#24863;&#22120;&#25511;&#21046;&#30340;&#26032;&#27010;&#24565;&#12290;&#20840;&#38754;&#30340;&#36719;&#20214;&#21644;&#30828;&#20214;&#35780;&#20272;&#23637;&#31034;&#20102;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#36890;&#36807;&#26368;&#39640;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#21644;&#26368;&#38497;&#30340;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24615;&#65288;ROC&#65289;&#26354;&#32447;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing HyperSense, our co-designed hardware and software system efficiently controls Analog-to-Digital Converter (ADC) modules' data generation rate based on object presence predictions in sensor data. Addressing challenges posed by escalating sensor quantities and data rates, HyperSense reduces redundant digital data using energy-efficient low-precision ADC, diminishing machine learning system costs. Leveraging neurally-inspired HyperDimensional Computing (HDC), HyperSense analyzes real-time raw low-precision sensor data, offering advantages in handling noise, memory-centricity, and real-time learning.  Our proposed HyperSense model combines high-performance software for object detection with real-time hardware prediction, introducing the novel concept of Intelligent Sensor Control. Comprehensive software and hardware evaluations demonstrate our solution's superior performance, evidenced by the highest Area Under the Curve (AUC) and sharpest Receiver Operating Characteristic (ROC
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;Tennessee Eastman Process&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#21183;&#12290;&#36824;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#26080;&#26631;&#35760;&#26679;&#26412;&#31561;&#25361;&#25112;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#24212;&#23545;&#12290;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;Tennessee Eastman Process&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;</title><link>http://arxiv.org/abs/2401.10266</link><description>&lt;p&gt;
&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;: &#26041;&#27861;&#35770;&#21644;&#19981;&#30830;&#23450;&#24615;&#31649;&#29702;&#31574;&#30053;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies. (arXiv:2401.10266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;Tennessee Eastman Process&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#21183;&#12290;&#36824;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#26080;&#26631;&#35760;&#26679;&#26412;&#31561;&#25361;&#25112;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#24212;&#23545;&#12290;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;Tennessee Eastman Process&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#30417;&#27979;&#22312;&#29616;&#20195;&#24037;&#19994;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#26085;&#30410;&#21463;&#21040;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#20851;&#27880;&#30340;&#22686;&#38271;&#20027;&#39064;&#21644;&#19968;&#31181;&#24378;&#22823;&#30340;&#25925;&#38556;&#35782;&#21035;&#26041;&#24335;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#24320;&#28304;&#22522;&#20934;Tennessee Eastman Process&#65288;TEP&#65289;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#24635;&#32467;&#20102;&#29992;&#20110;&#24037;&#19994;&#21378;&#25151;&#29366;&#24577;&#30417;&#27979;&#12289;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#27599;&#31181;&#31639;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#36824;&#28085;&#30422;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#12289;&#26080;&#26631;&#35760;&#26679;&#26412;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#27604;&#36739;&#20102;&#21033;&#29992;Tennessee Eastman Process&#30340;&#19981;&#21516;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Condition monitoring plays a significant role in the safety and reliability of modern industrial systems. Artificial intelligence (AI) approaches are gaining attention from academia and industry as a growing subject in industrial applications and as a powerful way of identifying faults. This paper provides an overview of intelligent condition monitoring and fault detection and diagnosis methods for industrial plants with a focus on the open-source benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and state-of-the-art deep learning (DL) and machine learning (ML) algorithms for industrial plant condition monitoring, fault detection, and diagnosis are summarized and the advantages and disadvantages of each algorithm are studied. Challenges like imbalanced data, unlabelled samples and how deep learning models can handle them are also covered. Finally, a comparison of the accuracies and specifications of different algorithms utilizing the Tennessee Eastman Process 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36879;&#26126;&#30340;&#23398;&#20064;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#20010;&#20307;&#23398;&#29983;&#22312;&#38754;&#23545;&#38754;&#21327;&#20316;&#23398;&#20064;&#20013;&#30340;&#21442;&#19982;&#24230;&#65292;&#20026;&#20010;&#24615;&#21270;&#25903;&#25345;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10264</link><description>&lt;p&gt;
&#36879;&#26126;&#23398;&#20064;&#20998;&#26512;&#22312;&#38754;&#23545;&#38754;&#21327;&#20316;&#23398;&#20064;&#20013;&#30340;&#20010;&#24615;&#21270;&#25903;&#25345;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Harnessing Transparent Learning Analytics for Individualized Support through Auto-detection of Engagement in Face-to-Face Collaborative Learning. (arXiv:2401.10264v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10264
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36879;&#26126;&#30340;&#23398;&#20064;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#20010;&#20307;&#23398;&#29983;&#22312;&#38754;&#23545;&#38754;&#21327;&#20316;&#23398;&#20064;&#20013;&#30340;&#21442;&#19982;&#24230;&#65292;&#20026;&#20010;&#24615;&#21270;&#25903;&#25345;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#23398;&#20064;&#20998;&#26512;&#30740;&#31350;&#21644;&#25903;&#25345;&#21327;&#20316;&#23398;&#20064;&#24050;&#26377;&#22810;&#24180;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#22312;&#24314;&#27169;&#21644;&#39044;&#27979;&#21327;&#20316;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#23398;&#29983;&#21442;&#19982;&#21644;&#34920;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#20998;&#26512;&#35774;&#35745;&#21644;&#23454;&#26045;&#20013;&#20351;&#29992;&#20102;&#8220;&#40657;&#30418;&#8221;&#26041;&#27861;&#30340;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25945;&#23398;&#21644;&#23398;&#20064;&#23454;&#36341;&#30340;&#25351;&#23548;&#21487;&#33021;&#20250;&#25104;&#20026;&#19968;&#20010;&#25361;&#25112;&#12290;&#19968;&#26041;&#38754;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#27169;&#22411;&#21019;&#24314;&#30340;&#40657;&#30418;&#38459;&#27490;&#29992;&#25143;&#33719;&#24471;&#25945;&#32946;&#19978;&#26377;&#24847;&#20041;&#30340;&#23398;&#20064;&#21644;&#25945;&#23398;&#24314;&#35758;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20165;&#20851;&#27880;&#32676;&#20307;&#21644;&#38431;&#21015;&#32423;&#21035;&#30340;&#20998;&#26512;&#21487;&#33021;&#20250;&#38590;&#20197;&#20026;&#21327;&#20316;&#23567;&#32452;&#20013;&#30340;&#20010;&#21035;&#23398;&#29983;&#25552;&#20379;&#20855;&#20307;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#26816;&#27979;&#23398;&#29983;&#22312;&#36807;&#31243;&#20013;&#30340;&#20010;&#20307;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using learning analytics to investigate and support collaborative learning has been explored for many years. Recently, automated approaches with various artificial intelligence approaches have provided promising results for modelling and predicting student engagement and performance in collaborative learning tasks. However, due to the lack of transparency and interpretability caused by the use of "black box" approaches in learning analytics design and implementation, guidance for teaching and learning practice may become a challenge. On the one hand, the black box created by machine learning algorithms and models prevents users from obtaining educationally meaningful learning and teaching suggestions. On the other hand, focusing on group and cohort level analysis only can make it difficult to provide specific support for individual students working in collaborative groups. This paper proposes a transparent approach to automatically detect student's individual engagement in the process 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#31354;&#38388;&#29305;&#24615;&#65292;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#24369;&#28857;&#65292;&#24182;&#38024;&#23545;&#27492;&#24369;&#28857;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#38544;&#20889;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10262</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#31354;&#38388;&#29305;&#24615;&#21450;&#20854;&#22312;&#22270;&#20687;&#38544;&#20889;&#26415;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Null Space Properties of Neural Networks with Applications to Image Steganography. (arXiv:2401.10262v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#31354;&#38388;&#29305;&#24615;&#65292;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#24369;&#28857;&#65292;&#24182;&#38024;&#23545;&#27492;&#24369;&#28857;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#38544;&#20889;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#31354;&#38388;&#29305;&#24615;&#12290;&#25105;&#20204;&#23558;&#38646;&#31354;&#38388;&#23450;&#20041;&#20174;&#32447;&#24615;&#26144;&#23556;&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#24182;&#35752;&#35770;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#38646;&#31354;&#38388;&#30340;&#23384;&#22312;&#12290;&#32473;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#31354;&#38388;&#21487;&#20197;&#21578;&#35785;&#25105;&#20204;&#21738;&#20123;&#36755;&#20837;&#25968;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#27809;&#26377;&#20219;&#20309;&#36129;&#29486;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#23427;&#26469;&#27450;&#39575;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22266;&#26377;&#24369;&#28857;&#65292;&#21487;&#20197;&#34987;&#21033;&#29992;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#24212;&#29992;&#65292;&#21363;&#22270;&#20687;&#38544;&#20889;&#26415;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#35832;&#22914;MNIST&#20043;&#31867;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#38646;&#31354;&#38388;&#25104;&#20998;&#26469;&#24378;&#21046;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#25152;&#36873;&#30340;&#38544;&#34255;&#22270;&#20687;&#31867;&#65292;&#21363;&#20351;&#25972;&#20307;&#22270;&#20687;&#30475;&#36215;&#26469;&#23436;&#20840;&#19981;&#21516;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#26174;&#31034;&#20154;&#31867;&#35266;&#23519;&#32773;&#33021;&#22815;&#30475;&#21040;&#30340;&#20869;&#23481;&#19982;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#22270;&#20687;&#37096;&#20998;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#35266;&#27979;&#8221;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the null space properties of neural networks. We extend the null space definition from linear to nonlinear maps and discuss the presence of a null space in neural networks. The null space of a given neural network can tell us the part of the input data that makes no contribution to the final prediction so that we can use it to trick the neural network. This reveals an inherent weakness in neural networks that can be exploited. One application described here leads to a method of image steganography. Through experiments on image datasets such as MNIST, we show that we can use null space components to force the neural network to choose a selected hidden image class, even though the overall image can be made to look like a completely different image. We conclude by showing comparisons between what a human viewer would see, and the part of the image that the neural network is actually using to make predictions and, hence, show that what the neural network ``sees'' is com
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31574;&#30053;&#65292;&#22312;&#21516;&#27493;&#35757;&#32451;&#35821;&#20041;&#19979;&#25104;&#21151;&#23454;&#29616;&#20102;&#38646;&#31649;&#36947;&#27873;&#27819;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#35745;&#31639;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#35774;&#35745;&#20102;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#30340;&#26032;&#39062;&#31649;&#36947;&#35843;&#24230;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25214;&#21040;&#26368;&#20248;&#35843;&#24230;&#30340;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#26032;&#39062;&#25216;&#26415;&#32469;&#36807;&#21516;&#27493;&#25805;&#20316;&#23454;&#29616;&#38646;&#27873;&#27819;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30456;&#20284;&#26465;&#20214;&#19979;&#65292;&#26412;&#26041;&#27861;&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#20248;&#20110;1F1B&#35843;&#24230;23%&#12290;</title><link>http://arxiv.org/abs/2401.10241</link><description>&lt;p&gt;
&#38646;&#27873;&#27819;&#31649;&#36947;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Zero Bubble Pipeline Parallelism. (arXiv:2401.10241v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31574;&#30053;&#65292;&#22312;&#21516;&#27493;&#35757;&#32451;&#35821;&#20041;&#19979;&#25104;&#21151;&#23454;&#29616;&#20102;&#38646;&#31649;&#36947;&#27873;&#27819;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#35745;&#31639;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#35774;&#35745;&#20102;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#30340;&#26032;&#39062;&#31649;&#36947;&#35843;&#24230;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25214;&#21040;&#26368;&#20248;&#35843;&#24230;&#30340;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#26032;&#39062;&#25216;&#26415;&#32469;&#36807;&#21516;&#27493;&#25805;&#20316;&#23454;&#29616;&#38646;&#27873;&#27819;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30456;&#20284;&#26465;&#20214;&#19979;&#65292;&#26412;&#26041;&#27861;&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#20248;&#20110;1F1B&#35843;&#24230;23%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#36947;&#24182;&#34892;&#24615;&#26159;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#28982;&#32780;&#20854;&#25928;&#29575;&#21463;&#21040;&#34987;&#35270;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#31649;&#36947;&#27873;&#27819;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35843;&#24230;&#31574;&#30053;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#22312;&#21516;&#27493;&#35757;&#32451;&#35821;&#20041;&#19979;&#25104;&#21151;&#23454;&#29616;&#38646;&#31649;&#36947;&#27873;&#27819;&#12290;&#36825;&#20010;&#25913;&#36827;&#32972;&#21518;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#21453;&#21521;&#35745;&#31639;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#19968;&#37096;&#20998;&#35745;&#31639;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#21478;&#19968;&#37096;&#20998;&#35745;&#31639;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22522;&#20110;&#36825;&#20010;&#24605;&#24819;&#65292;&#25105;&#20204;&#25163;&#24037;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#31649;&#36947;&#35843;&#24230;&#26041;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#26681;&#25454;&#29305;&#23450;&#30340;&#27169;&#22411;&#37197;&#32622;&#21644;&#20869;&#23384;&#38480;&#21046;&#33258;&#21160;&#25214;&#21040;&#26368;&#20248;&#35843;&#24230;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30495;&#27491;&#23454;&#29616;&#38646;&#27873;&#27819;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#22312;&#20248;&#21270;&#22120;&#27493;&#39588;&#20013;&#32469;&#36807;&#21516;&#27493;&#25805;&#20316;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#27604;1F1B&#35843;&#24230;&#39640;&#20986;&#22810;&#36798;23%&#12290;
&lt;/p&gt;
&lt;p&gt;
Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 23% in throughput under a simila
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.09691</link><description>&lt;p&gt;
&#23558;&#22270;&#20687;&#29305;&#24449;&#36755;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning Inputting Image Feature to Each Layer of Neural Network. (arXiv:2401.09691v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#24182;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#12290;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#22312;&#20351;&#29992;&#30701;&#37319;&#26679;&#21608;&#26399;&#26102;&#26080;&#24847;&#20013;&#24573;&#30053;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#36755;&#20986;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#32435;&#20837;&#21040;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#21644;&#20851;&#33410;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#31616;&#21333;&#30340;&#25342;&#21462;&#25918;&#32622;&#25805;&#20316;&#30340;&#23454;&#39564;&#65292;&#21363;&#20351;&#22788;&#29702;&#26469;&#33258;&#30701;&#37319;&#26679;&#21608;&#26399;&#30340;&#25968;&#25454;&#65292;&#20063;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39118;&#26684;&#23545;&#40784;&#65292;&#36991;&#20813;&#20154;&#31867;&#24178;&#39044;&#65292;&#24182;&#25104;&#21151;&#22521;&#20859;&#20986;&#21487;&#21462;&#34892;&#20026;&#21644;&#20943;&#36731;&#19981;&#21487;&#21462;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.09566</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39118;&#26684;&#23545;&#40784;&#65292;&#36991;&#20813;&#20154;&#31867;&#24178;&#39044;&#65292;&#24182;&#25104;&#21151;&#22521;&#20859;&#20986;&#21487;&#21462;&#34892;&#20026;&#21644;&#20943;&#36731;&#19981;&#21487;&#21462;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#27493;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#36830;&#36143;&#19988;&#28085;&#30422;&#24191;&#27867;&#20027;&#39064;&#30340;&#25991;&#26412;&#34917;&#20840;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35757;&#32451;&#25152;&#38656;&#30340;&#22823;&#37327;&#25968;&#25454;&#20351;&#24471;&#22312;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#23545;&#40784;&#21709;&#24212;&#39118;&#26684;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#20250;&#37319;&#29992;&#39069;&#22806;&#30340;&#23545;&#40784;&#38454;&#27573;&#65292;&#36827;&#19968;&#27493;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#26356;&#22909;&#22320;&#23558;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#12290;&#34429;&#28982;&#36825;&#20010;&#36807;&#31243;&#26412;&#36523;&#24182;&#27809;&#26377;&#24341;&#20837;&#26032;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#31361;&#20986;&#20102;&#27169;&#22411;&#22266;&#26377;&#30340;&#29983;&#25104;&#39118;&#26684;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#26694;&#26550;&#20869;&#21033;&#29992;&#21453;&#20107;&#23454;&#25552;&#31034;&#26469;&#23545;&#40784;&#27169;&#22411;&#30340;&#39118;&#26684;&#65292;&#32780;&#19981;&#20381;&#36182;&#20154;&#31867;&#24178;&#39044;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#22521;&#20859;&#20102;&#21487;&#21462;&#30340;&#34892;&#20026;&#65292;&#20943;&#36731;&#20102;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and
&lt;/p&gt;</description></item><item><title>CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;</title><link>http://arxiv.org/abs/2401.08897</link><description>&lt;p&gt;
CFASL&#65306;&#29992;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#30340;&#22797;&#21512;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08897
&lt;/p&gt;
&lt;p&gt;
CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#21644;&#28508;&#22312;&#21521;&#37327;&#30340;&#23545;&#31216;&#24615;&#20026;VAE&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29978;&#33267;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20063;&#38656;&#35201;&#24050;&#30693;&#30340;&#22240;&#23376;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Composite Factor-Aligned Symmetry Learning (CFASL)&#65292;&#23558;&#20854;&#38598;&#25104;&#21040;VAE&#20013;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#19981;&#38656;&#35201;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;CFASL&#21253;&#25324;&#19977;&#20010;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#30340;&#26032;&#29305;&#24449;&#65306;1)&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#23558;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#23545;&#40784;&#21040;&#26126;&#30830;&#21487;&#23398;&#20064;&#30340;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65307;2)&#23398;&#20064;&#19968;&#20010;&#22797;&#21512;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65292;&#26469;&#34920;&#36798;&#20004;&#20010;&#38543;&#26426;&#26679;&#26412;&#20043;&#38388;&#30340;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65307;3)&#22312;&#35757;&#32451;VAE&#26102;&#65292;&#24341;&#20837;&#20855;&#26377;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#20004;&#20010;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metri
&lt;/p&gt;</description></item><item><title>RoTBench&#26159;&#19968;&#20010;&#22810;&#32423;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22122;&#22768;&#19979;&#34920;&#29616;&#20986;&#30340;&#31283;&#23450;&#24615;&#38656;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.08326</link><description>&lt;p&gt;
RoTBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#22810;&#32423;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning. (arXiv:2401.08326v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08326
&lt;/p&gt;
&lt;p&gt;
RoTBench&#26159;&#19968;&#20010;&#22810;&#32423;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22122;&#22768;&#19979;&#34920;&#29616;&#20986;&#30340;&#31283;&#23450;&#24615;&#38656;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#23398;&#20064;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#20114;&#21160;&#30340;&#37325;&#35201;&#25163;&#27573;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;LLMs&#22312;&#32467;&#26500;&#33391;&#22909;&#30340;&#29615;&#22659;&#20013;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#20294;&#24573;&#35270;&#20102;&#23427;&#20204;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#22122;&#22768;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RoTBench&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#40065;&#26834;&#24615;&#30340;&#22810;&#32423;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20116;&#20010;&#22806;&#37096;&#29615;&#22659;&#65292;&#27599;&#20010;&#29615;&#22659;&#37117;&#20855;&#26377;&#19981;&#21516;&#32423;&#21035;&#30340;&#22122;&#22768;&#65288;&#21363;&#28165;&#27905;&#12289;&#36731;&#24494;&#12289;&#20013;&#31561;&#12289;&#37325;&#24230;&#21644;&#32852;&#21512;&#65289;&#65292;&#23545;&#27169;&#22411;&#22312;&#24037;&#20855;&#36873;&#25321;&#12289;&#21442;&#25968;&#35782;&#21035;&#21644;&#20869;&#23481;&#22635;&#20805;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#20845;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25552;&#39640;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#36843;&#22312;&#30473;&#30571;&#12290;&#20363;&#22914;&#65292;&#24403;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#22122;&#22768;&#23384;&#22312;&#26102;&#65292;GPT-4&#30340;&#24615;&#33021;&#29978;&#33267;&#20174;80.00&#19979;&#38477;&#21040;58.10&#12290;
&lt;/p&gt;
&lt;p&gt;
Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substanti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05273</link><description>&lt;p&gt;
INACIA&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges. (arXiv:2401.05273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#65288;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36741;&#21161;&#25351;&#20196;&#31995;&#32479;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#24052;&#35199;&#32852;&#37030;&#23457;&#35745;&#27861;&#38498;&#65288;TCU&#65289;&#30340;&#36816;&#33829;&#26694;&#26550;&#20013;&#12290;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#20102;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#22522;&#26412;&#20449;&#24687;&#25552;&#21462;&#12289;&#21487;&#21463;&#29702;&#24615;&#23457;&#26597;&#12289;Periculum in mora&#21644;Fumus boni iuris&#20998;&#26512;&#20197;&#21450;&#24314;&#35758;&#29983;&#25104;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;INACIA&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12289;&#35780;&#20272;&#20854;&#21512;&#27861;&#24615;&#24182;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#30340;&#28508;&#21147;&#12290;&#21033;&#29992;&#39564;&#35777;&#25968;&#25454;&#38598;&#21644;LLMs&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;INACIA&#22788;&#29702;&#22797;&#26434;&#27861;&#24459;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#34920;&#26126;&#20854;&#36866;&#29992;&#20110;&#22686;&#21152;&#27861;&#24459;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21496;&#27861;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU). The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations. Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment. The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems. The pap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.04336</link><description>&lt;p&gt;
&#28145;&#24230;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#29992;&#20110;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#24040;&#22823;&#22270;&#36890;&#24120;&#20197;&#38750;&#20013;&#24515;&#21270;&#23376;&#22270;&#30340;&#24418;&#24335;&#30001;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#20998;&#25955;&#23384;&#20648;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#32771;&#34385;&#21040;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#65288;subgraph FL&#65289;&#22330;&#26223;&#26159;&#24456;&#33258;&#28982;&#30340;&#65292;&#20854;&#20013;&#27599;&#20010;&#26412;&#22320;&#23458;&#25143;&#31471;&#25345;&#26377;&#25972;&#20010;&#20840;&#23616;&#22270;&#30340;&#23376;&#22270;&#65292;&#20197;&#33719;&#21462;&#20840;&#23616;&#19968;&#33324;&#21270;&#30340;&#22270;&#25366;&#25496;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#32570;&#23569;&#36328;&#23376;&#22270;&#37051;&#23621;&#32780;&#23548;&#33268;&#30340;&#23616;&#37096;&#23376;&#22270;&#19978;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#32570;&#22833;&#37051;&#23621;&#29983;&#25104;&#22120;&#21644;GNN&#30340;&#32852;&#21512;FL&#26469;&#22686;&#21152;&#26412;&#22320;&#37051;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;FL&#30340;&#25928;&#29992;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#38544;&#31169;&#30446;&#26631;&#26041;&#38754;&#23384;&#22312;&#28145;&#23618;&#27425;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDEP&#26469;&#20840;&#38754;&#35299;&#20915;&#23376;&#22270;FL&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;FedDEP&#21253;&#25324;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65306;(1) &#21033;&#29992;&#28508;&#22312;&#32570;&#22833;&#37051;&#23621;&#30340;GNN&#23884;&#20837;&#36827;&#34892;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#65307;(2) Effic...
&lt;/p&gt;
&lt;p&gt;
Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Effic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31038;&#21306;&#26816;&#27979;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#31185;&#23398;&#25991;&#29486;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#31038;&#21306;&#26816;&#27979;&#30340;&#32467;&#26524;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#21512;&#65292;&#25105;&#20204;&#31361;&#30772;&#20102;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20998;&#36776;&#29575;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02542</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#26816;&#27979;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31185;&#23398;&#25991;&#29486;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Community Detection and Graph Neural Network Based Link Prediction Approach for Scientific Literature. (arXiv:2401.02542v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31038;&#21306;&#26816;&#27979;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#31185;&#23398;&#25991;&#29486;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#31038;&#21306;&#26816;&#27979;&#30340;&#32467;&#26524;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#21512;&#65292;&#25105;&#20204;&#31361;&#30772;&#20102;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20998;&#36776;&#29575;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#31185;&#23398;&#25991;&#29486;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21033;&#29992;Louvain&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#25581;&#31034;&#36825;&#20123;&#32593;&#32476;&#20013;&#30340;&#28508;&#22312;&#31038;&#21306;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;GNN&#26550;&#26500;&#20013;&#20197;&#39044;&#27979;&#28508;&#22312;&#38142;&#25509;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#29702;&#35299;&#31038;&#21306;&#21160;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21033;&#29992;&#31038;&#21306;&#26816;&#27979;&#21644;GNN&#30340;&#20248;&#21183;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#31185;&#23398;&#21512;&#20316;&#21644;&#24341;&#25991;&#20851;&#31995;&#30340;&#20108;&#20998;&#22270;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#31361;&#26174;&#20102;&#31038;&#21306;&#26816;&#27979;&#21644;GNN&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#36824;&#35299;&#20915;&#20102;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#19968;&#20123;&#26222;&#36941;&#25361;&#25112;&#65292;&#22914;&#21487;&#25193;&#23637;&#24615;&#21644;&#20998;&#36776;&#29575;&#38480;&#21046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#31038;&#21306;&#32423;&#21035;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38142;&#25509;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces an innovative approach that integrates community detection algorithms with Graph Neural Network (GNN) models to enhance link prediction in scientific literature networks. We specifically focus on the utilization of the Louvain community detection algorithm to uncover latent community structures within these networks, which are then incorporated into GNN architectures to predict potential links. Our methodology demonstrates the importance of understanding community dynamics in complex networks and leverages the strengths of both community detection and GNNs to improve predictive accuracy. Through extensive experiments on bipartite graphs representing scientific collaborations and citations, our approach not only highlights the synergy between community detection and GNNs but also addresses some of the prevalent challenges in link prediction, such as scalability and resolution limits. The results suggest that incorporating community-level information can significant
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.00110</link><description>&lt;p&gt;
&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#38752;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26469;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#28982;&#32780;&#20854;&#24778;&#20154;&#30340;&#25928;&#26524;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#28304;&#33258;&#20854;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#24863;&#30693;&#25351;&#23548;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;&#25193;&#25955;&#35757;&#32451;&#20013;&#21152;&#20837;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#30001;&#20110;&#25193;&#25955;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#20998;&#25968;&#21305;&#37197;&#30446;&#26631;&#19982;&#26080;&#30417;&#30563;&#35757;&#32451;&#24863;&#30693;&#32593;&#32476;&#26102;&#20351;&#29992;&#30340;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#30446;&#26631;&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#27492;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#24863;&#30693;&#32593;&#32476;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#24863;&#30693;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#24863;&#30693;&#30446;&#26631;&#65292;&#20854;&#32467;&#26524;&#26159;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#26465;&#20214;&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#32780;&#19981;&#19982;&#26465;&#20214;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#21464;&#21270;&#30340;&#36755;&#20837;&#25968;&#25454;&#30456;&#20301;&#32780;&#19981;&#26159;&#22266;&#23450;&#30456;&#20301;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#30456;&#20301;&#27874;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#31243;&#24230;&#21306;&#20998;&#30456;&#20301;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.16451</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#35201;&#38454;&#27573;&#22686;&#24378;&#30340;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization with Vital Phase Augmentation. (arXiv:2312.16451v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#21464;&#21270;&#30340;&#36755;&#20837;&#25968;&#25454;&#30456;&#20301;&#32780;&#19981;&#26159;&#22266;&#23450;&#30456;&#20301;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#30456;&#20301;&#27874;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#31243;&#24230;&#21306;&#20998;&#30456;&#20301;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21463;&#25439;&#30340;&#36755;&#20837;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#35757;&#32451;&#38024;&#23545;&#20110;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#31283;&#20581;&#27169;&#22411;&#12290;&#22312;&#39057;&#22495;&#20013;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#26159;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30456;&#20301;&#29305;&#24449;&#20197;&#24314;&#31435;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#21464;&#36755;&#20837;&#25968;&#25454;&#30340;&#25391;&#24133;&#32780;&#20445;&#25345;&#30456;&#20301;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22266;&#23450;&#30456;&#20301;&#20250;&#20351;&#27169;&#22411;&#23545;&#30456;&#20301;&#27874;&#21160;&#21464;&#24471;&#25935;&#24863;&#65292;&#22240;&#20026;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#20013;&#25391;&#24133;&#21644;&#30456;&#20301;&#27874;&#21160;&#24120;&#24120;&#21457;&#29983;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#36755;&#20837;&#25968;&#25454;&#30456;&#20301;&#30340;&#26377;&#38480;&#21464;&#21270;&#32780;&#19981;&#26159;&#20445;&#25345;&#22266;&#23450;&#30456;&#20301;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#31243;&#24230;&#22240;&#30456;&#20301;&#32780;&#24322;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36825;&#31181;&#31243;&#24230;&#21306;&#20998;&#30456;&#20301;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have shown remarkable performance in image classification. However, their performance significantly deteriorates with corrupted input data. Domain generalization methods have been proposed to train robust models against out-of-distribution data. Data augmentation in the frequency domain is one of such approaches that enable a model to learn phase features to establish domain-invariant representations. This approach changes the amplitudes of the input data while preserving the phases. However, using fixed phases leads to susceptibility to phase fluctuations because amplitudes and phase fluctuations commonly occur in out-of-distribution. In this study, to address this problem, we introduce an approach using finite variation of the phases of input data rather than maintaining fixed phases. Based on the assumption that the degree of domain-invariant features varies for each phase, we propose a method to distinguish phases based on this degree. In addition, we propose a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#22312;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#22270;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#19978;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2312.10401</link><description>&lt;p&gt;
&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective. (arXiv:2312.10401v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#22312;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#22270;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#19978;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#22270;&#20013;&#25429;&#25417;&#19981;&#21464;&#20449;&#24687;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#25506;&#32034;&#22270;&#30340;&#32467;&#26500;&#29702;&#35770;&#65292;&#20174;&#32780;&#22686;&#21152;&#19981;&#21464;&#20449;&#24687;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22270;&#27169;&#22411;&#26397;&#21521;&#35299;&#37322;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#38169;&#35823;&#23398;&#20064;&#65292;&#22240;&#27492;&#23398;&#20064;&#21040;&#30340;&#22122;&#22768;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#24178;&#25200;&#20102;&#22270;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#25506;&#32034;&#22270;&#30340;&#20869;&#22312;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#19978;&#36848;&#36335;&#24452;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#38416;&#26126;&#32500;&#24230;&#29702;&#35770;&#23545;&#24615;&#33021;&#25913;&#36827;&#30340;&#20869;&#22312;&#26426;&#21046;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning is a general learning paradigm excelling at capturing invariant information from diverse perturbations in graphs. Recent works focus on exploring the structural rationale from graphs, thereby increasing the discriminability of the invariant information. However, such methods may incur in the mis-learning of graph models towards the interpretability of graphs, and thus the learned noisy and task-agnostic information interferes with the prediction of graphs. To this end, with the purpose of exploring the intrinsic rationale of graphs, we accordingly propose to capture the dimensional rationale from graphs, which has not received sufficient attention in the literature. The conducted exploratory experiments attest to the feasibility of the aforementioned roadmap. To elucidate the innate mechanism behind the performance improvement arising from the dimensional rationale, we rethink the dimensional rationale in graph contrastive learning from a causal perspective a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#21464;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#25552;&#20986;&#20102;&#22312;&#19981;&#30830;&#23450;&#12289;&#38543;&#26426;&#21644;&#26102;&#21464;&#29615;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35760;&#24518;&#20248;&#20808;&#29366;&#24577;&#20272;&#35745;&#21644;&#35268;&#21010;&#31574;&#30053;&#30340;&#38598;&#25104;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#38271;&#26399;&#22870;&#21169;&#30340;&#20248;&#21270;&#65292;&#22312;&#20223;&#30495;&#21644;&#30828;&#20214;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.03263</link><description>&lt;p&gt;
&#27668;&#20505;&#19981;&#30830;&#23450;&#24615;&#20013;&#30340;&#23398;&#20064;&#21644;&#35268;&#21010;&#65306;&#22312;&#26102;&#21464;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment. (arXiv:2312.03263v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#21464;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#25552;&#20986;&#20102;&#22312;&#19981;&#30830;&#23450;&#12289;&#38543;&#26426;&#21644;&#26102;&#21464;&#29615;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35760;&#24518;&#20248;&#20808;&#29366;&#24577;&#20272;&#35745;&#21644;&#35268;&#21010;&#31574;&#30053;&#30340;&#38598;&#25104;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#38271;&#26399;&#22870;&#21169;&#30340;&#20248;&#21270;&#65292;&#22312;&#20223;&#30495;&#21644;&#30828;&#20214;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#12289;&#38543;&#26426;&#21644;&#26102;&#21464;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#20915;&#31574;&#23545;&#20110;&#33258;&#20027;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#29615;&#22659;&#30340;&#21464;&#21270;&#21487;&#20197;&#23545;&#31995;&#32479;&#30340;&#26368;&#20248;&#20915;&#31574;&#31574;&#30053;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#20026;&#20102;&#23545;&#36825;&#26679;&#30340;&#29615;&#22659;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20043;&#21069;&#30340;&#26102;&#21464;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;(time-varying Markov Decision Processes, TVMDP)&#30340;&#27010;&#24565;&#19982;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#26102;&#21464;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;(time-varying Partially Observable Markov Decision Processes, TV-POMDP)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#31649;&#40784;&#19979;&#30340;&#26041;&#27861;&#26469;&#22312;TV-POMDP&#20013;&#20934;&#30830;&#20272;&#35745;&#21644;&#35268;&#21010;&#65306;1&#65289;&#35760;&#24518;&#20248;&#20808;&#29366;&#24577;&#20272;&#35745;(Memory Prioritized State Estimation, MPSE)&#65292;&#21033;&#29992;&#21152;&#26435;&#35760;&#24518;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#26102;&#21464;&#36716;&#31227;&#20272;&#35745;&#65307;2&#65289;MPSE&#38598;&#25104;&#30340;&#35268;&#21010;&#31574;&#30053;&#65292;&#20248;&#21270;&#38271;&#26399;&#22870;&#21169;&#30340;&#21516;&#26102;&#32771;&#34385;&#26102;&#38388;&#32422;&#26463;&#12290;&#25105;&#20204;&#20351;&#29992;&#20223;&#30495;&#21644;&#30828;&#20214;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#26426;&#22120;&#20154;&#22312;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#12289;&#26102;&#21464;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal decision-making presents a significant challenge for autonomous systems operating in uncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system's optimal decision making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#20687;&#37197;&#20934;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#26041;&#27861;&#30340;&#36755;&#20986;&#20316;&#20026;&#20248;&#21270;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#37325;&#28857;&#22788;&#29702;&#25439;&#22833;&#26368;&#22823;&#30340;&#22270;&#20687;&#23545;&#65292;&#21462;&#24471;&#20102;1.6%&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;1.0%&#30340;&#21464;&#24418;&#22330;&#24179;&#28369;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.15497</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22270;&#20687;&#37197;&#20934;&#65306;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#20989;&#25968;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26041;&#27861;&#20197;&#25552;&#39640;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision. (arXiv:2311.15497v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#20687;&#37197;&#20934;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#26041;&#27861;&#30340;&#36755;&#20986;&#20316;&#20026;&#20248;&#21270;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#37325;&#28857;&#22788;&#29702;&#25439;&#22833;&#26368;&#22823;&#30340;&#22270;&#20687;&#23545;&#65292;&#21462;&#24471;&#20102;1.6%&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;1.0%&#30340;&#21464;&#24418;&#22330;&#24179;&#28369;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#37197;&#20934;&#20256;&#32479;&#19978;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#20381;&#36182;&#20110;&#24378;&#22823;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#27861;&#21644;&#24212;&#29992;&#22797;&#26434;&#25968;&#23398;&#21464;&#25442;&#26469;&#20351;&#22270;&#20687;&#21464;&#24418;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#24403;&#28982;&#65292;&#36825;&#20004;&#31181;&#33539;&#24335;&#37117;&#26377;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23558;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#32467;&#21512;&#22312;&#19968;&#20010;&#31616;&#21270;&#30340;&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#23398;&#20064;&#26041;&#27861;&#30340;&#36755;&#20986;&#20316;&#20026;&#20248;&#21270;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#21516;&#26102;&#20248;&#20808;&#32771;&#34385;&#23545;&#25439;&#22833;&#26368;&#22823;&#30340;&#22270;&#20687;&#23545;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#25552;&#39640;&#20102;1.6%&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#24182;&#19988;&#21464;&#24418;&#22330;&#24179;&#28369;&#24230;&#26041;&#38754;&#33719;&#24471;&#20102;1.0%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image registration has traditionally been done using two distinct approaches: learning based methods, relying on robust deep neural networks, and optimization-based methods, applying complex mathematical transformations to warp images accordingly. Of course, both paradigms offer advantages and disadvantages, and, in this work, we seek to combine their respective strengths into a single streamlined framework, using the outputs of the learning based method as initial parameters for optimization while prioritizing computational power for the image pairs that offer the greatest loss. Our investigations showed improvements of up to 1.6% in test data, while maintaining the same inference time, and a substantial 1.0% points performance gain in deformation field smoothness.
&lt;/p&gt;</description></item><item><title>INTERVENOR&#27169;&#22411;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65292;&#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.09868</link><description>&lt;p&gt;
INTERVENOR: &#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing. (arXiv:2311.09868v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09868
&lt;/p&gt;
&lt;p&gt;
INTERVENOR&#27169;&#22411;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65292;&#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;INTERVENOR&#30340;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#65288;INTERactiVE chaiN Of Repairing&#65289;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65288;&#36845;&#20195;&#21028;&#26029;&#12289;&#37325;&#26032;&#24605;&#32771;&#21644;&#20462;&#22797;&#65289;&#65292;&#24182;&#20419;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;INTERVENOR&#37319;&#29992;&#20102;&#20004;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#21363;Code Learner&#21644;Code Teacher&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#20462;&#22797;&#20013;&#25198;&#28436;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#24182;&#36890;&#36807;&#20114;&#21160;&#26469;&#20462;&#22797;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;Code Learner&#26681;&#25454;Code Teacher&#30340;&#25351;&#23548;&#29983;&#25104;&#21644;&#20462;&#22797;&#20195;&#30721;&#65292;&#32780;Code Teacher&#26681;&#25454;&#32534;&#35793;&#22120;&#30340;&#21453;&#39304;&#37325;&#26032;&#24605;&#32771;&#20195;&#30721;&#38169;&#35823;&#65292;&#24182;&#36845;&#20195;&#29983;&#25104;&#20462;&#22797;&#38142;&#26465;&#65288;CoR&#65289;&#20197;&#24341;&#23548;Code Learner&#30340;&#20195;&#30721;&#20462;&#22797;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;INTERVENOR&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#36716;&#25442;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;GPT-3.5&#27169;&#22411;&#20998;&#21035;&#21462;&#24471;&#20102;&#32422;13%&#21644;4.5%&#30340;&#25552;&#21319;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;CoR&#33021;&#22815;&#25581;&#31034;bug&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of regard Large Language Models (LLMs). Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code Teacher, to play different roles in code repairing and work interactively to repair the generated codes. The Code Learner is asked to generate and repair code according to the instructions from the Code Teacher. The Code Teacher rethinks the code errors according to the corresponding feedback from compilers and iteratively generates the chain-of-repairing (CoR) to guide the code repairing process for Code Learner. Our experiments show that INTERVENOR outperforms the state-of-the-art methods and achieves about 13% and 4.5% improvements over the GPT-3.5 model in code generation and code translation tasks, respectively. Our further analyses show that CoR can illuminate the bug reasons and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.03976</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Graph Model. (arXiv:2311.03976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#22312;&#25968;&#25454;&#25110;&#26631;&#31614;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#20445;&#25345;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#19968;&#33268;&#12290;&#36825;&#20351;&#24471;&#26080;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#36801;&#31227;&#12290;&#33021;&#22815;&#22312;&#20219;&#24847;&#20219;&#21153;&#21644;&#39046;&#22495;&#19978;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#30340;&#27169;&#22411;&#23558;&#25104;&#20026;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#25552;&#20986;&#20102;FoToM&#65292;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#30340;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;FoToM&#22312;&#22810;&#20010;&#22270;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27491;&#21521;&#36801;&#31227;&#12290;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#65292;&#24615;&#33021;&#26368;&#24046;&#26102;&#19982;&#26377;&#30417;&#30563;&#22522;&#32447;&#30456;&#24403;&#65292;76%&#30340;&#25968;&#25454;&#38598;&#22312;95%&#32622;&#20449;&#24230;&#19979;&#37117;&#26174;&#33879;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#65288;P&#8804;0.01&#65289;&#65292;&#35823;&#24046;&#20943;&#23569;&#20102;8%&#33267;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
The principal benefit of unsupervised graph representation learning is that a pre-trained model can be fine-tuned where data or labels are scarce. Existing approaches are domain specific, maintaining consistent node and edge attributes across the pre-training and target datasets. This precludes transfer to other domains. A model capable of positive transfer on arbitrary tasks and domains would represent the first foundation graph model.  In this work we use adversarial contrastive learning to present FoToM, a graph pre-training method based on node and edge feature exclusion. We use FoToM to pre-train models over multiple graph domains, producing the first foundation graph models. We demonstrate positive transfer on evaluation datasets from multiple domains, including domains not present in pre-training data. On all datasets performance is at worst on-par and on 76% significantly better than a supervised baseline ($P \leq 0.01$), with an 8 to 40% reduction in error at 95% confidence. C
&lt;/p&gt;</description></item><item><title>SAGE&#26694;&#26550;&#36890;&#36807;&#26367;&#25442;&#25163;&#21160;&#23450;&#20041;&#30340;&#25512;&#29702;&#36923;&#36753;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23454;&#38469;&#25191;&#34892;&#30340;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;&#65292;&#25552;&#39640;&#20102;&#28789;&#27963;&#24615;&#12290;&#23427;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#19982;&#35774;&#22791;&#20132;&#20114;&#65292;&#30417;&#35270;&#35774;&#22791;&#65292;&#24182;&#29702;&#35299;&#33258;&#28982;&#30340;&#35774;&#22791;&#24341;&#29992;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;SAGE&#22312;43&#20010;&#26234;&#33021;&#23478;&#23621;&#20219;&#21153;&#20013;&#25104;&#21151;&#23436;&#25104;&#20102;23&#20010;&#20219;&#21153;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2311.00772</link><description>&lt;p&gt;
SAGE: &#20855;&#26377;&#22522;&#20110;&#23454;&#38469;&#25191;&#34892;&#30340;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
SAGE: Smart home Agent with Grounded Execution. (arXiv:2311.00772v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00772
&lt;/p&gt;
&lt;p&gt;
SAGE&#26694;&#26550;&#36890;&#36807;&#26367;&#25442;&#25163;&#21160;&#23450;&#20041;&#30340;&#25512;&#29702;&#36923;&#36753;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23454;&#38469;&#25191;&#34892;&#30340;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;&#65292;&#25552;&#39640;&#20102;&#28789;&#27963;&#24615;&#12290;&#23427;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#19982;&#35774;&#22791;&#20132;&#20114;&#65292;&#30417;&#35270;&#35774;&#22791;&#65292;&#24182;&#29702;&#35299;&#33258;&#28982;&#30340;&#35774;&#22791;&#24341;&#29992;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;SAGE&#22312;43&#20010;&#26234;&#33021;&#23478;&#23621;&#20219;&#21153;&#20013;&#25104;&#21151;&#23436;&#25104;&#20102;23&#20010;&#20219;&#21153;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SAGE&#65288;&#20855;&#26377;&#22522;&#20110;&#23454;&#38469;&#25191;&#34892;&#30340;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#25163;&#21160;&#23450;&#20041;&#30340;&#25512;&#29702;&#36923;&#36753;&#26367;&#25442;&#20026;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#20195;&#29702;&#31995;&#32479;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;&#30340;&#28789;&#27963;&#24615;&#12290;SAGE&#36890;&#36807;&#21327;&#35843;&#19968;&#31995;&#21015;&#24037;&#20855;&#25972;&#21512;&#29992;&#25143;&#20559;&#22909;&#12289;&#35774;&#22791;&#29366;&#24577;&#21644;&#22806;&#37096;&#22240;&#32032;&#65288;&#22914;&#22825;&#27668;&#21644;&#30005;&#35270;&#33410;&#30446;&#34920;&#65289;&#12290;SAGE&#30340;&#21151;&#33021;&#21253;&#25324;&#20174;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20013;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#36890;&#36807;&#38405;&#35835;&#35774;&#22791;&#30340;API&#25991;&#26723;&#19982;&#35774;&#22791;&#20132;&#20114;&#65292;&#32534;&#20889;&#20195;&#30721;&#20197;&#25345;&#32493;&#30417;&#35270;&#35774;&#22791;&#65292;&#24182;&#29702;&#35299;&#33258;&#28982;&#30340;&#35774;&#22791;&#24341;&#29992;&#12290;&#20026;&#20102;&#35780;&#20272;SAGE&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;43&#20010;&#39640;&#24230;&#25361;&#25112;&#30340;&#26234;&#33021;&#23478;&#23621;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;SAGE&#25104;&#21151;&#23436;&#25104;&#20102;23&#20010;&#20219;&#21153;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#22522;&#20934;&#65288;5/43&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces SAGE (Smart home Agent with Grounded Execution), a framework designed to maximize the flexibility of smart home assistants by replacing manually-defined inference logic with an LLM-powered autonomous agent system. SAGE integrates information about user preferences, device states, and external factors (such as weather and TV schedules) through the orchestration of a collection of tools. SAGE's capabilities include learning user preferences from natural-language utterances, interacting with devices by reading their API documentation, writing code to continuously monitor devices, and understanding natural device references. To evaluate SAGE, we develop a benchmark of 43 highly challenging smart home tasks, where SAGE successfully achieves 23 tasks, significantly outperforming existing LLM-enabled baselines (5/43).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23637;&#29616;&#20986;&#20102;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#20854;&#37319;&#29992;&#30340;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#20026;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#65292;IQL&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12955</link><description>&lt;p&gt;
&#26500;&#24314;&#20855;&#26377;&#22810;&#26679;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Offline Reinforcement Learning under Diverse Data Corruption. (arXiv:2310.12955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23637;&#29616;&#20986;&#20102;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#20854;&#37319;&#29992;&#30340;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#20026;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#65292;IQL&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24378;&#21270;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#26114;&#36149;&#25110;&#19981;&#23433;&#20840;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#23384;&#22312;&#22122;&#22768;&#65292;&#29978;&#33267;&#21487;&#33021;&#34987;&#24694;&#24847;&#25439;&#22351;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#23545;&#24403;&#21069;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21253;&#25324;&#29366;&#24577;&#12289;&#21160;&#20316;&#12289;&#22870;&#21169;&#21644;&#21160;&#21147;&#23398;&#22312;&#20869;&#30340;&#20840;&#38754;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#26174;&#31034;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#34920;&#29616;&#20986;&#20102;&#21487;&#38752;&#30340;&#25239;&#25968;&#25454;&#25439;&#22351;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;IQL&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#30830;&#23450;&#20026;&#20851;&#38190;&#22240;&#32032;&#12290;&#23613;&#31649;&#30456;&#23545;&#40065;&#26834;&#65292;&#20294;IQL&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tack
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05492</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#26631;&#35760;&#21644;&#21442;&#25968;&#65292;&#23637;&#29616;&#20986;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#38543;&#31561;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#24320;&#28304;&#31038;&#21306;&#24050;&#32463;&#30740;&#31350;&#20102;&#38024;&#23545;&#27599;&#31181;&#33021;&#21147;&#30340;&#20020;&#26102;SFT&#65292;&#32780;&#19987;&#26377;LLMs&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;SFT&#35299;&#38145;&#22810;&#37325;&#33021;&#21147;&#21464;&#24471;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;SFT&#36807;&#31243;&#20013;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20154;&#31867;&#23545;&#40784;&#33021;&#21147;&#20043;&#38388;&#30340;&#25968;&#25454;&#32452;&#21512;&#12290;&#20174;&#35268;&#27169;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#33021;&#21147;&#19982;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#25968;&#25454;&#37327;&#12289;&#25968;&#25454;&#32452;&#21512;&#27604;&#20363;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;SFT&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#33021;&#21147;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25193;&#23637;&#27169;&#24335;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;MultiScript&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#33050;&#26412;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#24320;&#25918;&#39046;&#22495;&#26085;&#24120;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.04965</link><description>&lt;p&gt;
MULTISCRIPT: &#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#29992;&#20110;&#25903;&#25345;&#24320;&#25918;&#39046;&#22495;&#30340;&#26085;&#24120;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks. (arXiv:2310.04965v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;MultiScript&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#33050;&#26412;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#24320;&#25918;&#39046;&#22495;&#26085;&#24120;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#28436;&#31034;&#20013;&#33258;&#21160;&#29983;&#25104;&#33050;&#26412;&#65288;&#21363;&#25991;&#26412;&#25551;&#36848;&#30340;&#20851;&#38190;&#27493;&#39588;&#24207;&#21015;&#65289;&#24182;&#25512;&#29702;&#21518;&#32493;&#27493;&#39588;&#23545;&#20110;&#29616;&#20195;AI&#34394;&#25311;&#21161;&#25163;&#26469;&#24341;&#23548;&#20154;&#20204;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#38476;&#29983;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#26041;&#27861;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32467;&#26500;&#33391;&#22909;&#30340;&#21069;&#32622;&#27493;&#39588;&#30340;&#25991;&#26412;&#21644;/&#25110;&#22270;&#20687;&#25551;&#36848;&#65292;&#25110;&#32773;&#38480;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#23548;&#33268;&#19982;&#30495;&#23454;&#19990;&#30028;&#20013;&#29992;&#25143;&#22330;&#26223;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;&#8212;&#8212;MultiScript&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#20851;&#20110;&#38754;&#21521;&#20219;&#21153;&#30340;&#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#30340;&#26032;&#20219;&#21153;&#65306;&#65288;1&#65289;&#22810;&#27169;&#24335;&#33050;&#26412;&#29983;&#25104;&#65292;&#21644;&#65288;2&#65289;&#21518;&#32493;&#27493;&#39588;&#39044;&#27979;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#36755;&#20837;&#21253;&#25324;&#30446;&#26631;&#20219;&#21153;&#21517;&#31216;&#21644;&#28436;&#31034;&#35270;&#39057;&#65292;&#39044;&#26399;&#36755;&#20986;&#20026;&#65288;1&#65289;&#22522;&#20110;&#28436;&#31034;&#35270;&#39057;&#30340;&#32467;&#26500;&#21270;&#27493;&#39588;&#25551;&#36848;&#30340;&#24207;&#21015;&#65292;&#21644;&#65288;2&#65289;&#38024;&#23545;&#27599;&#20010;&#27493;&#39588;&#30340;&#21333;&#19968;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14393</link><description>&lt;p&gt;
LLMCarbon: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30899;&#36275;&#36857;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#25512;&#29702;&#12289;&#23454;&#39564;&#21644;&#23384;&#20648;&#36807;&#31243;&#20013;&#30340;&#25490;&#25918;&#65292;&#21253;&#25324;&#36816;&#33829;&#21644;&#22266;&#23450;&#30899;&#25490;&#25918;&#12290;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#22312;LLMs&#35757;&#32451;&#20043;&#21069;&#20934;&#30830;&#20272;&#35745;&#20854;&#30899;&#24433;&#21709;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;GPU&#30340;&#20351;&#29992;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#25253;&#21578;&#20102;LLMs&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#24037;&#20855;mlco2&#33021;&#22815;&#22312;&#23454;&#38469;&#35757;&#32451;&#20043;&#21069;&#39044;&#27979;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#30899;&#36275;&#36857;&#12290;&#28982;&#32780;&#65292;mlco2&#23384;&#22312;&#19968;&#20123;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#23427;&#19981;&#33021;&#25193;&#23637;&#20854;&#23545;&#23494;&#38598;&#25110;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;LLMs&#30340;&#20272;&#35745;&#65292;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#20165;&#20851;&#27880;GPU&#65292;&#24182;&#19981;&#33021;&#24314;&#27169;&#22266;&#21270;&#30340;&#30899;&#36275;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#20026;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#19982;mlco2&#30456;&#27604;&#65292;LLMCarbon&#26174;&#33879;&#22686;&#24378;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{LLMCarbon}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly enhances the ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10444</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#33258;&#25105;&#24378;&#21270;&#20197;&#25913;&#36827;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#28041;&#21450;&#23398;&#29983;&#29983;&#25104;&#21644;&#20998;&#20139;&#23398;&#20064;&#36164;&#28304;&#12290;&#22312;&#23398;&#29983;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#21019;&#24314;&#35299;&#37322;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#23545;&#30456;&#20851;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#24448;&#24448;&#30001;&#20110;&#20027;&#39064;&#29702;&#35299;&#26377;&#38480;&#21644;&#20165;&#20165;&#37325;&#30003;&#38382;&#39064;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#27491;&#30830;&#31572;&#26696;&#30340;&#20542;&#21521;&#32780;&#38590;&#20197;&#32534;&#20889;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24110;&#21161;&#25903;&#25745;&#36825;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#29983;&#25104;&#19982;&#23398;&#29983;&#23545;&#40784;&#30340;&#35299;&#37322;&#65292;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#20197;&#30830;&#20445;&#20854;&#36136;&#37327;&#65292;&#24182;&#36845;&#20195;&#22686;&#24378;&#35299;&#37322;&#12290;&#22914;&#26524;&#19968;&#20010;&#35299;&#37322;&#30340;&#35780;&#20272;&#20998;&#25968;&#20302;&#20110;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#26694;&#26550;&#20250;&#36845;&#20195;&#22320;&#20248;&#21270;&#21644;&#37325;&#26032;&#35780;&#20272;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#19968;&#20010;&#23398;&#29983;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08565</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#33021;&#21542;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23450;&#21046;&#20026;&#31526;&#21512;&#32454;&#31890;&#24230;&#23646;&#24615;&#65288;&#22914;&#24418;&#24335;&#65289;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#33267;&#23569;&#19968;&#20123;&#24102;&#26377;&#23646;&#24615;&#27880;&#37322;&#30340;&#30417;&#30563;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#31232;&#32570;&#20173;&#28982;&#26159;&#23558;&#27492;&#23450;&#21046;&#33021;&#21147;&#26222;&#21450;&#21040;&#26356;&#24191;&#27867;&#35821;&#35328;&#33539;&#22260;&#65292;&#23588;&#20854;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#19968;&#20010;&#29942;&#39048;&#12290;&#37492;&#20110;&#26368;&#36817;&#22312;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20316;&#20026;&#23545;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#36827;&#34892;&#23646;&#24615;&#25511;&#21046;&#33021;&#21147;&#36801;&#31227;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;NLLB-200&#27169;&#22411;&#23545;&#23646;&#24615;&#25511;&#21046;&#22120;&#30340;&#36801;&#31227;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#20004;&#31181;&#33539;&#24335;&#26159;&#20114;&#34917;&#30340;&#65292;&#36890;&#36807;&#19968;&#33268;&#30340;&#25913;&#36827;&#26469;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customizing machine translation models to comply with fine-grained attributes such as formality has seen tremendous progress recently. However, current approaches mostly rely on at least some supervised data with attribute annotation. Data scarcity therefore remains a bottleneck to democratizing such customization possibilities to a wider range of languages, lower-resource ones in particular. Given recent progress in pretrained massively multilingual translation models, we use them as a foundation to transfer the attribute controlling capabilities to languages without supervised data. In this work, we present a comprehensive analysis of transferring attribute controllers based on a pretrained NLLB-200 model. We investigate both training- and inference-time control techniques under various data scenarios, and uncover their relative strengths and weaknesses in zero-shot performance and domain robustness. We show that both paradigms are complementary, as shown by consistent improvements o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#21644;&#21487;&#29992;&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#32534;&#35793;&#12289;&#26131;&#35835;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.08033</link><description>&lt;p&gt;
&#20351;&#29992;&#20195;&#30721;&#27169;&#22411;&#21644;&#39046;&#22495;&#36866;&#24212;&#24615;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automated Test Case Generation Using Code Models and Domain Adaptation. (arXiv:2308.08033v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#21644;&#21487;&#29992;&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#32534;&#35793;&#12289;&#26131;&#35835;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#25216;&#26415;&#65292;&#20363;&#22914;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#65292;&#36890;&#24120;&#23545;&#24320;&#21457;&#20154;&#21592;&#21019;&#24314;&#30340;&#27979;&#35797;&#29992;&#20363;&#19968;&#26080;&#25152;&#30693;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#19981;&#26131;&#38405;&#35835;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#26816;&#27979;&#25152;&#26377;&#22797;&#26434;&#32570;&#38519;&#65292;&#32780;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#29992;&#20363;&#21017;&#21487;&#20197;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#20197;&#34917;&#20805;&#22522;&#20110;&#25628;&#32034;&#27979;&#35797;&#29983;&#25104;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;CodeT5&#65292;&#21363;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#65292;&#24182;&#23545;&#27979;&#35797;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;Methods2test&#25968;&#25454;&#38598;&#23545;CodeT5&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;Defects4j&#36827;&#34892;&#39033;&#30446;&#32423;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#21644;&#21487;&#29992;&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#32534;&#35793;&#12289;&#26131;&#35835;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26032;&#30340;&#27979;&#35797;&#29992;&#20363;&#65292;&#35206;&#30422;&#20102;&#24050;&#32463;&#34987;&#27979;&#35797;&#36807;&#30340;&#20195;&#30721;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art automated test generation techniques, such as search-based testing, are usually ignorant about what a developer would create as a test case. Therefore, they typically create tests that are not human-readable and may not necessarily detect all types of complex bugs developer-written tests would do. In this study, we leverage Transformer-based code models to generate unit tests that can complement search-based test generation. Specifically, we use CodeT5, i.e., a state-of-the-art large code model, and fine-tune it on the test generation downstream task. For our analysis, we use the Methods2test dataset for fine-tuning CodeT5 and Defects4j for project-level domain adaptation and evaluation. The main contribution of this study is proposing a fully automated testing framework that leverages developer-written tests and available code models to generate compilable, human-readable unit tests. Results show that our approach can generate new test cases that cover lines that were
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;GBSD&#65292;&#24182;&#20351;&#29992;&#38454;&#27573;&#25193;&#25955;&#25216;&#26415;&#21512;&#25104;&#20855;&#26377;&#34394;&#21270;&#39118;&#26684;&#30340;&#29031;&#29255;&#12290;</title><link>http://arxiv.org/abs/2306.08251</link><description>&lt;p&gt;
GBSD: &#24102;&#26377;&#38454;&#27573;&#25193;&#25955;&#30340;&#29983;&#25104;&#34394;&#21270;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
GBSD: Generative Bokeh with Stage Diffusion. (arXiv:2306.08251v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;GBSD&#65292;&#24182;&#20351;&#29992;&#38454;&#27573;&#25193;&#25955;&#25216;&#26415;&#21512;&#25104;&#20855;&#26377;&#34394;&#21270;&#39118;&#26684;&#30340;&#29031;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bokeh&#25928;&#26524;&#26159;&#19968;&#31181;&#33402;&#26415;&#25216;&#24039;&#65292;&#21487;&#20197;&#20351;&#29031;&#29255;&#20013;&#30340;&#22833;&#28966;&#21306;&#22495;&#27169;&#31946;&#65292;&#36817;&#26399;&#30001;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#20197;&#21450;&#26234;&#33021;&#25163;&#26426;&#30340;&#26222;&#21450;&#21644;&#29031;&#29255;&#20998;&#20139;&#24212;&#29992;&#31243;&#24207;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#36825;&#31181;&#25928;&#26524;&#22791;&#21463;&#20851;&#27880;&#12290;&#20197;&#24448;&#30340;&#34394;&#21270;&#25928;&#26524;&#28210;&#26579;&#24037;&#20316;&#37117;&#26159;&#38024;&#23545;&#24050;&#26377;&#29031;&#29255;&#36827;&#34892;&#20107;&#21518;&#22270;&#20687;&#22788;&#29702;&#65292;&#20351;&#29992;&#32463;&#20856;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#25110;&#31070;&#32463;&#28210;&#26579;&#25216;&#26415;&#20135;&#29983;&#31867;&#20284;&#30340;&#27169;&#31946;&#25928;&#26524;&#65292;&#20294;&#26159;&#24448;&#24448;&#23384;&#22312;&#28145;&#24230;&#19981;&#36830;&#32493;&#30340;&#22270;&#20687;&#20266;&#24433;&#65292;&#32780;&#19988;&#38480;&#21046;&#20110;&#32451;&#20064;&#25968;&#25454;&#27979;&#35797;&#30340;&#34394;&#21270;&#25928;&#26524;&#12290;&#36817;&#26399;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#21512;&#25104;&#20855;&#26377;&#33402;&#26415;&#39118;&#26684;&#30340;&#22270;&#20687;&#65292;&#20294;&#26159;&#35201;&#27714;&#29983;&#25104;&#39640;&#32500;&#24230;&#25513;&#27169;&#25110;&#32773;&#36827;&#34892;&#26114;&#36149;&#30340;&#35843;&#25972;&#65292;&#21487;&#33021;&#24433;&#21709;&#25972;&#20307;&#22270;&#20687;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GBSD&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20855;&#26377;&#34394;&#21270;&#39118;&#26684;&#30340;&#29031;&#29255;&#12290;&#21463;&#25193;&#25955;&#27169;&#22411;&#20013;&#36880;&#27493;&#21512;&#25104;&#22270;&#20687;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#38454;&#27573;&#25193;&#25955;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bokeh effect is an artistic technique that blurs out-of-focus areas in a photograph and has gained interest due to recent developments in text-to-image synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior work on rendering bokeh effects have focused on post hoc image manipulation to produce similar blurring effects in existing photographs using classical computer graphics or neural rendering techniques, but have either depth discontinuity artifacts or are restricted to reproducing bokeh effects that are present in the training data. More recent diffusion based models can synthesize images with an artistic style, but either require the generation of high-dimensional masks, expensive fine-tuning, or affect global image characteristics. In this paper, we present GBSD, the first generative text-to-image model that synthesizes photorealistic images with a bokeh style. Motivated by how image synthesis occurs progressively in diffusion models, our approach combi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23569;&#37327;&#31034;&#20363;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#65307;&#36890;&#36807;&#20998;&#35299;&#28436;&#31034;&#12289;&#36807;&#28388;&#29366;&#24577;&#24182;&#37325;&#26032;&#26500;&#36896;&#20219;&#21153;&#25551;&#36848;&#65292;&#31034;&#20363;&#26816;&#32034;&#31561;&#27493;&#39588;&#65292;Synapse &#20855;&#22791;&#20102;&#36866;&#24212;&#22810;&#20219;&#21153;&#12289;&#27867;&#21270;&#22810;&#29615;&#22659;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07863</link><description>&lt;p&gt;
Synapse&#65306;&#21033;&#29992;&#23569;&#37327;&#31034;&#20363;&#20026;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#25171;&#19979;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Synapse: Leveraging Few-Shot Exemplars for Human-Level Computer Control. (arXiv:2306.07863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23569;&#37327;&#31034;&#20363;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#65307;&#36890;&#36807;&#20998;&#35299;&#28436;&#31034;&#12289;&#36807;&#28388;&#29366;&#24577;&#24182;&#37325;&#26032;&#26500;&#36896;&#20219;&#21153;&#25551;&#36848;&#65292;&#31034;&#20363;&#26816;&#32034;&#31561;&#27493;&#39588;&#65292;Synapse &#20855;&#22791;&#20102;&#36866;&#24212;&#22810;&#20219;&#21153;&#12289;&#27867;&#21270;&#22810;&#29615;&#22659;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#23569;&#37327;&#31034;&#20363;&#26469;&#36827;&#34892;&#35745;&#31639;&#26426;&#33258;&#21160;&#21270;&#30340;&#35774;&#35745;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#25552;&#31034;&#26041;&#27861;&#30528;&#37325;&#20110;&#33258;&#25105;&#32416;&#27491;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#26377;&#33391;&#22909;&#32467;&#26500;&#30340;&#31034;&#20363;&#23601;&#36275;&#20197;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Synapse&#65292;&#19968;&#31181;&#19978;&#19979;&#25991;&#35745;&#31639;&#26426;&#25511;&#21046;&#20195;&#29702;&#65292;&#22312; MiniWob++ &#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#20154;&#31867;&#32423;&#21035;&#30340;&#24615;&#33021;&#12290;Synapse &#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;1&#65289;&#29366;&#24577;&#26465;&#20214;&#20998;&#35299;&#65292;&#26681;&#25454;&#20195;&#29702;&#38656;&#35201;&#26032;&#29615;&#22659;&#29366;&#24577;&#23558;&#28436;&#31034;&#20998;&#20026;&#31034;&#20363;&#38598;&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#25277;&#35937;&#65307;2&#65289;&#32467;&#26500;&#21270;&#25552;&#31034;&#65292;&#36807;&#28388;&#29366;&#24577;&#24182;&#37325;&#26032;&#26500;&#36896;&#27599;&#20010;&#38598;&#21512;&#30340;&#20219;&#21153;&#25551;&#36848;&#20197;&#25913;&#21892;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#65307;3&#65289;&#31034;&#20363;&#26816;&#32034;&#65292;&#23558;&#20256;&#20837;&#30340;&#20219;&#21153;&#19982;&#31034;&#20363;&#25968;&#25454;&#24211;&#20013;&#30340;&#23545;&#24212;&#31034;&#20363;&#30456;&#20851;&#32852;&#65292;&#20197;&#23454;&#29616;&#22810;&#20219;&#21153;&#36866;&#24212;&#21644;&#27867;&#21270;&#12290;Synapse &#20811;&#26381;&#20102;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#65292;&#20943;&#23569;&#20102;&#22810;&#27493;&#25511;&#21046;&#20013;&#30340;&#38169;&#35823;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#28789;&#27963;&#22320;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35745;&#31639;&#26426;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the design of few-shot exemplars for computer automation through prompting large language models (LLMs). While previous prompting approaches focus on self-correction, we find that well-structured exemplars alone are sufficient for human-level performance. We present Synapse, an in-context computer control agent demonstrating human-level performance on the MiniWob++ benchmark. Synapse consists of three main components: 1) state-conditional decomposition, which divides demonstrations into exemplar sets based on the agent's need for new environment states, enabling temporal abstraction; 2) structured prompting, which filters states and reformulates task descriptions for each set to improve planning correctness; and 3) exemplar retrieval, which associates incoming tasks with corresponding exemplars in an exemplar database for multi-task adaptation and generalization. Synapse overcomes context length limits, reduces errors in multi-step control, and allows for more e
&lt;/p&gt;</description></item><item><title>&#22312;&#22826;&#38451;&#31995;&#30340;&#20912;&#21355;&#26143;&#23547;&#25214;&#22806;&#26143;&#29983;&#21629;&#65292;&#38656;&#35201;&#29992;&#19968;&#22871;&#34917;&#20805;&#20202;&#22120;&#23545;&#22810;&#20010;&#29420;&#31435;&#30340;&#29983;&#29289;&#26631;&#24535;&#36827;&#34892;&#37319;&#26679;&#12290;&#26426;&#36733;&#31185;&#23398;&#20202;&#22120;&#33258;&#20027;&#24615;&#65288;OSIA&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#23398;&#31185;&#65292;&#21487;&#20197;&#35780;&#20272;&#12289;&#24635;&#32467;&#21644;&#20248;&#20808;&#32771;&#34385;&#35266;&#27979;&#20202;&#22120;&#25968;&#25454;&#20197;&#26368;&#22823;&#21270;&#31185;&#23398;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2304.13189</link><description>&lt;p&gt;
&#25628;&#23547;&#28023;&#27915;&#19990;&#30028;&#29983;&#21629;&#25506;&#27979;&#22120;&#19978;&#29992;&#20110;&#26816;&#27979;&#26174;&#24494;&#29983;&#29289;&#29983;&#29289;&#26631;&#24535;&#30340;&#20202;&#22120;&#33258;&#20027;&#24615;
&lt;/p&gt;
&lt;p&gt;
Onboard Science Instrument Autonomy for the Detection of Microscopy Biosignatures on the Ocean Worlds Life Surveyor. (arXiv:2304.13189v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13189
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22826;&#38451;&#31995;&#30340;&#20912;&#21355;&#26143;&#23547;&#25214;&#22806;&#26143;&#29983;&#21629;&#65292;&#38656;&#35201;&#29992;&#19968;&#22871;&#34917;&#20805;&#20202;&#22120;&#23545;&#22810;&#20010;&#29420;&#31435;&#30340;&#29983;&#29289;&#26631;&#24535;&#36827;&#34892;&#37319;&#26679;&#12290;&#26426;&#36733;&#31185;&#23398;&#20202;&#22120;&#33258;&#20027;&#24615;&#65288;OSIA&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#23398;&#31185;&#65292;&#21487;&#20197;&#35780;&#20272;&#12289;&#24635;&#32467;&#21644;&#20248;&#20808;&#32771;&#34385;&#35266;&#27979;&#20202;&#22120;&#25968;&#25454;&#20197;&#26368;&#22823;&#21270;&#31185;&#23398;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#22806;&#26143;&#29983;&#21629;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#31185;&#23398;&#25506;&#32034;&#65292;&#23545;&#25991;&#26126;&#32423;&#21035;&#20855;&#26377;&#24433;&#21709;&#12290;&#22826;&#38451;&#31995;&#20013;&#30340;&#20912;&#21355;&#26143;&#22240;&#20854;&#28082;&#24577;&#28023;&#27915;&#20351;&#20854;&#25104;&#20026;&#24494;&#35266;&#29983;&#21629;&#28508;&#22312;&#23492;&#29983;&#22320;&#65292;&#26159;&#25506;&#32034;&#37325;&#35201;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#21629;&#30340;&#31934;&#30830;&#23450;&#20041;&#32570;&#20047;&#20351;&#21046;&#23450;&#25506;&#27979;&#31574;&#30053;&#38754;&#20020;&#22522;&#26412;&#25361;&#25112;&#12290;&#20026;&#22686;&#21152;&#26080;&#27495;&#20041;&#25506;&#27979;&#30340;&#26426;&#20250;&#65292;&#19968;&#22871;&#34917;&#20805;&#20202;&#22120;&#24517;&#39035;&#23545;&#22810;&#20010;&#29420;&#31435;&#30340;&#29983;&#29289;&#26631;&#24535;&#36827;&#34892;&#37319;&#26679;&#65288;&#20363;&#22914;&#65292;&#32452;&#25104;&#65292;&#36816;&#21160;/&#34892;&#20026;&#65292;&#21487;&#35265;&#32467;&#26500;&#65289;&#12290;&#36825;&#26679;&#30340;&#20202;&#22120;&#22871;&#20214;&#21487;&#20197;&#20135;&#29983;&#27604;&#20687;&#22303;&#21355;&#20845;&#25110;&#26408;&#21355;&#20108;&#36825;&#26679;&#30340;&#36965;&#36828;&#28023;&#27915;&#19990;&#30028;&#20256;&#36755;&#30340;&#21407;&#22987;&#25968;&#25454;&#22810;10,000&#20493;&#12290;&#20026;&#35299;&#20915;&#36825;&#31181;&#24102;&#23485;&#38480;&#21046;&#65292;&#26426;&#36733;&#31185;&#23398;&#20202;&#22120;&#33258;&#20027;&#24615;&#65288;OSIA&#65289;&#26159;&#19968;&#31181;&#35780;&#20272;&#12289;&#24635;&#32467;&#21644;&#20248;&#20808;&#32771;&#34385;&#35266;&#27979;&#20202;&#22120;&#25968;&#25454;&#20197;&#26368;&#22823;&#21270;&#31185;&#23398;&#22238;&#25253;&#30340;&#39134;&#34892;&#31995;&#32479;&#30340;&#26032;&#20852;&#23398;&#31185;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#20010;&#24320;&#21457;&#30340;OSIA&#23454;&#29616;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest to find extraterrestrial life is a critical scientific endeavor with civilization-level implications. Icy moons in our solar system are promising targets for exploration because their liquid oceans make them potential habitats for microscopic life. However, the lack of a precise definition of life poses a fundamental challenge to formulating detection strategies. To increase the chances of unambiguous detection, a suite of complementary instruments must sample multiple independent biosignatures (e.g., composition, motility/behavior, and visible structure). Such an instrument suite could generate 10,000x more raw data than is possible to transmit from distant ocean worlds like Enceladus or Europa. To address this bandwidth limitation, Onboard Science Instrument Autonomy (OSIA) is an emerging discipline of flight systems capable of evaluating, summarizing, and prioritizing observational instrument data to maximize science return. We describe two OSIA implementations developed a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11171</link><description>&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#65306;&#19968;&#31181;&#39640;&#25928;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20855;&#26377;&#8220;&#20808;&#22823;&#21518;&#23567;&#8221;&#30340;&#35748;&#30693;&#26426;&#21046;&#65292;&#22240;&#27492;&#20855;&#26377;&#33258;&#36866;&#24212;&#30340;&#22810;&#31890;&#24230;&#25551;&#36848;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#20102;&#26377;&#25928;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#35745;&#31639;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#20182;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#21382;&#21490;&#25163;&#31295;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS&#65292; &#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#26816;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#26032;&#22411;&#30340;&#8220;&#29615;&#24418;&#25439;&#22833;&#8221;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#26816;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#21019;&#24314;&#20102;&#21253;&#21547;&#21476;&#20195;&#19996;&#24052;&#35937;&#24418;&#25991;&#23383;&#30340;&#25163;&#31295;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.00746</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#21382;&#21490;&#25163;&#31295;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS
&lt;/p&gt;
&lt;p&gt;
OTS: A One-shot Learning Approach for Text Spotting in Historical Manuscripts. (arXiv:2304.00746v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#21382;&#21490;&#25163;&#31295;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS&#65292; &#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#26816;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#26032;&#22411;&#30340;&#8220;&#29615;&#24418;&#25439;&#22833;&#8221;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#26816;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#21019;&#24314;&#20102;&#21253;&#21547;&#21476;&#20195;&#19996;&#24052;&#35937;&#24418;&#25991;&#23383;&#30340;&#25163;&#31295;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#25163;&#31295;&#22788;&#29702;&#38754;&#20020;&#26377;&#38480;&#30340;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#21644;&#26032;&#31867;&#21035;&#20986;&#29616;&#31561;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS&#65292;&#36890;&#36807;&#20165;&#19968;&#20010;&#27880;&#37322;&#26679;&#26412;&#65292;&#20934;&#30830;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#26032;&#39062;&#23383;&#31526;&#12290;&#28789;&#24863;&#28304;&#33258;&#35748;&#30693;&#30740;&#31350;&#65292;&#24341;&#20837;&#31354;&#38388;&#23545;&#40784;&#27169;&#22359;&#65292;&#22522;&#20110;&#19968;&#20010;&#25903;&#25345;&#22270;&#20687;&#21457;&#29616;&#12289;&#20851;&#27880;&#21644;&#23398;&#20064;&#26597;&#35810;&#22270;&#20687;&#20013;&#26368;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#31354;&#38388;&#21306;&#22495;&#12290;&#23588;&#20854;&#26159;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29615;&#24418;&#25439;&#22833;&#8221;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#20351;&#36317;&#31163;&#24230;&#37327;&#30340;&#23884;&#20837;&#31354;&#38388;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#39640;&#25928;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20855;&#26377;&#22788;&#29702;&#26032;&#39062;&#23383;&#31526;&#21644;&#31526;&#21495;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#21476;&#20195;&#19996;&#24052;&#35937;&#24418;&#25991;&#23383;&#65288;DBH&#65289;&#30340;&#25163;&#31295;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historical manuscript processing poses challenges like limited annotated training data and novel class emergence. To address this, we propose a novel One-shot learning-based Text Spotting (OTS) approach that accurately and reliably spots novel characters with just one annotated support sample. Drawing inspiration from cognitive research, we introduce a spatial alignment module that finds, focuses on, and learns the most discriminative spatial regions in the query image based on one support image. Especially, since the low-resource spotting task often faces the problem of example imbalance, we propose a novel loss function called torus loss which can make the embedding space of distance metric more discriminative. Our approach is highly efficient and requires only a few training samples while exhibiting the remarkable ability to handle novel characters, and symbols. To enhance dataset diversity, a new manuscript dataset that contains the ancient Dongba hieroglyphics (DBH) is created. We
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.02506</link><description>&lt;p&gt;
Prismer: &#19968;&#31181;&#20855;&#26377;&#19987;&#23478;&#38598;&#21512;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prismer: A Vision-Language Model with An Ensemble of Experts. (arXiv:2303.02506v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02506
&lt;/p&gt;
&lt;p&gt;
Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts, achieving fine-tuned and few-shot learning performance competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#23427;&#20204;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24222;&#22823;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Prismer&#65292;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#12290;Prismer&#21482;&#38656;&#35201;&#35757;&#32451;&#23569;&#37327;&#32452;&#20214;&#65292;&#22823;&#37096;&#20998;&#32593;&#32476;&#26435;&#37325;&#20174;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#39046;&#22495;&#19987;&#23478;&#20013;&#32487;&#25215;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#20923;&#32467;&#29366;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#21487;&#20197;&#26377;&#25928;&#22320;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/NVlabs/prismer&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#23454;&#26102;&#34394;&#20551;&#26032;&#38395;&#32531;&#35299;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#24182;&#37319;&#21462;&#23454;&#26102;&#30340;&#32593;&#32476;&#24863;&#30693;&#31574;&#30053;&#26469;&#20943;&#23569;&#20854;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2302.12190</link><description>&lt;p&gt;
MCWDST: &#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#23454;&#26102;&#34394;&#20551;&#26032;&#38395;&#32531;&#35299;&#30340;&#26368;&#23567;&#25104;&#26412;&#21152;&#26435;&#26377;&#21521;&#29983;&#25104;&#26641;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MCWDST: a Minimum-Cost Weighted Directed Spanning Tree Algorithm for Real-Time Fake News Mitigation in Social Media. (arXiv:2302.12190v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#23454;&#26102;&#34394;&#20551;&#26032;&#38395;&#32531;&#35299;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#24182;&#37319;&#21462;&#23454;&#26102;&#30340;&#32593;&#32476;&#24863;&#30693;&#31574;&#30053;&#26469;&#20943;&#23569;&#20854;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#30340;&#24191;&#27867;&#26222;&#21450;&#21644;&#25163;&#25345;&#35774;&#22791;&#30340;&#21487;&#29992;&#24615;&#20351;&#24471;&#31038;&#20132;&#23186;&#20307;&#20855;&#26377;&#31867;&#20284;&#25253;&#32440;&#30340;&#24433;&#21709;&#21147;&#12290;&#20154;&#20204;&#21487;&#20197;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#21363;&#26102;&#33719;&#21462;&#24265;&#20215;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20415;&#21033;&#24615;&#20063;&#24102;&#26469;&#20102;&#21361;&#38505;&#65292;&#20219;&#20309;&#29992;&#25143;&#37117;&#21487;&#20197;&#33258;&#30001;&#21457;&#24067;&#20219;&#20309;&#20869;&#23481;&#65292;&#32780;&#19981;&#35770;&#20854;&#30495;&#23454;&#24615;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#20063;&#31216;&#20026;&#20551;&#26032;&#38395;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#26816;&#27979;&#20551;&#26032;&#38395;&#65292;&#24182;&#22312;&#23454;&#26102;&#20013;&#20813;&#20110;&#20256;&#25773;&#23427;&#20204;&#30340;&#32593;&#32476;&#33410;&#28857;&#19978;&#36827;&#34892;&#20813;&#30123;&#12290;&#20026;&#20102;&#26816;&#27979;&#20551;&#26032;&#38395;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22534;&#26632;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21033;&#29992;&#21367;&#31215;&#21644;&#21452;&#21521;LSTM&#23618;&#12290;&#20026;&#20102;&#32531;&#35299;&#20551;&#26032;&#38395;&#30340;&#20256;&#25773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#32593;&#32476;&#24863;&#30693;&#31574;&#30053;&#65292;&#23427;&#65288;1&#65289;&#20026;&#26816;&#27979;&#21040;&#30340;&#33410;&#28857;&#26500;&#24314;&#20102;&#19968;&#20010;&#26368;&#23567;&#25104;&#26412;&#21152;&#26435;&#30340;&#26377;&#21521;&#29983;&#25104;&#26641;&#65292;&#24182;&#19988;&#65288;2&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#20998;&#25968;&#21270;&#21361;&#38505;&#24615;&#26041;&#27861;&#26469;&#20813;&#30123;&#35813;&#26641;&#20013;&#30340;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread availability of internet access and handheld devices confers to social media a power similar to the one newspapers used to have. People seek affordable information on social media and can reach it within seconds. Yet this convenience comes with dangers; any user may freely post whatever they please and the content can stay online for a long period, regardless of its truthfulness. A need to detect untruthful information, also known as fake news, arises. In this paper, we present an end-to-end solution that accurately detects fake news and immunizes network nodes that spread them in real-time. To detect fake news, we propose two new stack deep learning architectures that utilize convolutional and bidirectional LSTM layers. To mitigate the spread of fake news, we propose a real-time network-aware strategy that (1) constructs a minimum-cost weighted directed spanning tree for a detected node, and (2) immunizes nodes in that tree by scoring their harmfulness using a novel ran
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;IM-IAD&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35774;&#32622;&#35780;&#20272;&#20102;16&#20010;&#31639;&#27861;&#22312;7&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#36825;&#19968;&#39046;&#22495;&#30740;&#31350;&#30340;&#19981;&#35268;&#33539;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13359</link><description>&lt;p&gt;
IM-IAD&#65306;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;IM-IAD&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35774;&#32622;&#35780;&#20272;&#20102;16&#20010;&#31639;&#27861;&#22312;7&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#36825;&#19968;&#39046;&#22495;&#30740;&#31350;&#30340;&#19981;&#35268;&#33539;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#65288;IAD&#65289;&#26159;&#24037;&#19994;&#21046;&#36896;&#20013;&#19968;&#39033;&#26032;&#20852;&#19988;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#20808;&#36827;&#30340;&#31639;&#27861;&#24050;&#32463;&#34987;&#21457;&#24067;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#32570;&#20047;&#23454;&#38469;&#30340;&#24037;&#19994;&#21046;&#36896;&#35774;&#32622;&#24456;&#21487;&#33021;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21457;&#23637;&#21644;&#20351;&#29992;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;IAD&#26041;&#27861;&#23578;&#26410;&#32463;&#36807;&#31995;&#32479;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#20998;&#26512;&#23427;&#20204;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20026;&#19981;&#21516;&#25110;&#29305;&#27530;&#24773;&#20917;&#32780;&#35774;&#35745;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#19994;&#21046;&#36896;&#35774;&#32622;&#26469;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20960;&#20010;&#26041;&#38754;&#65292;&#22914;&#21508;&#31181;&#30417;&#30563;&#32423;&#21035;&#65288;&#26080;&#30417;&#30563;vs&#21322;&#30417;&#30563;&#65289;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#22122;&#22768;&#26631;&#31614;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#26029;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24039;&#22937;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65288;IM-IAD&#65289;&#65292;&#35813;&#22522;&#20934;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#19979;&#21253;&#25324;&#20102;16&#20010;&#31639;&#27861;&#21644;7&#20010;&#20027;&#27969;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image anomaly detection (IAD) is an emerging and vital computer vision task in industrial manufacturing (IM). Recently many advanced algorithms have been published, but their performance deviates greatly. We realize that the lack of actual IM settings most probably hinders the development and usage of these methods in real-world applications. As far as we know, IAD methods are not evaluated systematically. As a result, this makes it difficult for researchers to analyze them because they are designed for different or special cases. To solve this problem, we first propose a uniform IM setting to assess how well these algorithms perform, which includes several aspects, i.e., various levels of supervision (unsupervised vs. semi-supervised), few-shot learning, continual learning, noisy labels, memory usage, and inference speed. Moreover, we skillfully build a comprehensive image anomaly detection benchmark (IM-IAD) that includes 16 algorithms on 7 mainstream datasets with uniform settings. 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#21457;&#29616;&#40479;&#30640;&#22270;&#34920;&#31034;&#23545;&#23450;&#20301;&#25915;&#20987;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#19988;&#19981;&#38656;&#35201;&#28145;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23545;&#25239;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.10766</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Adversarial Robustness of Camera-based 3D Object Detection. (arXiv:2301.10766v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10766
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#21457;&#29616;&#40479;&#30640;&#22270;&#34920;&#31034;&#23545;&#23450;&#20301;&#25915;&#20987;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#19988;&#19981;&#38656;&#35201;&#28145;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23545;&#25239;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22240;&#20854;&#22312;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#38754;&#23545;&#38024;&#23545;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#30340;&#37096;&#32626;&#26102;&#65292;&#20854;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#39046;&#20808;&#30340;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#25239;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#20004;&#20010;&#25915;&#20987;&#35774;&#32622;&#19979;&#30340;&#24377;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20998;&#31867;&#21644;&#23450;&#20301;&#36825;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#20687;&#32032;&#21644;&#22522;&#20110;&#34917;&#19969;&#20004;&#31181;&#23545;&#25239;&#25915;&#20987;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#24471;&#20986;&#20102;&#22235;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;(a)&#40479;&#30640;&#22270;&#34920;&#31034;&#23545;&#23450;&#20301;&#25915;&#20987;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65307;(b)&#19981;&#38656;&#35201;&#28145;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23545;&#25239;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, camera-based 3D object detection has gained widespread attention for its ability to achieve high performance with low computational cost. However, the robustness of these methods to adversarial attacks has not been thoroughly examined, especially when considering their deployment in safety-critical domains like autonomous driving. In this study, we conduct the first comprehensive investigation of the robustness of leading camera-based 3D object detection approaches under various adversarial conditions. We systematically analyze the resilience of these models under two attack settings: white-box and black-box; focusing on two primary objectives: classification and localization. Additionally, we delve into two types of adversarial attack techniques: pixel-based and patch-based. Our experiments yield four interesting findings: (a) bird's-eye-view-based representations exhibit stronger robustness against localization attacks; (b) depth-estimation-free approaches have the p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25511;&#21046;&#26080;&#20851;&#21477;&#23376;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#25277;&#35937;&#25688;&#35201;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AnswerSumm&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;20\%&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2212.09726</link><description>&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#26080;&#20851;&#21477;&#23376;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#25552;&#39640;&#25277;&#35937;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Faithfulness of Abstractive Summarization by Controlling Confounding Effect of Irrelevant Sentences. (arXiv:2212.09726v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09726
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#26080;&#20851;&#21477;&#23376;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#25277;&#35937;&#25688;&#35201;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AnswerSumm&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;20\%&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#31995;&#32479;&#22312;&#29983;&#25104;&#30475;&#20284;&#27969;&#21033;&#30340;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#20107;&#23454;&#27491;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#26080;&#20851;&#30340;&#36755;&#20837;&#25991;&#26412;&#37096;&#20998;&#21487;&#33021;&#23548;&#33268;&#20107;&#23454;&#19981;&#19968;&#33268;&#65292;&#20316;&#20026;&#28151;&#28102;&#22240;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#25928;&#24212;&#30340;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26469;&#37327;&#21270;&#28151;&#28102;&#30340;&#31243;&#24230;&#65292;&#24182;&#20934;&#30830;&#34913;&#37327;&#20854;&#23545;&#25688;&#35201;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#20174;&#29702;&#35770;&#32467;&#26524;&#20013;&#24471;&#20986;&#30340;&#35265;&#35299;&#65292;&#24403;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#30456;&#20851;&#21477;&#23376;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#26469;&#25511;&#21046;&#36825;&#31181;&#28151;&#28102;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#21407;&#21017;&#24615;&#30340;&#21051;&#30011;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#30456;&#20851;&#21477;&#23376;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;AnswerSumm&#19978;&#65288;&#21442;&#32771;&#25991;&#29486;&#65306;fabbri2021answersumm&#65289;&#19978;&#24378;&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#23558;&#20934;&#30830;&#24615;&#24471;&#20998;&#25552;&#39640;&#20102;20\%&#12290;
&lt;/p&gt;
&lt;p&gt;
Lack of factual correctness is an issue that still plagues state-of-the-art summarization systems despite their impressive progress on generating seemingly fluent summaries. In this paper, we show that factual inconsistency can be caused by irrelevant parts of the input text, which act as confounders. To that end, we leverage information-theoretic measures of causal effects to quantify the amount of confounding and precisely quantify how they affect the summarization performance. Based on insights derived from our theoretical results, we design a simple multi-task model to control such confounding by leveraging human-annotated relevant sentences when available. Crucially, we give a principled characterization of data distributions where such confounding can be large thereby necessitating the use of human annotated relevant sentences to generate factual summaries. Our approach improves faithfulness scores by 20\% over strong baselines on AnswerSumm \citep{fabbri2021answersumm}, a conver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#30340;&#20998;&#24067;&#25311;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#23616;&#20998;&#24067;&#25311;&#21512;&#21644;&#23616;&#37096;&#20998;&#24067;&#25311;&#21512;&#65292;&#21487;&#20197;&#38480;&#21046;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01521</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#30340;&#20998;&#24067;&#25311;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distribution Fitting for Combating Mode Collapse in Generative Adversarial Networks. (arXiv:2212.01521v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#30340;&#20998;&#24067;&#25311;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#23616;&#20998;&#24067;&#25311;&#21512;&#21644;&#23616;&#37096;&#20998;&#24067;&#25311;&#21512;&#65292;&#21487;&#20197;&#38480;&#21046;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#24335;&#23849;&#28291;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#32771;&#23519;&#20102;&#27169;&#24335;&#23849;&#28291;&#30340;&#21407;&#22240;&#12290;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38750;&#22343;&#21248;&#37319;&#26679;&#65292;&#19968;&#20123;&#23376;&#20998;&#24067;&#21487;&#33021;&#34987;&#38169;&#36807;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#29983;&#25104;&#30340;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#19981;&#21516;&#65292;GAN&#30446;&#26631;&#20173;&#28982;&#21487;&#20197;&#36798;&#21040;&#26368;&#23567;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#24809;&#32602;&#39033;&#30340;&#20840;&#23616;&#20998;&#24067;&#25311;&#21512;&#65288;GDF&#65289;&#26041;&#27861;&#26469;&#38480;&#21046;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#24403;&#29983;&#25104;&#30340;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#19981;&#21516;&#26102;&#65292;GDF&#23558;&#20351;&#30446;&#26631;&#21464;&#24471;&#26356;&#38590;&#36798;&#21040;&#26368;&#23567;&#20540;&#65292;&#32780;&#21407;&#22987;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#19981;&#21464;&#12290;&#38024;&#23545;&#26080;&#27861;&#33719;&#24471;&#25972;&#20307;&#30495;&#23454;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#20998;&#24067;&#25311;&#21512;&#65288;LDF&#65289;&#26041;&#27861;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;GDF&#21644;LDF&#30340;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mode collapse is a significant unsolved issue of generative adversarial networks. In this work, we examine the causes of mode collapse from a novel perspective. Due to the nonuniform sampling in the training process, some sub-distributions may be missed when sampling data. As a result, even when the generated distribution differs from the real one, the GAN objective can still achieve the minimum. To address the issue, we propose a global distribution fitting (GDF) method with a penalty term to confine the generated data distribution. When the generated distribution differs from the real one, GDF will make the objective harder to reach the minimal value, while the original global minimum is not changed. To deal with the circumstance when the overall real data is unreachable, we also propose a local distribution fitting (LDF) method. Experiments on several benchmarks demonstrate the effectiveness and competitive performance of GDF and LDF.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#27169;&#24072;&#20195;&#29702;&#65292;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#22312;&#24819;&#35937;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#25216;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#25506;&#32034;&#21644;&#25216;&#33021;&#23398;&#20064;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#21457;&#29616;&#25216;&#33021;&#65292;&#24182;&#36890;&#36807;&#20803;&#25511;&#21046;&#22120;&#36827;&#34892;&#39640;&#25928;&#30340;&#36866;&#24212;&#12290;&#24314;&#27169;&#24072;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#21644;&#21516;&#26102;&#25910;&#38598;&#25968;&#25454;&#20013;&#23398;&#20064;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.13350</link><description>&lt;p&gt;
&#24314;&#27169;&#24072;&#65306;&#22312;&#24819;&#35937;&#20013;&#23398;&#20064;&#21644;&#36866;&#24212;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Choreographer: Learning and Adapting Skills in Imagination. (arXiv:2211.13350v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#27169;&#24072;&#20195;&#29702;&#65292;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#22312;&#24819;&#35937;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#25216;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#25506;&#32034;&#21644;&#25216;&#33021;&#23398;&#20064;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#21457;&#29616;&#25216;&#33021;&#65292;&#24182;&#36890;&#36807;&#20803;&#25511;&#21046;&#22120;&#36827;&#34892;&#39640;&#25928;&#30340;&#36866;&#24212;&#12290;&#24314;&#27169;&#24072;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#21644;&#21516;&#26102;&#25910;&#38598;&#25968;&#25454;&#20013;&#23398;&#20064;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25216;&#33021;&#23398;&#20064;&#26088;&#22312;&#22312;&#27809;&#26377;&#22806;&#37096;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20016;&#23500;&#30340;&#34892;&#20026;&#24211;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#25511;&#21046;&#21644;&#24433;&#21709;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#32570;&#20047;&#36866;&#24403;&#30340;&#30693;&#35782;&#21644;&#25506;&#32034;&#65292;&#25216;&#33021;&#21487;&#33021;&#21482;&#33021;&#25511;&#21046;&#29615;&#22659;&#30340;&#26377;&#38480;&#21306;&#22495;&#65292;&#38480;&#21046;&#20854;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#21033;&#29992;&#23398;&#24471;&#30340;&#25216;&#33021;&#34892;&#20026;&#20197;&#25968;&#25454;&#39640;&#25928;&#30340;&#26041;&#24335;&#36866;&#24212;&#21518;&#32493;&#20219;&#21153;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24314;&#27169;&#24072;&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#21033;&#29992;&#20854;&#19990;&#30028;&#27169;&#22411;&#22312;&#24819;&#35937;&#20013;&#23398;&#20064;&#21644;&#36866;&#24212;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25506;&#32034;&#21644;&#25216;&#33021;&#23398;&#20064;&#36807;&#31243;&#35299;&#32806;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#21457;&#29616;&#25216;&#33021;&#12290;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#20195;&#29702;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#26469;&#35780;&#20272;&#21644;&#35843;&#25972;&#36890;&#36807;&#22312;&#24819;&#35937;&#20013;&#24182;&#34892;&#37096;&#32626;&#24050;&#23398;&#24471;&#30340;&#25216;&#33021;&#65292;&#20197;&#39640;&#25928;&#22320;&#36866;&#24212;&#23427;&#20204;&#12290;&#24314;&#27169;&#24072;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#21644;&#21516;&#26102;&#25910;&#38598;&#25968;&#25454;&#20013;&#23398;&#20064;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised skill learning aims to learn a rich repertoire of behaviors without external supervision, providing artificial agents with the ability to control and influence the environment. However, without appropriate knowledge and exploration, skills may provide control only over a restricted area of the environment, limiting their applicability. Furthermore, it is unclear how to leverage the learned skill behaviors for adapting to downstream tasks in a data-efficient manner. We present Choreographer, a model-based agent that exploits its world model to learn and adapt skills in imagination. Our method decouples the exploration and skill learning processes, being able to discover skills in the latent state space of the model. During adaptation, the agent uses a meta-controller to evaluate and adapt the learned skills efficiently by deploying them in parallel in imagination. Choreographer is able to learn skills both from offline data, and by collecting data simultaneously with an exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#26469;&#20998;&#26512;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.05359</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#25506;&#32034;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Local Explanations of Nonlinear Models Using Animated Linear Projections. (arXiv:2205.05359v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#26469;&#20998;&#26512;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#26085;&#30410;&#22686;&#24378;&#65292;&#20294;&#19982;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#30456;&#27604;&#65292;&#20854;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19979;&#38477;&#12290;&#36825;&#31181;&#25240;&#34935;&#23548;&#33268;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#20986;&#29616;&#65292;&#25552;&#20379;&#20102;&#35832;&#22914;&#23616;&#37096;&#35299;&#37322;&#65288;LE&#65289;&#21644;&#23616;&#37096;&#21464;&#37327;&#24402;&#22240;&#65288;LVA&#65289;&#20043;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#39044;&#27979;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;LVA&#36890;&#24120;&#19981;&#33021;&#26377;&#25928;&#22788;&#29702;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#23558;LVA&#36716;&#25442;&#20026;&#32447;&#24615;&#25237;&#24433;&#65292;&#24182;&#20351;&#29992;&#24452;&#21521;&#28216;&#35272;&#12290;&#36825;&#23545;&#20110;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#29359;&#38169;&#65292;&#25110;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#25110;&#35266;&#27979;&#20540;&#30340;&#32858;&#31867;&#20063;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#20351;&#29992;&#21508;&#31181;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#65288;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#31034;&#20363;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased predictive power of machine learning models comes at the cost of increased complexity and loss of interpretability, particularly in comparison to parametric statistical models. This trade-off has led to the emergence of eXplainable AI (XAI) which provides methods, such as local explanations (LEs) and local variable attributions (LVAs), to shed light on how a model use predictors to arrive at a prediction. These provide a point estimate of the linear variable importance in the vicinity of a single observation. However, LVAs tend not to effectively handle association between predictors. To understand how the interaction between predictors affects the variable importance estimate, we can convert LVAs into linear projections and use the radial tour. This is also useful for learning how a model has made a mistake, or the effect of outliers, or the clustering of observations. The approach is illustrated with examples from categorical (penguin species, chocolate types) and quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20046;&#38646;&#26679;&#26412;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36335;&#24452;&#20449;&#24687;&#26500;&#24314;&#20808;&#39564;&#31181;&#32676;&#65292;&#21487;&#20197;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2109.06826</link><description>&lt;p&gt;
&#20960;&#20046;&#38646;&#26679;&#26412;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Few-shot Quality-Diversity Optimization. (arXiv:2109.06826v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.06826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20046;&#38646;&#26679;&#26412;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36335;&#24452;&#20449;&#24687;&#26500;&#24314;&#20808;&#39564;&#31181;&#32676;&#65292;&#21487;&#20197;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#21033;&#29992;&#20197;&#21069;&#30340;&#23398;&#20064;&#32463;&#39564;&#21644;&#35774;&#35745;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#21644;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#28041;&#21450;&#30340;&#38382;&#39064;&#39046;&#22495;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#12290;&#19968;&#20010;&#26126;&#26174;&#30340;&#20363;&#22806;&#26159;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#20248;&#21270;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#20960;&#20046;&#27809;&#26377;&#20570;&#20986;&#21162;&#21147;&#12290;QD&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27450;&#39575;&#24615;&#26497;&#23567;&#20540;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#26412;&#36136;&#19978;&#20855;&#26377;&#20302;&#26679;&#26412;&#25928;&#29575;&#30340;&#36827;&#21270;&#36807;&#31243;&#65292;&#23427;&#20204;&#20173;&#28982;&#24456;&#26114;&#36149;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#20013;&#20248;&#21270;&#36335;&#24452;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#26500;&#24314;&#20808;&#39564;&#31181;&#32676;&#65292;&#24403;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#20351;&#29992;&#35813;&#31181;&#32676;&#21021;&#22987;&#21270;QD&#26041;&#27861;&#26102;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#36866;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#65292;&#23454;&#29616;&#36215;&#26469;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, a considerable amount of research has been dedicated to the exploitation of previous learning experiences and the design of Few-shot and Meta Learning approaches, in problem domains ranging from Computer Vision to Reinforcement Learning based control. A notable exception, where to the best of our knowledge, little to no effort has been made in this direction is Quality-Diversity (QD) optimization. QD methods have been shown to be effective tools in dealing with deceptive minima and sparse rewards in Reinforcement Learning. However, they remain costly due to their reliance on inherently sample inefficient evolutionary processes. We show that, given examples from a task distribution, information about the paths taken by optimization in parameter space can be leveraged to build a prior population, which when used to initialize QD methods in unseen environments, allows for few-shot adaptation. Our proposed method does not require backpropagation. It is simple to impl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#22823;&#24133;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#20256;&#32479;&#28857;&#31215;&#27880;&#24847;&#21147;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#39640;&#25928;&#26426;&#21046;&#30340;&#24212;&#29992;&#20351;&#24471;&#27880;&#24847;&#21147;&#27169;&#22359;&#21487;&#20197;&#26356;&#24191;&#27867;&#22320;&#38598;&#25104;&#21040;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/1812.01243</link><description>&lt;p&gt;
&#39640;&#25928;&#27880;&#24847;&#21147;&#65306;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Efficient Attention: Attention with Linear Complexities. (arXiv:1812.01243v10 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1812.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#22823;&#24133;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#20256;&#32479;&#28857;&#31215;&#27880;&#24847;&#21147;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#39640;&#25928;&#26426;&#21046;&#30340;&#24212;&#29992;&#20351;&#24471;&#27880;&#24847;&#21147;&#27169;&#22359;&#21487;&#20197;&#26356;&#24191;&#27867;&#22320;&#38598;&#25104;&#21040;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#31215;&#27880;&#24847;&#21147;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#36755;&#20837;&#22823;&#23567;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#36825;&#31181;&#22686;&#38271;&#38480;&#21046;&#20102;&#20854;&#22312;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#19982;&#28857;&#31215;&#27880;&#24847;&#21147;&#31561;&#25928;&#65292;&#20294;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#22823;&#22823;&#38477;&#20302;&#12290;&#20854;&#36164;&#28304;&#25928;&#29575;&#20351;&#24471;&#27880;&#24847;&#21147;&#27169;&#22359;&#33021;&#26356;&#24191;&#27867;&#12289;&#28789;&#27963;&#22320;&#38598;&#25104;&#21040;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#32463;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#21183;&#30340;&#26377;&#25928;&#24615;&#12290;&#39640;&#25928;&#27880;&#24847;&#21147;&#27169;&#22359;&#26174;&#33879;&#25552;&#21319;&#20102;&#23545;MS-COCO 2017&#19978;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#21644;&#23454;&#20363;&#20998;&#21106;&#22120;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36164;&#28304;&#25928;&#29575;&#20351;&#24471;&#22797;&#26434;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#27880;&#24847;&#21147;&#65292;&#32780;&#39640;&#25104;&#26412;&#38480;&#21046;&#20102;&#20351;&#29992;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#20197;&#31435;&#20307;&#35270;&#35273;&#20026;&#20363;&#65292;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo d
&lt;/p&gt;</description></item></channel></rss>