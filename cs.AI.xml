<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#29992;&#29289;&#29702;&#24341;&#25806;&#24378;&#21046;&#23454;&#29616;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#29289;&#29702;&#21512;&#29702;&#24615;&#22312;&#23454;&#36341;&#20013;&#26377;&#24456;&#22823;&#22256;&#38590;&#12290;&#36825;&#31687;&#35770;&#25991;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#35273;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#20511;&#21161;&#21387;&#21147;&#28909;&#22270;&#12289;&#21387;&#21147;&#20013;&#24515;&#21644;&#36523;&#20307;&#36136;&#24515;&#31561;&#26415;&#35821;&#65292;&#22312;&#20272;&#35745;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#21512;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.18246</link><description>&lt;p&gt;
&#22522;&#20110;&#30452;&#35273;&#29289;&#29702;&#30340;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
3D Human Pose Estimation via Intuitive Physics. (arXiv:2303.18246v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18246
&lt;/p&gt;
&lt;p&gt;
&#29992;&#29289;&#29702;&#24341;&#25806;&#24378;&#21046;&#23454;&#29616;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#29289;&#29702;&#21512;&#29702;&#24615;&#22312;&#23454;&#36341;&#20013;&#26377;&#24456;&#22823;&#22256;&#38590;&#12290;&#36825;&#31687;&#35770;&#25991;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#35273;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#20511;&#21161;&#21387;&#21147;&#28909;&#22270;&#12289;&#21387;&#21147;&#20013;&#24515;&#21644;&#36523;&#20307;&#36136;&#24515;&#31561;&#26415;&#35821;&#65292;&#22312;&#20272;&#35745;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20272;&#35745;&#20154;&#20307;&#23039;&#24577;&#26102;&#24448;&#24448;&#20250;&#20986;&#29616;&#19981;&#21512;&#29702;&#30340;&#36523;&#20307;&#20542;&#26012;&#12289;&#28014;&#21160;&#25110;&#31359;&#36879;&#22320;&#26495;&#30340;&#24773;&#20917;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#36523;&#20307;&#36890;&#24120;&#30001;&#22330;&#26223;&#25903;&#25745;&#30340;&#20107;&#23454;&#12290;&#29289;&#29702;&#24341;&#25806;&#21487;&#20197;&#29992;&#26469;&#24378;&#21046;&#23454;&#29616;&#29289;&#29702;&#21512;&#29702;&#24615;&#65292;&#20294;&#36825;&#20123;&#24341;&#25806;&#19981;&#21487;&#24494;&#20998;&#65292;&#20381;&#36182;&#20110;&#19981;&#29616;&#23454;&#30340;&#20195;&#29702;&#29289;&#20307;&#65292;&#24182;&#19988;&#38590;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#21644;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#26032;&#39062;&#30340;&#30452;&#35273;&#29289;&#29702;&#65288;IP&#65289;&#26415;&#35821;&#65292;&#36825;&#20123;&#26415;&#35821;&#21487;&#20197;&#20174;&#19968;&#20010;&#19982;&#22330;&#26223;&#30456;&#20114;&#20316;&#29992;&#30340;3D SMPL&#36523;&#20307;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#21463;&#29983;&#29289;&#21147;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#36523;&#20307;&#19978;&#30340;&#21387;&#21147;&#28909;&#22270;&#12289;&#28909;&#22270;&#19978;&#30340;&#21387;&#21147;&#20013;&#24515;&#65288;CoP&#65289;&#20197;&#21450;SMPL&#36523;&#20307;&#30340;&#36136;&#24515;&#12290;&#20511;&#21161;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;IPMAN&#65292;&#36890;&#36807;&#40723;&#21169;&#21512;&#29702;&#30340;&#22320;&#26495;&#25509;&#35302;&#21644;&#37325;&#21472;&#30340;CoP&#21644;CoM&#65292;&#22312;&#24425;&#33394;&#22270;&#20687;&#20013;&#20272;&#35745;&#19968;&#20010;&#8220;&#31283;&#23450;&#8221;&#30340;3D&#36523;&#20307;&#12290;&#25105;&#20204;&#30340;IP&#26415;&#35821;&#30452;&#35266;&#26131;&#25026;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#35745;&#31639;&#36895;&#24230;&#24555;&#65292;&#21487;&#24494;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#21644;&#22238;&#24402;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating 3D humans from images often produces implausible bodies that lean, float, or penetrate the floor. Such methods ignore the fact that bodies are typically supported by the scene. A physics engine can be used to enforce physical plausibility, but these are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. In contrast, we exploit novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Inspired by biomechanics, we infer the pressure heatmap on the body, the Center of Pressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). With these, we develop IPMAN, to estimate a 3D body from a color image in a "stable" configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, differentiable, and can be integrated into existing optimization and regression m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#26469;&#23454;&#29616;&#36523;&#20307;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20182;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#34920;&#24449;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18240</link><description>&lt;p&gt;
&#23547;&#25214;&#20855;&#26377;&#36523;&#20307;&#26234;&#33021;&#30340;&#20154;&#24037;&#35270;&#35273;&#30382;&#23618;&#22312;&#21738;&#37324;&#65311;
&lt;/p&gt;
&lt;p&gt;
Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?. (arXiv:2303.18240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#26469;&#23454;&#29616;&#36523;&#20307;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20182;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#34920;&#24449;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#65288;PVR&#65289;&#25110;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#29992;&#20110;&#36523;&#20307;&#26234;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102; CortexBench&#65292;&#20854;&#20013;&#21253;&#25324;&#28085;&#30422;&#21160;&#21147;&#23398;&#12289;&#23548;&#33322;&#12289;&#29087;&#32451;&#21644;&#31227;&#21160;&#25805;&#20316;&#30340;17&#31181;&#19981;&#21516;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31995;&#32479;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;PVR&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#12290;&#20026;&#20102;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#26469;&#33258;7&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#36229;&#36807;4000&#23567;&#26102;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65288;&#36229;&#36807;560&#19975;&#24352;&#22270;&#20687;&#65289;&#21644;ImageNet&#65292;&#20351;&#29992;&#20999;&#29255;&#25968;&#25454;&#30340;&#36974;&#30422;&#33258;&#32534;&#30721;&#65288;MAE&#65289;&#26469;&#35757;&#32451;&#19981;&#21516;&#22823;&#23567;&#30340;&#35270;&#35273;&#21464;&#24418;&#22120;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#25512;&#26029;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#25193;&#23637;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#65288;&#20294;&#24179;&#22343;&#24615;&#33021;&#26377;&#25152;&#25552;&#39640;&#65289;&#12290;&#25105;&#20204;&#26368;&#22823;&#30340;&#27169;&#22411;&#21517;&#20026;VC-1&#65292;&#24179;&#22343;&#34920;&#29616;&#36229;&#36807;&#25152;&#26377;&#20808;&#21069;&#30340;PVR&#65292;&#20294;&#20063;&#27809;&#26377;&#26222;&#36941;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;VC-1&#30340;&#29305;&#23450;&#20110;&#20219;&#21153;&#25110;&#39046;&#22495;&#30340;&#36866;&#24212;&#20250;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant.  To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average).  Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TPMCF&#65292;&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.18201</link><description>&lt;p&gt;
TPMCF: &#20351;&#29992;&#22810;&#28304;&#21327;&#21516;&#29305;&#24449;&#36827;&#34892;&#26102;&#38388;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features. (arXiv:2303.18201v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TPMCF&#65292;&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#26381;&#21153;API&#30340;&#24555;&#36895;&#37096;&#32626;&#65292;&#20010;&#24615;&#21270;&#30340;&#26381;&#21153;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#30340;&#22686;&#38271;&#20013;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20915;&#23450;&#26381;&#21153;&#24615;&#33021;&#30340;&#26381;&#21153;&#36136;&#37327;(QoS)&#21442;&#25968;&#32463;&#24120;&#34987;&#29992;&#20110;&#25512;&#33616;&#65292;&#20294;&#38543;&#26102;&#38388;&#27874;&#21160;&#12290;&#22240;&#27492;&#65292;QoS&#30340;&#39044;&#27979;&#23545;&#20110;&#22312;&#31561;&#20215;&#26381;&#21153;&#20013;&#35782;&#21035;&#21512;&#36866;&#30340;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#20195;&#30340;&#26102;&#38388;QoS&#39044;&#27979;&#26041;&#27861;&#30001;&#20110;&#21508;&#31181;&#38480;&#21046;&#32780;&#24456;&#38590;&#36798;&#21040;&#26399;&#26395;&#30340;&#31934;&#24230;&#65292;&#20363;&#22914;&#26080;&#27861;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#20197;&#21450;&#25429;&#33719;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#39640;&#38454;&#26102;&#38388;&#20851;&#31995;&#12290;&#34429;&#28982;&#26368;&#36817;&#19968;&#20123;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#24314;&#27169;QoS&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20854;&#20182;&#29305;&#24449;&#65288;&#20363;&#22914;&#21327;&#20316;&#29305;&#24449;&#65289;&#26469;&#29702;&#35299;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39044;&#27979;&#31934;&#24230;&#20250;&#38477;&#20302;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;TPMCF&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;TPMCF&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#65288;&#21253;&#25324;&#26102;&#38388;&#12289;&#29992;&#25143;&#21644;&#26381;&#21153;&#29305;&#24449;&#65289;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#20855;&#20307;&#22320;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#39062;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#21033;&#29992;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#39640;&#38454;&#26102;&#38388;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#26469;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;TPMCF&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the rapid deployment of service APIs, personalized service recommendations have played a paramount role in the growth of the e-commerce industry. Quality-of-Service (QoS) parameters determining the service performance, often used for recommendation, fluctuate over time. Thus, the QoS prediction is essential to identify a suitable service among functionally equivalent services over time. The contemporary temporal QoS prediction methods hardly achieved the desired accuracy due to various limitations, such as the inability to handle data sparsity and outliers and capture higher-order temporal relationships among user-service interactions. Even though some recent recurrent neural-network-based architectures can model temporal relationships among QoS data, prediction accuracy degrades due to the absence of other features (e.g., collaborative features) to comprehend the relationship among the user-service interactions. This paper addresses the above challenges and proposes a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#38388;&#35302;&#21457;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861; TeCo&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21463;&#23475;&#27169;&#22411;&#30340;&#30828;&#26631;&#31614;&#36755;&#20986;&#65292;&#36890;&#36807;&#35780;&#20272;&#27979;&#35797;&#26102;&#38388;&#40065;&#26834;&#24615;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#21518;&#38376;&#65292;&#19981;&#38656;&#35201;&#20854;&#20182;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.18191</link><description>&lt;p&gt;
&#22522;&#20110;&#30772;&#22351;&#40065;&#26834;&#24615;&#19968;&#33268;&#24615;&#30340;&#25512;&#29702;&#38454;&#27573;&#21518;&#38376;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency. (arXiv:2303.18191v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#38388;&#35302;&#21457;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861; TeCo&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21463;&#23475;&#27169;&#22411;&#30340;&#30828;&#26631;&#31614;&#36755;&#20986;&#65292;&#36890;&#36807;&#35780;&#20272;&#27979;&#35797;&#26102;&#38388;&#40065;&#26834;&#24615;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#21518;&#38376;&#65292;&#19981;&#38656;&#35201;&#20854;&#20182;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#26816;&#27979;&#35302;&#21457;&#26679;&#26412;&#65292;&#21363;&#27979;&#35797;&#26102;&#38388;&#35302;&#21457;&#26679;&#26412;&#26816;&#27979;&#65292;&#21487;&#20197;&#38450;&#27490;&#21518;&#38376;&#34987;&#35302;&#21457;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#38450;&#24481;&#32773;&#23545;&#21463;&#23475;&#27169;&#22411;&#20855;&#26377;&#39640;&#24230;&#21487;&#35775;&#38382;&#24615;&#12289;&#39069;&#22806;&#30340;&#28165;&#27905;&#25968;&#25454;&#25110;&#20102;&#35299;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#22806;&#35266;&#30693;&#35782;&#31561;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#38388;&#35302;&#21457;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861; TeCo&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21463;&#23475;&#27169;&#22411;&#30340;&#30828;&#26631;&#31614;&#36755;&#20986;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#19968;&#39033;&#26377;&#36259;&#30340;&#35266;&#23519;&#24320;&#22987;&#65292;&#21363;&#34987;&#24863;&#26579;&#30340;&#21518;&#38376;&#27169;&#22411;&#22312;&#23545;&#20110;&#24178;&#20928;&#22270;&#20687;&#30340;&#19981;&#21516;&#22270;&#20687;&#30772;&#22351;&#26041;&#38754;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#35302;&#21457;&#26679;&#26412;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102; TeCo &#26469;&#35780;&#20272;&#27979;&#35797;&#26102;&#38388;&#40065;&#26834;&#24615;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#30340;&#20559;&#24046;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are proven to be vulnerable to backdoor attacks. Detecting the trigger samples during the inference stage, i.e., the test-time trigger sample detection, can prevent the backdoor from being triggered. However, existing detection methods often require the defenders to have high accessibility to victim models, extra clean data, or knowledge about the appearance of backdoor triggers, limiting their practicality. In this paper, we propose the test-time corruption robustness consistency evaluation (TeCo), a novel test-time trigger sample detection method that only needs the hard-label outputs of the victim models without any extra information. Our journey begins with the intriguing observation that the backdoor-infected models have similar performance across different image corruptions for the clean images, but perform discrepantly for the trigger samples. Based on this phenomenon, we design TeCo to evaluate test-time robustness consistency by calculating the deviation o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.18171</link><description>&lt;p&gt;
&#20170;&#22825;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#26377;&#22810;&#39640;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Efficient Are Today's Continual Learning Algorithms?. (arXiv:2303.18171v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18171
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#36845;&#20195;&#23398;&#20064;&#28041;&#21450;&#20174;&#19981;&#26029;&#22686;&#38271;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#27969;&#20013;&#26356;&#26032;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#19978;&#65292;&#20294;&#36845;&#20195;&#23398;&#20064;&#32972;&#21518;&#30340;&#20027;&#35201;&#21160;&#26426;&#20043;&#19968;&#26159;&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#26032;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#38598;&#38543;&#26102;&#38388;&#22686;&#38271;&#65292;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#22522;&#26412;&#19978;&#35299;&#20915;&#20102;&#28798;&#38590;&#36951;&#24536;&#38382;&#39064;&#65292;&#20294;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#25928;&#29575;&#20851;&#27880;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#26377;&#20123;&#26041;&#27861;&#29978;&#33267;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#23436;&#25104;&#35757;&#32451;&#65281;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#36845;&#20195;&#23398;&#20064;&#19981;&#20165;&#20165;&#26159;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Continual learning involves updating a deep neural network (DNN) from an ever-growing stream of labeled data. While most work has focused on overcoming catastrophic forgetting, one of the major motivations behind continual learning is being able to efficiently update a network with new information, rather than retraining from scratch on the training dataset as it grows over time. Despite recent continual learning methods largely solving the catastrophic forgetting problem, there has been little attention paid to the efficiency of these algorithms. Here, we study recent methods for incremental class learning and illustrate that many are highly inefficient in terms of compute, memory, and storage. Some methods even require more compute than training from scratch! We argue that for continual learning to have real-world applicability, the research community cannot ignore the resources used by these algorithms. There is more to continual learning than mitigating catastrophic forg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.18158</link><description>&lt;p&gt;
&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#31209;&#19968;&#20989;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Optimization of Rank-One Functions with Indicator Variables. (arXiv:2303.18158v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#28041;&#21450;&#21040;&#36890;&#36807;&#32422;&#26463;&#26469;&#24314;&#27169;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#37319;&#29992;&#25351;&#26631;&#21464;&#37327;&#26469;&#35782;&#21035;&#36830;&#32493;&#21464;&#37327;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#36890;&#36807;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25903;&#25345;&#20989;&#25968;&#21442;&#25968;&#21644;&#31163;&#25955;&#35268;&#21010;&#25216;&#26415;&#20197;&#25552;&#20379;&#20984;&#21253;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26041;&#27861;&#65292;&#21033;&#29992;&#36879;&#35270;&#20989;&#25968;&#24341;&#36215;&#30340;&#38544;&#34255;&#22278;&#38181;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#27599;&#20010;&#22278;&#38181;&#32422;&#26463;&#28041;&#21450;&#29420;&#31435;&#36830;&#32493;&#21464;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#21644;&#19968;&#32452;&#20108;&#20803;&#21464;&#37327;&#30340;&#19968;&#33324;&#22278;&#38181;&#28151;&#21512;&#20108;&#36827;&#21046;&#38598;&#21512;&#24314;&#31435;&#20102;&#19968;&#20010;&#20984;&#21253;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#24212;&#23545;epi&#30456;&#20851;&#30340;&#38598;&#21512;&#30340;&#25193;&#23637;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems involving minimization of a rank-one convex function over constraints modeling restrictions on the support of the decision variables emerge in various machine learning applications. These problems are often modeled with indicator variables for identifying the support of the continuous variables. In this paper we investigate compact extended formulations for such problems through perspective reformulation techniques. In contrast to the majority of previous work that relies on support function arguments and disjunctive programming techniques to provide convex hull results, we propose a constructive approach that exploits a hidden conic structure induced by perspective functions. To this end, we first establish a convex hull result for a general conic mixed-binary set in which each conic constraint involves a linear function of independent continuous variables and a set of binary variables. We then demonstrate that extended representations of sets associated with epi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#31361;&#20986;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.18136</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids. (arXiv:2303.18136v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#31361;&#20986;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#65292;&#30001;&#20110;&#32463;&#27982;&#21644;&#20851;&#38190;&#24615;&#30340;&#21407;&#22240;&#65292;&#25925;&#38556;&#26816;&#27979;&#20219;&#21153;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#26234;&#33021;&#30005;&#32593;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#32570;&#38519;&#26816;&#27979;&#21644;&#36127;&#36733;&#39044;&#27979;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#30740;&#31350;&#26234;&#33021;&#30005;&#32593;&#24773;&#20917;&#19979;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#23433;&#20840;&#24615;&#25361;&#25112;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#23578;&#26410;&#19982;&#25152;&#26377;&#30005;&#32593;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#22320;&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#31361;&#20986;&#23637;&#31034;&#20102;&#25925;&#38556;&#23450;&#20301;&#21644;&#31867;&#22411;&#20998;&#31867;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#35828;&#26126;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In smart electrical grids, fault detection tasks may have a high impact on society due to their economic and critical implications. In the recent years, numerous smart grid applications, such as defect detection and load forecasting, have embraced data-driven methodologies. The purpose of this study is to investigate the challenges associated with the security of machine learning (ML) applications in the smart grid scenario. Indeed, the robustness and security of these data-driven algorithms have not been extensively studied in relation to all power grid applications. We demonstrate first that the deep neural network method used in the smart grid is susceptible to adversarial perturbation. Then, we highlight how studies on fault localization and type classification illustrate the weaknesses of present ML algorithms in smart grids to various adversarial attacks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;AdvCheck&#65292;&#36890;&#36807;&#35745;&#31639;&#26412;&#22320;&#26799;&#24230;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.18131</link><description>&lt;p&gt;
AdvCheck&#65306;&#36890;&#36807;&#26412;&#22320;&#26799;&#24230;&#26816;&#26597;&#34920;&#24449;&#23545;&#25239;&#29983;&#25104;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
AdvCheck: Characterizing Adversarial Examples via Local Gradient Checking. (arXiv:2303.18131v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;AdvCheck&#65292;&#36890;&#36807;&#35745;&#31639;&#26412;&#22320;&#26799;&#24230;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26679;&#26412;&#25915;&#20987;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#26469;&#34920;&#24449;&#23545;&#25239;&#29983;&#25104;&#26679;&#26412;&#30340;&#29305;&#24449;&#21807;&#19968;&#24615;&#65292;&#25110;&#21306;&#20998;&#30001;&#23545;&#25239;&#24615;&#26679;&#26412;&#35302;&#21457;&#30340;DNN&#30340;&#34892;&#20026;&#12290;&#22522;&#20110;&#29305;&#24449;&#30340;&#26816;&#27979;&#26041;&#27861;&#19981;&#33021;&#22788;&#29702;&#21463;&#21040;&#22823;&#25200;&#21160;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#36824;&#38656;&#35201;&#22823;&#37327;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#21478;&#19968;&#20010;&#20027;&#27969;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#34892;&#20026;&#34920;&#24449;&#36755;&#20837;&#23646;&#24615;&#65292;&#35745;&#31639;&#20195;&#20215;&#24456;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26412;&#22320;&#26799;&#24230;&#30340;&#27010;&#24565;&#65292;&#24182;&#25581;&#31034;&#20986;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#26412;&#22320;&#26799;&#24230;&#36739;&#27491;&#24120;&#26679;&#26412;&#26377;&#26356;&#22823;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#26412;&#22320;&#26799;&#24230;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;AdvCheck&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#20174;&#19968;&#20123;&#27491;&#24120;&#26679;&#26412;&#21644;&#28155;&#21152;&#22122;&#22768;&#30340;&#26434;&#39033;&#26679;&#26412;&#35745;&#31639;&#26412;&#22320;&#26799;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#25239;&#24615;&#26679;&#26412;&#21644;&#27491;&#24120;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdvCheck-LIME&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#24615;&#26469;&#22788;&#29702;&#26412;&#22320;&#26799;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#25928;&#29575;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to adversarial examples, which may lead to catastrophe in security-critical domains. Numerous detection methods are proposed to characterize the feature uniqueness of adversarial examples, or to distinguish DNN's behavior activated by the adversarial examples. Detections based on features cannot handle adversarial examples with large perturbations. Besides, they require a large amount of specific adversarial examples. Another mainstream, model-based detections, which characterize input properties by model behaviors, suffer from heavy computation cost. To address the issues, we introduce the concept of local gradient, and reveal that adversarial examples have a quite larger bound of local gradient than the benign ones. Inspired by the observation, we leverage local gradient for detecting adversarial examples, and propose a general framework AdvCheck. Specifically, by calculating the local gradient from a few benign examples and noise-added misc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#23569;&#37327;&#26631;&#31614;&#23454;&#29616;&#21307;&#30103;&#35270;&#39057;&#20013;&#20307;&#22806;&#38236;&#22836;&#33258;&#21160;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20445;&#25252;&#38544;&#31169;&#65292;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#20351;&#29992; 95% &#26356;&#23569;&#30340;&#26631;&#31614;&#26102;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.18106</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#26368;&#23569;&#26631;&#31614;&#23454;&#29616;&#21307;&#30103;&#35270;&#39057;&#20013;&#20307;&#22806;&#38236;&#22836;&#30340;&#33258;&#21160;&#26816;&#27979;&#20197;&#20445;&#25252;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Automatic Detection of Out-of-body Frames in Surgical Videos for Privacy Protection Using Self-supervised Learning and Minimal Labels. (arXiv:2303.18106v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#23569;&#37327;&#26631;&#31614;&#23454;&#29616;&#21307;&#30103;&#35270;&#39057;&#20013;&#20307;&#22806;&#38236;&#22836;&#33258;&#21160;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20445;&#25252;&#38544;&#31169;&#65292;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#20351;&#29992; 95% &#26356;&#23569;&#30340;&#26631;&#31614;&#26102;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#31397;&#38236;&#35270;&#39057;&#35760;&#24405;&#24191;&#27867;&#24212;&#29992;&#20110;&#24494;&#21019;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#65292;&#20294;&#24403;&#20869;&#38236;&#22312;&#30149;&#20154;&#20307;&#22806;&#26102;&#65292;&#21487;&#33021;&#20250;&#25429;&#33719;&#21040;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#26080;&#20851;&#29255;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#26368;&#23569;&#30340;&#25968;&#25454;&#26631;&#31614;&#65292;&#20934;&#30830;&#26816;&#27979;&#21307;&#30103;&#35270;&#39057;&#20013;&#30340;&#20307;&#22806;&#38236;&#22836;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#20869;&#31397;&#38236;&#22270;&#20687;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26377;&#38480;&#30340;&#30417;&#30563;&#19979;&#36827;&#34892;&#24494;&#35843;&#65292;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#20174; da Vinci X &#21644; Xi &#25163;&#26415;&#31995;&#32479;&#25293;&#25668;&#30340;&#21307;&#30103;&#35270;&#39057;&#20013;&#26816;&#27979;&#20307;&#22806;&#30011;&#38754;&#26102;&#65292;&#24179;&#22343; F1 &#24471;&#20998;&#22312; 96.00 &#21040; 98.02 &#20043;&#38388;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20165;&#20351;&#29992; 5% &#30340;&#35757;&#32451;&#26631;&#31614;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#28982;&#20445;&#25345;&#24179;&#22343; F1 &#24471;&#20998;&#22312; 97 &#20197;&#19978;&#65292;&#27604;&#20840;&#30417;&#30563;&#26041;&#27861;&#23569;&#29992; 95% &#30340;&#26631;&#31614;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Endoscopic video recordings are widely used in minimally invasive robot-assisted surgery, but when the endoscope is outside the patient's body, it can capture irrelevant segments that may contain sensitive information. To address this, we propose a framework that accurately detects out-of-body frames in surgical videos by leveraging self-supervision with minimal data labels. We use a massive amount of unlabeled endoscopic images to learn meaningful representations in a self-supervised manner. Our approach, which involves pre-training on an auxiliary task and fine-tuning with limited supervision, outperforms previous methods for detecting out-of-body frames in surgical videos captured from da Vinci X and Xi surgical systems. The average F1 scores range from 96.00 to 98.02. Remarkably, using only 5% of the training labels, our approach still maintains an average F1 score performance above 97, outperforming fully-supervised methods with 95% fewer labels. These results demonstrate the pote
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#35206;&#30422;14&#31181;&#35821;&#35328;&#12289;&#22810;&#20803;&#21270;&#30340;&#26102;&#38388;&#21644;&#25968;&#23383;&#34920;&#36798;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#12289;&#35268;&#33539;&#21270;&#21644;&#35299;&#26512;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;(NTX)&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20316;&#20026;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.18103</link><description>&lt;p&gt;
&#22810;&#35821;&#31181;&#26102;&#38388;&#21644;&#25968;&#23383;&#34920;&#36798;&#24335;&#30340;&#25277;&#21462;&#21644;&#35268;&#33539;&#21270;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Dataset and Baseline System for Multi-lingual Extraction and Normalization of Temporal and Numerical Expressions. (arXiv:2303.18103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#35206;&#30422;14&#31181;&#35821;&#35328;&#12289;&#22810;&#20803;&#21270;&#30340;&#26102;&#38388;&#21644;&#25968;&#23383;&#34920;&#36798;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#12289;&#35268;&#33539;&#21270;&#21644;&#35299;&#26512;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;(NTX)&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20316;&#20026;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#26102;&#38388;&#21644;&#25968;&#23383;&#34920;&#36798;&#24335;&#30340;&#29702;&#35299;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#24037;&#20316;&#20165;&#28085;&#30422;&#20102;&#23569;&#37327;&#30340;&#23376;&#31867;&#22411;&#65292;&#24182;&#19988;&#21482;&#20851;&#27880;&#23454;&#20307;&#25277;&#21462;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#35782;&#21035;&#21040;&#30340;&#25552;&#21450;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#22312;&#19979;&#28216;&#22330;&#26223;&#20013;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#65292;&#23376;&#31867;&#22411;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#31890;&#24230;&#24456;&#37325;&#35201;&#65307;&#24182;&#19988;&#26356;&#21152;&#37325;&#35201;&#30340;&#26159;&#65292;&#25552;&#20379;&#21487;&#20197;&#25805;&#20316;&#30340;&#20855;&#20307;&#20540;&#35299;&#26512;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20165;&#22788;&#29702;&#20960;&#31181;&#35821;&#35328;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#35780;&#20272;&#25968;&#25454;&#38598;-NTX-&#28085;&#30422;&#20102;14&#31181;&#35821;&#35328;&#30340;&#21508;&#31181;&#26102;&#38388;&#21644;&#25968;&#23383;&#34920;&#36798;&#24335;&#65292;&#24182;&#35206;&#30422;&#20102;&#25552;&#21462;&#65292;&#35268;&#33539;&#21270;&#21644;&#35299;&#26512;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20316;&#20026;&#19982;&#22312;&#35813;&#25968;&#25454;&#38598;&#20013;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#30340;&#24378;&#22823;&#22522;&#20934;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312; \url{https://aka.ms/NTX}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal and numerical expression understanding is of great importance in many downstream Natural Language Processing (NLP) and Information Retrieval (IR) tasks. However, much previous work covers only a few sub-types and focuses only on entity extraction, which severely limits the usability of identified mentions. In order for such entities to be useful in downstream scenarios, coverage and granularity of sub-types are important; and, even more so, providing resolution into concrete values that can be manipulated. Furthermore, most previous work addresses only a handful of languages. Here we describe a multi-lingual evaluation dataset - NTX - covering diverse temporal and numerical expressions across 14 languages and covering extraction, normalization, and resolution. Along with the dataset we provide a robust rule-based system as a strong baseline for comparisons against other models to be evaluated in this dataset. Data and code are available at \url{https://aka.ms/NTX}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;INoD&#30340;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;&#65292;&#36890;&#36807;&#29305;&#24449;&#26367;&#25442;&#21644;&#25968;&#25454;&#38598;&#37492;&#21035;&#30340;&#21407;&#21017;&#36827;&#34892;&#20892;&#30000;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18101</link><description>&lt;p&gt;
&#20892;&#30000;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
INoD: Injected Noise Discriminator for Self-Supervised Representation Learning in Agricultural Fields. (arXiv:2303.18101v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;INoD&#30340;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;&#65292;&#36890;&#36807;&#29305;&#24449;&#26367;&#25442;&#21644;&#25968;&#25454;&#38598;&#37492;&#21035;&#30340;&#21407;&#21017;&#36827;&#34892;&#20892;&#30000;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#39046;&#22495;&#30340;&#24863;&#30693;&#25968;&#25454;&#38598;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#37117;&#21463;&#38480;&#65292;&#36825;&#24433;&#21709;&#20102;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#35757;&#32451;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#27492;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#38024;&#23545;&#20892;&#19994;&#39046;&#22495;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;&#65288;INoD&#65289;&#65292;&#21033;&#29992;&#29305;&#24449;&#26367;&#25442;&#21644;&#25968;&#25454;&#38598;&#37492;&#21035;&#30340;&#21407;&#21017;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;INoD&#36890;&#36807;&#22312;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#21367;&#31215;&#32534;&#30721;&#20013;&#20132;&#38169;&#29305;&#24449;&#22270;&#65292;&#24182;&#39044;&#27979;&#20135;&#29983;&#30340;&#29305;&#24449;&#22270;&#30340;&#25968;&#25454;&#38598;&#38582;&#23646;&#20851;&#31995;&#20316;&#20026;&#39044;&#25991;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23545;&#35937;&#30340;&#26126;&#30830;&#34920;&#31034;&#65292;&#21516;&#26102;&#19982;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#30456;&#20284;&#29305;&#24449;&#19968;&#36215;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception datasets for agriculture are limited both in quantity and diversity which hinders effective training of supervised learning approaches. Self-supervised learning techniques alleviate this problem, however, existing methods are not optimized for dense prediction tasks in agriculture domains which results in degraded performance. In this work, we address this limitation with our proposed Injected Noise Discriminator (INoD) which exploits principles of feature replacement and dataset discrimination for self-supervised representation learning. INoD interleaves feature maps from two disjoint datasets during their convolutional encoding and predicts the dataset affiliation of the resultant feature map as a pretext task. Our approach enables the network to learn unequivocal representations of objects seen in one dataset while observing them in conjunction with similar features from the disjoint dataset. This allows the network to reason about higher-level semantics of the entailed o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#20013;&#30340;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20043;&#21069;&#19981;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.18062</link><description>&lt;p&gt;
&#35299;&#20915;&#24418;&#24577;&#23398;&#31867;&#27604;&#38382;&#39064;&#65306;&#20174;&#26816;&#32034;&#21040;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Solving morphological analogies: from retrieval to generation. (arXiv:2303.18062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#20013;&#30340;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20043;&#21069;&#19981;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#31181;&#38750;&#20961;&#33021;&#21147;&#65292;&#24182;&#19988;&#24050;&#34987;&#29992;&#26469;&#35299;&#20915;&#38590;&#20197;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290; &#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#65288;AR&#65289;&#21463;&#21040;&#20102;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20854;&#28508;&#21147;&#65292;&#20363;&#22914;&#20998;&#31867;&#65292;&#20915;&#31574;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#32467;&#26524;&#30340;&#25512;&#33616;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;AR&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#12290;&#35813;&#26694;&#26550;&#22312;&#25972;&#20010;Siganalogies&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#21333;&#35789;&#20043;&#38388;&#30340;&#24418;&#24577;&#23398;&#31867;&#27604;&#27604;&#20363;&#65288;APs&#65289;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20248;&#20110;&#31526;&#21495;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290; &#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#31867;&#27604;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#65288;ANNc&#65289;&#21644;&#26816;&#32034;&#38382;&#39064;&#19978;&#30340;&#31867;&#27604;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#65288;ANNr&#65289;&#65292;&#20197;&#21450;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#22312;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#21333;&#35789;&#19978;&#30340;&#28508;&#21147;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#24635;&#32467;&#24182;&#25193;&#23637;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20849;&#21516;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#22312;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#19981;&#23384;&#22312;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical inference is a remarkable capability of human reasoning, and has been used to solve hard reasoning tasks. Analogy based reasoning (AR) has gained increasing interest from the artificial intelligence community and has shown its potential in multiple machine learning tasks such as classification, decision making and recommendation with competitive results. We propose a deep learning (DL) framework to address and tackle two key tasks in AR: analogy detection and solving. The framework is thoroughly tested on the Siganalogies dataset of morphological analogical proportions (APs) between words, and shown to outperform symbolic approaches in many languages. Previous work have explored the behavior of the Analogy Neural Network for classification (ANNc) on analogy detection and of the Analogy Neural Network for retrieval (ANNr) on analogy solving by retrieval, as well as the potential of an autoencoder (AE) for analogy solving by generating the solution word. In this article we sum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#24314;&#27169;&#23545;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#30740;&#31350;&#30340;&#20215;&#20540;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#23454;&#29616;&#27431;&#27954;ATM&#24635;&#20307;&#35745;&#21010;&#20013;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#26041;&#38754;&#25152;&#36215;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.18060</link><description>&lt;p&gt;
NOSTROMO: &#25945;&#35757;&#12289;&#32467;&#35770;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
NOSTROMO: Lessons learned, conclusions and way forward. (arXiv:2303.18060v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#24314;&#27169;&#23545;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#30740;&#31350;&#30340;&#20215;&#20540;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#23454;&#29616;&#27431;&#27954;ATM&#24635;&#20307;&#35745;&#21010;&#20013;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#26041;&#38754;&#25152;&#36215;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30333;&#30382;&#20070;&#26088;&#22312;&#35299;&#37322;&#20803;&#24314;&#27169;&#23545;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;(ATM)&#30740;&#31350;&#30340;&#20215;&#20540;&#12290;&#23427;&#23558;&#23450;&#20041;&#20803;&#24314;&#27169;&#24182;&#25506;&#35752;&#20854;&#33021;&#21147;&#21644;&#19981;&#33021;&#20570;&#21040;&#30340;&#20107;&#24773;&#12290;&#35835;&#32773;&#20551;&#23450;&#20855;&#26377;SESAR&#30340;&#22522;&#30784;&#30693;&#35782;&#65306;&#21333;&#19968;&#27431;&#27954;&#22825;&#31354;ATM&#30740;&#31350;&#39033;&#30446;&#12290; SESAR&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#24102;&#26469;&#25913;&#36827;&#65292;&#35813;&#39033;&#30446;&#26159;&#21333;&#19968;&#27431;&#27954;&#22825;&#31354;&#20513;&#35758;&#30340;&#25216;&#26415;&#25903;&#26609;&#65292;&#25913;&#36827;&#26159;&#36890;&#36807;&#29305;&#23450;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;(KPIs)&#34913;&#37327;&#30340;&#65292;&#24182;&#30001;&#25152;&#35859;&#30340;SESAR&#8220;&#35299;&#20915;&#26041;&#26696;&#8221;&#31995;&#21015;&#26469;&#23454;&#26045;&#12290;&#36825;&#20123;&#8220;&#35299;&#20915;&#26041;&#26696;&#8221;&#26159;&#26032;&#30340;&#25110;&#25913;&#36827;&#30340;&#25805;&#20316;&#31243;&#24207;&#25110;&#25216;&#26415;&#65292;&#26088;&#22312;&#28385;&#36275;&#27431;&#27954;ATM&#24635;&#20307;&#35745;&#21010;&#20013;&#25551;&#36848;&#30340;&#25805;&#20316;&#21644;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This White Paper sets out to explain the value that metamodelling can bring to air traffic management (ATM) research. It will define metamodelling and explore what it can, and cannot, do. The reader is assumed to have basic knowledge of SESAR: the Single European Sky ATM Research project. An important element of SESAR, as the technological pillar of the Single European Sky initiative, is to bring about improvements, as measured through specific key performance indicators (KPIs), and as implemented by a series of so-called SESAR 'Solutions'. These 'Solutions' are new or improved operational procedures or technologies, designed to meet operational and performance improvements described in the European ATM Master Plan.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#22312;&#24320;&#25918;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;CORAL&#21644;MMD&#31561;&#31616;&#21333;DG&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#31454;&#20105;&#21147;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#31616;&#21333;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.18031</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#26159;&#24320;&#25918;&#39046;&#22495;&#27867;&#21270;&#30340;&#24378;&#22823;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simple Domain Generalization Methods are Strong Baselines for Open Domain Generalization. (arXiv:2303.18031v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18031
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#22312;&#24320;&#25918;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;CORAL&#21644;MMD&#31561;&#31616;&#21333;DG&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#31454;&#20105;&#21147;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#31616;&#21333;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22788;&#29702;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#65292;&#21363;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20986;&#29616;&#26410;&#30693;&#31867;&#21035;&#65292;&#20197;&#21450;&#39046;&#22495;&#28418;&#31227;&#65288;domain shift&#65289;&#65292;&#21363;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#26088;&#22312;&#22788;&#29702;&#25512;&#29702;&#38454;&#27573;&#30340;&#30446;&#26631;&#39046;&#22495;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#19981;&#21487;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#30340;&#39046;&#22495;&#28418;&#31227;&#24773;&#20917;&#12290;&#24320;&#25918;&#39046;&#22495;&#27867;&#21270;&#65288;ODG&#65289;&#21516;&#26102;&#32771;&#34385;&#20102;DG&#21644;OSR&#12290;&#39046;&#22495;&#22686;&#24378;&#20803;&#23398;&#20064;&#65288;DAML&#65289;&#26159;&#19968;&#20010;&#38754;&#21521;ODG&#30340;&#26041;&#27861;&#65292;&#20294;&#20854;&#23398;&#20064;&#36807;&#31243;&#36739;&#20026;&#22797;&#26434;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25552;&#20986;&#20102;&#21508;&#31181;DG&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#22312;ODG&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#29616;&#26377;&#30340;DG&#26041;&#27861;&#22312;ODG&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;DG&#26041;&#27861;&#65292;&#21363;CORrelation ALignment&#65288;CORAL&#65289;&#21644;Maximum Mean Discrepancy&#65288;MMD&#65289;&#22312;&#33509;&#24178;&#24773;&#20917;&#19979;&#19982;DAML&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23567;&#35843;&#25972;&#65292;&#25552;&#20986;&#20102;CORAL&#21644;MMD&#30340;&#31616;&#21333;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, a machine learning model is required to handle an open-set recognition (OSR), where unknown classes appear during the inference, in addition to a domain shift, where the distribution of data differs between the training and inference phases. Domain generalization (DG) aims to handle the domain shift situation where the target domain of the inference phase is inaccessible during model training. Open domain generalization (ODG) takes into account both DG and OSR. Domain-Augmented Meta-Learning (DAML) is a method targeting ODG but has a complicated learning process. On the other hand, although various DG methods have been proposed, they have not been evaluated in ODG situations. This work comprehensively evaluates existing DG methods in ODG and shows that two simple DG methods, CORrelation ALignment (CORAL) and Maximum Mean Discrepancy (MMD), are competitive with DAML in several cases. In addition, we propose simple extensions of CORAL and MMD by introducing th
&lt;/p&gt;</description></item><item><title>LaCViT&#26159;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#30340;&#21508;&#21521;&#31561;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#39640;&#20854;&#34920;&#31034;&#31354;&#38388;&#31561;&#24615;&#30340;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20116;&#20010;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18013</link><description>&lt;p&gt;
LaCViT&#65306;&#19968;&#31181;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550;&#65292;&#25552;&#39640;&#35270;&#35273;Transformer&#30340;&#34920;&#31034;&#31354;&#38388;&#30340;&#31561;&#24615;
&lt;/p&gt;
&lt;p&gt;
LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers. (arXiv:2303.18013v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18013
&lt;/p&gt;
&lt;p&gt;
LaCViT&#26159;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#30340;&#21508;&#21521;&#31561;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#39640;&#20854;&#34920;&#31034;&#31354;&#38388;&#31561;&#24615;&#30340;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20116;&#20010;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273; Transformer &#24050;&#32463;&#22312;&#22788;&#29702;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#27169;&#25311;&#38271;&#26102;&#38388;&#30340;&#29305;&#24449;&#20381;&#36182;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#21508;&#31181;&#33258;&#30417;&#30563;&#20449;&#21495;&#65288;&#20363;&#22914;&#65292;&#36974;&#34109;&#38543;&#26426;&#22359;&#65289;&#65292;&#35270;&#35273; Transformer &#22312; ImageNet-1k &#21644; CIFAR-10 &#31561;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#36890;&#29992;&#22823;&#35268;&#27169;&#22270;&#20687;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21482;&#33021;&#20135;&#29983;&#21508;&#21521;&#24322;&#24615;&#34920;&#31034;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550; LaCViT&#65292;&#23427;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#30340;&#31561;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LaCViT&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers have been incredibly effective when tackling computer vision tasks due to their ability to model long feature dependencies. By using large-scale training data and various self-supervised signals (e.g., masked random patches), vision transformers provide state-of-the-art performance on several benchmarking datasets, such as ImageNet-1k and CIFAR-10. However, these vision transformers pretrained over general large-scale image corpora could only produce an anisotropic representation space, limiting their generalizability and transferability to the target downstream tasks. In this paper, we propose a simple and effective Label-aware Contrastive Training framework LaCViT, which improves the isotropy of the pretrained representation space for vision transformers, thereby enabling more effective transfer learning amongst a wide range of image classification tasks. Through experimentation over five standard image classification datasets, we demonstrate that LaCViT-trained m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22686;&#24378;&#38598;&#20307;&#26234;&#33021;&#65288;ACI&#65289;&#22312;&#21327;&#21516;&#21019;&#26032;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#35774;&#35745;&#19968;&#20010;&#23454;&#39564;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#30340;&#34920;&#29616;&#65292;&#24182;&#32771;&#23519;&#19968;&#31181;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#24037;&#20855;Polis&#12290;&#20316;&#32773;&#35752;&#35770;&#20102;&#35774;&#35745;ACI&#23454;&#39564;&#26102;&#38656;&#35201;&#32771;&#34385;&#30340;&#19977;&#20010;&#25361;&#25112;&#65306;&#20027;&#39064;&#36873;&#25321;&#65292;&#21442;&#19982;&#32773;&#36873;&#25321;&#21644;&#32467;&#26524;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.18010</link><description>&lt;p&gt;
&#21327;&#21516;&#21019;&#26032;&#20013;&#30340;&#22686;&#24378;&#38598;&#20307;&#26234;&#33021;&#65306;&#35758;&#31243;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Augmented Collective Intelligence in Collaborative Ideation: Agenda and Challenges. (arXiv:2303.18010v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22686;&#24378;&#38598;&#20307;&#26234;&#33021;&#65288;ACI&#65289;&#22312;&#21327;&#21516;&#21019;&#26032;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#35774;&#35745;&#19968;&#20010;&#23454;&#39564;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#30340;&#34920;&#29616;&#65292;&#24182;&#32771;&#23519;&#19968;&#31181;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#24037;&#20855;Polis&#12290;&#20316;&#32773;&#35752;&#35770;&#20102;&#35774;&#35745;ACI&#23454;&#39564;&#26102;&#38656;&#35201;&#32771;&#34385;&#30340;&#19977;&#20010;&#25361;&#25112;&#65306;&#20027;&#39064;&#36873;&#25321;&#65292;&#21442;&#19982;&#32773;&#36873;&#25321;&#21644;&#32467;&#26524;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#26356;&#24212;&#34987;&#30475;&#20316;&#26159;&#21516;&#20276;&#32780;&#19981;&#26159;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI systems may be better thought of as peers than as tools. This paper explores applications of augmented collective intelligence (ACI) beneficial to collaborative ideation. Design considerations are offered for an experiment that evaluates the performance of hybrid human- AI collectives. The investigation described combines humans and large language models (LLMs) to ideate on increasingly complex topics. A promising real-time collection tool called Polis is examined to facilitate ACI, including case studies from citizen engagement projects in Taiwan and Bowling Green, Kentucky. The authors discuss three challenges to consider when designing an ACI experiment: topic selection, participant selection, and evaluation of results. The paper concludes that researchers should address these challenges to conduct empirical studies of ACI in collaborative ideation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#25919;&#27835;&#31185;&#23398;&#24037;&#20855;&#8212;&#8212;&#39044;&#27979;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25104;&#21151;&#26696;&#20363;&#25581;&#31034;&#20102;&#19968;&#31867;&#8220;&#36229;&#32423;&#39044;&#27979;&#32773;&#8221;&#65292;&#23545;&#20110;&#36866;&#24212;&#24555;&#36895;&#21464;&#21270;&#30340;&#25216;&#26415;&#29615;&#22659;&#65292;&#24314;&#35758;&#23558;&#39044;&#27979;&#20316;&#20026;&#31532;&#19968;&#36947;&#38450;&#32447;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2303.18006</link><description>&lt;p&gt;
&#25552;&#20986;&#26356;&#22909;&#30340;&#38382;&#39064;--&#39044;&#27979;&#30340;&#33402;&#26415;&#19982;&#31185;&#23398;&#65306;&#23454;&#29616;&#23545;&#39640;&#39118;&#38505;&#38382;&#39064;&#30495;&#23454;&#31572;&#26696;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Asking Better Questions -- The Art and Science of Forecasting: A mechanism for truer answers to high-stakes questions. (arXiv:2303.18006v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#25919;&#27835;&#31185;&#23398;&#24037;&#20855;&#8212;&#8212;&#39044;&#27979;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25104;&#21151;&#26696;&#20363;&#25581;&#31034;&#20102;&#19968;&#31867;&#8220;&#36229;&#32423;&#39044;&#27979;&#32773;&#8221;&#65292;&#23545;&#20110;&#36866;&#24212;&#24555;&#36895;&#21464;&#21270;&#30340;&#25216;&#26415;&#29615;&#22659;&#65292;&#24314;&#35758;&#23558;&#39044;&#27979;&#20316;&#20026;&#31532;&#19968;&#36947;&#38450;&#32447;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26080;&#27861;&#35780;&#20272;&#21644;&#22522;&#20934;&#23450;&#37327;&#25216;&#26415;&#21457;&#23637;&#27700;&#24179;&#65292;&#32452;&#32455;&#23601;&#23558;&#34987;&#36843;&#37319;&#21462;&#21453;&#24212;&#24615;&#26041;&#24335;&#24212;&#23545;&#27599;&#19968;&#27425;&#21464;&#21270;&#65292;&#20174;&#32780;&#38459;&#30861;&#20854;&#24314;&#31435;&#21487;&#34892;&#30340;&#20013;&#38271;&#26399;&#25112;&#30053;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25919;&#27835;&#31185;&#23398;&#24037;&#20855;&#8212;&#8212;&#39044;&#27979;&#30340;&#26368;&#36817;&#21457;&#23637;&#24773;&#20917;&#65292;&#35813;&#24037;&#20855;&#20351;&#29992;&#26126;&#30830;&#30340;&#20551;&#35774;&#21644;&#23450;&#37327;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#24403;&#39044;&#27979;&#22312;&#38598;&#20307;&#23618;&#38754;&#36827;&#34892;&#26102;&#65292;&#21487;&#20197;&#30830;&#23450;&#21644;&#39564;&#35777;&#20154;&#25165;&#65292;&#20351;&#39046;&#23548;&#32773;&#33021;&#22815;&#26500;&#24314;&#26356;&#22909;&#30340;&#25216;&#26415;&#21457;&#23637;&#27169;&#22411;&#20197;&#21450;&#25913;&#36827;&#21046;&#23450;&#25919;&#31574;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#39044;&#27979;&#30340;&#25104;&#21151;&#26696;&#20363;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31867;&#8220;&#36229;&#32423;&#39044;&#27979;&#32773;&#8221;&#65292;&#20182;&#20204;&#30340;&#35265;&#35299;&#26368;&#20026;&#21487;&#38752;&#65292;&#36229;&#36807;&#20102;98%&#30340;&#20154;&#32676;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#27010;&#36848;&#20102;&#25104;&#21151;&#39044;&#27979;&#32972;&#21518;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#33778;&#21033;&#26222;&#183;&#29305;&#29305;&#27931;&#20811;&#30340;&#8220;&#21313;&#35819;&#8221;&#12290;&#20026;&#20102;&#36866;&#24212;&#24555;&#36895;&#21464;&#21270;&#30340;&#25216;&#26415;&#29615;&#22659;&#65292;&#35774;&#35745;&#24072;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#24212;&#35813;&#23558;&#39044;&#27979;&#20316;&#20026;&#31532;&#19968;&#36947;&#38450;&#32447;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without the ability to estimate and benchmark AI capability advancements, organizations are left to respond to each change reactively, impeding their ability to build viable mid and long-term strategies. This paper explores the recent growth of forecasting, a political science tool that uses explicit assumptions and quantitative estimation that leads to improved prediction accuracy. Done at the collective level, forecasting can identify and verify talent, enable leaders to build better models of AI advancements and improve inputs into design policy. Successful approaches to forecasting and case studies are examined, revealing a subclass of "superforecasters" who outperform 98% of the population and whose insights will be most reliable. Finally, techniques behind successful forecasting are outlined, including Phillip Tetlock's "Ten Commandments." To adapt to a quickly changing technology landscape, designers and policymakers should consider forecasting as a first line of defense.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;36&#31687;&#25991;&#31456;&#30340;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#21463;&#21040;&#23567;&#26679;&#26412;&#37327;&#65292;&#28508;&#22312;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.18005</link><description>&lt;p&gt;
&#21365;&#24034;&#30284;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic Review. (arXiv:2303.18005v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;36&#31687;&#25991;&#31456;&#30340;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#21463;&#21040;&#23567;&#26679;&#26412;&#37327;&#65292;&#28508;&#22312;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;-&#29305;&#24449;&#21270;&#21644;&#35780;&#20272;&#24050;&#21457;&#34920;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#21033;&#29992;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#36827;&#34892;&#21365;&#24034;&#30284;&#35786;&#26029;&#25110;&#39044;&#21518;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#30340;&#36136;&#37327;&#12290;&#26041;&#27861;-&#22312;2022&#24180;1&#26376;12&#26085;&#20043;&#21069;&#65292;&#23545;5&#20010;&#26469;&#28304;&#36827;&#34892;&#25628;&#32034;&#12290;&#21253;&#25324;&#26631;&#20934;&#35201;&#27714;&#30740;&#31350;&#35780;&#20272;AI&#22312;&#21365;&#24034;&#30284;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#19978;&#65292;&#23545;&#21365;&#24034;&#30284;&#65292;&#21253;&#25324;&#36755;&#21365;&#31649;&#21365;&#24034;&#21644;&#33145;&#33180;&#32959;&#30244;&#30340;&#35786;&#26029;&#25110;&#39044;&#21518;&#25512;&#26029;&#12290;&#25490;&#38500;&#35780;&#35770;&#21644;&#38750;&#33521;&#35821;&#25991;&#31456;&#12290;&#23545;&#27599;&#20010;&#21253;&#21547;&#30340;&#27169;&#22411;&#20351;&#29992;PROBAST&#35780;&#20272;&#20559;&#20506;&#39118;&#38505;&#12290;&#32467;&#26524;-&#20849;&#21457;&#29616;1434&#31687;&#30740;&#31350;&#25991;&#31456;&#65292;&#20854;&#20013;36&#31687;&#31526;&#21512;&#32435;&#20837;&#26631;&#20934;&#12290;&#36825;&#20123;&#30740;&#31350;&#25253;&#21578;&#20102;62&#20010;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;35&#20010;&#20998;&#31867;&#22120;&#65292;14&#20010;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;7&#20010;&#20998;&#21106;&#27169;&#22411;&#21644;6&#20010;&#22238;&#24402;&#27169;&#22411;&#12290;&#20351;&#29992;1-1375&#24352;&#20174;1-664&#20010;&#21365;&#24034;&#30284;&#24739;&#32773;&#20013;&#24471;&#21040;&#30340;&#24187;&#28783;&#29255;&#24320;&#21457;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;&#39044;&#27979;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#24635;&#20307;&#29983;&#23384;&#65288;9/62&#65289;&#65292;&#32452;&#32455;&#23398;&#20122;&#22411;&#65288;7/62&#65289;&#21644;&#28107;&#24052;&#32467;&#29366;&#24577;&#65288;6/62&#65289;&#12290;&#32467;&#35770;-&#22522;&#20110;&#21487;&#29992;&#30340;&#25991;&#29486;&#65292;AI&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#21463;&#21040;&#26679;&#26412;&#37327;&#23567;&#12289;&#28508;&#22312;&#30340;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose - To characterise and assess the quality of published research evaluating artificial intelligence (AI) methods for ovarian cancer diagnosis or prognosis using histopathology data. Methods - A search of 5 sources was conducted up to 01/12/2022. The inclusion criteria required that research evaluated AI on histopathology images for diagnostic or prognostic inferences in ovarian cancer, including tubo-ovarian and peritoneal tumours. Reviews and non-English language articles were excluded. The risk of bias was assessed for every included model using PROBAST. Results - A total of 1434 research articles were identified, of which 36 were eligible for inclusion. These studies reported 62 models of interest, including 35 classifiers, 14 survival prediction models, 7 segmentation models, and 6 regression models. Models were developed using 1-1375 slides from 1-664 ovarian cancer patients. A wide array of outcomes were predicted, including overall survival (9/62), histological subtypes (7
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;NNetEn&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#65292;&#24182;&#22312;&#20998;&#31163;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17995</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#29109;(NNetEn)&#65306;&#22522;&#20110;&#29109;&#29305;&#24449;&#30340;&#33041;&#30005;&#20449;&#21495;&#21644;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#20998;&#31163;&#65292;&#29992;&#20110;NNetEn&#35745;&#31639;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series Separation by Entropy Features, Python Package for NNetEn Calculation. (arXiv:2303.17995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;NNetEn&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#65292;&#24182;&#22312;&#20998;&#31163;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#27979;&#37327;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;&#20256;&#32479;&#30340;&#29109;&#27979;&#37327;&#26041;&#27861;&#65292;&#20363;&#22914;&#39321;&#20892;&#29109;&#65292;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#65292;&#38656;&#35201;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;&#26469;&#34920;&#24449;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#29109;(NNetEn)&#27010;&#24565;&#26159;&#22522;&#20110;&#29305;&#27530;&#25968;&#25454;&#38598;(MNIST-10&#21644;SARS-CoV-2-RBV1)&#30340;&#20998;&#31867;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19982;&#35760;&#24405;&#22312;LogNNet&#31070;&#32463;&#32593;&#32476;&#20648;&#23618;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#29109;&#30456;&#20851;&#12290;NNetEn&#20197;&#21407;&#22987;&#26041;&#24335;&#20272;&#35745;&#26102;&#38388;&#24207;&#21015;&#30340;&#28151;&#27788;&#21160;&#24577;&#12290;&#22522;&#20110;NNetEn&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#20998;&#31867;&#24230;&#37327;&#65306;R2&#25928;&#29575;&#21644;&#30382;&#23572;&#36874;&#25928;&#29575;&#12290;NNetEn&#30340;&#25928;&#29575;&#22312;&#20351;&#29992;&#31163;&#25955;&#20998;&#26512;(ANOVA)&#20998;&#31163;&#27491;&#24358;&#26144;&#23556;&#30340;&#20004;&#20010;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#24471;&#21040;&#39564;&#35777;&#12290;&#23545;&#20110;&#20004;&#20010;&#25509;&#36817;&#30340;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015; (r=1.1918&#21644;r=1.2243)&#65292;F&#27604;&#20540;&#36798;&#21040;&#20102;124&#30340;&#20540;&#65292;&#21453;&#26144;&#20102;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entropy measures are effective features for time series classification problems. Traditional entropy measures, such as Shannon entropy, use probability distribution function. However, for the effective separation of time series, new entropy estimation methods are required to characterize the chaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn) is based on the classification of special datasets (MNIST-10 and SARS-CoV-2-RBV1) in relation to the entropy of the time series recorded in the reservoir of the LogNNet neural network. NNetEn estimates the chaotic dynamics of time series in an original way. Based on the NNetEn algorithm, we propose two new classification metrics: R2 Efficiency and Pearson Efficiency. The efficiency of NNetEn is verified on separation of two chaotic time series of sine mapping using dispersion analysis (ANOVA). For two close dynamic time series (r = 1.1918 and r = 1.2243), the F-ratio has reached the value of 124 and reflects high efficien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;FL4M&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;FL&#23545;&#20110;&#20445;&#25252;&#20803;&#23431;&#23449;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38544;&#31169;&#21644;&#38477;&#20302;&#26381;&#21153;&#22120;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17987</link><description>&lt;p&gt;
&#38754;&#21521;&#20803;&#23431;&#23449;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Metaverse: A Survey. (arXiv:2303.17987v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;FL4M&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;FL&#23545;&#20110;&#20445;&#25252;&#20803;&#23431;&#23449;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38544;&#31169;&#21644;&#38477;&#20302;&#26381;&#21153;&#22120;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20803;&#23431;&#23449;&#21457;&#23637;&#30340;&#36807;&#31243;&#20013;&#65292;&#25968;&#25454;&#37319;&#38598;&#21644;&#31169;&#20154;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#25104;&#20026;&#20102;&#21046;&#32422;&#20854;&#24191;&#27867;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#21151;&#33021;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22823;&#37327;&#36793;&#32536;&#35774;&#22791;&#30340;&#35757;&#32451;&#20219;&#21153;&#12290;&#23558;FL&#24212;&#29992;&#20110;&#20803;&#23431;&#23449;&#19981;&#20165;&#21487;&#20197;&#20445;&#25252;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;&#26381;&#21153;&#22120;&#19978;&#39640;&#35745;&#31639;&#33021;&#21147;&#21644;&#39640;&#23384;&#20648;&#37327;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;FL4M&#30340;&#19968;&#20123;&#26089;&#26399;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The metaverse, which is at the stage of innovation and exploration, faces the dilemma of data collection and the problem of private data leakage in the process of development. This can seriously hinder the widespread deployment of the metaverse. Fortunately, federated learning (FL) is a solution to the above problems. FL is a distributed machine learning paradigm with privacy-preserving features designed for a large number of edge devices. Federated learning for metaverse (FL4M) will be a powerful tool. Because FL allows edge devices to participate in training tasks locally using their own data, computational power, and model-building capabilities. Applying FL to the metaverse not only protects the data privacy of participants but also reduces the need for high computing power and high memory on servers. Until now, there have been many studies about FL and the metaverse, respectively. In this paper, we review some of the early advances of FL4M, which will be a research direction with u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20132;&#34588;&#32592;&#27010;&#24565;&#65292;&#29992;&#20110;&#21560;&#24341;&#23545;&#36890;&#29992;&#30446;&#26631;&#20027;&#39064;&#24863;&#20852;&#36259;&#30340;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#29992;&#25143;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#20869;&#23481;&#29983;&#25104;&#31574;&#30053;&#21644;&#21442;&#19982;&#35745;&#21010;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#32463;&#27982;&#23454;&#24800;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31038;&#20132;&#25968;&#25454;&#25910;&#38598;&#21644;&#30740;&#31350;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.17946</link><description>&lt;p&gt;
&#20154;&#31867;&#31038;&#20132;&#34588;&#32592;&#65306;&#36890;&#36807;&#33258;&#31649;&#29702;&#30340;Instagram&#39029;&#38754;&#21560;&#24341;&#20154;&#20204;
&lt;/p&gt;
&lt;p&gt;
Social Honeypot for Humans: Luring People through Self-managed Instagram Pages. (arXiv:2303.17946v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20132;&#34588;&#32592;&#27010;&#24565;&#65292;&#29992;&#20110;&#21560;&#24341;&#23545;&#36890;&#29992;&#30446;&#26631;&#20027;&#39064;&#24863;&#20852;&#36259;&#30340;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#29992;&#25143;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#20869;&#23481;&#29983;&#25104;&#31574;&#30053;&#21644;&#21442;&#19982;&#35745;&#21010;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#32463;&#27982;&#23454;&#24800;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31038;&#20132;&#25968;&#25454;&#25910;&#38598;&#21644;&#30740;&#31350;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#34588;&#32592;&#26159;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#37096;&#32626;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21560;&#24341;&#22403;&#22334;&#37038;&#20214;&#21644;&#26426;&#22120;&#20154;&#25191;&#34892;&#30340;&#24694;&#24847;&#27963;&#21160;&#12290;&#20026;&#27492;&#65292;&#23427;&#20204;&#30340;&#20869;&#23481;&#34987;&#35774;&#35745;&#25104;&#23545;&#24694;&#24847;&#29992;&#25143;&#26368;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#20869;&#23481;&#20027;&#39064;&#65292;&#36825;&#31181;&#21560;&#24341;&#26426;&#21046;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#29992;&#25143;&#65292;&#32780;&#19981;&#20165;&#26159;&#21560;&#24341;&#24694;&#24847;&#29992;&#25143;&#12290;&#22240;&#27492;&#65292;&#34588;&#32592;&#21487;&#20197;&#29992;&#20110;&#21560;&#24341;&#23545;&#24191;&#27867;&#20027;&#39064;&#24863;&#20852;&#36259;&#30340;&#20010;&#20154;&#65292;&#20174;&#20307;&#32946;&#21644;&#29233;&#22909;&#21040;&#26356;&#25935;&#24863;&#30340;&#25919;&#27835;&#35266;&#28857;&#21644;&#38452;&#35851;&#35770;&#12290;&#26377;&#20102;&#25152;&#26377;&#36825;&#20123;&#20010;&#20154;&#32858;&#38598;&#22312;&#21516;&#19968;&#20010;&#22320;&#26041;&#65292;&#34588;&#32592;&#25152;&#26377;&#32773;&#21487;&#20197;&#36827;&#34892;&#35768;&#22810;&#20998;&#26512;&#65292;&#20174;&#31038;&#20132;&#21040;&#24066;&#22330;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31038;&#20132;&#34588;&#32592;&#27010;&#24565;&#65292;&#29992;&#20110;&#21560;&#24341;&#23545;&#36890;&#29992;&#30446;&#26631;&#20027;&#39064;&#24863;&#20852;&#36259;&#30340;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#29992;&#25143;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#20869;&#23481;&#29983;&#25104;&#31574;&#30053;&#21644;&#21442;&#19982;&#35745;&#21010;&#30340;&#26694;&#26550;&#65292;&#20197;&#27169;&#25311;&#21512;&#27861;&#30340;Instagram&#39029;&#38754;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;Instagram&#19978;&#21019;&#24314;&#20102;21&#20010;&#33258;&#25105;&#31649;&#29702;&#30340;&#31038;&#20132;&#34588;&#32592;&#65292;&#20391;&#37325;&#20110;21&#20010;&#19981;&#21516;&#30340;&#20027;&#39064;&#65292;&#20174;&#31185;&#25216;&#21040;&#29983;&#27963;&#26041;&#24335;&#65292;&#24182;&#24635;&#20849;&#33719;&#24471;&#20102;3,592&#20010;&#26377;&#26426;&#31881;&#19997;&#12290;&#36890;&#36807;&#36825;&#20123;&#31038;&#20132;&#34588;&#32592;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#31038;&#20132;&#25968;&#25454;&#65292;&#29992;&#25143;&#34892;&#20026;&#21644;&#21442;&#19982;&#24230;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#25105;&#20204;&#34588;&#32592;&#21560;&#24341;&#20855;&#26377;&#19981;&#21516;&#20852;&#36259;&#21644;&#32972;&#26223;&#30340;&#29992;&#25143;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#32463;&#27982;&#23454;&#24800;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31038;&#20132;&#25968;&#25454;&#25910;&#38598;&#21644;&#30740;&#31350;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social Honeypots are tools deployed in Online Social Networks (OSN) to attract malevolent activities performed by spammers and bots. To this end, their content is designed to be of maximum interest to malicious users. However, by choosing an appropriate content topic, this attractive mechanism could be extended to any OSN users, rather than only luring malicious actors. As a result, honeypots can be used to attract individuals interested in a wide range of topics, from sports and hobbies to more sensitive subjects like political views and conspiracies. With all these individuals gathered in one place, honeypot owners can conduct many analyses, from social to marketing studies.  In this work, we introduce a novel concept of social honeypot for attracting OSN users interested in a generic target topic. We propose a framework based on fully-automated content generation strategies and engagement plans to mimic legit Instagram pages. To validate our framework, we created 21 self-managed soc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38468;&#36817;&#36947;&#36335;&#32467;&#26500;&#30340;&#20960;&#20309;&#38480;&#21046;&#30340;&#36807;&#31243;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38750;&#23436;&#20840;&#22278;&#24418;&#19988;&#31867;&#20284;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36710;&#36947;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.17900</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27979;&#35797;&#30340;&#22797;&#26434;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36807;&#31243;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Procedural Generation of Complex Roundabouts for Autonomous Vehicle Testing. (arXiv:2303.17900v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38468;&#36817;&#36947;&#36335;&#32467;&#26500;&#30340;&#20960;&#20309;&#38480;&#21046;&#30340;&#36807;&#31243;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38750;&#23436;&#20840;&#22278;&#24418;&#19988;&#31867;&#20284;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36710;&#36947;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#28165;&#36947;&#36335;&#26159;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#27169;&#25311;&#27979;&#35797;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#32780;&#29615;&#24418;&#20132;&#21449;&#21475;&#26159;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#36335;&#27573;&#65292;&#30446;&#21069;&#23545;&#20854;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#38468;&#36817;&#36947;&#36335;&#32467;&#26500;&#30340;&#20960;&#20309;&#38480;&#21046;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#36896;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36807;&#31243;&#29983;&#25104;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#19981;&#23436;&#20840;&#22278;&#24418;&#19988;&#31867;&#20284;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36710;&#36947;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#36830;&#25509;&#21040;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36884;&#24452;&#36947;&#36335;&#30340;&#20219;&#24847;&#35282;&#24230;&#12290;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#29615;&#24418;&#20132;&#21449;&#21475;&#34701;&#20837;&#39640;&#28165;&#36947;&#36335;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25110;&#20351;&#29992;&#29420;&#31435;&#30340;&#29615;&#24418;&#20132;&#21449;&#21475;&#36827;&#34892;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-definition roads are an essential component of realistic driving scenario simulation for autonomous vehicle testing. Roundabouts are one of the key road segments that have not been thoroughly investigated. Based on the geometric constraints of the nearby road structure, this work presents a novel method for procedurally building roundabouts. The suggested method can result in roundabout lanes that are not perfectly circular and resemble real-world roundabouts by allowing approaching roadways to be connected to a roundabout at any angle. One can easily incorporate the roundabout in their HD road generation process or use the standalone roundabouts in scenario-based testing of autonomous driving.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#26080;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#33976;&#39311;&#35757;&#32451;&#32858;&#31867;&#22836;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36890;&#36807;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#26469;&#23398;&#20064;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#22312;ImageNet&#21644;CIFAR100&#30340;17&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#23558;&#32858;&#31867;&#31934;&#24230;&#30456;&#23545;&#20110;k-&#22343;&#20540;&#25552;&#39640;&#20102;6.1%&#21644;12.2%&#12290;&#22312;ImageNet&#19978;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#33021;&#22815;&#23558;&#32858;&#31867;&#20934;&#30830;&#24230;&#25552;&#39640;&#21040;61.6%&#12290;</title><link>http://arxiv.org/abs/2303.17896</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25506;&#32034;&#28145;&#24230;&#22270;&#20687;&#32858;&#31867;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limits of Deep Image Clustering using Pretrained Models. (arXiv:2303.17896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#26080;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#33976;&#39311;&#35757;&#32451;&#32858;&#31867;&#22836;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36890;&#36807;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#26469;&#23398;&#20064;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#22312;ImageNet&#21644;CIFAR100&#30340;17&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#23558;&#32858;&#31867;&#31934;&#24230;&#30456;&#23545;&#20110;k-&#22343;&#20540;&#25552;&#39640;&#20102;6.1%&#21644;12.2%&#12290;&#22312;ImageNet&#19978;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#33021;&#22815;&#23558;&#32858;&#31867;&#20934;&#30830;&#24230;&#25552;&#39640;&#21040;61.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#23398;&#20064;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#20013;&#26368;&#36817;&#37051;&#23621;&#20849;&#20139;&#30456;&#21516;&#26631;&#31614;&#30340;&#20107;&#23454;&#23545;&#32858;&#31867;&#22836;&#36827;&#34892;&#33258;&#33976;&#39311;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#28857;&#23545;&#28857;&#30340;&#20114;&#20449;&#24687;&#21464;&#37327;&#20197;&#21450;&#23454;&#20363;&#21152;&#26435;&#26469;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#33021;&#22815;&#20943;&#24369;&#20551;&#38451;&#24615;&#23545;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#39640;&#25928;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;ImageNet&#21644;CIFAR100&#30340;17&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#23558;&#32858;&#31867;&#31934;&#24230;&#30456;&#23545;&#20110;k-&#22343;&#20540;&#25552;&#39640;&#20102;6.1%&#21644;12.2%&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#25105;&#20204;&#23558;&#22312;ImageNet&#19978;&#30340;&#32858;&#31867;&#20934;&#30830;&#24230;&#25552;&#39640;&#21040;&#20102;61.6%&#12290;&#20195;&#30721;&#23558;&#20844;&#24320;&#28304;&#20195;&#30721;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general methodology that learns to classify images without labels by leveraging pretrained feature extractors. Our approach involves self-distillation training of clustering heads, based on the fact that nearest neighbors in the pretrained feature space are likely to share the same label. We propose a novel objective to learn associations between images by introducing a variant of pointwise mutual information together with instance weighting. We demonstrate that the proposed objective is able to attenuate the effect of false positive pairs while efficiently exploiting the structure in the pretrained feature space. As a result, we improve the clustering accuracy over $k$-means on $17$ different pretrained models by $6.1$\% and $12.2$\% on ImageNet and CIFAR100, respectively. Finally, using self-supervised pretrained vision transformers we push the clustering accuracy on ImageNet to $61.6$\%. The code will be open-sourced.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Interval Logic Tensor Networks (ILTN)&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#21487;&#20197;&#22788;&#29702;&#27169;&#31946;&#36923;&#36753;&#12289;&#27169;&#31946;&#26102;&#38388;&#21306;&#38388;&#30340;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#25512;&#29702;&#20107;&#20214;&#21644;&#39044;&#27979;&#23427;&#20204;&#30340;&#27169;&#31946;&#25345;&#32493;&#26102;&#38388;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.17892</link><description>&lt;p&gt;
&#21306;&#38388;&#36923;&#36753;&#24352;&#37327;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Interval Logic Tensor Networks. (arXiv:2303.17892v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Interval Logic Tensor Networks (ILTN)&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#21487;&#20197;&#22788;&#29702;&#27169;&#31946;&#36923;&#36753;&#12289;&#27169;&#31946;&#26102;&#38388;&#21306;&#38388;&#30340;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#25512;&#29702;&#20107;&#20214;&#21644;&#39044;&#27979;&#23427;&#20204;&#30340;&#27169;&#31946;&#25345;&#32493;&#26102;&#38388;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20004;&#25490;&#24207;&#30693;&#35782;&#34920;&#31034;&#31995;&#32479;Interval Real Logic (IRL)&#65292;&#29992;&#20110;&#35299;&#37322;&#39034;&#24207;&#23646;&#24615;&#65288;&#30165;&#36857;&#65289;&#21644;&#20107;&#20214;&#23646;&#24615;&#65292;&#20351;&#29992;&#23454;&#25968;&#29305;&#24449;&#25968;&#25454;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#31946;&#36923;&#36753;&#23545;&#36830;&#25509;&#35789;&#36827;&#34892;&#35299;&#37322;&#65292;&#20351;&#29992;&#26799;&#24418;&#27169;&#31946;&#21306;&#38388;&#23545;&#20107;&#20214;&#25345;&#32493;&#26102;&#38388;&#36827;&#34892;&#35299;&#37322;&#65292;&#20351;&#29992;&#31034;&#24615;&#20989;&#25968;&#23545;&#27169;&#31946;&#26102;&#38388;&#20851;&#31995;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Interval Logic Tensor Networks(ILTN)&#36825;&#20010;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#36890;&#36807;IRL&#36890;&#36807;&#26799;&#24230;&#20256;&#25773;&#36827;&#34892;&#23398;&#20064;&#12290;&#20026;&#20102;&#25903;&#25345;&#26377;&#25928;&#30340;&#23398;&#20064;&#65292;ILTN&#20351;&#29992;softplus&#28608;&#27963;&#20989;&#25968;&#23450;&#20041;&#20102;&#24179;&#28369;&#29256;&#26412;&#30340;IRL&#30340;&#27169;&#31946;&#21306;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#38656;&#35201;&#25512;&#29702;&#20107;&#20214;&#24182;&#39044;&#27979;&#23427;&#20204;&#30340;&#27169;&#31946;&#25345;&#32493;&#26102;&#38388;&#30340;&#21512;&#25104;&#20219;&#21153;&#20013;&#65292;ILTN&#21487;&#20197;&#25104;&#21151;&#21033;&#29992;&#34920;&#31034;&#22312;IRL&#20013;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20351;&#20107;&#20214;&#31526;&#21512;&#32972;&#26223;&#26102;&#38388;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Interval Real Logic (IRL), a two-sorted logic that interprets knowledge such as sequential properties (traces) and event properties using sequences of real-featured data. We interpret connectives using fuzzy logic, event durations using trapezoidal fuzzy intervals, and fuzzy temporal relations using relationships between the intervals' areas. We propose Interval Logic Tensor Networks (ILTN), a neuro-symbolic system that learns by propagating gradients through IRL. In order to support effective learning, ILTN defines smoothened versions of the fuzzy intervals and temporal relations of IRL using softplus activations. We show that ILTN can successfully leverage knowledge expressed in IRL in synthetic tasks that require reasoning about events to predict their fuzzy durations. Our results show that the system is capable of making events compliant with background temporal knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#29983;&#25104;&#20266;&#26631;&#31614;&#20316;&#20026;&#19968;&#31181;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.17841</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Generative Probabilistic Model for Weak Supervised Learning. (arXiv:2303.17841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#29983;&#25104;&#20266;&#26631;&#31614;&#20316;&#20026;&#19968;&#31181;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#30456;&#20851;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#23454;&#36341;&#32773;&#26469;&#35828;&#26159;&#19968;&#20010;&#20027;&#35201; bottleneck&#12290;&#32780;&#19988;&#65292;&#20026;&#20102;&#35299;&#20915;&#37326;&#24515;&#21187;&#21187;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#38382;&#39064;&#65292;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#38468;&#24102;&#24102;&#26377;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#26631;&#31614;&#65292;&#20197;&#20415;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25163;&#21160;&#26631;&#35760;&#20855;&#26377;&#39640;&#36136;&#37327;&#26631;&#31614;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24448;&#24448;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#29942;&#39048;&#12290;&#24369;&#30417;&#30563;&#23398;&#20064; (WSL) &#26041;&#27861;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#36890;&#36807;&#26681;&#25454;&#21551;&#21457;&#24335;&#12289;&#36828;&#31243;&#30417;&#35270;&#21644;&#30693;&#35782;&#24211;&#26469;&#36171;&#20104;&#26410;&#26631;&#35760;&#25968;&#25454;&#22823;&#32422;&#26631;&#31614; (&#20266;&#26631;&#31614;) &#30340;&#33258;&#21160;&#26041;&#24335;&#65292;&#20174;&#32780;&#20943;&#36731;&#27880;&#37322;&#36127;&#25285;&#12290;&#25105;&#20204;&#24212;&#29992;&#27010;&#29575;&#29983;&#25104;&#38544;&#21464;&#37327;&#27169;&#22411; (PLVMs)&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#34920;&#31034;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20316;&#20026;&#19968;&#31181;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; PLVMs &#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#22810;&#25165;&#22810;&#33402;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding relevant and high-quality datasets to train machine learning models is a major bottleneck for practitioners. Furthermore, to address ambitious real-world use-cases there is usually the requirement that the data come labelled with high-quality annotations that can facilitate the training of a supervised model. Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns out to be the bottleneck in a machine learning project. Weak Supervised Learning (WSL) approaches have been developed to alleviate the annotation burden by offering an automatic way of assigning approximate labels (pseudo-labels) to unlabelled data based on heuristics, distant supervision and knowledge bases. We apply probabilistic generative latent variable models (PLVMs), trained on heuristic labelling representations of the original dataset, as an accurate, fast and cost-effective way to generate pseudo-labels. We show that the PLVMs achieve state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#20013;&#23398;&#20064;&#36807;&#31243;&#24863;&#30693;&#30340;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#65292;&#32852;&#21512;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#21644;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;&#36807;&#31243;&#25512;&#29702;&#30340;&#26032;&#21151;&#33021;&#65292;&#21516;&#26102;&#23545;&#20010;&#20307;&#27493;&#39588;&#30340;&#35782;&#21035;&#20063;&#33021;&#24471;&#21040;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2303.17839</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#20013;&#23398;&#20064;&#36807;&#31243;&#24863;&#30693;&#30340;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations. (arXiv:2303.17839v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#20013;&#23398;&#20064;&#36807;&#31243;&#24863;&#30693;&#30340;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#65292;&#32852;&#21512;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#21644;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;&#36807;&#31243;&#25512;&#29702;&#30340;&#26032;&#21151;&#33021;&#65292;&#21516;&#26102;&#23545;&#20010;&#20307;&#27493;&#39588;&#30340;&#35782;&#21035;&#20063;&#33021;&#24471;&#21040;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#30340;&#20016;&#23500;&#36164;&#28304;&#20026;&#29702;&#35299;&#36807;&#31243;&#27963;&#21160;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#34920;&#31034;&#23545;&#22522;&#20110;&#22823;&#35268;&#27169;&#32593;&#32476;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#30340;&#20010;&#20307;&#27493;&#39588;&#21450;&#20854;&#26102;&#38388;&#39034;&#24207;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#19981;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#12290;&#26041;&#27861;&#32852;&#21512;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#21644;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#27493;&#39588;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#24040;&#22823;&#20010;&#20307;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23398;&#20064;&#26102;&#38388;&#25490;&#24207;&#19981;&#20165;&#33021;&#22815;&#22686;&#24378;&#36807;&#31243;&#25512;&#29702;&#30340;&#26032;&#21151;&#33021;&#65292;&#36824;&#21487;&#20197;&#21152;&#24378;&#23545;&#20010;&#20307;&#27493;&#39588;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27493;&#39588;&#20998;&#31867;&#65288;&#22312;COIN/EPIC-Kitchens&#19978;&#20998;&#21035;&#22686;&#21152;2.8% / 3.3%&#65289;&#21644;&#27493;&#39588;&#39044;&#27979;&#65288;&#22312;COIN&#19978;&#22686;&#21152;7.4%&#65289;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27493;&#39588;&#25552;&#21462;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abundance of instructional videos and their narrations over the Internet offers an exciting avenue for understanding procedural activities. In this work, we propose to learn video representation that encodes both action steps and their temporal ordering, based on a large-scale dataset of web instructional videos and their narrations, without using human annotations. Our method jointly learns a video representation to encode individual step concepts, and a deep probabilistic model to capture both temporal dependencies and immense individual variations in the step ordering. We empirically demonstrate that learning temporal ordering not only enables new capabilities for procedure reasoning, but also reinforces the recognition of individual steps. Our model significantly advances the state-of-the-art results on step classification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting (+7.4% on COIN). Moreover, our model attains promising results in zero-shot inference for step c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#29305;&#23450;&#36755;&#20837;&#26174;&#33879;&#24615;&#26144;&#23556;&#35270;&#35282;&#65292;&#23427;&#35745;&#31639;&#20102;&#27169;&#22411;&#23545;&#20854;&#36755;&#20986;&#25152;&#24402;&#23646;&#30340;&#39640;&#32423;&#29305;&#24449;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#29420;&#31435;&#20110;&#36755;&#20837;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#65292;&#19988;&#40065;&#26834;&#24615;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.17836</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#35299;&#37322;&#65306;&#28145;&#24230;&#35270;&#35273;&#20998;&#31867;&#22120;&#30340;&#26080;&#29305;&#23450;&#36755;&#20837;&#26174;&#33879;&#24615;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Rethinking interpretation: Input-agnostic saliency mapping of deep visual classifiers. (arXiv:2303.17836v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17836
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#29305;&#23450;&#36755;&#20837;&#26174;&#33879;&#24615;&#26144;&#23556;&#35270;&#35282;&#65292;&#23427;&#35745;&#31639;&#20102;&#27169;&#22411;&#23545;&#20854;&#36755;&#20986;&#25152;&#24402;&#23646;&#30340;&#39640;&#32423;&#29305;&#24449;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#29420;&#31435;&#20110;&#36755;&#20837;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#65292;&#19988;&#40065;&#26834;&#24615;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#24615;&#26041;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#29305;&#24449;&#24402;&#23646;&#20110;&#27169;&#22411;&#36755;&#20986;&#65292;&#25552;&#20379;&#20107;&#21518;&#30340;&#27169;&#22411;&#35299;&#37322;&#12290; &#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#21333;&#20010;&#36755;&#20837;&#26679;&#26412;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22240;&#27492;&#26080;&#27861;&#22238;&#31572;&#26377;&#20851;&#27169;&#22411;&#30340;&#29420;&#31435;&#20110;&#36755;&#20837;&#30340;&#26597;&#35810;&#12290; &#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#26174;&#33879;&#24615;&#26144;&#23556;&#26412;&#36136;&#19978;&#23481;&#26131;&#21463;&#21040;&#35823;&#23548;&#24615;&#29305;&#24449;&#24402;&#23646;&#30340;&#24433;&#21709;&#12290;&#35797;&#22270;&#20351;&#29992;&#8220;&#36890;&#29992;&#8221;&#36755;&#20837;&#29305;&#24449;&#26469;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#30340;&#29616;&#26377;&#23581;&#35797;&#20551;&#23450;&#21487;&#20197;&#35775;&#38382;&#21253;&#21547;&#36825;&#20123;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20250;&#23548;&#33268;&#35299;&#37322;&#30340;&#20559;&#24046;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#29305;&#23450;&#36755;&#20837;&#26174;&#33879;&#24615;&#26144;&#23556;&#35270;&#35282;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#20102;&#27169;&#22411;&#23545;&#20854;&#36755;&#20986;&#25152;&#24402;&#23646;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290; &#36825;&#20123;&#29305;&#24449;&#26159;&#20960;&#20309;&#30456;&#20851;&#30340;&#65292;&#24182;&#36890;&#36807;&#31215;&#32047;&#27169;&#22411;&#30456;&#23545;&#20110;&#26080;&#38480;&#21046;&#25968;&#25454;&#20998;&#24067;&#30340;&#26799;&#24230;&#20449;&#24687;&#26469;&#35745;&#31639;&#12290; &#20026;&#20102;&#35745;&#31639;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#29420;&#31435;&#30340;&#25968;&#25454;&#28857;&#27839;&#30528;&#20154;&#31867;&#21487;&#29702;&#35299;&#26631;&#31614;&#30456;&#20851;&#32852;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#25512;&#21521;&#27169;&#22411;&#25439;&#22833;&#38754;&#12290; &#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#28145;&#24230;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#65292;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#36755;&#20837;&#20449;&#24687;&#65292;&#24182;&#19988;&#32463;&#36807;&#26816;&#39564;&#65292;&#22312;&#36755;&#20837;&#21464;&#21270;&#12289;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#37117;&#24456;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency methods provide post-hoc model interpretation by attributing input features to the model outputs. Current methods mainly achieve this using a single input sample, thereby failing to answer input-independent inquiries about the model. We also show that input-specific saliency mapping is intrinsically susceptible to misleading feature attribution. Current attempts to use 'general' input features for model interpretation assume access to a dataset containing those features, which biases the interpretation. Addressing the gap, we introduce a new perspective of input-agnostic saliency mapping that computationally estimates the high-level features attributed by the model to its outputs. These features are geometrically correlated, and are computed by accumulating model's gradient information with respect to an unrestricted data distribution. To compute these features, we nudge independent data points over the model loss surface towards the local minima associated by a human-understa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AceCoder&#30340;&#20195;&#30721;&#29983;&#25104;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#27604;&#65292;&#23427;&#36890;&#36807;&#31034;&#20363;&#26816;&#32034;&#21644;&#24341;&#23548;&#20195;&#30721;&#29983;&#25104;&#26469;&#25552;&#39640;&#29983;&#25104;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17780</link><description>&lt;p&gt;
&#12298;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Enhancing In-Context Learning for Code Generation. (arXiv:2303.17780v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AceCoder&#30340;&#20195;&#30721;&#29983;&#25104;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#27604;&#65292;&#23427;&#36890;&#36807;&#31034;&#20363;&#26816;&#32034;&#21644;&#24341;&#23548;&#20195;&#30721;&#29983;&#25104;&#26469;&#25552;&#39640;&#29983;&#25104;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#32463;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#25104;&#21151;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#27169;&#22411;&#21482;&#38656;&#35201;&#36755;&#20837;&#19968;&#20010;&#30001;&#23569;&#37327;&#38656;&#27714;-&#20195;&#30721;&#31034;&#20363;&#21644;&#19968;&#20010;&#26032;&#38656;&#27714;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#23601;&#33021;&#29983;&#25104;&#20986;&#26032;&#30340;&#31243;&#24207;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20165;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65292;&#24573;&#30053;&#20102;&#20195;&#30721;&#29983;&#25104;&#30340;&#29420;&#29305;&#29305;&#24615;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#30740;&#31350;&#20026;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20154;&#31867;&#32534;&#30721;&#36807;&#31243;&#30340;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#20026;AceCoder&#30340;&#20195;&#30721;&#29983;&#25104;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#27604;&#65292;AceCoder&#26377;&#20004;&#20010;&#26032;&#39062;&#20043;&#22788;&#12290;(1)&#31034;&#20363;&#26816;&#32034;&#12290;&#23427;&#26816;&#32034;&#31867;&#20284;&#31243;&#24207;&#20316;&#20026;&#31034;&#20363;&#65292;&#24182;&#20174;&#20013;&#23398;&#20064;&#32534;&#31243;&#25216;&#33021;(&#22914;&#31639;&#27861;&#12289;API)&#12290;(2)&#24341;&#23548;&#20195;&#30721;&#29983;&#25104;&#12290;&#23427;&#40723;&#21169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#38388;&#39044;&#22791;&#20195;&#30721;(&#22914;&#27979;&#35797;&#29992;&#20363;&#12289;API)&#24182;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#38656;&#27714;&#21644;&#25351;&#23548;&#19979;&#19968;&#27493;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#23558;AceCoder&#24212;&#29992;&#21040;&#22823;&#37327;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#30456;&#27604;&#65292;AceCoder&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with pre-trained language models (PTLMs) has shown great success in code generation. ICL does not require training. PTLMs take as the input a prompt consisting of a few requirement-code examples and a new requirement, and output a new program. However, existing studies simply reuse ICL techniques for natural language generation and ignore unique features of code generation. We refer to these studies as standard ICL.  Inspired by observations of the human coding process, we propose a novel ICL approach for code generation named AceCoder. Compared to standard ICL, AceCoder has two novelties. (1) Example retrieval. It retrieves similar programs as examples and learns programming skills (e.g., algorithms, APIs) from them. (2) Guided Code Generation. It encourages PTLMs to output an intermediate preliminary (e.g., test cases, APIs) before generating programs. The preliminary can help PTLMs understand requirements and guide the next code generation. We apply AceCode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#37096;&#20214;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29289;&#20307;&#36816;&#21160;&#21160;&#21147;&#23398;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#26816;&#27979;&#24213;&#23618;3D&#32467;&#26500;&#20013;&#30340;&#31227;&#21160;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2303.17774</link><description>&lt;p&gt;
&#21322;&#24369;&#30417;&#30563;&#19979;&#30340;&#29289;&#20307;&#36816;&#21160;&#21160;&#21147;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-Weakly Supervised Object Kinematic Motion Prediction. (arXiv:2303.17774v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#37096;&#20214;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29289;&#20307;&#36816;&#21160;&#21160;&#21147;&#23398;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#26816;&#27979;&#24213;&#23618;3D&#32467;&#26500;&#20013;&#30340;&#31227;&#21160;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;3D&#29289;&#20307;&#65292;&#36816;&#21160;&#21160;&#21147;&#23398;&#39044;&#27979;&#26088;&#22312;&#30830;&#23450;&#31227;&#21160;&#37096;&#20214;&#20197;&#21450;&#30456;&#24212;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#30001;&#20110;3D&#29289;&#20307;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#20960;&#20309;&#32454;&#33410;&#30340;&#24040;&#22823;&#21464;&#21270;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#32570;&#20047;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#20063;&#38480;&#21046;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20197;&#21322;&#24369;&#30417;&#30563;&#26041;&#24335;&#35299;&#20915;&#20102;&#29289;&#20307;&#36816;&#21160;&#21160;&#21147;&#23398;&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#32467;&#26524;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#23613;&#31649;&#20840;&#38754;&#27880;&#37322;&#36816;&#21160;&#26631;&#31614;&#30340;3D&#25968;&#25454;&#38598;&#26159;&#26377;&#38480;&#30340;&#65292;&#20294;&#23384;&#22312;&#22823;&#35268;&#27169;&#30340;&#29289;&#20307;&#37096;&#20214;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#35821;&#20041;&#37096;&#20998;&#20998;&#21106;&#21644;&#31227;&#21160;&#37096;&#20998;&#20998;&#21106;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#21487;&#20197;&#20174;&#24213;&#23618;3D&#32467;&#26500;&#20013;&#26816;&#27979;&#20986;&#31227;&#21160;&#37096;&#20998;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20998;&#23618;&#37096;&#20214;&#32423;&#21035;&#20998;&#21106;&#21644;&#31227;&#21160;&#37096;&#20214;&#21442;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a 3D object, kinematic motion prediction aims to identify the mobile parts as well as the corresponding motion parameters. Due to the large variations in both topological structure and geometric details of 3D objects, this remains a challenging task and the lack of large scale labeled data also constrain the performance of deep learning based approaches. In this paper, we tackle the task of object kinematic motion prediction problem in a semi-weakly supervised manner. Our key observations are two-fold. First, although 3D dataset with fully annotated motion labels is limited, there are existing datasets and methods for object part semantic segmentation at large scale. Second, semantic part segmentation and mobile part segmentation is not always consistent but it is possible to detect the mobile parts from the underlying 3D structure. Towards this end, we propose a graph neural network to learn the map between hierarchical part-level segmentation and mobile parts parameters, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#34701;&#21512;&#39046;&#22495;&#30693;&#35782;&#30340;&#20998;&#31867;&#27169;&#22411;&#26694;&#26550;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#20998;&#31867;&#22120;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#23433;&#20840;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#23398;&#20064;&#30446;&#26631;&#19979;&#29190;&#28809;&#22797;&#26434;&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17769</link><description>&lt;p&gt;
&#29190;&#28809;&#20998;&#31867;&#22120;&#35774;&#35745;&#20013;&#34701;&#21512;&#39046;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Domain Knowledge integrated for Blast Furnace Classifier Design. (arXiv:2303.17769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#34701;&#21512;&#39046;&#22495;&#30693;&#35782;&#30340;&#20998;&#31867;&#27169;&#22411;&#26694;&#26550;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#20998;&#31867;&#22120;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#23433;&#20840;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#23398;&#20064;&#30446;&#26631;&#19979;&#29190;&#28809;&#22797;&#26434;&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29190;&#28809;&#24314;&#27169;&#21644;&#25511;&#21046;&#26159;&#24037;&#19994;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#40657;&#21283;&#23376;&#27169;&#22411;&#26159;&#25551;&#36848;&#22797;&#26434;&#29190;&#28809;&#31995;&#32479;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#20026;&#20102;&#28385;&#36275;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#21644;&#33410;&#33021;&#31561;&#19981;&#21516;&#23398;&#20064;&#30446;&#26631;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#34701;&#21512;&#39046;&#22495;&#30693;&#35782;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#30693;&#35782;&#34701;&#20837;&#23398;&#20064;&#26041;&#26696;&#20801;&#35768;&#29992;&#25143;&#26356;&#27491;&#30830;&#22320;&#21019;&#24314;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#20197;&#35782;&#21035;&#8220;&#37325;&#35201;&#26679;&#26412;&#8221;&#65288;&#20854;&#38169;&#35823;&#20998;&#31867;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#24688;&#24403;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#32463;&#36890;&#36807;&#20004;&#20010;&#30495;&#23454;&#30340;&#29190;&#28809;&#25968;&#25454;&#38598;&#24471;&#21040;&#39564;&#35777;&#65292;&#36825;&#23558;&#25351;&#23548;&#25805;&#20316;&#20154;&#21592;&#26356;&#22909;&#22320;&#21033;&#29992;&#20182;&#20204;&#30340;&#20808;&#21069;&#32463;&#39564;&#26469;&#25511;&#21046;&#29190;&#28809;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blast furnace modeling and control is one of the important problems in the industrial field, and the black-box model is an effective mean to describe the complex blast furnace system. In practice, there are often different learning targets, such as safety and energy saving in industrial applications, depending on the application. For this reason, this paper proposes a framework to design a domain knowledge integrated classification model that yields a classifier for industrial application. Our knowledge incorporated learning scheme allows the users to create a classifier that identifies "important samples" (whose misclassifications can lead to severe consequences) more correctly, while keeping the proper precision of classifying the remaining samples. The effectiveness of the proposed method has been verified by two real blast furnace datasets, which guides the operators to utilize their prior experience for controlling the blast furnace systems better.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#39318;&#27425;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#8220;&#20219;&#21153;&#24863;&#30693;&#36793;&#30028;&#22686;&#24378;&#65288;TABA&#65289;&#8221;&#65292;&#24182;&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#23454;&#39564;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17764</link><description>&lt;p&gt;
&#26397;&#21521;&#23545;&#25239;&#40065;&#26834;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Adversarially Robust Continual Learning. (arXiv:2303.17764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#39318;&#27425;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#8220;&#20219;&#21153;&#24863;&#30693;&#36793;&#30028;&#22686;&#24378;&#65288;TABA&#65289;&#8221;&#65292;&#24182;&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#23454;&#39564;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#25345;&#32493;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#28789;&#27963;&#24615;&#20351;&#24471;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26174;&#31034;&#20986;&#23545;&#25239;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;&#34429;&#28982;&#22312;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#26377;&#35768;&#22810;&#20851;&#20110;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#20294;&#20445;&#25252;&#25345;&#32493;&#23398;&#20064;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#23578;&#26410;&#21463;&#21040;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#20219;&#21153;&#24863;&#30693;&#36793;&#30028;&#22686;&#24378;&#65288;Task-Aware Boundary Augmentation&#65292;TABA&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25239;&#35757;&#32451;&#21644;TABA&#22312;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that models trained by continual learning can achieve the comparable performances as the standard supervised learning and the learning flexibility of continual learning models enables their wide applications in the real world. Deep learning models, however, are shown to be vulnerable to adversarial attacks. Though there are many studies on the model robustness in the context of standard supervised learning, protecting continual learning from adversarial attacks has not yet been investigated. To fill in this research gap, we are the first to study adversarial robustness in continual learning and propose a novel method called \textbf{T}ask-\textbf{A}ware \textbf{B}oundary \textbf{A}ugmentation (TABA) to boost the robustness of continual learning models. With extensive experiments on CIFAR-10 and CIFAR-100, we show the efficacy of adversarial training and TABA in defending adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17760</link><description>&lt;p&gt;
CAMEL: &#29992;&#20110;&#8220;&#24515;&#26234;&#8221;&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#31038;&#32676;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society. (arXiv:2303.17760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#21462;&#24471;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20154;&#31867;&#30340;&#25351;&#23548;&#65292;&#20197;&#24341;&#23548;&#23545;&#35805;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#24314;&#21487;&#25193;&#23637;&#25216;&#26415;&#20197;&#20419;&#36827;&#20132;&#20114;&#24335;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#8220;&#35748;&#30693;&#8221;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#23454;&#29616;&#33258;&#20027;&#21512;&#20316;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#21551;&#21160;&#25552;&#31034;&#26469;&#24341;&#23548;&#32842;&#22825;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20154;&#31867;&#24847;&#22270;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35282;&#33394;&#25198;&#28436;&#26469;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#32842;&#22825;&#20195;&#29702;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framewor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#22686;&#30410;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#38544;&#34255;&#26435;&#37325;&#20248;&#21270;&#31561;&#31639;&#27861;&#26102;&#12290;</title><link>http://arxiv.org/abs/2303.17732</link><description>&lt;p&gt;
&#26368;&#20248;&#36755;&#20837;&#22686;&#30410;&#65306;&#36229;&#32423;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#20840;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Input Gain: All You Need to Supercharge a Feed-Forward Neural Network. (arXiv:2303.17732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17732
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#22686;&#30410;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#38544;&#34255;&#26435;&#37325;&#20248;&#21270;&#31561;&#31639;&#27861;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#30340;&#32447;&#24615;&#36716;&#25442;&#25913;&#21464;&#20102;&#31561;&#25928;&#30340;&#21069;&#39304;&#32593;&#32476;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32447;&#24615;&#21464;&#25442;&#34987;&#35270;&#20026;&#19982;&#23454;&#38469;&#35757;&#32451;&#20998;&#31163;&#30340;&#39044;&#22788;&#29702;&#25805;&#20316;&#12290;&#20174;&#31561;&#25928;&#32593;&#32476;&#24320;&#22987;&#65292;&#36890;&#36807;&#32447;&#24615;&#36716;&#25442;&#23545;&#36755;&#20837;&#36827;&#34892;&#39044;&#22788;&#29702;&#31561;&#25928;&#20110;&#22312;&#27599;&#27425;&#35757;&#32451;&#36845;&#20195;&#20013;&#23558;&#36127;&#26799;&#24230;&#30697;&#38453;&#19982;&#33258;&#30456;&#20851;&#30697;&#38453;&#30456;&#20056;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#38454;&#26041;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#22312;&#32473;&#23450;&#36845;&#20195;&#20013;&#26368;&#22823;&#21270;&#23398;&#20064;&#30340;&#33258;&#30456;&#20851;&#30697;&#38453;&#12290;&#24403;&#33258;&#30456;&#20851;&#30697;&#38453;&#20026;&#23545;&#35282;&#32447;&#30697;&#38453;&#26102;&#65292;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#36755;&#20837;&#22686;&#30410;&#12290;&#35813;&#26368;&#20248;&#36755;&#20837;&#22686;&#30410;&#65288;OIG&#65289;&#26041;&#27861;&#29992;&#20110;&#25913;&#36827;&#20004;&#20010;&#19968;&#38454;&#20108;&#32423;&#35757;&#32451;&#31639;&#27861;&#65292;&#21363;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#21644;&#38544;&#34255;&#26435;&#37325;&#20248;&#21270;&#65288;HWO&#65289;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#20132;&#26367;&#26356;&#26032;&#36755;&#20837;&#26435;&#37325;&#24182;&#35299;&#32447;&#24615;&#26041;&#31243;&#20197;&#30830;&#23450;&#36755;&#20986;&#26435;&#37325;&#12290;&#32467;&#26524;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;OIG&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31532;&#19968;&#39034;&#24207;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear transformation of the inputs alters the training performance of feed-forward networks that are otherwise equivalent. However, most linear transforms are viewed as a pre-processing operation separate from the actual training. Starting from equivalent networks, it is shown that pre-processing inputs using linear transformation are equivalent to multiplying the negative gradient matrix with an autocorrelation matrix per training iteration. Second order method is proposed to find the autocorrelation matrix that maximizes learning in a given iteration. When the autocorrelation matrix is diagonal, the method optimizes input gains. This optimal input gain (OIG) approach is used to improve two first-order two-stage training algorithms, namely back-propagation (BP) and hidden weight optimization (HWO), which alternately update the input weights and solve linear equations for output weights. Results show that the proposed OIG approach greatly enhances the performance of the first-order al
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT&#21644;BERT)&#35782;&#21035;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#24615;&#33021;, &#32467;&#26524;&#26174;&#31034;BERT&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;&#21644;F1&#20998;&#25968;&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17728</link><description>&lt;p&gt;
&#22522;&#20110;GPT&#21644;BERT&#30340;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#37492;&#23450;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text. (arXiv:2303.17728v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17728
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT&#21644;BERT)&#35782;&#21035;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#24615;&#33021;, &#32467;&#26524;&#26174;&#31034;BERT&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;&#21644;F1&#20998;&#25968;&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;(PPIs)&#23545;&#20110;&#29702;&#35299;&#36951;&#20256;&#26426;&#21046;&#12289;&#30142;&#30149;&#21457;&#30149;&#26426;&#29702;&#21644;&#33647;&#29289;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#21644;&#20934;&#30830;&#25552;&#21462;PPIs&#20197;&#20419;&#36827;&#31185;&#23398;&#30693;&#35782;&#30340;&#21457;&#25496;&#12290;&#24050;&#32463;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;(GPT)&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#21464;&#21387;&#22120;(BERT)&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#25163;&#21160;&#32534;&#21046;&#30340;LLL&#22522;&#20934;&#35821;&#26009;&#24211;&#35780;&#20272;&#20102;&#21508;&#31181;GPT&#21644;BERT&#27169;&#22411;&#30340;PPI&#35782;&#21035;&#24615;&#33021;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;77&#20010;&#21477;&#23376;&#20013;&#30340;164&#20010;PPIs&#12290;BERT&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;(85.17%)&#21644;F1&#20998;&#25968;(86.47%)&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;(93.83%)&#12290;&#23613;&#31649;GPT-4&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#20854;&#24615;&#33021;&#21487;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting protein-protein interactions (PPIs) is crucial for understanding genetic mechanisms, disease pathogenesis, and drug design. However, with the fast-paced growth of biomedical literature, there is a growing need for automated and accurate extraction of PPIs to facilitate scientific knowledge discovery. Pre-trained language models, such as generative pre-trained transformer (GPT) and bidirectional encoder representations from transformers (BERT), have shown promising results in natural language processing (NLP) tasks. We evaluated the PPI identification performance of various GPT and BERT models using a manually curated benchmark corpus of 164 PPIs in 77 sentences from learning language in logic (LLL). BERT-based models achieved the best overall performance, with PubMedBERT achieving the highest precision (85.17%) and F1-score (86.47%) and BioM-ALBERT achieving the highest recall (93.83%). Despite not being explicitly trained for biomedical texts, GPT-4 achieved comparable perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#8212;&#8212;&#35299;&#37322;&#21512;&#29702;&#24615;&#12290;&#34429;&#28982;&#19968;&#30452;&#34987;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#65292;&#20294;&#26159;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.17707</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#21487;&#35299;&#37322;&#24615;&#19982;&#21512;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking AI Explainability and Plausibility. (arXiv:2303.17707v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#8212;&#8212;&#35299;&#37322;&#21512;&#29702;&#24615;&#12290;&#34429;&#28982;&#19968;&#30452;&#34987;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#65292;&#20294;&#26159;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31639;&#27861;&#31526;&#21512;&#20154;&#31867;&#20132;&#27969;&#35268;&#33539;&#65292;&#25903;&#25345;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#28385;&#36275;&#20154;&#31867;&#23545;&#20110;AI&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#35774;&#23450;&#36866;&#24403;&#30340;&#35780;&#20272;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#37322;&#21512;&#29702;&#24615;&#65292;&#36825;&#26159;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#12290;&#21512;&#29702;&#24615;&#34913;&#37327;&#26426;&#22120;&#35299;&#37322;&#19982;&#20154;&#31867;&#35299;&#37322;&#30456;&#27604;&#30340;&#21512;&#29702;&#31243;&#24230;&#12290;&#21512;&#29702;&#24615;&#19968;&#30452;&#34987;&#20256;&#32479;&#22320;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#12290;&#25105;&#20204;&#21453;&#23545;&#36825;&#20010;&#24819;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20248;&#21270;&#21644;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35780;&#20272;XAI&#31639;&#27861;&#30340;&#21512;&#29702;&#24615;&#20250;&#35268;&#33539;&#26426;&#22120;&#35299;&#37322;&#65292;&#20197;&#34920;&#36798;&#19982;&#20154;&#31867;&#35299;&#37322;&#23436;&#20840;&#30456;&#21516;&#30340;&#20869;&#23481;&#65292;&#36825;&#20559;&#31163;&#20102;&#20154;&#31867;&#35299;&#37322;&#30340;&#22522;&#26412;&#21160;&#26426;&#65306;&#34920;&#36798;&#33258;&#24049;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Setting proper evaluation objectives for explainable artificial intelligence (XAI) is vital for making XAI algorithms follow human communication norms, support human reasoning processes, and fulfill human needs for AI explanations. In this article, we examine explanation plausibility, which is the most pervasive human-grounded concept in XAI evaluation. Plausibility measures how reasonable the machine explanation is compared to the human explanation. Plausibility has been conventionally formulated as an important evaluation objective for AI explainability tasks. We argue against this idea, and show how optimizing and evaluating XAI for plausibility is sometimes harmful, and always ineffective to achieve model understandability, transparency, and trustworthiness. Specifically, evaluating XAI algorithms for plausibility regularizes the machine explanation to express exactly the same content as human explanation, which deviates from the fundamental motivation for humans to explain: expres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23383;&#31526;&#32423;&#22122;&#38899;&#24494;&#35843;BERT&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#26041;&#35328;&#21644;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#21482;&#26377;&#22312;&#20219;&#21153;&#20381;&#36182;&#34920;&#38754;&#32423;&#21035;&#25552;&#31034;&#24182;&#19988;&#28304;-&#30446;&#26631;&#36328;&#35821;&#35328;&#23545;&#20855;&#26377;&#30456;&#23545;&#36739;&#39640;&#30340;&#35789;&#27719;&#37325;&#21472;&#26102;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#23383;&#31526;&#32423;&#22122;&#38899;&#23545;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#25928;&#26524;&#25165;&#29305;&#21035;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.17683</link><description>&lt;p&gt;
&#29992;&#23383;&#31526;&#32423;&#22122;&#38899;&#24494;&#35843;BERT&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#26041;&#35328;&#21450;&#30456;&#20851;&#35821;&#35328;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning BERT with Character-Level Noise for Zero-Shot Transfer to Dialects and Closely-Related Languages. (arXiv:2303.17683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23383;&#31526;&#32423;&#22122;&#38899;&#24494;&#35843;BERT&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#26041;&#35328;&#21644;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#21482;&#26377;&#22312;&#20219;&#21153;&#20381;&#36182;&#34920;&#38754;&#32423;&#21035;&#25552;&#31034;&#24182;&#19988;&#28304;-&#30446;&#26631;&#36328;&#35821;&#35328;&#23545;&#20855;&#26377;&#30456;&#23545;&#36739;&#39640;&#30340;&#35789;&#27719;&#37325;&#21472;&#26102;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#23383;&#31526;&#32423;&#22122;&#38899;&#23545;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#25928;&#26524;&#25165;&#29305;&#21035;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#24418;&#24335;&#30340;&#23383;&#31526;&#32423;&#22122;&#38899;&#36827;&#34892;BERT&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#26041;&#35328;&#21644;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21477;&#23376;&#32423;&#20998;&#31867;&#20219;&#21153;&#19978;&#24494;&#35843;BERT&#65292;&#24182;&#22312;&#19968;&#20123;&#26410;&#35265;&#26041;&#35328;&#21644;&#35821;&#35328;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#23383;&#31526;&#32423;&#22122;&#38899;&#21487;&#20197;&#26159;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#26497;&#20854;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21017;&#19981;&#22826;&#26377;&#24110;&#21161;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#20219;&#21153;&#30340;&#24615;&#36136;&#21644;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#25506;&#35752;&#20102;&#36825;&#20123;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#20219;&#21153;&#20381;&#36182;&#34920;&#38754;&#32423;&#21035;&#25552;&#31034;&#24182;&#19988;&#28304;-&#30446;&#26631;&#36328;&#35821;&#35328;&#23545;&#20855;&#26377;&#30456;&#23545;&#36739;&#39640;&#30340;&#35789;&#27719;&#37325;&#21472;&#26102;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#23383;&#31526;&#32423;&#22122;&#38899;&#29305;&#21035;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we induce character-level noise in various forms when fine-tuning BERT to enable zero-shot cross-lingual transfer to unseen dialects and languages. We fine-tune BERT on three sentence-level classification tasks and evaluate our approach on an assortment of unseen dialects and languages. We find that character-level noise can be an extremely effective agent of cross-lingual transfer under certain conditions, while it is not as helpful in others. Specifically, we explore these differences in terms of the nature of the task and the relationships between source and target languages, finding that introduction of character-level noise during fine-tuning is particularly helpful when a task draws on surface level cues and the source-target cross-lingual pair has a relatively high lexical overlap with shorter (i.e., less meaningful) unseen tokens on average.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#36965;&#24863;&#25216;&#26415;&#20998;&#26512;&#20102;&#24052;&#21400;&#23707;&#27700;&#31291;&#30000;&#20013;&#30340;&#31354;&#38388;&#20998;&#24067;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#20915;&#31574;&#21644;&#29615;&#22659;&#22240;&#32032;&#23545;&#31181;&#26893;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.17670</link><description>&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#25216;&#26415;&#20998;&#26512;&#22303;&#22320;&#21033;&#29992;&#21644;&#27700;&#31291;&#31181;&#26893;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Utilizing Remote Sensing to Analyze Land Usage and Rice Planting Patterns. (arXiv:2303.17670v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17670
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#36965;&#24863;&#25216;&#26415;&#20998;&#26512;&#20102;&#24052;&#21400;&#23707;&#27700;&#31291;&#30000;&#20013;&#30340;&#31354;&#38388;&#20998;&#24067;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#20915;&#31574;&#21644;&#29615;&#22659;&#22240;&#32032;&#23545;&#31181;&#26893;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24052;&#21400;&#23707;&#21512;&#20316;&#31649;&#29702;&#27700;&#31291;&#26799;&#30000;&#20013;&#65292;&#20154;&#31867;&#20915;&#31574;&#21644;&#29983;&#24577;&#31995;&#32479;&#36807;&#31243;&#20043;&#38388;&#30340;&#21453;&#39304;&#29615;&#36335;&#24341;&#20986;&#20102;&#26377;&#36259;&#30340;&#29616;&#35937;&#12290;&#29305;&#21035;&#22320;&#65292;&#35266;&#23519;&#21040;&#31354;&#38388;&#20998;&#24067;&#27169;&#24335;&#65292;&#21463;&#20892;&#27665;&#31181;&#26893;&#20316;&#29289;&#30340;&#20915;&#31574;&#20197;&#21450;&#29289;&#29702;&#29615;&#22659;&#22914;&#23475;&#34411;&#25439;&#23475;&#21644;&#27700;&#28304;&#30701;&#32570;&#30340;&#21709;&#24212;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20915;&#31574;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#28436;&#21270;&#21338;&#24328;&#29702;&#35770;&#27169;&#22411;&#65292;&#25512;&#26029;&#20102;&#25903;&#37197;&#36825;&#31181;&#31354;&#38388;&#20998;&#24067;&#27169;&#24335;&#30340;&#29305;&#23450;&#24130;&#24459;&#35268;&#24459;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24052;&#21400;&#23707;&#27700;&#31291;&#30000;&#20809;&#20142;&#30340;&#24555;&#29031;&#65292;&#20351;&#29992;&#39068;&#33394;&#26469;&#25351;&#31034;&#19981;&#21516;&#29983;&#38271;&#38454;&#27573;&#30340;&#27700;&#31291;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cooperative management of rice terraces in Bali reveals an interesting phenomenon that stems from the feedback loop between human decisions and the ecosystem process. In particular, spatial patterning is observed, which is heavily reliant on the farmer's decision to plant crops as well as the response from the physical environment like pest damage and water shortage. A recent study proposed an evolutionary game theoretic model to infer particular power laws governing this spatial patterning along the Bali region. In this paper, we show a snapshot of rice patches in Bali with colors to indicate the different stages of rice growth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MetaEnhance&#65292;&#19968;&#20010;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#25552;&#39640;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#20851;&#38190;&#23383;&#27573;&#36136;&#37327;&#30340;&#26694;&#26550;&#65292;&#24182;&#25104;&#21151;&#22312;500&#20221;&#26679;&#26412;&#20013;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#25968;&#25454;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#12290;</title><link>http://arxiv.org/abs/2303.17661</link><description>&lt;p&gt;
MetaEnhance:&#22823;&#23398;&#22270;&#20070;&#39302;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#20803;&#25968;&#25454;&#36136;&#37327;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries. (arXiv:2303.17661v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MetaEnhance&#65292;&#19968;&#20010;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#25552;&#39640;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#20851;&#38190;&#23383;&#27573;&#36136;&#37327;&#30340;&#26694;&#26550;&#65292;&#24182;&#25104;&#21151;&#22312;500&#20221;&#26679;&#26412;&#20013;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#25968;&#25454;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23545;&#35937;&#30340;&#20803;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#36890;&#36807;&#25968;&#23383;&#24211;&#30028;&#38754;&#36827;&#34892;&#21457;&#29616;&#38750;&#24120;&#37325;&#35201;&#12290;&#20294;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#25968;&#23383;&#23545;&#35937;&#30340;&#20803;&#25968;&#25454;&#36890;&#24120;&#23637;&#31034;&#20986;&#19981;&#23436;&#25972;&#12289;&#19981;&#19968;&#33268;&#21644;&#19981;&#27491;&#30830;&#30340;&#20540;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#26816;&#27979;&#12289;&#32416;&#27491;&#21644;&#35268;&#33539;&#23398;&#26415;&#20803;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#30340;&#19971;&#20010;&#20851;&#38190;&#23383;&#27573;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MetaEnhance&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#25552;&#39640;&#36825;&#20123;&#23383;&#27573;&#36136;&#37327;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#35780;&#20272;MetaEnhance&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#20803;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;500&#20010;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#65292;&#36890;&#36807;&#22810;&#20010;&#26631;&#20934;&#36827;&#34892;&#37319;&#26679;&#23376;&#38598;&#26469;&#32452;&#21512;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;MetaEnhance&#65292;&#32467;&#26524;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20960;&#20046;&#23436;&#32654;&#22320;&#23454;&#29616;&#20102;&#38169;&#35823;&#26816;&#27979;&#30340;F1&#20998;&#25968;&#20197;&#21450;&#20116;&#20010;&#23383;&#27573;&#20013;&#30340;&#38169;&#35823;&#32416;&#27491;&#30340;F1&#20998;&#25968;&#22312;0.85&#21040;1.00&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#20248;&#21270;&#65292;&#20174;&#32780;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#20154;&#21147;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.17655</link><description>&lt;p&gt;
&#22522;&#20110;Q&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#38598;&#32676;&#38556;&#30861;&#29289;&#36335;&#24452;&#35268;&#21010;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Q-learning Based System for Path Planning with UAV Swarms in Obstacle Environments. (arXiv:2303.17655v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#20248;&#21270;&#65292;&#20174;&#32780;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#20154;&#26426;&#38598;&#32676;&#30340;&#33258;&#20027;&#25511;&#21046;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38754;&#23545;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#38556;&#30861;&#29289;&#65292;&#36335;&#24452;&#35268;&#21010;&#23545;&#20110;&#20248;&#21270;&#33021;&#37327;&#28040;&#32791;&#21644;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#19981;&#26029;&#35843;&#25972;&#23398;&#20064;&#65292;&#23454;&#29616;&#22312;&#26377;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path Planning methods for autonomous control of Unmanned Aerial Vehicle (UAV) swarms are on the rise because of all the advantages they bring. There are more and more scenarios where autonomous control of multiple UAVs is required. Most of these scenarios present a large number of obstacles, such as power lines or trees. If all UAVs can be operated autonomously, personnel expenses can be decreased. In addition, if their flight paths are optimal, energy consumption is reduced. This ensures that more battery time is left for other operations. In this paper, a Reinforcement Learning based system is proposed for solving this problem in environments with obstacles by making use of Q-Learning. This method allows a model, in this particular case an Artificial Neural Network, to self-adjust by learning from its mistakes and achievements. Regardless of the size of the map or the number of UAVs in the swarm, the goal of these paths is to ensure complete coverage of an area with fixed obstacles f
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.17651</link><description>&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65306;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LM&#25913;&#36827;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17651
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#24635;&#26159;&#33021;&#22312;&#31532;&#19968;&#27425;&#33391;&#22909;&#22320;&#35299;&#20915;&#29983;&#25104;&#38382;&#39064;&#65288;&#22914;&#25688;&#35201;&#12289;&#31572;&#26696;&#12289;&#35299;&#37322;&#31561;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65288;SELF-REFINE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#21644;&#31934;&#28860;&#30456;&#20284;&#22320;&#20248;&#21270;LLMs&#30340;&#21021;&#22987;&#36755;&#20986;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#65306;&#20351;&#29992;LLM&#29983;&#25104;&#36755;&#20986;&#65292;&#28982;&#21518;&#20801;&#35768;&#21516;&#19968;&#27169;&#22411;&#25552;&#20379;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#22810;&#26041;&#38754;&#21453;&#39304;&#65292;&#26368;&#21518;&#21033;&#29992;&#21453;&#39304;&#20351;&#30456;&#21516;&#27169;&#22411;&#31934;&#28860;&#20808;&#21069;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#31934;&#28860;&#26694;&#26550;&#19982;&#26089;&#26399;&#24037;&#20316;&#19981;&#21516;&#65292;&#26080;&#38656;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#25110;&#21152;&#24378;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#21333;&#20010;LLM&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#33539;&#22260;&#20174;&#35780;&#35770;&#37325;&#20889;&#21040;&#25968;&#23398;&#25512;&#29702;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#12290;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;SELF-REFINE&#29983;&#25104;&#30340;&#36755;&#20986;&#34987;&#20154;&#31867;&#21644;&#33258;&#21160;&#21270;&#25351;&#26631;&#20248;&#20808;&#20110;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#30452;&#25509;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20957;&#35270;&#30340;&#27880;&#24847;&#21147;&#35782;&#21035;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#20154;&#26426;&#21327;&#20316;&#20307;&#39564;&#65292;&#20943;&#23569;&#24515;&#29702;&#21387;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.17619</link><description>&lt;p&gt;
&#22522;&#20110;&#20957;&#35270;&#30340;&#20154;&#26426;&#21327;&#20316;&#20013;&#30340;&#27880;&#24847;&#21147;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Gaze-based Attention Recognition for Human-Robot Collaboration. (arXiv:2303.17619v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17619
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20957;&#35270;&#30340;&#27880;&#24847;&#21147;&#35782;&#21035;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#20154;&#26426;&#21327;&#20316;&#20307;&#39564;&#65292;&#20943;&#23569;&#24515;&#29702;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#65288;&#21644;&#20998;&#24515;&#65289;&#35782;&#21035;&#26159;&#25913;&#21892;&#20154;&#26426;&#21327;&#20316;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32452;&#35013;&#22330;&#26223;&#65292;&#22312;&#20854;&#20013;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#21644;&#21327;&#20316;&#26426;&#22120;&#20154;&#24179;&#31561;&#22320;&#21512;&#20316;&#25340;&#35013;&#21464;&#36895;&#31665;&#12290;&#35813;&#35774;&#32622;&#25552;&#20379;&#20102;&#22810;&#31181;&#26426;&#20250;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#25805;&#20316;&#21592;&#30340;&#27880;&#24847;&#21147;&#32780;&#36866;&#24212;&#20854;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#20197;&#25552;&#39640;&#21327;&#20316;&#20307;&#39564;&#24182;&#20943;&#23569;&#24515;&#29702;&#21387;&#21147;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#35782;&#21035;&#20154;&#31867;&#25805;&#20316;&#21592;&#20851;&#27880;&#30340;&#24037;&#20316;&#21306;&#22495;&#65292;&#24182;&#22240;&#27492;&#26816;&#27979;&#20986;&#25805;&#20316;&#21592;&#27880;&#24847;&#21147;&#20998;&#25955;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#24320;&#21457;&#27880;&#24847;&#21147;&#35782;&#21035;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#27880;&#35270;&#26041;&#21521;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#23398;&#20064;&#23558;&#20957;&#35270;&#26041;&#21521;&#26144;&#23556;&#21040;&#39044;&#23450;&#20041;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#22312;&#25105;&#20204;&#23454;&#39564;&#23460;&#25910;&#38598;&#30340;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#26102;&#65292;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#38750;&#24120;&#22909;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#35782;&#21035;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#36890;&#36807;&#20351;&#26426;&#22120;&#20154;&#26681;&#25454;&#25805;&#20316;&#32773;&#30340;&#27880;&#24847;&#21147;&#35843;&#25972;&#20854;&#34892;&#20026;&#26469;&#25913;&#21892;&#20154;&#26426;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention (and distraction) recognition is a key factor in improving human-robot collaboration. We present an assembly scenario where a human operator and a cobot collaborate equally to piece together a gearbox. The setup provides multiple opportunities for the cobot to adapt its behavior depending on the operator's attention, which can improve the collaboration experience and reduce psychological strain. As a first step, we recognize the areas in the workspace that the human operator is paying attention to, and consequently, detect when the operator is distracted. We propose a novel deep-learning approach to develop an attention recognition model. First, we train a convolutional neural network to estimate the gaze direction using a publicly available image dataset. Then, we use transfer learning with a small dataset to map the gaze direction onto pre-defined areas of interest. Models trained using this approach performed very well in leave-one-subject-out evaluation on the small datas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#31181;&#32908;&#32905;&#29305;&#24449;&#30340;&#26080;&#30417;&#30563;&#20272;&#35745;&#65292;&#26377;&#25928;&#22320;&#35780;&#20272;&#20102;&#28041;&#21450;&#22810;&#32908;&#32905;&#21327;&#35843;&#36816;&#21160;&#30340;&#30130;&#21171;&#65292;&#20026;&#21046;&#23450;&#24247;&#22797;&#21644;&#35757;&#32451;&#35745;&#21010;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.17614</link><description>&lt;p&gt;
&#35780;&#20272;&#22810;&#32908;&#32905;&#21327;&#35843;&#36816;&#21160;&#30340;&#25345;&#32493;&#32908;&#32905;&#30130;&#21171;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Estimating Continuous Muscle Fatigue For Multi-Muscle Coordinated Exercise: A Pilot Study. (arXiv:2303.17614v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#31181;&#32908;&#32905;&#29305;&#24449;&#30340;&#26080;&#30417;&#30563;&#20272;&#35745;&#65292;&#26377;&#25928;&#22320;&#35780;&#20272;&#20102;&#28041;&#21450;&#22810;&#32908;&#32905;&#21327;&#35843;&#36816;&#21160;&#30340;&#30130;&#21171;&#65292;&#20026;&#21046;&#23450;&#24247;&#22797;&#21644;&#35757;&#32451;&#35745;&#21010;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26085;&#24120;&#38203;&#28860;&#20013;&#32908;&#32905;&#30130;&#21171;&#31243;&#24230;&#20026;&#31934;&#30830;&#23450;&#21046;&#24247;&#22797;&#21644;&#20010;&#24615;&#21270;&#35757;&#32451;&#21058;&#37327;&#25552;&#20379;&#37325;&#35201;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#22312;Metaverse&#30340;&#32972;&#26223;&#19979;&#12290;&#35780;&#20272;&#28041;&#21450;&#22810;&#32908;&#32905;&#21327;&#35843;&#36816;&#21160;&#30340;&#30130;&#21171;&#38656;&#35201;&#34920;&#31034;&#22810;&#32908;&#32905;&#26102;&#31354;&#36866;&#24212;&#30340;&#30130;&#21171;&#29305;&#24449;&#21644;&#25429;&#25417;&#30130;&#21171;&#26102;&#38388;&#28436;&#21464;&#36827;&#31243;&#30340;&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32908;&#32905;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32908;&#32905;&#34917;&#20607;&#21644;&#33034;&#39635;&#27169;&#22359;&#28608;&#27963;&#21464;&#21270;&#30340;&#29305;&#24449;&#26469;&#25551;&#36848;&#30130;&#21171;&#65292;&#24182;&#36890;&#36807;&#29983;&#29702;&#22522;&#30784;&#27169;&#22411;&#20272;&#35745;&#25345;&#32493;&#24615;&#30130;&#21171;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#32908;&#32905;&#21327;&#21516;&#20998;&#25968;&#21644;&#33034;&#39635;&#27169;&#22359;&#23574;&#23792;&#20540;&#26041;&#24046;&#20316;&#20026;&#30130;&#21171;&#35825;&#23548;&#31070;&#32463;&#32908;&#32905;&#36866;&#24212;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#35270;&#20026;&#35266;&#27979;&#20540;&#65292;&#24320;&#21457;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#25429;&#25417;&#26102;&#38388;&#28436;&#21464;&#36807;&#31243;&#12290;&#25105;&#20204;&#37319;&#29992;&#26080;&#30417;&#30563;&#20272;&#35745;&#31574;&#30053;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#32570;&#20047;&#30417;&#30563;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;8&#21517;&#20581;&#24247;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#21327;&#21516;&#25260;&#33151;&#32451;&#20064;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26681;&#25454;&#31070;&#32463;&#32908;&#32905;&#29305;&#24449;&#26377;&#25928;&#22320;&#20272;&#35745;&#20102;&#25345;&#32493;&#24615;&#30130;&#21171;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing the progression of muscle fatigue for daily exercises provides vital indicators for precise rehabilitation, personalized training dose, especially under the context of Metaverse. Assessing fatigue of multi-muscle coordination-involved daily exercises requires the neuromuscular features that represent the fatigue-induced characteristics of spatiotemporal adaptions of multiple muscles and the estimator that captures the time-evolving progression of fatigue. In this paper, we propose to depict fatigue by the features of muscle compensation and spinal module activation changes and estimate continuous fatigue by a physiological rationale model. First, we extract muscle synergy fractionation and the variance of spinal module spikings as features inspired by the prior of fatigue-induced neuromuscular adaptations. Second, we treat the features as observations and develop a Bayesian Gaussian process to capture the time-evolving progression. Third, we solve the issue of lacking supervi
&lt;/p&gt;</description></item><item><title>oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17612</link><description>&lt;p&gt;
oBERTa: &#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#21644;&#21098;&#26525;&#26469;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17612
&lt;/p&gt;
&lt;p&gt;
oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;oBERTa&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23427;&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#19994;&#32773;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;3.8&#21040;24.3&#20493;&#30340;&#26356;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;oBERTa&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#21098;&#26525;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#24037;&#20316;&#65292;&#24182;&#21033;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#22312;&#24191;&#27867;&#30340;&#20256;&#36882;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#29983;&#25104;oBERTa&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;RoBERTa&#19982;BERT&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26399;&#38388;&#21098;&#26525;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#24494;&#35843;&#26399;&#38388;&#19981;&#22826;&#36866;&#21512;&#21387;&#32553;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;oBERTa&#22312;&#19971;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#36827;&#30340;&#21387;&#32553;&#25216;&#26415;&#20351;&#24471;&#32463;&#36807;&#21098;&#26525;&#30340;oBERTa&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;BERTBASE&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;SQUAD V1.1&#38382;&#31572;&#25968;&#25454;&#30340;Prune OFA Large&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#37319;&#29992;&#26102;&#38388;&#21367;&#31215;&#29305;&#23450;&#32534;&#30721;&#22120;&#21644;&#20849;&#20139;&#32534;&#30721;&#22120;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#21033;&#29992;&#33258;&#21160;&#26631;&#35760;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#24773;&#24863;&#30456;&#20851;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17611</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#30340;&#21487;&#31359;&#25140;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Self-supervised Multimodal Representation Learning for Wearable Emotion Recognition. (arXiv:2303.17611v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17611
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#37319;&#29992;&#26102;&#38388;&#21367;&#31215;&#29305;&#23450;&#32534;&#30721;&#22120;&#21644;&#20849;&#20139;&#32534;&#30721;&#22120;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#21033;&#29992;&#33258;&#21160;&#26631;&#35760;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#24773;&#24863;&#30456;&#20851;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21608;&#36793;&#29983;&#29702;&#20449;&#21495;&#30340;&#21487;&#31359;&#25140;&#24773;&#24863;&#35782;&#21035;&#22240;&#20854;&#38750;&#20405;&#20837;&#24615;&#21644;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#20173;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#21478;&#22806;&#65292;&#20256;&#32479;&#30340;&#23436;&#20840;&#30417;&#30563;&#24335;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#19979;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#12290;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#22522;&#20110;&#26102;&#38388;&#21367;&#31215;&#30340;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;Transformer&#30340;&#20849;&#20139;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#25429;&#33719;&#20102;&#27169;&#24577;&#20869;&#21644;&#27169;&#24577;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#21033;&#29992;5&#20010;&#20449;&#21495;&#21464;&#25442;&#65292;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#34987;&#33258;&#21160;&#36171;&#20104;&#26631;&#31614;&#65292;&#25152;&#25552;&#20986;&#30340;SSL&#27169;&#22411;&#36890;&#36807;&#20449;&#21495;&#21464;&#25442;&#30340;&#35782;&#21035;&#20316;&#20026;&#25513;&#39280;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20801;&#35768;&#25552;&#21462;&#29992;&#20110;&#24773;&#24863;&#30456;&#20851;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#20041;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, wearable emotion recognition based on peripheral physiological signals has drawn massive attention due to its less invasive nature and its applicability in real-life scenarios. However, how to effectively fuse multimodal data remains a challenging problem. Moreover, traditional fully-supervised based approaches suffer from overfitting given limited labeled data. To address the above issues, we propose a novel self-supervised learning (SSL) framework for wearable emotion recognition, where efficient multimodal fusion is realized with temporal convolution-based modality-specific encoders and a transformer-based shared encoder, capturing both intra-modal and inter-modal correlations. Extensive unlabeled data is automatically assigned labels by five signal transforms, and the proposed SSL model is pre-trained with signal transformation recognition as a pretext task, allowing the extraction of generalized multimodal representations for emotion-related downstream tasks. For evaluat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#20114;&#33402;&#26415;&#35013;&#32622;&#8220;&#24773;&#32490;&#24377;&#31783;&#8221;&#65292;&#36890;&#36807;&#35299;&#37322;&#35821;&#35328;&#21644;&#35821;&#35843;&#26469;&#21453;&#26144;&#29615;&#22659;&#30340;&#24515;&#24773;&#65292;&#24182;&#20351;&#29992;&#24773;&#24863;&#26816;&#27979;&#26041;&#27861;&#22788;&#29702;&#29992;&#25143;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2303.17608</link><description>&lt;p&gt;
&#20132;&#20114;&#24773;&#24863;&#29366;&#24577;&#30340;&#35270;&#35273;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Visual Response to Emotional State of User Interaction. (arXiv:2303.17608v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#20114;&#33402;&#26415;&#35013;&#32622;&#8220;&#24773;&#32490;&#24377;&#31783;&#8221;&#65292;&#36890;&#36807;&#35299;&#37322;&#35821;&#35328;&#21644;&#35821;&#35843;&#26469;&#21453;&#26144;&#29615;&#22659;&#30340;&#24515;&#24773;&#65292;&#24182;&#20351;&#29992;&#24773;&#24863;&#26816;&#27979;&#26041;&#27861;&#22788;&#29702;&#29992;&#25143;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#33402;&#26415;&#35013;&#32622;&#8220;&#24773;&#32490;&#24377;&#31783;&#8221;&#65292;&#35774;&#35745;&#30446;&#30340;&#26159;&#36890;&#36807;&#35299;&#37322;&#35821;&#35328;&#21644;&#35821;&#35843;&#26469;&#21453;&#26144;&#29615;&#22659;&#30340;&#24515;&#24773;&#12290;&#24773;&#32490;&#24377;&#31783;&#21253;&#21547;&#19968;&#20010;&#25511;&#21046;&#23395;&#33410;&#30340;&#27785;&#28024;&#24335;3D&#21160;&#30011;&#30340;AI&#31243;&#24207;&#12290;&#22914;&#26524;AI&#31243;&#24207;&#24863;&#30693;&#21040;&#29992;&#25143;&#30340;&#35821;&#35328;&#21644;&#35821;&#35843;&#26159;&#24841;&#24742;&#30340;&#65292;&#21160;&#30011;&#23558;&#36827;&#23637;&#21040;&#29702;&#24819;&#21270;&#30340;&#23395;&#33410;&#20877;&#29616;&#12290;&#21542;&#21017;&#65292;&#23427;&#23558;&#28369;&#20837;&#23395;&#33410;&#20013;&#19981;&#24841;&#24555;&#30340;&#22825;&#27668;&#21644;&#33258;&#28982;&#28798;&#23475;&#12290;&#20026;&#20102;&#35299;&#37322;&#29992;&#25143;&#20132;&#20114;&#30340;&#35821;&#35328;&#21644;&#35821;&#35843;&#65292;&#28151;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#24773;&#24863;&#26816;&#27979;&#26041;&#27861;&#26469;&#22788;&#29702;&#29992;&#25143;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#36755;&#20837;&#12290;&#20174;&#35821;&#35843;&#21644;&#35821;&#35328;&#20998;&#21035;&#26816;&#27979;&#21040;&#30340;&#24773;&#24863;&#29366;&#24577;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#34701;&#21512;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#36328;&#19981;&#21516;&#20154;&#32676;&#30340;&#21487;&#33021;&#27169;&#22411;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes an interactive art installation "Mood spRing" designed to reflect the mood of the environment through interpretation of language and tone. Mood spRing consists of an AI program that controls an immersive 3D animation of the seasons. If the AI program perceives the language and tone of the users as pleasant, the animation progresses through idealized renditions of seasons. Otherwise, it slips into unpleasant weather and natural disasters of the season. To interpret the language and tone of the user interaction, hybrid state-of-the-art emotion detection methods are applied to the user audio and text inputs. The emotional states detected separately from tone and language are fused by a novel approach that aims at minimizing the possible model disparity across diverse demographic groups.
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#65292;&#32467;&#21512;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#20004;&#20010;&#36807;&#31243;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26500;&#24314;&#29702;&#35770;&#65292;&#21487;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26032;&#26041;&#27861;&#35299;&#20915;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17607</link><description>&lt;p&gt;
&#21457;&#29616;&#33258;&#28982;&#23450;&#24459;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine learning for discovering laws of nature. (arXiv:2303.17607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17607
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#65292;&#32467;&#21512;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#20004;&#20010;&#36807;&#31243;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26500;&#24314;&#29702;&#35770;&#65292;&#21487;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26032;&#26041;&#27861;&#35299;&#20915;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35266;&#31890;&#23376;&#36981;&#24490;&#37327;&#23376;&#21147;&#23398;&#30340;&#21407;&#29702;&#8212;&#8212;&#37027;&#20040;&#23439;&#35266;&#21644;&#24494;&#35266;&#19990;&#30028;&#20043;&#38388;&#30340;&#26126;&#30830;&#30028;&#38480;&#22312;&#21738;&#37324;&#21602;&#65311;&#27491;&#26159;&#36825;&#20010;&#8220;&#35299;&#37322;&#38382;&#39064;&#8221;&#20419;&#20351;&#34203;&#23450;&#35860;&#25552;&#20986;&#20102;&#20182;&#33879;&#21517;&#30340;&#24605;&#24819;&#23454;&#39564;&#65288;&#19968;&#21482;&#21516;&#26102;&#27515;&#20129;&#21644;&#27963;&#30528;&#30340;&#29483;&#65289;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#37327;&#23376;&#27979;&#37327;&#38382;&#39064;&#30340;&#28608;&#28872;&#20105;&#35770;&#65292;&#20294;&#33267;&#20170;&#20173;&#27809;&#26377;&#20196;&#20154;&#28385;&#24847;&#30340;&#31572;&#26696;&#12290;&#36825;&#27491;&#26159;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#30340;&#35745;&#31639;&#27169;&#22411;&#26469;&#25551;&#36848;&#21644;&#29702;&#35299;&#33258;&#28982;&#23450;&#24459;&#12290;&#23454;&#38469;&#19978;&#65292;&#26080;&#35770;&#26159;&#23439;&#35266;&#31890;&#23376;&#12289;&#24494;&#35266;&#30005;&#23376;&#36824;&#26159;&#23433;&#20840;&#38382;&#39064;&#65292;&#23427;&#20204;&#37117;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#23454;&#20307;&#65292;&#36825;&#20010;&#23454;&#20307;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21464;&#21270;&#65292;&#21487;&#20197;&#29992;&#29366;&#24577;&#21644;&#20540;&#32452;&#25104;&#30340;&#25968;&#25454;&#24207;&#21015;&#26469;&#25551;&#36848;&#12290;&#35266;&#23519;&#32773;&#21487;&#20197;&#20174;&#36825;&#20010;&#25968;&#25454;&#24207;&#21015;&#20013;&#23398;&#20064;&#65292;&#26500;&#24314;&#29702;&#35770;&#65288;&#36890;&#24120;&#30001;&#20989;&#25968;&#21644;&#24494;&#20998;&#26041;&#31243;&#32452;&#25104;&#65289;&#12290;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#29992;&#25143;&#30340;&#32463;&#39564;&#25110;&#36923;&#36753;&#26469;&#24314;&#27169;&#65292;&#32780;&#26159;&#20351;&#29992;&#25968;&#25454;&#12290;&#35745;&#31639;&#27169;&#22411;&#30340;&#26680;&#24515;&#22522;&#20110;&#20004;&#20010;&#36807;&#31243;&#65306;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#12290;&#20989;&#25968;&#36873;&#25321;&#36807;&#31243;&#31867;&#20284;&#20110;&#36798;&#23572;&#25991;&#30340;&#36827;&#21270;&#65292;&#20801;&#35768;&#20855;&#26377;&#20248;&#21183;&#29305;&#24449;&#30340;&#20989;&#25968;&#29983;&#23384;&#21644;&#32321;&#27542;&#65307;&#32780;&#36816;&#31639;&#31526;&#36873;&#25321;&#36807;&#31243;&#25429;&#25417;&#20102;&#33258;&#28982;&#23450;&#24459;&#30340;&#30456;&#20114;&#20381;&#23384;&#24615;&#65292;&#21487;&#20197;&#24179;&#34913;&#33258;&#28982;&#30028;&#20013;&#19981;&#21516;&#20989;&#25968;&#30340;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#37327;&#23376;&#21147;&#23398;&#12289;&#32463;&#20856;&#21147;&#23398;&#21644;&#31995;&#32479;&#29983;&#29289;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
A microscopic particle obeys the principles of quantum mechanics -- so where is the sharp boundary between the macroscopic and microscopic worlds? It was this "interpretation problem" that prompted Schr\"odinger to propose his famous thought experiment (a cat that is simultaneously both dead and alive) and sparked a great debate about the quantum measurement problem, and there is still no satisfactory answer yet. This is precisely the inadequacy of rigorous mathematical models in describing the laws of nature. We propose a computational model to describe and understand the laws of nature based on Darwin's natural selection. In fact, whether it's a macro particle, a micro electron or a security, they can all be considered as an entity, the change of this entity over time can be described by a data series composed of states and values. An observer can learn from this data series to construct theories (usually consisting of functions and differential equations). We don't model with the us
&lt;/p&gt;</description></item><item><title>OCPD&#33539;&#20363;&#36716;&#21464;&#20102;&#27969;&#31243;&#25366;&#25496;&#65292;&#21487;&#20197;&#22788;&#29702;&#19982;&#19968;&#31995;&#21015;&#23545;&#35937;&#30456;&#20851;&#32852;&#30340;&#20107;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#25193;&#23637;OCPD&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#21407;&#26041;&#27861;&#20013;&#20851;&#20110;&#22810;&#23545;&#35937;&#20132;&#20114;&#24490;&#29615;&#30340;&#38169;&#35823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16680</link><description>&lt;p&gt;
&#38024;&#23545;&#24490;&#29615;&#21327;&#21516;&#31995;&#32479;&#20013;&#23545;&#35937;&#20132;&#20114;&#30340;&#19981;&#33391;&#36807;&#31243;&#27169;&#22411;&#30340;&#30446;&#26631;&#23548;&#21521;&#38450;&#27490;&#26041;&#27861;&#65306;&#25193;&#23637;&#29256;
&lt;/p&gt;
&lt;p&gt;
Preventing Object-centric Discovery of Unsound Process Models for Object Interactions with Loops in Collaborative Systems: Extended Version. (arXiv:2303.16680v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16680
&lt;/p&gt;
&lt;p&gt;
OCPD&#33539;&#20363;&#36716;&#21464;&#20102;&#27969;&#31243;&#25366;&#25496;&#65292;&#21487;&#20197;&#22788;&#29702;&#19982;&#19968;&#31995;&#21015;&#23545;&#35937;&#30456;&#20851;&#32852;&#30340;&#20107;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#25193;&#23637;OCPD&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#21407;&#26041;&#27861;&#20013;&#20851;&#20110;&#22810;&#23545;&#35937;&#20132;&#20114;&#24490;&#29615;&#30340;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#20013;&#24515;&#30340;&#36807;&#31243;&#21457;&#29616;&#65288;OCPD&#65289;&#26159;&#27969;&#31243;&#25366;&#25496;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290; OCPD&#33021;&#22815;&#22788;&#29702;&#19981;&#20855;&#26377;&#21333;&#20010;&#26696;&#20363;&#27010;&#24565;&#20294;&#19982;&#20855;&#26377;&#29305;&#23450;&#31867;&#22411;&#30340;&#19968;&#31995;&#21015;&#23545;&#35937;&#30456;&#20851;&#32852;&#30340;&#20107;&#20214;&#12290;&#23545;&#35937;&#31867;&#22411;&#26500;&#25104;&#22810;&#20010;&#20132;&#20114;&#26696;&#20363;&#27010;&#24565;&#12290; OCPD&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#23545;&#35937;&#20013;&#24515;Petri&#32593;&#65292;&#21363;&#20855;&#26377;&#23545;&#35937;&#31867;&#22411;&#20301;&#32622;&#30340;Petri&#32593;&#65292;&#34920;&#31034;&#19982;&#23545;&#35937;&#31867;&#22411;&#23545;&#24212;&#30340;&#22810;&#20010;&#25191;&#34892;&#27969;&#30340;&#24182;&#34892;&#25191;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;OCPD&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#19981;&#20250;&#21463;&#21040;&#21407;&#22987;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#19981;&#33391;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object-centric process discovery (OCPD) constitutes a paradigm shift in process mining. Instead of assuming a single case notion present in the event log, OCPD can handle events without a single case notion, but that are instead related to a collection of objects each having a certain type. The object types constitute multiple, interacting case notions. The output of OCPD is an object-centric Petri net, i.e. a Petri net with object-typed places, that represents the parallel execution of multiple execution flows corresponding to object types. Similar to classical process discovery, where we aim for behaviorally sound process models as a result, in OCPD, we aim for soundness of the resulting object-centric Petri nets. However, the existing OCPD approach can result in violations of soundness. As we will show, one violation arises for multiple interacting object types with loops that arise in collaborative systems. This paper proposes an extended OCPD approach and proves that it does not s
&lt;/p&gt;</description></item><item><title>&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16166</link><description>&lt;p&gt;
&#27809;&#26377;&#27491;&#30830;&#24615;&#30340;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#37325;&#35201;&#65306;&#22312;NLP&#39046;&#22495;&#20013;&#27979;&#35797;&#20195;&#30721;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16166
&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#22312;&#30740;&#31350;&#23454;&#39564;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20195;&#30721;&#27491;&#30830;&#24615;&#24448;&#24448;&#20165;&#22522;&#20110;&#32467;&#26524;&#30340;&#24863;&#30693;&#36136;&#37327;&#32780;&#34987;&#20551;&#23450;&#12290;&#36825;&#24102;&#26469;&#20102;&#38169;&#35823;&#32467;&#26524;&#21644;&#28508;&#22312;&#35823;&#23548;&#24615;&#21457;&#29616;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#20851;&#27880;&#32467;&#26524;&#37325;&#29616;&#24212;&#35813;&#19982;&#24378;&#35843;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#21521;NLP&#31038;&#21306;&#21457;&#20986;&#30340;&#21495;&#21484;&#65292;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24182;&#32416;&#27491;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;Conformer&#26550;&#26500;&#30340;&#24320;&#28304;&#23454;&#29616;&#20013;&#30340;&#19977;&#20010;Bug&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Bug&#30340;&#23384;&#22312;&#24182;&#19981;&#20250;&#22952;&#30861;&#33719;&#24471;&#33391;&#22909;&#30340;&#21644;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#21453;&#32780;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#33021;&#25552;&#20379;&#38169;&#35823;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#37319;&#29992;&#26088;&#22312;&#20419;&#36827;NLP&#30740;&#31350;&#20013;&#27491;&#30830;&#24615;&#30340;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14061</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Reward Machines in Cooperative Multi-Agent Tasks. (arXiv:2303.14061v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#32534;&#30721;&#23376;&#20219;&#21153;&#30340;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#24212;&#23545;&#37096;&#20998;&#35266;&#27979;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#25152;&#23398;&#20064;&#30340;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#23436;&#25104;&#21512;&#20316;&#20219;&#21153;&#12290;&#19982;&#27599;&#20010;&#23376;&#20219;&#21153;&#30456;&#20851;&#32852;&#30340;&#22870;&#21169;&#26426;&#21046;&#26159;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#23398;&#20064;&#30340;&#65292;&#28982;&#21518;&#29992;&#20110;&#25351;&#23548;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#24471;&#21040;&#20102;&#38477;&#20302;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26410;&#26469;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26377; promising &#30340;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to Multi-Agent Reinforcement Learning (MARL) that combines cooperative task decomposition with the learning of reward machines (RMs) encoding the structure of the sub-tasks. The proposed method helps deal with the non-Markovian nature of the rewards in partially observable environments and improves the interpretability of the learnt policies required to complete the cooperative task. The RMs associated with each sub-task are learnt in a decentralised manner and then used to guide the behaviour of each agent. By doing so, the complexity of a cooperative multi-agent problem is reduced, allowing for more effective learning. The results suggest that our approach is a promising direction for future research in MARL, especially in complex environments with large state spaces and multiple agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.13592</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#25552;&#31034;&#65306;&#19996;&#21335;&#20122;&#35821;&#35328;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28151;&#21512;&#20195;&#30721;&#22312;&#19990;&#30028;&#35768;&#22810;&#22320;&#21306;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#23454;&#36341;&#65292;&#20294;&#25910;&#38598;&#39640;&#36136;&#37327;&#19988;&#20302;&#25104;&#26412;&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#36843;&#20351;&#20154;&#20204;&#38382;&#65306;&#36825;&#20123;&#31995;&#32479;&#33021;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#38646;-shot&#30340;&#26041;&#24335;&#19979;&#22914;&#20309;&#25552;&#31034;LLMs&#20026;&#19996;&#21335;&#20122;&#65288;SEA&#65289;&#30340;&#20116;&#31181;&#35821;&#35328;&#65288;&#21360;&#23612;&#35821;&#65292;&#39532;&#26469;&#35821;&#65292;&#20013;&#25991;&#65292;&#22612;&#21152;&#36335;&#35821;&#65292;&#36234;&#21335;&#35821;&#65289;&#21450;&#20811;&#37324;&#22885;&#23572;&#35821;S ingl ish&#21019;&#36896;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#26174;&#31034;&#20986;&#26368;&#22823;&#30340;&#28508;&#21147;&#65292;&#24403;&#26126;&#30830;&#23450;&#20041;&#8220;&#28151;&#21512;&#20195;&#30721;&#8221;&#26415;&#35821;&#26102;&#65292;&#33021;&#22815;68%&#30340;&#26102;&#38388;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;ChatGPT&#21644;InstructGPT&#65288;davinci-003&#65289;&#29983;&#25104;S ingl ish&#25991;&#26412;&#30340;&#34920;&#29616;&#20063;&#20540;&#24471;&#27880;&#24847;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#25552;&#31034;&#19979;&#30340;&#25104;&#21151;&#29575;&#24179;&#22343;&#20026;96%&#12290;&#20294;&#26159;&#65292;ChatGPT&#21644;InstructGPT&#30340;&#28151;&#21512;&#20195;&#30721;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13217</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#27491;&#24341;&#23548;&#23569;&#26679;&#26412;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#26500;&#24314;&#30340;&#25552;&#31034;&#36827;&#34892;&#30452;&#25509;&#24212;&#29992;&#26469;&#35299;&#20915;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#35757;&#32451;&#31034;&#20363;&#65292;&#31034;&#20363;&#39034;&#24207;&#21644;&#25552;&#31034;&#26684;&#24335;&#30340;&#21464;&#21270;&#23548;&#33268;&#19978;&#19979;&#25991;&#23398;&#20064;&#23481;&#26131;&#20986;&#29616;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26500;&#24314;&#36866;&#24403;&#30340;&#25552;&#31034;&#23545;&#20110;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20174;&#39044;&#27979;&#20559;&#24046;&#30340;&#35282;&#24230;&#37325;&#26032;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25351;&#26631;&#26469;&#35780;&#20272;&#22266;&#23450;&#25552;&#31034;&#30456;&#23545;&#20110;&#26631;&#31614;&#25110;&#32473;&#23450;&#23646;&#24615;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#39044;&#27979;&#20559;&#24046;&#36739;&#22823;&#30340;&#25552;&#31034;&#24635;&#26159;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22522;&#20110;&#36138;&#23146;&#25628;&#32034;&#26469;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21483;&#20570;"&#20844;&#27491;&#25552;&#31034;"&#65292;&#20854;&#20013;&#34701;&#20837;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#20197;&#25351;&#23548;&#25628;&#32034;&#19981;&#23637;&#29616;&#20986;&#23545;&#26576;&#20123;&#20154;&#32676;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;FairPrompt&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26469;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#30041;&#23384;&#33021;&#21147;&#12290;&#20855;&#20307;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#19968;&#31867;&#30340;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#25928;&#26524;&#12290;&#22312;&#35797;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#25552;&#39640;70%&#12290;</title><link>http://arxiv.org/abs/2303.06135</link><description>&lt;p&gt;
&#22522;&#20110;&#30334;&#19975;&#29992;&#25143;&#30340;&#29616;&#23454;&#19990;&#30028;&#20114;&#21160;&#26469;&#22870;&#21169;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26469;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#30041;&#23384;&#33021;&#21147;&#12290;&#20855;&#20307;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#19968;&#31867;&#30340;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#25928;&#26524;&#12290;&#22312;&#35797;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#25552;&#39640;70%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23548;&#33268;&#37096;&#32626;&#20102;&#19968;&#31995;&#21015;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#34429;&#28982;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#20854;&#35821;&#35328;&#33021;&#21147;&#21644;&#27969;&#30021;&#24615;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#33021;&#20445;&#35777;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#24456;&#23481;&#26131;&#22833;&#21435;&#29992;&#25143;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#21457;&#20248;&#20808;&#32771;&#34385;&#29992;&#25143;&#21442;&#19982;&#24230;&#20197;&#22686;&#24378;&#30041;&#23384;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20855;&#20307;&#25506;&#35752;&#20102;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#20197;&#39640;&#25928;&#22320;&#24320;&#21457;&#39640;&#24230;&#26377;&#21560;&#24341;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20174;&#29992;&#25143;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#25512;&#29702;&#26102;&#25298;&#32477;&#20302;&#24471;&#20998;&#30340;&#26679;&#26412;&#21709;&#24212;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#24341;&#20837;&#20102;&#30452;&#35266;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20363;&#22914;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#65288;MCL&#65289;&#65292;&#20316;&#20026;&#34913;&#37327;&#24050;&#37096;&#32626;&#32842;&#22825;&#26426;&#22120;&#20154;&#21442;&#19982;&#24230;&#27700;&#24179;&#30340;&#20195;&#29702;&#12290;&#22312;Chai Research&#24179;&#21488;&#19978;&#23545;&#27599;&#26085;&#30340;10,000&#20010;&#26032;&#32842;&#22825;&#26426;&#22120;&#20154;&#29992;&#25143;&#36827;&#34892;A/B&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20351;MCL&#22686;&#21152;70&#65285;&#65292;&#36825;&#30456;&#24403;&#20110;&#23558;&#30041;&#23384;&#26102;&#38388;&#24310;&#38271;1.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a
&lt;/p&gt;</description></item><item><title>BO-Muse&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#19987;&#23478;&#21327;&#20316;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#35753;&#20154;&#31867;&#19987;&#23478;&#21457;&#25381;&#20027;&#23548;&#20316;&#29992;&#65292;&#36890;&#36807;&#27880;&#20837;&#26032;&#39062;&#24615;&#24182;&#21457;&#29616;&#24369;&#28857;&#26469;&#25171;&#30772;&#36807;&#24230;&#24320;&#21457;&#65292;&#20197;&#21152;&#36895;&#23454;&#39564;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.01684</link><description>&lt;p&gt;
BO-Muse&#65306;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#23454;&#39564;&#35774;&#35745;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#19987;&#23478;&#30340;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BO-Muse: A human expert and AI teaming framework for accelerated experimental design. (arXiv:2303.01684v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01684
&lt;/p&gt;
&lt;p&gt;
BO-Muse&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#19987;&#23478;&#21327;&#20316;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#35753;&#20154;&#31867;&#19987;&#23478;&#21457;&#25381;&#20027;&#23548;&#20316;&#29992;&#65292;&#36890;&#36807;&#27880;&#20837;&#26032;&#39062;&#24615;&#24182;&#21457;&#29616;&#24369;&#28857;&#26469;&#25171;&#30772;&#36807;&#24230;&#24320;&#21457;&#65292;&#20197;&#21152;&#36895;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BO-Muse&#65292;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#19987;&#23478;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;&#21463;&#21040;&#20174;&#19987;&#23478;&#30693;&#35782;&#20013;&#25552;&#21462;&#21644;&#33976;&#39311;&#22238;AI&#27169;&#22411;&#30340;&#20869;&#22312;&#22256;&#38590;&#20197;&#21450;&#23545;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#35774;&#35745;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#35753;&#20154;&#31867;&#19987;&#23478;&#22312;&#23454;&#39564;&#36807;&#31243;&#20013;&#21457;&#25381;&#20027;&#23548;&#20316;&#29992;&#12290;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#20182;&#20204;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#21017;&#25198;&#28436;&#30528;&#28789;&#24863;&#30340;&#35282;&#33394;&#65292;&#22312;&#23547;&#25214;&#24369;&#28857;&#30340;&#21516;&#26102;&#27880;&#20837;&#26032;&#39062;&#24615;&#65292;&#20174;&#32780;&#25171;&#30772;&#30001;&#35748;&#30693;&#34701;&#20837;&#24341;&#36215;&#30340;&#36807;&#24230;&#24320;&#21457;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20122;&#32447;&#24615;&#25910;&#25947;&#65292;&#36895;&#24230;&#24555;&#20110;&#21333;&#29420;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#19987;&#23478;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20197;&#21450;&#20154;&#31867;&#19987;&#23478;&#36827;&#34892;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce BO-Muse, a new approach to human-AI teaming for the optimization of expensive black-box functions. Inspired by the intrinsic difficulty of extracting expert knowledge and distilling it back into AI models and by observations of human behavior in real-world experimental design, our algorithm lets the human expert take the lead in the experimental process. The human expert can use their domain expertise to its full potential, while the AI plays the role of a muse, injecting novelty and searching for areas of weakness to break the human out of over-exploitation induced by cognitive entrenchment. With mild assumptions, we show that our algorithm converges sub-linearly, at a rate faster than the AI or human alone. We validate our algorithm using synthetic data and with human experts performing real-world experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>CitySpec with Shield&#26159;&#19968;&#31181;&#23433;&#20840;&#30340;&#26234;&#33021;&#21161;&#25163;&#65292;&#29992;&#20110;&#24110;&#21161;&#22478;&#24066;&#20915;&#31574;&#32773;&#23558;&#20154;&#31867;&#25351;&#23450;&#30340;&#38656;&#27714;&#36716;&#25442;&#20026;&#30417;&#27979;&#31995;&#32479;&#21487;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2302.09665</link><description>&lt;p&gt;
CitySpec with Shield&#65306;&#23433;&#20840;&#30340;&#26234;&#33021;&#21161;&#25163;&#23454;&#29616;&#38656;&#27714;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
CitySpec with Shield: A Secure Intelligent Assistant for Requirement Formalization. (arXiv:2302.09665v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09665
&lt;/p&gt;
&lt;p&gt;
CitySpec with Shield&#26159;&#19968;&#31181;&#23433;&#20840;&#30340;&#26234;&#33021;&#21161;&#25163;&#65292;&#29992;&#20110;&#24110;&#21161;&#22478;&#24066;&#20915;&#31574;&#32773;&#23558;&#20154;&#31867;&#25351;&#23450;&#30340;&#38656;&#27714;&#36716;&#25442;&#20026;&#30417;&#27979;&#31995;&#32479;&#21487;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#22478;&#24066;&#20013;&#38656;&#35201;&#24320;&#21457;&#36234;&#26469;&#36234;&#22810;&#30340;&#30417;&#27979;&#31995;&#32479;&#26469;&#30830;&#20445;&#22478;&#24066;&#30340;&#23454;&#26102;&#36816;&#33829;&#36798;&#21040;&#23433;&#20840;&#21644;&#24615;&#33021;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#22478;&#24066;&#35201;&#27714;&#37117;&#26159;&#29992;&#33521;&#35821;&#32534;&#20889;&#30340;&#65292;&#23384;&#22312;&#32570;&#22833;&#12289;&#19981;&#20934;&#30830;&#25110;&#27169;&#31946;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;CitySpec&#65292;&#36825;&#26159;&#26234;&#33021;&#22478;&#24066;&#38656;&#27714;&#35268;&#26684;&#21270;&#30340;&#31532;&#19968;&#20010;&#26234;&#33021;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing number of monitoring systems have been developed in smart cities to ensure that the real-time operations of a city satisfy safety and performance requirements. However, many existing city requirements are written in English with missing, inaccurate, or ambiguous information. There is a high demand for assisting city policymakers in converting human-specified requirements to machine-understandable formal specifications for monitoring systems. To tackle this limitation, we build CitySpec, the first intelligent assistant system for requirement specification in smart cities. To create CitySpec, we first collect over 1,500 real-world city requirements across different domains (e.g., transportation and energy) from over 100 cities and extract city-specific knowledge to generate a dataset of city vocabulary with 3,061 words. We also build a translation model and enhance it through requirement synthesis and develop a novel online learning framework with shielded validation. The e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoRobust&#30340;&#40657;&#30418;&#40065;&#26834;&#24615;&#20998;&#26512;&#22120;&#65292;&#26088;&#22312;&#23545;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#20010;&#20960;&#20309;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#65292;&#24182;&#19988;&#26080;&#35770;&#32593;&#32476;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#31070;&#32463;&#20803;&#25968;&#37327;&#22914;&#20309;&#65292;GeoRobust&#37117;&#33021;&#22815;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#21464;&#25442;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2301.12456</link><description>&lt;p&gt;
&#20026;&#39564;&#35777;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#40065;&#26834;&#24615;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Verifying the Geometric Robustness of Large-scale Neural Networks. (arXiv:2301.12456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoRobust&#30340;&#40657;&#30418;&#40065;&#26834;&#24615;&#20998;&#26512;&#22120;&#65292;&#26088;&#22312;&#23545;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#20010;&#20960;&#20309;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#65292;&#24182;&#19988;&#26080;&#35770;&#32593;&#32476;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#31070;&#32463;&#20803;&#25968;&#37327;&#22914;&#20309;&#65292;GeoRobust&#37117;&#33021;&#22815;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#21464;&#25442;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#30693;&#23545;&#25239;&#24615;&#20960;&#20309;&#21464;&#25442;&#26131;&#21463;&#25915;&#20987;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#26469;&#39564;&#35777;&#22823;&#35268;&#27169;DNNs&#23545;&#22810;&#20010;&#20960;&#20309;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;GeoRobust&#65292;&#19968;&#20010;&#22522;&#20110;&#26032;&#22411;&#20840;&#23616;&#20248;&#21270;&#31574;&#30053;&#30340;&#40657;&#30418;&#23376;&#40065;&#26834;&#24615;&#20998;&#26512;&#22120;&#65292;&#29992;&#20110;&#23450;&#20301;&#24433;&#21709;&#29978;&#33267;&#25913;&#21464;&#32593;&#32476;&#36755;&#20986;&#30340;&#26368;&#22351;&#21464;&#25442;&#32452;&#21512;&#12290; GeoRobust&#21487;&#20197;&#26681;&#25454;Lipschitzian&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#25214;&#21040;&#26368;&#22351;&#24773;&#20917;&#32452;&#21512;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#12290;&#30001;&#20110;&#20854;&#40657;&#30418;&#23376;&#24615;&#36136;&#65292;GeoRobust&#21487;&#20197;&#37096;&#32626;&#22312;&#22823;&#35268;&#27169;DNNs&#19978;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#31070;&#32463;&#20803;&#25968;&#37327;&#22914;&#20309;&#12290;&#23454;&#38469;&#19978;&#65292;GeoRobust&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#20197;&#39640;&#31934;&#24230;&#23450;&#20301;ImageNet&#19978;ResNet50&#27169;&#22411;&#30340;&#26368;&#22351;&#20960;&#20309;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are known to be vulnerable to adversarial geometric transformation. This paper aims to verify the robustness of large-scale DNNs against the combination of multiple geometric transformations with a provable guarantee. Given a set of transformations (e.g., rotation, scaling, etc.), we develop GeoRobust, a black-box robustness analyser built upon a novel global optimisation strategy, for locating the worst-case combination of transformations that affect and even alter a network's output. GeoRobust can provide provable guarantees on finding the worst-case combination based on recent advances in Lipschitzian theory. Due to its black-box nature, GeoRobust can be deployed on large-scale DNNs regardless of their architectures, activation functions, and the number of neurons. In practice, GeoRobust can locate the worst-case geometric transformation with high precision for the ResNet50 model on ImageNet in a few seconds on average. We examined 18 ImageNet classifiers
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29983;&#25104;&#26041;&#27861;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;Image-based Joint-Embedding Predictive Architecture&#65288;I-JEPA&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#21644;&#25513;&#27169;&#31574;&#30053;&#36798;&#21040;&#36825;&#19968;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2301.08243</link><description>&lt;p&gt;
&#20855;&#26377;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#30340;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. (arXiv:2301.08243v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29983;&#25104;&#26041;&#27861;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;Image-based Joint-Embedding Predictive Architecture&#65288;I-JEPA&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#21644;&#25513;&#27169;&#31574;&#30053;&#36798;&#21040;&#36825;&#19968;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;I-JEPA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#22270;&#20687;&#20013;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#38750;&#29983;&#25104;&#26041;&#27861;&#12290;I-JEPA&#30340;&#26680;&#24515;&#35774;&#35745;&#36873;&#25321;&#26159;&#25513;&#27169;&#31574;&#30053;&#65292;&#20197;&#24341;&#23548;I-JEPA&#20135;&#29983;&#35821;&#20041;&#34920;&#31034;&#12290;&#24403;&#19982;Vision Transformers&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#35777;&#26126;I-JEPA&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#32479;&#37319;&#26679;&#26041;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#25130;&#26029;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.08546</link><description>&lt;p&gt;
&#20351;&#29992;&#37319;&#26679;&#31639;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#30340;&#25130;&#26029;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Estimating truncation effects of quantum bosonic systems using sampling algorithms. (arXiv:2212.08546v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#32479;&#37319;&#26679;&#26041;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#25130;&#26029;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#22312;&#22522;&#20110;&#37327;&#23376;&#27604;&#29305;&#25110;&#37327;&#23376;&#20301;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#27169;&#25311;&#29627;&#33394;&#23376;&#65292;&#24517;&#39035;&#36890;&#36807;&#23558;&#26080;&#38480;&#32500;&#23616;&#37096;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25130;&#26029;&#20026;&#26377;&#38480;&#32500;&#26469;&#35268;&#33539;&#29702;&#35770;&#12290;&#22312;&#23547;&#27714;&#23454;&#38469;&#37327;&#23376;&#24212;&#29992;&#30340;&#36807;&#31243;&#20013;&#65292;&#20102;&#35299;&#25130;&#26029;&#35823;&#24046;&#26377;&#22810;&#22823;&#38750;&#24120;&#37325;&#35201;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#38500;&#38750;&#25105;&#20204;&#25317;&#26377;&#22909;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#21542;&#21017;&#24456;&#38590;&#20272;&#35745;&#35823;&#24046;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#32463;&#20856;&#35774;&#22791;&#37319;&#26679;&#26041;&#27861;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#21487;&#20197;&#29992;&#29616;&#26377;&#21512;&#29702;&#30340;&#35745;&#31639;&#36164;&#28304;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#20108;&#32500;&#26684;&#28857;&#19978;&#30340;&#26631;&#37327;&#22330;&#29702;&#35770;&#20026;&#20363;&#28436;&#31034;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;&#20854;&#22823;&#23567;&#36229;&#36807;&#20102;&#20351;&#29992;&#30830;&#20999;&#23545;&#35282;&#21270;&#26041;&#27861;&#25152;&#33021;&#36798;&#21040;&#30340;&#33539;&#22260;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To simulate bosons on a qubit- or qudit-based quantum computer, one has to regularize the theory by truncating infinite-dimensional local Hilbert spaces to finite dimensions. In the search for practical quantum applications, it is important to know how big the truncation errors can be. In general, it is not easy to estimate errors unless we have a good quantum computer. In this paper we show that traditional sampling methods on classical devices, specifically Markov Chain Monte Carlo, can address this issue with a reasonable amount of computational resources available today. As a demonstration, we apply this idea to the scalar field theory on a two-dimensional lattice, with a size that goes beyond what is achievable using exact diagonalization methods. This method can be used to estimate the resources needed for realistic quantum simulations of bosonic theories, and also, to check the validity of the results of the corresponding quantum simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#22270;&#20687;&#25193;&#25955;&#30340;&#22810;&#26679;&#24615;&#20445;&#30041;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#35757;&#32451;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#23558;2D&#29983;&#25104;&#27169;&#22411;&#36716;&#21270;&#20026;&#20854;&#23427;&#39118;&#26684;&#30340;&#39046;&#22495;&#27169;&#22411;&#65292;&#36890;&#36807;CLIP&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2211.16374</link><description>&lt;p&gt;
DATID-3D: &#22522;&#20110;&#25991;&#26412;&#22270;&#20687;&#25193;&#25955;&#30340;&#22810;&#26679;&#24615;&#20445;&#30041;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#24212;&#29992;&#20110;3D&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model. (arXiv:2211.16374v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#22270;&#20687;&#25193;&#25955;&#30340;&#22810;&#26679;&#24615;&#20445;&#30041;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#35757;&#32451;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#23558;2D&#29983;&#25104;&#27169;&#22411;&#36716;&#21270;&#20026;&#20854;&#23427;&#39118;&#26684;&#30340;&#39046;&#22495;&#27169;&#22411;&#65292;&#36890;&#36807;CLIP&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;3D&#29983;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#36924;&#30495;&#22270;&#20687;&#19988;&#20445;&#25345;&#35270;&#35282;&#19968;&#33268;&#24615;&#21644;&#35814;&#32454;&#30340;&#19977;&#32500;&#24418;&#29366;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#22312;&#22810;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#35757;&#32451;&#19978;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#22270;&#20687;&#21450;&#20854;&#30456;&#26426;&#20998;&#24067;&#20449;&#24687;&#12290;&#25351;&#23548;&#25991;&#26412;&#20449;&#24687;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;CLIP&#65288;Contrastive Language-Image Pre-training&#65289;&#23558;&#19968;&#20010;2D&#29983;&#25104;&#27169;&#22411;&#36716;&#25442;&#20026;&#20855;&#26377;&#19981;&#21516;&#39118;&#26684;&#30340;&#20854;&#20182;&#39046;&#22495;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#25910;&#38598;&#36825;&#20123;&#39046;&#22495;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#30001;&#20110;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#30830;&#23450;&#24615;&#65292;&#21407;&#22987;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#22312;&#39046;&#22495;&#36866;&#24212;&#29983;&#25104;&#27169;&#22411;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#20445;&#30041;&#12290;&#23545;3D&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25351;&#23548;&#25991;&#26412;&#20449;&#24687;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#19981;&#20165;&#30001;&#20110;&#28798;&#38590;&#24615;&#22810;&#26679;&#24615;&#25439;&#22833;&#30340;&#21407;&#22240;&#65292;&#36824;&#30001;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#19981;&#20339;&#32780;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent 3D generative models have achieved remarkable performance in synthesizing high resolution photorealistic images with view consistency and detailed 3D shapes, but training them for diverse domains is challenging since it requires massive training images and their camera distribution information. Text-guided domain adaptation methods have shown impressive performance on converting the 2D generative model on one domain into the models on other domains with different styles by leveraging the CLIP (Contrastive Language-Image Pre-training), rather than collecting massive datasets for those domains. However, one drawback of them is that the sample diversity in the original generative model is not well-preserved in the domain-adapted generative models due to the deterministic nature of the CLIP text encoder. Text-guided domain adaptation will be even more challenging for 3D generative models not only because of catastrophic diversity loss, but also because of inferior text-image corresp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#30340;&#31574;&#30053;&#26469;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.00939</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#25552;&#39640;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving Sample Quality of Diffusion Models Using Self-Attention Guidance. (arXiv:2210.00939v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00939
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#30340;&#31574;&#30053;&#26469;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#20986;&#33394;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#31181;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#20351;&#29992;&#20998;&#31867;&#25110;&#25991;&#26412;&#26465;&#20214;&#30340;&#25193;&#25955;&#25351;&#23548;&#26041;&#27861;&#65292;&#22914;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25351;&#23548;&#26041;&#27861;&#12290;&#20174;&#36825;&#20010;&#24191;&#20041;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#26080;&#26465;&#20214;&#21644;&#26080;&#30417;&#30563;&#30340;&#31574;&#30053;&#26469;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#27169;&#31946;&#25351;&#23548;&#25913;&#21892;&#20102;&#20013;&#38388;&#26679;&#26412;&#30340;&#36866;&#29992;&#24615;&#65292;&#20351;&#24471;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20197;&#36866;&#24230;&#30340;&#25351;&#23548;&#23610;&#24230;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#65288;SAG&#65289;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20013;&#38388;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#22686;&#24378;&#23427;&#20204;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SAG&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20165;&#23545;&#25193;&#25955;&#27169;&#22411;&#20851;&#27880;&#30340;&#21306;&#22495;&#36827;&#34892;&#23545;&#25239;&#24615;&#27169;&#31946;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models (DDMs) have attracted attention for their exceptional generation quality and diversity. This success is largely attributed to the use of class- or text-conditional diffusion guidance methods, such as classifier and classifier-free guidance. In this paper, we present a more comprehensive perspective that goes beyond the traditional guidance methods. From this generalized perspective, we introduce novel condition- and training-free strategies to enhance the quality of generated images. As a simple solution, blur guidance improves the suitability of intermediate samples for their fine-scale information and structures, enabling diffusion models to generate higher quality samples with a moderate guidance scale. Improving upon this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps of diffusion models to enhance their stability and efficacy. Specifically, SAG adversarially blurs only the regions that diffusion models attend to at each iteratio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#27700;&#21360;&#25216;&#26415;&#39564;&#35777;&#24050;&#21457;&#24067;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#26435;&#30340;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#20854;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#65288;&#21487;&#30097;&#30340;&#65289;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2209.06015</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#38376;&#27700;&#21360;&#30340;&#40657;&#30418;&#25968;&#25454;&#38598;&#25152;&#26377;&#26435;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Black-box Dataset Ownership Verification via Backdoor Watermarking. (arXiv:2209.06015v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#27700;&#21360;&#25216;&#26415;&#39564;&#35777;&#24050;&#21457;&#24067;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#26435;&#30340;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#20854;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#65288;&#21487;&#30097;&#30340;&#65289;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#30001;&#20110;&#20854;&#39640;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#19988;&#25104;&#21151;&#22320;&#37319;&#29992;&#12290;DNN&#30340;&#24555;&#36895;&#21457;&#23637;&#21463;&#30410;&#20110;&#19968;&#20123;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;ImageNet&#65289;&#30340;&#23384;&#22312;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#36731;&#26494;&#39564;&#35777;&#20854;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#30340;&#24050;&#21457;&#24067;&#25968;&#25454;&#38598;&#37117;&#35201;&#27714;&#23427;&#20204;&#20165;&#33021;&#29992;&#20110;&#23398;&#26415;&#25110;&#25945;&#32946;&#30446;&#30340;&#32780;&#38750;&#21830;&#19994;&#30446;&#30340;&#65292;&#20294;&#20173;&#28982;&#27809;&#26377;&#24456;&#22909;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#36825;&#19968;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20445;&#25252;&#24050;&#21457;&#24067;&#25968;&#25454;&#38598;&#30340;&#24418;&#24335;&#21270;&#20026;&#39564;&#35777;&#23427;&#20204;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#65288;&#21487;&#30097;&#30340;&#65289;&#31532;&#19977;&#26041;&#27169;&#22411;&#65292;&#32780;&#38450;&#24481;&#32773;&#21482;&#33021;&#26597;&#35810;&#27169;&#22411;&#65292;&#32780;&#27809;&#26377;&#20851;&#20110;&#20854;&#21442;&#25968;&#21644;&#35757;&#32451;&#32454;&#33410;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning, especially deep neural networks (DNNs), has been widely and successfully adopted in many critical applications for its high effectiveness and efficiency. The rapid development of DNNs has benefited from the existence of some high-quality datasets ($e.g.$, ImageNet), which allow researchers and developers to easily verify the performance of their methods. Currently, almost all existing released datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes without permission. However, there is still no good way to ensure that. In this paper, we formulate the protection of released datasets as verifying whether they are adopted for training a (suspicious) third-party model, where defenders can only query the model while having no information about its parameters and training details. Based on this formulation, we propose to embed external patterns via backdoor watermarking for the ownership verification to protect them. 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Transformer &#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#32593;&#32476;&#30340;&#38899;&#39057;&#25340;&#25509;&#26816;&#27979;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#20934;&#30830;&#26816;&#27979;&#20986;&#25340;&#25509;&#24182;&#23450;&#20301;&#65292;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.14682</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32422;&#26463;&#38899;&#39057;&#25340;&#25509;&#26816;&#27979;&#19982;&#23450;&#20301;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks. (arXiv:2207.14682v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14682
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Transformer &#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#32593;&#32476;&#30340;&#38899;&#39057;&#25340;&#25509;&#26816;&#27979;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#20934;&#30830;&#26816;&#27979;&#20986;&#25340;&#25509;&#24182;&#23450;&#20301;&#65292;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23481;&#26131;&#33719;&#21462;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#38899;&#39057;&#32534;&#36753;&#24037;&#20855;&#20351;&#24471;&#38899;&#39057;&#25340;&#25509;&#21464;&#24471;&#31616;&#21333;&#12290;&#21487;&#36890;&#36807;&#32452;&#21512;&#21516;&#19968;&#20154;&#30340;&#21508;&#31181;&#35821;&#38899;&#26679;&#26412;&#26469;&#21019;&#24314;&#20196;&#20154;&#20449;&#26381;&#30340;&#20266;&#36896;&#21697;&#12290;&#26816;&#27979;&#27492;&#31867;&#25340;&#25509;&#22312;&#20844;&#20849;&#39046;&#22495;&#20013;&#32771;&#34385;&#21040;&#34394;&#20551;&#20449;&#24687;&#29978;&#33267;&#22312;&#27861;&#24459;&#32972;&#26223;&#19979;&#26680;&#23454;&#35777;&#25454;&#30340;&#23436;&#25972;&#24615;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38899;&#39057;&#25340;&#25509;&#26816;&#27979;&#31639;&#27861;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#24182;&#20570;&#20986;&#20102;&#29305;&#23450;&#30340;&#20551;&#35774;&#12290;&#20294;&#26159;&#65292;&#21009;&#20107;&#35843;&#26597;&#20154;&#21592;&#36890;&#24120;&#38754;&#20020;&#26469;&#33258;&#26410;&#30693;&#26469;&#28304;&#20855;&#26377;&#26410;&#30693;&#29305;&#24449;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#26356;&#26222;&#36866;&#30340;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#26088;&#22312;&#26397;&#30528;&#26080;&#32422;&#26463;&#38899;&#39057;&#25340;&#25509;&#26816;&#27979;&#26041;&#21521;&#36808;&#20986;&#31532;&#19968;&#27493;&#20197;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#25915;&#20987;&#24418;&#24335;&#30340;&#21518;&#22788;&#29702;&#25805;&#20316;&#26469;&#27169;&#25311;&#19981;&#21516;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#21487;&#33021;&#25513;&#30422;&#25340;&#25509;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Transformer &#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#32593;&#32476;&#29992;&#20110;&#25340;&#25509;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21644;&#23450;&#20301;&#29978;&#33267;&#23384;&#22312;&#24378;&#28872;&#22833;&#30495;&#21644;&#32972;&#26223;&#22122;&#22768;&#30340;&#25340;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Freely available and easy-to-use audio editing tools make it straightforward to perform audio splicing. Convincing forgeries can be created by combining various speech samples from the same person. Detection of such splices is important both in the public sector when considering misinformation, and in a legal context to verify the integrity of evidence. Unfortunately, most existing detection algorithms for audio splicing use handcrafted features and make specific assumptions. However, criminal investigators are often faced with audio samples from unconstrained sources with unknown characteristics, which raises the need for more generally applicable methods.  With this work, we aim to take a first step towards unconstrained audio splicing detection to address this need. We simulate various attack scenarios in the form of post-processing operations that may disguise splicing. We propose a Transformer sequence-to-sequence (seq2seq) network for splicing detection and localization. Our exte
&lt;/p&gt;</description></item><item><title>&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#35757;&#32451;&#20855;&#26377;&#22256;&#38590;&#24615;&#12290;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#24320;&#22987;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#23384;&#22312;&#36731;&#24494;&#30340;&#36951;&#24536;&#65292;&#38543;&#21518;&#20250;&#26377;&#19968;&#27573;&#24615;&#33021;&#24674;&#22797;&#30340;&#38454;&#27573;&#36319;&#38543;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#31283;&#23450;&#24615;&#24046;&#36317;&#8221;&#12290;</title><link>http://arxiv.org/abs/2205.13452</link><description>&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#30340;&#25345;&#32493;&#35780;&#20272;&#65306;&#35782;&#21035;&#31283;&#23450;&#24615;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual evaluation for lifelong learning: Identifying the stability gap. (arXiv:2205.13452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13452
&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#35757;&#32451;&#20855;&#26377;&#22256;&#38590;&#24615;&#12290;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#24320;&#22987;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#23384;&#22312;&#36731;&#24494;&#30340;&#36951;&#24536;&#65292;&#38543;&#21518;&#20250;&#26377;&#19968;&#27573;&#24615;&#33021;&#24674;&#22797;&#30340;&#38454;&#27573;&#36319;&#38543;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#31283;&#23450;&#24615;&#24046;&#36317;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#24050;&#34987;&#35777;&#26126;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#35757;&#32451;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#36138;&#23146;&#30340;&#26356;&#26032;&#20250;&#23548;&#33268;&#20197;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#28798;&#38590;&#24615;&#22320;&#34987;&#36951;&#24536;&#12290;&#23613;&#31649;&#32456;&#36523;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#20197;&#20811;&#26381;&#36825;&#31181;&#36951;&#24536;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19968;&#32452;&#24120;&#35265;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#24320;&#22987;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#36951;&#24536;&#65292;&#21482;&#26159;&#36825;&#31181;&#36951;&#24536;&#26159;&#26242;&#26102;&#30340;&#65292;&#20250;&#34987;&#19968;&#27573;&#24615;&#33021;&#24674;&#22797;&#30340;&#38454;&#27573;&#25152;&#36319;&#38543;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26377;&#36259;&#20294;&#28508;&#22312;&#26377;&#38382;&#39064;&#30340;&#29616;&#35937;&#31216;&#20026;&#31283;&#23450;&#24615;&#24046;&#36317;&#12290;&#30001;&#20110;&#32456;&#36523;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#39046;&#22495;&#20165;&#22312;&#27599;&#20010;&#20219;&#21153;&#20043;&#21518;&#36827;&#34892;&#35780;&#20272;&#30340;&#26631;&#20934;&#20570;&#27861;&#65292;&#22240;&#27492;&#31283;&#23450;&#24615;&#24046;&#36317;&#21487;&#33021;&#20173;&#28982;&#26410;&#34987;&#21457;&#29616;&#12290;&#32780;&#25105;&#20204;&#21017;&#24314;&#31435;&#20102;&#19968;&#20010;&#32456;&#36523;&#35780;&#20272;&#26694;&#26550;&#65292;&#20351;&#29992;&#27599;&#27425;&#36845;&#20195;&#30340;&#35780;&#20272;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#32452;&#26032;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#32463;&#39564;&#22238;&#25918;&#12289;&#22522;&#20110;&#32422;&#26463;&#30340;&#22238;&#25918;&#12289;&#30693;&#35782;_distillation&#21644;_expansion&#37117;&#23384;&#22312;&#31283;&#23450;&#24615;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-dependent data-generating distributions have proven to be difficult for gradient-based training of neural networks, as the greedy updates result in catastrophic forgetting of previously learned knowledge. Despite the progress in the field of continual learning to overcome this forgetting, we show that a set of common state-of-the-art methods still suffers from substantial forgetting upon starting to learn new tasks, except that this forgetting is temporary and followed by a phase of performance recovery. We refer to this intriguing but potentially problematic phenomenon as the stability gap. The stability gap had likely remained under the radar due to standard practice in the field of evaluating continual learning models only after each task. Instead, we establish a framework for continual evaluation that uses per-iteration evaluation and we define a new set of metrics to quantify worst-case performance. Empirically we show that experience replay, constraint-based replay, knowledg
&lt;/p&gt;</description></item><item><title>HIT-UAV&#26159;&#19968;&#20010;&#39640;&#31354;&#32418;&#22806;&#28909;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#30446;&#26631;&#26816;&#27979;&#65292;&#21253;&#25324;2898&#20010;&#22270;&#20687;&#21644;&#39134;&#34892;&#25968;&#25454;&#65292;&#25163;&#21160;&#27880;&#37322;&#20102;&#23450;&#21521;&#21644;&#26631;&#20934;&#36793;&#30028;&#26694;&#12290;&#27492;&#25968;&#25454;&#38598;&#26159;&#20844;&#24320;&#30340;&#12289;&#39318;&#20010;&#29992;&#20110;&#26816;&#27979;&#20154;&#21644;&#36710;&#36742;&#30340;&#39640;&#31354;&#26080;&#20154;&#26426;&#32418;&#22806;&#28909;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2204.03245</link><description>&lt;p&gt;
HIT-UAV&#65306;&#38754;&#21521;&#26080;&#20154;&#26426;&#30446;&#26631;&#26816;&#27979;&#30340;&#39640;&#31354;&#32418;&#22806;&#28909;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HIT-UAV: A high-altitude infrared thermal dataset for Unmanned Aerial Vehicle-based object detection. (arXiv:2204.03245v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03245
&lt;/p&gt;
&lt;p&gt;
HIT-UAV&#26159;&#19968;&#20010;&#39640;&#31354;&#32418;&#22806;&#28909;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#30446;&#26631;&#26816;&#27979;&#65292;&#21253;&#25324;2898&#20010;&#22270;&#20687;&#21644;&#39134;&#34892;&#25968;&#25454;&#65292;&#25163;&#21160;&#27880;&#37322;&#20102;&#23450;&#21521;&#21644;&#26631;&#20934;&#36793;&#30028;&#26694;&#12290;&#27492;&#25968;&#25454;&#38598;&#26159;&#20844;&#24320;&#30340;&#12289;&#39318;&#20010;&#29992;&#20110;&#26816;&#27979;&#20154;&#21644;&#36710;&#36742;&#30340;&#39640;&#31354;&#26080;&#20154;&#26426;&#32418;&#22806;&#28909;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HIT-UAV&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#26080;&#20154;&#26426;&#30446;&#26631;&#26816;&#27979;&#24212;&#29992;&#30340;&#39640;&#31354;&#32418;&#22806;&#28909;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;2898&#20010;&#32418;&#22806;&#28909;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#26469;&#33258;&#20110;&#25968;&#30334;&#20010;&#26080;&#20154;&#26426;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#25293;&#25668;&#30340;43470&#24103;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;HIT-UAV&#36824;&#25552;&#20379;&#20102;&#27599;&#20010;&#22270;&#20687;&#30340;&#20851;&#38190;&#39134;&#34892;&#25968;&#25454;&#65292;&#20363;&#22914;&#39134;&#34892;&#39640;&#24230;&#12289;&#30456;&#26426;&#35270;&#35282;&#12289;&#26085;&#26399;&#21644;&#26085;&#20809;&#24378;&#24230;&#12290;&#23545;&#20110;&#27599;&#20010;&#22270;&#20687;&#65292;&#25105;&#20204;&#25163;&#21160;&#27880;&#37322;&#20102;&#20004;&#31181;&#31867;&#22411;&#65288;&#23450;&#21521;&#21644;&#26631;&#20934;&#65289;&#30340;&#36793;&#30028;&#26694;&#65292;&#20197;&#35299;&#20915;&#31354;&#20013;&#22270;&#20687;&#20013;&#29289;&#20307;&#23454;&#20363;&#37325;&#21472;&#30340;&#38590;&#39064;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;HIT-UAV&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#30340;&#39640;&#31354;&#26080;&#20154;&#26426;&#32418;&#22806;&#28909;&#25968;&#25454;&#38598;&#29992;&#20110;&#26816;&#27979;&#20154;&#21644;&#36710;&#36742;&#12290;&#25105;&#20204;&#22312;HIT-UAV&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#19968;&#20123;&#30693;&#21517;&#30340;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26816;&#27979;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the HIT-UAV dataset, a high-altitude infrared thermal dataset for object detection applications on Unmanned Aerial Vehicles (UAVs). The dataset comprises 2,898 infrared thermal images extracted from 43,470 frames in hundreds of videos captured by UAVs in various scenarios including schools, parking lots, roads, and playgrounds. Moreover, the HIT-UAV provides essential flight data for each image, such as flight altitude, camera perspective, date, and daylight intensity. For each image, we have manually annotated object instances with bounding boxes of two types (oriented and standard) to tackle the challenge of significant overlap of object instances in aerial images. To the best of our knowledge, the HIT-UAV is the first publicly available high-altitude UAV-based infrared thermal dataset for detecting persons and vehicles. We have trained and evaluated well-established object detection algorithms on the HIT-UAV. Our results demonstrate that the detection algorithms perform e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#21160;&#21270;&#33021;&#21147;&#24212;&#29992;&#21040;&#29289;&#27969;&#31995;&#32479;&#27169;&#25311;&#24314;&#27169;&#20013;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;GPT-3 Codex&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#21151;&#33021;&#26377;&#25928;&#30340;&#25490;&#38431;&#21644;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#30340;&#27169;&#25311;&#27169;&#22411;&#65292;&#20026;&#31616;&#21270;&#27169;&#25311;&#27169;&#22411;&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#24320;&#21551;&#20102;&#37325;&#35201;&#22823;&#38376;&#12290;</title><link>http://arxiv.org/abs/2202.12107</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;&#27169;&#25311;&#65306;&#24212;&#29992;GPT-3 Codex&#33258;&#21160;&#21270;&#29289;&#27969;&#31995;&#32479;&#27169;&#25311;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems. (arXiv:2202.12107v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12107
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#21160;&#21270;&#33021;&#21147;&#24212;&#29992;&#21040;&#29289;&#27969;&#31995;&#32479;&#27169;&#25311;&#24314;&#27169;&#20013;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;GPT-3 Codex&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#21151;&#33021;&#26377;&#25928;&#30340;&#25490;&#38431;&#21644;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#30340;&#27169;&#25311;&#27169;&#22411;&#65292;&#20026;&#31616;&#21270;&#27169;&#25311;&#27169;&#22411;&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#24320;&#21551;&#20102;&#37325;&#35201;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33258;&#21160;&#21270;&#24320;&#21457;&#29289;&#27969;&#31995;&#32479;&#27169;&#25311;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22522;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-3 Codex&#30340;&#26694;&#26550;&#19978;&#33021;&#22815;&#26681;&#25454;&#21475;&#22836;&#25551;&#36848;&#29983;&#25104;&#21151;&#33021;&#26377;&#25928;&#30340;&#25490;&#38431;&#21644;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#27169;&#25311;&#27169;&#22411;&#12290;&#22312;&#25152;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;GPT-3 Codex&#23637;&#29616;&#20986;&#23545;Python&#32534;&#31243;&#30340;&#28145;&#21402;&#25216;&#33021;&#20197;&#21450;&#23545;&#34892;&#19994;&#29305;&#23450;&#35789;&#27719;&#30340;&#29702;&#35299;&#12290;&#32467;&#26524;&#65292;&#35813;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#32473;&#23450;&#34892;&#19994;&#29305;&#23450;&#22330;&#26223;&#19979;&#65292;&#26681;&#25454;&#27969;&#31243;&#35828;&#26126;&#21644;&#21464;&#37327;&#20540;&#21015;&#34920;&#29983;&#25104;&#21333;&#21697;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#21644;&#21333;&#26381;&#21153;&#22120;&#25490;&#38431;&#31995;&#32479;&#30340;&#27169;&#25311;&#27169;&#22411;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#21576;&#29616;&#65292;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#25345;&#32493;&#30340;&#36805;&#36895;&#36827;&#27493;&#65292;&#25171;&#24320;&#20102;&#31616;&#21270;&#27169;&#25311;&#27169;&#22411;&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#30340;&#37325;&#35201;&#22823;&#38376;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#21152;&#24555;&#33258;&#21160;&#21270;&#29289;&#27969;&#31995;&#32479;&#30340;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work is the first attempt to apply Natural Language Processing to automate the development of simulation models of systems vitally important for logistics. We demonstrated that the framework built on top of the fine-tuned GPT-3 Codex, a Transformer-based language model, could produce functionally valid simulations of queuing and inventory control systems given the verbal description. In conducted experiments, GPT-3 Codex demonstrated convincing expertise in Python as well as an understanding of the domain-specific vocabulary. As a result, the language model could produce simulations of a single-product inventory-control system and single-server queuing system given the domain-specific context, a detailed description of the process, and a list of variables with the corresponding values. The demonstrated results, along with the rapid improvement of language models, open the door for significant simplification of the workflow behind the simulation model development, which will allow e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;VRL3&#65292;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#21644;&#36924;&#30495;&#35270;&#35273;&#36755;&#20837;&#30340;&#25163;&#37096;&#25805;&#32437;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2202.10324</link><description>&lt;p&gt;
VRL3&#65306;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning. (arXiv:2202.10324v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;VRL3&#65292;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#21644;&#36924;&#30495;&#35270;&#35273;&#36755;&#20837;&#30340;&#25163;&#37096;&#25805;&#32437;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VRL3&#65292;&#36825;&#26159;&#19968;&#20010;&#37319;&#29992;&#31616;&#21333;&#35774;&#35745;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20219;&#21153;&#30340;&#24378;&#22823;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#12290;&#20316;&#32773;&#20998;&#26512;&#20102;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#35774;&#35745;&#21407;&#21017;&#12289;&#26032;&#30340;&#21457;&#29616;&#21644;&#20851;&#20110;&#25968;&#25454;&#39537;&#21160;&#35270;&#35273;DRL&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#20316;&#32773;&#21033;&#29992;&#38750;RL&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;ImageNet&#65289;&#26469;&#23398;&#20064;&#20219;&#21153;&#26080;&#20851;&#30340;&#35270;&#35273;&#34920;&#31034;&#65307;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#20316;&#32773;&#21033;&#29992;&#31163;&#32447;RL&#25968;&#25454;&#65288;&#20363;&#22914;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#28436;&#31034;&#65289;&#23558;&#20219;&#21153;&#26080;&#20851;&#30340;&#34920;&#31034;&#36716;&#21270;&#20026;&#26356;&#24378;&#22823;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#65307;&#22312;&#31532;&#19977;&#38454;&#27573;&#65292;&#20316;&#32773;&#36890;&#36807;&#22312;&#32447;RL&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#19968;&#32452;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#21644;&#36924;&#30495;&#35270;&#35273;&#36755;&#20837;&#30340;&#25361;&#25112;&#24615;&#25163;&#37096;&#25805;&#32437;&#20219;&#21153;&#20013;&#65292;&#19982;&#20043;&#21069;&#30340;SOTA&#30456;&#27604;&#65292;VRL3&#30340;&#26679;&#26412;&#25928;&#29575;&#24179;&#22343;&#25552;&#39640;&#20102;780%&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#19978;&#65292;VRL3&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#20102;1220%&#65288;&#20351;&#29992;&#26356;&#23485;&#30340;&#32534;&#30721;&#22120;&#65292;&#25552;&#39640;&#21040;2440%&#65289;&#65292;&#24182;&#20197;&#36229;&#36807;SOTA&#30340;&#24615;&#33021;&#35299;&#20915;&#20102;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose VRL3, a powerful data-driven framework with a simple design for solving challenging visual deep reinforcement learning (DRL) tasks. We analyze a number of major obstacles in taking a data-driven approach, and present a suite of design principles, novel findings, and critical insights about data-driven visual DRL. Our framework has three stages: in stage 1, we leverage non-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations; in stage 2, we use offline RL data (e.g. a limited number of expert demonstrations) to convert the task-agnostic representations into more powerful task-specific representations; in stage 3, we fine-tune the agent with online RL. On a set of challenging hand manipulation tasks with sparse reward and realistic visual inputs, compared to the previous SOTA, VRL3 achieves an average of 780% better sample efficiency. And on the hardest task, VRL3 is 1220% more sample efficient (2440% when using a wider encoder) and solves the task with on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31995;&#21015;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2202.01752</link><description>&lt;p&gt;
&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#36817;&#20284;&#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31995;&#21015;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#35774;&#35745;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#31639;&#27861;&#31995;&#21015;&#65292;&#20165;&#38656;&#35201; $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ &#23616;&#28216;&#25103;&#21363;&#21487;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010; $\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#20854;&#20013; $X,Y$ &#26159;&#20449;&#24687;&#38598;&#30340;&#25968;&#37327;&#65292;$A,B$ &#26159;&#20004;&#21517;&#29609;&#23478;&#30340;&#34892;&#21160;&#25968;&#12290;&#36825;&#27604;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230; $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ &#26377;&#30528; $\widetilde{\mathcal{O}}(\max\{X, Y\})$ &#30340;&#24040;&#22823;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#25968;&#22240;&#23376;&#20869;&#19982;&#20449;&#24687;&#29702;&#35770;&#19979;&#38480;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26032;&#31639;&#27861;&#23454;&#29616;&#20102;&#36825;&#31181;&#26679;&#26412;&#22797;&#26434;&#24230;&#65306;&#24179;&#34913;&#22312;&#32447;&#38236;&#38754;&#19979;&#38477;&#21644;&#24179;&#34913;&#21453;&#20107;&#23454;&#21518;&#24724;&#26368;&#23567;&#21270;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#23558;&#8220;&#24179;&#34913;&#25506;&#32034;&#31574;&#30053;&#8221;&#38598;&#25104;&#21040;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#25163;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;&#25903;&#25345;&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#30340;&#20108;&#20154;&#21338;&#24328;&#21644;&#22810;&#20154;&#21338;&#24328;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ episodes of play to find an $\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ by a factor of $\widetilde{\mathcal{O}}(\max\{X, Y\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \emph{balanced exploration policies} into their classical counterparts. We also extend our results t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#65292;&#21516;&#26102;&#25351;&#20986;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2111.07533</link><description>&lt;p&gt;
&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65306;&#27010;&#24565;&#12289;&#25216;&#26415;&#19982;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated scholarly paper review: Concepts, technologies, and challenges. (arXiv:2111.07533v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07533
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#65292;&#21516;&#26102;&#25351;&#20986;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#34892;&#35780;&#23457;&#26159;&#30740;&#31350;&#35780;&#20215;&#30340;&#24191;&#27867;&#25509;&#21463;&#26426;&#21046;&#65292;&#22312;&#23398;&#26415;&#20986;&#29256;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#25928;&#29575;&#20302;&#19979;&#21644;&#21487;&#37325;&#22797;&#24615;&#24046;&#65292;&#36825;&#19968;&#26426;&#21046;&#38271;&#26399;&#20197;&#26469;&#22791;&#21463;&#25209;&#35780;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#36741;&#21161;&#21516;&#34892;&#35780;&#23457;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#28041;&#21450;&#20154;&#21592;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38480;&#21046;&#20173;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#24182;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#12290;&#22312;&#23457;&#26597;&#21644;&#35752;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;ASPR &#30340;&#27599;&#20010;&#38454;&#27573;&#24050;&#32463;&#26377;&#30456;&#24212;&#30340;&#30740;&#31350;&#21644;&#21021;&#27493;&#23454;&#26045;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;ASPR&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer review is a widely accepted mechanism for research evaluation, playing a pivotal role in academic publishing. However, criticisms have long been leveled on this mechanism, mostly because of its poor efficiency and low reproducibility. Recent years have seen the application of artificial intelligence (AI) in assisting the peer review process. Nonetheless, with the involvement of humans, such limitations remain inevitable. In this paper, we propose the concept and pipeline of automated scholarly paper review (ASPR) and review the relevant literature and technologies of achieving a full-scale computerized review process. On the basis of the review and discussion, we conclude that there is already corresponding research and preliminary implementation at each stage of ASPR. We further look into the challenges in ASPR with the existing technologies. The major difficulties lie in imperfect document parsing and representation, inadequate data, defective human-computer interaction, and fla
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#22788;&#29702;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2110.10325</link><description>&lt;p&gt;
&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#21450;&#20854;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#22788;&#29702;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32467;&#21512;&#65292;&#21253;&#25324;&#25968;&#25454;&#39537;&#21160;&#30340;&#36923;&#36753;&#25512;&#29702;&#12289;&#30693;&#35782;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#35825;&#23548;&#23398;&#20064;&#65292;&#22312;&#21457;&#26126;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;OSAMTL&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#21463;&#35825;&#23548;&#23398;&#20064;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#19968;&#31181;&#24179;&#34913;&#30340;&#26041;&#24335;&#31616;&#21333;&#22320;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22788;&#29702;&#21333;&#20010;&#22024;&#26434;&#26631;&#31614;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#30340;&#26377;&#25928;&#24615;&#12290;&#20294;&#26159;&#65292;OSAMTL&#19981;&#36866;&#29992;&#20110;&#25552;&#20379;&#22810;&#31181;&#22024;&#26434;&#26679;&#26412;&#65288;DiNS&#65289;&#30340;&#23398;&#20064;&#20219;&#21153;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;DiNS&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#25193;&#23637;&#21407;&#22987;&#30340;OSAMTL&#20197;&#22788;&#29702;DiNS&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#23558;OSAMTL-DiNS&#24212;&#29992;&#20110;MHWSIA&#20013;&#30340;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the effectiveness of the combination of machine learning and logical reasoning, including data-driven logical reasoning, knowledge driven machine learning and abductive learning, in inventing advanced artificial intelligence technologies. One-step abductive multi-target learning (OSAMTL), an approach inspired by abductive learning, via simply combining machine learning and logical reasoning in a one-step balanced way, has as well shown its effectiveness in handling complex noisy labels of a single noisy sample in medical histopathology whole slide image analysis (MHWSIA). However, OSAMTL is not suitable for the situation where diverse noisy samples (DiNS) are provided for a learning task. In this paper, giving definition of DiNS, we propose one-step abductive multi-target learning with DiNS (OSAMTL-DiNS) to expand the original OSAMTL to handle complex noisy labels of DiNS. Applying OSAMTL-DiNS to tumour segmentation for breast cancer in MHWSIA, we show 
&lt;/p&gt;</description></item></channel></rss>