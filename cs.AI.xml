<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>SHARCS&#26159;&#19968;&#31181;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#21644;&#26144;&#23556;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#24615;&#30340;&#20219;&#21153;&#39044;&#27979;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00316</link><description>&lt;p&gt;
SHARCS: &#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#20849;&#20139;&#27010;&#24565;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
SHARCS: Shared Concept Space for Explainable Multimodal Learning. (arXiv:2307.00316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00316
&lt;/p&gt;
&lt;p&gt;
SHARCS&#26159;&#19968;&#31181;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#21644;&#26144;&#23556;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#24615;&#30340;&#20219;&#21153;&#39044;&#27979;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#37325;&#35201;&#33539;&#24335;&#65292;&#22312;&#36825;&#31181;&#38382;&#39064;&#20013;&#65292;&#21333;&#20010;&#25968;&#25454;&#27169;&#24577;&#36890;&#24120;&#26080;&#27861;&#20934;&#30830;&#35299;&#20915;&#32473;&#23450;&#30340;&#24314;&#27169;&#20219;&#21153;&#12290;&#34429;&#28982;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#38480;&#21046;&#20102;&#21407;&#21017;&#24615;&#30340;&#21487;&#35299;&#37322;&#36328;&#27169;&#24577;&#20998;&#26512;&#21644;&#39046;&#22495;&#19987;&#23478;&#24178;&#39044;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SHARCS&#65288;SHARed Concept Space&#65289;&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26032;&#27010;&#24565;&#26041;&#27861;&#12290;SHARCS&#20174;&#19981;&#21516;&#30340;&#24322;&#26500;&#27169;&#24577;&#20013;&#23398;&#20064;&#21644;&#26144;&#23556;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#27969;&#24418;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#35821;&#20041;&#30456;&#20284;&#30340;&#36328;&#27169;&#24577;&#27010;&#24565;&#30340;&#30452;&#35266;&#25237;&#24433;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23548;&#33268;&#22266;&#26377;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#39044;&#27979;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#19979;&#28216;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;SHARCS&#21487;&#20197;&#36816;&#34892;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning is an essential paradigm for addressing complex real-world problems, where individual data modalities are typically insufficient to accurately solve a given modelling task. While various deep learning approaches have successfully addressed these challenges, their reasoning process is often opaque; limiting the capabilities for a principled explainable cross-modal analysis and any domain-expert intervention. In this paper, we introduce SHARCS (SHARed Concept Space) -- a novel concept-based approach for explainable multimodal learning. SHARCS learns and maps interpretable concepts from different heterogeneous modalities into a single unified concept-manifold, which leads to an intuitive projection of semantically similar cross-modal concepts. We demonstrate that such an approach can lead to inherently explainable task predictions while also improving downstream predictive performance. Moreover, we show that SHARCS can operate and significantly outperform other approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.00310</link><description>&lt;p&gt;
&#26799;&#24230;&#30456;&#20284;&#65306;&#25935;&#24863;&#24230;&#32463;&#24120;&#34987;&#36807;&#39640;&#20272;&#35745;&#22312;DP-SGD&#20013;
&lt;/p&gt;
&lt;p&gt;
Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD. (arXiv:2307.00310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#26159;&#31169;&#26377;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#31639;&#27861;&#12290;&#34429;&#28982;&#24050;&#30693;&#20854;&#38544;&#31169;&#20998;&#26512;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#26159;&#32039;&#23494;&#30340;&#65292;&#20294;&#26159;&#19968;&#20123;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#26174;&#33879;&#20943;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;DP-SGD&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25429;&#25417;&#21040;&#22312;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#30456;&#20284;&#37051;&#23621;&#30340;&#28857;&#20139;&#21463;&#26356;&#22909;&#38544;&#31169;&#24615;&#30340;&#30452;&#35273;&#12290;&#24418;&#24335;&#19978;&#26469;&#35828;&#65292;&#36825;&#26159;&#36890;&#36807;&#20462;&#25913;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#35745;&#31639;&#24471;&#21040;&#30340;&#27169;&#22411;&#26356;&#26032;&#30340;&#27599;&#27493;&#38544;&#31169;&#24615;&#20998;&#26512;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#23450;&#29702;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#27599;&#27493;&#20998;&#26512;&#26469;&#25512;&#29702;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;DP-SGD&#20998;&#26512;&#20351;&#25105;&#20204;&#33021;&#22815;&#27491;&#24335;&#22320;&#26174;&#31034;DP-SGD&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private stochastic gradient descent (DP-SGD) is the canonical algorithm for private deep learning. While it is known that its privacy analysis is tight in the worst-case, several empirical results suggest that when training on common benchmark datasets, the models obtained leak significantly less privacy for many datapoints. In this paper, we develop a new analysis for DP-SGD that captures the intuition that points with similar neighbors in the dataset enjoy better privacy than outliers. Formally, this is done by modifying the per-step privacy analysis of DP-SGD to introduce a dependence on the distribution of model updates computed from a training dataset. We further develop a new composition theorem to effectively use this new per-step analysis to reason about an entire training run. Put all together, our evaluation shows that this novel DP-SGD analysis allows us to now formally show that DP-SGD leaks significantly less privacy for many datapoints. In particular, we ob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;SyMFM6D&#30340;&#38754;&#21521;&#23545;&#31216;&#22810;&#35270;&#35282;&#34701;&#21512;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#26041;&#21521;&#34701;&#21512;&#32593;&#32476;&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#20010;&#35270;&#35282;&#30340;RGB-D&#24103;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#20851;&#38190;&#28857;&#21644;&#23454;&#20363;&#35821;&#20041;&#20998;&#21106;&#26469;&#35745;&#31639;6D&#23039;&#24577;&#12290;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#23545;&#31216;&#29289;&#20307;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.00306</link><description>&lt;p&gt;
SyMFM6D&#65306;&#38754;&#21521;&#23545;&#31216;&#22810;&#26041;&#20301;&#34701;&#21512;&#30340;&#22810;&#35270;&#35282;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation. (arXiv:2307.00306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;SyMFM6D&#30340;&#38754;&#21521;&#23545;&#31216;&#22810;&#35270;&#35282;&#34701;&#21512;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#26041;&#21521;&#34701;&#21512;&#32593;&#32476;&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#20010;&#35270;&#35282;&#30340;RGB-D&#24103;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#20851;&#38190;&#28857;&#21644;&#23454;&#20363;&#35821;&#20041;&#20998;&#21106;&#26469;&#35745;&#31639;6D&#23039;&#24577;&#12290;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#23545;&#31216;&#29289;&#20307;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#29289;&#20307;&#24182;&#20272;&#35745;&#20854;6D&#23039;&#24577;&#23545;&#20110;&#33258;&#21160;&#21270;&#31995;&#32479;&#19982;&#29615;&#22659;&#23433;&#20840;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;6D&#23039;&#24577;&#20272;&#35745;&#22120;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25668;&#20687;&#22836;&#24103;&#65292;&#24182;&#19988;&#21463;&#21040;&#30001;&#20110;&#29289;&#20307;&#23545;&#31216;&#24615;&#32780;&#24341;&#36215;&#30340;&#36974;&#25377;&#21644;&#27169;&#31946;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#23545;&#31216;&#22810;&#35270;&#35282;6D&#23039;&#24577;&#20272;&#35745;&#22120;SyMFM6D&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#22810;&#26041;&#21521;&#34701;&#21512;&#32593;&#32476;&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#20010;&#35282;&#24230;&#30340;RGB-D&#24103;&#65292;&#24182;&#21516;&#26102;&#39044;&#27979;&#22330;&#26223;&#20013;&#25152;&#26377;&#29289;&#20307;&#30340;&#39044;&#23450;&#20041;&#20851;&#38190;&#28857;&#12290;&#22522;&#20110;&#20851;&#38190;&#28857;&#21644;&#23454;&#20363;&#35821;&#20041;&#20998;&#21106;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#25311;&#21512;&#39640;&#25928;&#35745;&#31639;6D&#23039;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#31216;&#29289;&#20307;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#29992;&#20110;&#23545;&#31216;&#24863;&#30693;&#30340;&#20851;&#38190;&#28857;&#26816;&#27979;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;SyMFM6D&#32593;&#32476;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#23039;&#24577;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting objects and estimating their 6D poses is essential for automated systems to interact safely with the environment. Most 6D pose estimators, however, rely on a single camera frame and suffer from occlusions and ambiguities due to object symmetries. We overcome this issue by presenting a novel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach efficiently fuses the RGB-D frames from multiple perspectives in a deep multi-directional fusion network and predicts predefined keypoints for all objects in the scene simultaneously. Based on the keypoints and an instance semantic segmentation, we efficiently compute the 6D poses by least-squares fitting. To address the ambiguity issues for symmetric objects, we propose a novel training procedure for symmetry-aware keypoint detection including a new objective function. Our SyMFM6D network significantly outperforms the state-of-the-art in both single-view and multi-view 6D pose estimation. We furthermore show the effe
&lt;/p&gt;</description></item><item><title>SysNoise&#26159;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;-&#37096;&#32626;&#21608;&#26399;&#20013;&#32463;&#24120;&#21457;&#29983;&#30340;&#22122;&#38899;&#65292;&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;SysNoise&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#27169;&#22411;&#31283;&#20581;&#24615;&#20250;&#24102;&#26469;&#19968;&#23450;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#24120;&#35265;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.00280</link><description>&lt;p&gt;
SysNoise: &#25506;&#32034;&#21644;&#35780;&#20272;&#35757;&#32451;-&#37096;&#32626;&#31995;&#32479;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency. (arXiv:2307.00280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00280
&lt;/p&gt;
&lt;p&gt;
SysNoise&#26159;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;-&#37096;&#32626;&#21608;&#26399;&#20013;&#32463;&#24120;&#21457;&#29983;&#30340;&#22122;&#38899;&#65292;&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;SysNoise&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#27169;&#22411;&#31283;&#20581;&#24615;&#20250;&#24102;&#26469;&#19968;&#23450;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#24120;&#35265;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#21644;&#33258;&#28982;&#22122;&#38899;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#23545;&#20110;&#30001;&#19981;&#21516;&#31995;&#32479;&#23454;&#29616;&#24341;&#36215;&#30340;&#22122;&#38899;&#23545;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;SysNoise&#65292;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;-&#37096;&#32626;&#21608;&#26399;&#20013;&#32463;&#24120;&#21457;&#29983;&#20294;&#24448;&#24448;&#34987;&#24573;&#35270;&#30340;&#22122;&#38899;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SysNoise&#21457;&#29983;&#22312;&#28304;&#35757;&#32451;&#31995;&#32479;&#22312;&#37096;&#32626;&#26102;&#20999;&#25442;&#21040;&#19981;&#21516;&#30340;&#30446;&#26631;&#31995;&#32479;&#26102;&#65292;&#21508;&#31181;&#24494;&#23567;&#31995;&#32479;&#19981;&#21305;&#37197;&#32047;&#21152;&#36215;&#26469;&#20250;&#20135;&#29983;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;SysNoise&#36827;&#34892;&#20102;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#20998;&#20026;&#22522;&#20110;&#25512;&#29702;&#38454;&#27573;&#30340;&#19977;&#20010;&#31867;&#21035;&#65307;&#28982;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#23450;&#37327;&#35780;&#20272;SysNoise&#23545;20&#22810;&#31181;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;SysNoise&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#27169;&#22411;&#31283;&#20581;&#24615;&#20250;&#24102;&#26469;&#19968;&#23450;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#24120;&#35265;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensive studies have shown that deep learning models are vulnerable to adversarial and natural noises, yet little is known about model robustness on noises caused by different system implementations. In this paper, we for the first time introduce SysNoise, a frequently occurred but often overlooked noise in the deep learning training-deployment cycle. In particular, SysNoise happens when the source training system switches to a disparate target system in deployments, where various tiny system mismatch adds up to a non-negligible difference. We first identify and classify SysNoise into three categories based on the inference stage; we then build a holistic benchmark to quantitatively measure the impact of SysNoise on 20+ models, comprehending image classification, object detection, instance segmentation and natural language processing tasks. Our extensive experiments revealed that SysNoise could bring certain impacts on model robustness across different tasks and common mitigations li
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#39044;&#35757;&#32451;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#23884;&#20837;&#65292;&#36890;&#36807;&#23558;&#20020;&#24202;&#26415;&#35821;&#34920;&#31034;&#20026;&#35821;&#20041;&#23884;&#20837;&#24182;&#21033;&#29992;&#20302;&#32500;&#23884;&#20837;&#20316;&#20026;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#25552;&#39640;&#20020;&#24202;&#35760;&#24405;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.00266</link><description>&lt;p&gt;
&#20998;&#23618;&#39044;&#35757;&#32451;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Pretraining for Biomedical Term Embeddings. (arXiv:2307.00266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00266
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#39044;&#35757;&#32451;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#23884;&#20837;&#65292;&#36890;&#36807;&#23558;&#20020;&#24202;&#26415;&#35821;&#34920;&#31034;&#20026;&#35821;&#20041;&#23884;&#20837;&#24182;&#21033;&#29992;&#20302;&#32500;&#23884;&#20837;&#20316;&#20026;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#25552;&#39640;&#20020;&#24202;&#35760;&#24405;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#21253;&#21547;&#20102;&#20851;&#20110;&#24739;&#32773;&#30340;&#21307;&#30103;&#29366;&#20917;&#21644;&#31649;&#29702;&#30340;&#35814;&#32454;&#35828;&#26126;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#23545;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#22788;&#29702;&#65292;&#21487;&#20197;&#21033;&#29992;&#20020;&#24202;&#26415;&#35821;&#30340;&#35266;&#23519;&#39057;&#29575;&#20316;&#20026;&#39044;&#27979;&#29305;&#24449;&#65292;&#29992;&#20110;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#36712;&#36857;&#39044;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#37327;&#30456;&#20284;&#19988;&#30456;&#20851;&#30340;&#20020;&#24202;&#27010;&#24565;&#65292;&#26356;&#26377;&#25928;&#30340;&#24314;&#27169;&#31574;&#30053;&#26159;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#23558;&#20020;&#24202;&#26415;&#35821;&#34920;&#31034;&#20026;&#35821;&#20041;&#23884;&#20837;&#65292;&#24182;&#23558;&#20302;&#32500;&#23884;&#20837;&#20316;&#20026;&#29305;&#24449;&#21521;&#37327;&#29992;&#20110;&#39044;&#27979;&#24314;&#27169;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#21033;&#29992;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#27604;&#20165;&#20351;&#29992;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#30340;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#23884;&#20837;&#26356;&#22909;&#30340;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#21516;&#20041;&#35789;&#23545;&#21644;&#19981;&#30456;&#20851;&#30340;&#35789;&#23545;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#19981;&#21516;&#30340;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHR) contain narrative notes that provide extensive details on the medical condition and management of patients. Natural language processing (NLP) of clinical notes can use observed frequencies of clinical terms as predictive features for downstream applications such as clinical decision making and patient trajectory prediction. However, due to the vast number of highly similar and related clinical concepts, a more effective modeling strategy is to represent clinical terms as semantic embeddings via representation learning and use the low dimensional embeddings as feature vectors for predictive modeling. To achieve efficient representation, fine-tuning pretrained language models with biomedical knowledge graphs may generate better embeddings for biomedical terms than those from standard language models alone. These embeddings can effectively discriminate synonymous pairs of from those that are unrelated. However, they often fail to capture different degrees o
&lt;/p&gt;</description></item><item><title>InstructEval&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#22871;&#20214;&#65292;&#29992;&#20110;&#23545;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#31574;&#21010;&#30340;&#25163;&#21160;&#32534;&#20889;&#30340;&#25351;&#20196;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00259</link><description>&lt;p&gt;
InstructEval: &#31995;&#32479;&#35780;&#20272;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InstructEval: Systematic Evaluation of Instruction Selection Methods. (arXiv:2307.00259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00259
&lt;/p&gt;
&lt;p&gt;
InstructEval&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#22871;&#20214;&#65292;&#29992;&#20110;&#23545;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#31574;&#21010;&#30340;&#25163;&#21160;&#32534;&#20889;&#30340;&#25351;&#20196;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064; (ICL) &#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#21644;&#19968;&#23567;&#32452;&#27880;&#37322;&#31034;&#20363;&#26469;&#25552;&#31034;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#25191;&#34892;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25552;&#31034;&#20013;&#20351;&#29992;&#30340;&#36755;&#20837;&#30340;&#32454;&#33410;&#23545; ICL &#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#36825;&#28608;&#21169;&#20102;&#25351;&#20196;&#36873;&#25321;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#36873;&#25321;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#20165;&#38480;&#20110;&#27169;&#22411;&#21644;&#20219;&#21153;&#30340;&#27973;&#23618;&#23376;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#27934;&#23519;&#21147;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010; ICL &#35780;&#20272;&#22871;&#20214;&#65292;&#20197;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#35813;&#22871;&#20214;&#21253;&#25324;&#26469;&#33258;4&#20010;&#19981;&#21516;&#27169;&#22411;&#23478;&#26063;&#30340;13&#20010;&#24320;&#28304;LLM&#65292;&#28085;&#30422;9&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20195;&#34920;&#20102;3&#20010;&#20998;&#31867;&#20013;&#21508;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#20102;7&#31181;&#21463;&#27426;&#36814;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#30456;&#23545;&#20110;ICL&#30456;&#20851;&#30340;&#20116;&#39033;&#26399;&#26395;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#31574;&#21010;&#30340;&#25163;&#21160;&#32534;&#20889;&#30340;&#25351;&#20196;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that the precise details of the inputs used in the prompt significantly impacts ICL, which has incentivized instruction selection algorithms. The effect of instruction-choice however is severely underexplored, with existing analyses being restricted to shallow subsets of models and tasks, which limits the generalizability of their insights. We develop an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from 4 distinct model families and covers 9 different tasks, representing a range of task types across 3 categories. In this work, we evaluate the relative performance of 7 popular instruction selection methods using our benchmark over five desiderata relevant to ICL. We discover that using curated manually-written instru
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#39640;&#25928;&#23398;&#20064;&#32454;&#31890;&#24230;&#23376;&#31867;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#35774;&#35745;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20219;&#21153;&#39537;&#21160;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#20998;&#21106;&#30340;&#32622;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.00257</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20013;&#39640;&#25928;&#30340;&#23376;&#31867;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Efficient Subclass Segmentation in Medical Images. (arXiv:2307.00257v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#39640;&#25928;&#23398;&#20064;&#32454;&#31890;&#24230;&#23376;&#31867;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#35774;&#35745;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20219;&#21153;&#39537;&#21160;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#20998;&#21106;&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#30740;&#31350;&#20852;&#36259;&#36234;&#26469;&#36234;&#32454;&#33268;&#65292;&#24191;&#27867;&#26631;&#27880;&#30340;&#25104;&#26412;&#20063;&#22312;&#19978;&#21319;&#12290;&#20026;&#20102;&#20943;&#23569;&#25104;&#26412;&#65292;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#26159;&#29992;&#31895;&#31890;&#24230;&#30340;&#36229;&#31867;&#26631;&#31614;&#36827;&#34892;&#26631;&#27880;&#65292;&#21516;&#26102;&#20351;&#29992;&#26377;&#38480;&#30340;&#31934;&#32454;&#26631;&#27880;&#20316;&#20026;&#34917;&#20805;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#20805;&#36275;&#30340;&#31895;&#31890;&#24230;&#27880;&#37322;&#36741;&#21161;&#20102;&#31934;&#32454;&#25968;&#25454;&#23398;&#20064;&#12290;&#26368;&#36817;&#30340;&#20998;&#31867;&#20219;&#21153;&#30740;&#31350;&#22312;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#20197;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#39640;&#25928;&#23398;&#20064;&#32454;&#31890;&#24230;&#23376;&#31867;&#30340;&#30740;&#31350;&#36824;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31867;&#21035;&#30340;&#23618;&#27425;&#32467;&#26500;&#26469;&#35774;&#35745;&#32593;&#32476;&#26550;&#26500;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#39537;&#21160;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#24471;&#32593;&#32476;&#26356;&#23481;&#26131;&#35782;&#21035;&#19981;&#21516;&#30340;&#23376;&#31867;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21069;&#21521;&#36830;&#25509;&#27169;&#22359;&#65292;&#36890;&#36807;&#36830;&#25509;&#26469;&#33258;&#36229;&#31867;&#39044;&#27979;&#32467;&#26524;&#30340;&#26631;&#31614;&#65292;&#22686;&#24378;&#20102;&#23376;&#31867;&#20998;&#21106;&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As research interests in medical image analysis become increasingly fine-grained, the cost for extensive annotation also rises. One feasible way to reduce the cost is to annotate with coarse-grained superclass labels while using limited fine-grained annotations as a complement. In this way, fine-grained data learning is assisted by ample coarse annotations. Recent studies in classification tasks have adopted this method to achieve satisfactory results. However, there is a lack of research on efficient learning of fine-grained subclasses in semantic segmentation tasks. In this paper, we propose a novel approach that leverages the hierarchical structure of categories to design network architecture. Meanwhile, a task-driven data generation method is presented to make it easier for the network to recognize different subclass categories. Specifically, we introduce a Prior Concatenation module that enhances confidence in subclass segmentation by concatenating predicted logits from the superc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#36817;&#26399;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00252</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An ML approach to resolution of singularities. (arXiv:2307.00252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#36817;&#26399;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#26041;&#31243;&#32452;&#30340;&#35299;&#38598;&#36890;&#24120;&#21253;&#21547;&#19981;&#20809;&#28369;&#12289;&#22855;&#24322;&#30340;&#28857;&#12290;&#35299;&#20915;&#22855;&#28857;&#26159;&#20960;&#20309;&#20013;&#30340;&#22522;&#26412;&#36807;&#31243;&#65292;&#25105;&#20204;&#23558;&#22855;&#28857;&#26367;&#25442;&#20026;&#20809;&#28369;&#28857;&#65292;&#21516;&#26102;&#20445;&#25345;&#35299;&#38598;&#30340;&#21097;&#20313;&#37096;&#20998;&#19981;&#21464;&#12290;&#35299;&#20915;&#22855;&#28857;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#65306;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#21453;&#22797;&#36827;&#34892;&#34987;&#31216;&#20026;&#8220;blowing-up&#8221;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#35299;&#20915;&#30340;&#22797;&#26434;&#24615;&#39640;&#24230;&#20381;&#36182;&#20110;&#26576;&#20123;&#36873;&#25321;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#36716;&#21270;&#25104;&#19981;&#21516;&#29256;&#26412;&#30340;&#20004;&#20154;&#21338;&#24328;&#65292;&#21363;&#25152;&#35859;&#30340;Hironaka&#28216;&#25103;&#65292;&#32780;&#31532;&#19968;&#20301;&#29609;&#23478;&#30340;&#33719;&#32988;&#31574;&#30053;&#25552;&#20379;&#20102;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#30340;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Hironaka&#28216;&#25103;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#23547;&#25214;&#22855;&#28857;&#30340;&#26368;&#20248;&#35299;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36825;&#35777;&#26126;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution set of a system of polynomial equations typically contains ill-behaved, singular points. Resolution is a fundamental process in geometry in which we replace singular points with smooth points, while keeping the rest of the solution set unchanged. Resolutions are not unique: the usual way to describe them involves repeatedly performing a fundamental operation known as "blowing-up", and the complexity of the resolution highly depends on certain choices. The process can be translated into various versions of a 2-player game, the so-called Hironaka game, and a winning strategy for the first player provides a solution to the resolution problem. In this paper we introduce a new approach to the Hironaka game that uses reinforcement learning agents to find optimal resolutions of singularities. In certain domains, the trained model outperforms state-of-the-art selection heuristics in total number of polynomial additions performed, which provides a proof-of-concept that recent devel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;THUIR2&#22242;&#38431;&#22312;NTCIR-16 Session&#25628;&#32034;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20182;&#20204;&#22312;FOSS&#21644;POSS&#23376;&#20219;&#21153;&#20013;&#20351;&#29992;&#20102;&#23398;&#20064;&#25490;&#24207;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00250</link><description>&lt;p&gt;
THUIR2&#22312;NTCIR-16 Session&#25628;&#32034;&#65288;SS&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
THUIR2 at NTCIR-16 Session Search (SS) Task. (arXiv:2307.00250v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;THUIR2&#22242;&#38431;&#22312;NTCIR-16 Session&#25628;&#32034;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20182;&#20204;&#22312;FOSS&#21644;POSS&#23376;&#20219;&#21153;&#20013;&#20351;&#29992;&#20102;&#23398;&#20064;&#25490;&#24207;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#22242;&#38431;&#65288;THUIR2&#65289;&#21442;&#21152;&#20102;NTCIR-16 Session&#25628;&#32034;&#65288;SS&#65289;&#20219;&#21153;&#30340;FOSS&#21644;POSS&#23376;&#20219;&#21153;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#22312;FOSS&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23398;&#20064;&#25490;&#24207;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20132;&#20102;&#20116;&#20010;&#36816;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#21644;&#20250;&#35805;&#20449;&#24687;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#32452;&#35013;&#36215;&#26469;&#12290;&#32452;&#35013;&#30340;&#27169;&#22411;&#22312;&#21021;&#27493;&#35780;&#20272;&#20013;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;POSS&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#32452;&#35013;&#30340;&#27169;&#22411;&#65292;&#22312;&#21021;&#27493;&#35780;&#20272;&#20013;&#20063;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our team(THUIR2) participated in both FOSS and POSS subtasks of the NTCIR-161 Session Search (SS) Task. This paper describes our approaches and results. In the FOSS subtask, we submit five runs using learning-to-rank and fine-tuned pre-trained language models. We fine-tuned the pre-trained language model with ad-hoc data and session information and assembled them by a learning-to-rank method. The assembled model achieves the best performance among all participants in the preliminary evaluation. In the POSS subtask, we used an assembled model which also achieves the best performance in the preliminary evaluation.
&lt;/p&gt;</description></item><item><title>VesselMorph&#20351;&#29992;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#24863;&#30693;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#34880;&#31649;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#26469;&#25512;&#24191;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00240</link><description>&lt;p&gt;
VesselMorph: &#22522;&#20110;&#24418;&#29366;&#24863;&#30693;&#34920;&#24449;&#30340;&#39046;&#22495;&#36890;&#29992;&#30340;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VesselMorph: Domain-Generalized Retinal Vessel Segmentation via Shape-Aware Representation. (arXiv:2307.00240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00240
&lt;/p&gt;
&lt;p&gt;
VesselMorph&#20351;&#29992;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#24863;&#30693;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#34880;&#31649;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#26469;&#25512;&#24191;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#32479;&#19968;&#30340;&#25104;&#20687;&#21327;&#35758;&#65292;&#26469;&#33258;&#19981;&#21516;&#31449;&#28857;&#30340;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#26159;&#21307;&#23398;&#22270;&#20687;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#31639;&#27861;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#35270;&#32593;&#33180;&#34880;&#31649;&#22270;&#20687;&#65292;&#39046;&#22495;&#36716;&#31227;&#36890;&#24120;&#34920;&#29616;&#20026;&#24378;&#24230;&#12289;&#23545;&#27604;&#24230;&#21644;&#20998;&#36776;&#29575;&#30340;&#21464;&#21270;&#65292;&#32780;&#34880;&#31649;&#30340;&#22522;&#26412;&#31649;&#29366;&#24418;&#29366;&#20445;&#25345;&#19981;&#21464;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#36825;&#31181;&#39046;&#22495;&#19981;&#21464;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#28145;&#24230;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VesselMorph&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#24418;&#29366;&#24863;&#30693;&#34920;&#24449;&#26469;&#25512;&#24191;2D&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#20219;&#21153;&#12290;&#21463;&#20256;&#32479;Frangi&#28388;&#27874;&#22120;&#21644;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#30340;&#21452;&#26497;&#24352;&#37327;&#22330;&#26469;&#25551;&#36848;&#34880;&#31649;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#20174;&#32780;&#32771;&#34385;&#21040;&#24418;&#29366;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#24378;&#24230;&#22270;&#20687;&#21644;&#24352;&#37327;&#22330;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the absence of a single standardized imaging protocol, domain shift between data acquired from different sites is an inherent property of medical images and has become a major obstacle for large-scale deployment of learning-based algorithms. For retinal vessel images, domain shift usually presents as the variation of intensity, contrast and resolution, while the basic tubular shape of vessels remains unaffected. Thus, taking advantage of such domain-invariant morphological features can greatly improve the generalizability of deep models. In this study, we propose a method named VesselMorph which generalizes the 2D retinal vessel segmentation task by synthesizing a shape-aware representation. Inspired by the traditional Frangi filter and the diffusion tensor imaging literature, we introduce a Hessian-based bipolar tensor field to depict the morphology of the vessels so that the shape information is taken into account. We map the intensity image and the tensor field to a latent sp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#24212;&#29992;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#65288;FFA&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35745;&#31639;&#23616;&#37096;&#20248;&#21183;&#20989;&#25968;&#26469;&#20943;&#36731;&#23545;&#35745;&#31639;&#36164;&#28304;&#21644;&#26550;&#26500;&#25193;&#23637;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2307.00231</link><description>&lt;p&gt;
&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#29992;&#20110;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Forward-Forward Algorithm for Hyperspectral Image Classification: A Preliminary Study. (arXiv:2307.00231v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#24212;&#29992;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#65288;FFA&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35745;&#31639;&#23616;&#37096;&#20248;&#21183;&#20989;&#25968;&#26469;&#20943;&#36731;&#23545;&#35745;&#31639;&#36164;&#28304;&#21644;&#26550;&#26500;&#25193;&#23637;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31070;&#32463;&#32593;&#32476;&#20013;&#20248;&#21270;&#26435;&#37325;&#21644;&#20559;&#24046;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#23588;&#20854;&#22312;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#36965;&#24863;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#26041;&#38754;&#24341;&#21457;&#20102;&#38761;&#21629;&#12290;&#21453;&#21521;&#20256;&#25773;&#30340;&#27969;&#34892;&#28304;&#20110;&#23427;&#22312;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#33021;&#22815;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21453;&#21521;&#20256;&#25773;&#24182;&#38750;&#27809;&#26377;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#12289;&#26799;&#24230;&#28040;&#22833;&#12289;&#36807;&#25311;&#21512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#19968;&#31181;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#65288;FFA&#65289;&#36890;&#36807;&#35745;&#31639;&#23616;&#37096;&#20248;&#21183;&#20989;&#25968;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#19981;&#26029;&#25193;&#23637;&#26550;&#26500;&#30340;&#20381;&#36182;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;FFA&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The back-propagation algorithm has long been the de-facto standard in optimizing weights and biases in neural networks, particularly in cutting-edge deep learning models. Its widespread adoption in fields like natural language processing, computer vision, and remote sensing has revolutionized automation in various tasks. The popularity of back-propagation stems from its ability to achieve outstanding performance in tasks such as classification, detection, and segmentation. Nevertheless, back-propagation is not without its limitations, encompassing sensitivity to initial conditions, vanishing gradients, overfitting, and computational complexity. The recent introduction of a forward-forward algorithm (FFA), which computes local goodness functions to optimize network parameters, alleviates the dependence on substantial computational resources and the constant need for architectural scaling. This study investigates the application of FFA for hyperspectral image classification. Experimental
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22840;&#24352;&#26816;&#27979;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00209</link><description>&lt;p&gt;
&#22270;&#20687;&#30340;&#37325;&#35201;&#24615;&#65306;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22840;&#24352;&#26816;&#27979;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22840;&#24352;&#65292;&#21363;&#22840;&#22823;&#20854;&#35789;&#65292;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#22840;&#24352;&#26816;&#27979;&#26159;&#29702;&#35299;&#20154;&#31867;&#34920;&#36798;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#24050;&#32463;&#26377;&#20960;&#39033;&#20851;&#20110;&#22840;&#24352;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#30340;&#30740;&#31350;&#21482;&#20851;&#27880;&#25991;&#26412;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#21457;&#23637;&#65292;&#20154;&#20204;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#27169;&#24577;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#31561;&#65289;&#26469;&#34920;&#36798;&#22840;&#24352;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#12290;&#25105;&#20204;&#20174;&#24494;&#21338;&#65288;&#20013;&#22269;&#30340;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#19968;&#20123;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#24494;&#21338;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#35270;&#20026;&#20004;&#31181;&#27169;&#24577;&#65292;&#25506;&#32034;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#22312;&#22840;&#24352;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#36825;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#20116;&#20010;&#19981;&#21516;&#30340;&#20027;&#39064;&#26500;&#24314;&#30340;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection of hyperbole is an important part of understanding human expression. There have been several studies on hyperbole detection, but most of which focus on text modality only. However, with the development of social media, people can create hyperbolic expressions with various modalities, including text, images, videos, etc. In this paper, we focus on multimodal hyperbole detection. We create a multimodal detection dataset\footnote{The dataset will be released to the community.} from Weibo (a Chinese social media) and carry out some studies on it. We treat the text and image from a piece of weibo as two modalities and explore the role of text and image for hyperbole detection. Different pre-trained multimodal encoders are also evaluated on this downstream task to show their performance. Besides, since this dataset is constructed from five different topics, we also evaluate the cross-domain performance of different 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26550;&#26500;GPAT&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#38646;&#20214;&#30340;&#23039;&#24577;&#24182;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00206</link><description>&lt;p&gt;
&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
General Part Assembly Planning. (arXiv:2307.00206v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26550;&#26500;GPAT&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#38646;&#20214;&#30340;&#23039;&#24577;&#24182;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#32452;&#35013;&#39046;&#22495;&#30340;&#22823;&#22810;&#25968;&#25104;&#21151;&#37117;&#23616;&#38480;&#20110;&#21333;&#20010;&#30446;&#26631;&#25110;&#32773;&#21333;&#19968;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#30740;&#31350;&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;&#65292;&#21363;&#20351;&#29992;&#26410;&#35265;&#36807;&#30340;&#38646;&#20214;&#24418;&#29366;&#21019;&#24314;&#26032;&#39062;&#30446;&#26631;&#32452;&#35013;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;Transformer&#65288;GPAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25512;&#26029;&#27599;&#20010;&#38646;&#20214;&#24418;&#29366;&#22914;&#20309;&#23545;&#24212;&#30446;&#26631;&#24418;&#29366;&#65292;&#20934;&#30830;&#39044;&#27979;&#38646;&#20214;&#30340;&#23039;&#24577;&#12290;&#25105;&#20204;&#22312;3D CAD&#27169;&#22411;&#21644;&#29616;&#23454;&#19990;&#30028;&#25195;&#25551;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GPAT&#20855;&#26377;&#23545;&#26032;&#39062;&#21644;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#21644;&#38646;&#20214;&#24418;&#29366;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most successes in autonomous robotic assembly have been restricted to single target or category. We propose to investigate general part assembly, the task of creating novel target assemblies with unseen part shapes. To tackle the planning of general part assembly, we present General Part Assembly Transformer (GPAT), a transformer based model architecture that accurately predicts part poses by inferring how each part shape corresponds to the target shape. Our experiments on both 3D CAD models and real-world scans demonstrate GPAT's generalization abilities to novel and diverse target and part shapes. Project website: https://general-part-assembly.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00185</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#24212;&#29992;&#30340;&#26500;&#36896;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application. (arXiv:2307.00185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;(IRWNNs)&#30001;&#20110;&#26131;&#20110;&#23454;&#29616;&#21644;&#24555;&#36895;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;IRWNNs&#30340;&#19968;&#20010;&#26174;&#33879;&#32570;&#28857;&#26159;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#65288;&#33410;&#28857;&#65289;&#19982;&#27531;&#24046;&#35823;&#24046;&#65288;&#27169;&#22411;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#30340;&#21487;&#35299;&#37322;&#30340;&#26500;&#36896;&#31639;&#27861;(ICA)&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#26469;&#38543;&#26426;&#20998;&#37197;&#38544;&#34255;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#33410;&#28857;&#27744;&#31574;&#30053;&#33719;&#21462;&#26356;&#26377;&#21033;&#20110;&#25910;&#25947;&#30340;&#38544;&#34255;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#35777;&#26126;&#20102;ICA&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;ICA&#30340;&#36731;&#37327;&#32423;&#29256;&#26412;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#12290;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental random weight neural networks (IRWNNs) have gained attention in view of its easy implementation and fast learning. However, a significant drawback of IRWNNs is that the elationship between the hidden parameters (node)and the residual error (model performance) is difficult to be interpreted. To address the above issue, this article proposes an interpretable constructive algorithm (ICA) with geometric information constraint. First, based on the geometric relationship between the hidden parameters and the residual error, an interpretable geometric information constraint is proposed to randomly assign the hidden parameters. Meanwhile, a node pool strategy is employed to obtain hidden parameters that is more conducive to convergence from hidden parameters satisfying the proposed constraint. Furthermore, the universal approximation property of the ICA is proved. Finally, a lightweight version of ICA is presented for large-scale data modeling tasks. Experimental results on six ben
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;</title><link>http://arxiv.org/abs/2307.00184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;
&lt;/p&gt;
&lt;p&gt;
Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#39537;&#21160;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#33719;&#24471;&#30340;&#20154;&#26684;&#29305;&#36136;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#20154;&#26684;&#26159;&#20915;&#23450;&#20132;&#27969;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#39564;&#35777;&#30340;&#24515;&#29702;&#27979;&#37327;&#27979;&#35797;&#65292;&#24182;&#23545;&#20174;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#36827;&#34892;&#37327;&#21270;&#12289;&#20998;&#26512;&#21644;&#22609;&#36896;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#26576;&#20123;LLMs&#30340;&#36755;&#20986;&#20013;&#27169;&#25311;&#30340;&#20154;&#26684;&#65288;&#22312;&#29305;&#23450;&#30340;&#25552;&#31034;&#37197;&#32622;&#19979;&#65289;&#26159;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#65307;2&#65289;LLM&#27169;&#25311;&#30340;&#20154;&#26684;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#23545;&#20110;&#26356;&#22823;&#30340;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#26356;&#24378;&#65307;3&#65289;LLM&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#30340;&#32500;&#24230;&#36827;&#34892;&#22609;&#36896;&#65292;&#20197;&#27169;&#20223;&#29305;&#23450;&#30340;&#20154;&#26684;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As LLMs increasingly power conversational agents, the synthesized personality embedded in these models by virtue of their training on large amounts of human-generated data draws attention. Since personality is an important factor determining the effectiveness of communication, we present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used LLMs. We find that: 1) personality simulated in the outputs of some LLMs (under specific prompting configurations) is reliable and valid; 2) evidence of reliability and validity of LLM-simulated personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific personality profiles. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#25512;&#29702;&#25163;&#20876;&#65292;&#29992;&#20110;&#23558;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#23454;&#20363;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25216;&#24039;&#30340;&#28436;&#31034;&#65292;&#24110;&#21161;&#35835;&#32773;&#29702;&#35299;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;&#35770;&#25991;&#26368;&#21518;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#20197;&#35828;&#26126;&#36825;&#20123;&#25216;&#24039;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.00171</link><description>&lt;p&gt;
&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#25512;&#29702;&#25163;&#20876;
&lt;/p&gt;
&lt;p&gt;
The Integer Linear Programming Inference Cookbook. (arXiv:2307.00171v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#25512;&#29702;&#25163;&#20876;&#65292;&#29992;&#20110;&#23558;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#23454;&#20363;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25216;&#24039;&#30340;&#28436;&#31034;&#65292;&#24110;&#21161;&#35835;&#32773;&#29702;&#35299;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;&#35770;&#25991;&#26368;&#21518;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#20197;&#35828;&#26126;&#36825;&#20123;&#25216;&#24039;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#24050;&#34987;&#29992;&#20110;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#20013;&#30340;&#25512;&#29702;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25351;&#23548;&#35835;&#32773;&#23558;&#26032;&#30340;&#25512;&#29702;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23454;&#20363;&#65292;&#24182;&#20197;&#19968;&#31995;&#21015;&#30340;&#25216;&#24039;&#36827;&#34892;&#32452;&#32455;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36890;&#36807;&#20004;&#20010;&#23454;&#20363;&#26469;&#35828;&#26126;&#36825;&#20123;&#25216;&#24039;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, integer linear programs have been employed to model inference in many natural language processing problems. This survey is meant to guide the reader through the process of framing a new inference problem as an instance of an integer linear program and is structured as a collection of recipes. At the end, we will see two worked examples to illustrate the use of these recipes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;VoxCeleb&#30340;&#24320;&#25918;&#24335;&#28436;&#35762;&#32773;&#35782;&#21035;&#22522;&#20934;&#65292;&#24182;&#25506;&#35752;&#20102;&#22788;&#29702;&#35266;&#23519;&#21517;&#21333;&#22823;&#23567;&#23545;&#26816;&#27979;&#24615;&#33021;&#24433;&#21709;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.00169</link><description>&lt;p&gt;
VoxWatch: VoxCeleb&#19978;&#30340;&#24320;&#25918;&#24335;&#28436;&#35762;&#32773;&#35782;&#21035;&#22522;&#20934;&#12290; (arXiv:2307.00169v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
VoxWatch: An open-set speaker recognition benchmark on VoxCeleb. (arXiv:2307.00169v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;VoxCeleb&#30340;&#24320;&#25918;&#24335;&#28436;&#35762;&#32773;&#35782;&#21035;&#22522;&#20934;&#65292;&#24182;&#25506;&#35752;&#20102;&#22788;&#29702;&#35266;&#23519;&#21517;&#21333;&#22823;&#23567;&#23545;&#26816;&#27979;&#24615;&#33021;&#24433;&#21709;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24320;&#25918;&#24335;&#28436;&#35762;&#32773;&#35782;&#21035;&#65288;OSI&#65289;&#22312;&#27450;&#35784;&#39044;&#38450;&#31561;&#24191;&#27867;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#19982;&#28436;&#35762;&#32773;&#39564;&#35777;&#65288;SV&#65289;&#30456;&#27604;&#65292;&#28436;&#35762;&#32773;&#35782;&#21035;&#31038;&#21306;&#23545;&#20854;&#20851;&#27880;&#36739;&#23569;&#12290; OSI&#28041;&#21450;&#30830;&#23450;&#27979;&#35797;&#35821;&#38899;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#19968;&#32452;&#39044;&#20808;&#27880;&#20876;&#30340;&#20010;&#20307;&#65288;&#20869;&#37096;&#38598;&#65289;&#30340;&#28436;&#35762;&#32773;&#65292;&#25110;&#32773;&#26159;&#21542;&#26469;&#33258;&#19968;&#20010;&#22806;&#37096;&#38598;&#28436;&#35762;&#32773;&#12290;&#38500;&#20102;&#19982;&#35821;&#38899;&#21464;&#24322;&#30456;&#20851;&#30340;&#20856;&#22411;&#25361;&#25112;&#22806;&#65292;OSI&#36824;&#23481;&#26131;&#20986;&#29616;&#8220;&#35823;&#25253;&#38382;&#39064;&#8221;&#65307;&#38543;&#30528;&#20869;&#37096;&#38598;&#28436;&#35762;&#32773;&#20154;&#21475;&#65288;&#20063;&#31216;&#20026;&#35266;&#23519;&#21517;&#21333;&#65289;&#30340;&#22686;&#21152;&#65292;&#22806;&#37096;&#38598;&#20998;&#25968;&#21464;&#22823;&#65292;&#23548;&#33268;&#35823;&#25253;&#29575;&#22686;&#21152;&#12290;&#36825;&#23545;&#37329;&#34701;&#26426;&#26500;&#21644;&#36793;&#22659;&#23433;&#20840;&#31561;&#24212;&#29992;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#20013;&#35266;&#23519;&#21517;&#21333;&#30340;&#22823;&#23567;&#36890;&#24120;&#20026;&#20960;&#21315;&#20010;&#28436;&#35762;&#32773;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#22320;&#37327;&#21270;&#35823;&#25253;&#38382;&#39064;&#24182;&#24320;&#21457;&#20943;&#36731;&#35266;&#23519;&#21517;&#21333;&#22823;&#23567;&#23545;&#26816;&#27979;&#24615;&#33021;&#24433;&#21709;&#30340;&#25216;&#26415;&#26159;&#37325;&#35201;&#30340;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Despite its broad practical applications such as in fraud prevention, open-set speaker identification (OSI) has received less attention in the speaker recognition community compared to speaker verification (SV). OSI deals with determining if a test speech sample belongs to a speaker from a set of pre-enrolled individuals (in-set) or if it is from an out-of-set speaker. In addition to the typical challenges associated with speech variability, OSI is prone to the "false-alarm problem"; as the size of the in-set speaker population (a.k.a watchlist) grows, the out-of-set scores become larger, leading to increased false alarm rates. This is in particular challenging for applications in financial institutions and border security where the watchlist size is typically of the order of several thousand speakers. Therefore, it is important to systematically quantify the false-alarm problem, and develop techniques that alleviate the impact of watchlist size on detection performance. Prior studies 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20844;&#24179;&#12289;&#28789;&#27963;&#21644;&#31169;&#23494;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#24212;&#29992;&#22330;&#26223;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.00161</link><description>&lt;p&gt;
FFPDG: &#24555;&#36895;&#12289;&#20844;&#24179;&#21644;&#31169;&#23494;&#30340;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FFPDG: Fast, Fair and Private Data Generation. (arXiv:2307.00161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20844;&#24179;&#12289;&#28789;&#27963;&#21644;&#31169;&#23494;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#24212;&#29992;&#22330;&#26223;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#32463;&#24120;&#34987;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#26435;&#26159;&#21512;&#25104;&#25968;&#25454;&#38754;&#20020;&#30340;&#20004;&#20010;&#22823;&#38382;&#39064;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#33021;&#26356;&#20855;&#20559;&#35265;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39640;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20844;&#24179;&#12289;&#28789;&#27963;&#21644;&#31169;&#23494;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#35813;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30495;&#23454;&#24212;&#29992;&#22330;&#26223;&#19979;&#65288;&#25512;&#26029;&#38454;&#27573;&#65289;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling has been used frequently in synthetic data generation. Fairness and privacy are two big concerns for synthetic data. Although Recent GAN [\cite{goodfellow2014generative}] based methods show good results in preserving privacy, the generated data may be more biased. At the same time, these methods require high computation resources. In this work, we design a fast, fair, flexible and private data generation method. We show the effectiveness of our method theoretically and empirically. We show that models trained on data generated by the proposed method can perform well (in inference stage) on real application scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#65292;&#25552;&#20986;&#20102;SN-Netv2&#65292;&#23427;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#23454;&#29616;&#22810;&#26679;&#24615;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.00154</link><description>&lt;p&gt;
Stitched ViTs&#26159;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stitched ViTs are Flexible Vision Backbones. (arXiv:2307.00154v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#65292;&#25552;&#20986;&#20102;SN-Netv2&#65292;&#23427;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#23454;&#29616;&#22810;&#26679;&#24615;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#26222;&#36890;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#21147;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;ViTs&#30340;&#29616;&#26377;&#24037;&#20316;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#37319;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;ViTs&#38656;&#35201;&#21333;&#29420;&#35757;&#32451;&#65292;&#24182;&#21463;&#21040;&#22266;&#23450;&#30340;&#24615;&#33021;-&#25928;&#29575;&#26435;&#34913;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#21487;&#25340;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#26469;&#24555;&#36895;&#29983;&#25104;&#28085;&#30422;&#20016;&#23500;&#23376;&#32593;&#32476;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#25903;&#25345;&#22312;&#36816;&#34892;&#26102;&#30340;&#22810;&#26679;&#24615;&#24615;&#33021;-&#25928;&#29575;&#26435;&#34913;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SN-Netv2&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#25913;&#36827;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#20419;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#25340;&#25509;&#26041;&#26696;&#26469;&#25193;&#22823;&#25340;&#25509;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32771;&#34385;&#31354;&#38388;&#20013;&#24213;&#23618;FLOPs&#20998;&#24067;&#30340;&#36164;&#28304;&#21463;&#38480;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#37319;&#26679;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;SN-Netv2&#36827;&#34892;&#20102;&#32454;&#24494;&#35843;&#25972;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained plain vision Transformers (ViTs) have been the workhorse for many downstream tasks. However, existing works utilizing off-the-shelf ViTs are inefficient in terms of training and deployment, because adopting ViTs with individual sizes requires separate training and is restricted by fixed performance-efficiency trade-offs. In this paper, we are inspired by stitchable neural networks, which is a new framework that cheaply produces a single model that covers rich subnetworks by stitching pretrained model families, supporting diverse performance-efficiency trade-offs at runtime. Building upon this foundation, we introduce SN-Netv2, a systematically improved model stitching framework to facilitate downstream task adaptation. Specifically, we first propose a Two-way stitching scheme to enlarge the stitching space. We then design a resource-constrained sampling strategy that takes into account the underlying FLOPs distributions in the space for improved sampling. Finally, we o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20316;&#19994;&#30340;&#20010;&#24615;&#21270;&#25552;&#31034;&#65292;&#35813;&#25552;&#31034;&#34987;&#23398;&#29983;&#20204;&#35780;&#20215;&#20026;&#26377;&#29992;&#12290;&#23454;&#39564;&#32452;&#22312;&#21551;&#29992;GPT&#25552;&#31034;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25552;&#20132;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#20943;&#23569;&#23545;&#24179;&#21488;&#24120;&#35268;&#21453;&#39304;&#30340;&#20381;&#36182;&#12290;&#23545;&#20110;&#26080;&#27861;&#20351;&#29992;GPT&#25552;&#31034;&#30340;&#20219;&#21153;&#65292;&#23454;&#39564;&#32452;&#23398;&#29983;&#35299;&#20915;&#20316;&#19994;&#25152;&#38656;&#30340;&#26102;&#38388;&#20063;&#36739;&#23569;&#12290;</title><link>http://arxiv.org/abs/2307.00150</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#21270;&#32534;&#31243;&#20316;&#19994;&#21453;&#39304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT&#65289;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (GPT) for automating feedback on programming assignments. (arXiv:2307.00150v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20316;&#19994;&#30340;&#20010;&#24615;&#21270;&#25552;&#31034;&#65292;&#35813;&#25552;&#31034;&#34987;&#23398;&#29983;&#20204;&#35780;&#20215;&#20026;&#26377;&#29992;&#12290;&#23454;&#39564;&#32452;&#22312;&#21551;&#29992;GPT&#25552;&#31034;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25552;&#20132;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#20943;&#23569;&#23545;&#24179;&#21488;&#24120;&#35268;&#21453;&#39304;&#30340;&#20381;&#36182;&#12290;&#23545;&#20110;&#26080;&#27861;&#20351;&#29992;GPT&#25552;&#31034;&#30340;&#20219;&#21153;&#65292;&#23454;&#39564;&#32452;&#23398;&#29983;&#35299;&#20915;&#20316;&#19994;&#25152;&#38656;&#30340;&#26102;&#38388;&#20063;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#29983;&#25104;&#20010;&#24615;&#21270;&#32534;&#31243;&#20316;&#19994;&#21453;&#39304;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#20195;&#30721;&#35821;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#27491;&#30830;&#35299;&#20915;&#20219;&#21153;&#30340;&#19981;&#21516;&#26041;&#27861;&#31561;&#22810;&#20010;&#22240;&#32032;&#65292;&#35201;&#27714;&#24456;&#39640;&#12290;&#22312;&#36825;&#20010;&#23454;&#39564;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#33258;&#21160;&#21270;&#20102;&#21453;&#39304;&#29983;&#25104;&#30340;&#36807;&#31243;&#65292;&#20026;&#23398;&#29983;&#22312;&#19968;&#20010;&#33258;&#21160;&#35780;&#20272;&#24179;&#21488;&#19978;&#35299;&#20915;&#32534;&#31243;&#20316;&#19994;&#29983;&#25104;&#20010;&#24615;&#21270;&#25552;&#31034;&#12290;&#23398;&#29983;&#23545;GPT&#29983;&#25104;&#30340;&#25552;&#31034;&#30340;&#26377;&#29992;&#24615;&#36827;&#34892;&#20102;&#31215;&#26497;&#35780;&#20215;&#12290;&#23454;&#39564;&#32452;&#65288;&#21551;&#29992;&#20102;GPT&#25552;&#31034;&#65289;&#22312;&#20219;&#21153;&#20013;&#30340;&#36830;&#32493;&#23581;&#35797;&#20013;&#65292;&#25552;&#20132;&#25104;&#21151;&#30340;&#30334;&#20998;&#27604;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#23545;&#24179;&#21488;&#30340;&#24120;&#35268;&#21453;&#39304;&#20381;&#36182;&#36739;&#23569;&#12290;&#23545;&#20110;&#19981;&#21487;&#29992;GPT&#21453;&#39304;&#30340;&#20219;&#21153;&#65292;&#23454;&#39564;&#32452;&#35299;&#20915;&#20316;&#19994;&#25152;&#38656;&#30340;&#26102;&#38388;&#26174;&#33879;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#24403;GPT&#25552;&#31034;&#19981;&#21487;&#29992;&#26102;&#65292;&#23454;&#39564;&#32452;&#23398;&#29983;&#21021;&#22987;&#23545;&#35299;&#20915;&#20316;&#19994;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the challenge of generating personalized feedback for programming assignments is demanding due to several factors, like the complexity of code syntax or different ways to correctly solve a task. In this experimental study, we automated the process of feedback generation by employing OpenAI's GPT-3.5 model to generate personalized hints for students solving programming assignments on an automated assessment platform. Students rated the usefulness of GPT-generated hints positively. The experimental group (with GPT hints enabled) relied less on the platform's regular feedback but performed better in terms of percentage of successful submissions across consecutive attempts for tasks, where GPT hints were enabled. For tasks where the GPT feedback was made unavailable, the experimental group needed significantly less time to solve assignments. Furthermore, when GPT hints were unavailable, students in the experimental condition were initially less likely to solve the assignment cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#23478;&#24237;&#36741;&#21161;&#26426;&#22120;&#20154;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#23398;&#20064;&#24182;&#21019;&#36896;&#26032;&#30340;&#26089;&#39184;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2307.00114</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#23398;&#20064;&#21644;&#21019;&#36896;&#26032;&#30340;&#26089;&#39184;&#36873;&#25321;&#30340;&#20010;&#24615;&#21270;&#23478;&#24237;&#36741;&#21161;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
A Personalized Household Assistive Robot that Learns and Creates New Breakfast Options through Human-Robot Interaction. (arXiv:2307.00114v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#23478;&#24237;&#36741;&#21161;&#26426;&#22120;&#20154;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#23398;&#20064;&#24182;&#21019;&#36896;&#26032;&#30340;&#26089;&#39184;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24110;&#21161;&#29992;&#25143;&#23436;&#25104;&#23478;&#21153;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#39318;&#20808;&#24517;&#39035;&#20174;&#29992;&#25143;&#37027;&#37324;&#23398;&#20064;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#27599;&#22825;&#20197;&#30456;&#21516;&#26041;&#24335;&#25191;&#34892;&#30456;&#21516;&#20219;&#21153;&#20250;&#35753;&#26426;&#22120;&#20154;&#30340;&#20351;&#29992;&#32773;&#24863;&#21040;&#26080;&#36259;&#65292;&#22240;&#27492;&#36741;&#21161;&#26426;&#22120;&#20154;&#24517;&#39035;&#25214;&#21040;&#21019;&#36896;&#24615;&#30340;&#26041;&#27861;&#26469;&#23436;&#25104;&#23478;&#21153;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#26550;&#26500;&#65292;&#20511;&#21161;&#27492;&#26550;&#26500;&#65292;&#23478;&#24237;&#36741;&#21161;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#29992;&#25143;&#37027;&#37324;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#26089;&#39184;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#26469;&#20934;&#22791;&#26089;&#39184;&#12290;&#35813;&#26550;&#26500;&#36824;&#21487;&#20197;&#21033;&#29992;&#25152;&#23398;&#30693;&#35782;&#22312;&#36739;&#38271;&#26102;&#38388;&#20869;&#21019;&#24314;&#26032;&#30340;&#26089;&#39184;&#36873;&#25321;&#12290;&#25152;&#25552;&#20986;&#30340;&#35748;&#30693;&#26550;&#26500;&#32467;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#24863;&#30693;&#23398;&#20064;&#31639;&#27861;&#12289;&#35745;&#31639;&#23454;&#26045;&#30340;&#35748;&#30693;&#35760;&#24518;&#32534;&#30721;&#21644;&#23398;&#20064;&#30340;&#27169;&#22411;&#12289;&#19968;&#20010;&#29992;&#20110;&#22312;&#23478;&#24237;&#20013;&#25342;&#21462;&#21644;&#25918;&#32622;&#29289;&#20307;&#30340;&#20219;&#21153;&#35268;&#21010;&#22120;&#12289;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;(GUI)&#20197;&#21450;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#21019;&#24314;&#26032;&#26089;&#39184;&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For robots to assist users with household tasks, they must first learn about the tasks from the users. Further, performing the same task every day, in the same way, can become boring for the robot's user(s), therefore, assistive robots must find creative ways to perform tasks in the household. In this paper, we present a cognitive architecture for a household assistive robot that can learn personalized breakfast options from its users and then use the learned knowledge to set up a table for breakfast. The architecture can also use the learned knowledge to create new breakfast options over a longer period of time. The proposed cognitive architecture combines state-of-the-art perceptual learning algorithms, computational implementation of cognitive models of memory encoding and learning, a task planner for picking and placing objects in the household, a graphical user interface (GUI) to interact with the user and a novel approach for creating new breakfast options using the learned knowl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22238;&#31572;&#22797;&#26434;&#21307;&#23398;&#38382;&#39064;&#19978;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#20854;&#29983;&#25104;&#30340;&#31572;&#26696;&#26356;&#21152;&#27880;&#37325;&#19978;&#19979;&#25991;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00112</link><description>&lt;p&gt;
ChatGPT&#22312;USMLE&#19978;&#30340;&#34920;&#29616;&#65306;&#20026;AI&#36741;&#21161;&#21307;&#23398;&#25945;&#32946;&#24320;&#21551;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education. (arXiv:2307.00112v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00112
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22238;&#31572;&#22797;&#26434;&#21307;&#23398;&#38382;&#39064;&#19978;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#20854;&#29983;&#25104;&#30340;&#31572;&#26696;&#26356;&#21152;&#27880;&#37325;&#19978;&#19979;&#25991;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#33719;&#24471;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#33258;&#20174;OpenAI&#21457;&#24067;ChatGPT&#20197;&#26469;&#65292;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;AI&#30340;&#19994;&#21153;&#30340;&#27969;&#34892;&#24230;&#22823;&#24133;&#22686;&#38271;&#12290;&#20154;&#20204;&#22312;&#32844;&#19994;&#21644;&#20010;&#20154;&#29983;&#27963;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#22320;&#20351;&#29992;ChatGPT&#12290;&#32771;&#34385;&#21040;ChatGPT&#30340;&#24191;&#27867;&#20351;&#29992;&#21644;&#20154;&#20204;&#23545;&#20854;&#30340;&#20381;&#36182;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;ChatGPT&#22312;&#22238;&#31572;&#22797;&#26434;&#21307;&#23398;&#21644;&#20020;&#24202;&#38382;&#39064;&#19978;&#30340;&#21487;&#38752;&#24615;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#21704;&#20315;&#22823;&#23398;&#35299;&#21078;&#23398;&#30693;&#35782;&#21644;&#32654;&#22269;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65288;USMLE&#65289;&#38382;&#21367;&#26469;&#23454;&#29616;&#30446;&#26631;&#12290;&#26412;&#25991;&#20351;&#29992;&#21452;&#22240;&#32032;&#26041;&#24046;&#20998;&#26512;&#21644;&#20107;&#21518;&#20998;&#26512;&#35780;&#20272;&#20102;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;&#20004;&#32773;&#37117;&#26174;&#31034;&#20986;&#26684;&#24335;&#21644;&#25552;&#31034;&#20043;&#38388;&#30340;&#31995;&#32479;&#21327;&#21464;&#12290;&#27492;&#22806;&#65292;&#21307;&#29983;&#35780;&#20272;&#32773;&#36824;&#23545;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#27934;&#23519;&#21147;&#36827;&#34892;&#20102;&#29420;&#31435;&#35780;&#20215;&#12290;&#20998;&#26512;&#32467;&#26524;&#21457;&#29616;&#65292;ChatGPT&#29983;&#25104;&#30340;&#31572;&#26696;&#26356;&#21152;&#27880;&#37325;&#19978;&#19979;&#25991;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is gaining traction in more ways than ever before. The popularity of language models and AI-based businesses has soared since ChatGPT was made available to the general public via OpenAI. It is becoming increasingly common for people to use ChatGPT both professionally and personally. Considering the widespread use of ChatGPT and the reliance people place on it, this study determined how reliable ChatGPT can be for answering complex medical and clinical questions. Harvard University gross anatomy along with the United States Medical Licensing Examination (USMLE) questionnaire were used to accomplish the objective. The paper evaluated the obtained results using a 2-way ANOVA and posthoc analysis. Both showed systematic covariation between format and prompt. Furthermore, the physician adjudicators independently rated the outcome's accuracy, concordance, and insight. As a result of the analysis, ChatGPT-generated answers were found to be more context-oriented and rep
&lt;/p&gt;</description></item><item><title>Ticket-BERT&#26159;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#20107;&#20214;&#31649;&#29702;&#31080;&#25454;&#36827;&#34892;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#31080;&#25454;&#25968;&#25454;&#21644;&#26102;&#38388;&#25935;&#24863;&#24615;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00108</link><description>&lt;p&gt;
Ticket-BERT:&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#20107;&#20214;&#31649;&#29702;&#31080;&#25454;&#36827;&#34892;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Ticket-BERT: Labeling Incident Management Tickets with Language Models. (arXiv:2307.00108v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00108
&lt;/p&gt;
&lt;p&gt;
Ticket-BERT&#26159;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#20107;&#20214;&#31649;&#29702;&#31080;&#25454;&#36827;&#34892;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#31080;&#25454;&#25968;&#25454;&#21644;&#26102;&#38388;&#25935;&#24863;&#24615;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#20248;&#20808;&#32423;&#20107;&#20214;&#31080;&#25454;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#39640;&#25928;&#22320;&#20351;&#29992;&#31934;&#32454;&#20998;&#31867;&#26469;&#26631;&#27880;&#36825;&#20123;&#31080;&#25454;&#12290;&#28982;&#32780;&#65292;&#31080;&#25454;&#25968;&#25454;&#36890;&#24120;&#24456;&#22797;&#26434;&#65292;&#32473;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24102;&#26469;&#20102;&#20960;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#31080;&#25454;&#26082;&#21487;&#20197;&#30001;&#39044;&#23450;&#20041;&#31639;&#27861;&#30340;&#26426;&#22120;&#29983;&#25104;&#65292;&#20063;&#21487;&#20197;&#30001;&#20855;&#26377;&#19981;&#21516;&#21327;&#35758;&#30340;&#20855;&#26377;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#24037;&#31243;&#24072;&#26356;&#26032;&#21644;&#21019;&#24314;&#65292;&#65288;2&#65289;&#31080;&#25454;&#39057;&#32321;&#36827;&#34892;&#20462;&#35746;&#65292;&#36890;&#36807;&#20462;&#25913;&#20840;&#37096;&#25110;&#37096;&#20998;&#31080;&#25454;&#25551;&#36848;&#26469;&#26356;&#26032;&#31080;&#25454;&#29366;&#24577;&#65292;&#65288;3&#65289;&#31080;&#25454;&#26631;&#27880;&#26159;&#26102;&#38388;&#25935;&#24863;&#30340;&#65292;&#38656;&#35201;&#26681;&#25454;&#36719;&#20214;&#21644;&#30828;&#20214;&#25913;&#36827;&#30340;&#24555;&#36895;&#29983;&#21629;&#21608;&#26399;&#36827;&#34892;&#30693;&#35782;&#26356;&#26032;&#21644;&#26032;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Ticket-BERT&#65292;&#23427;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#31080;&#25454;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#20581;&#22766;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#20026;&#31080;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Ticket-BERT&#22312;Azure&#35748;&#30693;&#26381;&#21153;&#19978;&#20248;&#20110;&#22522;&#32447;&#21644;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;Ticket-BERT&#23553;&#35013;&#21040;&#19968;&#20010;&#31215;&#26497;&#30340;le...
&lt;/p&gt;
&lt;p&gt;
An essential aspect of prioritizing incident tickets for resolution is efficiently labeling tickets with fine-grained categories. However, ticket data is often complex and poses several unique challenges for modern machine learning methods: (1) tickets are created and updated either by machines with pre-defined algorithms or by engineers with domain expertise that share different protocols, (2) tickets receive frequent revisions that update ticket status by modifying all or parts of ticket descriptions, and (3) ticket labeling is time-sensitive and requires knowledge updates and new labels per the rapid software and hardware improvement lifecycle. To handle these issues, we introduce Ticket- BERT which trains a simple yet robust language model for labeling tickets using our proposed ticket datasets. Experiments demonstrate the superiority of Ticket-BERT over baselines and state-of-the-art text classifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT with an active le
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#35270;&#39057;&#24207;&#21015;&#20013;&#28895;&#38654;&#27169;&#24335;&#30340;&#26102;&#38388;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#37326;&#28779;&#28779;&#28976;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#28779;&#28798;&#20301;&#32622;&#26469;&#24110;&#21161;&#26080;&#20154;&#26426;&#23545;&#25239;&#26862;&#26519;&#28779;&#28798;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#29420;&#29305;&#30340;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#28779;&#28798;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00104</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#26080;&#20154;&#26426;&#25429;&#25417;&#30340;&#28895;&#38654;&#27169;&#24335;&#30340;&#26102;&#38388;&#20998;&#26512;&#26469;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#37326;&#28779;&#28779;&#28976;
&lt;/p&gt;
&lt;p&gt;
Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns Captured by Unmanned Aerial Systems. (arXiv:2307.00104v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#35270;&#39057;&#24207;&#21015;&#20013;&#28895;&#38654;&#27169;&#24335;&#30340;&#26102;&#38388;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#37326;&#28779;&#28779;&#28976;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#28779;&#28798;&#20301;&#32622;&#26469;&#24110;&#21161;&#26080;&#20154;&#26426;&#23545;&#25239;&#26862;&#26519;&#28779;&#28798;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#29420;&#29305;&#30340;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#28779;&#28798;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#35299;&#20915;&#20102;&#20351;&#29992;&#20165;&#37197;&#22791;RGB&#30456;&#26426;&#30340;&#26080;&#20154;&#26426;&#23454;&#26102;&#26816;&#27979;&#34987;&#26641;&#26408;&#12289;&#28895;&#38654;&#12289;&#20113;&#38654;&#21644;&#20854;&#20182;&#33258;&#28982;&#23631;&#38556;&#36974;&#25377;&#30340;&#37326;&#28779;&#65288;&#24403;&#28779;&#28976;&#34987;&#36974;&#25377;&#26102;&#65289;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#28895;&#38654;&#27169;&#24335;&#30340;&#35821;&#20041;&#20998;&#21106;&#36827;&#34892;&#26102;&#38388;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;CNN&#32534;&#30721;&#22120;&#21644;3D&#21367;&#31215;&#36827;&#34892;&#35299;&#30721;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#30340;&#39034;&#24207;&#21472;&#21152;&#26469;&#21033;&#29992;&#26102;&#38388;&#21464;&#21270;&#12290;&#39044;&#27979;&#30340;&#28779;&#28798;&#20301;&#32622;&#21487;&#20197;&#24110;&#21161;&#26080;&#20154;&#26426;&#26377;&#25928;&#22320;&#23545;&#25239;&#26862;&#26519;&#28779;&#28798;&#65292;&#24182;&#20934;&#30830;&#23450;&#20301;&#28779;&#28976;&#20301;&#32622;&#36827;&#34892;&#38459;&#29123;&#21270;&#23398;&#29289;&#36136;&#25237;&#25918;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20174;FLAME2&#25968;&#25454;&#38598;&#34893;&#29983;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;RGB&#35270;&#39057;&#21644;IR&#35270;&#39057;&#20197;&#30830;&#23450;&#22320;&#38754;&#30495;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#28779;&#28798;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;Dice&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper addresses the challenge of detecting obscured wildfires (when the fire flames are covered by trees, smoke, clouds, and other natural barriers) in real-time using drones equipped only with RGB cameras. We propose a novel methodology that employs semantic segmentation based on the temporal analysis of smoke patterns in video sequences. Our approach utilizes an encoder-decoder architecture based on deep convolutional neural network architecture with a pre-trained CNN encoder and 3D convolutions for decoding while using sequential stacking of features to exploit temporal variations. The predicted fire locations can assist drones in effectively combating forest fires and pinpoint fire retardant chemical drop on exact flame locations. We applied our method to a curated dataset derived from the FLAME2 dataset that includes RGB video along with IR video to determine the ground truth. Our proposed method has a unique property of detecting obscured fire and achieves a Dice sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#25551;&#36848;&#19981;&#21516;&#24615;&#21035;&#35748;&#21516;&#30340;&#20154;&#30340;&#25991;&#26412;&#26102;&#20135;&#29983;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#23545;&#21516;&#24535;&#20154;&#32676;&#30340;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#20998;&#26512;&#30340;&#24605;&#32500;&#38142;&#35302;&#21457;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#21477;&#23376;&#30340;regard&#65292;&#20026;&#21435;&#38500;LLMs&#36755;&#20986;&#20559;&#35265;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2307.00101</link><description>&lt;p&gt;
&#21516;&#24535;&#20154;&#32676;&#39318;&#20808;&#26159;&#20154;&#65306;&#35299;&#26500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#35748;&#21516;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models. (arXiv:2307.00101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#25551;&#36848;&#19981;&#21516;&#24615;&#21035;&#35748;&#21516;&#30340;&#20154;&#30340;&#25991;&#26412;&#26102;&#20135;&#29983;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#23545;&#21516;&#24535;&#20154;&#32676;&#30340;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#20998;&#26512;&#30340;&#24605;&#32500;&#38142;&#35302;&#21457;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#21477;&#23376;&#30340;regard&#65292;&#20026;&#21435;&#38500;LLMs&#36755;&#20986;&#20559;&#35265;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20027;&#35201;&#22312;&#32463;&#36807;&#26368;&#23567;&#21270;&#22788;&#29702;&#30340;&#32593;&#32476;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#23637;&#29616;&#20102;&#21019;&#24314;&#35813;&#20869;&#23481;&#30340;&#20154;&#20204;&#25152;&#25345;&#26377;&#30340;&#21508;&#31181;&#31038;&#20250;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#26080;&#24847;&#20013;&#23558;&#21051;&#26495;&#21360;&#35937;&#20256;&#36882;&#32473;&#36793;&#32536;&#21270;&#32676;&#20307;&#65292;&#22914;LGBTQIA+&#31038;&#32676;&#12290;&#26412;&#25991;&#23545;LLMs&#29983;&#25104;&#25551;&#36848;&#20855;&#26377;&#19981;&#21516;&#24615;&#21035;&#35748;&#21516;&#30340;&#20154;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#20351;&#29992;regard&#20998;&#25968;&#20998;&#26512;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#26174;&#31034;&#20986;&#23384;&#22312;&#23545;&#21516;&#24535;&#20154;&#32676;&#30340;&#21487;&#27979;&#37327;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#20998;&#26512;&#30340;&#24605;&#32500;&#38142;&#35302;&#21457;&#30340;&#20107;&#21518;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#21477;&#23376;&#30340;regard&#65292;&#36825;&#20195;&#34920;&#20102;&#35299;&#20915;&#27492;&#31867;&#24773;&#20917;&#19979;LLMs&#36755;&#20986;&#20559;&#35265;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.
&lt;/p&gt;</description></item><item><title>Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#12289;&#20020;&#24202;&#35786;&#26029;&#12289;&#25253;&#21578;&#29983;&#25104;&#12289;&#25968;&#25454;&#37325;&#24314;&#21644;&#33647;&#29289;/&#34507;&#30333;&#36136;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.00067</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;Transformer&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Transformers in Healthcare: A Survey. (arXiv:2307.00067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00067
&lt;/p&gt;
&lt;p&gt;
Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#12289;&#20020;&#24202;&#35786;&#26029;&#12289;&#25253;&#21578;&#29983;&#25104;&#12289;&#25968;&#25454;&#37325;&#24314;&#21644;&#33647;&#29289;/&#34507;&#30333;&#36136;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#31561;&#31038;&#20250;&#21508;&#20010;&#26041;&#38754;&#30340;&#26085;&#30410;&#28183;&#36879;&#65292;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#37319;&#29992;&#27491;&#22312;&#24555;&#36895;&#25913;&#21464;&#35768;&#22810;&#24212;&#29992;&#12290;Transformer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#26368;&#21021;&#26159;&#20026;&#20102;&#35299;&#20915;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#32780;&#24320;&#21457;&#30340;&#65292;&#24182;&#38543;&#21518;&#22312;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#22312;&#20869;&#30340;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#22312;&#26412;&#35843;&#26597;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#35813;&#26550;&#26500;&#22914;&#20309;&#34987;&#24212;&#29992;&#20110;&#20998;&#26512;&#21508;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#21307;&#23398;&#24433;&#20687;&#12289;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#12289;&#31038;&#20132;&#23186;&#20307;&#12289;&#29983;&#29702;&#20449;&#21495;&#21644;&#29983;&#29289;&#20998;&#23376;&#24207;&#21015;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#35786;&#26029;&#12289;&#25253;&#21578;&#29983;&#25104;&#12289;&#25968;&#25454;&#37325;&#24314;&#21644;&#33647;&#29289;/&#34507;&#30333;&#36136;&#21512;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#39318;&#36873;&#25253;&#21578;&#20107;&#39033;&#36827;&#34892;&#31995;&#32479;&#32508;&#36848;&#21644;meta&#20998;&#26512;&#65288;PRISMA&#65289;&#25351;&#21335;&#26469;&#30830;&#23450;&#30456;&#20851;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;Transformer&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of data, including medical imaging, structured and unstructured Electronic Health Records (EHR), social media, physiological signals, and biomolecular sequences. Those models could help in clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. We identified relevant studies using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We also discuss the benefits and limitations of using transformer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#23494;&#38598;&#22330;&#26223;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#30452;&#35266;&#30340;&#23450;&#24615;&#34920;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20102;&#36755;&#20837;&#21644;&#26102;&#38388;&#20851;&#27880;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20102;&#23450;&#24615;&#36712;&#36857;&#35745;&#31639;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#31526;&#21495;&#39537;&#21160;&#30340;&#31070;&#32463;&#32467;&#26500;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.00065</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31354;&#38388;&#20132;&#20114;&#30340;&#23450;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Qualitative Prediction of Multi-Agent Spatial Interactions. (arXiv:2307.00065v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#23494;&#38598;&#22330;&#26223;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#30452;&#35266;&#30340;&#23450;&#24615;&#34920;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20102;&#36755;&#20837;&#21644;&#26102;&#38388;&#20851;&#27880;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20102;&#23450;&#24615;&#36712;&#36857;&#35745;&#31639;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#31526;&#21495;&#39537;&#21160;&#30340;&#31070;&#32463;&#32467;&#26500;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39184;&#21381;&#12289;&#20179;&#24211;&#25110;&#21307;&#38498;&#31561;&#26085;&#24120;&#29983;&#27963;&#20013;&#37096;&#32626;&#26381;&#21153;&#26426;&#22120;&#20154;&#38656;&#35201;&#23545;&#23494;&#38598;&#21644;&#21160;&#24577;&#22330;&#26223;&#20013;&#21457;&#29983;&#30340;&#20132;&#20114;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#23545;&#19977;&#31181;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#39044;&#27979;&#23494;&#38598;&#22330;&#26223;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#65292;&#21253;&#25324;&#20351;&#29992;&#30452;&#35266;&#30340;&#23450;&#24615;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#32771;&#34385;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#20010;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#23427;&#20204;&#21033;&#29992;&#20102;&#36755;&#20837;&#21644;&#26102;&#38388;&#20851;&#27880;&#26426;&#21046;&#65292;&#24182;&#22312;&#20013;&#38271;&#26399;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#21069;&#20004;&#31181;&#26041;&#27861;&#23558;&#31216;&#20026;&#23450;&#24615;&#36712;&#36857;&#35745;&#31639;(QTC)&#30340;&#19981;&#21516;&#20851;&#31995;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#31526;&#21495;&#39537;&#21160;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#31354;&#38388;&#20132;&#20114;&#12290;&#31532;&#19977;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#31181;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#39044;&#27979;&#65292;&#35813;&#32593;&#32476;&#30340;&#36755;&#20986;&#32463;&#36807;&#21518;&#26399;&#22788;&#29702;&#20197;&#39044;&#27979;QTC&#31354;&#38388;&#20132;&#20114;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Deploying service robots in our daily life, whether in restaurants, warehouses or hospitals, calls for the need to reason on the interactions happening in dense and dynamic scenes. In this paper, we present and benchmark three new approaches to model and predict multi-agent interactions in dense scenes, including the use of an intuitive qualitative representation. The proposed solutions take into account static and dynamic context to predict individual interactions. They exploit an input- and a temporal-attention mechanism, and are tested on medium and long-term time horizons. The first two approaches integrate different relations from the so-called Qualitative Trajectory Calculus (QTC) within a state-of-the-art deep neural network to create a symbol-driven neural architecture for predicting spatial interactions. The third approach implements a purely data-driven network for motion prediction, the output of which is post-processed to predict QTC spatial interactions. Experimental resul
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65306;&#24341;&#29992;&#20154;&#31867;&#33310;&#36424;&#29983;&#25104;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#33310;&#36424;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35299;&#32806;&#25511;&#21046;&#26469;&#35299;&#20915;&#33310;&#36424;&#21512;&#25104;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#24544;&#23454;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#32452;&#21512;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00040</link><description>&lt;p&gt;
DisCo: &#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#22522;&#20110;&#20154;&#31867;&#33310;&#36424;&#30340;&#24341;&#29992;&#29983;&#25104;&#30340;&#35299;&#32806;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DisCo: Disentangled Control for Referring Human Dance Generation in Real World. (arXiv:2307.00040v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00040
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65306;&#24341;&#29992;&#20154;&#31867;&#33310;&#36424;&#29983;&#25104;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#33310;&#36424;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35299;&#32806;&#25511;&#21046;&#26469;&#35299;&#20915;&#33310;&#36424;&#21512;&#25104;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#24544;&#23454;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;/&#35270;&#39057;&#21512;&#25104;&#26041;&#38754;&#12290;&#23613;&#31649;&#26377;&#20102;&#36825;&#20123;&#36827;&#27493;&#65292;&#20294;&#22312;&#29983;&#25104;&#20154;&#31867;&#20013;&#24515;&#20869;&#23481;&#65288;&#22914;&#33310;&#36424;&#21512;&#25104;&#65289;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#33310;&#36424;&#21512;&#25104;&#26041;&#27861;&#22312;&#21512;&#25104;&#20869;&#23481;&#19982;&#29616;&#23454;&#19990;&#30028;&#33310;&#36424;&#22330;&#26223;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65306;&#24341;&#29992;&#20154;&#31867;&#33310;&#36424;&#29983;&#25104;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#19977;&#20010;&#37325;&#35201;&#23646;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#33310;&#36424;&#22330;&#26223;&#65306;&#65288;i&#65289;&#24544;&#23454;&#24615;&#65306;&#21512;&#25104;&#24212;&#35813;&#20445;&#30041;&#24341;&#29992;&#22270;&#20687;&#20013;&#20154;&#31867;&#20027;&#20307;&#21069;&#26223;&#21644;&#32972;&#26223;&#30340;&#22806;&#35266;&#65292;&#24182;&#31934;&#30830;&#22320;&#36981;&#24490;&#30446;&#26631;&#23039;&#21183;&#65307;&#65288;ii&#65289;&#27867;&#21270;&#33021;&#21147;&#65306;&#27169;&#22411;&#24212;&#35813;&#36866;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#20154;&#31867;&#20027;&#20307;&#12289;&#32972;&#26223;&#21644;&#23039;&#21183;&#65307;&#65288;iii&#65289;&#32452;&#21512;&#24615;&#65306;&#23427;&#24212;&#35813;&#20801;&#35768;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#24050;&#35265;/&#26410;&#35265;&#20027;&#20307;&#12289;&#32972;&#26223;&#21644;&#23039;&#21183;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;D
&lt;/p&gt;
&lt;p&gt;
Generative AI has made significant strides in computer vision, particularly in image/video synthesis conditioned on text descriptions. Despite the advancements, it remains challenging especially in the generation of human-centric content such as dance synthesis. Existing dance synthesis methods struggle with the gap between synthesized content and real-world dance scenarios. In this paper, we define a new problem setting: Referring Human Dance Generation, which focuses on real-world dance scenarios with three important properties: (i) Faithfulness: the synthesis should retain the appearance of both human subject foreground and background from the reference image, and precisely follow the target pose; (ii) Generalizability: the model should generalize to unseen human subjects, backgrounds, and poses; (iii) Compositionality: it should allow for composition of seen/unseen subjects, backgrounds, and poses from different sources. To address these challenges, we introduce a novel approach, D
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#23567;&#33041;&#20013;&#22522;&#20110;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#32452;&#32455;&#26041;&#24335;&#65292;&#21457;&#29616;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#38169;&#35823;&#35270;&#22270;&#30340;&#31070;&#32463;&#20803;&#32676;&#20307;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12289;&#20943;&#23569;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#25463;&#24452;&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00039</link><description>&lt;p&gt;
&#20197;&#33041;&#21551;&#21457;&#35774;&#35745;&#35299;&#20915;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#36275;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;
Towards Brain Inspired Design for Addressing the Shortcomings of ANNs. (arXiv:2307.00039v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#23567;&#33041;&#20013;&#22522;&#20110;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#32452;&#32455;&#26041;&#24335;&#65292;&#21457;&#29616;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#38169;&#35823;&#35270;&#22270;&#30340;&#31070;&#32463;&#20803;&#32676;&#20307;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12289;&#20943;&#23569;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#25463;&#24452;&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#33041;&#21151;&#33021;&#26426;&#21046;&#30340;&#29702;&#35299;&#25552;&#39640;&#65292;&#20174;&#31070;&#32463;&#31185;&#23398;&#20013;&#33719;&#24471;&#30340;&#27934;&#23519;&#21147;&#23545;&#20110;AI&#31639;&#27861;&#30340;&#21457;&#23637;&#20540;&#24471;&#36827;&#19968;&#27493;&#32771;&#34385;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;&#32467;&#26500;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19982;&#26368;&#36817;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;[27]&#36827;&#34892;&#23545;&#27604;&#65292;&#35748;&#20026;&#23567;&#33041;&#20013;&#22522;&#20110;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#32452;&#32455;&#26041;&#24335;&#21487;&#33021;&#26159;&#34892;&#20026;&#21644;&#23398;&#20064;&#30340;&#20960;&#20010;&#29702;&#24819;&#29305;&#24449;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#38543;&#21518;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#23398;&#20064;&#34892;&#20026;&#21644;&#29305;&#28857;&#65292;&#20197;&#35780;&#20272;&#31867;&#20284;&#26426;&#21046;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20010;&#24615;&#21270;&#38169;&#35823;&#35270;&#22270;&#30340;&#31070;&#32463;&#20803;&#32676;&#20307;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#20943;&#23569;&#21463;&#24847;&#22806;&#25463;&#24452;&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;&#23558;&#23398;&#20064;&#26041;&#24335;&#20174;&#31070;&#32463;&#31185;&#23398;&#20013;&#36716;&#21270;&#20026;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As our understanding of the mechanisms of brain function is enhanced, the value of insights gained from neuroscience to the development of AI algorithms deserves further consideration. Here, we draw parallels with an existing tree-based ANN architecture and a recent neuroscience study[27] arguing that the error-based organization of neurons in the cerebellum that share a preference for a personalized view of the entire error space, may account for several desirable features of behavior and learning. We then analyze the learning behavior and characteristics of the model under varying scenarios to gauge the potential benefits of a similar mechanism in ANN. Our empirical results suggest that having separate populations of neurons with personalized error views can enable efficient learning under class imbalance and limited data, and reduce the susceptibility to unintended shortcut strategies, leading to improved generalization. This work highlights the potential of translating the learning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#21307;&#30103;&#36164;&#28304;&#65288;&#30123;&#33495;&#65289;&#19981;&#30830;&#23450;&#24615;&#23548;&#21521;&#22320;&#20998;&#37197;&#32473;&#24322;&#36136;&#20154;&#32676;&#20197;&#31649;&#29702;&#27969;&#34892;&#30149;&#20256;&#25773;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#21442;&#25968;&#20272;&#35745;&#21644;&#25972;&#21512;&#12289;&#38750;&#32447;&#24615;ODE&#32422;&#26463;&#21644;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.00032</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#23548;&#21521;&#26368;&#20248;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Informed Optimal Resource Allocation with Gaussian Process based Bayesian Inference. (arXiv:2307.00032v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#21307;&#30103;&#36164;&#28304;&#65288;&#30123;&#33495;&#65289;&#19981;&#30830;&#23450;&#24615;&#23548;&#21521;&#22320;&#20998;&#37197;&#32473;&#24322;&#36136;&#20154;&#32676;&#20197;&#31649;&#29702;&#27969;&#34892;&#30149;&#20256;&#25773;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#21442;&#25968;&#20272;&#35745;&#21644;&#25972;&#21512;&#12289;&#38750;&#32447;&#24615;ODE&#32422;&#26463;&#21644;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#20110;&#23558;&#21307;&#30103;&#36164;&#28304;&#65288;&#30123;&#33495;&#65289;&#19981;&#30830;&#23450;&#24615;&#23548;&#21521;&#22320;&#20998;&#37197;&#32473;&#24322;&#36136;&#20154;&#32676;&#20197;&#31649;&#29702;&#27969;&#34892;&#30149;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#20004;&#20010;&#30456;&#20851;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23545;&#20110;&#19968;&#20010;&#27969;&#34892;&#30149;&#20256;&#25773;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#22914;&#20309;&#22312;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#20013;&#20272;&#35745;&#21644;&#25972;&#21512;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#65311;&#65288;2&#65289;&#23545;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65288;&#36164;&#28304;&#20998;&#37197;&#65289;&#22914;&#20309;&#35745;&#31639;&#22788;&#29702;&#38750;&#32447;&#24615;ODE&#32422;&#26463;&#21644;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#65311;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#24403;&#21069;&#30340;&#25991;&#29486;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#22312;&#26032;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#34920;&#36798;&#24335;&#20013;&#20934;&#30830;&#32780;&#21487;&#34892;&#22320;&#34920;&#31034;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#20272;&#35745;ODE&#27169;&#22411;&#21442;&#25968;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#29983;&#25104;&#21487;&#34892;&#30340;&#24773;&#26223;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24182;&#34892;&#21270;&#30340;&#35299;&#31639;&#31639;&#27861;&#65292;&#32771;&#34385;&#21040;&#24773;&#26223;&#21644;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of uncertainty informed allocation of medical resources (vaccines) to heterogeneous populations for managing epidemic spread. We tackle two related questions: (1) For a compartmental ordinary differential equation (ODE) model of epidemic spread, how can we estimate and integrate parameter uncertainty into resource allocation decisions? (2) How can we computationally handle both nonlinear ODE constraints and parameter uncertainties for a generic stochastic optimization problem for resource allocation? To the best of our knowledge current literature does not fully resolve these questions. Here, we develop a data-driven approach to represent parameter uncertainty accurately and tractably in a novel stochastic optimization problem formulation. We first generate a tractable scenario set by estimating the distribution on ODE model parameters using Bayesian inference with Gaussian processes. Next, we develop a parallelized solution algorithm that accounts for scenario-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#29942;&#39048;&#23398;&#20064;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#34920;&#31034;&#29305;&#24449;&#30340;&#35270;&#35273;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;ImageNet&#22270;&#20687;&#65292;&#21487;&#20197;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00028</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#29942;&#39048;&#23398;&#20064;&#20998;&#31867;&#30340;&#8220;&#30475;&#35265;&#25991;&#23383;&#8221;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Seeing in Words: Learning to Classify through Language Bottlenecks. (arXiv:2307.00028v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#29942;&#39048;&#23398;&#20064;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#34920;&#31034;&#29305;&#24449;&#30340;&#35270;&#35273;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;ImageNet&#22270;&#20687;&#65292;&#21487;&#20197;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#25552;&#21462;&#30340;&#29305;&#24449;&#24448;&#24448;&#26159;&#26080;&#27861;&#35299;&#37322;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21487;&#20197;&#29992;&#31616;&#27905;&#30452;&#35266;&#30340;&#25551;&#36848;&#26469;&#35299;&#37322;&#20182;&#20204;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#23558;&#21487;&#35299;&#37322;&#24615;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#23558;&#29305;&#24449;&#34920;&#31034;&#20026;&#25991;&#26412;&#30340;&#35270;&#35273;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#23545;ImageNet&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks for computer vision extract uninterpretable features despite achieving high accuracy on benchmarks. In contrast, humans can explain their predictions using succinct and intuitive descriptions. To incorporate explainability into neural networks, we train a vision model whose feature representations are text. We show that such a model can effectively classify ImageNet images, and we discuss the challenges we encountered when training it.
&lt;/p&gt;</description></item><item><title>CASEIN&#26159;&#19968;&#20010;&#23618;&#23618;&#32423;&#32852;&#30340;&#26174;&#24335;&#21644;&#38544;&#24335;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#20934;&#30830;&#35299;&#31163;&#21442;&#32771;&#35821;&#38899;&#20013;&#30340;&#24773;&#24863;&#27969;&#24418;&#65292;&#23398;&#20064;&#20302;&#35821;&#20041;&#32423;&#21035;&#30340;&#38544;&#24335;&#34920;&#31034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;CASEIN&#22312;&#21487;&#25511;&#24615;&#21644;&#33258;&#28982;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#22810;&#31181;&#24773;&#24863;&#24378;&#24230;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.00020</link><description>&lt;p&gt;
CASEIN: &#23618;&#23618;&#32423;&#32852;&#30340;&#26174;&#24335;&#21644;&#38544;&#24335;&#25511;&#21046;&#29992;&#20110;&#32454;&#31890;&#24230;&#24773;&#24863;&#24378;&#24230;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation. (arXiv:2307.00020v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00020
&lt;/p&gt;
&lt;p&gt;
CASEIN&#26159;&#19968;&#20010;&#23618;&#23618;&#32423;&#32852;&#30340;&#26174;&#24335;&#21644;&#38544;&#24335;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#20934;&#30830;&#35299;&#31163;&#21442;&#32771;&#35821;&#38899;&#20013;&#30340;&#24773;&#24863;&#27969;&#24418;&#65292;&#23398;&#20064;&#20302;&#35821;&#20041;&#32423;&#21035;&#30340;&#38544;&#24335;&#34920;&#31034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;CASEIN&#22312;&#21487;&#25511;&#24615;&#21644;&#33258;&#28982;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#22810;&#31181;&#24773;&#24863;&#24378;&#24230;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#24378;&#24230;&#35843;&#33410;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#39044;&#27979;&#30340;&#24773;&#24863;&#27010;&#29575;&#36827;&#34892;&#26174;&#24335;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39640;&#23618;&#35821;&#20041;&#27010;&#29575;&#22312;&#38899;&#32032;&#32423;&#21035;&#19978;&#24448;&#24448;&#19981;&#20934;&#30830;&#19988;&#19981;&#24179;&#28369;&#65292;&#23548;&#33268;&#23398;&#20064;&#20013;&#23384;&#22312;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#24403;&#25105;&#20204;&#23581;&#35797;&#22312;&#29305;&#23450;&#38899;&#32032;&#20013;&#28151;&#21512;&#22810;&#31181;&#24773;&#24863;&#24378;&#24230;&#26102;&#65292;&#20250;&#23548;&#33268;&#21512;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#33258;&#28982;&#24230;&#26126;&#26174;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;CASEIN&#8221;&#30340;&#23618;&#23618;&#32423;&#32852;&#30340;&#26174;&#24335;&#21644;&#38544;&#24335;&#25511;&#21046;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20934;&#30830;&#35299;&#31163;&#21442;&#32771;&#35821;&#38899;&#20013;&#24773;&#24863;&#27969;&#24418;&#65292;&#23398;&#20064;&#20302;&#35821;&#20041;&#32423;&#21035;&#30340;&#38544;&#24335;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#26725;&#25509;&#20102;&#26174;&#24335;&#27010;&#29575;&#21644;&#21512;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#20943;&#23569;&#20102;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;CASEIN&#22312;&#21487;&#25511;&#24615;&#21644;&#33258;&#28982;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#22810;&#31181;&#24773;&#24863;&#24378;&#24230;&#28151;&#21512;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing fine-grained intensity regulation methods rely on explicit control through predicted emotion probabilities. However, these high-level semantic probabilities are often inaccurate and unsmooth at the phoneme level, leading to bias in learning. Especially when we attempt to mix multiple emotion intensities for specific phonemes, resulting in markedly reduced controllability and naturalness of the synthesis. To address this issue, we propose the CAScaded Explicit and Implicit coNtrol framework (CASEIN), which leverages accurate disentanglement of emotion manifolds from the reference speech to learn the implicit representation at a lower semantic level. This representation bridges the semantical gap between explicit probabilities and the synthesis model, reducing bias in learning. In experiments, our CASEIN surpasses existing methods in both controllability and naturalness. Notably, we are the first to achieve fine-grained control over the mixed intensity of multiple emotions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#23548;&#33322;&#39046;&#22495;&#20013;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#19981;&#21516;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#30340;&#30740;&#31350;&#12289;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#20197;&#21450;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#21644;&#21435;&#22122;&#26041;&#27861;&#12290;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;</title><link>http://arxiv.org/abs/2307.00014</link><description>&lt;p&gt;
&#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions. (arXiv:2307.00014v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#23548;&#33322;&#39046;&#22495;&#20013;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#19981;&#21516;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#30340;&#30740;&#31350;&#12289;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#20197;&#21450;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#21644;&#21435;&#22122;&#26041;&#27861;&#12290;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24815;&#24615;&#20256;&#24863;&#22312;&#35768;&#22810;&#24212;&#29992;&#21644;&#24179;&#21488;&#20013;&#34987;&#20351;&#29992;&#65292;&#20174;&#26234;&#33021;&#25163;&#26426;&#31561;&#26085;&#24120;&#35774;&#22791;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31561;&#22797;&#26434;&#35774;&#22791;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24815;&#24615;&#20256;&#24863;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#36825;&#26159;&#30001;&#20110;&#39640;&#25928;&#30340;&#35745;&#31639;&#30828;&#20214;&#30340;&#21457;&#23637;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21487;&#33719;&#24471;&#24615;&#12290;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#34987;&#29992;&#20110;&#24378;&#21270;&#22522;&#20110;&#27169;&#22411;&#30340;&#23548;&#33322;&#21644;&#20256;&#24863;&#22120;&#34701;&#21512;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20837;&#32508;&#36848;&#12290;&#25105;&#20204;&#20998;&#21035;&#32771;&#23519;&#20102;&#27599;&#20010;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#65292;&#21253;&#25324;&#38470;&#22320;&#12289;&#31354;&#20013;&#21644;&#28023;&#27915;&#12290;&#27599;&#20010;&#39046;&#22495;&#20998;&#20026;&#32431;&#24815;&#24615;&#36827;&#23637;&#21644;&#22522;&#20110;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#29992;&#20110;&#26657;&#20934;&#21644;&#21435;&#22122;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#25972;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24120;&#29992;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inertial sensing is used in many applications and platforms, ranging from day-to-day devices such as smartphones to very complex ones such as autonomous vehicles. In recent years, the development of machine learning and deep learning techniques has increased significantly in the field of inertial sensing. This is due to the development of efficient computing hardware and the accessibility of publicly available sensor data. These data-driven approaches are used to empower model-based navigation and sensor fusion algorithms. This paper provides an in-depth review of those deep learning methods. We examine separately, each vehicle operation domain including land, air, and sea. Each domain is divided into pure inertial advances and improvements based on filter parameters learning. In addition, we review deep learning approaches for calibrating and denoising inertial sensors. Throughout the paper, we discuss these trends and future directions. We also provide statistics on the commonly used
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;</title><link>http://arxiv.org/abs/2307.00012</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#20462;&#22797;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26131;&#20986;&#38169;&#27979;&#35797;&#20250;&#22312;&#30456;&#21516;&#36719;&#20214;&#29256;&#26412;&#30340;&#27979;&#35797;&#19979;&#38750;&#30830;&#23450;&#24615;&#22320;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24341;&#36215;&#28151;&#20081;&#24182;&#28010;&#36153;&#24320;&#21457;&#32773;&#26102;&#38388;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#39044;&#27979;&#26131;&#20986;&#38169;&#24615;&#21450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#20294;&#22312;&#25552;&#20379;&#20462;&#22797;&#25903;&#25345;&#26041;&#38754;&#20173;&#26377;&#36739;&#23569;&#24037;&#20316;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;13&#20010;&#20462;&#22797;&#31867;&#21035;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#34429;&#28982;&#22312;&#24403;&#21069;&#38454;&#27573;&#20934;&#30830;&#39044;&#27979;&#20462;&#22797;&#26412;&#36523;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#20294;&#36825;&#20123;&#31867;&#21035;&#25552;&#20379;&#20102;&#20851;&#20110;&#38656;&#35201;&#26816;&#26597;&#30340;&#27979;&#35797;&#20195;&#30721;&#37096;&#20998;&#30340;&#31934;&#30830;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;CodeBERT&#21644;UniXcoder&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#25110;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;Few Shot Learning&#65288;FSL&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniXcoder&#22312;&#27491;&#30830;&#39044;&#27979;&#22823;&#22810;&#25968;&#20462;&#22797;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;&#25513;&#30721;&#30340;&#25968;&#25454;&#29983;&#25104;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00008</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#25513;&#30721;&#30340;&#25968;&#25454;&#29983;&#25104;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating Masking-based Data Generation in Language Models. (arXiv:2307.00008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;&#25513;&#30721;&#30340;&#25968;&#25454;&#29983;&#25104;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;BERT&#38382;&#19990;&#20197;&#26469;&#65292;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26102;&#20195;&#24050;&#32463;&#34987;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#25152;&#23450;&#20041;&#12290;BERT&#21644;&#31867;&#20284;&#32467;&#26500;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#29305;&#28857;&#26159;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30340;&#30446;&#26631;&#65292;&#20854;&#20013;&#37096;&#20998;&#36755;&#20837;&#34987;&#26377;&#24847;&#22320;&#25513;&#30422;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20197;&#39044;&#27979;&#36825;&#37096;&#20998;&#34987;&#25513;&#30721;&#30340;&#20449;&#24687;&#12290;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#26415;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#30740;&#31350;&#39046;&#22495;&#65292;&#36890;&#36807;&#25351;&#23450;&#30340;&#25216;&#26415;&#20154;&#24037;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#26159;BERT&#30340;&#19968;&#20010;&#37325;&#35201;&#35757;&#32451;&#29305;&#28857;&#65292;&#23427;&#20026;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20154;&#24037;&#22686;&#24378;&#25968;&#25454;&#29992;&#20110;NLP&#19979;&#28216;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25513;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current era of natural language processing (NLP) has been defined by the prominence of pre-trained language models since the advent of BERT. A feature of BERT and models with similar architecture is the objective of masked language modeling, in which part of the input is intentionally masked and the model is trained to predict this piece of masked information. Data augmentation is a data-driven technique widely used in machine learning, including research areas like computer vision and natural language processing, to improve model performance by artificially augmenting the training data set by designated techniques. Masked language models (MLM), an essential training feature of BERT, have introduced a novel approach to perform effective pre-training on Transformer based models in natural language processing tasks. Recent studies have utilized masked language model to generate artificially augmented data for NLP downstream tasks. The experimental results show that Mask based data au
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24179;&#28369;&#21608;&#26399;&#39640;&#26031;Copula&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#20809;&#20239;&#31995;&#32479;&#32676;&#21457;&#30005;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#26172;&#22812;&#21464;&#21270;&#12289;&#31995;&#32479;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#38543;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12289;&#32570;&#22833;&#25968;&#25454;&#22635;&#34917;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.00004</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#28369;&#21608;&#26399;&#39640;&#26031;Copula&#27169;&#22411;&#24314;&#27169;&#20809;&#20239;&#30005;&#31449;&#32676;
&lt;/p&gt;
&lt;p&gt;
PV Fleet Modeling via Smooth Periodic Gaussian Copula. (arXiv:2307.00004v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24179;&#28369;&#21608;&#26399;&#39640;&#26031;Copula&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#20809;&#20239;&#31995;&#32479;&#32676;&#21457;&#30005;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#26172;&#22812;&#21464;&#21270;&#12289;&#31995;&#32479;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#38543;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12289;&#32570;&#22833;&#25968;&#25454;&#22635;&#34917;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#24314;&#27169;&#20809;&#20239;&#65288;PV&#65289;&#31995;&#32479;&#32676;&#30340;&#21457;&#30005;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30333;&#30418;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#20989;&#25968;&#65292;&#21487;&#20197;&#23558;&#30690;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26144;&#23556;&#20026;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26631;&#20934;&#27491;&#24577;&#21464;&#37327;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25311;&#21512;&#24179;&#28369;&#21608;&#26399;Copula&#21464;&#25442;&#21040;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#35768;&#22810;&#26041;&#38754;&#65292;&#22914;&#21151;&#29575;&#36755;&#20986;&#20998;&#24067;&#30340;&#26172;&#22812;&#21464;&#21270;&#12289;&#19981;&#21516;PV&#31995;&#32479;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#30001;&#21487;&#35299;&#37322;&#30340;&#27493;&#39588;&#32452;&#25104;&#65292;&#24182;&#19988;&#21487;&#25193;&#23637;&#21040;&#35768;&#22810;&#31995;&#32479;&#12290;&#36890;&#36807;&#31995;&#32479;&#21644;&#26102;&#38388;&#30340;&#20809;&#20239;&#30005;&#31449;&#32676;&#32852;&#21512;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#29992;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12289;&#22635;&#34917;&#32570;&#22833;&#25968;&#25454;&#12289;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#21644;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#35813;&#26041;&#27861;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for jointly modeling power generation from a fleet of photovoltaic (PV) systems. We propose a white-box method that finds a function that invertibly maps vector time-series data to independent and identically distributed standard normal variables. The proposed method, based on a novel approach for fitting a smooth, periodic copula transform to data, captures many aspects of the data such as diurnal variation in the distribution of power output, dependencies among different PV systems, and dependencies across time. It consists of interpretable steps and is scalable to many systems. The resulting joint probability model of PV fleet output across systems and time can be used to generate synthetic data, impute missing data, perform anomaly detection, and make forecasts. In this paper, we explain the method and demonstrate these applications.
&lt;/p&gt;</description></item><item><title>Sphere2Vec&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17624</link><description>&lt;p&gt;
Sphere2Vec&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#29699;&#38754;&#19978;&#36890;&#29992;&#20301;&#32622;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions. (arXiv:2306.17624v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17624
&lt;/p&gt;
&lt;p&gt;
Sphere2Vec&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20026;&#31354;&#38388;&#20013;&#30340;&#28857;&#29983;&#25104;&#36866;&#21512;&#23398;&#20064;&#30340;&#34920;&#31034;&#26159;&#19968;&#20010;&#22522;&#26412;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#32534;&#30721;&#26041;&#26696;&#65288;&#22914;Space2Vec&#21644;NeRF&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#23558;&#20108;&#32500;/&#19977;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#28857;&#32534;&#30721;&#20026;&#39640;&#32500;&#21521;&#37327;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25152;&#26377;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#20301;&#32622;&#32534;&#30721;&#22120;&#37117;&#26159;&#35774;&#35745;&#29992;&#26469;&#27169;&#25311;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#22312;&#24212;&#29992;&#20110;&#38656;&#35201;&#22312;&#29699;&#38754;&#19978;&#36827;&#34892;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#37117;&#20250;&#20986;&#29616;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#22320;&#22270;&#25237;&#24433;&#22833;&#30495;&#38382;&#39064;&#65288;2D&#65289;&#21644;&#29699;&#38754;&#21040;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#36817;&#20284;&#35823;&#24046;&#65288;3D&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Sphere2Vec&#30340;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#12290;&#25105;&#20204;&#22312;&#29699;&#38754;&#19978;&#30340;&#20301;&#32622;&#32534;&#30721;&#30340;&#36317;&#31163;&#20445;&#25345;&#32534;&#30721;&#30340;&#32479;&#19968;&#35270;&#35282;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating learning-friendly representations for points in space is a fundamental and long-standing problem in ML. Recently, multi-scale encoding schemes (such as Space2Vec and NeRF) were proposed to directly encode any point in 2D/3D Euclidean space as a high-dimensional vector, and has been successfully applied to various geospatial prediction and generative tasks. However, all current 2D and 3D location encoders are designed to model point distances in Euclidean space. So when applied to large-scale real-world GPS coordinate datasets, which require distance metric learning on the spherical surface, both types of models can fail due to the map projection distortion problem (2D) and the spherical-to-Euclidean distance approximation error (3D). To solve these problems, we propose a multi-scale location encoder called Sphere2Vec which can preserve spherical distances when encoding point coordinates on a spherical surface. We developed a unified view of distance-reserving encoding on sph
&lt;/p&gt;</description></item><item><title>LMBot&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.17408</link><description>&lt;p&gt;
LMBot: &#23558;&#22270;&#24418;&#30693;&#35782;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#26080;&#22270;&#24418;&#37096;&#32626;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection. (arXiv:2306.17408v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17408
&lt;/p&gt;
&lt;p&gt;
LMBot&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#34892;&#20026;&#32773;&#20351;&#29992;&#36234;&#26469;&#36234;&#20808;&#36827;&#21644;&#24191;&#27867;&#30340;&#26426;&#22120;&#20154;&#26469;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#25805;&#32437;&#33286;&#35770;&#65292;&#25512;&#29305;&#26426;&#22120;&#20154;&#30340;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#25512;&#29702;&#20381;&#36182;&#20110;&#36317;&#31163;&#30446;&#26631;&#29992;&#25143;&#22810;&#36339;&#30340;&#37051;&#23621;&#29992;&#25143;&#65292;&#24182;&#19988;&#33719;&#21462;&#37051;&#23621;&#29992;&#25143;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#21487;&#33021;&#24341;&#20837;&#20559;&#24046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#19978;&#24494;&#35843;&#21518;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31454;&#20105;&#24615;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#22270;&#24418;&#32467;&#26500;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;LMBot&#65292;&#23427;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#30693;&#35782;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;(LMs)&#65292;&#20197;&#22312;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#36827;&#34892;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;LMBot&#23545;&#22522;&#20110;&#22270;&#24418;&#21644;&#19981;&#20351;&#29992;&#22270;&#24418;&#30340;&#25968;&#25454;&#38598;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#19968;&#27573;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
As malicious actors employ increasingly advanced and widespread bots to disseminate misinformation and manipulate public opinion, the detection of Twitter bots has become a crucial task. Though graph-based Twitter bot detection methods achieve state-of-the-art performance, we find that their inference depends on the neighbor users multi-hop away from the targets, and fetching neighbors is time-consuming and may introduce bias. At the same time, we find that after finetuning on Twitter bot detection, pretrained language models achieve competitive performance and do not require a graph structure during deployment. Inspired by this finding, we propose a novel bot detection framework LMBot that distills the knowledge of graph neural networks (GNNs) into language models (LMs) for graph-less deployment in Twitter bot detection to combat the challenge of data dependency. Moreover, LMBot is compatible with graph-based and graph-less datasets. Specifically, we first represent each user as a tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#33014;&#22218;&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#21644;&#27874;&#26031;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#29420;&#30340;&#33014;&#22218;&#32593;&#32476;&#24182;&#20351;&#29992;&#21152;&#26435;&#24230;&#37327;&#26469;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17068</link><description>&lt;p&gt;
&#22522;&#20110;&#21152;&#26435;CapsuleNet&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#21644;&#27874;&#26031;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Presenting an approach based on weighted CapsuleNet networks for Arabic and Persian multi-domain sentiment analysis. (arXiv:2306.17068v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#33014;&#22218;&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#21644;&#27874;&#26031;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#29420;&#30340;&#33014;&#22218;&#32593;&#32476;&#24182;&#20351;&#29992;&#21152;&#26435;&#24230;&#37327;&#26469;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#23545;&#33258;&#30001;&#25991;&#26412;&#36827;&#34892;&#27491;&#38754;&#12289;&#36127;&#38754;&#25110;&#20013;&#24615;&#30340;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#65292;&#20998;&#31867;&#22120;&#22312;&#19968;&#20010;&#39046;&#22495;&#20013;&#21487;&#33021;&#20855;&#26377;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#20013;&#30001;&#20110;&#35789;&#35821;&#30340;&#35821;&#20041;&#22810;&#37325;&#24615;&#32780;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27874;&#26031;&#35821;/&#38463;&#25289;&#20271;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#32047;&#31215;&#21152;&#26435;&#33014;&#22218;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#21152;&#26435;&#33014;&#22218;&#38598;&#21512;&#30001;&#20026;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#30340;&#21333;&#29420;&#30340;&#33014;&#22218;&#32593;&#32476;&#21644;&#31216;&#20026;&#39046;&#22495;&#25152;&#23646;&#24230;&#65288;DBD&#65289;&#30340;&#21152;&#26435;&#24230;&#37327;&#32452;&#25104;&#12290;&#36825;&#20010;&#24230;&#37327;&#30001;TF&#21644;IDF&#32452;&#25104;&#65292;&#35745;&#31639;&#27599;&#20010;&#25991;&#26723;&#23545;&#20110;&#27599;&#20010;&#39046;&#22495;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#28982;&#21518;&#20056;&#20197;&#27599;&#20010;&#33014;&#22218;&#21019;&#24314;&#30340;&#21487;&#33021;&#36755;&#20986;&#12290;&#26368;&#32456;&#65292;&#36825;&#20123;&#20056;&#31215;&#30340;&#24635;&#21644;&#26159;&#26368;&#32456;&#36755;&#20986;&#30340;&#26631;&#31614;&#65292;&#24182;&#29992;&#20110;&#30830;&#23450;&#26497;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment classification is a fundamental task in natural language processing, assigning one of the three classes, positive, negative, or neutral, to free texts. However, sentiment classification models are highly domain dependent; the classifier may perform classification with reasonable accuracy in one domain but not in another due to the Semantic multiplicity of words getting poor accuracy. This article presents a new Persian/Arabic multi-domain sentiment analysis method using the cumulative weighted capsule networks approach. Weighted capsule ensemble consists of training separate capsule networks for each domain and a weighting measure called domain belonging degree (DBD). This criterion consists of TF and IDF, which calculates the dependency of each document for each domain separately; this value is multiplied by the possible output that each capsule creates. In the end, the sum of these multiplications is the title of the final output, and is used to determine the polarity. And 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#25913;&#36827;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.16817</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#38598;&#25104;&#25913;&#36827;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Online Continual Learning Performance and Stability with Temporal Ensembles. (arXiv:2306.16817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16817
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#25913;&#36827;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#36845;&#20195;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#22312;&#38750;&#24179;&#31283;&#30340;&#25968;&#25454;&#27969;&#21644;&#22312;&#32447;&#26041;&#24335;&#19979;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#19979;&#38477;&#65306;(1)&#22312;&#32447;&#35774;&#32622;&#38480;&#21046;&#20102;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;(2)&#30001;&#20110;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#20960;&#31687;&#26368;&#36817;&#30340;&#25991;&#31456;&#34920;&#26126;&#36830;&#32493;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#37325;&#25918;&#26041;&#27861;&#22312;&#27169;&#22411;&#25345;&#32493;&#35780;&#20272;&#26102;&#23384;&#22312;&#31283;&#23450;&#24615;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#38598;&#25104;&#20316;&#20026;&#25913;&#36827;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#31616;&#21333;&#22320;&#38598;&#25104;&#26469;&#33258;&#21508;&#31181;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#24182;&#20174;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#21033;&#29992;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are very effective when trained on large datasets for a large number of iterations. However, when they are trained on non-stationary streams of data and in an online fashion, their performance is reduced (1) by the online setup, which limits the availability of data, (2) due to catastrophic forgetting because of the non-stationary nature of the data. Furthermore, several recent works (Caccia et al., 2022; Lange et al., 2023) arXiv:2205.1345(2) showed that replay methods used in continual learning suffer from the stability gap, encountered when evaluating the model continually (rather than only on task boundaries). In this article, we study the effect of model ensembling as a way to improve performance and stability in online continual learning. We notice that naively ensembling models coming from a variety of training tasks increases the performance in online continual learning considerably. Starting from this observation, and drawing inspirations from semi-supervised l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;&#65288;GraMMaR&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23039;&#21183;&#21644;&#20851;&#33410;&#19982;&#22320;&#38754;&#20043;&#38388;&#30340;&#20114;&#21160;&#30340;&#36807;&#28193;&#20998;&#24067;&#65292;&#26126;&#30830;&#20419;&#36827;&#36816;&#21160;&#21644;&#19982;&#22320;&#38754;&#36317;&#31163;&#21464;&#21270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16736</link><description>&lt;p&gt;
GraMMaR: &#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraMMaR: Ground-aware Motion Model for 3D Human Motion Reconstruction. (arXiv:2306.16736v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16736
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;&#65288;GraMMaR&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23039;&#21183;&#21644;&#20851;&#33410;&#19982;&#22320;&#38754;&#20043;&#38388;&#30340;&#20114;&#21160;&#30340;&#36807;&#28193;&#20998;&#24067;&#65292;&#26126;&#30830;&#20419;&#36827;&#36816;&#21160;&#21644;&#19982;&#22320;&#38754;&#36317;&#31163;&#21464;&#21270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20934;&#30830;&#21644;&#30495;&#23454;&#30340;&#20174;RGB&#35270;&#39057;&#20013;&#37325;&#24314;3D&#20154;&#20307;&#36816;&#21160;&#65292;&#35299;&#23494;&#22797;&#26434;&#30340;&#20154;&#22320;&#20114;&#21160;&#23545;&#20110;&#20445;&#35777;&#20154;&#31867;&#21644;&#22320;&#38754;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#35201;&#20040;&#38544;&#24335;&#22320;&#27169;&#25311;&#20154;&#22320;&#20114;&#21160;&#65292;&#35201;&#20040;&#20197;&#31232;&#30095;&#30340;&#26041;&#24335;&#27169;&#25311;&#65292;&#24448;&#24448;&#22312;&#38754;&#23545;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#26102;&#23548;&#33268;&#19981;&#30495;&#23454;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#19968;&#31181;&#23494;&#38598;&#21644;&#36830;&#32493;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#36825;&#20123;&#20114;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;&#65292;&#31216;&#20026;GraMMaR&#65292;&#23427;&#22312;&#36816;&#21160;&#24207;&#21015;&#20013;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21516;&#26102;&#23398;&#20064;&#23039;&#21183;&#21644;&#27599;&#20010;&#20851;&#33410;&#19982;&#22320;&#38754;&#20043;&#38388;&#30340;&#20114;&#21160;&#30340;&#36807;&#28193;&#20998;&#24067;&#12290;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;&#26126;&#30830;&#20419;&#36827;&#36816;&#21160;&#21644;&#19982;&#22320;&#38754;&#36317;&#31163;&#21464;&#21270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#32852;&#21512;&#20248;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;GraMMaR&#20316;&#20026;&#21452;&#37325;&#20808;&#39564;&#65292;&#35268;&#33539;&#20248;&#21270;&#36807;&#31243;&#26397;&#30528;
&lt;/p&gt;
&lt;p&gt;
Demystifying complex human-ground interactions is essential for accurate and realistic 3D human motion reconstruction from RGB videos, as it ensures consistency between the humans and the ground plane. Prior methods have modeled human-ground interactions either implicitly or in a sparse manner, often resulting in unrealistic and incorrect motions when faced with noise and uncertainty. In contrast, our approach explicitly represents these interactions in a dense and continuous manner. To this end, we propose a novel Ground-aware Motion Model for 3D Human Motion Reconstruction, named GraMMaR, which jointly learns the distribution of transitions in both pose and interaction between every joint and ground plane at each time step of a motion sequence. It is trained to explicitly promote consistency between the motion and distance change towards the ground. After training, we establish a joint optimization strategy that utilizes GraMMaR as a dual-prior, regularizing the optimization towards 
&lt;/p&gt;</description></item><item><title>NNQS-Transformer&#26159;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#12290;&#20854;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;Transformer&#30340;&#37327;&#23376;&#27874;&#20989;&#25968;&#23433;&#33832;&#33576;&#12289;&#25968;&#25454;&#20013;&#24515;&#24182;&#34892;&#21270;&#26041;&#26696;&#12289;&#24182;&#34892;&#25209;&#37327;&#37319;&#26679;&#31574;&#30053;&#21644;&#24182;&#34892;&#23616;&#22495;&#33021;&#37327;&#35780;&#20272;&#26041;&#26696;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#31934;&#24230;&#21644;&#23545;&#20110;&#22823;&#20998;&#23376;&#31995;&#32479;&#30340;&#24378;&#21487;&#25193;&#23637;&#24615;&#21644;&#24369;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16705</link><description>&lt;p&gt;
NNQS-Transformer: &#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#26041;&#27861;&#29992;&#20110;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;
&lt;/p&gt;
&lt;p&gt;
NNQS-Transformer: an Efficient and Scalable Neural Network Quantum States Approach for Ab initio Quantum Chemistry. (arXiv:2306.16705v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16705
&lt;/p&gt;
&lt;p&gt;
NNQS-Transformer&#26159;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#12290;&#20854;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;Transformer&#30340;&#37327;&#23376;&#27874;&#20989;&#25968;&#23433;&#33832;&#33576;&#12289;&#25968;&#25454;&#20013;&#24515;&#24182;&#34892;&#21270;&#26041;&#26696;&#12289;&#24182;&#34892;&#25209;&#37327;&#37319;&#26679;&#31574;&#30053;&#21644;&#24182;&#34892;&#23616;&#22495;&#33021;&#37327;&#35780;&#20272;&#26041;&#26696;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#31934;&#24230;&#21644;&#23545;&#20110;&#22823;&#20998;&#23376;&#31995;&#32479;&#30340;&#24378;&#21487;&#25193;&#23637;&#24615;&#21644;&#24369;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#65288;NNQS&#65289;&#24050;&#25104;&#20026;&#37327;&#23376;&#22810;&#20307;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#20294;&#20854;&#23454;&#38469;&#24212;&#29992;&#24120;&#21463;&#21040;&#37319;&#26679;&#21644;&#23616;&#22495;&#33021;&#37327;&#35745;&#31639;&#30340;&#39640;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;NNQS&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22836;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23558;Transformer&#20316;&#20026;&#37327;&#23376;&#27874;&#20989;&#25968;&#30340;&#23433;&#33832;&#33576;&#65307;&#65288;2&#65289;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#24182;&#34892;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#65288;VMC&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#25345;&#25968;&#25454;&#30340;&#23616;&#37096;&#24615;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#35745;&#31639;&#26550;&#26500;&#65307;&#65288;3&#65289;&#19968;&#31181;&#24182;&#34892;&#25209;&#37327;&#37319;&#26679;&#31574;&#30053;&#65292;&#38477;&#20302;&#37319;&#26679;&#25104;&#26412;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#36127;&#36733;&#24179;&#34913;&#65307;&#65288;4&#65289;&#19968;&#31181;&#26082;&#20855;&#26377;&#20869;&#23384;&#21448;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#30340;&#24182;&#34892;&#23616;&#22495;&#33021;&#37327;&#35780;&#20272;&#26041;&#26696;&#65307;&#65288;5&#65289;&#23545;&#30495;&#23454;&#21270;&#23398;&#31995;&#32479;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#39640;&#36798;120&#20010;&#33258;&#26059;&#30340;&#22823;&#20998;&#23376;&#31995;&#32479;&#20855;&#26377;&#24456;&#24378;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24369;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network quantum state (NNQS) has emerged as a promising candidate for quantum many-body problems, but its practical applications are often hindered by the high cost of sampling and local energy calculation. We develop a high-performance NNQS method for \textit{ab initio} electronic structure calculations. The major innovations include: (1) A transformer based architecture as the quantum wave function ansatz; (2) A data-centric parallelization scheme for the variational Monte Carlo (VMC) algorithm which preserves data locality and well adapts for different computing architectures; (3) A parallel batch sampling strategy which reduces the sampling cost and achieves good load balance; (4) A parallel local energy evaluation scheme which is both memory and computationally efficient; (5) Study of real chemical systems demonstrates both the superior accuracy of our method compared to state-of-the-art and the strong and weak scalability for large molecular systems with up to $120$ spin o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15969</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#26377;&#24076;&#26395;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;PDE&#27714;&#35299;&#22120;&#65292;&#22312;&#21508;&#31181;PDE&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;PINNs&#26469;&#35299;&#20915;&#22810;&#32500;PDE&#21644;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#35299;&#20989;&#25968;&#23384;&#22312;&#26681;&#26412;&#38480;&#21046;&#12290;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PDE&#19978;&#25152;&#38656;&#30340;&#35757;&#32451;&#28857;&#25968;&#37327;(&#37197;&#28857;)&#22823;&#22823;&#22686;&#21152;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24222;&#22823;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20854;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PINNs&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20998;&#31163;&#30340;PINN (SPINN)&#65292;&#22312;&#22810;&#32500;PDE&#20013;&#25353;&#36724;&#36880;&#20010;&#22788;&#29702;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#20256;&#25773;&#30340;&#25968;&#37327;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;PINNs&#20013;&#30340;&#36880;&#28857;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#26469;&#38477;&#20302;&#35745;&#31639;PDE&#27531;&#24046;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#22312;&#21333;&#20010;&#26222;&#36890;GPU&#19978;&#21487;&#20197;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;(&gt;10^7)&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (&gt;10^7) on a single commodity GPU. The
&lt;/p&gt;</description></item><item><title>SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15656</link><description>&lt;p&gt;
SparseOptimizer: &#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#26469;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#32534;&#35793;&#22120;&#20849;&#21516;&#35774;&#35745;&#26469;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15656
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SparseOptimizer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;ALBERT&#21644;GPT&#65289;&#20013;&#33258;&#28982;&#22320;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;SparseOptimizer&#35774;&#35745;&#30340;&#20851;&#38190;&#26159;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30452;&#25509;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#25805;&#20316;&#31526;&#36890;&#36807;&#22362;&#23454;&#30340;&#29702;&#35770;&#26694;&#26550;&#25903;&#25345;&#65292;&#24182;&#21253;&#21547;&#20102;&#19968;&#20010;&#20998;&#26512;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20248;&#21270;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;SparseOptimizer&#30340;&#21363;&#25554;&#21363;&#29992;&#21151;&#33021;&#28040;&#38500;&#20102;&#23545;&#20195;&#30721;&#20462;&#25913;&#30340;&#38656;&#27714;&#65292;&#20351;&#20854;&#25104;&#20026;&#36866;&#29992;&#20110;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#36866;&#24212;&#24037;&#20855;&#12290;&#22312;GLUE&#12289;RACE&#12289;SQuAD1&#21644;SQuAD2&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;SparseOptimizer&#31232;&#30095;&#21270;&#21518;&#30340;SparseBERT&#21644;SparseALBERT&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#22411;&#30340;BERT&#21644;ALBERT&#30456;&#24403;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15156</link><description>&lt;p&gt;
&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#27169;&#20223;&#23398;&#20064;&#20551;&#35774;&#33021;&#22815;&#33719;&#24471;&#23637;&#31034;&#32773;&#30340;&#21160;&#20316;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#36825;&#20123;&#21160;&#20316;&#36890;&#24120;&#26080;&#27861;&#35266;&#27979;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#34892;&#20026;&#21487;&#33021;&#20559;&#31163;&#26631;&#20934;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;nMDP&#65289;&#20013;&#20165;&#29366;&#24577;&#24207;&#21015;&#30340;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#20854;&#20013;&#31574;&#30053;&#26159;&#28508;&#22312;&#29366;&#24577;&#36716;&#31227;&#29983;&#25104;&#22120;&#30340;&#33021;&#37327;&#20808;&#39564;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20808;&#39564;&#36827;&#34892;&#30701;&#26399;MCMC&#37319;&#26679;&#21644;&#23545;&#21518;&#39564;&#36827;&#34892;&#37325;&#35201;&#24615;&#37319;&#26679;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#21363;&#26080;&#27169;&#22411;&#31574;&#30053;&#25191;&#34892;&#31561;&#20215;&#20110;&#20808;&#39564;&#37319;&#26679;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21017;&#26159;&#20174;&#31574;&#30053;&#21021;&#22987;&#21270;&#30340;&#21518;&#39564;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20855;&#26377;&#38750;&#39532;&#23572;&#31185;&#22827;&#29305;&#24449;&#30340;&#21407;&#22411;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
&lt;/p&gt;</description></item><item><title>G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11667</link><description>&lt;p&gt;
G-NM&#65306;&#19968;&#32452;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11667
&lt;/p&gt;
&lt;p&gt;
G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#19968;&#20010;&#32508;&#21512;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#65292;&#32479;&#31216;&#20026;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#32452;&#65288;G-NM&#65289;&#12290;&#35813;&#38598;&#21512;&#21253;&#25324;&#20256;&#32479;&#27169;&#22411;&#22914;&#33258;&#22238;&#24402;&#32508;&#21512;&#31227;&#21160;&#24179;&#22343;&#65288;ARIMA&#65289;&#12289;Holt-Winters&#26041;&#27861;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#65292;&#20197;&#21450;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;G-NM&#26126;&#30830;&#26500;&#24314;&#20197;&#22686;&#24378;&#25105;&#20204;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#22266;&#26377;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#19982;&#36825;&#20123;&#20107;&#20214;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;G-NM&#20415;&#20110;&#23545;&#27492;&#31867;&#29616;&#35937;&#22312;&#24310;&#38271;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25512;&#36827;&#25105;&#20204;&#23545;&#27492;&#31867;&#20107;&#20214;&#30340;&#29702;&#35299;&#65292;&#24182;&#22823;&#24133;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;G-NM&#21253;&#25324;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#23395;&#33410;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10946</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#30456;&#23545;&#25104;&#29087;&#38454;&#27573;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#33616;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#20363;&#22914;&#22312;&#26053;&#28216;&#39046;&#22495;&#65292;&#36873;&#25321;&#36866;&#21512;&#30340;&#26053;&#28216;&#26223;&#28857;&#23646;&#24615;&#27969;&#31243;&#20316;&#20026;&#25512;&#33616;&#22522;&#30784;&#36739;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(Att-KGCN)&#65292;&#33258;&#21160;&#35821;&#20041;&#22320;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#23558;&#30456;&#23545;&#30456;&#20284;&#30340;&#20301;&#32622;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#26053;&#23458;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#32034;&#31185;&#29305;&#25289;&#23707;-&#20063;&#38376;&#30340;&#26053;&#28216;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#26053;&#28216;&#39046;&#22495;&#30340;&#26223;&#28857;&#25512;&#33616;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#24247;&#22797;&#35757;&#32451;&#30340;&#31649;&#29702;&#12290;&#24739;&#32773;&#21487;&#20197;&#36890;&#36807;&#31995;&#32479;&#36827;&#34892;&#24212;&#29992;&#31243;&#24207;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27835;&#30103;&#24072;&#21487;&#20197;&#36890;&#36807;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#36827;&#24230;&#30340;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.10756</link><description>&lt;p&gt;
&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A HRNet-based Rehabilitation Monitoring System. (arXiv:2306.10756v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#24247;&#22797;&#35757;&#32451;&#30340;&#31649;&#29702;&#12290;&#24739;&#32773;&#21487;&#20197;&#36890;&#36807;&#31995;&#32479;&#36827;&#34892;&#24212;&#29992;&#31243;&#24207;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27835;&#30103;&#24072;&#21487;&#20197;&#36890;&#36807;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#36827;&#24230;&#30340;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24247;&#22797;&#27835;&#30103;&#26377;&#21161;&#20110;&#27835;&#24840;&#36731;&#24494;&#30340;&#20307;&#32946;&#21644;&#32844;&#19994;&#20260;&#23475;&#12290;&#20256;&#32479;&#30340;&#24247;&#22797;&#36807;&#31243;&#20013;&#65292;&#27835;&#30103;&#24072;&#20250;&#25351;&#23450;&#26576;&#20123;&#21160;&#20316;&#20379;&#24739;&#32773;&#22312;&#21307;&#38498;&#35775;&#38382;&#20043;&#38388;&#25191;&#34892;&#65292;&#36825;&#23558;&#20381;&#36182;&#20110;&#24739;&#32773;&#27491;&#30830;&#22320;&#35760;&#20303;&#21160;&#20316;&#21644;&#25191;&#34892;&#35745;&#21010;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#24739;&#32773;&#20250;&#24536;&#35760;&#25191;&#34892;&#21160;&#20316;&#25110;&#26080;&#27861;&#35814;&#32454;&#22238;&#24819;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#24247;&#22797;&#27835;&#30103;&#21463;&#21040;&#38459;&#30861;&#65292;&#25110;&#32773;&#22312;&#26368;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#24739;&#32773;&#21487;&#33021;&#20250;&#22240;&#25191;&#34892;&#19981;&#27491;&#30830;&#30340;&#21160;&#20316;&#32780;&#36973;&#21463;&#39069;&#22806;&#30340;&#20260;&#23475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#24739;&#32773;&#30340;&#26234;&#33021;&#25163;&#26426;&#25552;&#37266;&#24739;&#32773;&#20309;&#26102;&#25191;&#34892;&#21160;&#20316;&#24182;&#23637;&#31034;&#21160;&#20316;&#20379;&#24739;&#32773;&#36319;&#38543;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#24110;&#21161;&#27835;&#30103;&#24072;&#30417;&#27979;&#24739;&#32773;&#30340;&#24247;&#22797;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#30001;&#19968;&#27454;iOS&#24212;&#29992;&#31243;&#24207;&#21644;&#20960;&#20010;&#26381;&#21153;&#22120;&#32452;&#20214;&#32452;&#25104;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#36127;&#36131;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
The rehabilitation treatment helps to heal minor sports and occupational injuries. In a traditional rehabilitation process, a therapist will assign certain actions to a patient to perform in between hospital visits, and it will rely on the patient to remember actions correctly and the schedule to perform them. Unfortunately, many patients forget to perform actions or fail to recall actions in detail. As a consequence, the rehabilitation treatment is hampered or, in the worst case, the patient may suffer from additional injury caused by performing incorrect actions. To resolve these issues, we propose a HRNet-based rehabilitation monitoring system, which can remind a patient when to perform the actions and display the actions for the patient to follow via the patient's smartphone. In addition, it helps the therapist to monitor the progress of the rehabilitation for the patient. Our system consists of an iOS app and several components at the server side. The app is in charge of displayin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31454;&#20105;&#22810;&#26234;&#33021;&#20307;&#25628;&#32034;&#30340;&#28436;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36827;&#21270;&#35745;&#31639;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#22312;&#19981;&#21516;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#25628;&#32034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.10640</link><description>&lt;p&gt;
&#22522;&#20110;&#31454;&#20105;&#22810;&#26234;&#33021;&#20307;&#25628;&#32034;&#30340;&#28436;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Evolving Strategies for Competitive Multi-Agent Search. (arXiv:2306.10640v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31454;&#20105;&#22810;&#26234;&#33021;&#20307;&#25628;&#32034;&#30340;&#28436;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36827;&#21270;&#35745;&#31639;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#22312;&#19981;&#21516;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36827;&#21270;&#35745;&#31639;&#36866;&#29992;&#20110;&#24037;&#31243;&#39046;&#22495;&#30340;&#33258;&#21160;&#21457;&#29616;&#65292;&#20294;&#23427;&#20063;&#21487;&#29992;&#20110;&#25581;&#31034;&#20154;&#31867;&#21644;&#32452;&#32455;&#22914;&#20309;&#26356;&#26377;&#25928;&#22320;&#25191;&#34892;&#20219;&#21153;&#12290;&#36890;&#36807;&#20197;&#32452;&#32455;&#20013;&#30340;&#21019;&#26032;&#25628;&#32034;&#38382;&#39064;&#20026;&#20363;&#65292;&#26412;&#25991;&#39318;&#20808;&#23558;&#20154;&#31867;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#24418;&#24335;&#21270;&#20026;&#31454;&#20105;&#22810;&#26234;&#33021;&#20307;&#25628;&#32034;&#65288;CMAS&#65289;&#12290;CMAS&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#21333;&#26234;&#33021;&#20307;&#21644;&#22242;&#38431;&#25628;&#32034;&#38382;&#39064;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#36890;&#36807;&#20102;&#35299;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#25628;&#32034;&#24773;&#20917;&#20197;&#21450;&#36825;&#20123;&#25628;&#32034;&#23548;&#33268;&#30340;&#25628;&#32034;&#26223;&#35266;&#30340;&#21160;&#24577;&#21464;&#21270;&#26469;&#36827;&#34892;&#20132;&#20114;&#12290;&#20027;&#35201;&#20551;&#35774;&#26159;&#65292;&#36827;&#21270;&#35745;&#31639;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;CMAS&#30340;&#26377;&#25928;&#31574;&#30053;&#65307;&#35813;&#20551;&#35774;&#22312;&#19968;&#31995;&#21015;&#20851;&#20110;NK&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#21363;&#37096;&#20998;&#30456;&#20851;&#19988;&#21487;&#35843;&#25972;&#23822;&#23702;&#30340;&#36866;&#24212;&#24230;&#26223;&#35266;&#12290;&#19981;&#21516;&#30340;&#19987;&#38376;&#31574;&#30053;&#38024;&#23545;&#27599;&#20010;&#19981;&#21516;&#30340;&#31454;&#20105;&#29615;&#22659;&#36827;&#34892;&#20102;&#28436;&#21270;&#65292;&#21516;&#26102;&#36824;&#28436;&#21270;&#20986;&#20102;&#34920;&#29616;&#33391;&#22909;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
While evolutionary computation is well suited for automatic discovery in engineering, it can also be used to gain insight into how humans and organizations could perform more effectively. Using a real-world problem of innovation search in organizations as the motivating example, this article first formalizes human creative problem solving as competitive multi-agent search (CMAS). CMAS is different from existing single-agent and team search problems in that the agents interact through knowledge of other agents' searches and through the dynamic changes in the search landscape that result from these searches. The main hypothesis is that evolutionary computation can be used to discover effective strategies for CMAS; this hypothesis is verified in a series of experiments on the NK model, i.e.\ partially correlated and tunably rugged fitness landscapes. Different specialized strategies are evolved for each different competitive environment, and also general strategies that perform well acros
&lt;/p&gt;</description></item><item><title>AI-generated educational content has been found to be perceived as equivalent in quality to content created by students, suggesting that AI-generated resources may serve as viable learning materials.</title><link>http://arxiv.org/abs/2306.10509</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#30456;&#20449;AI&#29983;&#25104;&#30340;&#25945;&#32946;&#20869;&#23481;&#21527;&#65311;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#23398;&#20064;&#36164;&#28304;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Can We Trust AI-Generated Educational Content? Comparative Analysis of Human and AI-Generated Learning Resources. (arXiv:2306.10509v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10509
&lt;/p&gt;
&lt;p&gt;
AI-generated educational content has been found to be perceived as equivalent in quality to content created by students, suggesting that AI-generated resources may serve as viable learning materials.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#23398;&#29983;&#36716;&#21521;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#30340;&#22312;&#32447;&#23398;&#20064;&#24179;&#21488;&#65292;&#23545;&#39640;&#36136;&#37327;&#25945;&#32946;&#20869;&#23481;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20284;&#20046;&#20026;&#24555;&#36895;&#21019;&#24314;&#22823;&#35268;&#27169;&#23398;&#20064;&#26448;&#26009;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20943;&#36731;&#20102;&#25945;&#24072;&#30340;&#36127;&#25285;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;LLMs&#29983;&#25104;&#30340;&#36164;&#28304;&#19982;&#23398;&#29983;&#20316;&#20026;&#23398;&#20064;&#26469;&#28304;&#30340;&#19968;&#37096;&#20998;&#21019;&#24314;&#30340;&#36164;&#28304;&#30340;&#36136;&#37327;&#65292;&#22312;&#20837;&#38376;&#32534;&#31243;&#35821;&#22659;&#19979;&#30740;&#31350;&#20102;LLMs&#29983;&#25104;&#23398;&#20064;&#36164;&#28304;&#30340;&#28508;&#21147;&#12290;&#22312;&#21463;&#35797;&#32773;&#25253;&#21578;&#35780;&#20272;&#20013;&#65292;&#23398;&#29983;&#22312;&#21021;&#27493;&#25552;&#20379;&#30456;&#21516;&#31034;&#20363;&#21518;&#35780;&#20272;&#20102;AI&#21644;&#21516;&#23398;&#29983;&#25104;&#30340;&#36164;&#28304;&#30340;&#27491;&#30830;&#24615;&#21644;&#24110;&#21161;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#29983;&#24863;&#30693;&#21040;&#30340;AI&#29983;&#25104;&#36164;&#28304;&#30340;&#36136;&#37327;&#19982;&#21516;&#23398;&#29983;&#25104;&#30340;&#36164;&#28304;&#30340;&#36136;&#37327;&#30456;&#24403;&#12290;&#36825;&#34920;&#26126;AI&#29983;&#25104;&#30340;&#36164;&#28304;&#21487;&#33021;&#20316;&#20026;&#36741;&#21161;&#23398;&#20064;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an increasing number of students move to online learning platforms that deliver personalized learning experiences, there is a great need for the production of high-quality educational content. Large language models (LLMs) appear to offer a promising solution to the rapid creation of learning materials at scale, reducing the burden on instructors. In this study, we investigated the potential for LLMs to produce learning resources in an introductory programming context, by comparing the quality of the resources generated by an LLM with those created by students as part of a learnersourcing activity. Using a blind evaluation, students rated the correctness and helpfulness of resources generated by AI and their peers, after both were initially provided with identical exemplars. Our results show that the quality of AI-generated resources, as perceived by students, is equivalent to the quality of resources generated by their peers. This suggests that AI-generated resources may serve as vi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#26469;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07874</link><description>&lt;p&gt;
&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taxonomy-Structured Domain Adaptation. (arXiv:2306.07874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#26469;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#26088;&#22312;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#22823;&#22810;&#38480;&#20110;&#20998;&#31867;&#39046;&#22495;&#65292;&#36825;&#20005;&#37325;&#31616;&#21270;&#20102;&#30495;&#23454;&#19990;&#30028;&#20013;&#24494;&#22937;&#30340;&#39046;&#22495;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#36827;&#34892;&#25512;&#24191;&#65292;&#23558;&#39046;&#22495;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#23884;&#22871;&#30340;&#23618;&#27425;&#30456;&#20284;&#32467;&#26500;&#65292;&#20363;&#22914;&#21160;&#29289;&#29289;&#31181;&#21644;&#20135;&#21697;&#30446;&#24405;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#32463;&#20856;&#23545;&#25239;&#26694;&#26550;&#20043;&#19978;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#19982;&#23545;&#25239;&#24615;&#37492;&#21035;&#22120;&#31454;&#20105;&#20197;&#20445;&#30041;&#23618;&#27425;&#32467;&#26500;&#20449;&#24687;&#12290;&#24403;&#32473;&#23450;&#38750;&#20449;&#24687;&#39046;&#22495;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#25152;&#26377;&#21494;&#33410;&#28857;&#37117;&#38142;&#25509;&#21040;&#26681;&#33410;&#28857;&#30340;&#25153;&#24179;&#20998;&#31867;&#65289;&#26102;&#65292;&#24179;&#34913;&#28857;&#24674;&#22797;&#32463;&#20856;&#30340;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20998;&#31867;&#20013;&#20135;&#29983;&#38750;&#24179;&#20961;&#30340;&#32467;&#26524;&#12290;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#36827;&#34892;&#20102;&#33258;&#36866;&#24212;&#12290;&#20195;&#30721;&#21487;&#22312;https://gith&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to mitigate distribution shifts among different domains. However, traditional formulations are mostly limited to categorical domains, greatly simplifying nuanced domain relationships in the real world. In this work, we tackle a generalization with taxonomy-structured domains, which formalizes domains with nested, hierarchical similarity structures such as animal species and product catalogs. We build on the classic adversarial framework and introduce a novel taxonomist, which competes with the adversarial discriminator to preserve the taxonomy information. The equilibrium recovers the classic adversarial domain adaptation's solution if given a non-informative domain taxonomy (e.g., a flat taxonomy where all leaf nodes connect to the root node) while yielding non-trivial results with other taxonomies. Empirically, our method achieves state-of-the-art performance on both synthetic and real-world datasets with successful adaptation. Code is available at https://gith
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2306.07812</link><description>&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#33258;&#21160;&#21270;&#19977;&#32500;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Automated 3D Pre-Training for Molecular Property Prediction. (arXiv:2306.07812v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07812
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26159;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#30001;&#20110;&#20998;&#23376;&#30340;&#20960;&#20309;&#32467;&#26500;&#23545;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#24517;&#35201;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#65292;&#22240;&#27492;&#23558;3D&#20449;&#24687;&#19982;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#33719;&#24471;&#20998;&#23376;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;3D&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;&#31216;&#20026;3D PGT&#65289;&#65292;&#23427;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#27809;&#26377;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22522;&#20110;&#21270;&#23398;&#38190;&#38271;&#65292;&#21270;&#23398;&#38190;&#35282;&#21644;&#20108;&#38754;&#35282;&#36825;&#19977;&#20010;&#22522;&#26412;&#20960;&#20309;&#25551;&#36848;&#31526;&#23545;&#24212;&#20110;&#23436;&#25972;&#30340;&#20998;&#23376;3D&#26500;&#24418;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#36825;&#19977;&#20010;&#23646;&#24615;&#30340;&#22810;&#20219;&#21153;&#29983;&#25104;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#33258;&#21160;&#34701;&#21512;&#36825;&#19977;&#39033;&#29983;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#24635;&#33021;&#37327;&#8221;&#26469;&#25628;&#32034;&#30340;&#26367;&#20195;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction, 3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the \textit{total energy} to search for 
&lt;/p&gt;</description></item><item><title>V-LoL&#26159;&#19968;&#20010;&#32467;&#21512;&#35270;&#35273;&#21644;&#36923;&#36753;&#25361;&#25112;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;V-LoL-Trains&#65292;&#35813;&#25968;&#25454;&#38598;&#39318;&#27425;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2306.07743</link><description>&lt;p&gt;
V-LoL: &#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
V-LoL: A Diagnostic Dataset for Visual Logical Learning. (arXiv:2306.07743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07743
&lt;/p&gt;
&lt;p&gt;
V-LoL&#26159;&#19968;&#20010;&#32467;&#21512;&#35270;&#35273;&#21644;&#36923;&#36753;&#25361;&#25112;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;V-LoL-Trains&#65292;&#35813;&#25968;&#25454;&#38598;&#39318;&#27425;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#22312;&#35270;&#35273;AI&#39046;&#22495;&#26377;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19981;&#21516;&#30340;&#32570;&#28857;&#65307;&#21253;&#25324;&#32570;&#23569;&#31934;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#12289;&#25277;&#35937;&#30340;&#27010;&#25324;&#33021;&#21147;&#20197;&#21450;&#29702;&#35299;&#22797;&#26434;&#21644;&#22024;&#26434;&#30340;&#22330;&#26223;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#25429;&#25417;&#21040;&#36825;&#20123;&#26041;&#38754;&#20013;&#30340;&#22810;&#25968;&#12290;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#38598;&#20851;&#27880;&#35270;&#35273;&#22797;&#26434;&#25968;&#25454;&#20294;&#21482;&#26377;&#31616;&#21333;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#24402;&#32435;&#36923;&#36753;&#25968;&#25454;&#38598;&#21253;&#25324;&#22797;&#26434;&#30340;&#36923;&#36753;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#26159;&#32570;&#20047;&#35270;&#35273;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25968;&#25454;&#38598;V-LoL&#65292;&#23427;&#26080;&#32541;&#22320;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#36923;&#36753;&#30340;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#25512;&#20986;&#20102;V-LoL&#30340;&#31532;&#19968;&#20010;&#23454;&#20363;&#65292;&#21517;&#20026;V-LoL-Trains&#65292;&#23427;&#26159;&#31526;&#21495;AI&#20013;&#19968;&#20010;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30340;&#35270;&#35273;&#21576;&#29616;&#65292;&#21363;Michalski&#28779;&#36710;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#32467;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;V-LoL-Trains&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the successes of recent developments in visual AI, different shortcomings still exist; from missing exact logical reasoning, to abstract generalization abilities, to understanding complex and noisy scenes. Unfortunately, existing benchmarks, were not designed to capture more than a few of these aspects. Whereas deep learning datasets focus on visually complex data but simple visual reasoning tasks, inductive logic datasets involve complex logical learning tasks, however, lack the visual component. To address this, we propose the visual logical learning dataset, V-LoL, that seamlessly combines visual and logical challenges. Notably, we introduce the first instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic benchmark in symbolic AI, the Michalski train problem. By incorporating intricate visual scenes and flexible logical reasoning tasks within a versatile framework, V-LoL-Trains provides a platform for investigating a wide range of visual logical learning ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#26469;&#24320;&#21457;&#21644;&#37096;&#32626;AGI&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05480</link><description>&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence for Medical Imaging. (arXiv:2306.05480v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#26469;&#24320;&#21457;&#21644;&#37096;&#32626;AGI&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#23558;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#25972;&#21512;&#21040;AGI&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;AGI&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#36335;&#32447;&#22270;&#12290;&#22312;&#25972;&#20010;&#32508;&#36848;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#21307;&#23398;&#39046;&#22495;&#37096;&#32626;&#22823;&#35268;&#27169;AGI&#27169;&#22411;&#21487;&#33021;&#38754;&#20020;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#32570;&#38519;&#30340;&#20851;&#38190;&#35266;&#28857;&#12290;&#36825;&#31687;&#32508;&#21512;&#24615;&#30340;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;AGI&#23545;&#21307;&#23398;&#25104;&#20687;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#20854;&#20182;&#39046;&#22495;&#26410;&#26469;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this review, we explore the potential applications of Artificial General Intelligence (AGI) models in healthcare, focusing on foundational Large Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We emphasize the importance of integrating clinical expertise, domain knowledge, and multimodal capabilities into AGI models. In addition, we lay out key roadmaps that guide the development and deployment of healthcare AGI models. Throughout the review, we provide critical perspectives on the potential challenges and pitfalls associated with deploying large-scale AGI models in the medical field. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DFM&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26631;&#31614;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#19978;&#38480;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04376</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#29305;&#24449;&#21305;&#37197;&#30340;&#26631;&#31614;&#20559;&#31227;&#37327;&#37327;&#21270;&#21450;&#20854;&#40065;&#26834;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching. (arXiv:2306.04376v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DFM&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26631;&#31614;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#19978;&#38480;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#23398;&#20064;&#22788;&#29702;&#22312;&#26631;&#31614;&#20559;&#31227;&#19979;&#20272;&#35745;&#30446;&#26631;&#26631;&#31614;&#20998;&#24067;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20998;&#24067;&#29305;&#24449;&#21305;&#37197;&#65288;DFM&#65289;&#65292;&#23558;&#20808;&#21069;&#25991;&#29486;&#20013;&#24341;&#20837;&#30340;&#21508;&#31181;&#20272;&#35745;&#22120;&#24674;&#22797;&#20026;&#29305;&#23450;&#23454;&#20363;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;DFM&#31243;&#24207;&#30340;&#19968;&#33324;&#24615;&#33021;&#30028;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#25512;&#23548;&#30340;&#30028;&#38480;&#30340;&#33509;&#24178;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#20998;&#26512;&#25193;&#23637;&#21040;&#30740;&#31350;DFM&#31243;&#24207;&#22312;&#26410;&#31934;&#30830;&#20551;&#35774;&#26631;&#31614;&#20559;&#31227;&#37327;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30446;&#26631;&#21463;&#21040;&#26410;&#30693;&#20998;&#24067;&#27745;&#26579;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35814;&#32454;&#30340;&#25968;&#23383;&#30740;&#31350;&#30830;&#35748;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#21407;&#29702;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification learning deals with the task of estimating the target label distribution under label shift. In this paper, we first present a unifying framework, distribution feature matching (DFM), that recovers as particular instances various estimators introduced in previous literature. We derive a general performance bound for DFM procedures, improving in several key aspects upon previous bounds derived in particular cases. We then extend this analysis to study robustness of DFM procedures in the misspecified setting under departure from the exact label shift hypothesis, in particular in the case of contamination of the target by an unknown distribution. These theoretical findings are confirmed by a detailed numerical study on simulated and real-world datasets. We also introduce an efficient, scalable and robust version of kernel-based DFM using the Random Fourier Feature principle.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#20219;&#21153;&#28436;&#31034;&#20013;&#33258;&#21160;&#29983;&#25104;&#21487;&#25191;&#34892;&#26426;&#22120;&#20154;&#25511;&#21046;&#31243;&#24207;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#21644;&#22522;&#20110;&#28216;&#25103;&#24341;&#25806;&#30340;&#29289;&#29702;&#23398;&#36827;&#34892;&#35821;&#20041;&#35299;&#37322;&#65292;&#24182;&#32467;&#21512;&#36890;&#29992;&#20219;&#21153;&#34920;&#31034;&#21644;&#33258;&#21160;&#36335;&#24452;&#35268;&#21010;&#21644;&#20195;&#30721;&#29983;&#25104;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36141;&#29289;&#21161;&#25163;&#30340;&#21147;&#25935;&#25235;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.02739</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#34394;&#25311;&#29616;&#23454;&#28436;&#31034;&#20013;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#31243;&#24207;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Driven Robot Program Synthesis from Human VR Demonstrations. (arXiv:2306.02739v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#20219;&#21153;&#28436;&#31034;&#20013;&#33258;&#21160;&#29983;&#25104;&#21487;&#25191;&#34892;&#26426;&#22120;&#20154;&#25511;&#21046;&#31243;&#24207;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#21644;&#22522;&#20110;&#28216;&#25103;&#24341;&#25806;&#30340;&#29289;&#29702;&#23398;&#36827;&#34892;&#35821;&#20041;&#35299;&#37322;&#65292;&#24182;&#32467;&#21512;&#36890;&#29992;&#20219;&#21153;&#34920;&#31034;&#21644;&#33258;&#21160;&#36335;&#24452;&#35268;&#21010;&#21644;&#20195;&#30721;&#29983;&#25104;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36141;&#29289;&#21161;&#25163;&#30340;&#21147;&#25935;&#25235;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32769;&#21270;&#12289;&#21171;&#21160;&#21147;&#30701;&#32570;&#21644;&#19981;&#26029;&#22686;&#21152;&#30340;&#24037;&#36164;&#25104;&#26412;&#38656;&#35201;&#33021;&#22815;&#33258;&#20027;&#25191;&#34892;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#36741;&#21161;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#24320;&#25918;&#24335;&#26426;&#22120;&#20154;&#25805;&#20316;&#19981;&#20165;&#38656;&#35201;&#24378;&#22823;&#30340;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#65288;KR&#65286;R&#65289;&#31639;&#27861;&#65292;&#36824;&#38656;&#35201;&#20154;&#20204;&#25945;&#23548;&#26426;&#22120;&#20154;&#25191;&#34892;&#20219;&#21153;&#21644;&#22914;&#20309;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#20219;&#21153;&#28436;&#31034;&#20013;&#33258;&#21160;&#29983;&#25104;&#21487;&#25191;&#34892;&#26426;&#22120;&#20154;&#25511;&#21046;&#31243;&#24207;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#21644;&#22522;&#20110;&#28216;&#25103;&#24341;&#25806;&#30340;&#29289;&#29702;&#23398;&#26469;&#35821;&#20041;&#21270;&#35299;&#37322;&#20154;&#31867;&#34394;&#25311;&#29616;&#23454;&#28436;&#31034;&#65292;&#20197;&#21450;&#29992;&#20110;&#34920;&#36798;&#21644;&#33258;&#21160;&#36335;&#24452;&#35268;&#21010;&#21644;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#20219;&#21153;&#34920;&#31034;&#21644;&#26368;&#20808;&#36827;&#35748;&#30693;&#26550;&#26500;&#19978;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#36141;&#29289;&#21161;&#25163;&#30340;&#21147;&#25935;&#25235;&#21462;&#21644;&#25918;&#32622;&#29615;&#22659;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/ease-crc/vr-program-synthesis&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aging societies, labor shortages and increasing wage costs call for assistance robots capable of autonomously performing a wide array of real-world tasks. Such open-ended robotic manipulation requires not only powerful knowledge representations and reasoning (KR&amp;R) algorithms, but also methods for humans to instruct robots what tasks to perform and how to perform them. In this paper, we present a system for automatically generating executable robot control programs from human task demonstrations in virtual reality (VR). We leverage common-sense knowledge and game engine-based physics to semantically interpret human VR demonstrations, as well as an expressive and general task representation and automatic path planning and code generation, embedded into a state-of-the-art cognitive architecture. We demonstrate our approach in the context of force-sensitive fetch-and-place for a robotic shopping assistant. The source code is available at https://github.com/ease-crc/vr-program-synthesis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19981;&#21516;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#31168;&#29305;&#24615;&#65292;&#23454;&#29616;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;PairRanker&#21644;GenFuser&#26159;&#35813;&#26694;&#26550;&#30340;&#20004;&#20010;&#27169;&#22359;&#65292;PairRanker&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#65292;&#24182;&#19988;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#20197;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2306.02561</link><description>&lt;p&gt;
LLM-Blender: &#21033;&#29992;&#25104;&#23545;&#25490;&#21517;&#21644;&#29983;&#25104;&#34701;&#21512;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19981;&#21516;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#31168;&#29305;&#24615;&#65292;&#23454;&#29616;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;PairRanker&#21644;GenFuser&#26159;&#35813;&#26694;&#26550;&#30340;&#20004;&#20010;&#27169;&#22359;&#65292;PairRanker&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#65292;&#24182;&#19988;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#20197;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#21516;&#20248;&#21183;&#26469;&#36798;&#21040;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;PairRanker&#21644;GenFuser&#65292;&#20197;&#24212;&#23545;&#19981;&#21516;&#31034;&#20363;&#30340;&#26368;&#20248;LLMs&#21487;&#20197;&#26174;&#30528;&#21464;&#21270;&#30340;&#35266;&#23519;&#12290;PairRanker&#20351;&#29992;&#19987;&#38376;&#30340;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#20043;&#38388;&#30340;&#24494;&#23567;&#24046;&#24322;&#12290;&#23427;&#32852;&#21512;&#32534;&#30721;&#36755;&#20837;&#25991;&#26412;&#21644;&#19968;&#23545;&#20505;&#36873;&#32773;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#20248;&#36234;&#32773;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PairRanker&#19982;ChatGPT&#30340;&#25490;&#21517;&#30456;&#20851;&#24615;&#26368;&#39640;&#12290;&#28982;&#21518;&#65292;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#20943;&#23569;&#23427;&#20204;&#30340;&#24369;&#28857;&#26469;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#20419;&#36827;&#22823;&#35268;&#27169;&#35780;&#20272;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MixInstruct&#65292;&#23427;&#26159;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#65292;&#20855;&#26377;oracle p&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#26689;&#26550;&#35774;&#35745;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#35780;&#20272;&#20505;&#36873;&#35774;&#35745;&#24182;&#26356;&#26032;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#24335;&#20248;&#21270;&#26689;&#26550;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#20248;&#21270;&#22797;&#26434;&#31995;&#32479;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01763</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26689;&#26550;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization for truss design using Bayesian optimization. (arXiv:2306.01763v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#26689;&#26550;&#35774;&#35745;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#35780;&#20272;&#20505;&#36873;&#35774;&#35745;&#24182;&#26356;&#26032;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#24335;&#20248;&#21270;&#26689;&#26550;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#20248;&#21270;&#22797;&#26434;&#31995;&#32479;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35745;&#31639;&#26426;&#36741;&#21161;&#26377;&#38480;&#20803;&#20998;&#26512;&#36827;&#34892;&#26426;&#26800;&#26689;&#26550;&#20960;&#20309;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#26689;&#26550;&#30340;&#24418;&#29366;&#26159;&#30830;&#23450;&#20854;&#25215;&#36733;&#33021;&#21147;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#22312;&#32473;&#23450;&#30340;&#21442;&#25968;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#26368;&#22823;&#21270;&#25215;&#36733;&#33021;&#21147;&#19988;&#19981;&#20250;&#20135;&#29983;&#24212;&#21147;&#30340;&#22806;&#22771;&#21442;&#25968;&#12290;&#25105;&#20204;&#36873;&#25321;&#36125;&#21494;&#26031;&#20248;&#21270;&#20316;&#20026;&#25105;&#20204;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#31181;&#26114;&#36149;&#35745;&#31639;&#30340;&#35774;&#35745;&#35780;&#20272;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26689;&#26550;&#35774;&#35745;&#28041;&#21450;&#36845;&#20195;&#35780;&#20272;&#19968;&#32452;&#20505;&#36873;&#26689;&#26550;&#35774;&#35745;&#65292;&#24182;&#22522;&#20110;&#32467;&#26524;&#26356;&#26032;&#35774;&#35745;&#31354;&#38388;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#27599;&#20010;&#20505;&#36873;&#35774;&#35745;&#30340;&#24615;&#33021;&#65292;&#19979;&#19968;&#20010;&#20505;&#36873;&#35774;&#35745;&#26159;&#22522;&#20110;&#27169;&#22411;&#23545;&#24615;&#33021;&#25913;&#36827;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27010;&#29575;&#27169;&#22411;&#26377;&#21161;&#20110;&#35780;&#20272;&#26689;&#26550;&#35774;&#35745;&#31354;&#38388;&#65292;&#24182;&#19988;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#20248;&#21270;&#22797;&#26434;&#31995;&#32479;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, geometry optimization of mechanical truss using computer-aided finite element analysis is presented. The shape of the truss is a dominant factor in determining the capacity of load it can bear. At a given parameter space, our goal is to find the parameters of a hull that maximize the load-bearing capacity and also don't yield to the induced stress. We rely on finite element analysis, which is a computationally costly design analysis tool for design evaluation. For such expensive to-evaluate functions, we chose Bayesian optimization as our optimization framework which has empirically proven sample efficient than other simulation-based optimization methods.  By utilizing Bayesian optimization algorithms, the truss design involves iteratively evaluating a set of candidate truss designs and updating a probabilistic model of the design space based on the results. The model is used to predict the performance of each candidate design, and the next candidate design is selected ba
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#25552;&#20379;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#27979;&#35797;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#31995;&#32479;&#65292;&#36890;&#36807;&#20849;&#20139;&#26426;&#22120;&#20154;&#30828;&#20214;&#21644;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#20415;&#21644;&#30452;&#25509;&#30340;&#27604;&#36739;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00942</link><description>&lt;p&gt;
&#31163;&#32447;&#35757;&#32451;&#65292;&#22312;&#32447;&#27979;&#35797;&#65306;&#19968;&#20010;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Train Offline, Test Online: A Real Robot Learning Benchmark. (arXiv:2306.00942v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00942
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#25552;&#20379;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#27979;&#35797;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#31995;&#32479;&#65292;&#36890;&#36807;&#20849;&#20139;&#26426;&#22120;&#20154;&#30828;&#20214;&#21644;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#20415;&#21644;&#30452;&#25509;&#30340;&#27604;&#36739;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#21463;&#21040;&#19977;&#20010;&#25361;&#25112;&#30340;&#38480;&#21046;&#65306;&#26426;&#22120;&#20154;&#26114;&#36149;&#65288;&#24456;&#23569;&#26377;&#23454;&#39564;&#23460;&#21487;&#20197;&#21442;&#19982;&#65289;&#65292;&#27599;&#20010;&#20154;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#65288;&#30740;&#31350;&#32467;&#26524;&#22312;&#23454;&#39564;&#23460;&#20043;&#38388;&#19981;&#20855;&#26377;&#26222;&#36941;&#24615;&#65289;&#65292;&#25105;&#20204;&#32570;&#20047;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#12290;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#31995;&#32479;&#65306;&#31163;&#32447;&#35757;&#32451;&#12289;&#22312;&#32447;&#27979;&#35797;&#65288;TOTO&#65289;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;TOTO&#20026;&#36828;&#31243;&#29992;&#25143;&#25552;&#20379;&#20849;&#20139;&#26426;&#22120;&#20154;&#30828;&#20214;&#20197;&#35780;&#20272;&#24120;&#35265;&#20219;&#21153;&#19978;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#36825;&#20123;&#20219;&#21153;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#12290;&#23427;&#30340;&#25805;&#20316;&#20219;&#21153;&#22871;&#20214;&#38656;&#35201;&#22312;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#12289;&#20301;&#32622;&#21644;&#20809;&#29031;&#26465;&#20214;&#19979;&#36827;&#34892;&#25361;&#25112;&#24615;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;TOTO&#22312;&#20116;&#20010;&#26426;&#26500;&#36828;&#31243;&#36129;&#29486;&#30340;&#65292;&#27604;&#36739;&#20102;&#20116;&#20010;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#31034;&#21644;&#22235;&#20010;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#22522;&#32447;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;TOTO&#30495;&#27491;&#30340;&#28508;&#21147;&#22312;&#20110;&#26410;&#26469;&#65306;&#25105;&#20204;&#21457;&#24067;&#20102;&#36825;&#20010;&#22522;&#20934;&#31995;&#32479;&#65292;&#20219;&#20309;&#29992;&#25143;&#37117;&#21487;&#20197;&#25552;&#20132;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#36731;&#26494;&#22320;&#30452;&#25509;&#19982;&#22810;&#31181;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#26080;&#38656;&#33719;&#24471;&#30828;&#20214;&#25110;&#25910;&#38598;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three challenges limit the progress of robot learning research: robots are expensive (few labs can participate), everyone uses different robots (findings do not generalize across labs), and we lack internet-scale robotics data. We take on these challenges via a new benchmark: Train Offline, Test Online (TOTO). TOTO provides remote users with access to shared robotic hardware for evaluating methods on common tasks and an open-source dataset of these tasks for offline training. Its manipulation task suite requires challenging generalization to unseen objects, positions, and lighting. We present initial results on TOTO comparing five pretrained visual representations and four offline policy learning baselines, remotely contributed by five institutions. The real promise of TOTO, however, lies in the future: we release the benchmark for additional submissions from any user, enabling easy, direct comparison to several methods without the need to obtain hardware or collect data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;BigVideo&#65292; &#38598;&#25104;&#20102;4.5 million&#21477;&#23376;&#23545;&#21644;9981&#23567;&#26102;&#35270;&#39057;&#65292;&#35774;&#35745;&#20102;&#26377;&#27495;&#20041;&#21644;&#26126;&#30830;&#30340;&#27979;&#35797;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;NMT&#27169;&#22411;&#30340;BLEU&#12289;BLEURT&#21644;COMET&#24471;&#20998;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#65292;&#25968;&#25454;&#38598;&#21644;&#32763;&#35793;&#27169;&#22411;&#20844;&#24320;&#21487;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.18326</link><description>&lt;p&gt;
BigVideo:&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation. (arXiv:2305.18326v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;BigVideo&#65292; &#38598;&#25104;&#20102;4.5 million&#21477;&#23376;&#23545;&#21644;9981&#23567;&#26102;&#35270;&#39057;&#65292;&#35774;&#35745;&#20102;&#26377;&#27495;&#20041;&#21644;&#26126;&#30830;&#30340;&#27979;&#35797;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;NMT&#27169;&#22411;&#30340;BLEU&#12289;BLEURT&#21644;COMET&#24471;&#20998;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#65292;&#25968;&#25454;&#38598;&#21644;&#32763;&#35793;&#27169;&#22411;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;BigVideo&#65292;&#20197;&#20419;&#36827;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;How2&#21644;VaTeX&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;BigVideo&#36229;&#36807;10&#20493;&#65292;&#21253;&#25324;450&#19975;&#20010;&#21477;&#23376;&#23545;&#21644;9981&#23567;&#26102;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#26377;&#24847;&#35774;&#35745;&#30340;&#27979;&#35797;&#38598;&#26469;&#39564;&#35777;&#35270;&#35273;&#20449;&#24687;&#30340;&#24517;&#35201;&#24615;&#65306;&#19968;&#20010;&#26159;&#19968;&#20010;&#26377;&#27495;&#20041;&#35789;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#20854;&#20013;&#25991;&#26412;&#19978;&#19979;&#25991;&#23545;&#20110;&#32763;&#35793;&#26159;&#33258;&#21253;&#21547;&#30340;&#26126;&#30830;&#38598;&#21512;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#24314;&#27169;&#25991;&#26412;&#21644;&#35270;&#39057;&#20849;&#20139;&#30340;&#20849;&#21516;&#35821;&#20041;&#65292;&#25105;&#20204;&#22312;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;BigVideo&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65306;a&#65289;&#35270;&#35273;&#20449;&#24687;&#22312;&#27495;&#20041;&#21644;&#26126;&#30830;&#30340;&#27979;&#35797;&#38598;&#19978;&#22987;&#32456;&#25552;&#39640;NMT&#27169;&#22411;&#30340;BLEU&#12289;BLEURT&#21644;COMET&#24471;&#20998;&#12290;b&#65289;&#35270;&#35273;&#20449;&#24687;&#23545;&#20110;&#26415;&#35821;&#23450;&#20301;&#24471;&#20998;&#21644;&#20154;&#24037;&#35780;&#20272;&#32780;&#35328;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#65292;&#19982;&#24378;&#25991;&#26412;&#22522;&#32447;&#30456;&#27604;&#12290;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#32763;&#35793;&#27169;&#22411;&#37117;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a large-scale video subtitle translation dataset, BigVideo, to facilitate the study of multi-modality machine translation. Compared with the widely used How2 and VaTeX datasets, BigVideo is more than 10 times larger, consisting of 4.5 million sentence pairs and 9,981 hours of videos. We also introduce two deliberately designed test sets to verify the necessity of visual information: Ambiguous with the presence of ambiguous words, and Unambiguous in which the text context is self-contained for translation. To better model the common semantics shared across texts and videos, we introduce a contrastive learning method in the cross-modal encoder. Extensive experiments on the BigVideo show that: a) Visual information consistently improves the NMT model in terms of BLEU, BLEURT, and COMET on both Ambiguous and Unambiguous test sets. b) Visual information helps disambiguation, compared to the strong text baseline on terminology-targeted scores and human evaluation. Dataset and our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28216;&#25103;&#25151;&#38388;&#65292;&#22312;&#20165;&#26377;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#36798;37%&#30340;&#21487;&#29609;&#26032;&#39062;&#20851;&#21345;&#65292;&#35813;&#25216;&#26415;&#26377;&#21161;&#20110;&#35299;&#20915;&#21253;&#21547;&#35768;&#22810;&#23616;&#37096;&#21644;&#20840;&#23616;&#32422;&#26463;&#30340;PCG&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18243</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23454;&#29992;&#30340;PCG
&lt;/p&gt;
&lt;p&gt;
Practical PCG Through Large Language Models. (arXiv:2305.18243v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28216;&#25103;&#25151;&#38388;&#65292;&#22312;&#20165;&#26377;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#36798;37%&#30340;&#21487;&#29609;&#26032;&#39062;&#20851;&#21345;&#65292;&#35813;&#25216;&#26415;&#26377;&#21161;&#20110;&#35299;&#20915;&#21253;&#21547;&#35768;&#22810;&#23616;&#37096;&#21644;&#20840;&#23616;&#32422;&#26463;&#30340;PCG&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20043;&#22806;&#30340;&#21508;&#31181;&#39046;&#22495;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22914;&#20309;&#20351;&#29992;LLMs&#20026;&#27491;&#22312;&#24320;&#21457;&#20013;&#30340;&#28216;&#25103;Metavoidal&#29983;&#25104;2D&#28216;&#25103;&#25151;&#38388;&#30340;&#23454;&#29992;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21442;&#19982;&#30340;&#24494;&#35843;&#65292;&#21033;&#29992;GPT-3&#30340;&#33021;&#21147;&#65292;&#20165;&#20351;&#29992;60&#20010;&#25163;&#21160;&#35774;&#35745;&#30340;&#25151;&#38388;&#25968;&#25454;&#65292;&#22312;&#22797;&#26434;&#30340;&#28216;&#25103;&#22330;&#26223;&#19979;&#65292;&#29983;&#25104;37%&#30340;&#21487;&#29609;&#26032;&#39062;&#20851;&#21345;&#65292;&#36825;&#26159;&#38024;&#23545;&#23384;&#22312;&#22823;&#37327;&#23616;&#37096;&#21644;&#20840;&#23616;&#32422;&#26463;&#30340;PCG&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven to be useful tools in various domains outside of the field of their inception, which was natural language processing. In this study, we provide practical directions on how to use LLMs to generate 2D-game rooms for an under-development game, named Metavoidal. Our technique can harness the power of GPT-3 by Human-in-the-loop fine-tuning which allows our method to create 37% Playable-Novel levels from as scarce data as only 60 hand-designed rooms under a scenario of the non-trivial game, with respect to (Procedural Content Generation) PCG, that has a good amount of local and global constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MERGE&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MERGE&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;26.5&#20493;&#30340;&#21152;&#36895;&#21644;80%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.15769</link><description>&lt;p&gt;
MERGE: &#24555;&#36895;&#30340;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MERGE: Fast Private Text Generation. (arXiv:2305.15769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MERGE&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MERGE&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;26.5&#20493;&#30340;&#21152;&#36895;&#21644;80%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;NLP&#26381;&#21153;&#21644;Transformer&#27169;&#22411;&#30340;&#31169;&#26377;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20004;&#26041;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#20165;&#32771;&#34385;NLU&#22330;&#26223;&#65292;&#32780;&#25991;&#26412;&#29983;&#25104;&#30340;&#31169;&#26377;&#25512;&#29702;&#65292;&#22914;&#32763;&#35793;&#12289;&#23545;&#35805;&#21644;&#20195;&#30721;&#34917;&#20840;&#65292;&#20173;&#26410;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#23558;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#36801;&#31227;&#21040;NLG&#27169;&#22411;&#26102;&#65292;&#24615;&#33021;&#34920;&#29616;&#24046;&#65292;&#32780;&#22312;&#35757;&#32451;&#38454;&#27573;&#21463;&#21040;&#25910;&#25947;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MERGE&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MERGE&#37325;&#29992;&#36755;&#20986;&#38544;&#34255;&#29366;&#24577;&#20316;&#20026;&#21333;&#35789;&#23884;&#20837;&#65292;&#20197;&#36339;&#36807;&#23884;&#20837;&#35745;&#31639;&#65292;&#24182;&#37325;&#26032;&#32452;&#32455;Transformer&#27169;&#22359;&#20013;&#30340;&#32447;&#24615;&#25805;&#20316;&#20197;&#21152;&#36895;&#21521;&#21069;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#20248;&#21270;&#65292;&#22823;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#38271;&#24230;&#20026;512&#26102;&#65292;MERGE&#21487;&#23454;&#29616;26.5&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#20943;&#23569;80\%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen increasing concerns about the private inference of NLP services and Transformer models. However, existing two-party privacy-preserving methods solely consider NLU scenarios, while the private inference of text generation such as translation, dialogue, and code completion remains unsolved. Besides, while migrated to NLG models, existing privacy-preserving methods perform poorly in terms of inference speed, and suffer from the convergence problem during the training stage. To address these issues, we propose MERGE, a fast private text generation framework for Transformer-based language models. Specifically, MERGE reuse the output hidden state as the word embedding to bypass the embedding computation, and reorganize the linear operations in the Transformer module to accelerate the forward procedure. Based on these two optimizations, extensive experiments show that MERGE can achieve a 26.5x speedup under the sequence length 512, and reduce 80\% communication bytes, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#22810;&#20010;&#31867;&#20284;&#20110;&#22522;&#30784;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25552;&#31034;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#65292;&#24182;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15689</link><description>&lt;p&gt;
&#20811;&#26381;&#25552;&#31034;&#25200;&#21160;&#25935;&#24863;&#24615;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts. (arXiv:2305.15689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#22810;&#20010;&#31867;&#20284;&#20110;&#22522;&#30784;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25552;&#31034;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#65292;&#24182;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#21487;&#20197;&#24110;&#21161;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#30693;&#35782;&#36827;&#34892;&#20108;&#20803;&#21477;&#32423;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#20351;&#29992;&#25163;&#21160;&#25110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25552;&#31034;&#26469;&#24494;&#35843;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#23545;&#25152;&#20351;&#29992;&#25552;&#31034;&#30340;&#25200;&#21160;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23569;&#37327;&#24102;&#26631;&#31614;&#23454;&#20363;&#36827;&#34892;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#25552;&#31034;&#25490;&#24207;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#20026;&#25152;&#32473;&#23450;&#30340;&#20219;&#21153;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#32473;&#23450;&#19968;&#20010;&#22522;&#30784;&#25552;&#31034;&#65292;&#37319;&#29992;&#20301;&#32622;&#12289;&#25512;&#29702;&#21644;&#37322;&#20041;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#22810;&#20010;&#31867;&#20284;&#20110;&#22522;&#30784;&#25552;&#31034;&#30340;&#25552;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#23545;&#25552;&#31034;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#20174;&#23454;&#39564;&#19978;&#35777;&#26126;&#65292;&#25490;&#21517;&#38752;&#21069;&#30340;&#25552;&#31034;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#25552;&#31034;&#25200;&#21160;&#40065;&#26834;&#24615;&#21644;&#25972;&#20307;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#30784;&#25552;&#31034;&#21644;&#20854;&#20182;&#29616;&#26377;&#30340;&#25552;&#31034;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated that natural-language prompts can help to leverage the knowledge learned by pre-trained language models for the binary sentence-level sentiment classification task. Specifically, these methods utilize few-shot learning settings to fine-tune the sentiment classification model using manual or automatically generated prompts. However, the performance of these methods is sensitive to the perturbations of the utilized prompts. Furthermore, these methods depend on a few labeled instances for automatic prompt generation and prompt ranking. This study aims to find high-quality prompts for the given task in a zero-shot setting. Given a base prompt, our proposed approach automatically generates multiple prompts similar to the base prompt employing positional, reasoning, and paraphrasing techniques and then ranks the prompts using a novel metric. We empirically demonstrate that the top-ranked prompts are high-quality and significantly outperform the base prompt an
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13681</link><description>&lt;p&gt;
GUARD: &#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13681
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#38169;&#30340;&#24615;&#36136;&#65292;&#23558;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#29616;&#23454;&#24212;&#29992;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#20154;&#26426;&#20132;&#20114;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#65289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26368;&#36817;&#65292;&#23433;&#20840;RL&#65288;&#21363;&#32422;&#26463;RL&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#36805;&#36895;&#20986;&#29616;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#25506;&#32034;&#29615;&#22659;&#12290;&#30001;&#20110;&#31639;&#27861;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#27604;&#36739;&#29616;&#26377;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GUARD&#65292;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;GUARD&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#20855;&#26377;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#12290;&#20854;&#27425;&#65292;GUARD&#20840;&#38754;&#28085;&#30422;&#20102;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#33258;&#21253;&#21547;&#30340;&#23454;&#29616;&#12290;&#31532;&#19977;&#65292;GUARD&#22312;&#20219;&#21153;&#21644;&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29366;&#24577;&#19979;&#29616;&#26377;&#26041;&#27861;&#22312;GUARD&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#21644;&#22522;&#20110;&#23545;&#35937;&#32423;&#21035;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#65292;&#21363;Restore Anything Pipeline (RAP)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#19982;&#21487;&#25511;&#30340;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20026;&#22810;&#20010;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.13093</link><description>&lt;p&gt;
Restore Anything Pipeline: Segment Anything Meets Image Restoration.
&lt;/p&gt;
&lt;p&gt;
Restore Anything Pipeline: Segment Anything Meets Image Restoration. (arXiv:2305.13093v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#21644;&#22522;&#20110;&#23545;&#35937;&#32423;&#21035;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#65292;&#21363;Restore Anything Pipeline (RAP)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#19982;&#21487;&#25511;&#30340;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20026;&#22810;&#20010;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#23558;&#25972;&#20010;&#22270;&#20687;&#35270;&#20026;&#19968;&#20010;&#21333;&#19968;&#23454;&#20307;&#65292;&#26080;&#27861;&#32771;&#34385;&#21040;&#22270;&#20687;&#20013;&#34920;&#29616;&#20986;&#20010;&#20307;&#32441;&#29702;&#23646;&#24615;&#30340;&#19981;&#21516;&#23545;&#35937;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#29983;&#25104;&#21333;&#19968;&#32467;&#26524;&#65292;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#19981;&#21516;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#21644;&#22522;&#20110;&#23545;&#35937;&#32423;&#21035;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#65292;&#21363;Restore Anything Pipeline (RAP)&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21487;&#25511;&#27169;&#22411;&#26469;&#29983;&#25104;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#30340;&#19981;&#21516;&#32467;&#26524;&#12290;RAP&#23558;&#26368;&#36817;&#30340;Segment Anything Model (SAM)&#30340;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#19982;&#21487;&#25511;&#30340;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20026;&#22810;&#20010;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;RAP&#24212;&#29992;&#20110;&#22270;&#20687;&#21435;&#27169;&#31946;&#12289;&#22270;&#20687;&#21435;&#22122;&#21644;JPEG&#20266;&#24433;&#21435;&#38500;&#36825;&#19977;&#20010;&#24120;&#35265;&#30340;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#26469;&#23637;&#31034;&#20854;&#22810;&#21151;&#33021;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RAP&#33021;&#22815;&#22312;&#19981;&#21516;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent image restoration methods have produced significant advancements using deep learning. However, existing methods tend to treat the whole image as a single entity, failing to account for the distinct objects in the image that exhibit individual texture properties. Existing methods also typically generate a single result, which may not suit the preferences of different users. In this paper, we introduce the Restore Anything Pipeline (RAP), a novel interactive and per-object level image restoration approach that incorporates a controllable model to generate different results that users may choose from. RAP incorporates image segmentation through the recent Segment Anything Model (SAM) into a controllable image restoration model to create a user-friendly pipeline for several image restoration tasks. We demonstrate the versatility of RAP by applying it to three common image restoration tasks: image deblurring, image denoising, and JPEG artifact removal. Our experiments show that RAP p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22914;&#20309;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;</title><link>http://arxiv.org/abs/2305.06174</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27668;&#20505;&#23459;&#20256;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22914;&#20309;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#26159;&#25105;&#20204;&#26102;&#20195;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#25105;&#20204;&#27491;&#22788;&#20110;&#19968;&#20010;&#20851;&#38190;&#26102;&#21051;&#12290;&#21508;&#31181;&#21033;&#30410;&#38598;&#22242;&#12289;&#31038;&#20250;&#36816;&#21160;&#32452;&#32455;&#21644;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24320;&#23637;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#38598;&#20307;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38382;&#39064;&#20513;&#23548;&#27963;&#21160;&#24448;&#24448;&#26159;&#38024;&#23545;&#24403;&#21069;&#31038;&#20250;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#33021;&#28304;&#34892;&#19994;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20998;&#26512;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22914;&#20309;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#28040;&#24687;&#20027;&#39064;&#26469;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#31435;&#22330;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#19982;&#27668;&#20505;&#23459;&#20256;&#27963;&#21160;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#20379;&#26410;&#26469;&#30340;&#33286;&#24773;&#25366;&#25496;&#21644;&#33258;&#21160;&#26816;&#27979;&#27668;&#20505;&#21464;&#21270;&#31435;&#22330;&#30340;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change is the defining issue of our time, and we are at a defining moment. Various interest groups, social movement organizations, and individuals engage in collective action on this issue on social media. In addition, issue advocacy campaigns on social media often arise in response to ongoing societal concerns, especially those faced by energy industries. Our goal in this paper is to analyze how those industries, their advocacy group, and climate advocacy group use social media to influence the narrative on climate change. In this work, we propose a minimally supervised model soup [56] approach combined with messaging themes to identify the stances of climate ads on Facebook. Finally, we release our stance dataset, model, and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances.
&lt;/p&gt;</description></item><item><title>SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;SAM&#21333;&#29420;&#20351;&#29992;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05392</link><description>&lt;p&gt;
&#20851;&#20110;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#19982;&#23545;&#25239;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Relation between Sharpness-Aware Minimization and Adversarial Robustness. (arXiv:2305.05392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05392
&lt;/p&gt;
&lt;p&gt;
SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;SAM&#21333;&#29420;&#20351;&#29992;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#23545;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#65288;SAM&#65289;&#30340;&#26032;&#29702;&#35299;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;&#22312;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#22768;&#26126;&#30340;&#29702;&#35770;&#35777;&#25454;&#21644;&#20005;&#26684;&#30340;&#25968;&#23398;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#21033;&#29992;SAM&#21487;&#20197;&#23454;&#29616;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#24847;&#22806;&#30340;&#22909;&#22788;&#12290;&#30001;&#20110;&#23545;&#25239;&#35757;&#32451;&#21487;&#33021;&#20250;&#23548;&#33268;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#38477;&#20302;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;SAM&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/weizeming/SAM_AT&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel understanding of Sharpness-Aware Minimization (SAM) in the context of adversarial robustness. In this paper, we point out that both SAM and adversarial training (AT) can be viewed as specific feature perturbations, which improve adversarial robustness. However, we note that SAM and AT are distinct in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model with rigorous mathematical proofs. Furthermore, we conduct experiment to demonstrate that only utilizing SAM can achieve superior adversarial robustness compared to standard training, which is an unexpected benefit. As adversarial training can suffer from a decrease in clean accuracy, we show that using SAM alone can improve robustness without sacrificing clean accuracy. Code is available at https://github.com/weizeming/SAM_AT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312; COVID-19 &#30123;&#24773;&#26399;&#38388;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21475;&#32617;&#20329;&#25140;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#65292;&#36873;&#25321;&#20102;&#36866;&#29992;&#20110;&#23454;&#26102;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.00068</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21475;&#32617;&#20329;&#25140;&#26816;&#27979;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Wearing face mask detection using deep learning through COVID-19 pandemic. (arXiv:2305.00068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312; COVID-19 &#30123;&#24773;&#26399;&#38388;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21475;&#32617;&#20329;&#25140;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#65292;&#36873;&#25321;&#20102;&#36866;&#29992;&#20110;&#23454;&#26102;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;COVID-19&#30123;&#24773;&#26399;&#38388;&#65292;&#20329;&#25140;&#21475;&#32617;&#24050;&#34987;&#30693;&#26195;&#20026;&#39044;&#38450;&#30149;&#27602;&#20256;&#25773;&#30340;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#21462;&#20195;&#20102;&#20154;&#31867;&#22312;&#35768;&#22810;&#30417;&#25511;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#12290;&#30417;&#25511;&#21475;&#32617;&#20329;&#25140;&#23601;&#26159;&#36825;&#26679;&#19968;&#39033;&#21487;&#20197;&#30001;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290; &#30001;&#20110;&#38548;&#31163;&#30340;&#21407;&#22240;&#65292;&#38754;&#37096;&#21475;&#32617;&#29031;&#29255;&#30340;&#25968;&#37327;&#26377;&#38480;&#65292;&#22240;&#27492;&#26159;&#36825;&#39033;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#20351;&#29992;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#21475;&#32617;&#26816;&#27979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21253;&#25324; Single Shot Detector&#65288;SSD&#65289;&#12289;&#20004;&#20010;&#29256;&#26412;&#30340; You Only Look Once&#12290;&#26681;&#25454;&#19981;&#21516;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#22312;&#29616;&#23454;&#19990;&#30028;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#29992;&#20110;&#23454;&#26102;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#30340;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the COVID-19 pandemic, wearing a face mask has been known to be an effective way to prevent the spread of COVID-19. In lots of monitoring tasks, humans have been replaced with computers thanks to the outstanding performance of the deep learning models. Monitoring the wearing of a face mask is another task that can be done by deep learning models with acceptable accuracy. The main challenge of this task is the limited amount of data because of the quarantine. In this paper, we did an investigation on the capability of three state-of-the-art object detection neural networks on face mask detection for real-time applications. As mentioned, here are three models used, Single Shot Detector (SSD), two versions of You Only Look Once (YOLO) i.e., YOLOv4-tiny, and YOLOv4-tiny-3l from which the best was selected. In the proposed method, according to the performance of different models, the best model that can be suitable for use in real-world and mobile device applications in comparison to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;ChartSpark&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23558;&#35821;&#20041;&#19978;&#19979;&#25991;&#23884;&#20837;&#21040;&#22270;&#34920;&#20013;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#35821;&#20041;&#19978;&#19979;&#25991;&#30340;&#22270;&#31034;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.14630</link><description>&lt;p&gt;
&#35753;&#22270;&#34920;&#38378;&#20142;&#36215;&#26469;&#65306;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23558;&#35821;&#20041;&#19978;&#19979;&#25991;&#23884;&#20837;&#21040;&#22270;&#34920;&#20013;
&lt;/p&gt;
&lt;p&gt;
Let the Chart Spark: Embedding Semantic Context into Chart with Text-to-Image Generative Model. (arXiv:2304.14630v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;ChartSpark&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23558;&#35821;&#20041;&#19978;&#19979;&#25991;&#23884;&#20837;&#21040;&#22270;&#34920;&#20013;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#35821;&#20041;&#19978;&#19979;&#25991;&#30340;&#22270;&#31034;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31034;&#21487;&#35270;&#21270;&#23558;&#25968;&#25454;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#33391;&#22909;&#22320;&#25972;&#21512;&#21040;&#35270;&#35273;&#34920;&#29616;&#20013;&#65292;&#20197;&#19968;&#31181;&#26082;&#24341;&#20154;&#21448;&#20449;&#24687;&#37327;&#22823;&#30340;&#26041;&#24335;&#20256;&#36798;&#22797;&#26434;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31995;&#32479;ChartSpark&#65292;&#23427;&#22522;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23558;&#35821;&#20041;&#19978;&#19979;&#25991;&#23884;&#20837;&#22312;&#22270;&#34920;&#20013;&#29983;&#25104;&#22270;&#31034;&#21270;&#30340;&#21487;&#35270;&#21270;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21069;&#26223;&#21644;&#32972;&#26223;&#30340;&#22270;&#31034;&#29983;&#25104;&#65292;&#26088;&#22312;&#21019;&#36896;&#20855;&#26377;&#35821;&#20041;&#19978;&#19979;&#25991;&#30340;&#39640;&#36136;&#37327;&#22270;&#31034;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pictorial visualization seamlessly integrates data and semantic context into visual representation, conveying complex information in a manner that is both engaging and informative. Extensive studies have been devoted to developing authoring tools to simplify the creation of pictorial visualizations. However, mainstream works mostly follow a retrieving-and-editing pipeline that heavily relies on retrieved visual elements from a dedicated corpus, which often compromise the data integrity. Text-guided generation methods are emerging, but may have limited applicability due to its predefined recognized entities. In this work, we propose ChartSpark, a novel system that embeds semantic context into chart based on text-to-image generative model. ChartSpark generates pictorial visualizations conditioned on both semantic context conveyed in textual inputs and data information embedded in plain charts. The method is generic for both foreground and background pictorial generation, satisfying the d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#21363;&#20415;&#20351;&#29992;FedMD&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#20173;&#23384;&#22312;&#34987;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#21033;&#29992;&#30340;&#39118;&#38505;&#65292;&#22914;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#65292;&#20250;&#23548;&#33268;&#38544;&#31169;&#25968;&#25454;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2304.11436</link><description>&lt;p&gt;
&#36890;&#36807;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#24674;&#22797;&#22270;&#20687;&#30340;FedMD
&lt;/p&gt;
&lt;p&gt;
Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack. (arXiv:2304.11436v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#21363;&#20415;&#20351;&#29992;FedMD&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#20173;&#23384;&#22312;&#34987;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#21033;&#29992;&#30340;&#39118;&#38505;&#65292;&#22914;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#65292;&#20250;&#23548;&#33268;&#38544;&#31169;&#25968;&#25454;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;&#27169;&#22411;&#33976;&#39311;&#65288;FedMD&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#21327;&#20316;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#20165;&#20256;&#36755;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986;logits&#20316;&#20026;&#33976;&#39311;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#20256;&#36882;&#26131;&#21463;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#31169;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24050;&#30693;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#21363;&#20351;&#20849;&#20139;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986; logit&#27604;&#30452;&#25509;&#20849;&#20139;&#26799;&#24230;&#26356;&#23433;&#20840;&#65292;&#20173;&#23384;&#22312;&#22240;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#23548;&#33268;&#30340;&#25968;&#25454;&#26333;&#20809;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#21453;&#28436;&#31070;&#32463;&#32593;&#32476;&#26469;&#21033;&#29992;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#32622;&#20449;&#24230;&#24046;&#65292;&#38024;&#23545;FedMD&#21450;&#20854;&#21464;&#31181;&#36827;&#34892;PLI&#65288;&#37197;&#23545;logits&#21453;&#28436;&#65289;&#25915;&#20987;&#12290;&#22312;&#22810;&#20010;&#20154;&#33080;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31867;&#20284;&#20110;FedMD&#30340;&#26041;&#26696;&#20013;&#65292;&#20165;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#37197;&#23545;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;logits&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#33021;&#22815;&#37325;&#26500;&#31169;&#26377;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Model Distillation (FedMD) is a nascent collaborative learning paradigm, where only output logits of public datasets are transmitted as distilled knowledge, instead of passing on private model parameters that are susceptible to gradient inversion attacks, a known privacy risk in federated learning. In this paper, we found that even though sharing output logits of public datasets is safer than directly sharing gradients, there still exists a substantial risk of data exposure caused by carefully designed malicious attacks. Our study shows that a malicious server can inject a PLI (Paired-Logits Inversion) attack against FedMD and its variants by training an inversion neural network that exploits the confidence gap between the server and client models. Experiments on multiple facial recognition datasets validate that under FedMD-like schemes, by using paired server-client logits of public datasets only, the malicious server is able to reconstruct private images on a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#21442;&#25968;&#20449;&#24687;&#24182;&#20174;&#20043;&#21069;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26426;&#22120;&#20154;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#23558;&#35813;&#31639;&#27861;&#19982;Adversarial Motion Priors&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#27493;&#24577;&#65292;&#21487;&#29992;&#20110;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#12290;</title><link>http://arxiv.org/abs/2304.10888</link><description>&lt;p&gt;
&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;AMP&#65306;&#23398;&#20064;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#26377;&#33151;&#31227;&#21160;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
AMP in the wild: Learning robust, agile, natural legged locomotion skills. (arXiv:2304.10888v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#21442;&#25968;&#20449;&#24687;&#24182;&#20174;&#20043;&#21069;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26426;&#22120;&#20154;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#23558;&#35813;&#31639;&#27861;&#19982;Adversarial Motion Priors&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#27493;&#24577;&#65292;&#21487;&#29992;&#20110;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#20010;&#23398;&#20064;&#25511;&#21046;&#22120;&#20174;&#20223;&#30495;&#29615;&#22659;&#36716;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#38656;&#35201;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#31995;&#32479;&#65292;&#32780;&#19988;&#36824;&#38656;&#35201;&#20934;&#30830;&#22320;&#20272;&#35745;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#21442;&#25968;&#20449;&#24687;&#65292;&#36824;&#21487;&#20197;&#20174;&#20043;&#21069;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26426;&#22120;&#20154;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#19982;Adversarial Motion Priors&#30456;&#32467;&#21512;&#65292;&#22312;&#20223;&#30495;&#21644;&#22312;Unitree A1&#22235;&#36275;&#26426;&#22120;&#20154;&#30495;&#23454;&#19990;&#30028;&#20013;&#23454;&#29616;&#20102;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#27493;&#24577;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#20197;&#26356;&#20302;&#30340;&#21151;&#32791;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The successful transfer of a learned controller from simulation to the real world for a legged robot requires not only the ability to identify the system, but also accurate estimation of the robot's state. In this paper, we propose a novel algorithm that can infer not only information about the parameters of the dynamic system, but also estimate important information about the robot's state from previous observations. We integrate our algorithm with Adversarial Motion Priors and achieve a robust, agile, and natural gait in both simulation and on a Unitree A1 quadruped robot in the real world. Empirical results demonstrate that our proposed algorithm enables traversing challenging terrains with lower power consumption compared to the baselines. Both qualitative and quantitative results are presented in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07702</link><description>&lt;p&gt;
&#29992;BREC&#25968;&#25454;&#38598;&#26356;&#22909;&#22320;&#35780;&#20272;GNN&#34920;&#36798;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29702;&#35770;&#34920;&#36798;&#21147;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22686;&#24378;&#34920;&#36798;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20005;&#26684;&#36981;&#24490;k&#32500;Weisfeiler-Lehman&#65288;k-WL&#65289;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#30340;&#23569;&#25968;&#26041;&#27861;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#27809;&#26377;&#32479;&#19968;&#30340;&#34920;&#36798;&#21147;&#24230;&#37327;&#12290;&#23427;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38480;&#20110;&#21306;&#20998;&#26576;&#20123;&#38750;&#21516;&#26500;&#22270;&#26063;&#65292;&#23548;&#33268;&#22312;&#23450;&#37327;&#27604;&#36739;&#34920;&#36798;&#21147;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#29702;&#35770;&#20998;&#26512;&#30456;&#21453;&#65292;&#34913;&#37327;&#34920;&#36798;&#33021;&#21147;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21253;&#21547;1-WL&#19981;&#21487;&#21306;&#20998;&#22270;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#25968;&#25454;&#38598;&#38754;&#20020;&#30528;&#38590;&#24230;&#65288;&#20219;&#20309;&#36229;&#36234;1-WL&#30340;&#27169;&#22411;&#20934;&#30830;&#29575;&#20960;&#20046;&#36798;&#21040;100&#65285;&#65289;&#12289;&#31890;&#24230;&#65288;&#27169;&#22411;&#20542;&#21521;&#20110;&#35201;&#20040;&#23436;&#20840;&#27491;&#30830;&#65292;&#35201;&#20040;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#65289;&#21644;&#35268;&#27169;&#65288;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#23569;&#37327;&#26412;&#36136;&#19981;&#21516;&#30340;&#22270;&#65289;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GNN&#40065;&#26834;&#24615;&#35780;&#20272;&#22522;&#20934;&#65288;BREC&#65289;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#35768;&#22810;&#32467;&#26500;&#22810;&#26679;&#30340;&#22270;&#65292;&#24182;&#20801;&#35768;&#23545;&#27169;&#22411;&#34920;&#36798;&#21147;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;2022&#24180;&#22522;&#22240;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#20030;&#21150;&#30340;&#31454;&#36187;&#65292;&#35780;&#20272;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01117</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#21487;&#35299;&#37322;&#31526;&#21495;&#22238;&#24402;&#65306;2022&#24180;&#31454;&#36187;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition. (arXiv:2304.01117v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;2022&#24180;&#22522;&#22240;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#20030;&#21150;&#30340;&#31454;&#36187;&#65292;&#35780;&#20272;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#23547;&#25214;&#33021;&#22815;&#20934;&#30830;&#25551;&#36848;&#30740;&#31350;&#29616;&#35937;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#36820;&#22238;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#32473;&#29992;&#25143;&#25552;&#20379;&#28145;&#21051;&#30340;&#35265;&#35299;&#12290;&#21382;&#21490;&#19978;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#37117;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#22823;&#37327;&#26032;&#30340;&#25552;&#26696;&#65292;&#36825;&#20123;&#25552;&#26696;&#20351;&#29992;&#20102;&#21015;&#20030;&#31639;&#27861;&#12289;&#28151;&#21512;&#32447;&#24615;&#25972;&#25968;&#35268;&#21010;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#19968;&#32452;&#24120;&#35265;&#25361;&#25112;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#25105;&#20204;&#22312;2022&#24180;&#36951;&#20256;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#19978;&#20030;&#21150;&#20102;&#19968;&#27425;&#31454;&#36187;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#21516;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#21442;&#36187;&#32773;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#30450;&#27979;&#35797;&#30340;&#12290;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39046;&#22495;&#19987;&#23478;&#26469;&#35780;&#20272;&#20505;&#36873;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression searches for analytic expressions that accurately describe studied phenomena. The main attraction of this approach is that it returns an interpretable model that can be insightful to users. Historically, the majority of algorithms for symbolic regression have been based on evolutionary algorithms. However, there has been a recent surge of new proposals that instead utilize approaches such as enumeration algorithms, mixed linear integer programming, neural networks, and Bayesian optimization. In order to assess how well these new approaches behave on a set of common challenges often faced in real-world data, we hosted a competition at the 2022 Genetic and Evolutionary Computation Conference consisting of different synthetic and real-world datasets which were blind to entrants. For the real-world track, we assessed interpretability in a realistic way by using a domain expert to judge the trustworthiness of candidate models.We present an in-depth analysis of the result
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;Regional Point-Language Contrastive Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#21644;&#28857;&#29420;&#31435;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#23545;&#26032;&#39062;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.00962</link><description>&lt;p&gt;
RegionPLC&#65306;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#21306;&#22495;&#28857;-&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding. (arXiv:2304.00962v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;Regional Point-Language Contrastive Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#21644;&#28857;&#29420;&#31435;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#23545;&#26032;&#39062;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#22312;&#38381;&#38598;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26080;&#27861;&#22788;&#29702;&#26032;&#39062;&#31867;&#21035;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RegionPLC&#30340;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#21306;&#22495;&#28857;-&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#32463;&#36807;&#23553;&#38381;&#38598;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#22791;&#24320;&#25918;&#35789;&#27719;&#35782;&#21035;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#36890;&#36807;&#26631;&#39064;&#29983;&#25104;&#20174;2D&#22522;&#30784;&#27169;&#22411;&#20013;&#24341;&#21457;&#21306;&#22495;&#32423;&#35270;&#35273;-&#35821;&#35328;&#30693;&#35782;&#65292;&#36827;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#24314;&#31435;&#23494;&#38598;&#30340;&#21306;&#22495;&#28857;-&#35821;&#35328;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28857;&#21028;&#21035;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#20351;&#24471;&#20174;&#26631;&#39064;&#20013;&#36827;&#34892;&#28857;&#29420;&#31435;&#23398;&#20064;&#20197;&#23454;&#29616;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;ScanNet&#12289;ScanNet200&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#30456;&#27604;&#20043;&#21069;&#30340;&#22522;&#20110;&#27880;&#37322;&#30340;3D&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#29702;&#35299;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;RegionPLC&#22312;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#26041;&#38754;&#30340;&#24615;&#33021;&#24179;&#22343;&#25552;&#39640;&#20102;11.6%&#21644;6.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing 3D scene understanding tasks have achieved high performance on close-set benchmarks but fail to handle novel categories in real-world applications. To this end, we propose a Regional Point-Language Contrastive learning framework, namely RegionPLC, for open-world 3D scene understanding, which equips models trained on closed-set datasets with open-vocabulary recognition capabilities. We propose dense visual prompts to elicit region-level visual-language knowledge from 2D foundation models via captioning, which further allows us to build dense regional point-language associations. Then, we design a point-discriminative contrastive learning objective to enable point-independent learning from captions for dense scene understanding. We conduct extensive experiments on ScanNet, ScanNet200, and nuScenes datasets. Our RegionPLC significantly outperforms previous base-annotated 3D open-world scene understanding approaches by an average of 11.6\% and 6.6\% for semantic and instance segme
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.12040</link><description>&lt;p&gt;
&#21327;&#21516;&#20154;&#24037;&#26234;&#33021;&#30340;&#26681;&#28304;&#21644;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Roots and Requirements for Collaborative AI. (arXiv:2303.12040v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12040
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21327;&#20316;&#32773;&#30340;&#24895;&#26223;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31185;&#24187;&#23567;&#35828;&#30340;&#32463;&#20856;&#32032;&#26448;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#21327;&#20316;&#21644;&#20154;&#31867;&#27807;&#36890;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#23427;&#20204;&#36890;&#36807;&#36129;&#29486;&#29305;&#27530;&#30340;&#25165;&#33021;&#32473;&#20182;&#20204;&#30340;&#20154;&#31867;&#21512;&#20316;&#32773;&#21644;&#22242;&#38431;&#24102;&#26469;&#20248;&#21183;&#12290;&#22810;&#24180;&#26469;&#65292;&#25919;&#24220;&#21672;&#35810;&#22242;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#39046;&#34966;&#19968;&#30452;&#20513;&#23548;AIs&#24212;&#35813;&#20855;&#26377;&#20154;&#31867;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#21327;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20855;&#22791;&#20687;&#25165;&#21326;&#27178;&#28322;&#30340;&#20154;&#37027;&#26679;&#21327;&#20316;&#33021;&#21147;&#30340;&#24378;&#22823;&#30340;AI&#20173;&#28982;&#36965;&#19981;&#21487;&#21450;&#12290;&#36825;&#31687;&#35770;&#25991;&#20381;&#25454;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20195;&#29702;&#26377;&#25928;&#21644;&#24378;&#22823;&#21327;&#20316;&#25152;&#38656;&#35748;&#30693;&#30340;&#20998;&#26512;&#65292;&#27010;&#36848;&#20102;&#20844;&#20247;&#21644;AI&#24895;&#26223;&#20013;&#20851;&#20110;&#20154;&#24037;&#21327;&#20316;&#32773;&#30340;&#21382;&#21490;&#65292;&#24320;&#22987;&#20110;&#26089;&#26399;&#26234;&#33021;&#22686;&#24378;(IA)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#24895;&#26223;&#12290;&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#25104;&#20026;&#21327;&#21516;AI&#30340;&#31532;&#20108;&#20010;&#31435;&#22330;&#25991;&#20214;(Stefik &amp; Price, 2023)&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;&#31532;&#20108;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#22810;&#23398;&#31185;&#30340;&#29616;&#29366;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#21327;&#20316;&#30740;&#31350;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vision of AI collaborators has long been a staple of science fiction, where artificial agents understand nuances of collaboration and human communication. They bring advantages to their human collaborators and teams by contributing their special talents. Government advisory groups and leaders in AI have advocated for years that AIs should be human compatible and be capable of effective collaboration. Nonetheless, robust AIs that can collaborate like talented people remain out of reach. This position paper draws on a cognitive analysis of what effective and robust collaboration requires of human and artificial agents. It sketches a history of public and AI visions for artificial collaborators, starting with early visions of intelligence augmentation (IA) and artificial intelligence (AI). It is intended as motivation and context for a second position paper on collaborative AI (Stefik &amp; Price, 2023). The second paper reviews the multi-disciplinary state-of-the-art and proposes a roadm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#36719;Actor-Critic&#65288;SAC&#65289;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#29366;&#24577;&#30456;&#20851;&#30340;&#26494;&#24347;&#21464;&#37327;&#26469;&#36866;&#24403;&#22788;&#29702;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#12290;&#36825;&#23545;&#20110;&#22686;&#24378;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.04356</link><description>&lt;p&gt;
&#20855;&#26377;&#30495;&#27491;&#28385;&#36275;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#36719;Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Soft Actor-Critic Algorithm with Truly-satisfied Inequality Constraint. (arXiv:2303.04356v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#36719;Actor-Critic&#65288;SAC&#65289;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#29366;&#24577;&#30456;&#20851;&#30340;&#26494;&#24347;&#21464;&#37327;&#26469;&#36866;&#24403;&#22788;&#29702;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#12290;&#36825;&#23545;&#20110;&#22686;&#24378;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36719;Actor-Critic&#65288;SAC&#65289;&#34987;&#35748;&#20026;&#26159;&#19979;&#19968;&#20195;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#26696;&#20043;&#19968;&#12290;&#20854;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#30340;&#33021;&#21147;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#23545;&#22122;&#22768;&#21644;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#21069;&#30340;&#23454;&#29616;&#20013;&#65292;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#30340;&#20248;&#20808;&#32423;&#26159;&#33258;&#21160;&#35843;&#33410;&#30340;&#65292;&#20854;&#35268;&#21017;&#21487;&#20197;&#35299;&#37322;&#20026;&#31561;&#24335;&#32422;&#26463;&#65292;&#23558;&#31574;&#30053;&#29109;&#32465;&#23450;&#21040;&#25351;&#23450;&#30340;&#19979;&#30028;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;SAC&#19981;&#20877;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#65292;&#19982;&#25105;&#20204;&#30340;&#26399;&#26395;&#30456;&#21453;&#12290;&#20026;&#20102;&#35299;&#20915;SAC&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25913;&#36827;&#20102;&#20854;&#23454;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#29366;&#24577;&#30456;&#20851;&#30340;&#26494;&#24347;&#21464;&#37327;&#65292;&#20197;&#36866;&#24403;&#22788;&#29702;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#36890;&#36807;&#23558;&#20854;&#37325;&#26032;&#21046;&#23450;&#20026;&#30456;&#24212;&#30340;&#31561;&#24335;&#32422;&#26463;&#26469;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#12290;&#24341;&#20837;&#30340;&#26494;&#24347;&#21464;&#37327;&#36890;&#36807;&#32771;&#34385;&#28385;&#36275;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#21452;&#37325;&#30446;&#26631;&#30340;&#20999;&#25442;&#22411;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft actor-critic (SAC) in reinforcement learning is expected to be one of the next-generation robot control schemes. Its ability to maximize policy entropy would make a robotic controller robust to noise and perturbation, which is useful for real-world robot applications. However, the priority of maximizing the policy entropy is automatically tuned in the current implementation, the rule of which can be interpreted as one for equality constraint, binding the policy entropy into its specified lower bound. The current SAC is therefore no longer maximize the policy entropy, contrary to our expectation. To resolve this issue in SAC, this paper improves its implementation with a learnable state-dependent slack variable for appropriately handling the inequality constraint to maximize the policy entropy by reformulating it as the corresponding equality constraint. The introduced slack variable is optimized by a switching-type loss function that takes into account the dual objectives of satis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#25191;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#65292;&#24182;&#25104;&#21151;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2302.14229</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Cross-Lingual Summarization via Large Language Models. (arXiv:2302.14229v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#25191;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#65292;&#24182;&#25104;&#21151;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#28304;&#35821;&#35328;&#25991;&#26412;&#65292;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;CLS&#65289;&#26088;&#22312;&#29983;&#25104;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#27604;&#22914;GPT-3.5&#12289;ChatGPT&#21644;GPT-4&#65292;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;LLM&#22312;CLS&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;LLM&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#65288;&#21363;&#31471;&#21040;&#31471;&#21644;&#27969;&#27700;&#32447;&#65289;&#25191;&#34892;&#38646;&#26679;&#26412;CLS&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#25688;&#35201;&#36827;&#34892;&#21021;&#27493;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#21644;GPT-4&#21407;&#26412;&#26356;&#21916;&#27426;&#29983;&#25104;&#35814;&#32454;&#20449;&#24687;&#30340;&#38271;&#25688;&#35201;&#12290;&#20294;&#36825;&#20004;&#20010;LLM&#22312;&#20132;&#20114;&#24335;&#25552;&#31034;&#30340;&#24110;&#21161;&#19979;&#21487;&#20197;&#36827;&#19968;&#27493;&#24179;&#34913;&#20449;&#24687;&#37327;&#21644;&#31616;&#27905;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;CLS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20855;&#26377;&#21487;&#23398;&#20064;&#21644;&#26368;&#20248;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#21644;&#35745;&#31639;&#26368;&#20248;&#22522;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#22312;&#27169;&#22411;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.12432</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#23398;&#20064;&#21644;&#26368;&#20248;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks with Learnable and Optimal Polynomial Bases. (arXiv:2302.12432v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20855;&#26377;&#21487;&#23398;&#20064;&#21644;&#26368;&#20248;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#21644;&#35745;&#31639;&#26368;&#20248;&#22522;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#22312;&#27169;&#22411;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#26159;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#24120;&#20351;&#29992;&#39044;&#23450;&#30340;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#65292;&#24182;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#31995;&#25968;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#30340;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#20204;&#33021;&#22815;&#32943;&#23450;&#22238;&#31572;&#19978;&#36848;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#21463;&#21040;Favard&#23450;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FavardGNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#25152;&#26377;&#21487;&#33021;&#30340;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#23398;&#20064;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Wang&#21644;Zhang&#65288;2022&#24180;&#65289;&#25552;&#20986;&#30340;&#25152;&#35859;&#26080;&#27861;&#35299;&#20915;&#30340;&#26368;&#20248;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;OptBasisGNN&#65292;&#21487;&#35745;&#31639;&#32473;&#23450;&#22270;&#32467;&#26500;&#21644;&#22270;&#20449;&#21495;&#30340;&#26368;&#20248;&#22522;&#20989;&#25968;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polynomial filters, a kind of Graph Neural Networks, typically use a predetermined polynomial basis and learn the coefficients from the training data. It has been observed that the effectiveness of the model is highly dependent on the property of the polynomial basis. Consequently, two natural and fundamental questions arise: Can we learn a suitable polynomial basis from the training data? Can we determine the optimal polynomial basis for a given graph and node features?  In this paper, we propose two spectral GNN models that provide positive answers to the questions posed above. First, inspired by Favard's Theorem, we propose the FavardGNN model, which learns a polynomial basis from the space of all possible orthonormal bases. Second, we examine the supposedly unsolvable definition of optimal polynomial basis from Wang &amp; Zhang (2022) and propose a simple model, OptBasisGNN, which computes the optimal basis for a given graph structure and graph signal. Extensive experiments are conduct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#20869;&#22312;&#21160;&#26426;&#31639;&#27861;&#31867;&#21035;SND&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25506;&#32034;&#22256;&#38590;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.11563</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#21033;&#29992;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration by self-supervised exploitation. (arXiv:2302.11563v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#20869;&#22312;&#21160;&#26426;&#31639;&#27861;&#31867;&#21035;SND&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25506;&#32034;&#22256;&#38590;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#26681;&#25454;&#39044;&#20808;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#22312;&#29615;&#22659;&#20013;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#22870;&#21169;&#36807;&#20110;&#31232;&#30095;&#65292;&#20195;&#29702;&#22312;&#29615;&#22659;&#25506;&#32034;&#20013;&#19981;&#20250;&#36935;&#21040;&#22870;&#21169;&#65292;&#36825;&#31181;&#26041;&#27861;&#23601;&#21464;&#24471;&#38750;&#24120;&#26840;&#25163;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#21487;&#33021;&#26159;&#20026;&#20195;&#29702;&#35013;&#22791;&#20869;&#22312;&#21160;&#26426;&#65292;&#36825;&#26679;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#20063;&#21487;&#33021;&#36935;&#21040;&#22806;&#37096;&#22870;&#21169;&#12290;&#26032;&#39062;&#24615;&#26816;&#27979;&#26159;&#20869;&#22312;&#21160;&#26426;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#20998;&#25903;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#35823;&#24046;&#20316;&#20026;&#26032;&#39062;&#24615;&#25351;&#26631;&#30340;&#33258;&#25105;&#30417;&#30563;&#32593;&#32476;&#33976;&#39311;&#65288;SND&#65289;&#31639;&#27861;&#31867;&#21035;&#65292;&#20854;&#20013;&#30446;&#26631;&#27169;&#22411;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#20026;&#27492;&#25913;&#32534;&#20102;&#19977;&#31181;&#29616;&#26377;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#34987;&#35748;&#20026;&#38590;&#20197;&#25506;&#32034;&#30340;&#21313;&#20010;&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning can solve decision-making problems and train an agent to behave in an environment according to a predesigned reward function. However, such an approach becomes very problematic if the reward is too sparse and the agent does not come across the reward during the environmental exploration. The solution to such a problem may be in equipping the agent with an intrinsic motivation, which will provide informed exploration, during which the agent is likely to also encounter external reward. Novelty detection is one of the promising branches of intrinsic motivation research. We present Self-supervised Network Distillation (SND), a class of internal motivation algorithms based on the distillation error as a novelty indicator, where the target model is trained using self-supervised learning. We adapted three existing self-supervised methods for this purpose and experimentally tested them on a set of ten environments that are considered difficult to explore. The results sho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20284;&#28982;&#36864;&#28779;&#30340;&#24555;&#36895;&#26657;&#20934;&#22238;&#24402;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25913;&#36827;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#24182;&#20135;&#29983;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2302.11012</link><description>&lt;p&gt;
&#21487;&#29992;&#20110;&#22238;&#24402;&#30340;&#24555;&#36895;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#20284;&#28982;&#36864;&#28779;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Likelihood Annealing: Fast Calibrated Uncertainty for Regression. (arXiv:2302.11012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11012
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20284;&#28982;&#36864;&#28779;&#30340;&#24555;&#36895;&#26657;&#20934;&#22238;&#24402;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25913;&#36827;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#24182;&#20135;&#29983;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#34920;&#26126;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22312;&#21307;&#23398;&#24433;&#20687;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#20027;&#31995;&#32479;&#31561;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#36755;&#20986;&#31354;&#38388;&#36830;&#32493;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#12290;&#20801;&#35768;&#22238;&#24402;&#38382;&#39064;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#24182;&#20135;&#29983;&#19981;&#33391;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#19981;&#33021;&#26377;&#25928;&#29992;&#20110;&#37327;&#21270;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#20107;&#21518;&#26657;&#20934;&#25216;&#26415;&#24456;&#23569;&#36866;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#19988;&#24120;&#24120;&#32473;&#24050;&#32463;&#36739;&#24930;&#30340;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#22686;&#21152;&#20102;&#39069;&#22806;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#30340;&#24555;&#36895;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;&#20284;&#28982;&#36864;&#28779;&#65292;&#23427;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#27809;&#26377;&#20219;&#20309;&#20107;&#21518;&#26657;&#20934;&#38454;&#27573;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have shown that uncertainty estimation is becoming increasingly important in applications such as medical imaging, natural language processing, and autonomous systems. However, accurately quantifying uncertainty remains a challenging problem, especially in regression tasks where the output space is continuous. Deep learning approaches that allow uncertainty estimation for regression problems often converge slowly and yield poorly calibrated uncertainty estimates that can not be effectively used for quantification. Recently proposed post hoc calibration techniques are seldom applicable to regression problems and often add overhead to an already slow model training phase. This work presents a fast calibrated uncertainty estimation method for regression tasks called Likelihood Annealing, that consistently improves the convergence of deep regression models and yields calibrated uncertainty without any post hoc calibration phase. Unlike previous methods for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26080;&#27861;&#20998;&#26512;&#26410;&#30693;&#29305;&#24449;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.10894</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#21512;&#25104;&#24037;&#20855;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32418;&#38431;&#28436;&#32451;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Deep Neural Networks with Feature Synthesis Tools. (arXiv:2302.10894v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26080;&#27861;&#20998;&#26512;&#26410;&#30693;&#29305;&#24449;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36890;&#24120;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#23613;&#31649;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#22312;&#36825;&#20123;&#24037;&#20855;&#20013;&#24456;&#23569;&#26377;&#33021;&#22815;&#21457;&#29616;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#12289;&#20197;&#21069;&#26410;&#30693;&#30340;&#38169;&#35823;&#30340;&#26696;&#20363;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#20849;&#21516;&#29305;&#28857;&#65306;&#23427;&#20204;&#20351;&#29992;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#34429;&#28982;&#36825;&#24456;&#26377;&#29992;&#65292;&#20294;&#36825;&#20123;&#24037;&#20855;&#21482;&#33021;&#20998;&#26512;&#29992;&#25143;&#21487;&#20197;&#20107;&#20808;&#37319;&#26679;&#25110;&#35782;&#21035;&#30340;&#29305;&#24449;&#25152;&#24341;&#21457;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20010;&#19981;&#26029;&#22686;&#21152;&#30340;&#30740;&#31350;&#39046;&#22495;&#28041;&#21450;&#20351;&#29992;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#65288;&#20363;&#22914;&#65292;&#25554;&#20837;&#22270;&#20687;&#30340;&#29305;&#23450;&#34917;&#19969;&#65289;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#65288;&#21363;&#26631;&#31614;&#65289;&#65292;&#28982;&#21518;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified novel, previously unknown, bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze and explain the behavior of a model using a particular dataset. While this is useful, such tools can only analyze behaviors induced by features that the user can sample or identify in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods which do not depend on a dataset.  In this paper, our primary contribution is a benchmark to evaluate interpretability tools. Our key insight is that we can train models that respond to specific triggers (e.g., a specific patch inserted into an image) with specific outputs (i.e. a label) and then evaluate inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.10681</link><description>&lt;p&gt;
FrankenSplit:&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection. (arXiv:2302.10681v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;AI&#21152;&#36895;&#22120;&#30340;&#23835;&#36215;&#20351;&#24471;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#24378;&#22823;&#27169;&#22411;&#30340;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#23558;&#35831;&#27714;&#19979;&#25918;&#65292;&#32780;&#39640;&#32500;&#25968;&#25454;&#23558;&#20105;&#22842;&#26377;&#38480;&#30340;&#24102;&#23485;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#24182;&#22312;&#21453;&#26144;&#36793;&#32536;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#19981;&#23545;&#31216;&#36164;&#28304;&#20998;&#37197;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60\% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22240;&#26524;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#23545;&#20110;&#26576;&#20123;&#20013;&#38388;&#35745;&#31639;&#26469;&#35828;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#65292;&#21487;&#20197;&#24320;&#21457;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#65292;&#29983;&#25104;&#33021;&#22815;&#20351;&#30446;&#26631;&#31639;&#27861;&#26377;&#23436;&#20840;&#30456;&#21516;&#19979;&#19968;&#36712;&#36857;&#27493;&#39588;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10258</link><description>&lt;p&gt;
&#20855;&#26377;&#22240;&#26524;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning with Causal Regularisation. (arXiv:2302.10258v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10258
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22240;&#26524;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#23545;&#20110;&#26576;&#20123;&#20013;&#38388;&#35745;&#31639;&#26469;&#35828;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#65292;&#21487;&#20197;&#24320;&#21457;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#65292;&#29983;&#25104;&#33021;&#22815;&#20351;&#30446;&#26631;&#31639;&#27861;&#26377;&#23436;&#20840;&#30456;&#21516;&#19979;&#19968;&#36712;&#36857;&#27493;&#39588;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#30740;&#31350;&#25506;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#23398;&#20064;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#25191;&#34892;&#32463;&#20856;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31070;&#32463;&#25512;&#29702;&#22120;&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#36755;&#20837;&#30340;&#35268;&#27169;&#26356;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#37325;&#35201;&#35266;&#23519;&#65306;&#23545;&#20110;&#31639;&#27861;&#26469;&#35828;&#65292;&#26377;&#24456;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#20250;&#34920;&#29616;&#20986;&#23436;&#20840;&#30456;&#21516;&#30340;&#20013;&#38388;&#35745;&#31639;&#12290;&#36825;&#31181;&#27934;&#23519;&#21147;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#25968;&#25454;&#22686;&#24378;&#30340;&#31243;&#24207;&#65292;&#26681;&#25454;&#31639;&#27861;&#30340;&#20013;&#38388;&#36712;&#36857;&#29983;&#25104;&#33021;&#22815;&#20351;&#30446;&#26631;&#31639;&#27861;&#26377;&#23436;&#20840;&#30456;&#21516;&#19979;&#19968;&#36712;&#36857;&#27493;&#39588;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30001;&#25105;&#20204;&#30340;&#35266;&#23519;&#23548;&#20986;&#24182;&#22312;&#22240;&#26524;&#22270;&#20013;&#24418;&#24335;&#21270;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#26469;&#30830;&#20445;&#36825;&#20123;&#36755;&#20837;&#19979;&#30340;&#19979;&#19968;&#27493;&#39044;&#27979;&#30340;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Hint-ReLIC&#65292;&#25913;&#21892;&#20102;OOD&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on neural algorithmic reasoning has investigated the reasoning capabilities of neural networks, effectively demonstrating they can learn to execute classical algorithms on unseen data coming from the train distribution. However, the performance of existing neural reasoners significantly degrades on out-of-distribution (OOD) test data, where inputs have larger sizes. In this work, we make an important observation: there are many different inputs for which an algorithm will perform certain intermediate computations identically. This insight allows us to develop data augmentation procedures that, given an algorithm's intermediate trajectory, produce inputs for which the target algorithm would have exactly the same next trajectory step. We ensure invariance in the next-step prediction across such inputs, by employing a self-supervised objective derived by our observation, formalised in a causal graph. We prove that the resulting method, which we call Hint-ReLIC, improves the OO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21033;&#29992;&#25299;&#25169;&#23398;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#22312;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#20248;&#20110;&#25110;&#21305;&#37197;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.09543</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topological Feature Selection: A Graph-Based Filter Feature Selection Approach. (arXiv:2302.09543v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21033;&#29992;&#25299;&#25169;&#23398;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#22312;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#20248;&#20110;&#25110;&#21305;&#37197;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#35770;&#36807;&#28388;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#20102;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23041;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#24358;&#22270;&#65288;&#19977;&#35282;&#26368;&#22823;&#36807;&#28388;&#22270;&#65289;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#29305;&#24449;&#22312;&#32593;&#32476;&#20869;&#30340;&#30456;&#23545;&#20301;&#32622;&#26469;&#26368;&#22823;&#21270;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#26377;&#19977;&#20010;&#29305;&#28857;&#65306;&#65288;i&#65289;&#39640;&#24230;&#21487;&#35843;&#65292;&#26131;&#20110;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#30340;&#24615;&#36136;&#65307;&#65288;ii&#65289;&#23436;&#20840;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#26174;&#33879;&#30340;&#31616;&#21333;&#24615;&#65307;&#65288;iii&#65289;&#35745;&#31639;&#25104;&#26412;&#27604;&#20854;&#26367;&#20195;&#26041;&#26696;&#26356;&#21152;&#20415;&#23452;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#24322;&#26500;&#35780;&#20272;&#26465;&#20214;&#19979;&#65292;&#23427;&#20248;&#20110;&#25110;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel unsupervised, graph-based filter feature selection technique which exploits the power of topologically constrained network representations. We model dependency structures among features using a family of chordal graphs (the Triangulated Maximally Filtered Graph), and we maximise the likelihood of features' relevance by studying their relative position inside the network. Such an approach presents three aspects that are particularly satisfactory compared to its alternatives: (i) it is highly tunable and easily adaptable to the nature of input data; (ii) it is fully explainable, maintaining, at the same time, a remarkable level of simplicity; (iii) it is computationally cheaper compared to its alternatives. We test our algorithm on 16 benchmark datasets from different applicative domains showing that it outperforms or matches the current state-of-the-art under heterogeneous evaluation conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#23618;&#24494;&#35843;&#19982;&#23376;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#23618;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#23558;&#20854;&#20313;&#26435;&#37325;&#20445;&#25345;&#22312;&#39044;&#35757;&#32451;&#20540;&#19978;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#33021;&#22815;&#19982;&#20840;&#27169;&#22411;&#24494;&#35843;&#30456;&#23218;&#32654;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#20219;&#21153;&#38388;&#30340;&#36164;&#28304;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2302.06354</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#36873;&#25321;&#24615;&#23618;&#24494;&#35843;&#19982;&#23376;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Less is More: Selective Layer Finetuning with SubTuning. (arXiv:2302.06354v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#23618;&#24494;&#35843;&#19982;&#23376;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#23618;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#23558;&#20854;&#20313;&#26435;&#37325;&#20445;&#25345;&#22312;&#39044;&#35757;&#32451;&#20540;&#19978;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#33021;&#22815;&#19982;&#20840;&#27169;&#22411;&#24494;&#35843;&#30456;&#23218;&#32654;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#20219;&#21153;&#38388;&#30340;&#36164;&#28304;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#25104;&#20026;&#22312;&#26032;&#20219;&#21153;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#23548;&#33268;&#20102;&#24555;&#36895;&#25910;&#25947;&#21644;&#25913;&#21892;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21363;&#19981;&#23545;&#32593;&#32476;&#30340;&#25152;&#26377;&#26435;&#37325;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#26159;&#21482;&#35757;&#32451;&#19968;&#32452;&#31934;&#24515;&#36873;&#25321;&#30340;&#23618;&#65292;&#20351;&#20854;&#20313;&#30340;&#26435;&#37325;&#20445;&#25345;&#22312;&#20854;&#21021;&#22987;&#65288;&#39044;&#35757;&#32451;&#65289;&#20540;&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;\emph{&#23376;&#24494;&#35843;}&#65288;SubTuning&#65289;&#32463;&#24120;&#33021;&#22815;&#36798;&#21040;&#19982;&#23545;&#27169;&#22411;&#36827;&#34892;&#20840;&#24494;&#35843;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#26102;&#29978;&#33267;&#36229;&#36807;&#20102;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;SubTuning&#20801;&#35768;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#37096;&#32626;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20139;&#21463;&#25972;&#20010;&#27169;&#22411;&#24494;&#35843;&#30340;&#22909;&#22788;&#12290;&#36825;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#24178;&#25200;&#65292;&#32780;&#22312;&#22823;&#37096;&#20998;&#36164;&#28304;&#19978;&#20849;&#20139;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SubTuning&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning a pretrained model has become a standard approach for training neural networks on novel tasks, resulting in fast convergence and improved performance. In this work, we study an alternative finetuning method, where instead of finetuning all the weights of the network, we only train a carefully chosen subset of layers, keeping the rest of the weights frozen at their initial (pretrained) values. We demonstrate that \emph{subset finetuning} (or SubTuning) often achieves accuracy comparable to full finetuning of the model, and even surpasses the performance of full finetuning when training data is scarce. Therefore, SubTuning allows deploying new tasks at minimal computational cost, while enjoying the benefits of finetuning the entire model. This yields a simple and effective method for multi-task learning, where different tasks do not interfere with one another, and yet share most of the resources at inference time. We demonstrate the efficiency of SubTuning across multiple task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#22238;&#39038;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.03122</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
State-wise Safe Reinforcement Learning: A Survey. (arXiv:2302.03122v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#22238;&#39038;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20173;&#28982;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#23433;&#20840;&#24615;&#65292;&#20063;&#23601;&#26159;&#32422;&#26463;&#28385;&#36275;&#12290;&#29366;&#24577;&#32422;&#26463;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#26368;&#24120;&#35265;&#19988;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#32422;&#26463;&#20043;&#19968;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#32780;&#35328;&#26159;&#24517;&#35201;&#21644;&#20851;&#38190;&#30340;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#30340;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#29366;&#24577;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#19979;&#65292;&#20174;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#65292;&#35752;&#35770;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potentia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02119</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#23545;&#25112;&#23454;&#29616;&#22810;&#26679;&#21270;&#35825;&#23548;&#30340;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29615;&#22659;&#20998;&#24067;&#35774;&#35745;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20986;&#35757;&#32451;&#26377;&#25928;&#30340;&#36890;&#29992;&#33021;&#21147;&#20195;&#29702;&#30340;&#21069;&#26223;&#12290;&#23427;&#30340;&#25104;&#21151;&#37096;&#20998;&#22312;&#20110;&#19968;&#31181;&#33258;&#36866;&#24212;&#35838;&#31243;&#23398;&#20064;&#30340;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#36890;&#36807;&#29983;&#25104;&#20195;&#29702;&#33021;&#21147;&#30340;&#21069;&#27839;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#32463;&#24120;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#21457;&#29616;&#26377;&#25928;&#32423;&#21035;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#39640;&#25104;&#26412;&#20132;&#20114;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#22312;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26694;&#26550;&#20013;&#24341;&#20837;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#32473;&#23450;&#32423;&#21035;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#26469;&#34920;&#24449;&#20004;&#20010;&#32423;&#21035;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#36825;&#23545;&#20110;&#26377;&#25928;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#65292;&#20351;&#24471;&#29615;&#22659;&#29983;&#25104;&#22120;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#38754;&#20020;&#26799;&#24230;&#33539;&#22260;&#21464;&#21270;&#22823;&#12289;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#12289;&#39640;&#26041;&#24046;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01470</link><description>&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Optimize for Reinforcement Learning. (arXiv:2302.01470v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01470
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#38754;&#20020;&#26799;&#24230;&#33539;&#22260;&#21464;&#21270;&#22823;&#12289;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#12289;&#39640;&#26041;&#24046;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#30340;&#25968;&#25454;&#12289;&#35745;&#31639;&#21644;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#25163;&#21160;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#19982;&#30417;&#30563;&#23398;&#20064;&#26412;&#36136;&#19978;&#19981;&#21516;&#65292;&#36825;&#20123;&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#31616;&#21333;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#21457;&#29616;&#20102;&#19977;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26799;&#24230;&#22312;&#23545;&#25968;&#19978;&#21464;&#21270;&#33539;&#22260;&#24456;&#22823;&#65292;&#32780;&#22312;&#32477;&#23545;&#20540;&#19978;&#33539;&#22260;&#36739;&#23567;&#65292;&#36825;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#21442;&#25968;&#26356;&#26032;&#12290;&#20854;&#27425;&#65292;&#20195;&#29702;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#65292;&#23548;&#33268;&#20803;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#20195;&#29702;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#39640;&#24230;&#38543;&#26426;&#20132;&#20114;&#65292;&#20195;&#29702;&#26799;&#24230;&#23384;&#22312;&#36739;&#39640;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#22686;&#21152;&#20102;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, by leveraging more data, computation, and diverse tasks, learned optimizers have achieved remarkable success in supervised learning, outperforming classical hand-designed optimizers. Reinforcement learning (RL) is essentially different from supervised learning and in practice these learned optimizers do not work well even in simple RL tasks. We investigate this phenomenon and identity three issues. First, the gradients of an RL agent vary across a wide range in logarithms while their absolute values are in a small range, making neural networks hard to obtain accurate parameter updates. Second, the agent-gradient distribution is non-independent and identically distributed, leading to inefficient meta-training. Finally, due to highly stochastic agent-environment interactions, the agent-gradients have high bias and variance, which increase the difficulty of learning an optimizer for RL. We propose gradient processing, pipeline training, and a novel optimizer structure wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UFLP&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#32622;&#29615;&#22659;&#21040;&#39640;&#19981;&#30830;&#23450;&#24615;&#29366;&#24577;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37319;&#26679;&#25104;&#26412;&#65292;&#24182;&#22312;&#22256;&#38590;&#30340;&#25506;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.12579</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#35268;&#21010;&#23454;&#29616;&#26679;&#26412;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Deep Reinforcement Learning via Local Planning. (arXiv:2301.12579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UFLP&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#32622;&#29615;&#22659;&#21040;&#39640;&#19981;&#30830;&#23450;&#24615;&#29366;&#24577;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37319;&#26679;&#25104;&#26412;&#65292;&#24182;&#22312;&#22256;&#38590;&#30340;&#25506;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#22312;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#26679;&#26412;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#27169;&#25311;&#22120;&#30340;&#19968;&#20010;&#26377;&#29992;&#29305;&#24615;&#26159;&#21487;&#20197;&#23558;&#29615;&#22659;&#37325;&#32622;&#21040;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19981;&#30830;&#23450;&#24615;&#20248;&#20808;&#26412;&#22320;&#35268;&#21010;&#8221;&#65288;UFLP&#65289;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#36825;&#20010;&#29305;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27599;&#20010;&#25968;&#25454;&#25910;&#38598;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#20803;&#31639;&#27861;&#20197;&#19968;&#23450;&#30340;&#27010;&#29575;&#23558;&#29615;&#22659;&#37325;&#32622;&#20026;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#35266;&#23519;&#29366;&#24577;&#65292;&#32780;&#19981;&#26159;&#26681;&#25454;&#21021;&#22987;&#29366;&#24577;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#12290;&#28982;&#21518;&#65292;&#20195;&#29702;-&#29615;&#22659;&#20132;&#20114;&#23601;&#20687;&#22312;&#26631;&#20934;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#19968;&#26679;&#36827;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20960;&#20010;&#22522;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22256;&#38590;&#30340;&#25506;&#32034;&#20219;&#21153;&#19978;&#30340;&#37319;&#26679;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#31616;&#21333;&#65288;&#20998;&#24067;&#24335;&#65289;&#21452;&#37325;DQN&#22312;&#33261;&#21517;&#26157;&#33879;&#30340;&#38590;&#24230;&#24456;&#39640;&#30340;Atari&#28216;&#25103;&#8220;&#33945;&#29305;&#31062;&#29595;&#20043;&#22797;&#20167;&#8221;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The focus of this work is sample-efficient deep reinforcement learning (RL) with a simulator. One useful property of simulators is that it is typically easy to reset the environment to a previously observed state. We propose an algorithmic framework, named uncertainty-first local planning (UFLP), that takes advantage of this property. Concretely, in each data collection iteration, with some probability, our meta-algorithm resets the environment to an observed state which has high uncertainty, instead of sampling according to the initial-state distribution. The agent-environment interaction then proceeds as in the standard online RL setting. We demonstrate that this simple procedure can dramatically improve the sample cost of several baseline RL algorithms on difficult exploration tasks. Notably, with our framework, we can achieve super-human performance on the notoriously hard Atari game, Montezuma's Revenge, with a simple (distributional) double DQN. Our work can be seen as an efficie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.10813</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20445;&#35777;&#25552;&#39640;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#20110;&#38544;&#34255;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#27491;&#22312;&#22686;&#21152;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#22686;&#24378;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#24120;&#29992;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#24230;&#37327;&#21644;&#20960;&#31181;&#32467;&#21512;&#38598;&#25104;&#23398;&#20064;&#30340;&#20844;&#24179;&#24863;&#30693;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;&#24230;&#37327;&#21482;&#33021;&#20851;&#27880;&#20854;&#20013;&#20043;&#19968;&#65292;&#21363;&#32676;&#20307;&#20844;&#24179;&#24615;&#25110;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#30828;&#24615;&#20860;&#23481;&#24615;&#26263;&#31034;&#20102;&#21363;&#20351;&#20854;&#20013;&#20043;&#19968;&#24471;&#21040;&#28385;&#36275;&#65292;&#20173;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25552;&#21319;&#20844;&#24179;&#24615;&#30340;&#26426;&#21046;&#36890;&#24120;&#21482;&#25552;&#20379;&#32463;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#35770;&#25991;&#35752;&#35770;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#21028;&#21035;&#39118;&#38505;&#65292;&#20197;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;p...
&lt;/p&gt;
&lt;p&gt;
The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20197;&#23458;&#25143;&#20026;&#20013;&#24515;&#30340;&#33829;&#38144;&#27963;&#21160;&#20013;&#23547;&#25214;&#30456;&#20284;&#23458;&#25143;&#30340;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22788;&#29702;&#20159;&#32423;&#23458;&#25143;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23884;&#20837;&#27169;&#22411;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#26469;&#23547;&#25214;&#24863;&#20852;&#36259;&#30340;&#30456;&#20284;&#23458;&#25143;&#12290;&#36890;&#36807;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#26377;&#24847;&#20041;&#30340;&#23458;&#25143;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#19994;&#21153;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2301.03147</link><description>&lt;p&gt;
&#23547;&#25214;&#30005;&#23376;&#21830;&#21153;&#33829;&#38144;&#30340;&#30456;&#20284;&#23458;&#25143;
&lt;/p&gt;
&lt;p&gt;
Finding Lookalike Customers for E-Commerce Marketing. (arXiv:2301.03147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20197;&#23458;&#25143;&#20026;&#20013;&#24515;&#30340;&#33829;&#38144;&#27963;&#21160;&#20013;&#23547;&#25214;&#30456;&#20284;&#23458;&#25143;&#30340;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22788;&#29702;&#20159;&#32423;&#23458;&#25143;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23884;&#20837;&#27169;&#22411;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#26469;&#23547;&#25214;&#24863;&#20852;&#36259;&#30340;&#30456;&#20284;&#23458;&#25143;&#12290;&#36890;&#36807;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#26377;&#24847;&#20041;&#30340;&#23458;&#25143;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#19994;&#21153;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#23458;&#25143;&#20026;&#20013;&#24515;&#30340;&#33829;&#38144;&#27963;&#21160;&#20026;&#27779;&#23572;&#29595;&#30340;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#27969;&#37327;&#36129;&#29486;&#20102;&#24456;&#22823;&#30340;&#19968;&#37096;&#20998;&#12290;&#38543;&#30528;&#23458;&#25143;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#25193;&#22823;&#33829;&#38144;&#21463;&#20247;&#20197;&#35302;&#36798;&#26356;&#22810;&#23458;&#25143;&#23545;&#30005;&#23376;&#21830;&#21153;&#20844;&#21496;&#30340;&#19994;&#21153;&#22686;&#38271;&#21644;&#20026;&#23458;&#25143;&#24102;&#26469;&#26356;&#22810;&#20215;&#20540;&#21464;&#24471;&#26356;&#20026;&#20851;&#38190;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#39640;&#25928;&#30340;&#31995;&#32479;&#26469;&#25193;&#22823;&#33829;&#38144;&#27963;&#21160;&#30340;&#30446;&#26631;&#21463;&#20247;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22788;&#29702;&#20159;&#32423;&#23458;&#25143;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23884;&#20837;&#27169;&#22411;&#26469;&#34920;&#31034;&#23458;&#25143;&#65292;&#20351;&#29992;&#19968;&#31181;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#24555;&#36895;&#25214;&#21040;&#24863;&#20852;&#36259;&#30340;&#30456;&#20284;&#23458;&#25143;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#26377;&#24847;&#20041;&#30340;&#23458;&#25143;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#22788;&#29702;&#21508;&#31181;&#19994;&#21153;&#20852;&#36259;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#31995;&#32479;&#21644;&#23458;&#25143;&#23884;&#20837;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customer-centric marketing campaigns generate a large portion of e-commerce website traffic for Walmart. As the scale of customer data grows larger, expanding the marketing audience to reach more customers is becoming more critical for e-commerce companies to drive business growth and bring more value to customers. In this paper, we present a scalable and efficient system to expand targeted audience of marketing campaigns, which can handle hundreds of millions of customers. We use a deep learning based embedding model to represent customers and an approximate nearest neighbor search method to quickly find lookalike customers of interest. The model can deal with various business interests by constructing interpretable and meaningful customer similarity metrics. We conduct extensive experiments to demonstrate the great performance of our system and customer embedding model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CC-FedAvg&#30340;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#21487;&#35753;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#35745;&#31639;&#39044;&#31639;&#20915;&#23450;&#22312;&#27599;&#36718;&#20013;&#26159;&#21542;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#25110;&#27169;&#22411;&#20272;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CC-FedAvg&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.13679</link><description>&lt;p&gt;
CC-FedAvg&#65306;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CC-FedAvg: Computationally Customized Federated Averaging. (arXiv:2212.13679v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CC-FedAvg&#30340;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#21487;&#35753;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#35745;&#31639;&#39044;&#31639;&#20915;&#23450;&#22312;&#27599;&#36718;&#20013;&#26159;&#21542;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#25110;&#27169;&#22411;&#20272;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CC-FedAvg&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#24335;&#65292;&#36890;&#36807;&#20998;&#24067;&#22312;&#20247;&#22810;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#23427;&#22312;&#26412;&#36136;&#19978;&#20551;&#35774;&#21442;&#19982;&#32773;&#30340;&#35745;&#31639;&#33021;&#21147;&#30456;&#21516;&#65292;&#20294;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#33021;&#28304;&#39044;&#31639;&#25110;&#24182;&#34892;&#25191;&#34892;&#30340;&#20219;&#21153;&#19981;&#21516;&#65292;&#21442;&#19982;&#32773;&#35745;&#31639;&#36164;&#28304;&#23384;&#22312;&#30528;&#24046;&#24322;&#12290;&#32570;&#20047;&#35745;&#31639;&#39044;&#31639;&#30340;&#21442;&#19982;&#32773;&#24517;&#39035;&#36866;&#24403;&#35268;&#21010;&#20854;&#21463;&#38480;&#35745;&#31639;&#36164;&#28304;&#30340;&#20351;&#29992;&#65292;&#21542;&#21017;&#20182;&#20204;&#23558;&#26080;&#27861;&#23436;&#25104;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#31639;&#26412;&#22320;&#27169;&#22411;&#32780;&#26080;&#38656;&#35745;&#31639;&#23494;&#38598;&#36845;&#20195;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;(CC-FedAvg)&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#24403;&#21069;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#27599;&#20010;&#36718;&#27425;&#20013;&#20915;&#23450;&#26159;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#36824;&#26159;&#27169;&#22411;&#20272;&#31639;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#30456;&#27604;&#65292;CC-FedAvg&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging paradigm to train model with distributed data from numerous Internet of Things (IoT) devices. It inherently assumes a uniform capacity among participants. However, due to different conditions such as differing energy budgets or executing parallel unrelated tasks, participants have diverse computational resources in practice. Participants with insufficient computation budgets must plan for the use of restricted computational resources appropriately, otherwise they would be unable to complete the entire training procedure, resulting in model performance decline. To address the this issue, we propose a strategy for estimating local models without computationally intensive iterations. Based on it, we propose Computationally Customized Federated Averaging (CC-FedAvg), which allows participants to determine whether to perform traditional local training or model estimation in each round based on their current computational budgets. Both theoretical analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10511</link><description>&lt;p&gt;
&#20309;&#26102;&#19981;&#20449;&#20219;&#35821;&#35328;&#27169;&#22411;&#65306;&#25506;&#32034;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#35760;&#24518;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#26263;&#31034;&#20102;&#20165;&#20381;&#38752;&#20854;&#21442;&#25968;&#26469;&#32534;&#30721;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#22312;PopQA&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#30693;&#35782;&#25506;&#27979;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#38271;&#23614;&#20013;&#65292;&#25193;&#23637;&#35268;&#27169;&#26080;&#27861;&#26126;&#26174;&#25913;&#21892;&#35760;&#24518;&#23454;&#38469;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32988;&#36807;&#32423;&#21035;&#22823;&#24471;&#22810;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26410;&#32463;&#21327;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#39640;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#22312;&#38656;&#35201;&#26102;&#26816;&#32034;&#38750;&#21442;&#25968;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#30693;&#35782;&#23548;&#20837;&#29616;&#26377;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2211.11736</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22686;&#24378;&#23454;&#29616;&#26426;&#22120;&#20154;&#25216;&#33021;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models. (arXiv:2211.11736v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#30693;&#35782;&#23548;&#20837;&#29616;&#26377;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#23398;&#20064;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20174;&#26426;&#22120;&#20154;-&#35821;&#35328;&#25968;&#25454;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#65292;&#35813;&#25968;&#25454;&#35201;&#20040;&#26159;&#20026;&#29305;&#23450;&#20219;&#21153;&#32780;&#25910;&#38598;&#30340;&#65292;&#35201;&#20040;&#26159;&#22312;&#20107;&#21518;&#30001;&#20154;&#24037;&#26114;&#36149;&#22320;&#37325;&#26032;&#26631;&#27880;&#30340;&#65292;&#24102;&#26377;&#20016;&#23500;&#30340;&#35821;&#35328;&#25551;&#36848;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#25110;ViLD&#24050;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#34920;&#31034;&#21644;&#22330;&#26223;&#25551;&#36848;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#21542;&#20316;&#20026;&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#33258;&#21160;&#26631;&#27880;&#24037;&#20855;&#65292;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#30693;&#35782;&#23548;&#20837;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#20351;&#20854;&#23545;&#20110;&#26410;&#22312;&#20854;&#22320;&#38754;&#30495;&#23454;&#27880;&#37322;&#20013;&#21453;&#26144;&#30340;&#20219;&#21153;&#20063;&#33021;&#21457;&#25381;&#20316;&#29992;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#25351;&#20196;&#22686;&#24378;&#65288;DIAL&#65289;&#26469;&#29992;&#20110;&#22522;&#20110;&#35821;&#35328;&#30340;&#25511;&#21046;&#65306;&#25105;&#20204;&#21033;&#29992;CLIP&#30340;&#35821;&#20041;&#29702;&#35299;&#65292;&#21033;&#29992;&#21322;&#30417;&#30563;&#30340;&#35821;&#35328;&#26631;&#31614;&#23558;&#30693;&#35782;&#20256;&#36882;&#21040;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#28436;&#31034;&#25968;&#25454;&#38598;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAX-Pose&#30340;&#31995;&#32479;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#35937;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#32467;&#26524;&#25351;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2211.09325</link><description>&lt;p&gt;
TAX-Pose&#65306;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAX-Pose&#30340;&#31995;&#32479;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#35937;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#32467;&#26524;&#25351;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#36171;&#20104;&#26426;&#22120;&#20154;&#26377;&#25928;&#22320;&#25805;&#20316;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22522;&#20110;&#31034;&#33539;&#36716;&#31227;&#30456;&#20851;&#25216;&#33021;&#65311;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#27867;&#21270;&#21040;&#26032;&#30340;&#29289;&#20307;&#25110;&#26410;&#35265;&#36807;&#30340;&#37197;&#32622;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20851;&#27880;&#20132;&#20114;&#23545;&#35937;&#30456;&#20851;&#37096;&#20998;&#30340;&#20219;&#21153;&#29305;&#23450;&#23039;&#21183;&#20851;&#31995;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#31181;&#20851;&#31995;&#26159;&#19968;&#31181;&#21487;&#20197;&#36716;&#31227;&#21040;&#21516;&#19968;&#31867;&#21035;&#26032;&#29289;&#20307;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#27010;&#24565;&#65307;&#20363;&#22914;&#65292;&#24179;&#24213;&#38149;&#30456;&#23545;&#20110;&#28900;&#31665;&#30340;&#23039;&#21183;&#20851;&#31995;&#25110;&#32773;&#26479;&#23376;&#30456;&#23545;&#20110;&#26479;&#26550;&#30340;&#23039;&#21183;&#20851;&#31995;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#23039;&#21183;&#20851;&#31995;&#20026;&#8220;&#36328;&#23039;&#21183;&#8221;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#27010;&#24565;&#30340;&#25968;&#23398;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#23398;&#20064;&#30340;&#23545;&#35937;&#38388;&#23545;&#24212;&#20851;&#31995;&#26469;&#23398;&#20064;&#20272;&#35745;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#12290;&#28982;&#21518;&#65292;&#20272;&#35745;&#30340;&#36328;&#23039;&#21183;&#29992;&#20110;&#24341;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#23558;&#23545;&#35937;&#25805;&#32437;&#21040;&#25152;&#38656;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship "cross-pose" and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired po
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;MixUp-MIL&#29992;&#20110;&#22810;&#31034;&#20363;&#23398;&#20064;&#65292;&#36890;&#36807;&#24212;&#29992;&#20999;&#29255;&#20869;&#25554;&#20540;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05862</link><description>&lt;p&gt;
MixUp-MIL&#65306;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#22810;&#31034;&#20363;&#23398;&#20064;&#24182;&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis. (arXiv:2211.05862v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05862
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;MixUp-MIL&#29992;&#20110;&#22810;&#31034;&#20363;&#23398;&#20064;&#65292;&#36890;&#36807;&#24212;&#29992;&#20999;&#29255;&#20869;&#25554;&#20540;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;&#32032;&#25110;&#34917;&#19969;&#32423;&#21035;&#27880;&#37322;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#31034;&#20363;&#23398;&#20064;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#30340;&#35786;&#26029;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#23613;&#31649;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#30340;&#22823;&#23567;&#24456;&#22823;&#65292;&#20294;&#20010;&#21035;&#20999;&#29255;&#30340;&#25968;&#37327;&#24448;&#24448;&#30456;&#23545;&#36739;&#23567;&#65292;&#23548;&#33268;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#23569;&#12290;&#20026;&#20102;&#25552;&#39640;&#35757;&#32451;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#29305;&#24449;&#21521;&#37327;&#30340;&#32447;&#24615;&#25554;&#20540;&#65288;&#21363;MixUp&#65289;&#24605;&#24819;&#30340;&#19981;&#21516;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#31034;&#20363;&#23398;&#20064;&#26550;&#26500;&#21644;&#20004;&#20010;&#30002;&#29366;&#33146;&#30284;&#25968;&#25454;&#38598;&#65292;&#36827;&#34892;&#20102;&#19968;&#39033;&#35814;&#23613;&#30340;&#30740;&#31350;&#65292;&#32771;&#34385;&#20102;&#22810;&#31181;&#24120;&#35265;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#23613;&#31649;&#22522;&#20110;&#21407;&#22987;MixUp&#26041;&#27861;&#30340;&#31574;&#30053;&#34920;&#29616;&#20986;&#20934;&#30830;&#24230;&#38477;&#20302;&#65292;&#20294;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20999;&#29255;&#20869;&#25554;&#20540;&#26041;&#27861;&#21364;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple instance learning exhibits a powerful approach for whole slide image-based diagnosis in the absence of pixel- or patch-level annotations. In spite of the huge size of hole slide images, the number of individual slides is often rather small, leading to a small number of labeled samples. To improve training, we propose and investigate different data augmentation strategies for multiple instance learning based on the idea of linear interpolations of feature vectors (known as MixUp). Based on state-of-the-art multiple instance learning architectures and two thyroid cancer data sets, an exhaustive study is conducted considering a range of common data augmentation strategies. Whereas a strategy based on to the original MixUp approach showed decreases in accuracy, the use of a novel intra-slide interpolation method led to consistent increases in accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20915;&#31574;&#20219;&#21153;&#65292;&#20027;&#21160;&#33719;&#21462;&#22810;&#27169;&#24577;&#26102;&#38388;&#25968;&#25454;&#12290;&#36890;&#36807;&#26435;&#34913;&#33719;&#21462;&#25104;&#26412;&#21644;&#39044;&#27979;&#24615;&#33021;&#65292;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#26469;&#20027;&#21160;&#36873;&#25321;&#33719;&#21462;&#30340;&#36755;&#20837;&#27169;&#24577;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#25512;&#29702;&#25216;&#33021;&#30340;&#21512;&#25104;&#24773;&#26223;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#23398;&#20064;&#21040;&#25104;&#26412;&#21453;&#24212;&#24335;&#30340;&#33719;&#21462;&#34892;&#20026;&#65292;&#20294;&#26080;&#27861;&#23398;&#20064;&#21040;&#33258;&#36866;&#24212;&#30340;&#33719;&#21462;&#31574;&#30053;&#65292;&#31361;&#26174;&#20102;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05039</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26102;&#38388;&#25968;&#25454;&#30340;&#20027;&#21160;&#33719;&#21462;&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20915;&#31574;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task. (arXiv:2211.05039v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20915;&#31574;&#20219;&#21153;&#65292;&#20027;&#21160;&#33719;&#21462;&#22810;&#27169;&#24577;&#26102;&#38388;&#25968;&#25454;&#12290;&#36890;&#36807;&#26435;&#34913;&#33719;&#21462;&#25104;&#26412;&#21644;&#39044;&#27979;&#24615;&#33021;&#65292;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#26469;&#20027;&#21160;&#36873;&#25321;&#33719;&#21462;&#30340;&#36755;&#20837;&#27169;&#24577;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#25512;&#29702;&#25216;&#33021;&#30340;&#21512;&#25104;&#24773;&#26223;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#23398;&#20064;&#21040;&#25104;&#26412;&#21453;&#24212;&#24335;&#30340;&#33719;&#21462;&#34892;&#20026;&#65292;&#20294;&#26080;&#27861;&#23398;&#20064;&#21040;&#33258;&#36866;&#24212;&#30340;&#33719;&#21462;&#31574;&#30053;&#65292;&#31361;&#26174;&#20102;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20915;&#31574;&#20219;&#21153;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22810;&#27169;&#24577;&#26102;&#38388;&#25968;&#25454;&#30340;&#20027;&#21160;&#33719;&#21462;&#65288;A2MT&#65289;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#36755;&#20837;&#29305;&#24449;&#22312;&#27979;&#35797;&#26102;&#19981;&#23481;&#26131;&#33719;&#24471;&#65292;&#24517;&#39035;&#20197;&#36739;&#22823;&#20195;&#20215;&#33719;&#21462;&#12290;&#36890;&#36807;A2MT&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#65292;&#20351;&#20854;&#33021;&#22815;&#20027;&#21160;&#36873;&#25321;&#35201;&#33719;&#21462;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#26435;&#34913;&#33719;&#21462;&#25104;&#26412;&#19982;&#39044;&#27979;&#24615;&#33021;&#12290;A2MT&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#20027;&#21160;&#29305;&#24449;&#33719;&#21462;&#65292;&#20197;&#20415;&#36827;&#34892;&#20851;&#20110;&#39640;&#32500;&#36755;&#20837;&#30340;&#26102;&#38388;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Perceiver IO&#26550;&#26500;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;A2MT&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#31243;&#24207;&#33021;&#22815;&#35299;&#20915;&#19968;&#20010;&#38656;&#35201;&#23454;&#38469;&#30456;&#20851;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#25216;&#33021;&#30340;&#26032;&#39062;&#21512;&#25104;&#24773;&#26223;&#12290;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;Kinetics-700&#21644;AudioSet&#19978;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#31243;&#24207;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#25104;&#26412;&#21453;&#24212;&#24335;&#30340;&#33719;&#21462;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#28040;&#34701;&#23454;&#39564;&#34920;&#26126;&#23427;&#20204;&#26080;&#27861;&#23398;&#20064;&#21040;&#33258;&#36866;&#24212;&#30340;&#33719;&#21462;&#31574;&#30053;&#65292;&#31361;&#26174;&#20102;&#35813;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a challenging decision-making task that we call active acquisition for multimodal temporal data (A2MT). In many real-world scenarios, input features are not readily available at test time and must instead be acquired at significant cost. With A2MT, we aim to learn agents that actively select which modalities of an input to acquire, trading off acquisition cost and predictive performance. A2MT extends a previous task called active feature acquisition to temporal decision making about high-dimensional inputs. We propose a method based on the Perceiver IO architecture to address A2MT in practice. Our agents are able to solve a novel synthetic scenario requiring practically relevant cross-modal reasoning skills. On two large-scale, real-world datasets, Kinetics-700 and AudioSet, our agents successfully learn cost-reactive acquisition behavior. However, an ablation reveals they are unable to learn adaptive acquisition strategies, emphasizing the difficulty of the task even for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#37322;&#26426;&#21046;&#65292;&#33021;&#22815;&#37327;&#21270;&#29366;&#24577;&#23545;&#34892;&#21160;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#21644;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26426;&#21046;&#22312;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.13507</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#22240;&#26524;&#35299;&#37322;&#65306;&#37327;&#21270;&#29366;&#24577;&#21644;&#26102;&#38388;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Explanation for Reinforcement Learning: Quantifying State and Temporal Importance. (arXiv:2210.13507v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#37322;&#26426;&#21046;&#65292;&#33021;&#22815;&#37327;&#21270;&#29366;&#24577;&#23545;&#34892;&#21160;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#21644;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26426;&#21046;&#22312;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36215;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#36890;&#36807;&#22240;&#26524;&#38236;&#22836;&#26469;&#30475;&#24453;&#19990;&#30028;&#65292;&#22240;&#27492;&#26356;&#20542;&#21521;&#20110;&#22240;&#26524;&#35299;&#37322;&#32780;&#38750;&#20851;&#32852;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#37322;&#26426;&#21046;&#65292;&#20197;&#37327;&#21270;&#29366;&#24577;&#23545;&#34892;&#21160;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#21644;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#21253;&#25324;&#20892;&#30000;&#28748;&#28297;&#12289;21&#28857;&#12289;&#36991;&#30896;&#21644;&#26376;&#29699;&#30528;&#38470;&#22120;&#31561;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26426;&#21046;&#22312;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#26041;&#38754;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20851;&#32852;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability plays an increasingly important role in machine learning. Furthermore, humans view the world through a causal lens and thus prefer causal explanations over associational ones. Therefore, in this paper, we develop a causal explanation mechanism that quantifies the causal importance of states on actions and such importance over time. We also demonstrate the advantages of our mechanism over state-of-the-art associational methods in terms of RL policy explanation through a series of simulation studies, including crop irrigation, Blackjack, collision avoidance, and lunar lander.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#31232;&#30095;&#35745;&#31639;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31232;&#30095;&#22270;&#25805;&#20316;&#38590;&#20197;&#21152;&#36895;&#21644;&#19981;&#35268;&#21017;&#25968;&#25454;&#26684;&#24335;&#23548;&#33268;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.10737</link><description>&lt;p&gt;
RSC: &#36890;&#36807;&#38543;&#26426;&#31232;&#30095;&#35745;&#31639;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
RSC: Accelerating Graph Neural Networks Training via Randomized Sparse Computations. (arXiv:2210.10737v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10737
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#31232;&#30095;&#35745;&#31639;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31232;&#30095;&#22270;&#25805;&#20316;&#38590;&#20197;&#21152;&#36895;&#21644;&#19981;&#35268;&#21017;&#25968;&#25454;&#26684;&#24335;&#23548;&#33268;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35757;&#32451;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#20026;&#30828;&#20214;&#38590;&#20197;&#21152;&#36895;&#31232;&#30095;&#22270;&#25805;&#20316;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#37319;&#26679;&#30340;&#36924;&#36817;&#26469;&#38477;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#29306;&#29298;&#20102;&#35745;&#31639;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#24605;&#36335;&#30340;&#22522;&#30784;&#19978;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#25104;&#21151;&#22320;&#21152;&#36895;&#20102;&#22522;&#20110;&#31264;&#23494;&#30697;&#38453;&#30340;&#25805;&#20316;&#65288;&#22914;&#21367;&#31215;&#21644;&#32447;&#24615;&#25805;&#20316;&#65289;&#65292;&#20934;&#30830;&#24230;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#28982;&#32780;&#65292;&#19982;&#31264;&#23494;&#30697;&#38453;&#19981;&#21516;&#65292;&#31232;&#30095;&#30697;&#38453;&#20197;&#19981;&#35268;&#21017;&#30340;&#25968;&#25454;&#26684;&#24335;&#23384;&#20648;&#65292;&#27599;&#34892;/&#21015;&#21487;&#33021;&#26377;&#19981;&#21516;&#25968;&#37327;&#30340;&#38750;&#38646;&#20803;&#32032;&#12290;&#22240;&#27492;&#65292;&#19982;&#31264;&#23494;&#30697;&#38453;&#30456;&#27604;&#65292;&#36924;&#36817;&#31232;&#30095;&#25805;&#20316;&#23384;&#22312;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#25511;&#21046;&#36924;&#36817;&#31232;&#30095;&#25805;&#20316;&#30340;&#25928;&#29575;&#65292;&#22240;&#20026;&#35745;&#31639;&#20165;&#22312;&#38750;&#38646;&#20803;&#32032;&#19978;&#25191;&#34892;&#65307;&#65288;2&#65289;&#22522;&#20110;&#23376;&#37319;&#26679;&#30340;&#31232;&#30095;&#30697;&#38453;&#22788;&#29702;&#25928;&#29575;&#26356;&#20302;&#65292;&#22240;&#20026;&#23384;&#22312;&#19981;&#35268;&#21017;&#25968;&#25454;&#26684;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25511;&#21046;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of graph neural networks (GNNs) is extremely time consuming because sparse graph-based operations are hard to be accelerated by hardware. Prior art explores trading off the computational precision to reduce the time complexity via sampling-based approximation. Based on the idea, previous works successfully accelerate the dense matrix based operations (e.g., convolution and linear) with negligible accuracy drop. However, unlike dense matrices, sparse matrices are stored in the irregular data format such that each row/column may have different number of non-zero entries. Thus, compared to the dense counterpart, approximating sparse operations has two unique challenges (1) we cannot directly control the efficiency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sub-sampling sparse matrices is much more inefficient due to the irregular data format. To address the issues, our key idea is to control the accuracy-efficiency trade o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#27719;&#32858;&#20449;&#24687;&#30340;&#21508;&#31181;&#27719;&#32858;&#36816;&#31639;&#31526;&#22312;&#23884;&#20837;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#32771;&#34385;&#30340;&#27719;&#32858;&#36816;&#31639;&#31526;&#37117;&#21487;&#20197;&#28385;&#36275;&#35748;&#35782;&#27719;&#32858;&#21407;&#21017;&#65292;&#20294;&#36825;&#20165;&#22312;&#23884;&#20837;&#20855;&#26377;&#36275;&#22815;&#39640;&#32500;&#24230;&#24182;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#12290;&#36825;&#20123;&#32422;&#26463;&#23545;&#23884;&#20837;&#22312;&#23454;&#36341;&#20013;&#30340;&#20351;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.05723</link><description>&lt;p&gt;
&#23884;&#20837;&#20316;&#20026;&#35748;&#35782;&#29366;&#24577;&#65306;&#20851;&#20110;&#29992;&#20110;&#33719;&#24471;&#30693;&#35782;&#30340;&#27719;&#32858;&#36816;&#31639;&#31526;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Embeddings as Epistemic States: Limitations on the Use of Pooling Operators for Accumulating Knowledge. (arXiv:2210.05723v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#27719;&#32858;&#20449;&#24687;&#30340;&#21508;&#31181;&#27719;&#32858;&#36816;&#31639;&#31526;&#22312;&#23884;&#20837;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#32771;&#34385;&#30340;&#27719;&#32858;&#36816;&#31639;&#31526;&#37117;&#21487;&#20197;&#28385;&#36275;&#35748;&#35782;&#27719;&#32858;&#21407;&#21017;&#65292;&#20294;&#36825;&#20165;&#22312;&#23884;&#20837;&#20855;&#26377;&#36275;&#22815;&#39640;&#32500;&#24230;&#24182;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#12290;&#36825;&#20123;&#32422;&#26463;&#23545;&#23884;&#20837;&#22312;&#23454;&#36341;&#20013;&#30340;&#20351;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20381;&#38752;&#27719;&#32858;&#36816;&#31639;&#31526;&#26469;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#40664;&#35748;&#21521;&#37327;&#32534;&#30721;&#20026;&#35748;&#35782;&#29366;&#24577;&#65292;&#21363;&#21521;&#37327;&#25429;&#25417;&#21040;&#24050;&#33719;&#21462;&#26377;&#20851;&#26576;&#20123;&#24863;&#20852;&#36259;&#23646;&#24615;&#30340;&#35777;&#25454;&#65292;&#24182;&#19988;&#27719;&#32858;&#36825;&#20123;&#21521;&#37327;&#20250;&#24471;&#21040;&#19968;&#20010;&#32467;&#21512;&#36825;&#20123;&#35777;&#25454;&#30340;&#21521;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#26631;&#20934;&#27719;&#32858;&#36816;&#31639;&#31526;&#65292;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#23427;&#20204;&#19982;&#36825;&#20010;&#34987;&#31216;&#20026;&#35748;&#35782;&#27719;&#32858;&#21407;&#21017;&#30340;&#24819;&#27861;&#20860;&#23481;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#32771;&#34385;&#30340;&#27719;&#32858;&#36816;&#31639;&#31526;&#37117;&#21487;&#20197;&#28385;&#36275;&#35748;&#35782;&#27719;&#32858;&#21407;&#21017;&#65292;&#20294;&#36825;&#20165;&#22312;&#23884;&#20837;&#20855;&#26377;&#36275;&#22815;&#39640;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#27719;&#32858;&#36816;&#31639;&#31526;&#65292;&#23884;&#20837;&#38656;&#35201;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#65288;&#20363;&#22914;&#20855;&#26377;&#38750;&#36127;&#22352;&#26631;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#32422;&#26463;&#23545;&#23884;&#20837;&#22312;&#23454;&#36341;&#20013;&#30340;&#20351;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#35748;&#35782;&#27719;&#32858;&#21407;&#21017;&#19981;&#33021;&#34987;&#28385;&#36275;&#26102;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#31232;&#30095;&#35299;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various neural network architectures rely on pooling operators to aggregate information coming from different sources. It is often implicitly assumed in such contexts that vectors encode epistemic states, i.e. that vectors capture the evidence that has been obtained about some properties of interest, and that pooling these vectors yields a vector that combines this evidence. We study, for a number of standard pooling operators, under what conditions they are compatible with this idea, which we call the epistemic pooling principle. While we find that all the considered pooling operators can satisfy the epistemic pooling principle, this only holds when embeddings are sufficiently high-dimensional and, for most pooling operators, when the embeddings satisfy particular constraints (e.g. having non-negative coordinates). We furthermore show that these constraints have important implications on how the embeddings can be used in practice. In particular, we find that when the epistemic pooling
&lt;/p&gt;</description></item><item><title>FAIR-FATE&#26159;&#19968;&#31181;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20844;&#24179;&#24863;&#30693;&#30340;&#32858;&#21512;&#26041;&#27861;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#39640;&#25928;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.13678</link><description>&lt;p&gt;
FAIR-FATE: &#20855;&#26377;&#21160;&#37327;&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FAIR-FATE: Fair Federated Learning with Momentum. (arXiv:2209.13678v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13678
&lt;/p&gt;
&lt;p&gt;
FAIR-FATE&#26159;&#19968;&#31181;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20844;&#24179;&#24863;&#30693;&#30340;&#32858;&#21512;&#26041;&#27861;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#39640;&#25928;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20844;&#24179;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#37325;&#28857;&#19968;&#30452;&#26159;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#65292;&#23545;&#20998;&#25955;&#24335;&#26041;&#27861;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#24418;&#24335;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#26381;&#21153;&#22120;&#27719;&#24635;&#23427;&#20204;&#20197;&#33719;&#24471;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#20010;&#24120;&#35265;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25110;&#21152;&#21095;&#30001;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#25110;&#24615;&#21035;&#65289;&#23450;&#20041;&#30340;&#29305;&#26435;&#32452;&#30340;&#27495;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FAIR-FATE&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#30340;&#32858;&#21512;&#26041;&#27861;&#35745;&#31639;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#29992;&#24615;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#37319;&#29992;&#19968;&#20010;&#21160;&#37327;&#39033;&#26469;&#20272;&#35745;&#20844;&#24179;&#27169;&#22411;&#26356;&#26032;&#65292;&#24110;&#21161;&#20811;&#26381;&#38750;&#20844;&#24179;&#26799;&#24230;&#30340;&#38663;&#33633;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fairness-aware machine learning algorithms have been receiving increasing attention, the focus has been on centralized machine learning, leaving decentralized methods underexplored. Federated Learning is a decentralized form of machine learning where clients train local models with a server aggregating them to obtain a shared global model. Data heterogeneity amongst clients is a common characteristic of Federated Learning, which may induce or exacerbate discrimination of unprivileged groups defined by sensitive attributes such as race or gender. In this work we propose FAIR-FATE: a novel FAIR FederATEd Learning algorithm that aims to achieve group fairness while maintaining high utility via a fairness-aware aggregation method that computes the global model by taking into account the fairness of the clients. To achieve that, the global model update is computed by estimating a fair model update using a Momentum term that helps to overcome the oscillations of non-fair gradients. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#65292;&#20854;&#20013;&#31639;&#27861;&#19982;&#20154;&#31867;&#29992;&#25143;&#20197;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#24335;&#20132;&#20114;&#65292;&#20165;&#22312;&#23545;&#29992;&#25143;&#30340;&#20915;&#31574;&#26377;&#30410;&#26102;&#25552;&#20379;&#24314;&#35758;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#24182;&#22312;&#20419;&#36827;&#20154;&#31867;&#23398;&#20064;&#12289;&#20445;&#30041;&#20154;&#31867;&#20915;&#31574;&#30340;&#34917;&#20805;&#20248;&#21183;&#26041;&#38754;&#20855;&#26377;&#39069;&#22806;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2209.13578</link><description>&lt;p&gt;
&#23398;&#20064;&#20160;&#20040;&#26102;&#20505;&#20026;&#20154;&#31867;&#20915;&#31574;&#32773;&#25552;&#20379;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Learning When to Advise Human Decision Makers. (arXiv:2209.13578v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#65292;&#20854;&#20013;&#31639;&#27861;&#19982;&#20154;&#31867;&#29992;&#25143;&#20197;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#24335;&#20132;&#20114;&#65292;&#20165;&#22312;&#23545;&#29992;&#25143;&#30340;&#20915;&#31574;&#26377;&#30410;&#26102;&#25552;&#20379;&#24314;&#35758;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#24182;&#22312;&#20419;&#36827;&#20154;&#31867;&#23398;&#20064;&#12289;&#20445;&#30041;&#20154;&#31867;&#20915;&#31574;&#30340;&#34917;&#20805;&#20248;&#21183;&#26041;&#38754;&#20855;&#26377;&#39069;&#22806;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#25552;&#20379;&#24314;&#35758;&#65292;&#20197;&#20419;&#36827;&#22312;&#21307;&#30103;&#12289;&#21009;&#20107;&#21496;&#27861;&#21644;&#37329;&#34701;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#20154;&#31867;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#65292;&#20854;&#20013;&#31639;&#27861;&#19982;&#20154;&#31867;&#29992;&#25143;&#20197;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#24335;&#20132;&#20114;&#65292;&#26088;&#22312;&#20165;&#22312;&#23545;&#29992;&#25143;&#30340;&#20915;&#31574;&#26377;&#30410;&#26102;&#25552;&#20379;&#24314;&#35758;&#12290;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22266;&#23450;&#30340;&#38750;&#20132;&#20114;&#24335;&#24314;&#35758;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24314;&#35758;&#26041;&#27861;&#33021;&#22815;&#22312;&#38656;&#35201;&#30340;&#26102;&#20505;&#25552;&#20379;&#24314;&#35758;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20419;&#36827;&#20154;&#31867;&#23398;&#20064;&#12289;&#20445;&#30041;&#20154;&#31867;&#20915;&#31574;&#30340;&#34917;&#20805;&#20248;&#21183;&#26041;&#38754;&#36824;&#20855;&#26377;&#39069;&#22806;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems are increasingly used for providing advice to facilitate human decision making in a wide range of domains, such as healthcare, criminal justice, and finance. Motivated by limitations of the current practice where algorithmic advice is provided to human users as a constant element in the decision-making pipeline, in this paper we raise the question of when should algorithms provide advice? We propose a novel design of AI systems in which the algorithm interacts with the human user in a two-sided manner and aims to provide advice only when it is likely to be beneficial for the user in making their decision. The results of a large-scale experiment show that our advising approach manages to provide advice at times of need and to significantly improve human decision making compared to fixed, non-interactive, advising approaches. This approach has additional advantages in facilitating human learning, preserving complementary strengths of human decision ma
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#37319;&#26679;&#21152;&#36895;&#12289;&#26032;&#30340;&#25193;&#25955;&#36807;&#31243;&#35774;&#35745;&#20197;&#21450;&#22312;&#19981;&#21516;&#31354;&#38388;&#20013;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#21019;&#26032;&#21162;&#21147;&#26088;&#22312;&#25552;&#39640;&#25193;&#25955;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2209.02646</link><description>&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Generative Diffusion Model. (arXiv:2209.02646v9 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#37319;&#26679;&#21152;&#36895;&#12289;&#26032;&#30340;&#25193;&#25955;&#36807;&#31243;&#35774;&#35745;&#20197;&#21450;&#22312;&#19981;&#21516;&#31354;&#38388;&#20013;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#21019;&#26032;&#21162;&#21147;&#26088;&#22312;&#25552;&#39640;&#25193;&#25955;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#34987;&#29992;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#29983;&#25104;&#36136;&#37327;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20855;&#26377;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#32791;&#26102;&#30340;&#36845;&#20195;&#29983;&#25104;&#36807;&#31243;&#21644;&#38480;&#21046;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22823;&#37327;&#26088;&#22312;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#37319;&#26679;&#21152;&#36895;&#21644;&#26032;&#30340;&#25193;&#25955;&#36807;&#31243;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#27969;&#24418;&#21644;&#31163;&#25955;&#31354;&#38388;&#20013;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#21019;&#24314;&#20004;&#20010;&#20219;&#24847;&#20998;&#24067;&#20043;&#38388;&#26725;&#26753;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#30340;&#21019;&#26032;&#20195;&#34920;&#20102;&#36817;&#24180;&#26469;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#21151;&#33021;&#21644;&#25928;&#29575;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are a prominent approach for data generation, and have been used to produce high quality samples in various domains. Diffusion models, an emerging class of deep generative models, have attracted considerable attention owing to their exceptional generative quality. Despite this, they have certain limitations, including a time-consuming iterative generation process and confinement to high-dimensional Euclidean space. This survey presents a plethora of advanced techniques aimed at enhancing diffusion models, including sampling acceleration and the design of new diffusion processes. In addition, we delve into strategies for implementing diffusion models in manifold and discrete spaces, maximum likelihood training for diffusion models, and methods for creating bridges between two arbitrary distributions. The innovations we discuss represent the efforts for improving the functionality and efficiency of diffusion models in recent years. To examine the efficacy of existi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2208.06648</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#23384;&#22312;&#19979;&#30340;&#22635;&#34917;&#31574;&#30053;&#65306;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness. (arXiv:2208.06648v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#24378;&#21270;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#65292;&#32780;&#25105;&#20204;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#25552;&#20986;&#65292;&#25968;&#25454;&#20013;&#32570;&#22833;&#30340;&#20869;&#23481;&#20063;&#20250;&#20135;&#29983;&#20559;&#35265;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#20559;&#35265;&#24050;&#32463;&#22312;&#21307;&#30103;&#21382;&#21490;&#19978;&#30041;&#19979;&#20102;&#28145;&#28145;&#30340;&#28889;&#21360;&#65292;&#23548;&#33268;&#36793;&#32536;&#21270;&#32676;&#20307;&#21463;&#21040;&#19981;&#24179;&#31561;&#30340;&#25252;&#29702;&#12290;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#36890;&#24120;&#21453;&#26144;&#20102;&#36825;&#20123;&#32676;&#20307;&#30340;&#24046;&#24322;&#65292;&#20294;&#26159;&#29305;&#23450;&#32676;&#20307;&#32570;&#22833;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#24433;&#21709;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#23613;&#31649;&#20854;&#28508;&#22312;&#24433;&#21709;&#24040;&#22823;&#65292;&#20294;&#22635;&#34917;&#24448;&#24448;&#34987;&#24573;&#35270;&#20026;&#19968;&#20010;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#32780;&#20851;&#27880;&#28857;&#25918;&#22312;&#20102;&#37325;&#24314;&#35823;&#24046;&#30340;&#20943;&#23569;&#21644;&#25972;&#20307;&#24615;&#33021;&#19978;&#65292;&#24573;&#30053;&#20102;&#22635;&#34917;&#22914;&#20309;&#23545;&#19981;&#21516;&#32676;&#20307;&#20135;&#29983;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning risks reinforcing biases present in data, and, as we argue in this work, in what is absent from data. In healthcare, biases have marked medical history, leading to unequal care affecting marginalised groups. Patterns in missing data often reflect these group discrepancies, but the algorithmic fairness implications of group-specific missingness are not well understood. Despite its potential impact, imputation is often an overlooked preprocessing step, with attention placed on the reduction of reconstruction error and overall performance, ignoring how imputation can affect groups differently. Our work studies how imputation choices affect reconstruction errors across groups and algorithmic fairness properties of downstream predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30495;&#20540;&#38598;&#20195;&#25968;&#65292;&#19968;&#31181;&#29992;&#20110;&#35777;&#26126;&#36923;&#36753;&#36830;&#25509;&#31526;&#30456;&#20114;&#26080;&#27861;&#23450;&#20041;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.04422</link><description>&lt;p&gt;
&#30495;&#20540;&#38598;&#20195;&#25968;&#65306;&#19968;&#31181;&#35777;&#26126;&#36923;&#36753;&#36830;&#25509;&#31526;&#26080;&#27861;&#23450;&#20041;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truth Set Algebra: A New Way to Prove Undefinability. (arXiv:2208.04422v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30495;&#20540;&#38598;&#20195;&#25968;&#65292;&#19968;&#31181;&#29992;&#20110;&#35777;&#26126;&#36923;&#36753;&#36830;&#25509;&#31526;&#30456;&#20114;&#26080;&#27861;&#23450;&#20041;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#35777;&#26126;&#36890;&#36807;&#24444;&#27492;&#20043;&#38388;&#30340;&#36923;&#36753;&#36830;&#25509;&#31526;&#30340;&#19981;&#21487;&#23450;&#20041;&#24615;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#20363;&#23376;&#35828;&#26126;&#20102;&#35813;&#25216;&#26415;&#12290;&#20854;&#20013;&#19968;&#20123;&#32467;&#26524;&#26159;&#29616;&#26377;&#23450;&#29702;&#30340;&#26032;&#35777;&#26126;&#65292;&#21478;&#19968;&#20123;&#32467;&#26524;&#26159;&#26412;&#25991;&#30340;&#21407;&#21019;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article proposes a new technique for proving the undefinability of logical connectives through each other and illustrates the technique with several examples. Some of the obtained results are new proofs of the existing theorems, others are original to this work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#22411;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#22120;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#23454;&#29616;&#26089;&#26399;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#21487;&#20197;&#21306;&#20998;&#23156;&#20799;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#65292;&#21363;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#21644;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#12290;</title><link>http://arxiv.org/abs/2208.00884</link><description>&lt;p&gt;
&#22522;&#20110;&#21387;&#21147;&#20998;&#24067;&#20998;&#26512;&#30340;&#23156;&#20799;&#36816;&#21160;&#20998;&#31867;&#8212;&#8212;&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#30340;&#38468;&#21152;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Infant movement classification through pressure distribution analysis -- added value for research and clinical implementation. (arXiv:2208.00884v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#22411;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#22120;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#23454;&#29616;&#26089;&#26399;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#21487;&#20197;&#21306;&#20998;&#23156;&#20799;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#65292;&#21363;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#21644;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#35774;&#22791;&#26469;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#26089;&#26399;&#30340;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#26469;&#21306;&#20998;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#65288;&#21363;&#22352;&#31435;&#19981;&#23433;&#36816;&#21160;&#65289;&#19982;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#65288;&#21363;&#25197;&#21160;&#36816;&#21160;&#65289;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#27599;&#20010;&#23156;&#20799;&#22312;&#20986;&#29983;&#21518; 4-16 &#21608;&#30340;&#38388;&#38548;&#26399;&#20869;&#36830;&#32493;&#19971;&#20010;&#23454;&#39564;&#23460;&#20250;&#35805;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#21253;&#25324;&#26469;&#33258;&#19968;&#20010; 32x32 &#32593;&#26684;&#21387;&#21147;&#20256;&#24863;&#22443;&#21450;&#20854; 1024 &#20010;&#20256;&#24863;&#22120;&#30340;&#21387;&#21147;&#25968;&#25454;&#12290;&#20026;&#20102;&#39564;&#35777;&#27010;&#24565;&#65292;&#20174;&#20004;&#20010;&#30446;&#26631;&#24180;&#40836;&#27573;&#20013;&#65292;&#27599;&#20010;&#25345;&#32493; 5 &#31186;&#30340; 1776 &#20010;&#21387;&#21147;&#25968;&#25454;&#29255;&#27573;&#34987;&#29992;&#20110;&#36816;&#21160;&#20998;&#31867;&#12290;&#27599;&#20010;&#29255;&#27573;&#37117;&#26159;&#26681;&#25454;&#30456;&#24212;&#30340;&#21516;&#27493;&#35270;&#39057;&#25968;&#25454;&#30001;&#20154;&#24037;&#35780;&#20272;&#21592;&#36827;&#34892;&#39044;&#27880;&#37322;&#30340;&#65292;&#26631;&#35760;&#20026;&#22352;&#31435;&#19981;&#23433;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at objective early detection of neuromotor disorders such as cerebral palsy, we proposed an innovative non-intrusive approach using a pressure sensing device to classify infant general movements (GMs). Here, we tested the feasibility of using pressure data to differentiate typical GM patterns of the ''fidgety period'' (i.e., fidgety movements) vs. the ''pre-fidgety period'' (i.e., writhing movements). Participants (N = 45) were sampled from a typically-developing infant cohort. Multi-modal sensor data, including pressure data from a 32x32-grid pressure sensing mat with 1024 sensors, were prospectively recorded for each infant in seven succeeding laboratory sessions in biweekly intervals from 4-16 weeks of post-term age. For proof-of-concept, 1776 pressure data snippets, each 5s long, from the two targeted age periods were taken for movement classification. Each snippet was pre-annotated based on corresponding synchronised video data by human assessors as either fidgety present (
&lt;/p&gt;</description></item><item><title>MABe22&#26159;&#19968;&#20010;&#22810;&#29289;&#31181;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#34892;&#20026;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#23427;&#37319;&#38598;&#20102;&#26469;&#33258;&#21508;&#31181;&#29983;&#29289;&#23398;&#23454;&#39564;&#30340;&#25968;&#25454;&#65292;&#27979;&#35797;&#20102;&#22810;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#34892;&#21160;&#25968;&#25454;&#38598;&#19978;&#30340;&#26041;&#27861;&#19981;&#33021;&#23436;&#20840;&#36866;&#29992;&#20110;&#21160;&#29289;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2207.10553</link><description>&lt;p&gt;
MABe22&#65306;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#34892;&#20026;&#34920;&#31034;&#30340;&#22810;&#29289;&#31181;&#22810;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior. (arXiv:2207.10553v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10553
&lt;/p&gt;
&lt;p&gt;
MABe22&#26159;&#19968;&#20010;&#22810;&#29289;&#31181;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#34892;&#20026;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#23427;&#37319;&#38598;&#20102;&#26469;&#33258;&#21508;&#31181;&#29983;&#29289;&#23398;&#23454;&#39564;&#30340;&#25968;&#25454;&#65292;&#27979;&#35797;&#20102;&#22810;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#34892;&#21160;&#25968;&#25454;&#38598;&#19978;&#30340;&#26041;&#27861;&#19981;&#33021;&#23436;&#20840;&#36866;&#29992;&#20110;&#21160;&#29289;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MABe22&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#26234;&#33021;&#20307;&#35270;&#39057;&#21644;&#36712;&#36857;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#34892;&#20026;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#35813;&#25968;&#25454;&#38598;&#37319;&#38598;&#33258;&#21508;&#31181;&#29983;&#29289;&#23398;&#23454;&#39564;&#65292;&#21253;&#25324;&#19977;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#23567;&#40736;&#19977;&#20803;&#32452;&#65288;470&#19975;&#24103;&#30340;&#35270;&#39057;+&#23039;&#24577;&#36319;&#36394;&#25968;&#25454;&#65292;1000&#19975;&#24103;&#30340;&#20165;&#23039;&#24577;&#25968;&#25454;&#65289;&#65292;&#20849;&#29983;&#30002;&#34411;-&#34434;&#34433;&#30456;&#20114;&#20316;&#29992;&#65288;1000&#19975;&#24103;&#30340;&#35270;&#39057;&#25968;&#25454;&#65289;&#21644;&#19968;&#32676;&#20114;&#21160;&#30340;&#33485;&#34631;&#65288;440&#19975;&#24103;&#30340;&#23039;&#24577;&#36319;&#36394;&#25968;&#25454;&#65289;&#12290;&#38500;&#20102;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#30495;&#23454;&#29983;&#27963;&#20013;&#30340;&#19979;&#28216;&#20998;&#26512;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#35780;&#20272;&#23427;&#20204;&#22312;&#20445;&#30041;&#20851;&#20110;&#23454;&#39564;&#26465;&#20214;&#65288;&#20363;&#22914;&#21697;&#31995;&#12289;&#26102;&#38388;&#12289;&#20809;&#36951;&#20256;&#21050;&#28608;&#65289;&#21644;&#21160;&#29289;&#34892;&#20026;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#35270;&#39057;&#21644;&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#22522;&#20934;&#30340;&#29992;&#36884;&#65292;&#24182;&#25581;&#31034;&#20102;&#20351;&#29992;&#20154;&#31867;&#34892;&#21160;&#25968;&#25454;&#38598;&#24320;&#21457;&#30340;&#26041;&#27861;&#19981;&#33021;&#23436;&#20840;&#36866;&#29992;&#20110;&#21160;&#29289;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MABe22, a large-scale, multi-agent video and trajectory benchmark to assess the quality of learned behavior representations. This dataset is collected from a variety of biology experiments, and includes triplets of interacting mice (4.7 million frames video+pose tracking data, 10 million frames pose only), symbiotic beetle-ant interactions (10 million frames video data), and groups of interacting flies (4.4 million frames of pose tracking data). Accompanying these data, we introduce a panel of real-life downstream analysis tasks to assess the quality of learned representations by evaluating how well they preserve information about the experimental conditions (e.g. strain, time of day, optogenetic stimulation) and animal behavior. We test multiple state-of-the-art self-supervised video and trajectory representation learning methods to demonstrate the use of our benchmark, revealing that methods developed using human action datasets do not fully translate to animal datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.10540</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31185;&#23398;&#21457;&#29616;&#20013;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#22522;&#20110;&#36153;&#26364;&#29289;&#29702;&#35762;&#20041;&#30340;&#19968;&#32452;&#20844;&#24335;&#65292;&#25105;&#20204;&#37325;&#26032;&#21019;&#24314;&#20102;120&#20010;&#25968;&#25454;&#38598;&#65292;&#35752;&#35770;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#24615;&#33021;&#65288;SRSD&#65289;&#12290;&#23545;&#20110;&#36825;120&#20010;SRSD&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;&#20844;&#24335;&#21450;&#20854;&#21464;&#37327;&#30340;&#23646;&#24615;&#65292;&#35774;&#35745;&#20102;&#21512;&#29702;&#30340;&#23454;&#20540;&#33539;&#22260;&#26469;&#37319;&#26679;&#20540;&#65292;&#20197;&#20415;&#25105;&#20204;&#30340;&#26032;SRSD&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;SRSD&#30340;&#28508;&#21147;&#65292;&#22914;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#20013;&#65288;&#37325;&#26032;&#65289;&#21457;&#29616;&#29289;&#29702;&#23450;&#24459;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#21478;&#22806;120&#20010;&#21253;&#21547;&#34394;&#25311;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#26816;&#39564;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20165;&#36873;&#25321;&#24517;&#35201;&#21464;&#37327;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39044;&#27979;&#26041;&#31243;&#19982;&#30495;&#23454;&#26041;&#31243;&#26641;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#65288;NED&#65289;&#26469;&#35299;&#20915;&#29616;&#26377;SR&#24230;&#37327;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20108;&#20803;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits datasets and evaluation criteria for Symbolic Regression (SR), specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. We also create another 120 datasets that contain dummy variables to examine whether SR methods can choose necessary variables only. Besides, we propose to use normalized edit distances (NED) between a predicted equation and the true equation trees for addressing a critical issue that existing SR metrics are either binary
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20844;&#27491;&#24433;&#21709;&#20989;&#25968;&#65288;FIF&#65289;&#65292;&#36890;&#36807;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#37327;&#21270;&#20102;&#19981;&#21516;&#29305;&#24449;&#23545;&#20998;&#31867;&#22120;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#20013;&#30340;&#26680;&#24515;&#20851;&#27880;&#28857;&#12290;</title><link>http://arxiv.org/abs/2206.00667</link><description>&lt;p&gt;
&#29305;&#24449;&#26377;&#22810;&#20559;&#35265;&#65311;&#65306;&#36890;&#36807;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#35745;&#31639;&#20844;&#27491;&#24433;&#21709;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
How Biased are Your Features?: Computing Fairness Influence Functions with Global Sensitivity Analysis. (arXiv:2206.00667v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20844;&#27491;&#24433;&#21709;&#20989;&#25968;&#65288;FIF&#65289;&#65292;&#36890;&#36807;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#37327;&#21270;&#20102;&#19981;&#21516;&#29305;&#24449;&#23545;&#20998;&#31867;&#22120;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#20013;&#30340;&#26680;&#24515;&#20851;&#27880;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#22240;&#20854;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26410;&#21463;&#30417;&#31649;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21487;&#33021;&#23545;&#25968;&#25454;&#20013;&#30340;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#34920;&#29616;&#20986;&#20559;&#35265;&#65292;&#22240;&#27492;&#37327;&#21270;&#21644;&#20943;&#36731;&#20998;&#31867;&#22120;&#20559;&#35265;&#26159;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#26680;&#24515;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#26088;&#22312;&#37327;&#21270;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#29305;&#24449;&#23545;&#20998;&#31867;&#22120;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20844;&#27491;&#24433;&#21709;&#20989;&#25968;&#65288;FIF&#65289;&#12290;&#35813;&#20989;&#25968;&#23558;&#20559;&#35265;&#20998;&#35299;&#20026;&#20854;&#22312;&#20010;&#20307;&#29305;&#24449;&#21644;&#22810;&#20010;&#29305;&#24449;&#30340;&#20132;&#38598;&#20013;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#29616;&#26377;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#24230;&#37327;&#34920;&#31034;&#20026;&#20998;&#31867;&#22120;&#39044;&#27979;&#30340;&#26465;&#20214;&#26041;&#24046;&#30340;&#24046;&#24322;&#65292;&#24182;&#26681;&#25454;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#20998;&#35299;&#36827;&#34892;&#26041;&#24046;&#20272;&#35745;&#12290;&#20026;&#20102;&#20272;&#35745;FIFs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FairXplainer&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#24212;&#29992;&#20998;&#31867;&#22120;&#39044;&#27979;&#30340;&#26041;&#24046;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier's prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm FairXplainer that applies variance decomposition of classifier's prediction following l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#32570;&#22833;&#35266;&#27979;&#30340;&#20301;&#32622;&#26410;&#30693;&#26102;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19981;&#38656;&#35201;&#20808;&#39564;&#20449;&#24687;&#30340;&#37325;&#24314;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.06527</link><description>&lt;p&gt;
&#24403;&#32570;&#22833;&#35266;&#27979;&#30340;&#20301;&#32622;&#26410;&#30693;&#26102;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Hidden Markov Models When the Locations of Missing Observations are Unknown. (arXiv:2203.06527v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#32570;&#22833;&#35266;&#27979;&#30340;&#20301;&#32622;&#26410;&#30693;&#26102;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19981;&#38656;&#35201;&#20808;&#39564;&#20449;&#24687;&#30340;&#37325;&#24314;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#26159;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#30340;&#26368;&#24120;&#29992;&#30340;&#32479;&#35745;&#27169;&#22411;&#20043;&#19968;&#12290;HMM&#20855;&#26377;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36825;&#20063;&#26159;&#23427;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#20851;&#38190;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;HMM&#23398;&#20064;&#31639;&#27861;&#22522;&#20110;&#32570;&#22833;&#35266;&#27979;&#22312;&#35266;&#27979;&#24207;&#21015;&#20013;&#30340;&#20301;&#32622;&#24050;&#30693;&#30340;&#20551;&#35774;&#12290;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#24120;&#24120;&#19981;&#25104;&#31435;&#65292;&#22240;&#27492;&#36890;&#24120;&#20351;&#29992;&#29305;&#27530;&#21464;&#20307;&#30340;HMM&#65292;&#31216;&#20026;Silent-state HMMs&#65288;SHMMs&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#28508;&#22312;&#38142;&#30340;&#29305;&#23450;&#32467;&#26500;&#20551;&#35774;&#65292;&#27604;&#22914;&#38750;&#24490;&#29615;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;&#22312;&#38750;&#24490;&#29615;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#37325;&#24314;&#25928;&#26524;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#20855;&#26377;&#26410;&#30693;&#32570;&#22833;&#35266;&#27979;&#20301;&#32622;&#25968;&#25454;&#20013;&#23398;&#20064;HMM&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#30340;&#37325;&#24314;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Hidden Markov Model (HMM) is one of the most widely used statistical models for sequential data analysis. One of the key reasons for this versatility is the ability of HMM to deal with missing data. However, standard HMM learning algorithms rely crucially on the assumption that the positions of the missing observations \emph{within the observation sequence} are known. In the natural sciences, where this assumption is often violated, special variants of HMM, commonly known as Silent-state HMMs (SHMMs), are used. Despite their widespread use, these algorithms strongly rely on specific structural assumptions of the underlying chain, such as acyclicity, thus limiting the applicability of these methods. Moreover, even in the acyclic case, it has been shown that these methods can lead to poor reconstruction. In this paper we consider the general problem of learning an HMM from data with unknown missing observation locations. We provide reconstruction algorithms that do not require any as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#23384;&#21477;&#27861;&#26641;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#19981;&#21516;&#35299;&#26512;&#22120;&#30340;&#21487;&#38752;&#24615;&#65292;&#20197;&#25345;&#32493;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32858;&#21512;&#20381;&#23384;&#21477;&#27861;&#26641;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26641;&#20043;&#38388;&#30340;&#32463;&#20856;&#23545;&#31216;&#36317;&#31163;&#24230;&#37327;&#65292;&#32599;&#23486;&#36874;-&#31119;&#23572;&#20857;&#36317;&#31163;&#30340;&#21152;&#26435;&#21644;&#65292;&#23454;&#29616;&#20102;&#26641;&#32467;&#26500;&#30340;&#30495;&#23454;&#24615;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2201.07905</link><description>&lt;p&gt;
CPTAM: &#20381;&#23384;&#21477;&#27861;&#26641;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CPTAM: Constituency Parse Tree Aggregation Method. (arXiv:2201.07905v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#23384;&#21477;&#27861;&#26641;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#19981;&#21516;&#35299;&#26512;&#22120;&#30340;&#21487;&#38752;&#24615;&#65292;&#20197;&#25345;&#32493;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32858;&#21512;&#20381;&#23384;&#21477;&#27861;&#26641;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26641;&#20043;&#38388;&#30340;&#32463;&#20856;&#23545;&#31216;&#36317;&#31163;&#24230;&#37327;&#65292;&#32599;&#23486;&#36874;-&#31119;&#23572;&#20857;&#36317;&#31163;&#30340;&#21152;&#26435;&#21644;&#65292;&#23454;&#29616;&#20102;&#26641;&#32467;&#26500;&#30340;&#30495;&#23454;&#24615;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20351;&#29992;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#26469;&#26681;&#25454;&#30701;&#35821;&#32467;&#26500;&#35821;&#27861;&#29702;&#35299;&#21477;&#23376;&#30340;&#21477;&#27861;&#32467;&#26500;&#12290;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#24050;&#34987;&#25552;&#20986;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#30456;&#21516;&#30340;&#21477;&#23376;&#21487;&#33021;&#20250;&#25552;&#20379;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#30340;&#35821;&#26009;&#24211;&#12290;&#26412;&#25991;&#37319;&#29992;&#30495;&#23454;&#24615;&#21457;&#29616;&#30340;&#24605;&#24819;&#65292;&#36890;&#36807;&#20272;&#35745;&#19981;&#21516;&#35299;&#26512;&#22120;&#30340;&#21487;&#38752;&#24615;&#26469;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#35299;&#26512;&#22120;&#30340;&#20381;&#23384;&#21477;&#27861;&#26641;&#65292;&#20197;&#25345;&#32493;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32858;&#21512;&#20381;&#23384;&#21477;&#27861;&#26641;&#12290;&#25105;&#20204;&#23558;&#20381;&#23384;&#21477;&#27861;&#26641;&#32858;&#21512;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#32467;&#26500;&#32858;&#21512;&#21644;&#25104;&#20998;&#26631;&#31614;&#32858;&#21512;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26641;&#32467;&#26500;&#30340;&#30495;&#23454;&#24615;&#21457;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26641;&#20043;&#38388;&#30340;&#32463;&#20856;&#23545;&#31216;&#36317;&#31163;&#24230;&#37327;&#65292;&#21363;&#32599;&#23486;&#36874;-&#31119;&#23572;&#20857;&#36317;&#31163;&#30340;&#21152;&#26435;&#21644;&#12290;&#23545;&#19981;&#21516;&#35821;&#35328;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diverse Natural Language Processing tasks employ constituency parsing to understand the syntactic structure of a sentence according to a phrase structure grammar. Many state-of-the-art constituency parsers are proposed, but they may provide different results for the same sentences, especially for corpora outside their training domains. This paper adopts the truth discovery idea to aggregate constituency parse trees from different parsers by estimating their reliability in the absence of ground truth. Our goal is to consistently obtain high-quality aggregated constituency parse trees. We formulate the constituency parse tree aggregation problem in two steps, structure aggregation and constituent label aggregation. Specifically, we propose the first truth discovery solution for tree structures by minimizing the weighted sum of Robinson-Foulds (RF) distances, a classic symmetric distance metric between two trees. Extensive experiments are conducted on benchmark datasets in different langu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TND-NAS&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#21644;&#22810;&#30446;&#26631;NAS&#30340;&#20860;&#23481;&#24615;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.03892</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-objective Neural Architecture Search Framework via Policy Gradient Algorithm. (arXiv:2111.03892v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TND-NAS&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#21644;&#22810;&#30446;&#26631;NAS&#30340;&#20860;&#23481;&#24615;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#24050;&#25104;&#20026;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#39046;&#22495;&#30340;&#20027;&#27969;&#30740;&#31350;&#35838;&#39064;&#65292;&#30456;&#23545;&#20110;&#26089;&#26399;&#30340;EA-based&#21644;RL-based&#26041;&#27861;&#65292;&#20854;&#39640;&#25928;&#29575;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24050;&#19981;&#20877;&#33021;&#22815;&#33258;&#28982;&#22320;&#24212;&#23545;&#19981;&#21487;&#24494;&#21442;&#25968;&#65292;&#22914;&#33021;&#28304;&#21644;&#36164;&#28304;&#21463;&#38480;&#25928;&#29575;&#31561;&#12290;&#38024;&#23545;&#22810;&#30446;&#26631;NAS&#39046;&#22495;&#30340;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#23545;&#27599;&#20010;&#20505;&#36873;&#26550;&#26500;&#36827;&#34892;&#21807;&#19968;&#30340;&#20248;&#21270;&#65292;&#22240;&#27492;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TND-NAS&#65292;&#23427;&#20855;&#26377;&#19981;&#21487;&#24494;&#21442;&#25968;&#30340;&#20860;&#23481;&#24615;&#21644;&#19981;&#21516;iable NAS&#26694;&#26550;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;TND-NAS&#22312;&#25628;&#32034;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable architecture search has gradually become the mainstream research topic in the field of Neural Architecture Search (NAS) for its high efficiency compared with the early NAS (EA-based, RL-based) methods. Recent differentiable NAS also aims at further improving the search performance and reducing the GPU-memory consumption. However, these methods are no longer naturally capable of tackling the non-differentiable objectives, e.g., energy, resource-constrained efficiency, and other metrics, let alone the multi-objective search demands. Researches in the multi-objective NAS field target this but requires vast computational resources cause of the sole optimization of each candidate architecture. In light of this discrepancy, we propose the TND-NAS, which is with the merits of the high efficiency in differentiable NAS framework and the compatibility among non-differentiable metrics in Multi-objective NAS. Under the differentiable NAS framework, with the continuous relaxation of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;ceteris paribus&#20026;&#22522;&#30784;&#30340;&#27169;&#24577;&#35821;&#35328;&#65292;&#29992;&#20110;&#35299;&#37322;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#21450;&#20854;&#23646;&#24615;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#20851;&#20110;&#35821;&#35328;&#22522;&#25968;&#30340;&#35777;&#26126;&#31995;&#32479;&#30340;&#23436;&#22791;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#26080;&#38480;&#21464;&#37327;&#21644;&#26377;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36825;&#31181;&#35821;&#35328;&#26469;&#24418;&#24335;&#21270;&#22810;&#31181;&#35299;&#37322;&#27010;&#24565;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#12289;&#23545;&#27604;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20197;&#21450;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2105.14452</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#36923;&#36753;&#26694;&#26550;&#29992;&#20110;&#20998;&#31867;&#22120;&#31995;&#32479;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A unified logical framework for explanations in classifier systems. (arXiv:2105.14452v6 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.14452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;ceteris paribus&#20026;&#22522;&#30784;&#30340;&#27169;&#24577;&#35821;&#35328;&#65292;&#29992;&#20110;&#35299;&#37322;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#21450;&#20854;&#23646;&#24615;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#20851;&#20110;&#35821;&#35328;&#22522;&#25968;&#30340;&#35777;&#26126;&#31995;&#32479;&#30340;&#23436;&#22791;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#26080;&#38480;&#21464;&#37327;&#21644;&#26377;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36825;&#31181;&#35821;&#35328;&#26469;&#24418;&#24335;&#21270;&#22810;&#31181;&#35299;&#37322;&#27010;&#24565;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#12289;&#23545;&#27604;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20197;&#21450;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20013;&#65292;&#24067;&#23572;&#20989;&#25968;&#23545;&#20110;&#35299;&#37322;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#24067;&#23572;&#20989;&#25968;&#26041;&#27861;&#37319;&#29992;&#21629;&#39064;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;ceteris paribus&#20026;&#22522;&#30784;&#30340;&#27169;&#24577;&#35821;&#35328;&#65292;&#25903;&#25345;&#23545;&#20108;&#36827;&#21046;&#36755;&#20837;&#20998;&#31867;&#22120;&#21450;&#20854;&#23646;&#24615;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#20998;&#31867;&#22120;&#27169;&#22411;&#65292;&#23558;&#20854;&#20844;&#29702;&#21270;&#20026;&#20851;&#20110;&#35821;&#35328;&#22522;&#25968;&#30340;&#20004;&#20010;&#35777;&#26126;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#20844;&#29702;&#31995;&#32479;&#30340;&#23436;&#22791;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26080;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#27169;&#24577;&#35821;&#35328;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#38382;&#39064;&#26159;NEXPTIME&#23436;&#20840;&#30340;&#65292;&#32780;&#22312;&#26377;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#65292;&#35813;&#38382;&#39064;&#21464;&#20026;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#22312;&#26080;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;NP&#29255;&#27573;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#35821;&#35328;&#26469;&#24418;&#24335;&#21270;&#23545;&#20107;&#23454;&#26465;&#20214;&#20197;&#21450;&#21253;&#25324;&#20174;&#23646;&#12289;&#23545;&#27604;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20197;&#21450;&#20559;&#35265;&#22312;&#20869;&#30340;&#21508;&#31181;&#35299;&#37322;&#27010;&#24565;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a renewed interest in Boolean function in explaining binary classifiers in the field of explainable AI (XAI). The standard approach of Boolean function is propositional logic. We present a modal language of a ceteris paribus nature which supports reasoning about binary input classifiers and their properties. We study a family of classifier models, axiomatize it as two proof systems regarding the cardinality of the language and show completeness of our axiomatics. Moreover, we prove that satisfiability checking problem for our modal language is NEXPTIME-complete in the infinite-variable case, while it becomes polynomial in the finite-variable case. We furthermore identify an interesting NP fragment of our language in the infinite-variable case. We leverage the language to formalize counterfactual conditional as well as a variety of notions of explanation including abductive, contrastive and counterfactual explanations, and biases. Finally, we present two exte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#28369;&#25554;&#20540;&#30340;&#26041;&#24335;&#23558;&#27169;&#20223;&#23398;&#20064;&#21644;&#32431;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#32479;&#19968;&#36215;&#26469;&#12290;&#26694;&#26550;&#22260;&#32469;&#30528;&#19968;&#31181;&#34913;&#37327;&#34892;&#20026;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#20559;&#31163;&#31243;&#24230;&#30340;&#24369;&#29256;&#26412;&#38598;&#20013;&#31995;&#25968;&#23637;&#24320;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#30740;&#31350;&#32773;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31639;&#27861;&#35774;&#35745;&#30340;&#38382;&#39064;&#65306;&#33021;&#21542;&#24320;&#21457;&#20986;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#30340;&#31639;&#27861;&#65311;</title><link>http://arxiv.org/abs/2103.12021</link><description>&lt;p&gt;
&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#19982;&#27169;&#20223;&#23398;&#20064;&#30340;&#26725;&#26753;&#65306;&#19968;&#20010;&#24754;&#35266;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism. (arXiv:2103.12021v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.12021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#28369;&#25554;&#20540;&#30340;&#26041;&#24335;&#23558;&#27169;&#20223;&#23398;&#20064;&#21644;&#32431;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#32479;&#19968;&#36215;&#26469;&#12290;&#26694;&#26550;&#22260;&#32469;&#30528;&#19968;&#31181;&#34913;&#37327;&#34892;&#20026;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#20559;&#31163;&#31243;&#24230;&#30340;&#24369;&#29256;&#26412;&#38598;&#20013;&#31995;&#25968;&#23637;&#24320;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#30740;&#31350;&#32773;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31639;&#27861;&#35774;&#35745;&#30340;&#38382;&#39064;&#65306;&#33021;&#21542;&#24320;&#21457;&#20986;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#30340;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#26426;&#65288;&#25110;&#25209;&#27425;&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#20174;&#22266;&#23450;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#20027;&#21160;&#25910;&#38598;&#25968;&#25454;&#12290;&#26681;&#25454;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#32452;&#25104;&#65292;&#20027;&#35201;&#20351;&#29992;&#20004;&#31181;&#26041;&#27861;&#65306;&#36866;&#29992;&#20110;&#19987;&#23478;&#25968;&#25454;&#38598;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36890;&#24120;&#38656;&#35201;&#22343;&#21248;&#35206;&#30422;&#25968;&#25454;&#38598;&#30340;&#32431;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#12290;&#20174;&#23454;&#36341;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25968;&#25454;&#38598;&#36890;&#24120;&#20559;&#31163;&#36825;&#20004;&#20010;&#26497;&#31471;&#65292;&#24182;&#19988;&#36890;&#24120;&#20107;&#20808;&#19981;&#30693;&#36947;&#30830;&#20999;&#30340;&#25968;&#25454;&#32452;&#25104;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#22312;&#25968;&#25454;&#32452;&#25104;&#30340;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#24179;&#28369;&#25554;&#20540;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#27169;&#20223;&#23398;&#20064;&#21644;&#32431;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#12290;&#26032;&#30340;&#26694;&#26550;&#22260;&#32469;&#19968;&#20010;&#24369;&#29256;&#26412;&#30340;&#38598;&#20013;&#31995;&#25968;&#23637;&#24320;&#65292;&#35813;&#31995;&#25968;&#34913;&#37327;&#20102;&#34892;&#20026;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#20043;&#38388;&#30340;&#20559;&#31163;&#31243;&#24230;&#12290;&#22312;&#36825;&#20010;&#26032;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31639;&#27861;&#35774;&#35745;&#30340;&#38382;&#39064;&#65306;&#33021;&#21542;&#24320;&#21457;&#20986;&#19968;&#31181;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#30340;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone.  Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2009.07888</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning in Deep Reinforcement Learning: A Survey. (arXiv:2009.07888v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.07888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38500;&#20102;&#22312;&#26426;&#22120;&#20154;&#21644;&#28216;&#25103;&#31561;&#35832;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#33391;&#22909;&#21069;&#26223;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#21508;&#31181;&#25361;&#25112;&#30340;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#19987;&#19994;&#30693;&#35782;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#26368;&#20808;&#36827;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20998;&#26512;&#26234;&#33021;&#20307;&#38169;&#35823;&#35299;&#35835;&#25110;&#38169;&#35823;&#24863;&#30693;&#30495;&#23454;&#20915;&#31574;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#23450;&#20041;&#26469;&#35780;&#20272;&#26234;&#33021;&#20307;&#22240;&#26524;&#24605;&#32500;&#27700;&#24179;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#31574;&#30053;&#26469;&#35782;&#21035;&#26234;&#33021;&#20307;&#22312;&#32570;&#20047;&#23436;&#20840;&#29702;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#20449;&#24565;&#12290;</title><link>http://arxiv.org/abs/2007.07703</link><description>&lt;p&gt;
&#12298;&#22240;&#26524;&#24605;&#32500;&#22833;&#36133;&#12299;
&lt;/p&gt;
&lt;p&gt;
Failures of Contingent Thinking. (arXiv:2007.07703v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20998;&#26512;&#26234;&#33021;&#20307;&#38169;&#35823;&#35299;&#35835;&#25110;&#38169;&#35823;&#24863;&#30693;&#30495;&#23454;&#20915;&#31574;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#23450;&#20041;&#26469;&#35780;&#20272;&#26234;&#33021;&#20307;&#22240;&#26524;&#24605;&#32500;&#27700;&#24179;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#31574;&#30053;&#26469;&#35782;&#21035;&#26234;&#33021;&#20307;&#22312;&#32570;&#20047;&#23436;&#20840;&#29702;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#19968;&#20010;&#38169;&#35823;&#35299;&#35835;&#25110;&#38169;&#35823;&#24863;&#30693;&#30495;&#23454;&#20915;&#31574;&#38382;&#39064;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#34892;&#20026;&#22312;&#23454;&#39564;&#29615;&#22659;&#20013;&#35266;&#23519;&#21040;&#30340;&#34920;&#29616;&#20026;&#26080;&#27861;&#29702;&#35299;&#21547;&#20041;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#26080;&#27861;&#27491;&#30830;&#32771;&#34385;&#21508;&#31181;&#19982;&#20851;&#38190;&#25903;&#20184;&#30456;&#20851;&#30340;&#24773;&#20917;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#24863;&#30693;&#22240;&#26524;&#20851;&#31995;&#30340;&#34892;&#20026;&#23450;&#20041;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#24341;&#23548;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#26234;&#33021;&#20307;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35299;&#37322;&#30830;&#23450;&#20102;&#20854;&#34892;&#20026;&#30340;&#20027;&#35266;&#29366;&#24577;&#31354;&#38388;&#12290;&#36890;&#36807;&#20998;&#26512;&#36825;&#20010;&#29366;&#24577;&#31354;&#38388;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#39537;&#21160;&#32463;&#39564;&#29616;&#35937;&#30340;&#36923;&#36753;&#22797;&#26434;&#24615;&#30340;&#19981;&#21516;&#22522;&#20934;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#29702;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26082;&#25552;&#20379;&#20102;&#35780;&#20272;&#26234;&#33021;&#20307;&#22240;&#26524;&#24605;&#32500;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#21448;&#25552;&#20379;&#20102;&#22312;&#27809;&#26377;&#23436;&#20840;&#29702;&#24615;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#20854;&#20449;&#24565;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a theoretical framework to analyze an agent who misinterprets or misperceives the true decision problem she faces. We show that a wide range of behavior observed in experimental settings manifest as failures to perceive implications, in other words, to properly account for the logical relationships between various payoff relevant contingencies. We present a behavioral definition of perceived implication, thereby providing an elicitation technique, and show that an agent's account of implication identifies a subjective state-space that underlies her behavior. By analyzing this state-space, we characterize distinct benchmarks of logical sophistication that drive empirical phenomena. We disentangle static and dynamic rationality. Thus, our framework delivers both a methodology for assessing an agent's level of contingent thinking and a strategy for identifying her beliefs in the absence full rationality.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#35821;&#20041;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#31995;&#32479;&#20132;&#20114;&#27169;&#22411;&#65292;&#24182;&#19981;&#20381;&#36182;&#20110;&#23383;&#31526;&#21040;&#27010;&#24565;&#30340;&#8220;&#24515;&#29702;&#8221;&#26144;&#23556;&#65292;&#26469;&#29702;&#35299;&#20132;&#20114;&#20013;&#23383;&#31526;&#30340;&#8220;&#21547;&#20041;&#8221;&#12290;</title><link>http://arxiv.org/abs/2007.06258</link><description>&lt;p&gt;
&#20132;&#20114;&#35821;&#20041;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A model of interaction semantics. (arXiv:2007.06258v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.06258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#35821;&#20041;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#31995;&#32479;&#20132;&#20114;&#27169;&#22411;&#65292;&#24182;&#19981;&#20381;&#36182;&#20110;&#23383;&#31526;&#21040;&#27010;&#24565;&#30340;&#8220;&#24515;&#29702;&#8221;&#26144;&#23556;&#65292;&#26469;&#29702;&#35299;&#20132;&#20114;&#20013;&#23383;&#31526;&#30340;&#8220;&#21547;&#20041;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#20132;&#20114;&#35821;&#20041;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#20132;&#20114;&#20013;&#8220;&#21547;&#20041;&#8221;&#29702;&#35299;&#30340;&#26576;&#31181;&#35266;&#28857;&#12290;&#36890;&#36807;&#26500;&#24314;&#31995;&#32479;&#20132;&#20114;&#27169;&#22411;&#65292;&#25105;&#23558;&#20132;&#20114;&#35821;&#20041;&#27169;&#22411;&#32467;&#26500;&#21270;&#65292;&#31867;&#20284;&#20110;&#24418;&#24335;&#35821;&#35328;&#30340;&#35821;&#20041;&#65306;&#39318;&#20808;&#65292;&#25105;&#30830;&#23450;&#36866;&#24403;&#30340;&#21464;&#37327;&#20197;&#36171;&#20540;&#65292;&#28982;&#21518;&#65292;&#25105;&#30830;&#23450;&#35299;&#37322;&#20989;&#25968;&#20197;&#25552;&#20379;&#24847;&#20041;&#12290;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#20132;&#20114;&#35821;&#20041;&#27169;&#22411;&#65292;&#21487;&#20197;&#19981;&#20381;&#36182;&#20110;&#20174;&#23383;&#31526;&#21040;&#27010;&#24565;&#30340;&#8220;&#24515;&#29702;&#8221;&#26144;&#23556;&#65292;&#36825;&#19982;&#36335;&#24503;&#32500;&#24076;&#183;&#32500;&#29305;&#26681;&#26031;&#22374;&#30340;&#35266;&#28857;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: The purpose of this article is to propose, based on a model of an interaction semantics, a certain understanding of the ''meaning'' of the exchanged characters within an interaction.  Methodology: Based on a model of system interaction, I structure the model of interaction semantics similar to the semantics of a formal language: first, I identify adequate variables in my interaction model to assign values to, and second, I identify the interpretation function to provide meaning. Thereby I arrive at a model of interaction semantics which, in the sense of the late Ludwig Wittgenstein, can do without a 'mental' mapping from characters to concepts.  Findings: The key findings are a better understanding of the tight relation between the informatical approach to model interactions and game theory; of the central 'chicken and egg' problem, any natural language has to solve, namely that to interact sensibly, we have to understand each other and to acquire a common understanding, we ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#26435;&#37325;&#31034;&#33539;&#30340;&#20803;&#36866;&#24212;&#24615;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#20808;&#21069;&#30693;&#35782;&#36827;&#34892;&#20998;&#37197;&#37325;&#35201;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#20309;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#22312;&#22810;&#26679;&#21270;&#29615;&#22659;&#20219;&#21153;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/1911.10322</link><description>&lt;p&gt;
&#20351;&#29992;&#37325;&#35201;&#26435;&#37325;&#31034;&#33539;&#30340;&#20803;&#36866;&#24212;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta Adaptation using Importance Weighted Demonstrations. (arXiv:1911.10322v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.10322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#26435;&#37325;&#31034;&#33539;&#30340;&#20803;&#36866;&#24212;&#24615;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#20808;&#21069;&#30693;&#35782;&#36827;&#34892;&#20998;&#37197;&#37325;&#35201;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#20309;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#22312;&#22810;&#26679;&#21270;&#29615;&#22659;&#20219;&#21153;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#27169;&#20223;&#23398;&#20064;&#21464;&#24471;&#26497;&#20026;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#36712;&#36857;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#65292;&#20165;&#20165;&#22522;&#20110;&#36830;&#32493;&#32858;&#21512;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#26159;&#24466;&#21171;&#30340;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20998;&#24067;&#21457;&#29983;&#22914;&#27492;&#22823;&#30340;&#21464;&#21270;&#65292;&#20197;&#33267;&#20110;&#26234;&#33021;&#20307;&#24456;&#38590;&#25512;&#26029;&#20986;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#19968;&#32452;&#29305;&#23450;&#20219;&#21153;&#30340;&#20808;&#21069;&#30693;&#35782;&#36827;&#34892;&#20998;&#37197;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#22312;&#20219;&#20309;&#30456;&#20851;&#20219;&#21153;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#23454;&#39564;&#65292;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#26426;&#22120;&#20154;&#20174;&#22810;&#26679;&#21270;&#30340;&#29615;&#22659;&#20219;&#21153;&#20013;&#35757;&#32451;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21407;&#22411;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#22312;&#35270;&#35273;&#23548;&#33322;&#20219;&#21153;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#33021;&#22815;&#39564;&#35777;&#36825;&#20123;&#20551;&#35774;&#30340;&#23454;&#39564;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning has gained immense popularity because of its high sample-efficiency. However, in real-world scenarios, where the trajectory distribution of most of the tasks dynamically shifts, model fitting on continuously aggregated data alone would be futile. In some cases, the distribution shifts, so much, that it is difficult for an agent to infer the new task. We propose a novel algorithm to generalize on any related task by leveraging prior knowledge on a set of specific tasks, which involves assigning importance weights to each past demonstration. We show experiments where the robot is trained from a diversity of environmental tasks and is also able to adapt to an unseen environment, using few-shot learning. We also developed a prototype robot system to test our approach on the task of visual navigation, and experimental results obtained were able to confirm these suppositions.
&lt;/p&gt;</description></item></channel></rss>