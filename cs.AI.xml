<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>MMICL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.07915</link><description>&lt;p&gt;
MMICL&#65306;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07915
&lt;/p&gt;
&lt;p&gt;
MMICL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28145;&#24230;&#23398;&#20064;&#30340;&#22797;&#33487;&#24320;&#22987;&#65292;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLM&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20219;&#21153;&#20449;&#24687;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22823;&#22810;&#25968;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;&#21253;&#21547;&#22810;&#20010;&#22270;&#20687;&#65289;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36861;&#28335;&#21040;VLM&#30340;&#26550;&#26500;&#35774;&#35745;&#25110;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#21069;&#30340;VLM&#20027;&#35201;&#24378;&#35843;&#21033;&#29992;&#24102;&#26377;&#21333;&#20010;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#24102;&#26377;&#20132;&#38169;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#23613;&#31649;&#19968;&#20123;&#26032;&#25552;&#20986;&#30340;VLM&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#22810;&#20010;&#22270;&#20687;&#30340;&#29992;&#25143;&#25552;&#31034;&#65292;&#20294;&#39044;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#25552;&#20379;&#27604;&#20174;Web&#25235;&#21462;&#26102;&#20132;&#38169;&#22270;&#20687;&#21644;&#25991;&#26412;&#26356;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MMICL&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Starting from the resurgence of deep learning, vision-language models (VLMs) benefiting from large language models (LLMs) have never been so popular. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images. The issue can traced back to the architectural design of VLMs or pre-training data. Specifically, the current VLMs primarily emphasize utilizing multi-modal data with a single image some, rather than multi-modal prompts with interleaved multiple images and text. Even though some newly proposed VLMs could handle user prompts with multiple images, pre-training data does not provide more sophisticated multi-modal prompts than interleaved image and text crawled from the web. We propose MMICL to address the issue by considering both the model and data perspectives. We introduce a well-designed architecture capable of seamlessly integrating vis
&lt;/p&gt;</description></item><item><title>beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.07867</link><description>&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07867
&lt;/p&gt;
&lt;p&gt;
beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;beta&#25193;&#25955;&#65292;&#19968;&#31181;&#23558;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#38598;&#25104;&#21040;&#19968;&#36215;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992;&#20102;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#65292;beta&#25193;&#25955;&#21033;&#29992;&#20102;&#38543;&#26102;&#38388;&#30340;&#20056;&#27861;&#36716;&#25442;&#26469;&#21019;&#24314;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#32500;&#25345;&#30528;&#27491;&#21521;&#36793;&#32536;&#20998;&#24067;&#21644;&#21453;&#21521;&#26465;&#20214;&#20998;&#24067;&#65292;&#32473;&#23450;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;&#20256;&#32479;&#27169;&#22411;&#20381;&#36182;&#20110;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#65292;beta&#25193;&#25955;&#26159;&#20056;&#27861;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20174;KL&#25955;&#24230;&#30340;&#20984;&#24615;&#25512;&#23548;&#20986;&#26469;&#30340;KL&#25955;&#24230;&#19978;&#30028;&#65288;KLUB&#65289;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;KLUB&#30456;&#23545;&#20110;&#36127;ELBO&#26469;&#35828;&#23545;&#20110;&#20248;&#21270;beta&#25193;&#25955;&#26356;&#21152;&#26377;&#25928;&#65292;&#36127;ELBO&#20063;&#21487;&#20197;&#20316;&#20026;&#30456;&#21516;KL&#25955;&#24230;&#30340;KLUB&#65292;&#21482;&#26159;&#20854;&#20004;&#20010;&#21442;&#25968;&#20132;&#25442;&#20102;&#20301;&#32622;&#12290;beta&#25193;&#25955;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;Bregman&#25955;&#24230;&#20026;&#25351;&#26631;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.07864</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07864
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#31867;&#19968;&#30452;&#36861;&#27714;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36798;&#21040;&#25110;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#30446;&#26631;&#65292;&#32780;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#26395;&#26041;&#24335;&#30340;AI&#20195;&#29702;&#12290;AI&#20195;&#29702;&#26159;&#33021;&#24863;&#30693;&#29615;&#22659;&#12289;&#20570;&#20986;&#20915;&#31574;&#21644;&#37319;&#21462;&#34892;&#21160;&#30340;&#20154;&#24037;&#23454;&#20307;&#12290;&#33258;20&#19990;&#32426;&#20013;&#21494;&#20197;&#26469;&#65292;&#20154;&#20204;&#20026;&#24320;&#21457;&#26234;&#33021;AI&#20195;&#29702;&#36827;&#34892;&#20102;&#35768;&#22810;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#31639;&#27861;&#25110;&#35757;&#32451;&#31574;&#30053;&#30340;&#36827;&#27493;&#19978;&#65292;&#20197;&#22686;&#24378;&#29305;&#23450;&#33021;&#21147;&#25110;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#31038;&#21306;&#25152;&#32570;&#20047;&#30340;&#26159;&#19968;&#20010;&#36275;&#22815;&#36890;&#29992;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#35774;&#35745;&#33021;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#30340;AI&#20195;&#29702;&#30340;&#36215;&#28857;&#12290;&#30001;&#20110;&#23637;&#31034;&#20986;&#30340;&#22810;&#21151;&#33021;&#21644;&#26174;&#33879;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#20026;&#26500;&#24314;&#36890;&#29992;AI&#20195;&#29702;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#21033;&#29992;LLMs&#20316;&#20026;&#26500;&#24314;AI&#20195;&#29702;&#30340;&#22522;&#30784;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CiwaGAN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#22768;&#38901;&#23398;&#24314;&#27169;&#21644;&#26080;&#30417;&#30563;&#21548;&#35273;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#65292;&#26159;&#23545;&#20154;&#31867;&#21475;&#35821;&#20064;&#24471;&#26368;&#29616;&#23454;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2309.07861</link><description>&lt;p&gt;
CiwaGAN: &#22768;&#38901;&#23398;&#20449;&#24687;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
CiwaGAN: Articulatory information exchange. (arXiv:2309.07861v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CiwaGAN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#22768;&#38901;&#23398;&#24314;&#27169;&#21644;&#26080;&#30417;&#30563;&#21548;&#35273;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#65292;&#26159;&#23545;&#20154;&#31867;&#21475;&#35821;&#20064;&#24471;&#26368;&#29616;&#23454;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#25511;&#21046;&#21457;&#38899;&#22120;&#23448;&#23558;&#20449;&#24687;&#32534;&#30721;&#25104;&#22768;&#38899;&#65292;&#24182;&#36890;&#36807;&#21548;&#35273;&#35013;&#32622;&#35299;&#30721;&#22768;&#38899;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CiwaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#22768;&#38901;&#23398;&#24314;&#27169;&#21644;&#26080;&#30417;&#30563;&#21548;&#35273;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#30340;&#20154;&#31867;&#21475;&#35821;&#20064;&#24471;&#27169;&#22411;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#20998;&#21035;&#21253;&#25324;&#20102;&#26080;&#30417;&#30563;&#22768;&#38901;&#23398;&#24314;&#27169;&#21644;&#20449;&#24687;&#20132;&#27969;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#23558;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32467;&#21512;&#22312;&#19968;&#36215;&#30340;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22768;&#38901;&#23398;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#21487;&#35299;&#37322;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#25552;&#20986;&#30340;CiwaGAN&#27169;&#22411;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#20154;&#31867;&#21475;&#35821;&#20064;&#24471;&#36827;&#34892;&#26368;&#29616;&#23454;&#30340;&#36817;&#20284;&#12290;&#22240;&#27492;&#65292;&#23427;&#23545;&#20110;&#35748;&#30693;&#19978;&#21487;&#34892;&#30340;&#20154;&#31867;&#35328;&#35821;&#34892;&#20026;&#27169;&#25311;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans encode information into sounds by controlling articulators and decode information from sounds using the auditory apparatus. This paper introduces CiwaGAN, a model of human spoken language acquisition that combines unsupervised articulatory modeling with an unsupervised model of information exchange through the auditory modality. While prior research includes unsupervised articulatory modeling and information exchange separately, our model is the first to combine the two components. The paper also proposes an improved articulatory model with more interpretable internal representations. The proposed CiwaGAN model is the most realistic approximation of human spoken language acquisition using deep learning. As such, it is useful for cognitively plausible simulations of the human speech act.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ExpertQA&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#24773;&#26223;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#31561;&#26041;&#38754;&#26469;&#30830;&#20445;&#25552;&#20379;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#30740;&#31350;&#36824;&#25910;&#38598;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#38382;&#39064;&#24182;&#35201;&#27714;&#20182;&#20204;&#35780;&#20272;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#19981;&#20250;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.07852</link><description>&lt;p&gt;
ExpertQA: &#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
ExpertQA: Expert-Curated Questions and Attributed Answers. (arXiv:2309.07852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ExpertQA&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#24773;&#26223;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#31561;&#26041;&#38754;&#26469;&#30830;&#20445;&#25552;&#20379;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#30740;&#31350;&#36824;&#25910;&#38598;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#38382;&#39064;&#24182;&#35201;&#27714;&#20182;&#20204;&#35780;&#20272;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#19981;&#20250;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#34987;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#25152;&#37319;&#29992;&#65292;&#30830;&#20445;&#23427;&#20204;&#25552;&#20379;&#22522;&#20110;&#21487;&#39564;&#35777;&#26469;&#28304;&#30340;&#20107;&#23454;&#20934;&#30830;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#32844;&#19994;&#20013;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#29305;&#21035;&#36866;&#29992;&#20110;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#22240;&#20026;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#39118;&#38505;&#36739;&#39640;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20107;&#23454;&#24615;&#21644;&#24402;&#22240;&#26041;&#38754;&#65292;&#24182;&#26410;&#19987;&#27880;&#20110;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#24773;&#26223;&#20013;&#30340;&#36825;&#20123;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39046;&#22495;&#19987;&#23478;&#32435;&#20837;&#20854;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#30740;&#31350;&#65292;&#20998;&#26512;&#26469;&#33258;&#20960;&#20010;&#31995;&#32479;&#30340;&#21709;&#24212;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20808;&#20174;32&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;484&#21517;&#21442;&#19982;&#32773;&#20013;&#25910;&#38598;&#30001;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#65292;&#28982;&#21518;&#35201;&#27714;&#36825;&#20123;&#19987;&#23478;&#35780;&#20272;&#23545;&#20182;&#20204;&#33258;&#24049;&#38382;&#39064;&#30340;&#20135;&#29983;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#36824;&#35201;&#27714;&#19987;&#23478;&#20462;&#25913;&#20135;&#29983;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models are adapted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study &amp; professions. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying factuality and attribution has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we present an evaluation study analyzing various axes of factuality and attribution provided in responses from a few systems, by bringing domain experts in the loop. Specifically, we first collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. We also ask experts to revise answers produce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#26657;&#20934;&#38543;&#26426;&#27874;&#21160;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23545;&#22522;&#20110;Heston&#27169;&#22411;&#30340;&#26631;&#30340;&#36164;&#20135;&#36827;&#34892;&#23450;&#20215;&#65292;&#24182;&#19988;&#22312;&#26657;&#20934;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.07843</link><description>&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#26657;&#20934;&#38543;&#26426;&#27874;&#21160;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Applying Deep Learning to Calibrate Stochastic Volatility Models. (arXiv:2309.07843v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#26657;&#20934;&#38543;&#26426;&#27874;&#21160;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23545;&#22522;&#20110;Heston&#27169;&#22411;&#30340;&#26631;&#30340;&#36164;&#20135;&#36827;&#34892;&#23450;&#20215;&#65292;&#24182;&#19988;&#22312;&#26657;&#20934;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#27874;&#21160;&#24615;&#27169;&#22411;&#26159;&#19968;&#31181;&#27874;&#21160;&#29575;&#26159;&#38543;&#26426;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#38544;&#21547;&#27874;&#21160;&#29575;&#26354;&#38754;&#30340;&#22823;&#37096;&#20998;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26356;&#30495;&#23454;&#30340;&#27874;&#21160;&#29575;&#31505;&#26354;&#32447;&#25110;&#20559;&#26012;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#26657;&#20934;&#26102;&#38388;&#36807;&#38271;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#30340;&#26367;&#20195;&#26657;&#20934;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#26500;&#24314;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#26657;&#20934;&#35299;&#20915;&#26041;&#26696;&#12290;Huge&#21644;Savine&#24320;&#21457;&#20102;&#19968;&#31181;&#24046;&#20998;&#28145;&#24230;&#23398;&#20064;&#65288;DDL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#20013;&#35757;&#32451;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#26679;&#26412;&#19981;&#20165;&#21253;&#25324;&#29305;&#24449;&#21644;&#26631;&#31614;&#65292;&#36824;&#21253;&#25324;&#26631;&#31614;&#23545;&#29305;&#24449;&#30340;&#24494;&#20998;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;DDL&#25216;&#26415;&#24212;&#29992;&#20110;&#23450;&#20215;&#22522;&#26412;&#27431;&#27954;&#26399;&#26435;&#65288;&#21363;&#26657;&#20934;&#24037;&#20855;&#65289;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#22312;&#22522;&#20110;Heston&#27169;&#22411;&#30340;&#26631;&#30340;&#36164;&#20135;&#19978;&#23450;&#20215;&#30475;&#28072;&#26399;&#26435;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#23545;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#12290;DDL&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#35757;&#32451;&#21644;&#20934;&#30830;&#23450;&#20215;&#12290;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#25103;&#21095;&#24615;&#22320;
&lt;/p&gt;
&lt;p&gt;
Stochastic volatility models, where the volatility is a stochastic process, can capture most of the essential stylized facts of implied volatility surfaces and give more realistic dynamics of the volatility smile or skew. However, they come with the significant issue that they take too long to calibrate.  Alternative calibration methods based on Deep Learning (DL) techniques have been recently used to build fast and accurate solutions to the calibration problem. Huge and Savine developed a Differential Deep Learning (DDL) approach, where Machine Learning models are trained on samples of not only features and labels but also differentials of labels to features. The present work aims to apply the DDL technique to price vanilla European options (i.e. the calibration instruments), more specifically, puts when the underlying asset follows a Heston model and then calibrate the model on the trained network. DDL allows for fast training and accurate pricing. The trained neural network dramatic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#26694;&#26550;&#26469;&#22788;&#29702;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#28431;&#27934;&#12290;&#36890;&#36807;&#23558;Slither&#30340;&#28431;&#27934;&#25253;&#21578;&#19982;&#28304;&#20195;&#30721;&#32467;&#21512;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26234;&#33021;&#21512;&#32422;&#30340;&#20998;&#31867;&#21644;&#20462;&#22797;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#28431;&#27934;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.07841</link><description>&lt;p&gt;
&#20004;&#27493;&#36208;&#65306;&#37319;&#29992;&#21452;&#23618;&#26041;&#27861;&#20462;&#22797;&#26234;&#33021;&#21512;&#32422;
&lt;/p&gt;
&lt;p&gt;
Two Timin': Repairing Smart Contracts With A Two-Layered Approach. (arXiv:2309.07841v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#26694;&#26550;&#26469;&#22788;&#29702;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#28431;&#27934;&#12290;&#36890;&#36807;&#23558;Slither&#30340;&#28431;&#27934;&#25253;&#21578;&#19982;&#28304;&#20195;&#30721;&#32467;&#21512;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26234;&#33021;&#21512;&#32422;&#30340;&#20998;&#31867;&#21644;&#20462;&#22797;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#28431;&#27934;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#29616;&#20195;&#37325;&#35201;&#24615;&#65292;&#26234;&#33021;&#21512;&#32422;&#26082;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#39118;&#38505;&#65292;&#20063;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#22909;&#22788;&#12290;&#20854;&#20013;&#30340;&#28431;&#27934;&#21487;&#20197;&#24341;&#21457;&#19968;&#31995;&#21015;&#21518;&#26524;&#65292;&#23548;&#33268;&#37325;&#22823;&#25439;&#22833;&#12290;&#35768;&#22810;&#24403;&#21069;&#30340;&#35770;&#25991;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#24694;&#24847;&#24847;&#22270;&#36827;&#34892;&#26234;&#33021;&#21512;&#32422;&#30340;&#20998;&#31867;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#26377;&#38480;&#30340;&#21512;&#32422;&#29305;&#24449;&#65292;&#22914;&#23383;&#33410;&#30721;&#25110;&#25805;&#20316;&#30721;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21452;&#23618;&#26694;&#26550;&#65306;1&#65289;&#36827;&#34892;&#20998;&#31867;&#21644;2&#65289;&#30452;&#25509;&#20462;&#22797;&#24694;&#24847;&#21512;&#32422;&#12290;&#23558;Slither&#30340;&#28431;&#27934;&#25253;&#21578;&#19982;&#28304;&#20195;&#30721;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65288;RFC&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20256;&#36882;&#65292;&#23545;&#27599;&#20010;&#24314;&#35758;&#30340;&#28431;&#27934;&#36827;&#34892;&#20998;&#31867;&#21644;&#20462;&#22797;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#32463;&#36807;&#32454;&#35843;&#21644;&#21450;&#26102;&#20462;&#22797;&#24037;&#31243;&#30340;LLMs&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;GPT-3.5-Turbo&#21644;&#32463;&#36807;&#32454;&#35843;&#30340;Llama-2-7B&#27169;&#22411;&#26500;&#24314;&#30340;&#26234;&#33021;&#21512;&#32422;&#20462;&#22797;&#27169;&#22411;&#20998;&#21035;&#23558;&#25972;&#20307;&#28431;&#27934;&#35745;&#25968;&#20943;&#23569;&#20102;97.5%&#21644;96.7%&#12290;&#23545;&#20462;&#22797;&#30340;&#25163;&#21160;&#26816;&#26597;&#34920;&#26126;&#65292;&#20462;&#22797;&#27169;&#22411;&#30340;&#20462;&#22797;&#27491;&#30830;&#29575;&#39640;&#36798;90.3%&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the modern relevance of blockchain technology, smart contracts present both substantial risks and benefits. Vulnerabilities within them can trigger a cascade of consequences, resulting in significant losses. Many current papers primarily focus on classifying smart contracts for malicious intent, often relying on limited contract characteristics, such as bytecode or opcode. This paper proposes a novel, two-layered framework: 1) classifying and 2) directly repairing malicious contracts. Slither's vulnerability report is combined with source code and passed through a pre-trained RandomForestClassifier (RFC) and Large Language Models (LLMs), classifying and repairing each suggested vulnerability. Experiments demonstrate the effectiveness of fine-tuned and prompt-engineered LLMs. The smart contract repair models, built from pre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overall vulnerability count by 97.5% and 96.7% respectively. A manual inspection of repair
&lt;/p&gt;</description></item><item><title>VAPOR&#26159;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23460;&#22806;&#22797;&#26434;&#26893;&#34987;&#29615;&#22659;&#20013;&#36827;&#34892;&#20840;&#21521;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#36890;&#36807;&#20174;&#26410;&#26631;&#35760;&#30340;&#30495;&#23454;&#26893;&#34987;&#25968;&#25454;&#20013;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#26893;&#34987;&#30340;&#29289;&#29702;&#21644;&#20960;&#20309;&#29305;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#33258;&#36866;&#24212;&#35268;&#21010;&#22120;&#29983;&#25104;&#21160;&#24577;&#21487;&#34892;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#22312;&#29421;&#31364;&#36890;&#36947;&#20013;&#23548;&#33322;&#65292;&#24182;&#36991;&#20813;&#34987;&#39640;&#33609;&#21644;&#28748;&#26408;&#19995;&#25152;&#22256;&#20303;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25104;&#21151;&#29575;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.07832</link><description>&lt;p&gt;
&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20840;&#21521;&#33151;&#24335;&#26426;&#22120;&#20154;&#22312;&#23460;&#22806;&#26893;&#34987;&#20013;&#36827;&#34892;&#23548;&#33322;&#30340;VAPOR&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VAPOR: Holonomic Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning. (arXiv:2309.07832v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07832
&lt;/p&gt;
&lt;p&gt;
VAPOR&#26159;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23460;&#22806;&#22797;&#26434;&#26893;&#34987;&#29615;&#22659;&#20013;&#36827;&#34892;&#20840;&#21521;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#36890;&#36807;&#20174;&#26410;&#26631;&#35760;&#30340;&#30495;&#23454;&#26893;&#34987;&#25968;&#25454;&#20013;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#26893;&#34987;&#30340;&#29289;&#29702;&#21644;&#20960;&#20309;&#29305;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#33258;&#36866;&#24212;&#35268;&#21010;&#22120;&#29983;&#25104;&#21160;&#24577;&#21487;&#34892;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#22312;&#29421;&#31364;&#36890;&#36947;&#20013;&#23548;&#33322;&#65292;&#24182;&#36991;&#20813;&#34987;&#39640;&#33609;&#21644;&#28748;&#26408;&#19995;&#25152;&#22256;&#20303;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25104;&#21151;&#29575;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VAPOR&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#32467;&#26500;&#28151;&#20081;&#12289;&#23494;&#38598;&#26893;&#34987;&#30340;&#23460;&#22806;&#29615;&#22659;&#20013;&#23454;&#29616;&#20840;&#21521;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20174;&#19977;&#32500;LiDAR&#28857;&#20113;&#23548;&#20986;&#30340;&#39640;&#24230;&#21644;&#24378;&#24230;&#22522;&#20110;&#25104;&#26412;&#22320;&#22270;&#12289;&#30446;&#26631;&#25104;&#26412;&#22320;&#22270;&#20197;&#21450;&#32463;&#36807;&#22788;&#29702;&#30340;&#22266;&#26377;&#24863;&#30693;&#25968;&#25454;&#20316;&#20026;&#29366;&#24577;&#36755;&#20837;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#23454;&#38469;&#23460;&#22806;&#26893;&#34987;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#19968;&#31181;&#26032;&#30340;RL&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#23398;&#20064;&#20102;&#21608;&#22260;&#26893;&#34987;&#30340;&#29289;&#29702;&#21644;&#20960;&#20309;&#29305;&#24615;&#65292;&#22914;&#39640;&#24230;&#12289;&#23494;&#24230;&#21644;&#22362;&#23454;&#24230;/&#21018;&#24615;&#65292;&#20197;&#36827;&#34892;&#23548;&#33322;&#12290;&#19982;&#20351;&#29992;&#31471;&#21040;&#31471;&#31574;&#30053;&#21160;&#20316;&#19981;&#21516;&#65292;&#23436;&#20840;&#35757;&#32451;&#22909;&#30340;RL&#31574;&#30053;&#30340;Q&#32593;&#32476;&#29992;&#20110;&#35780;&#20272;&#30001;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#35268;&#21010;&#22120;&#29983;&#25104;&#30340;&#21160;&#24577;&#21487;&#34892;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#35813;&#35268;&#21010;&#22120;&#33021;&#22815;&#31359;&#36807;&#29421;&#31364;&#30340;&#36890;&#36947;&#24182;&#38450;&#27490;&#34987;&#26893;&#34987;&#65292;&#20363;&#22914;&#39640;&#33609;&#21644;&#28748;&#26408;&#19995;&#25152;&#22256;&#20303;&#12290;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;&#23460;&#22806;&#26893;&#34987;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25104;&#21151;&#29575;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present VAPOR, a novel method for autonomous legged robot navigation in unstructured, densely vegetated outdoor environments using Offline Reinforcement Learning (RL). Our method trains a novel RL policy from unlabeled data collected in real outdoor vegetation. This policy uses height and intensity-based cost maps derived from 3D LiDAR point clouds, a goal cost map, and processed proprioception data as state inputs, and learns the physical and geometric properties of the surrounding vegetation such as height, density, and solidity/stiffness for navigation. Instead of using end-to-end policy actions, the fully-trained RL policy's Q network is used to evaluate dynamically feasible robot actions generated from a novel adaptive planner capable of navigating through dense narrow passages and preventing entrapment in vegetation such as tall grass and bushes. We demonstrate our method's capabilities on a legged robot in complex outdoor vegetation. We observe an improvement in success rates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;OpenStreetMap&#36947;&#36335;&#25968;&#25454;&#20316;&#20026;&#24369;&#26631;&#31614;&#21644;&#22823;&#35268;&#27169;&#21355;&#26143;&#22270;&#20687;&#39044;&#35757;&#32451;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#23545;&#20110;&#36947;&#36335;&#25552;&#21462;&#32780;&#35328;&#65292;&#39044;&#27979;&#20934;&#30830;&#24230;&#38543;&#30528;&#24369;&#26631;&#27880;&#25968;&#25454;&#37327;&#21644;&#36947;&#36335;&#23494;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2309.07823</link><description>&lt;p&gt;
&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#36827;&#34892;&#36947;&#36335;&#25552;&#21462;&#30340;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large-scale Weakly Supervised Learning for Road Extraction from Satellite Imagery. (arXiv:2309.07823v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;OpenStreetMap&#36947;&#36335;&#25968;&#25454;&#20316;&#20026;&#24369;&#26631;&#31614;&#21644;&#22823;&#35268;&#27169;&#21355;&#26143;&#22270;&#20687;&#39044;&#35757;&#32451;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#23545;&#20110;&#36947;&#36335;&#25552;&#21462;&#32780;&#35328;&#65292;&#39044;&#27979;&#20934;&#30830;&#24230;&#38543;&#30528;&#24369;&#26631;&#27880;&#25968;&#25454;&#37327;&#21644;&#36947;&#36335;&#23494;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#33258;&#21160;&#25552;&#21462;&#36947;&#36335;&#26159;&#20256;&#32479;&#25163;&#24037;&#32472;&#21046;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#23427;&#26368;&#36817;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#26377;&#30417;&#30563;&#30340;&#65292;&#38656;&#35201;&#20687;&#32032;&#32423;&#21035;&#30340;&#26631;&#27880;&#65292;&#36825;&#19968;&#36807;&#31243;&#32321;&#29712;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#22320;&#29699;&#19978;&#30340;&#22320;&#24418;&#12289;&#26893;&#34987;&#21644;&#20154;&#36896;&#29289;&#20307;&#26377;&#30528;&#21508;&#31181;&#21508;&#26679;&#30340;&#24046;&#24322;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20174;&#19968;&#20010;&#22320;&#21306;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#24448;&#24448;&#24456;&#38590;&#27867;&#21270;&#21040;&#20854;&#20182;&#22320;&#21306;&#12290;&#19981;&#21516;&#30340;&#25293;&#25668;&#26465;&#20214;&#65292;&#22914;&#20809;&#29031;&#21644;&#35282;&#24230;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#24320;&#21457;&#35206;&#30422;&#25152;&#26377;&#22270;&#20687;&#39118;&#26684;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;OpenStreetMap&#36947;&#36335;&#25968;&#25454;&#20316;&#20026;&#24369;&#26631;&#31614;&#21644;&#22823;&#35268;&#27169;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#20934;&#30830;&#24230;&#38543;&#30528;&#24369;&#26631;&#27880;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#20197;&#21450;&#22320;&#21306;&#36947;&#36335;&#23494;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic road extraction from satellite imagery using deep learning is a viable alternative to traditional manual mapping. Therefore it has received considerable attention recently. However, most of the existing methods are supervised and require pixel-level labeling, which is tedious and error-prone. To make matters worse, the earth has a diverse range of terrain, vegetation, and man-made objects. It is well known that models trained in one area generalize poorly to other areas. Various shooting conditions such as light and angel, as well as different image processing techniques further complicate the issue. It is impractical to develop training data to cover all image styles. This paper proposes to leverage OpenStreetMap road data as weak labels and large scale satellite imagery to pre-train semantic segmentation models. Our extensive experimental results show that the prediction accuracy increases with the amount of the weakly labeled data, as well as the road density in the areas 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07808</link><description>&lt;p&gt;
&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20840;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#65292;&#22312;&#36825;&#31181;&#25216;&#26415;&#20013;&#65292;&#25972;&#20010;&#39550;&#39542;&#27969;&#31243;&#34987;&#26367;&#25442;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#20854;&#32467;&#26500;&#31616;&#21333;&#21644;&#25512;&#29702;&#26102;&#38388;&#24555;&#65292;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#39550;&#39542;&#27969;&#31243;&#20013;&#30340;&#32452;&#20214;&#65292;&#20294;&#20854;&#31616;&#21333;&#24615;&#20063;&#23548;&#33268;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#35757;&#32451;&#24471;&#21040;&#30340;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#20132;&#36890;&#35268;&#21017;&#65292;&#21516;&#26102;&#20063;&#24456;&#38590;&#21457;&#29616;&#20854;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#22240;&#20026;&#32570;&#20047;&#20013;&#38388;&#36755;&#20986;&#12290;&#21516;&#26102;&#65292;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#34892;&#24615;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#24863;&#30693;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
More research attention has recently been given to end-to-end autonomous driving technologies where the entire driving pipeline is replaced with a single neural network because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the components in driving pipeline, its simplicity also leads to interpretability problems and safety issues arXiv:2003.06404. The trained policy is not always compliant with the traffic rules and it is also hard to discover the reason for the misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are also critical to autonomous driving's security and feasibility to perceive the surrounding environment under complex driving scenarios. In this paper, we proposed P-CSG, a novel penalty-based imitation learning approach with cross semantics generation sensor fusion technologies to increase the overall performance of End-to-End Autonomous Driving. We conducted an assessment of our model's perform
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#22686;&#24378;&#30340;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#19978;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#31639;&#27861;&#21033;&#29992;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#35299;&#20915;&#20102;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#32447;&#24615;&#26041;&#31243;&#32452;&#65292;&#24182;&#22312;Iris&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#32463;&#20856;&#35745;&#31639;&#21644;&#37327;&#23376;&#35745;&#31639;&#30340;&#31574;&#30053;&#24615;&#32452;&#21512;&#65292;&#31639;&#27861;&#22312;&#19981;&#21516;&#32500;&#24230;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#20102;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07770</link><description>&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#22686;&#24378;&#30340;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Variational Quantum Linear Solver enhanced Quantum Support Vector Machine. (arXiv:2309.07770v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07770
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#22686;&#24378;&#30340;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#19978;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#31639;&#27861;&#21033;&#29992;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#35299;&#20915;&#20102;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#32447;&#24615;&#26041;&#31243;&#32452;&#65292;&#24182;&#22312;Iris&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#32463;&#20856;&#35745;&#31639;&#21644;&#37327;&#23376;&#35745;&#31639;&#30340;&#31574;&#30053;&#24615;&#32452;&#21512;&#65292;&#31639;&#27861;&#22312;&#19981;&#21516;&#32500;&#24230;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#20102;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#20351;&#29992;&#37327;&#23376;&#36164;&#28304;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#30340;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#65288;VQLS&#65289;&#22686;&#24378;&#30340;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#36825;&#26159;&#22522;&#20110;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#22312;NISQ&#35774;&#22791;&#19978;&#35299;&#20915;&#26368;&#23567;&#20108;&#20056;SVM&#30340;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#24605;&#24819;&#26500;&#24314;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;Iris&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#25968;&#20540;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29616;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#20010;&#19981;&#21516;&#30340;&#40482;&#23614;&#26893;&#29289;&#29289;&#31181;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#22312;&#20174;&#19968;&#32500;&#21040;&#19971;&#32500;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#31867;&#30340;&#20998;&#31867;&#22120;&#65292;&#25506;&#32034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#21033;&#29992;&#32463;&#20856;&#35745;&#31639;&#21644;&#37327;&#23376;&#35745;&#31639;&#23545;&#21508;&#31181;&#23376;&#31243;&#24207;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;NISQ&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;QSVM&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Support Vector Machines (QSVM) play a vital role in using quantum resources for supervised machine learning tasks, such as classification. However, current methods are strongly limited in terms of scalability on Noisy Intermediate Scale Quantum (NISQ) devices. In this work, we propose a novel approach called the Variational Quantum Linear Solver (VQLS) enhanced QSVM. This is built upon our idea of utilizing the variational quantum linear solver to solve system of linear equations of a least squares-SVM on a NISQ device. The implementation of our approach is evaluated by an extensive series of numerical experiments with the Iris dataset, which consists of three distinct iris plant species. Based on this, we explore the practicality and effectiveness of our algorithm by constructing a classifier capable of classification in a feature space ranging from one to seven dimensions. Furthermore, by strategically exploiting both classical and quantum computing for various subroutines of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07760</link><description>&lt;p&gt;
PRE: &#35270;&#35273;-&#35821;&#35328;&#25552;&#31034;&#23398;&#20064;&#19982;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07760
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#25552;&#31034;&#20197;&#25913;&#36827;&#19979;&#28216;&#22270;&#20687;&#20998;&#24067;&#21644;&#25991;&#26412;&#31867;&#25551;&#36848;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#31181;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#36341;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#24182;&#19988;&#38750;&#24120;&#32791;&#26102;&#12290;&#20026;&#20102;&#36991;&#20813;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#26368;&#36817;&#30340;CoOp&#24037;&#20316;&#24341;&#20837;&#20102;&#22312;&#35270;&#35273;&#39046;&#22495;&#20351;&#29992;&#21487;&#25511;&#25991;&#26412;&#26631;&#35760;&#30340;&#25552;&#31034;&#23398;&#20064;&#27010;&#24565;&#12290;&#34429;&#28982;CoOp&#21487;&#20197;&#22312;&#25163;&#21160;&#25552;&#31034;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#20854;&#23398;&#21040;&#30340;&#19978;&#19979;&#25991;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#26356;&#24191;&#27867;&#30340;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Learning with Reparameterization Encoder (PRE) &#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#38598;&#25104;&#30340;LLM&#26041;&#27861;&#36827;&#34892;&#29983;&#25104;&#24335;AI&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#39044;&#35757;&#32451;LLM&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#29305;&#24449;&#26469;&#20934;&#30830;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#65292;&#24182;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#23646;&#12290;</title><link>http://arxiv.org/abs/2309.07755</link><description>&lt;p&gt;
&#21033;&#29992;&#38598;&#25104;&#30340;LLM&#26041;&#27861;&#36827;&#34892;&#29983;&#25104;&#24335;AI&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Generative AI Text Classification using Ensemble LLM Approaches. (arXiv:2309.07755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#38598;&#25104;&#30340;LLM&#26041;&#27861;&#36827;&#34892;&#29983;&#25104;&#24335;AI&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#39044;&#35757;&#32451;LLM&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#29305;&#24449;&#26469;&#20934;&#30830;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#65292;&#24182;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#23646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22914;&#20869;&#23481;&#21019;&#20316;&#12289;&#25253;&#21578;&#29983;&#25104;&#31561;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#24694;&#24847;&#24212;&#29992;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#21518;&#26524;&#65292;&#22914;&#34394;&#20551;&#26032;&#38395;&#29983;&#25104;&#12289;&#25220;&#34989;&#31561;&#12290;&#22240;&#27492;&#65292;&#22312;&#36127;&#36131;&#20219;&#22320;&#20351;&#29992;LLM&#26102;&#20934;&#30830;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#26576;&#20010;&#25991;&#26412;&#26159;&#30001;AI&#29983;&#25104;&#36824;&#26159;&#20154;&#31867;&#32534;&#20889;&#30340;&#65307;2&#65289;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#24402;&#23646;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#33521;&#25991;&#21644;&#35199;&#29677;&#29273;&#25991;&#30340;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#26159;Automated Text Identification (AuTexTification)&#20849;&#20139;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#12290;&#38024;&#23545;&#19978;&#36848;&#27599;&#20010;&#30740;&#31350;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#25104;&#31070;&#32463;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;LLM&#29983;&#25104;&#27010;&#29575;&#65292;&#36825;&#20123;&#27010;&#29575;&#34987;&#29992;&#20316;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive performance across a variety of Artificial Intelligence (AI) and natural language processing tasks, such as content creation, report generation, etc. However, unregulated malign application of these models can create undesirable consequences such as generation of fake news, plagiarism, etc. As a result, accurate detection of AI-generated language can be crucial in responsible usage of LLMs. In this work, we explore 1) whether a certain body of text is AI generated or written by human, and 2) attribution of a specific language model in generating a body of text. Texts in both English and Spanish are considered. The datasets used in this study are provided as part of the Automated Text Identification (AuTexTification) shared task. For each of the research objectives stated above, we propose an ensemble neural model that generates probabilities from different pre-trained LLMs which are used as features to a Traditional Machine Learning (T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#20998;&#24067;&#24335;&#27700;&#22768;&#20256;&#24863;&#22120;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#19982;&#38450;&#24481;&#31995;&#32479;&#65288;AIDPS&#65289;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#39640;&#27700;&#22768;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#27700;&#19979;&#30456;&#20851;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07730</link><description>&lt;p&gt;
AIDPS:&#33258;&#36866;&#24212;&#27700;&#22768;&#20256;&#24863;&#22120;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#19982;&#38450;&#24481;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater Acoustic Sensor Networks. (arXiv:2309.07730v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#20998;&#24067;&#24335;&#27700;&#22768;&#20256;&#24863;&#22120;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#19982;&#38450;&#24481;&#31995;&#32479;&#65288;AIDPS&#65289;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#39640;&#27700;&#22768;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#27700;&#19979;&#30456;&#20851;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#22768;&#20256;&#24863;&#22120;&#32593;&#32476;&#20027;&#35201;&#29992;&#20110;&#27700;&#19979;&#29615;&#22659;&#65292;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#27700;&#19979;&#29615;&#22659;&#30340;&#19981;&#31283;&#23450;&#21644;&#25361;&#25112;&#24615;&#12289;&#20256;&#24863;&#22120;&#33410;&#28857;&#36164;&#28304;&#26377;&#38480;&#19988;&#32570;&#20047;&#23433;&#20840;&#32771;&#34385;&#20351;&#24471;&#27700;&#22768;&#20256;&#24863;&#22120;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#24067;&#24335;&#30340;&#27700;&#22768;&#20256;&#24863;&#22120;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#19982;&#38450;&#24481;&#31995;&#32479;&#65288;AIDPS&#65289;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#27700;&#22768;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#27700;&#19979;&#25915;&#20987;&#12290;&#20026;&#30830;&#23450;&#25152;&#25552;&#20986;&#24314;&#31569;&#30340;&#26368;&#26377;&#25928;&#37197;&#32622;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20960;&#31181;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#33258;&#36866;&#24212;&#38543;&#26426;&#26862;&#26519;&#65288;ARF&#65289;&#12289;&#36731;&#37327;&#32423;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;K&#36817;&#37051;&#31639;&#27861;&#65289;&#21644;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Underwater Acoustic Sensor Networks (UW-ASNs) are predominantly used for underwater environments and find applications in many areas. However, a lack of security considerations, the unstable and challenging nature of the underwater environment, and the resource-constrained nature of the sensor nodes used for UW-ASNs (which makes them incapable of adopting security primitives) make the UW-ASN prone to vulnerabilities. This paper proposes an Adaptive decentralised Intrusion Detection and Prevention System called AIDPS for UW-ASNs. The proposed AIDPS can improve the security of the UW-ASNs so that they can efficiently detect underwater-related attacks (e.g., blackhole, grayhole and flooding attacks). To determine the most effective configuration of the proposed construction, we conduct a number of experiments using several state-of-the-art machine learning algorithms (e.g., Adaptive Random Forest (ARF), light gradient-boosting machine, and K-nearest neighbours) and concept drift detection
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;NutritionVerse-Synth&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;&#22823;&#35268;&#27169;&#21512;&#25104;&#39135;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22810;&#31181;&#35270;&#35282;&#12289;&#27169;&#24577;&#21644;&#39278;&#39135;&#27880;&#37322;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07704</link><description>&lt;p&gt;
NutritionVerse: &#21508;&#31181;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches. (arXiv:2309.07704v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;NutritionVerse-Synth&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;&#22823;&#35268;&#27169;&#21512;&#25104;&#39135;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22810;&#31181;&#35270;&#35282;&#12289;&#27169;&#24577;&#21644;&#39278;&#39135;&#27880;&#37322;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#23545;&#20110;&#25903;&#25345;&#20581;&#24247;&#39278;&#39135;&#30340;&#25919;&#31574;&#21644;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#33829;&#20859;&#19981;&#33391;&#19982;&#29983;&#27963;&#36136;&#37327;&#19979;&#38477;&#30452;&#25509;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#39135;&#29289;&#26085;&#35760;&#20043;&#31867;&#30340;&#33258;&#25105;&#25253;&#21578;&#26041;&#27861;&#23384;&#22312;&#26174;&#33879;&#20559;&#24046;&#12290;&#20854;&#20182;&#20256;&#32479;&#30340;&#39278;&#39135;&#35780;&#20272;&#25216;&#26415;&#21644;&#26032;&#20852;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22914;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#32791;&#26102;&#38271;&#65292;&#24182;&#19988;&#21487;&#33021;&#38656;&#35201;&#21463;&#36807;&#35757;&#32451;&#30340;&#20154;&#21592;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#20174;&#39135;&#29289;&#22270;&#20687;&#20013;&#33258;&#21160;&#20272;&#35745;&#39278;&#39135;&#25668;&#20837;&#37327;&#65292;&#20294;&#32570;&#20047;&#20855;&#26377;&#22810;&#26679;&#35270;&#35282;&#12289;&#27169;&#24577;&#21644;&#39135;&#29289;&#27880;&#37322;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#38480;&#21046;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NutritionVerse-Synth&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25317;&#26377;84,984&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;2D&#39135;&#29289;&#22270;&#20687;&#21450;&#30456;&#20851;&#39278;&#39135;&#20449;&#24687;&#21644;&#22810;&#27169;&#24577;&#26631;&#27880;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#28145;&#24230;&#22270;&#20687;&#12289;&#23454;&#20363;&#25513;&#33180;&#21644;&#35821;&#20041;&#25513;&#33180;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate dietary intake estimation is critical for informing policies and programs to support healthy eating, as malnutrition has been directly linked to decreased quality of life. However self-reporting methods such as food diaries suffer from substantial bias. Other conventional dietary assessment techniques and emerging alternative approaches such as mobile applications incur high time costs and may necessitate trained personnel. Recent work has focused on using computer vision and machine learning to automatically estimate dietary intake from food images, but the lack of comprehensive datasets with diverse viewpoints, modalities and food annotations hinders the accuracy and realism of such methods. To address this limitation, we introduce NutritionVerse-Synth, the first large-scale dataset of 84,984 photorealistic synthetic 2D food images with associated dietary information and multimodal annotations (including depth images, instance masks, and semantic masks). Additionally, we col
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;TouT&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#20002;&#24323;&#26469;&#37327;&#21270;&#20013;&#38388;&#27493;&#39588;&#19978;&#30340;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#21709;&#24212;&#30340;&#31934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07694</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;TouT&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#20002;&#24323;&#26469;&#37327;&#21270;&#20013;&#38388;&#27493;&#39588;&#19978;&#30340;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#21709;&#24212;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#24341;&#20837;&#30340;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;Tree of Thoughts, ToT&#65289;&#22312;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#39044;&#35265;&#21644;&#22238;&#28335;&#36827;&#34892;&#20840;&#23616;&#20915;&#31574;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#24573;&#35270;&#20102;&#20013;&#38388;&#20915;&#31574;&#28857;&#25110;&#8220;&#24605;&#32500;&#8221;&#20013;&#30340;&#22266;&#26377;&#23616;&#37096;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20123;&#22266;&#26377;&#30340;&#23616;&#37096;&#19981;&#30830;&#23450;&#24615;&#65292;&#30001;&#20110;LLMs&#28508;&#22312;&#30340;&#22810;&#26679;&#24615;&#21709;&#24212;&#33021;&#21147;&#65292;&#25104;&#20026;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;Tree of Uncertain Thoughts, TouT&#65289;-&#19968;&#31181;&#38024;&#23545;LLMs&#35774;&#35745;&#30340;&#25512;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;TouT&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#33945;&#29305;&#21345;&#27931;&#20002;&#24323;(Monte Carlo Dropout)&#26469;&#37327;&#21270;&#19982;LLMs&#22312;&#36825;&#20123;&#20013;&#38388;&#27493;&#39588;&#19978;&#30340;&#19981;&#21516;&#26412;&#22320;&#21709;&#24212;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#20840;&#23616;&#25628;&#32034;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;TouT&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#21709;&#24212;&#30340;&#31934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#33499;&#21051;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20005;&#26684;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;24&#28857;&#28216;&#25103;&#21644;&#36855;&#20320;&#22635;&#23383;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or "thoughts". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#24403;&#21069;&#29992;&#20110;&#21306;&#20998;ChatGPT&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#25991;&#26412;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#30740;&#31350;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.07689</link><description>&lt;p&gt;
&#26816;&#27979;ChatGPT&#65306;&#23545;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25991;&#26412;&#29366;&#24577;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text. (arXiv:2309.07689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07689
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#24403;&#21069;&#29992;&#20110;&#21306;&#20998;ChatGPT&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#25991;&#26412;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#30740;&#31350;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#65288;OpenAI, 2022&#65289;&#31561;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#23427;&#20204;&#29983;&#25104;&#27969;&#21033;&#30340;&#31867;&#20154;&#25991;&#26412;&#24102;&#26469;&#20102;&#21508;&#31181;&#22909;&#22788;&#65292;&#20294;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#25991;&#26412;&#30340;&#20219;&#21153;&#20063;&#22240;&#27492;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#36890;&#36807;&#29983;&#25104;&#30475;&#20284;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#20154;&#24037;&#25991;&#26412;&#32780;&#36827;&#34892;&#27450;&#39575;&#12290;&#22312;&#27861;&#24459;&#12289;&#25945;&#32946;&#21644;&#31185;&#23398;&#31561;&#39046;&#22495;&#65292;&#30830;&#20445;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#27492;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#26174;&#33879;&#12290;&#26412;&#35843;&#26597;&#27010;&#36848;&#20102;&#24403;&#21069;&#29992;&#20110;&#21306;&#20998;&#20154;&#24037;&#25991;&#26412;&#21644;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26500;&#24314;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#36827;&#34892;&#20102;&#21738;&#20123;&#20851;&#20110;&#20154;&#31867;&#19982;ChatGPT&#29983;&#25104;&#25991;&#26412;&#29305;&#24449;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#24182;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent advancements in the capabilities and widespread accessibility of generative language models, such as ChatGPT (OpenAI, 2022), have brought about various benefits by generating fluent human-like text, the task of distinguishing between human- and large language model (LLM) generated text has emerged as a crucial problem. These models can potentially deceive by generating artificial text that appears to be human-generated. This issue is particularly significant in domains such as law, education, and science, where ensuring the integrity of text is of the utmost importance. This survey provides an overview of the current approaches employed to differentiate between texts generated by humans and ChatGPT. We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings int
&lt;/p&gt;</description></item><item><title>deepFDEnet&#26159;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#27714;&#35299;&#21508;&#31181;&#24418;&#24335;&#30340;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.07684</link><description>&lt;p&gt;
deepFDEnet: &#19968;&#20010;&#29992;&#20110;&#27714;&#35299;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
deepFDEnet: A Novel Neural Network Architecture for Solving Fractional Differential Equations. (arXiv:2309.07684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07684
&lt;/p&gt;
&lt;p&gt;
deepFDEnet&#26159;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#27714;&#35299;&#21508;&#31181;&#24418;&#24335;&#30340;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#27714;&#35299;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#35774;&#35745;&#20351;&#29992;&#20102;&#39640;&#26031;&#31215;&#20998;&#35268;&#21017;&#21644;$L_1$&#31163;&#25955;&#21270;&#25216;&#26415;&#12290;&#22312;&#27599;&#20010;&#26041;&#31243;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26410;&#30693;&#20989;&#25968;&#12290;&#30740;&#31350;&#36824;&#23545;&#19977;&#20010;&#24418;&#24335;&#30340;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20102;&#26816;&#39564;&#65292;&#20197;&#31361;&#20986;&#35813;&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#65306;&#20998;&#25968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#12289;&#20998;&#25968;&#38454;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#21644;&#20998;&#25968;&#38454;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#33021;&#22815;&#20197;&#26497;&#39640;&#30340;&#31934;&#24230;&#35299;&#20915;&#19981;&#21516;&#24418;&#24335;&#30340;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary goal of this research is to propose a novel architecture for a deep neural network that can solve fractional differential equations accurately. A Gaussian integration rule and a $L_1$ discretization technique are used in the proposed design. In each equation, a deep neural network is used to approximate the unknown function. Three forms of fractional differential equations have been examined to highlight the method's versatility: a fractional ordinary differential equation, a fractional order integrodifferential equation, and a fractional order partial differential equation. The results show that the proposed architecture solves different forms of fractional differential equations with excellent precision.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.07683</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#36136;&#65306;&#23545;&#20154;&#31867;&#20013;&#24515;&#20027;&#20041;&#30340;&#35686;&#21578;
&lt;/p&gt;
&lt;p&gt;
Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36890;&#36807;OpenAI&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#20851;&#27880;&#21644;&#29468;&#27979;&#12290;&#30446;&#21069;&#23384;&#22312;&#20004;&#31181;&#24847;&#35265;&#38453;&#33829;&#65306;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#20026;&#20154;&#31867;&#20219;&#21153;&#24102;&#26469;&#30340;&#22522;&#26412;&#21464;&#38761;&#30340;&#21487;&#33021;&#24615;&#24863;&#21040;&#20852;&#22859;&#65292;&#21478;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#24863;&#21040;&#39640;&#24230;&#20851;&#20999;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#20851;&#20999;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#12289;&#35268;&#33539;&#21270;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#24037;&#20855;&#26469;&#35780;&#20272;GPT3.5&#12290;&#22312;&#36825;&#20010;&#21021;&#27493;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#27979;&#35797;&#65292;&#21487;&#20197;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#23427;&#20204;&#22312;&#30701;&#26102;&#38388;&#20869;&#30340;&#31283;&#23450;&#24615;&#20197;&#21450;&#19982;&#20154;&#31867;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT 3.5&#24456;&#21487;&#33021;&#27809;&#26377;&#20135;&#29983;&#24847;&#35782;&#65292;&#23613;&#31649;&#23427;&#23545;&#20010;&#24615;&#38382;&#21367;&#30340;&#22238;&#31572;&#33021;&#21147;&#20196;&#20154;&#24863;&#20852;&#36259;&#12290;&#23427;&#22312;&#37325;&#22797;&#35266;&#23519;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#26041;&#38754;&#30340;&#22823;&#37327;&#21464;&#24322;&#65292;&#36825;&#19982;&#20855;&#26377;&#20154;&#31867;&#33324;&#20010;&#24615;&#30340;&#27169;&#22411;&#26159;&#19981;&#31526;&#21512;&#39044;&#26399;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans.  Our results indicate that GPT 3.5 is unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#23458;&#25143;&#31471;&#38388;&#20998;&#24067;&#20559;&#31227;&#21644;&#37096;&#20998;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#25104;&#21151;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.07670</link><description>&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#23458;&#25143;&#31471;&#38388;&#20998;&#24067;&#20559;&#31227;&#21644;&#37096;&#20998;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#25104;&#21151;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#25143;&#31471;&#20013;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#19988;&#37096;&#20998;&#23458;&#25143;&#31471;&#20855;&#26377;&#26080;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;FedDaDiL&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#20195;&#34920;&#30528;&#29305;&#23450;&#30340;&#39046;&#22495;&#65292;&#32780;FedDaDiL&#21017;&#20849;&#21516;&#35757;&#32451;&#20102;&#19968;&#20010;&#32852;&#37030;&#32463;&#39564;&#20998;&#24067;&#23383;&#20856;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#19978;&#35774;&#35745;&#20102;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#12290;&#25152;&#36873;&#25321;&#30340;&#21327;&#35758;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#30456;&#27604;&#20110;&#38598;&#20013;&#24335;&#26041;&#27861;&#25552;&#39640;&#20102;&#25972;&#20307;&#38544;&#31169;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;Caltech-Office&#12289;TEP&#21644;CWRU&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#38598;&#20013;&#24335;&#26041;&#27861;&#21644;&#20854;&#20182;&#32852;&#37030;&#39046;&#22495;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we propose an approach for federated domain adaptation, a setting where distributional shift exists among clients and some have unlabeled data. The proposed framework, FedDaDiL, tackles the resulting challenge through dictionary learning of empirical distributions. In our setting, clients' distributions represent particular domains, and FedDaDiL collectively trains a federated dictionary of empirical distributions. In particular, we build upon the Dataset Dictionary Learning framework by designing collaborative communication protocols and aggregation operations. The chosen protocols keep clients' data private, thus enhancing overall privacy compared to its centralized counterpart. We empirically demonstrate that our approach successfully generates labeled data on the target domain with extensive experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks. Furthermore, we compare our method to its centralized counterpart and other benchmarks in federated doma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#31216;&#20026;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36935;&#35265;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;MSDA-DD&#65289;&#65292;&#36890;&#36807;&#36866;&#24212;&#20808;&#21069;&#30340;&#26041;&#27861;&#20197;&#21450;&#20998;&#37197;&#21305;&#37197;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20165;&#20351;&#29992;&#27599;&#31867;1&#20010;&#26679;&#26412;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#36866;&#24212;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07666</link><description>&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36935;&#35265;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning. (arXiv:2309.07666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#31216;&#20026;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36935;&#35265;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;MSDA-DD&#65289;&#65292;&#36890;&#36807;&#36866;&#24212;&#20808;&#21069;&#30340;&#26041;&#27861;&#20197;&#21450;&#20998;&#37197;&#21305;&#37197;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20165;&#20351;&#29992;&#27599;&#31867;1&#20010;&#26679;&#26412;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20004;&#20010;&#38382;&#39064;&#30340;&#20132;&#38598;&#65306;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#65288;MSDA&#65289;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;DD&#65289;&#12290;&#19968;&#26041;&#38754;&#65292;&#21069;&#32773;&#32771;&#34385;&#20102;&#23558;&#22810;&#20010;&#24322;&#36136;&#30340;&#26631;&#27880;&#28304;&#39046;&#22495;&#36866;&#24212;&#21040;&#19968;&#20010;&#26410;&#26631;&#27880;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21518;&#32773;&#25915;&#20987;&#20102;&#20851;&#20110;&#21512;&#25104;&#19968;&#20010;&#21253;&#21547;&#26377;&#20851;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#20449;&#24687;&#30340;&#23567;&#25688;&#35201;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#31216;&#20026;MSDA-DD&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36866;&#24212;&#20102;MSDA&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#20316;&#21697;&#65292;&#22914;Wasserstein Barycenter Transport&#21644;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#65292;&#20197;&#21450;DD&#26041;&#27861;Distribution Matching&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#23545;&#36825;&#20010;&#26032;&#38382;&#39064;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65288;Caltech-Office 10&#65292; Tennessee-Eastman Process&#65292; Continuous Stirred Tank Reactor&#21644;Case Western Reserve University&#65289;&#65292;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#27599;&#31867;&#21482;&#26377;1&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#20063;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the intersection of two problems in machine learning: Multi-Source Domain Adaptation (MSDA) and Dataset Distillation (DD). On the one hand, the first considers adapting multiple heterogeneous labeled source domains to an unlabeled target domain. On the other hand, the second attacks the problem of synthesizing a small summary containing all the information about the datasets. We thus consider a new problem called MSDA-DD. To solve it, we adapt previous works in the MSDA literature, such as Wasserstein Barycenter Transport and Dataset Dictionary Learning, as well as DD method Distribution Matching. We thoroughly experiment with this novel problem on four benchmarks (Caltech-Office 10, Tennessee-Eastman Process, Continuous Stirred Tank Reactor, and Case Western Reserve University), where we show that, even with as little as 1 sample per class, one achieves state-of-the-art adaptation performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31038;&#21306;&#38382;&#31572;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#25490;&#21517;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;BERT&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#65307;&#20854;&#27425;&#65292;&#32467;&#21512;&#38382;&#39064;&#21644;&#31572;&#26696;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#65307;&#31532;&#19977;&#65292;&#36890;&#36807;&#32463;&#39564;&#24615;&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25490;&#21517;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07610</link><description>&lt;p&gt;
&#23398;&#20064;&#21040;&#25490;&#21517;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#22312;&#31038;&#21306;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Feature Engineering in Learning-to-Rank for Community Question Answering Task. (arXiv:2309.07610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31038;&#21306;&#38382;&#31572;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#25490;&#21517;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;BERT&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#65307;&#20854;&#27425;&#65292;&#32467;&#21512;&#38382;&#39064;&#21644;&#31572;&#26696;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#65307;&#31532;&#19977;&#65292;&#36890;&#36807;&#32463;&#39564;&#24615;&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25490;&#21517;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#65288;CQA&#65289;&#35770;&#22363;&#26159;&#22522;&#20110;&#20114;&#32852;&#32593;&#30340;&#24179;&#21488;&#65292;&#29992;&#25143;&#22312;&#36825;&#37324;&#25552;&#20986;&#38382;&#39064;&#65292;&#20854;&#20182;&#19987;&#23478;&#29992;&#25143;&#35797;&#22270;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#35768;&#22810;CQA&#35770;&#22363;&#65292;&#22914;Quora&#65292;Stackoverflow&#65292;Yahoo&#65281;Answer&#65292;StackExchange&#31561;&#37117;&#26377;&#22823;&#37327;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#22312;&#33258;&#21160;&#21270;&#30340;CQA&#25490;&#21517;&#31995;&#32479;&#20013;&#24471;&#21040;&#21033;&#29992;&#65292;&#20197;&#22238;&#24212;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#21576;&#29616;&#31867;&#20284;&#30340;&#38382;&#39064;&#65288;&#21644;&#31572;&#26696;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35843;&#26597;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#29305;&#24449;&#22914;TF-IDF&#12289;BM25&#31561;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;BERT&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#38382;&#39064;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20854;&#27425;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20165;&#20174;&#38382;&#39064;&#37096;&#20998;&#25552;&#21462;&#30340;&#29305;&#24449;&#19978;&#65292;&#23578;&#26410;&#24191;&#27867;&#25506;&#32034;&#20174;&#31572;&#26696;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#20197;&#32447;&#24615;&#26041;&#24335;&#32467;&#21512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#12290;&#31532;&#19977;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#25490;&#21517;&#31639;&#27861;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community question answering (CQA) forums are Internet-based platforms where users ask questions about a topic and other expert users try to provide solutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer, StackExchange exist with a lot of user-generated data. These data are leveraged in automated CQA ranking systems where similar questions (and answers) are presented in response to the query of the user. In this work, we empirically investigate a few aspects of this domain. Firstly, in addition to traditional features like TF-IDF, BM25 etc., we introduce a BERT-based feature that captures the semantic similarity between the question and answer. Secondly, most of the existing research works have focused on features extracted only from the question part; features extracted from answers have not been explored extensively. We combine both types of features in a linear fashion. Thirdly, using our proposed concepts, we conduct an empirical investigation with different rank-lea
&lt;/p&gt;</description></item><item><title>&#22312;&#27604;&#36739;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20004;&#31181;&#27169;&#22411;SASRec&#21644;BERT4Rec&#26102;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#20004;&#20010;&#27169;&#22411;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;SASRec&#22312;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;BERT4Rec&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#36127;&#37319;&#26679;&#65292;SASRec&#20173;&#28982;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#24182;&#20248;&#20110;BERT4Rec&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#36127;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.07602</link><description>&lt;p&gt;
&#23558;&#24223;&#26009;&#21464;&#20026;&#40644;&#37329;&#30340;&#25439;&#22833;&#65306;BERT4Rec&#30495;&#30340;&#27604;SASRec&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?. (arXiv:2309.07602v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07602
&lt;/p&gt;
&lt;p&gt;
&#22312;&#27604;&#36739;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20004;&#31181;&#27169;&#22411;SASRec&#21644;BERT4Rec&#26102;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#20004;&#20010;&#27169;&#22411;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;SASRec&#22312;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;BERT4Rec&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#36127;&#37319;&#26679;&#65292;SASRec&#20173;&#28982;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#24182;&#20248;&#20110;BERT4Rec&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#39034;&#24207;&#25512;&#33616;&#21644;&#19979;&#19968;&#20010;&#39033;&#30446;&#39044;&#27979;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;SASRec&#21644;BERT4Rec&#26159;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#26377;&#24456;&#22810;&#21457;&#34920;&#30340;&#35770;&#25991;&#27604;&#36739;&#20102;&#36825;&#20004;&#20010;&#31639;&#27861;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#22312;&#22823;&#22810;&#25968;&#35770;&#25991;&#20013;&#65292;BERT4Rec&#30340;&#24615;&#33021;&#20248;&#20110;SASRec&#12290;&#20294;&#26159;&#65292;BERT4Rec&#23545;&#25152;&#26377;&#39033;&#30446;&#20351;&#29992;&#20132;&#21449;&#29109;&#65292;&#32780;SASRec&#20351;&#29992;&#36127;&#37319;&#26679;&#23545;&#19968;&#20010;&#27491;&#26679;&#26412;&#21644;&#19968;&#20010;&#36127;&#26679;&#26412;&#35745;&#31639;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#20004;&#20010;&#27169;&#22411;&#37117;&#20351;&#29992;BERT4Rec&#25152;&#29992;&#30340;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#37027;&#20040;SASRec&#22312;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#23558;&#26126;&#26174;&#20248;&#20110;BERT4Rec&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21363;&#20351;&#20351;&#29992;&#36127;&#37319;&#26679;&#65292;SASRec&#20173;&#28982;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#24182;&#20248;&#20110;BERT4Rec&#65292;&#20294;&#36127;&#26679;&#26412;&#30340;&#25968;&#37327;&#24212;&#35813;&#27604;BERT4Rec&#35201;&#22823;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently sequential recommendations and next-item prediction task has become increasingly popular in the field of recommender systems. Currently, two state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec. Over the past few years, there have been quite a few publications comparing these two algorithms and proposing new state-of-the-art models. In most of the publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec uses cross-entropy over softmax for all items, while SASRec uses negative sampling and calculates binary cross-entropy loss for one positive and one negative item. In our work, we show that if both models are trained with the same loss, which is used by BERT4Rec, then SASRec will significantly outperform BERT4Rec both in terms of quality and training speed. In addition, we show that SASRec could be effectively trained with negative sampling and still outperform BERT4Rec, but the number of negative examples should be much larger than on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.07601</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20449;&#21495;&#21644;&#24369;&#30417;&#30563;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#24230;&#20449;&#21495;&#20195;&#34920;&#20102;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#21592;&#36890;&#24120;&#29992;&#26469;&#35780;&#20272;&#22312;&#32447;&#20869;&#23481;&#30495;&#23454;&#24615;&#30340;&#19968;&#31995;&#21015;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#21487;&#20449;&#24230;&#20449;&#21495;&#25552;&#21462;&#30340;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#35757;&#32451;&#39640;&#20934;&#30830;&#29575;&#30340;&#29305;&#23450;&#20449;&#21495;&#25552;&#21462;&#22120;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;&#21487;&#20449;&#24230;&#20449;&#21495;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#19968;&#32452;18&#20010;&#21487;&#20449;&#24230;&#20449;&#21495;&#26469;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#20135;&#29983;&#27599;&#20010;&#20449;&#21495;&#30340;&#24369;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#36825;&#20123;&#28508;&#22312;&#30340;&#22122;&#22768;&#26631;&#31614;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#32467;&#21512;&#20102;&#38646;-shot LLM&#21487;&#20449;&#24230;&#20449;&#21495;&#26631;&#27880;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#35757;&#32451;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. Automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. This paper investigates whether large language models (LLMs) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. We then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. We demonstrate that our approach, which combines zero-shot LLM credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. We also analyse the contribution of the individual credibility signals towards p
&lt;/p&gt;</description></item><item><title>C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.07597</link><description>&lt;p&gt;
C-Pack: &#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#25171;&#21253;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07597
&lt;/p&gt;
&lt;p&gt;
C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;C-Pack&#65292;&#36825;&#26159;&#19968;&#22871;&#26174;&#33879;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#12290;C-Pack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#36164;&#28304;&#12290;1&#65289;C-MTEB&#26159;&#19968;&#20010;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;35&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;2&#65289;C-MTP&#26159;&#19968;&#20010;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#27721;&#35821;&#35821;&#26009;&#24211;&#20013;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;3&#65289;C-TEM&#26159;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;C-MTEB&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#25152;&#26377;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#36798;&#21040;&#20102;&#21457;&#24067;&#26102;&#30340;&#26368;&#39640;+10%&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#21644;&#20248;&#21270;&#20102;C-TEM&#30340;&#25972;&#22871;&#35757;&#32451;&#26041;&#27861;&#12290;&#38500;&#20102;&#25105;&#20204;&#20851;&#20110;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#36164;&#28304;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#33521;&#35821;&#27169;&#22411;&#22312;MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#24067;&#30340;&#33521;&#35821;&#25968;&#25454;&#27604;&#27721;&#35821;&#25968;&#25454;&#22823;2&#20493;&#12290;&#25152;&#26377;&#36825;&#20123;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;https://github.com/FlagOpen/FlagEmbedding&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#26597;&#35810;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;&#65292;&#23558;&#29992;&#25143;&#21382;&#21490;&#20132;&#20114;&#36716;&#21270;&#20026;&#36923;&#36753;&#34920;&#36798;&#24335;&#65292;&#24182;&#36890;&#36807;&#36923;&#36753;&#26597;&#35810;&#23454;&#29616;&#25512;&#33616;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.07594</link><description>&lt;p&gt;
&#22522;&#20110;&#36923;&#36753;&#26597;&#35810;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Recommendation Model based on Logic Query. (arXiv:2309.07594v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#26597;&#35810;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;&#65292;&#23558;&#29992;&#25143;&#21382;&#21490;&#20132;&#20114;&#36716;&#21270;&#20026;&#36923;&#36753;&#34920;&#36798;&#24335;&#65292;&#24182;&#36890;&#36807;&#36923;&#36753;&#26597;&#35810;&#23454;&#29616;&#25512;&#33616;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#19982;&#20182;&#20204;&#30456;&#20851;&#30340;&#29289;&#21697;&#12290;&#29616;&#26377;&#30340;&#25512;&#33616;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#39044;&#27979;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22797;&#26434;&#30340;&#21305;&#37197;&#27169;&#22411;&#25110;&#24341;&#20837;&#22823;&#37327;&#30340;&#22806;&#37096;&#20449;&#24687;&#26469;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#20851;&#32852;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#25512;&#33616;&#19981;&#20165;&#26159;&#19968;&#20010;&#21033;&#29992;&#25968;&#25454;&#36827;&#34892;&#24402;&#32435;&#32479;&#35745;&#30340;&#38382;&#39064;&#65292;&#20063;&#26159;&#19968;&#20010;&#22522;&#20110;&#20174;&#20449;&#24687;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#20915;&#31574;&#30340;&#35748;&#30693;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#65292;&#36923;&#36753;&#31995;&#32479;&#33258;&#28982;&#21487;&#20197;&#29992;&#20110;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22522;&#20110;&#36923;&#36753;&#31995;&#32479;&#30340;&#30828;&#35268;&#21017;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#19981;&#19968;&#33268;&#21644;&#19981;&#23436;&#25972;&#30340;&#29616;&#23454;&#20219;&#21153;&#20013;&#24456;&#38590;&#24212;&#23545;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#22914;&#25512;&#33616;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;&#65292;&#23558;&#29992;&#25143;&#21382;&#21490;&#20132;&#20114;&#36716;&#21270;&#20026;&#36923;&#36753;&#34920;&#36798;&#24335;&#65292;&#28982;&#21518;&#23558;&#25512;&#33616;&#36807;&#31243;&#21464;&#25442;&#20026;&#36923;&#36753;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recommendation system assists users in finding items that are relevant to them. Existing recommendation models are primarily based on predicting relationships between users and items and use complex matching models or incorporate extensive external information to capture association patterns in data. However, recommendation is not only a problem of inductive statistics using data; it is also a cognitive task of reasoning decisions based on knowledge extracted from information. Hence, a logic system could naturally be incorporated for the reasoning in a recommendation task. However, although hard-rule approaches based on logic systems can provide powerful reasoning ability, they struggle to cope with inconsistent and incomplete knowledge in real-world tasks, especially for complex tasks such as recommendation. Therefore, in this paper, we propose a neuro-symbolic recommendation model, which transforms the user history interactions into a logic expression and then transforms the recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07593</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22797;&#26434;&#23398;&#20064;&#22120;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#26102;&#65292;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;&#31227;&#38500;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;&#26159;&#21442;&#32771;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#32479;&#35745;&#20445;&#35777;&#26469;&#39564;&#35777;&#21464;&#37327;&#21253;&#21547;&#24615;&#26102;&#12290;&#36890;&#24120;&#65292;&#23427;&#20204;&#20351;&#29992;&#21464;&#37327;&#32622;&#25442;&#26041;&#26696;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23384;&#22312;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26102;&#23481;&#26131;&#23558;&#19981;&#37325;&#35201;&#30340;&#21464;&#37327;&#35823;&#35782;&#21035;&#20026;&#37325;&#35201;&#21464;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#30740;&#31350;&#26465;&#20214;&#32622;&#25442;&#37325;&#35201;&#24615;&#65288;Conditional Permutation Importance&#65292;CPI&#65289;&#65292;&#23427;&#26159;&#27169;&#22411;&#26080;&#20851;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;CPI&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;I&#22411;&#38169;&#35823;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#24403;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;CPI&#22987;&#32456;&#26174;&#31034;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variable importance assessment has become a crucial step in machine-learning applications when using complex learners, such as deep neural networks, on large-scale data. Removal-based importance assessment is currently the reference approach, particularly when statistical guarantees are sought to justify variable inclusion. It is often implemented with variable permutation schemes. On the flip side, these approaches risk misidentifying unimportant variables as important in the presence of correlations among covariates. Here we develop a systematic approach for studying Conditional Permutation Importance (CPI) that is model agnostic and computationally lean, as well as reusable benchmarks of state-of-the-art variable importance estimators. We show theoretically and empirically that $\textit{CPI}$ overcomes the limitations of standard permutation importance by providing accurate type-I error control. When used with a deep neural network, $\textit{CPI}$ consistently showed top accuracy ac
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#30340;&#31561;&#21464;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#31561;&#21464;&#38598;&#21512;&#24182;&#22686;&#24378;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07578</link><description>&lt;p&gt;
Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning.&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#27867;&#21270;&#30340;&#31561;&#21464;&#25968;&#25454;&#22686;&#24378;&#65289;
&lt;/p&gt;
&lt;p&gt;
Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning. (arXiv:2309.07578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07578
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#30340;&#31561;&#21464;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#31561;&#21464;&#38598;&#21512;&#24182;&#22686;&#24378;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#25361;&#25112;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#20174;&#22266;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#39069;&#22806;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#39640;&#26234;&#33021;&#20307;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#30446;&#26631;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#24182;&#26816;&#26597;&#20854;&#26159;&#21542;&#31561;&#21464;&#20110;&#22266;&#23450;&#31867;&#22411;&#30340;&#21464;&#25442;&#65292;&#21363;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#24179;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29109;&#27491;&#21017;&#21270;&#22120;&#26469;&#25193;&#23637;&#31561;&#21464;&#38598;&#24182;&#36890;&#36807;&#25152;&#24471;&#21040;&#30340;&#36716;&#25442;&#26679;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31163;&#32447;&#23398;&#20064;&#19968;&#20010;&#26032;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26497;&#22823;&#22320;&#25552;&#39640;&#31574;&#30053;&#22312;&#32771;&#34385;&#30340;&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to address the challenge of generalization in offline reinforcement learning (RL), where the agent learns from a fixed dataset without any additional interaction with the environment. Specifically, we aim to improve the agent's ability to generalize to out-of-distribution goals. To achieve this, we propose to learn a dynamics model and check if it is equivariant with respect to a fixed type of transformation, namely translations in the state space. We then use an entropy regularizer to increase the equivariant set and augment the dataset with the resulting transformed samples. Finally, we learn a new policy offline based on the augmented dataset, with an off-the-shelf offline RL algorithm. Our experimental results demonstrate that our approach can greatly improve the test performance of the policy on the considered environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#39118;&#26684;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#38899;&#33394;&#20445;&#30041;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20043;&#21069;&#26410;&#35265;&#30340;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#36328;&#35821;&#35328;&#39118;&#26684;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2309.07566</link><description>&lt;p&gt;
&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#30340;&#39118;&#26684;&#36716;&#25442;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer. (arXiv:2309.07566v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#39118;&#26684;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#38899;&#33394;&#20445;&#30041;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20043;&#21069;&#26410;&#35265;&#30340;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#36328;&#35821;&#35328;&#39118;&#26684;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65288;S2ST&#65289;&#36890;&#36807;&#31163;&#25955;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#26080;&#27861;&#20445;&#30041;&#28304;&#35821;&#38899;&#30340;&#35828;&#35805;&#20154;&#38899;&#33394;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#39640;&#36136;&#37327;&#35828;&#35805;&#20154;&#24179;&#34892;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#23545;&#20110;&#23398;&#20064;&#28304;&#35821;&#38899;&#21644;&#30446;&#26631;&#35821;&#38899;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#31163;&#25955;&#21333;&#20803;&#30340;&#22768;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#39118;&#26684;&#36716;&#25442;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#30340;S2ST&#26694;&#26550;&#12290;&#22768;&#23398;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#30417;&#30563;&#19978;&#19979;&#25991;&#23398;&#20064;&#33719;&#24471;&#20102;&#39118;&#26684;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#20381;&#36182;&#20110;&#20219;&#20309;&#35828;&#35805;&#20154;&#24179;&#34892;&#25968;&#25454;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#28304;&#35821;&#35328;&#19978;&#23454;&#29616;&#38646;-shot&#36328;&#35821;&#35328;&#39118;&#26684;&#36716;&#25442;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#32763;&#35793;&#35821;&#38899;&#20855;&#26377;&#39640;&#24230;&#30340;&#20445;&#30495;&#24230;&#21644;&#39118;&#26684;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct speech-to-speech translation (S2ST) with discrete self-supervised representations has achieved remarkable accuracy, but is unable to preserve the speaker timbre of the source speech during translation. Meanwhile, the scarcity of high-quality speaker-parallel data poses a challenge for learning style transfer between source and target speech. We propose an S2ST framework with an acoustic language model based on discrete units from a self-supervised model and a neural codec for style transfer. The acoustic language model leverages self-supervised in-context learning, acquiring the ability for style transfer without relying on any speaker-parallel data, thereby overcoming the issue of data scarcity. By using extensive training data, our model achieves zero-shot cross-lingual style transfer on previously unseen source languages. Experiments show that our model generates translated speeches with high fidelity and style similarity. Audio samples are available at this http URL .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21809;&#27468;&#22768;&#38899;Deepfake&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#23545;&#36825;&#19968;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2309.07525</link><description>&lt;p&gt;
SingFake&#65306;&#21809;&#27468;&#22768;&#38899;Deepfake&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SingFake: Singing Voice Deepfake Detection. (arXiv:2309.07525v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21809;&#27468;&#22768;&#38899;Deepfake&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#23545;&#36825;&#19968;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#21809;&#27468;&#22768;&#38899;&#30340;&#20852;&#36215;&#32473;&#33402;&#26415;&#23478;&#21644;&#34892;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#24102;&#26469;&#20102;&#26410;&#32463;&#25480;&#26435;&#30340;&#22768;&#38899;&#20351;&#29992;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#19982;&#21512;&#25104;&#35821;&#38899;&#19981;&#21516;&#65292;&#21512;&#25104;&#21809;&#27468;&#22768;&#38899;&#36890;&#24120;&#22312;&#21253;&#21547;&#24378;&#28872;&#32972;&#26223;&#38899;&#20048;&#30340;&#27468;&#26354;&#20013;&#21457;&#24067;&#65292;&#36825;&#21487;&#33021;&#25513;&#30422;&#20102;&#21512;&#25104;&#36896;&#25104;&#30340;&#29781;&#30133;&#12290;&#27492;&#22806;&#65292;&#21809;&#27468;&#22768;&#38899;&#19982;&#35821;&#38899;&#35805;&#35821;&#20855;&#26377;&#19981;&#21516;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#12290;&#36825;&#20123;&#29420;&#29305;&#30340;&#23646;&#24615;&#20351;&#24471;&#21809;&#27468;&#22768;&#38899;Deepfake&#26816;&#27979;&#25104;&#20026;&#19968;&#20010;&#30456;&#20851;&#20294;&#26174;&#30528;&#19981;&#21516;&#20110;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21809;&#27468;&#22768;&#38899;Deepfake&#26816;&#27979;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;SingFake&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#37326;&#22806;&#31934;&#36873;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;40&#20301;&#27468;&#25163;&#29992;&#20116;&#31181;&#35821;&#35328;&#28436;&#21809;&#30340;28.93&#23567;&#26102;&#30495;&#23454;&#38899;&#39057;&#21644;29.40&#23567;&#26102;&#30340;Deepfake&#38899;&#39057;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35757;&#32451;/&#39564;&#35777;/&#27979;&#35797;&#30340;&#21010;&#20998;&#65292;&#20854;&#20013;&#27979;&#35797;&#38598;&#21253;&#25324;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;SingFake&#35780;&#20272;&#20102;&#22235;&#20010;&#22312;&#35821;&#38899;&#35805;&#35821;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#23545;&#25239;&#31995;&#32479;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#31995;&#32479;&#22312;&#21809;&#27468;&#22768;&#38899;&#19978;&#30340;&#34920;&#29616;&#19981;&#21516;&#20110;&#22312;&#35821;&#38899;&#35805;&#35821;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of singing voice synthesis presents critical challenges to artists and industry stakeholders over unauthorized voice usage. Unlike synthesized speech, synthesized singing voices are typically released in songs containing strong background music that may hide synthesis artifacts. Additionally, singing voices present different acoustic and linguistic characteristics from speech utterances. These unique properties make singing voice deepfake detection a relevant but significantly different problem from synthetic speech detection. In this work, we propose the singing voice deepfake detection task. We first present SingFake, the first curated in-the-wild dataset consisting of 28.93 hours of bonafide and 29.40 hours of deepfake song clips in five languages from 40 singers. We provide a train/val/test split where the test sets include various scenarios. We then use SingFake to evaluate four state-of-the-art speech countermeasure systems trained on speech utterances. We find these sys
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07510</link><description>&lt;p&gt;
&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#36974;&#25377;&#19979;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#21487;&#20379;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#24863;&#30693;&#21644;&#25805;&#20316;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#23545;&#20110;&#23478;&#24237;&#21161;&#29702;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28857;&#32423;&#21487;&#20379;&#24615;&#20026;&#19979;&#28216;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#20808;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#29289;&#20307;&#22330;&#26223;&#20013;&#30340;&#22343;&#36136;&#20195;&#29702;&#65292;&#24573;&#35270;&#20102;&#29615;&#22659;&#21644;&#20195;&#29702;&#24418;&#24577;&#25152;&#26045;&#21152;&#30340;&#29616;&#23454;&#32422;&#26463;&#65292;&#22914;&#36974;&#25377;&#21644;&#29289;&#29702;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29289;&#20307;&#32423;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#12290;&#19982;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#21487;&#20379;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#38754;&#20020;&#30528;&#30001;&#21508;&#31181;&#36974;&#25377;&#30340;&#22797;&#26434;&#24615;&#24341;&#36215;&#30340;&#32452;&#21512;&#29190;&#28856;&#25361;&#25112;&#65292;&#36825;&#20123;&#36974;&#25377;&#20197;&#20854;&#25968;&#37327;&#12289;&#20960;&#20309;&#24418;&#29366;&#12289;&#20301;&#32622;&#21644;&#23039;&#21183;&#26469;&#21051;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#24335;&#21487;&#20379;&#24615;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21547;&#26377;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, e.g., occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26234;&#33021;&#33258;&#30417;&#30563;&#22522;&#30784;&#35774;&#26045;&#30340;&#35270;&#39057;&#39044;&#27979;&#26469;&#23454;&#29616;&#36830;&#25509;&#33258;&#20027;&#36710;&#36742;&#30340;&#36816;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20462;&#25913;&#39044;&#27979;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#21344;&#29992;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;&#35270;&#39057;&#65292;&#20943;&#23569;&#20102;&#24191;&#25773;&#39044;&#27979;&#30340;&#25968;&#25454;&#21344;&#29992;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#35774;&#35745;&#21487;&#20197;&#26377;&#25928;&#22320;&#36741;&#21161;CAV&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2309.07504</link><description>&lt;p&gt;
&#36890;&#36807;&#26469;&#33258;&#26234;&#33021;&#33258;&#30417;&#30563;&#22522;&#30784;&#35774;&#26045;&#30340;&#35270;&#39057;&#39044;&#27979;&#23454;&#29616;&#36830;&#25509;&#33258;&#20027;&#36710;&#36742;&#30340;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Connected Autonomous Vehicle Motion Planning with Video Predictions from Smart, Self-Supervised Infrastructure. (arXiv:2309.07504v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26234;&#33021;&#33258;&#30417;&#30563;&#22522;&#30784;&#35774;&#26045;&#30340;&#35270;&#39057;&#39044;&#27979;&#26469;&#23454;&#29616;&#36830;&#25509;&#33258;&#20027;&#36710;&#36742;&#30340;&#36816;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20462;&#25913;&#39044;&#27979;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#21344;&#29992;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;&#35270;&#39057;&#65292;&#20943;&#23569;&#20102;&#24191;&#25773;&#39044;&#27979;&#30340;&#25968;&#25454;&#21344;&#29992;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#35774;&#35745;&#21487;&#20197;&#26377;&#25928;&#22320;&#36741;&#21161;CAV&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#33258;&#20027;&#36710;&#36742;(CAVs)&#26377;&#26395;&#25552;&#39640;&#22478;&#24066;&#20132;&#36890;&#30340;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#21462;&#20915;&#20110;CAV&#33021;&#21542;&#27491;&#30830;&#39044;&#27979;&#21608;&#22260;&#36816;&#21160;&#30340;&#20195;&#29702;&#32773;&#65292;&#24182;&#23433;&#20840;&#35268;&#21010;&#33258;&#24049;&#30340;&#36816;&#21160;&#12290;&#22312;&#22797;&#26434;&#30340;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#39057;&#32321;&#30340;&#36974;&#25377;&#21644;&#22810;&#20010;&#20195;&#29702;&#32773;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#19968;&#28857;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#26469;&#22686;&#24378;CAV&#30340;&#24773;&#22659;&#24847;&#35782;&#65307;&#26412;&#25991;&#21033;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#8220;&#33258;&#30417;&#30563;&#20132;&#36890;&#25351;&#23548;&#22120;&#8221;(SSTA)&#26234;&#33021;&#20256;&#24863;&#22120;&#26694;&#26550;&#65292;&#35753;&#20854;&#33258;&#24049;&#29983;&#25104;&#21644;&#24191;&#25773;&#26377;&#29992;&#30340;&#36335;&#29992;&#25143;&#35270;&#39057;&#39044;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#23558;SSTA&#39044;&#27979;&#20462;&#25913;&#20026;&#39044;&#27979;&#26410;&#26469;&#30340;&#21344;&#29992;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;&#35270;&#39057;&#65292;&#20174;&#32780;&#20943;&#23569;&#24191;&#25773;&#39044;&#27979;&#30340;&#25968;&#25454;&#21344;&#29992;&#37327;&#12290;&#24471;&#21040;&#30340;&#39044;&#27979;&#32467;&#26524;&#22312;&#35268;&#21010;&#26694;&#26550;&#20013;&#20351;&#29992;&#65292;&#35777;&#26126;&#36825;&#31181;&#35774;&#35745;&#21487;&#20197;&#26377;&#25928;&#22320;&#36741;&#21161;CAV&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;&#22810;&#31181;&#25968;&#20540;&#23454;&#39564;&#30740;&#31350;&#20102;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connected autonomous vehicles (CAVs) promise to enhance safety, efficiency, and sustainability in urban transportation. However, this is contingent upon a CAV correctly predicting the motion of surrounding agents and planning its own motion safely. Doing so is challenging in complex urban environments due to frequent occlusions and interactions among many agents. One solution is to leverage smart infrastructure to augment a CAV's situational awareness; the present work leverages a recently proposed "Self-Supervised Traffic Advisor" (SSTA) framework of smart sensors that teach themselves to generate and broadcast useful video predictions of road users. In this work, SSTA predictions are modified to predict future occupancy instead of raw video, which reduces the data footprint of broadcast predictions. The resulting predictions are used within a planning framework, demonstrating that this design can effectively aid CAV motion planning. A variety of numerical experiments study the key fa
&lt;/p&gt;</description></item><item><title>HDTR-Net&#26159;&#19968;&#31181;&#23454;&#26102;&#39640;&#28165;&#29273;&#40831;&#20462;&#22797;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;Fine-Grained Feature Fusion (FGFF)&#27169;&#22359;&#26469;&#25429;&#25417;&#32454;&#32441;&#29702;&#29305;&#24449;&#20449;&#24687;&#24182;&#25552;&#39640;&#29273;&#40831;&#30340;&#28165;&#26224;&#24230;&#65292;HDTR-Net&#21487;&#20197;&#24555;&#36895;&#22686;&#24378;&#29273;&#40831;&#21306;&#22495;&#65292;&#24182;&#20445;&#25345;&#21516;&#27493;&#21644;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07495</link><description>&lt;p&gt;
HDTR-Net&#65306;&#19968;&#31181;&#29992;&#20110;&#20219;&#24847;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#26041;&#27861;&#30340;&#23454;&#26102;&#39640;&#28165;&#29273;&#40831;&#20462;&#22797;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HDTR-Net: A Real-Time High-Definition Teeth Restoration Network for Arbitrary Talking Face Generation Methods. (arXiv:2309.07495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07495
&lt;/p&gt;
&lt;p&gt;
HDTR-Net&#26159;&#19968;&#31181;&#23454;&#26102;&#39640;&#28165;&#29273;&#40831;&#20462;&#22797;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;Fine-Grained Feature Fusion (FGFF)&#27169;&#22359;&#26469;&#25429;&#25417;&#32454;&#32441;&#29702;&#29305;&#24449;&#20449;&#24687;&#24182;&#25552;&#39640;&#29273;&#40831;&#30340;&#28165;&#26224;&#24230;&#65292;HDTR-Net&#21487;&#20197;&#24555;&#36895;&#22686;&#24378;&#29273;&#40831;&#21306;&#22495;&#65292;&#24182;&#20445;&#25345;&#21516;&#27493;&#21644;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#33080;&#29983;&#25104; (TFG) &#26088;&#22312;&#37325;&#24314;&#38754;&#37096;&#36816;&#21160;&#65292;&#20197;&#23454;&#29616;&#20174;&#38899;&#39057;&#21644;&#38754;&#37096;&#29305;&#24449;&#20013;&#33719;&#24471;&#39640;&#33258;&#28982;&#24230;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;&#29616;&#26377;&#30340; TFG &#26041;&#27861;&#22312;&#20135;&#29983;&#33258;&#28982;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#24456;&#23569;&#32771;&#34385;&#35270;&#35273;&#36136;&#37327;&#12290;&#22312;&#36328;&#27169;&#24335;&#29983;&#25104;&#26041;&#27861;&#20013;&#65292;&#21516;&#26102;&#20445;&#35777;&#21767;&#37096;&#21516;&#27493;&#21644;&#36991;&#20813;&#35270;&#35273;&#36136;&#37327;&#19979;&#38477;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#39640;&#28165;&#29273;&#40831;&#20462;&#22797;&#32593;&#32476;&#65292;&#31216;&#20026;HDTR-Net&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#30340;TFG&#26041;&#27861;&#12290;HDTR-Net&#21487;&#20197;&#20197;&#26497;&#24555;&#30340;&#36895;&#24230;&#22686;&#24378;&#29273;&#40831;&#21306;&#22495;&#65292;&#24182;&#20445;&#25345;&#21516;&#27493;&#21644;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Fine-Grained Feature Fusion (FGFF) &#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#29273;&#40831;&#21608;&#22260;&#32454;&#32441;&#29702;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#23545;&#29305;&#24449;&#22270;&#36827;&#34892;&#32454;&#31890;&#24230;&#22788;&#29702;&#20197;&#25552;&#39640;&#29273;&#40831;&#30340;&#28165;&#26224;&#24230;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Talking Face Generation (TFG) aims to reconstruct facial movements to achieve high natural lip movements from audio and facial features that are under potential connections. Existing TFG methods have made significant advancements to produce natural and realistic images. However, most work rarely takes visual quality into consideration. It is challenging to ensure lip synchronization while avoiding visual quality degradation in cross-modal generation methods. To address this issue, we propose a universal High-Definition Teeth Restoration Network, dubbed HDTR-Net, for arbitrary TFG methods. HDTR-Net can enhance teeth regions at an extremely fast speed while maintaining synchronization, and temporal consistency. In particular, we propose a Fine-Grained Feature Fusion (FGFF) module to effectively capture fine texture feature information around teeth and surrounding regions, and use these features to fine-grain the feature map to enhance the clarity of teeth. Extensive experiments show that
&lt;/p&gt;</description></item><item><title>Where2Explore&#26159;&#19968;&#31181;&#38024;&#23545;&#26410;&#35265;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#26368;&#23569;&#25968;&#37327;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20855;&#26377;&#31867;&#20284;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26032;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.07473</link><description>&lt;p&gt;
Where2Explore: &#20026;&#26410;&#35265;&#36807;&#30340;&#20851;&#33410;&#29289;&#20307;&#36827;&#34892;&#23569;&#26679;&#26412;&#33021;&#21147;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects. (arXiv:2309.07473v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07473
&lt;/p&gt;
&lt;p&gt;
Where2Explore&#26159;&#19968;&#31181;&#38024;&#23545;&#26410;&#35265;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#26368;&#23569;&#25968;&#37327;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20855;&#26377;&#31867;&#20284;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26032;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#33410;&#29289;&#20307;&#30340;&#25805;&#20316;&#26159;&#26426;&#22120;&#20154;&#20013;&#19968;&#39033;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#29289;&#20307;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#24046;&#24322;&#65292;&#20197;&#24448;&#30340;&#25805;&#32437;&#27169;&#22411;&#38590;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#31867;&#21035;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20801;&#35768;&#26426;&#22120;&#20154;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#36827;&#34892;&#23569;&#37327;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#19982;&#27599;&#20010;&#26410;&#35265;&#23454;&#20363;&#36827;&#34892;&#26114;&#36149;&#19988;&#20302;&#25928;&#30340;&#27979;&#35797;&#20132;&#20114;&#12290;&#37492;&#20110;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#24418;&#29366;&#65292;&#19981;&#21516;&#30340;&#31867;&#21035;&#36890;&#24120;&#20849;&#20139;&#31867;&#20284;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#20123;&#32467;&#26500;&#23545;&#20110;&#25805;&#20316;&#26159;&#24517;&#35201;&#30340;&#65292;&#27604;&#22914;&#21487;&#25289;&#21160;&#30340;&#25163;&#26564;&#21644;&#21487;&#25235;&#21462;&#30340;&#36793;&#32536;-&#36825;&#20010;&#22240;&#32032;&#22312;&#20197;&#21069;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#31181;&#20849;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;Where2Explore&#8221;&#65292;&#19968;&#31181;&#25506;&#32034;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#23454;&#20363;&#19978;&#36827;&#34892;&#26368;&#23569;&#20132;&#20114;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Articulated object manipulation is a fundamental yet challenging task in robotics. Due to significant geometric and semantic variations across object categories, previous manipulation models struggle to generalize to novel categories. Few-shot learning is a promising solution for alleviating this issue by allowing robots to perform a few interactions with unseen objects. However, extant approaches often necessitate costly and inefficient test-time interactions with each unseen instance. Recognizing this limitation, we observe that despite their distinct shapes, different categories often share similar local geometries essential for manipulation, such as pullable handles and graspable edges - a factor typically underutilized in previous few-shot learning works. To harness this commonality, we introduce 'Where2Explore', an affordance learning framework that effectively explores novel categories with minimal interactions on a limited number of instances. Our framework explicitly estimates
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#23450;&#21046;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#22534;&#21472;&#23376;&#32858;&#31867;&#25216;&#26415;&#26469;&#35782;&#21035;&#26410;&#30693;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.07461</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#26816;&#27979;&#26410;&#30693;&#25915;&#20987;: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07461
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#23450;&#21046;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#22534;&#21472;&#23376;&#32858;&#31867;&#25216;&#26415;&#26469;&#35782;&#21035;&#26410;&#30693;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#35774;&#22791;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#36825;&#24341;&#20837;&#20102;&#20114;&#32852;&#30340;&#26102;&#20195;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24378;&#22823;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23433;&#20840;&#31995;&#32479;&#26159;&#22522;&#20110;&#23553;&#38381;&#19990;&#30028;&#35270;&#35282;&#35774;&#35745;&#30340;&#65292;&#24448;&#24448;&#38754;&#20020;&#19982;&#19981;&#26029;&#21457;&#23637;&#30340;&#23041;&#32961;&#29615;&#22659;&#20013;&#26032;&#30340;&#12289;&#38476;&#29983;&#30340;&#25915;&#20987;&#30456;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29289;&#32852;&#32593;&#29615;&#22659;&#19979;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#20013;&#30340;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20174;&#32593;&#32476;&#27969;&#37327;&#20013;&#25552;&#21462;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38598;&#25104;&#20102;&#22534;&#21472;&#21644;&#23376;&#32858;&#31867;&#25216;&#26415;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#33391;&#24615;&#34892;&#20026;&#65292;&#23454;&#29616;&#23545;&#26410;&#30693;&#25915;&#20987;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread integration of Internet of Things (IoT) devices across all facets of life has ushered in an era of interconnectedness, creating new avenues for cybersecurity challenges and underscoring the need for robust intrusion detection systems. However, traditional security systems are designed with a closed-world perspective and often face challenges in dealing with the ever-evolving threat landscape, where new and unfamiliar attacks are constantly emerging. In this paper, we introduce a framework aimed at mitigating the open set recognition (OSR) problem in the realm of Network Intrusion Detection Systems (NIDS) tailored for IoT environments. Our framework capitalizes on image-based representations of packet-level data, extracting spatial and temporal patterns from network traffic. Additionally, we integrate stacking and sub-clustering techniques, enabling the identification of unknown attacks by effectively modeling the complex and diverse nature of benign behavior. The empiric
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#29289;&#32852;&#32593;&#20013;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;AGI&#26080;&#32541;&#38598;&#25104;&#21040;&#29289;&#32852;&#32593;&#20013;&#30340;&#27010;&#24565;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.07438</link><description>&lt;p&gt;
&#26397;&#30528;&#29289;&#32852;&#32593;&#20013;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#21457;&#23637;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Towards Artificial General Intelligence (AGI) in the Internet of Things (IoT): Opportunities and Challenges. (arXiv:2309.07438v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#29289;&#32852;&#32593;&#20013;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;AGI&#26080;&#32541;&#38598;&#25104;&#21040;&#29289;&#32852;&#32593;&#20013;&#30340;&#27010;&#24565;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#20855;&#22791;&#29702;&#35299;&#12289;&#23398;&#20064;&#21644;&#25191;&#34892;&#20219;&#21153;&#30340;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;&#31185;&#23398;&#12289;&#21830;&#19994;&#21644;&#31038;&#20250;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#31181;&#20851;&#27880;&#29305;&#21035;&#24310;&#20280;&#21040;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#39046;&#22495;&#65292;&#35813;&#39046;&#22495;&#36890;&#36807;&#23558;&#26080;&#25968;&#35774;&#22791;&#12289;&#20256;&#24863;&#22120;&#21644;&#31995;&#32479;&#36830;&#25509;&#36215;&#26469;&#65292;&#20849;&#21516;&#25910;&#38598;&#21644;&#20849;&#20139;&#25968;&#25454;&#20197;&#23454;&#29616;&#26234;&#33021;&#20915;&#31574;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#23454;&#29616;AGI&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#39318;&#20808;&#27010;&#36848;&#20102;&#29289;&#32852;&#32593;&#30340;&#22522;&#26412;&#21407;&#29702;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#38543;&#21518;&#65292;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;AGI&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;AGI&#26080;&#32541;&#38598;&#25104;&#21040;&#29289;&#32852;&#32593;&#20013;&#30340;&#27010;&#24565;&#26694;&#26550;&#12290;AGI&#19982;&#29289;&#32852;&#32593;&#30456;&#32467;&#21512;&#30340;&#24212;&#29992;&#33539;&#22260;&#24191;&#27867;&#65292;&#28085;&#30422;&#20102;&#20174;&#26234;&#33021;&#22478;&#24066;&#21040;&#26234;&#33021;&#21307;&#30103;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence (AGI), possessing the capacity to comprehend, learn, and execute tasks with human cognitive abilities, engenders significant anticipation and intrigue across scientific, commercial, and societal arenas. This fascination extends particularly to the Internet of Things (IoT), a landscape characterized by the interconnection of countless devices, sensors, and systems, collectively gathering and sharing data to enable intelligent decision-making and automation. This research embarks on an exploration of the opportunities and challenges towards achieving AGI in the context of the IoT. Specifically, it starts by outlining the fundamental principles of IoT and the critical role of Artificial Intelligence (AI) in IoT systems. Subsequently, it delves into AGI fundamentals, culminating in the formulation of a conceptual framework for AGI's seamless integration within IoT. The application spectrum for AGI-infused IoT is broad, encompassing domains ranging from smart
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#35821;&#20041;&#35299;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20351;&#29992;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#12289;&#30693;&#35782;&#36801;&#31227;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#12290;&#20855;&#20307;&#26041;&#27861;&#21253;&#25324;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#12289;&#21033;&#29992;&#28304;&#39046;&#22495;&#30693;&#35782;&#25913;&#36827;&#30446;&#26631;&#39046;&#22495;&#35299;&#26512;&#65292;&#20197;&#21450;&#21033;&#29992;&#26377;&#38480;&#30340;&#20154;&#24037;&#32763;&#35793;&#39044;&#31639;&#21644;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#26469;&#35843;&#25972;&#35299;&#26512;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.07429</link><description>&lt;p&gt;
&#22312;&#26377;&#38480;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Semantic Parsing in Limited Resource Conditions. (arXiv:2309.07429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#35821;&#20041;&#35299;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20351;&#29992;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#12289;&#30693;&#35782;&#36801;&#31227;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#12290;&#20855;&#20307;&#26041;&#27861;&#21253;&#25324;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#12289;&#21033;&#29992;&#28304;&#39046;&#22495;&#30693;&#35782;&#25913;&#36827;&#30446;&#26631;&#39046;&#22495;&#35299;&#26512;&#65292;&#20197;&#21450;&#21033;&#29992;&#26377;&#38480;&#30340;&#20154;&#24037;&#32763;&#35793;&#39044;&#31639;&#21644;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#26469;&#35843;&#25972;&#35299;&#26512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#35821;&#20041;&#35299;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#12289;&#30693;&#35782;&#36801;&#31227;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#31561;&#25216;&#26415;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#27809;&#26377;&#24179;&#34892;&#35757;&#32451;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#27169;&#24335;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;&#24403;&#28304;&#39046;&#22495;&#26377;&#22823;&#37327;&#25968;&#25454;&#20294;&#30446;&#26631;&#39046;&#22495;&#21482;&#26377;&#26377;&#38480;&#30340;&#24179;&#34892;&#25968;&#25454;&#26102;&#65292;&#21033;&#29992;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#25913;&#36827;&#30446;&#26631;&#39046;&#22495;&#30340;&#35299;&#26512;&#12290;&#23545;&#20110;&#30446;&#26631;&#35821;&#35328;&#20013;&#26377;&#38480;&#25968;&#25454;&#30340;&#22810;&#35821;&#35328;&#24773;&#20917;&#65292;&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#24037;&#32763;&#35793;&#39044;&#31639;&#26469;&#35843;&#25972;&#35299;&#26512;&#22120;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#28304;&#35821;&#35328;&#26679;&#26412;&#36827;&#34892;&#25163;&#21160;&#32763;&#35793;&#65292;&#26368;&#22823;&#21270;&#30446;&#26631;&#35821;&#35328;&#35299;&#26512;&#22120;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#65292;&#24182;&#36741;&#20197;&#20154;&#24037;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis explores challenges in semantic parsing, specifically focusing on scenarios with limited data and computational resources. It offers solutions using techniques like automatic data curation, knowledge transfer, active learning, and continual learning.  For tasks with no parallel training data, the thesis proposes generating synthetic training examples from structured database schemas. When there is abundant data in a source domain but limited parallel data in a target domain, knowledge from the source is leveraged to improve parsing in the target domain.  For multilingual situations with limited data in the target languages, the thesis introduces a method to adapt parsers using a limited human translation budget. Active learning is applied to select source-language samples for manual translation, maximizing parser performance in the target language. In addition, an alternative method is also proposed to utilize machine translation services, supplemented by human-translated d
&lt;/p&gt;</description></item><item><title>JSMNet&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#23610;&#24230;&#25913;&#36827;&#23460;&#20869;&#28857;&#20113;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#12290;&#22312;&#23460;&#20869;3D&#28857;&#20113;&#25968;&#25454;&#20013;&#65292;JSMNet&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23460;&#20869;&#30446;&#26631;&#29305;&#24449;&#34920;&#36798;&#21644;&#35821;&#20041;&#12289;&#23454;&#20363;&#20998;&#21106;&#32467;&#26524;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07425</link><description>&lt;p&gt;
JSMNet&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#23610;&#24230;&#25913;&#36827;&#23460;&#20869;&#28857;&#20113;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
JSMNet Improving Indoor Point Cloud Semantic and Instance Segmentation through Self-Attention and Multiscale. (arXiv:2309.07425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07425
&lt;/p&gt;
&lt;p&gt;
JSMNet&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#23610;&#24230;&#25913;&#36827;&#23460;&#20869;&#28857;&#20113;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#12290;&#22312;&#23460;&#20869;3D&#28857;&#20113;&#25968;&#25454;&#20013;&#65292;JSMNet&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23460;&#20869;&#30446;&#26631;&#29305;&#24449;&#34920;&#36798;&#21644;&#35821;&#20041;&#12289;&#23454;&#20363;&#20998;&#21106;&#32467;&#26524;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#19968;&#31995;&#21015;&#21518;&#32493;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#23460;&#20869;&#26381;&#21153;&#26426;&#22120;&#20154;&#12289;&#23548;&#33322;&#31995;&#32479;&#21644;&#25968;&#23383;&#23402;&#29983;&#24037;&#31243;&#12290;&#20840;&#23616;&#29305;&#24449;&#23545;&#20110;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#23460;&#20869;&#28857;&#20113;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#38271;&#31243;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JSMNet&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#23618;&#32593;&#32476;&#21644;&#20840;&#23616;&#29305;&#24449;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20849;&#21516;&#20998;&#21106;&#19977;&#32500;&#28857;&#20113;&#30340;&#35821;&#20041;&#21644;&#23454;&#20363;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34920;&#36798;&#23460;&#20869;&#30446;&#26631;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#65292;&#32771;&#34385;&#20102;&#30001;&#20110;&#25195;&#25551;&#20202;&#36317;&#31163;&#30446;&#26631;&#30340;&#21464;&#21270;&#32780;&#23548;&#33268;&#30340;&#28857;&#20113;&#23494;&#24230;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#35821;&#20041;&#21644;&#23454;&#20363;&#29305;&#24449;&#30340;&#32852;&#21512;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#26694;&#26550;&#65292;&#20197;&#36798;&#21040;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;S3DIS&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22823;&#22411;&#30340;&#23460;&#20869;&#22320;&#29289;&#28857;&#20113;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The semantic understanding of indoor 3D point cloud data is crucial for a range of subsequent applications, including indoor service robots, navigation systems, and digital twin engineering. Global features are crucial for achieving high-quality semantic and instance segmentation of indoor point clouds, as they provide essential long-range context information. To this end, we propose JSMNet, which combines a multi-layer network with a global feature self-attention module to jointly segment three-dimensional point cloud semantics and instances. To better express the characteristics of indoor targets, we have designed a multi-resolution feature adaptive fusion module that takes into account the differences in point cloud density caused by varying scanner distances from the target. Additionally, we propose a framework for joint semantic and instance segmentation by integrating semantic and instance features to achieve superior results. We conduct experiments on S3DIS, which is a large thr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23458;&#25143;&#31471;&#27745;&#26579;&#26799;&#24230;&#21453;&#28436;&#65288;CGI&#65289;&#65292;&#33021;&#22815;&#20174;&#23458;&#25143;&#31471;&#21457;&#36215;&#24182;&#22312;&#26377;&#38480;&#30340;&#30693;&#35782;&#26465;&#20214;&#19979;&#24674;&#22797;&#35757;&#32451;&#26679;&#26412;&#12290;&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23637;&#31034;&#20102;&#23458;&#25143;&#31471;&#23545;FL&#30340;&#25915;&#20987;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07415</link><description>&lt;p&gt;
&#23545;&#25239;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#26799;&#24230;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
Client-side Gradient Inversion Against Federated Learning from Poisoning. (arXiv:2309.07415v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07415
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23458;&#25143;&#31471;&#27745;&#26579;&#26799;&#24230;&#21453;&#28436;&#65288;CGI&#65289;&#65292;&#33021;&#22815;&#20174;&#23458;&#25143;&#31471;&#21457;&#36215;&#24182;&#22312;&#26377;&#38480;&#30340;&#30693;&#35782;&#26465;&#20214;&#19979;&#24674;&#22797;&#35757;&#32451;&#26679;&#26412;&#12290;&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23637;&#31034;&#20102;&#23458;&#25143;&#31471;&#23545;FL&#30340;&#25915;&#20987;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#20998;&#24067;&#24335;&#21442;&#19982;&#32773;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#33021;&#22815;&#22312;&#19981;&#30452;&#25509;&#20849;&#20139;&#25968;&#25454;&#32473;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;FL&#23481;&#26131;&#21463;&#21040;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#65288;GIA&#65289;&#30340;&#23041;&#32961;&#65292;&#35813;&#25915;&#20987;&#26088;&#22312;&#37325;&#26500;&#21407;&#22987;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#23545;FL&#20013;&#30340;&#23458;&#25143;&#31471;&#38544;&#31169;&#26500;&#25104;&#39640;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GIA&#26041;&#27861;&#38656;&#35201;&#25511;&#21046;&#26381;&#21153;&#22120;&#24182;&#20381;&#36182;&#24378;&#20808;&#39564;&#30693;&#35782;&#65292;&#21253;&#25324;&#25209;&#24402;&#19968;&#21270;&#21644;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23458;&#25143;&#31471;&#27745;&#26579;&#26799;&#24230;&#21453;&#28436;&#65288;CGI&#65289;&#65292;&#21487;&#20197;&#20174;&#23458;&#25143;&#31471;&#21457;&#36215;&#12290;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#26377;&#38480;&#30693;&#35782;&#30340;&#23458;&#25143;&#31471;&#23545;&#25439;&#22833;&#24694;&#24847;&#27169;&#22411;&#30340;&#21033;&#29992;&#65292;&#33021;&#22815;&#20174;&#32858;&#21512;&#30340;&#20840;&#23616;&#27169;&#22411;&#20013;&#24674;&#22797;&#35757;&#32451;&#26679;&#26412;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables distributed participants (e.g., mobile devices) to train a global model without sharing data directly to a central server. Recent studies have revealed that FL is vulnerable to gradient inversion attack (GIA), which aims to reconstruct the original training samples and poses high risk against the privacy of clients in FL. However, most existing GIAs necessitate control over the server and rely on strong prior knowledge including batch normalization and data distribution information. In this work, we propose Client-side poisoning Gradient Inversion (CGI), which is a novel attack method that can be launched from clients. For the first time, we show the feasibility of a client-side adversary with limited knowledge being able to recover the training samples from the aggregated global model. We take a distinct approach in which the adversary utilizes a malicious model that amplifies the loss of a specific targeted class of interest. When honest clients employ
&lt;/p&gt;</description></item><item><title>FunCodec&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#12289;&#21487;&#22797;&#29616;&#30340;&#12289;&#21487;&#25972;&#21512;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#37325;&#26500;&#36136;&#37327;&#21644;&#25972;&#21512;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2309.07405</link><description>&lt;p&gt;
FunCodec:&#19968;&#20010;&#22522;&#30784;&#30340;&#12289;&#21487;&#22797;&#29616;&#30340;&#12289;&#21487;&#25972;&#21512;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#24320;&#28304;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec. (arXiv:2309.07405v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07405
&lt;/p&gt;
&lt;p&gt;
FunCodec&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#12289;&#21487;&#22797;&#29616;&#30340;&#12289;&#21487;&#25972;&#21512;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#37325;&#26500;&#36136;&#37327;&#21644;&#25972;&#21512;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FunCodec&#65292;&#19968;&#20010;&#22522;&#30784;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#24037;&#20855;&#21253;&#65292;&#23427;&#26159;&#24320;&#28304;&#35821;&#38899;&#22788;&#29702;&#24037;&#20855;&#21253;FunASR&#30340;&#25193;&#23637;&#12290;FunCodec&#25552;&#20379;&#20102;&#21487;&#22797;&#29616;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#25512;&#26029;&#33050;&#26412;&#65292;&#29992;&#20110;&#26368;&#26032;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#22914;SoundStream&#21644;Encodec&#12290;&#30001;&#20110;&#19982;FunASR&#30340;&#32479;&#19968;&#35774;&#35745;&#65292;FunCodec&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22914;&#35821;&#38899;&#35782;&#21035;&#12290;&#38500;&#20102;FunCodec&#65292;&#36824;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#23398;&#26415;&#25110;&#26222;&#36941;&#29992;&#36884;&#12290;&#22522;&#20110;&#35813;&#24037;&#20855;&#21253;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#39057;&#22495;&#32534;&#35299;&#30721;&#22120;&#27169;&#22411;FreqCodec&#65292;&#23427;&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#21442;&#25968;&#22797;&#26434;&#24230;&#23454;&#29616;&#30456;&#24403;&#30340;&#35821;&#38899;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#21387;&#32553;&#27604;&#19979;&#65292;FunCodec&#21487;&#20197;&#23454;&#29616;&#19982;&#20854;&#20182;&#24037;&#20855;&#21253;&#21644;&#21457;&#24067;&#27169;&#22411;&#30456;&#27604;&#26356;&#22909;&#30340;&#37325;&#26500;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#29992;&#20110;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#22312;&#20869;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pre-trained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;DNN&#30340;&#23398;&#20064;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#22534;&#21472;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#38543;&#30528;&#32593;&#32476;&#23618;&#25968;&#22686;&#21152;&#32780;&#23548;&#33268;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#22686;&#21152;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07401</link><description>&lt;p&gt;
&#22810;&#32423;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;Burgers&#26041;&#31243;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Multi-Grade Deep Learning for Partial Differential Equations with Applications to the Burgers Equation. (arXiv:2309.07401v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;DNN&#30340;&#23398;&#20064;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#22534;&#21472;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#38543;&#30528;&#32593;&#32476;&#23618;&#25968;&#22686;&#21152;&#32780;&#23548;&#33268;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#22686;&#21152;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;PDEs&#26041;&#38754;&#34920;&#29616;&#20986;&#36229;&#24378;&#30340;&#24615;&#33021;&#65292;&#38500;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#30340;&#21331;&#36234;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#38750;&#24120;&#28145;&#30340;&#32593;&#32476;&#24448;&#24448;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;DNN&#30340;&#23618;&#25968;&#22686;&#21152;&#65292;&#35299;&#20915;&#30001;PDEs&#30340;DNN&#27714;&#35299;&#32467;&#26524;&#23548;&#33268;&#30340;&#22823;&#35268;&#27169;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#38477;&#20302;&#32780;&#19981;&#26159;&#22686;&#21152;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#22810;&#32423;&#28145;&#24230;&#23398;&#20064;&#65288;TS-MGDL&#65289;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;DNN&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#22534;&#21472;&#22312;&#24444;&#27492;&#19978;&#26041;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#35299;&#20915;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#23398;&#20064;&#27531;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop in this paper a multi-grade deep learning method for solving nonlinear partial differential equations (PDEs). Deep neural networks (DNNs) have received super performance in solving PDEs in addition to their outstanding success in areas such as natural language processing, computer vision, and robotics. However, training a very deep network is often a challenging task. As the number of layers of a DNN increases, solving a large-scale non-convex optimization problem that results in the DNN solution of PDEs becomes more and more difficult, which may lead to a decrease rather than an increase in predictive accuracy. To overcome this challenge, we propose a two-stage multi-grade deep learning (TS-MGDL) method that breaks down the task of learning a DNN into several neural networks stacked on top of each other in a staircase-like manner. This approach allows us to mitigate the complexity of solving the non-convex optimization problem with large number of parameters and learn resid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#35821;&#20041;&#23545;&#25239;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#26377;&#20004;&#20010;&#21464;&#31181;&#26041;&#27861;&#65306;&#35821;&#20041;&#36716;&#25442;&#26041;&#27861;(ST)&#65292;&#36890;&#36807;&#24494;&#35843;&#28508;&#22312;&#31354;&#38388;&#21644;/&#25110;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#26469;&#29983;&#25104;&#22270;&#20687;&#65307;&#28508;&#22312;&#23631;&#34109;&#26041;&#27861;(LM)&#65292;&#21033;&#29992;&#21478;&#19968;&#30446;&#26631;&#22270;&#20687;&#23631;&#34109;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#23616;&#37096;&#21453;&#21521;&#20256;&#25773;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07398</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35821;&#20041;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#35821;&#20041;&#23545;&#25239;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#26377;&#20004;&#20010;&#21464;&#31181;&#26041;&#27861;&#65306;&#35821;&#20041;&#36716;&#25442;&#26041;&#27861;(ST)&#65292;&#36890;&#36807;&#24494;&#35843;&#28508;&#22312;&#31354;&#38388;&#21644;/&#25110;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#26469;&#29983;&#25104;&#22270;&#20687;&#65307;&#28508;&#22312;&#23631;&#34109;&#26041;&#27861;(LM)&#65292;&#21033;&#29992;&#21478;&#19968;&#30446;&#26631;&#22270;&#20687;&#23631;&#34109;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#23616;&#37096;&#21453;&#21521;&#20256;&#25773;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#20027;&#35201;&#36890;&#36807;&#22312;&#20687;&#32032;&#31354;&#38388;&#20013;&#28155;&#21152;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#25805;&#32437;&#24178;&#20928;&#30340;&#26679;&#26412;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35821;&#20041;&#23545;&#25239;&#25915;&#20987;&#26356;&#21152;&#20851;&#27880;&#25913;&#21464;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#20363;&#22914;&#39068;&#33394;&#12289;&#19978;&#19979;&#25991;&#21644;&#29305;&#24449;&#65292;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#26356;&#20855;&#21487;&#34892;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#35821;&#20041;&#23545;&#25239;&#25915;&#20987;&#65292;&#22240;&#20026;&#35821;&#20041;&#20449;&#24687;&#21253;&#21547;&#22312;&#35757;&#32451;&#26377;&#32032;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#65292;&#35813;&#26694;&#26550;&#26377;&#20004;&#20010;&#21464;&#31181;&#65306;1) &#35821;&#20041;&#36716;&#25442;&#26041;&#27861;(ST)&#36890;&#36807;&#24494;&#35843;&#29983;&#25104;&#22270;&#20687;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;/&#25110;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#65307;2) &#28508;&#22312;&#23631;&#34109;&#26041;&#27861;(LM)&#20351;&#29992;&#21478;&#19968;&#30446;&#26631;&#22270;&#20687;&#23631;&#34109;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#23616;&#37096;&#21453;&#21521;&#20256;&#25773;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;ST&#26041;&#27861;&#21487;&#22312;&#30333;&#30418;&#25110;&#40657;&#30418;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;&#22312;CelebA-HQ&#21644;AFHQ&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional adversarial attacks concentrate on manipulating clean examples in the pixel space by adding adversarial perturbations. By contrast, semantic adversarial attacks focus on changing semantic attributes of clean examples, such as color, context, and features, which are more feasible in the real world. In this paper, we propose a framework to quickly generate a semantic adversarial attack by leveraging recent diffusion models since semantic information is included in the latent space of well-trained diffusion models. Then there are two variants of this framework: 1) the Semantic Transformation (ST) approach fine-tunes the latent space of the generated image and/or the diffusion model itself; 2) the Latent Masking (LM) approach masks the latent space with another target image and local backpropagation-based interpretation methods. Additionally, the ST approach can be applied in either white-box or black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#26080;&#30417;&#30563;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#65292;&#24182;&#20174;&#21435;&#20559;&#35265;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;DebCSE&#26041;&#27861;&#12290;&#36890;&#36807;&#28040;&#38500;&#21508;&#31181;&#20559;&#24046;&#65292;&#21253;&#25324;&#35789;&#39057;&#20559;&#24046;&#12289;&#21477;&#23376;&#38271;&#24230;&#20559;&#24046;&#21644;&#20551;&#36127;&#26679;&#26412;&#20559;&#24046;&#65292;DebCSE&#26088;&#22312;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2309.07396</link><description>&lt;p&gt;
DebCSE: &#20197;&#21435;&#20559;&#35265;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#26080;&#30417;&#30563;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in the Debiasing Perspective. (arXiv:2309.07396v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#26080;&#30417;&#30563;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#65292;&#24182;&#20174;&#21435;&#20559;&#35265;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;DebCSE&#26041;&#27861;&#12290;&#36890;&#36807;&#28040;&#38500;&#21508;&#31181;&#20559;&#24046;&#65292;&#21253;&#25324;&#35789;&#39057;&#20559;&#24046;&#12289;&#21477;&#23376;&#38271;&#24230;&#20559;&#24046;&#21644;&#20551;&#36127;&#26679;&#26412;&#20559;&#24046;&#65292;DebCSE&#26088;&#22312;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#35789;&#39057;&#20559;&#24046;&#20250;&#23548;&#33268;Bert&#27169;&#22411;&#23398;&#20064;&#21040;&#26080;&#27861;&#21306;&#20998;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#19968;&#20123;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#65292;&#22914;SimCSE&#21644;ConSERT&#65292;&#24050;&#25104;&#21151;&#29992;&#20110;&#25913;&#21892;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#65292;&#20943;&#23569;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#20250;&#24341;&#20837;&#26032;&#30340;&#20559;&#24046;&#65292;&#22914;&#21477;&#23376;&#38271;&#24230;&#20559;&#24046;&#21644;&#20551;&#36127;&#26679;&#26412;&#20559;&#24046;&#65292;&#36825;&#20123;&#20559;&#24046;&#38459;&#30861;&#20102;&#27169;&#22411;&#23398;&#20064;&#26356;&#31934;&#32454;&#30340;&#35821;&#20041;&#12290;&#26412;&#25991;&#20174;&#21435;&#20559;&#35265;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#24182;&#35748;&#20026;&#26377;&#25928;&#28040;&#38500;&#21508;&#31181;&#20559;&#24046;&#23545;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#21477;&#23376;&#23884;&#20837;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25152;&#26377;&#36825;&#20123;&#20559;&#24046;&#37117;&#26159;&#30001;&#20110;&#23545;&#27604;&#23398;&#20064;&#20013;&#26500;&#24314;&#35757;&#32451;&#25968;&#25454;&#30340;&#31616;&#21333;&#35268;&#21017;&#24341;&#20837;&#30340;&#65292;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#20851;&#38190;&#26159;&#22312;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several prior studies have suggested that word frequency biases can cause the Bert model to learn indistinguishable sentence embeddings. Contrastive learning schemes such as SimCSE and ConSERT have already been adopted successfully in unsupervised sentence embedding to improve the quality of embeddings by reducing this bias. However, these methods still introduce new biases such as sentence length bias and false negative sample bias, that hinders model's ability to learn more fine-grained semantics. In this paper, we reexamine the challenges of contrastive sentence embedding learning from a debiasing perspective and argue that effectively eliminating the influence of various biases is crucial for learning high-quality sentence embeddings. We think all those biases are introduced by simple rules for constructing training data in contrastive learning and the key for contrastive learning sentence embedding is to mimic the distribution of training data in supervised machine learning in uns
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;&#22270;&#20687;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20860;&#23481;&#24615;&#65292;&#24341;&#20837;&#20102;Mask Image Modelling (MIM)&#27169;&#22359;&#65292;&#20174;&#32780;&#33021;&#22815;&#20805;&#20998;&#21457;&#25381;&#24403;&#21069;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#31397;&#38236;&#23548;&#33322;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07390</link><description>&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#20860;&#23481;&#30340;&#20869;&#31397;&#38236;&#22270;&#20687;&#26469;&#37322;&#25918;&#28145;&#24230;&#21644;&#23039;&#24577;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Depth and Pose Estimation Neural Networks by Designing Compatible Endoscopic Images. (arXiv:2309.07390v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;&#22270;&#20687;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20860;&#23481;&#24615;&#65292;&#24341;&#20837;&#20102;Mask Image Modelling (MIM)&#27169;&#22359;&#65292;&#20174;&#32780;&#33021;&#22815;&#20805;&#20998;&#21457;&#25381;&#24403;&#21069;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#31397;&#38236;&#23548;&#33322;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28145;&#24230;&#21644;&#23039;&#24577;&#20272;&#35745;&#24050;&#25104;&#20026;&#25104;&#21151;&#36827;&#34892;&#20869;&#31397;&#38236;&#23548;&#33322;&#30340;&#26377;&#25928;&#36884;&#24452;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#25216;&#26415;&#33268;&#21147;&#20110;&#24320;&#21457;&#26356;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#20869;&#31397;&#38236;&#22270;&#20687;&#30340;&#29305;&#27530;&#23646;&#24615;&#65292;&#23548;&#33268;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20869;&#31397;&#38236;&#22270;&#20687;&#30340;&#23646;&#24615;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25913;&#21892;&#22270;&#20687;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20860;&#23481;&#24615;&#65292;&#20197;&#37322;&#25918;&#24403;&#21069;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Mask Image Modelling (MIM)&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#36755;&#20837;&#37096;&#20998;&#22270;&#20687;&#20449;&#24687;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#22270;&#20687;&#20449;&#24687;&#65292;&#20801;&#35768;&#32593;&#32476;&#20174;&#37096;&#20998;&#20687;&#32032;&#20449;&#24687;&#20013;&#24674;&#22797;&#20840;&#23616;&#20449;&#24687;&#12290;&#36825;&#22686;&#24378;&#20102;&#32593;&#32476;&#24863;&#30693;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#20943;&#36731;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#23616;&#37096;&#20266;&#24433;&#32780;&#20135;&#29983;&#30340;&#23616;&#37096;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have witnessed depth and pose estimation framework on unannotated datasets as a effective pathway to succeed in endoscopic navigation. Most current techniques are dedicated to developing more advanced neural networks to improve the accuracy. However, existing methods ignore the special properties of endoscopic images, resulting in an inability to fully unleash the power of neural networks. In this study, we conduct a detail analysis of the properties of endoscopic images and improve the compatibility of images and neural networks, to unleash the power of current neural networks. First, we introcude the Mask Image Modelling (MIM) module, which inputs partial image information instead of complete image information, allowing the network to recover global information from partial pixel information. This enhances the network' s ability to perceive global information and alleviates the phenomenon of local overfitting in convolutional neural networks due to local artifact
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24179;&#34913;&#26041;&#31243;&#65292;&#35299;&#37322;&#20102;&#25968;&#25454;&#38598;&#20998;&#24067;&#20272;&#35745;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#23610;&#24230;&#26426;&#21046;&#12290;&#32593;&#32476;&#30340;&#36755;&#20986;&#26159;&#25968;&#25454;&#38598;&#30340;&#23616;&#37096;&#24179;&#22343;&#65292;&#24179;&#22343;&#30340;&#23610;&#24230;&#38543;&#30528;&#35757;&#32451;&#36880;&#28176;&#20943;&#23567;&#24182;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07367</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24179;&#34913;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
The kernel-balanced equation for deep neural networks. (arXiv:2309.07367v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24179;&#34913;&#26041;&#31243;&#65292;&#35299;&#37322;&#20102;&#25968;&#25454;&#38598;&#20998;&#24067;&#20272;&#35745;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#23610;&#24230;&#26426;&#21046;&#12290;&#32593;&#32476;&#30340;&#36755;&#20986;&#26159;&#25968;&#25454;&#38598;&#30340;&#23616;&#37096;&#24179;&#22343;&#65292;&#24179;&#22343;&#30340;&#23610;&#24230;&#38543;&#30528;&#35757;&#32451;&#36880;&#28176;&#20943;&#23567;&#24182;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#23637;&#29616;&#20986;&#35768;&#22810;&#26377;&#30410;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#26377;&#38480;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#32593;&#32476;&#21487;&#20197;&#33719;&#24471;&#24191;&#20041;&#20989;&#25968;&#12290;&#24191;&#20041;&#21270;&#31243;&#24230;&#26159;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#36817;&#20284;&#23610;&#24230;&#30340;&#23454;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#25968;&#25454;&#38598;&#22797;&#26434;&#26102;&#65292;&#23610;&#24230;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#38598;&#20998;&#24067;&#20272;&#35745;&#30340;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20272;&#35745;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#19981;&#31283;&#23450;&#24615;&#21462;&#20915;&#20110;&#25968;&#25454;&#23494;&#24230;&#21644;&#35757;&#32451;&#25345;&#32493;&#26102;&#38388;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26680;&#24179;&#34913;&#26041;&#31243;&#65292;&#23427;&#32473;&#20986;&#20102;&#35299;&#30340;&#31616;&#30701;&#29616;&#35937;&#23398;&#25551;&#36848;&#12290;&#35813;&#26041;&#31243;&#21578;&#35785;&#25105;&#20204;&#19981;&#31283;&#23450;&#24615;&#30340;&#21407;&#22240;&#21644;&#23610;&#24230;&#30340;&#26426;&#21046;&#12290;&#32593;&#32476;&#30340;&#36755;&#20986;&#20316;&#20026;&#39044;&#27979;&#26159;&#25968;&#25454;&#38598;&#30340;&#23616;&#37096;&#24179;&#22343;&#65292;&#24179;&#22343;&#30340;&#23610;&#24230;&#27839;&#30528;&#26041;&#31243;&#30830;&#23450;&#12290;&#23610;&#24230;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#65292;&#26368;&#32456;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have shown many fruitful applications in this decade. A network can get the generalized function through training with a finite dataset. The degree of generalization is a realization of the proximity scale in the data space. Specifically, the scale is not clear if the dataset is complicated. Here we consider a network for the distribution estimation of the dataset. We show the estimation is unstable and the instability depends on the data density and training duration. We derive the kernel-balanced equation, which gives a short phenomenological description of the solution. The equation tells us the reason for the instability and the mechanism of the scale. The network outputs a local average of the dataset as a prediction and the scale of averaging is determined along the equation. The scale gradually decreases along training and finally results in instability in our case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35946;&#22855;&#20998;&#35299;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21333;&#32431;&#22797;&#21512;&#20307;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#12290;&#36890;&#36807;&#32534;&#30721;&#25968;&#25454;&#19981;&#21464;&#24615;&#21644;&#35774;&#35745;&#21512;&#36866;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#37325;&#26032;&#26435;&#34913;&#36127;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21453;&#26144;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.07364</link><description>&lt;p&gt;
Hodge-Aware Contrastive Learning&#65288;&#22522;&#20110;&#35946;&#22855;&#20998;&#35299;&#30340;&#23545;&#27604;&#23398;&#20064;&#65289;
&lt;/p&gt;
&lt;p&gt;
Hodge-Aware Contrastive Learning. (arXiv:2309.07364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35946;&#22855;&#20998;&#35299;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21333;&#32431;&#22797;&#21512;&#20307;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#12290;&#36890;&#36807;&#32534;&#30721;&#25968;&#25454;&#19981;&#21464;&#24615;&#21644;&#35774;&#35745;&#21512;&#36866;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#37325;&#26032;&#26435;&#34913;&#36127;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21453;&#26144;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32431;&#22797;&#21512;&#20307;&#22312;&#23545;&#20855;&#26377;&#22810;&#21521;&#20381;&#36182;&#20851;&#31995;&#30340;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20363;&#22914;&#22312;&#32593;&#32476;&#30340;&#36793;&#32536;&#19978;&#23450;&#20041;&#30340;&#25968;&#25454;&#25110;&#20854;&#20182;&#39640;&#38454;&#32467;&#26500;&#20869;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#35946;&#22855;&#20998;&#35299;&#65292;&#21487;&#20197;&#23558;&#20854;&#35889;&#20998;&#35299;&#20026;&#19977;&#20010;&#21487;&#35299;&#37322;&#30340;&#23376;&#31354;&#38388;&#65292;&#36825;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#20855;&#26377;&#22522;&#30784;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#20998;&#35299;&#26469;&#24320;&#21457;&#19968;&#31181;&#23545;&#27604;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21333;&#32431;&#22797;&#21512;&#20307;&#25968;&#25454;&#24182;&#29983;&#25104;&#34164;&#21547;&#29305;&#23450;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#21333;&#32431;&#31070;&#32463;&#32593;&#32476;&#26469;&#32534;&#30721;&#30456;&#20851;&#25968;&#25454;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#20855;&#26377;&#36866;&#24403;&#35889;&#29305;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#27491;&#23545;&#27604;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#36127;&#26679;&#26412;&#30340;&#35946;&#22855;&#20998;&#37327;&#19982;&#38170;&#28857;&#30456;&#20284;&#24615;&#65292;&#37325;&#26032;&#26435;&#34913;&#23545;&#27604;&#25439;&#22833;&#20013;&#36127;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21152;&#24378;&#36739;&#23569;&#30456;&#20284;&#23454;&#20363;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#21453;&#26144;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicial complexes prove effective in modeling data with multiway dependencies, such as data defined along the edges of networks or within other higher-order structures. Their spectrum can be decomposed into three interpretable subspaces via the Hodge decomposition, resulting foundational in numerous applications. We leverage this decomposition to develop a contrastive self-supervised learning approach for processing simplicial data and generating embeddings that encapsulate specific spectral information.Specifically, we encode the pertinent data invariances through simplicial neural networks and devise augmentations that yield positive contrastive examples with suitable spectral properties for downstream tasks. Additionally, we reweight the significance of negative examples in the contrastive loss, considering the similarity of their Hodge components to the anchor. By encouraging a stronger separation among less similar instances, we obtain an embedding space that reflects the spect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21033;&#29992;&#22522;&#20110;&#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20648;&#27700;&#27744;&#30340;QRL&#20195;&#29702;&#65292;&#35299;&#20915;&#20102;QRL&#19982;QRNN&#30340;&#20302;&#25928;&#35757;&#32451;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07339</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#37327;&#23376;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#37327;&#23376;&#20648;&#27700;&#27744;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Efficient quantum recurrent reinforcement learning via quantum reservoir computing. (arXiv:2309.07339v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21033;&#29992;&#22522;&#20110;&#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20648;&#27700;&#27744;&#30340;QRL&#20195;&#29702;&#65292;&#35299;&#20915;&#20102;QRL&#19982;QRNN&#30340;&#20302;&#25928;&#35757;&#32451;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#37327;&#23376;&#20248;&#21183;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#23637;&#26159;&#36890;&#36807;&#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QRNN&#65289;&#26469;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#31561;&#20869;&#23384;&#23494;&#38598;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;QRL&#27169;&#22411;&#32467;&#21512;QRNN&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21253;&#25324;QRL&#19982;QRNN&#30340;&#20302;&#25928;&#35757;&#32451;&#65292;&#22240;&#20026;QRNN&#20013;&#30340;&#26799;&#24230;&#35745;&#31639;&#26082;&#32791;&#36153;&#35745;&#31639;&#36164;&#28304;&#21448;&#32791;&#26102;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#21033;&#29992;&#22522;&#20110;QRNN&#30340;&#20648;&#27700;&#27744;&#30340;QRL&#20195;&#29702;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20855;&#20307;&#37319;&#29992;&#37327;&#23376;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;QLSTM&#65289;&#12290;QLSTM&#21442;&#25968;&#26159;&#38543;&#26426;&#21021;&#22987;&#21270;&#24182;&#22266;&#23450;&#19981;&#21464;&#30340;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#24322;&#27493;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;A3C&#65289;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;QLSTM-Reservoir RL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#20854;&#24615;&#33021;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum reinforcement learning (QRL) has emerged as a framework to solve sequential decision-making tasks, showcasing empirical quantum advantages. A notable development is through quantum recurrent neural networks (QRNNs) for memory-intensive tasks such as partially observable environments. However, QRL models incorporating QRNN encounter challenges such as inefficient training of QRL with QRNN, given that the computation of gradients in QRNN is both computationally expensive and time-consuming. This work presents a novel approach to address this challenge by constructing QRL agents utilizing QRNN-based reservoirs, specifically employing quantum long short-term memory (QLSTM). QLSTM parameters are randomly initialized and fixed without training. The model is trained using the asynchronous advantage actor-aritic (A3C) algorithm. Through numerical simulations, we validate the efficacy of our QLSTM-Reservoir RL framework. Its performance is assessed on standard benchmarks, demonstrating 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#36777;&#35770;&#20889;&#20316;&#20013;&#20998;&#31867;&#29702;&#24819;&#30340;&#25512;&#29702;&#20462;&#25913;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20004;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#36741;&#21161;&#20462;&#35746;&#25968;&#25454;&#30340;&#26469;&#28304;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.07334</link><description>&lt;p&gt;
&#20174;&#36741;&#21161;&#26469;&#28304;&#20013;&#23398;&#20064;&#22312;&#36777;&#35770;&#20462;&#35746;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning from Auxiliary Sources in Argumentative Revision Classification. (arXiv:2309.07334v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#36777;&#35770;&#20889;&#20316;&#20013;&#20998;&#31867;&#29702;&#24819;&#30340;&#25512;&#29702;&#20462;&#25913;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20004;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#36741;&#21161;&#20462;&#35746;&#25968;&#25454;&#30340;&#26469;&#28304;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#27169;&#22411;&#26469;&#20998;&#31867;&#36777;&#35770;&#20889;&#20316;&#20013;&#24076;&#26395;&#25913;&#36827;&#30340;&#25512;&#29702;&#20462;&#35746;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#26041;&#27861; - &#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064; - &#21033;&#29992;&#31867;&#20284;&#20219;&#21153;&#30340;&#20462;&#35746;&#25968;&#25454;&#30340;&#36741;&#21161;&#26469;&#28304;&#12290;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#30830;&#23454;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#36229;&#36807;&#22522;&#32447;&#12290;&#23613;&#31649;&#22810;&#20219;&#21153;&#23398;&#20064;&#26174;&#31034;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#26469;&#28304;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#25552;&#39640;&#24615;&#33021;&#65292;&#36801;&#31227;&#23398;&#20064;&#26356;&#22909;&#22320;&#34920;&#31034;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop models to classify desirable reasoning revisions in argumentative writing. We explore two approaches -- multi-task learning and transfer learning -- to take advantage of auxiliary sources of revision data for similar tasks. Results of intrinsic and extrinsic evaluations show that both approaches can indeed improve classifier performance over baselines. While multi-task learning shows that training on different sources of data at the same time may improve performance, transfer-learning better represents the relationship between the data.
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#38752;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#65292;&#21033;&#29992;&#24402;&#32435;&#24615;&#31526;&#21512;&#39044;&#27979;&#26469;&#32416;&#27491;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#26631;&#35760;&#21644;&#24322;&#24120;&#20540;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#39564;&#35777;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07332</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#22522;&#20110;&#21487;&#38752;&#24615;&#30340;&#22122;&#22768;&#35757;&#32451;&#26631;&#31614;&#28165;&#27927;&#26041;&#27861;&#19982;&#24402;&#32435;&#24615;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining. (arXiv:2309.07332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07332
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#38752;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#65292;&#21033;&#29992;&#24402;&#32435;&#24615;&#31526;&#21512;&#39044;&#27979;&#26469;&#32416;&#27491;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#26631;&#35760;&#21644;&#24322;&#24120;&#20540;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#39564;&#35777;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#26631;&#35760;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21487;&#29992;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21487;&#38752;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#24402;&#32435;&#24615;&#31526;&#21512;&#39044;&#27979;&#65288;ICP&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#23567;&#37096;&#20998;&#20934;&#30830;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;ICP&#35745;&#31639;&#30340;&#21487;&#38752;&#24615;&#25351;&#26631;&#26469;&#32416;&#27491;&#28023;&#37327;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#26631;&#35760;&#21644;&#24322;&#24120;&#20540;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65306;&#20351;&#29992;&#26631;&#39064;&#21644;&#25688;&#35201;&#23545;&#33647;&#29289;&#35825;&#23548;&#30340;&#32925;&#25439;&#20260;&#65288;DILI&#65289;&#25991;&#29486;&#36827;&#34892;&#36807;&#28388;&#65292;&#36890;&#36807;CT&#24433;&#20687;&#23398;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#39044;&#27979;COVID-19&#24739;&#32773;&#30340;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20837;&#38498;&#24773;&#20917;&#65292;&#20197;&#21450;&#20351;&#29992;RNA&#27979;&#24207;&#25968;&#25454;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20122;&#22411;&#20998;&#22411;&#12290;&#36890;&#36807;&#26631;&#31614;&#25490;&#21015;&#24341;&#20837;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#35757;&#32451;&#26631;&#31614;&#22122;&#22768;&#12290;&#32467;&#26524;&#26174;&#31034;&#20998;&#31867;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65306;&#20934;&#30830;&#24230;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
Accurately labeling biomedical data presents a challenge. Traditional semi-supervised learning methods often under-utilize available unlabeled data. To address this, we propose a novel reliability-based training data cleaning method employing inductive conformal prediction (ICP). This method capitalizes on a small set of accurately labeled training data and leverages ICP-calculated reliability metrics to rectify mislabeled data and outliers within vast quantities of noisy training data. The efficacy of the method is validated across three classification tasks within distinct modalities: filtering drug-induced-liver-injury (DILI) literature with title and abstract, predicting ICU admission of COVID-19 patients through CT radiomics and electronic health records, and subtyping breast cancer using RNA-sequencing data. Varying levels of noise to the training labels were introduced through label permutation. Results show significant enhancements in classification performance: accuracy enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.07315</link><description>&lt;p&gt;
&#26053;&#34892;&#35789;&#65306;&#19968;&#31181;&#21464;&#21387;&#22120;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29702;&#35299;&#20854;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35828;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#23558;&#28508;&#22312;&#29305;&#24449;&#38480;&#21046;&#22312;&#19968;&#20010;&#36229;&#29699;&#38754;&#19978;&#65292;&#20174;&#32780;&#20351;&#27880;&#24847;&#21147;&#33021;&#22815;&#22312;&#35813;&#34920;&#38754;&#19978;&#22609;&#36896;&#21333;&#35789;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#36825;&#31181;&#20960;&#20309;&#35270;&#28857;&#26080;&#32541;&#22320;&#36830;&#25509;&#20102;&#36845;&#20195;&#25913;&#36827;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#31561;&#24050;&#30693;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#27979;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;124M&#21442;&#25968;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#26089;&#26399;&#23618;&#20013;&#28165;&#26224;&#30340;&#26597;&#35810;-&#38190;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#22312;&#26356;&#28145;&#30340;&#23618;&#27425;&#19978;&#24314;&#31435;&#22312;&#20808;&#21069;&#20851;&#20110;&#27880;&#24847;&#22836;&#30340;&#19987;&#38376;&#24615;&#30340;&#35266;&#23519;&#22522;&#30784;&#19978;&#12290;&#21033;&#29992;&#36825;&#20123;&#20960;&#20309;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#65292;&#23558;&#20854;&#25551;&#32472;&#20026;&#22609;&#36896;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;AudioSR&#65292;&#21487;&#22312;&#21508;&#31181;&#38899;&#39057;&#31867;&#22411;&#19978;&#36827;&#34892;&#40065;&#26834;&#30340;&#38899;&#39057;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#21487;&#20197;&#23558;2kHz&#21040;16kHz&#33539;&#22260;&#20869;&#30340;&#38899;&#39057;&#20449;&#21495;&#19978;&#37319;&#26679;&#20026;24kHz&#24102;&#23485;&#30340;&#39640;&#20998;&#36776;&#29575;&#38899;&#39057;&#20449;&#21495;&#65292;&#37319;&#26679;&#29575;&#20026;48kHz&#12290;</title><link>http://arxiv.org/abs/2309.07314</link><description>&lt;p&gt;
AudioSR: &#22823;&#35268;&#27169;&#24773;&#22659;&#19979;&#30340;&#22810;&#21151;&#33021;&#38899;&#39057;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
AudioSR: Versatile Audio Super-resolution at Scale. (arXiv:2309.07314v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;AudioSR&#65292;&#21487;&#22312;&#21508;&#31181;&#38899;&#39057;&#31867;&#22411;&#19978;&#36827;&#34892;&#40065;&#26834;&#30340;&#38899;&#39057;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#21487;&#20197;&#23558;2kHz&#21040;16kHz&#33539;&#22260;&#20869;&#30340;&#38899;&#39057;&#20449;&#21495;&#19978;&#37319;&#26679;&#20026;24kHz&#24102;&#23485;&#30340;&#39640;&#20998;&#36776;&#29575;&#38899;&#39057;&#20449;&#21495;&#65292;&#37319;&#26679;&#29575;&#20026;48kHz&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#36229;&#20998;&#36776;&#29575;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#29992;&#20110;&#39044;&#27979;&#20302;&#20998;&#36776;&#29575;&#38899;&#39057;&#30340;&#39640;&#39057;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#23383;&#24212;&#29992;&#20013;&#30340;&#38899;&#39057;&#36136;&#37327;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#38899;&#39057;&#65288;&#20363;&#22914;&#38899;&#20048;&#12289;&#35821;&#38899;&#65289;&#21644;&#29305;&#23450;&#30340;&#39057;&#24102;&#35774;&#32622;&#65288;&#20363;&#22914;4kHz&#21040;8kHz&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;AudioSR&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#38899;&#39057;&#31867;&#22411;&#19978;&#36827;&#34892;&#40065;&#26834;&#30340;&#38899;&#39057;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#65292;&#21253;&#25324;&#38899;&#25928;&#12289;&#38899;&#20048;&#21644;&#35821;&#38899;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AudioSR&#21487;&#20197;&#23558;2kHz&#21040;16kHz&#33539;&#22260;&#20869;&#30340;&#20219;&#20309;&#36755;&#20837;&#38899;&#39057;&#20449;&#21495;&#19978;&#37319;&#26679;&#20026;24kHz&#24102;&#23485;&#30340;&#39640;&#20998;&#36776;&#29575;&#38899;&#39057;&#20449;&#21495;&#65292;&#37319;&#26679;&#29575;&#20026;48kHz&#12290;&#23545;&#21508;&#31181;&#38899;&#39057;&#36229;&#20998;&#36776;&#29575;&#22522;&#20934;&#30340;&#24191;&#27867;&#23458;&#35266;&#35780;&#20272;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#25152;&#21462;&#24471;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20027;&#35266;&#35780;&#20272;&#34920;&#26126;&#65292;AudioSR&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#65292;&#25552;&#39640;&#24191;&#27867;&#33539;&#22260;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio super-resolution is a fundamental task that predicts high-frequency components for low-resolution audio, enhancing audio quality in digital applications. Previous methods have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4kHz to 8kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2kHz to 16kHz to a high-resolution audio signal at 24kHz bandwidth with a sampling rate of 48kHz. Extensive objective evaluation on various audio super-resolution benchmarks demonstrates the strong result achieved by the proposed model. In addition, our subjective evaluation shows that AudioSR can acts as a plug-and-play module to enhance the generation quality of a wide range of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#35266;&#27979;&#27169;&#22411;&#65288;LCOM&#65289;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#25628;&#32034;&#38382;&#39064;&#35270;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#65292;&#21033;&#29992;&#22522;&#20110;&#22797;&#26434;&#35821;&#35328;&#25551;&#36848;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#21160;&#24577;&#29983;&#25104;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#20013;&#38656;&#35201;&#20026;&#27599;&#20010;&#23545;&#35937;&#21046;&#20316;&#26032;&#26816;&#27979;&#22120;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07276</link><description>&lt;p&gt;
&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#30446;&#26631;&#25628;&#32034;&#35266;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language-Conditioned Observation Models for Visual Object Search. (arXiv:2309.07276v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#35266;&#27979;&#27169;&#22411;&#65288;LCOM&#65289;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#25628;&#32034;&#38382;&#39064;&#35270;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#65292;&#21033;&#29992;&#22522;&#20110;&#22797;&#26434;&#35821;&#35328;&#25551;&#36848;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#21160;&#24577;&#29983;&#25104;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#20013;&#38656;&#35201;&#20026;&#27599;&#20010;&#23545;&#35937;&#21046;&#20316;&#26032;&#26816;&#27979;&#22120;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#25628;&#32034;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#24403;&#32473;&#23450;&#22797;&#26434;&#30340;&#35821;&#35328;&#25551;&#36848;&#26102;&#65288;&#20363;&#22914;&#8220;&#22312;&#26700;&#23376;&#19978;&#25214;&#21040;&#30333;&#33394;&#30340;&#26479;&#23376;&#8221;&#65289;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#36890;&#36807;&#29615;&#22659;&#31227;&#21160;&#25668;&#20687;&#22836;&#24182;&#35782;&#21035;&#25551;&#36848;&#30340;&#23545;&#35937;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#35821;&#35328;&#25551;&#36848;&#26144;&#23556;&#21040;&#19968;&#32452;&#22266;&#23450;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#24182;&#37319;&#29992;&#39044;&#23450;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#65292;&#22240;&#20026;&#27599;&#20010;&#23545;&#35937;&#37117;&#38656;&#35201;&#21046;&#20316;&#26032;&#30340;&#26816;&#27979;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#29616;&#23454;&#20013;&#30340;&#30446;&#26631;&#25628;&#32034;&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#26469;&#35299;&#20915;&#12290;&#20854;&#20013;&#65292;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#35266;&#27979;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20256;&#24863;&#22120;&#22122;&#22768;&#30001;&#19968;&#20010;&#22522;&#20110;&#22797;&#26434;&#35821;&#35328;&#25551;&#36848;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30830;&#23450;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#32435;&#20837;&#21040;&#25105;&#20204;&#30340;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#35266;&#27979;&#27169;&#22411;&#65288;LCOM&#65289;&#20013;&#65292;&#20197;&#34920;&#31034;&#21160;&#24577;&#21464;&#21270;&#30340;&#20256;&#24863;&#22120;&#22122;&#22768;&#12290;&#36890;&#36807;LCOM&#65292;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#19968;&#20010;&#29289;&#20307;&#30340;&#35821;&#35328;&#25551;&#36848;&#26469;&#29983;&#25104;&#30456;&#24212;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object search is a challenging task because when given complex language descriptions (e.g., "find the white cup on the table"), the robot must move its camera through the environment and recognize the described object. Previous works map language descriptions to a set of fixed object detectors with predetermined noise models, but these approaches are challenging to scale because new detectors need to be made for each object. In this work, we bridge the gap in realistic object search by posing the search problem as a partially observable Markov decision process (POMDP) where the object detector and visual sensor noise in the observation model is determined by a single Deep Neural Network conditioned on complex language descriptions. We incorporate the neural network's outputs into our language-conditioned observation model (LCOM) to represent dynamically changing sensor noise. With an LCOM, any language description of an object can be used to generate an appropriate object detector and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;O-RAN&#32593;&#32476;&#20013;&#20351;&#29992;DRL&#31639;&#27861;&#36827;&#34892;&#38381;&#29615;&#25511;&#21046;&#26102;&#36935;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07265</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#21152;&#36895;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;O-RAN&#20999;&#29255;: &#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;O-RAN&#32593;&#32476;&#20013;&#20351;&#29992;DRL&#31639;&#27861;&#36827;&#34892;&#38381;&#29615;&#25511;&#21046;&#26102;&#36935;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#26550;&#26500;&#25903;&#25345;&#26234;&#33021;&#32593;&#32476;&#25511;&#21046;&#31639;&#27861;&#20316;&#20026;&#20854;&#26680;&#24515;&#33021;&#21147;&#20043;&#19968;&#12290;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36890;&#36807;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#26234;&#33021;&#25511;&#21046;&#22120;&#65288;RIC&#65289;&#26469;&#20248;&#21270;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;RAN&#65289;&#21151;&#33021;&#12290;&#22312;O-RAN&#25991;&#29486;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26159;&#35299;&#20915;&#21160;&#24577;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;O-RAN RIC&#24341;&#20837;&#20102;&#35832;&#22810;&#22909;&#22788;&#65292;&#20294;&#22312;&#30495;&#23454;&#32593;&#32476;&#37096;&#32626;&#20013;&#65292;DRL&#31639;&#27861;&#30340;&#23454;&#38469;&#37319;&#29992;&#21364;&#33853;&#21518;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;DRL&#20195;&#29702;&#22312;&#37096;&#32626;&#21644;&#38754;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#32593;&#32476;&#26465;&#20214;&#26102;&#25910;&#25947;&#36895;&#24230;&#24930;&#12289;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#20316;&#20026;O-RAN&#21151;&#33021;&#30340;DRL&#22522;&#20110;&#38381;&#29615;&#25511;&#21046;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#27969;&#31243;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;TL&#36741;&#21161;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The open radio access network (O-RAN) architecture supports intelligent network control algorithms as one of its core capabilities. Data-driven applications incorporate such algorithms to optimize radio access network (RAN) functions via RAN intelligent controllers (RICs). Deep reinforcement learning (DRL) algorithms are among the main approaches adopted in the O-RAN literature to solve dynamic radio resource management problems. However, despite the benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms in real network deployments falls behind. This is primarily due to the slow convergence and unstable performance exhibited by DRL agents upon deployment and when facing previously unseen network conditions. In this paper, we address these challenges by proposing transfer learning (TL) as a core component of the training and deployment workflows for the DRL-based closed-loop control of O-RAN functionalities. To this end, we propose and design a hybrid TL-aided a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;TVM&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#65292;&#20351;&#29992;TVM&#24352;&#37327;&#34920;&#36798;&#35821;&#35328;&#23454;&#29616;&#20102;LU&#20998;&#35299;&#12289;Cholesky&#20998;&#35299;&#21644;3mm&#31561;&#32447;&#24615;&#20195;&#25968;&#26680;&#12290;&#22312;GPU&#38598;&#32676;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;AutoTVM&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.07235</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#33258;&#21160;&#35843;&#20248;&#22522;&#20110;Apache TVM&#30340;&#31185;&#23398;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization. (arXiv:2309.07235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;TVM&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#65292;&#20351;&#29992;TVM&#24352;&#37327;&#34920;&#36798;&#35821;&#35328;&#23454;&#29616;&#20102;LU&#20998;&#35299;&#12289;Cholesky&#20998;&#35299;&#21644;3mm&#31561;&#32447;&#24615;&#20195;&#25968;&#26680;&#12290;&#22312;GPU&#38598;&#32676;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;AutoTVM&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Apache TVM&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#32534;&#35793;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#21508;&#31181;&#30828;&#20214;&#24179;&#21488;&#19978;&#30340;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26032;&#22411;TVM&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;TVM&#24352;&#37327;&#34920;&#36798;&#35821;&#35328;&#23454;&#29616;&#20102;LU&#20998;&#35299;&#12289;Cholesky&#20998;&#35299;&#21644;3mm&#31561;&#32447;&#24615;&#20195;&#25968;&#26680;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#31185;&#23398;&#35745;&#31639;&#26680;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38463;&#36129;&#22269;&#23478;&#23454;&#39564;&#23460;&#30340;GPU&#38598;&#32676;"Swing"&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#19982;TVM&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;AutoTVM&#30340;&#22235;&#20010;&#35843;&#20248;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;AutoTVM&#12290;
&lt;/p&gt;
&lt;p&gt;
Apache TVM (Tensor Virtual Machine), an open source machine learning compiler framework designed to optimize computations across various hardware platforms, provides an opportunity to improve the performance of dense matrix factorizations such as LU (Lower Upper) decomposition and Cholesky decomposition on GPUs and AI (Artificial Intelligence) accelerators. In this paper, we propose a new TVM autotuning framework using Bayesian Optimization and use the TVM tensor expression language to implement linear algebra kernels such as LU, Cholesky, and 3mm. We use these scientific computation kernels to evaluate the effectiveness of our methods on a GPU cluster, called Swing, at Argonne National Laboratory. We compare the proposed autotuning framework with the TVM autotuning framework AutoTVM with four tuners and find that our framework outperforms AutoTVM in most cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07200</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#28508;&#22312;&#34920;&#31034;&#21644;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#26159;&#25551;&#36848;&#21508;&#20010;&#39046;&#22495;&#20013;&#21160;&#24577;&#31995;&#32479;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#20934;&#30830;&#31215;&#20998;&#30340;&#30701;&#26102;&#38388;&#27493;&#38271;&#65292;&#31934;&#30830;&#27169;&#25311;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#22823;&#35268;&#27169;&#31995;&#32479;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#21407;&#21017;&#30446;&#26631;-&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#65288;T-IB&#65289;&#65292;&#23427;&#26088;&#22312;&#25429;&#25417;&#30456;&#20851;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#21516;&#26102;&#20002;&#24323;&#39640;&#39057;&#20449;&#24687;&#20197;&#31616;&#21270;&#27169;&#25311;&#20219;&#21153;&#24182;&#26368;&#23567;&#21270;&#25512;&#29702;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;T-IB&#23398;&#20064;&#20102;&#20449;&#24687;&#26368;&#20248;&#30340;&#34920;&#31034;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#22312;&#36873;&#25321;&#30340;&#26102;&#38388;&#28382;&#21518;&#19979;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#24182;&#19988;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov processes are widely used mathematical models for describing dynamic systems in various fields. However, accurately simulating large-scale systems at long time scales is computationally expensive due to the short time steps required for accurate integration. In this paper, we introduce an inference process that maps complex systems into a simplified representational space and models large jumps in time. To achieve this, we propose Time-lagged Information Bottleneck (T-IB), a principled objective rooted in information theory, which aims to capture relevant temporal features while discarding high-frequency information to simplify the simulation task and minimize the inference error. Our experiments demonstrate that T-IB learns information-optimal representations for accurately modeling the statistical properties and dynamics of the original process at a selected time lag, outperforming existing time-lagged dimensionality reduction methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#39044;&#27979;&#28378;&#29664;&#36724;&#25215;&#30340;&#22833;&#25928;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#39057;&#22495;&#25968;&#25454;&#21644;&#35757;&#32451;&#22810;&#20010;&#29983;&#23384;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#27169;&#22411;&#21487;&#20197;&#20197;&#26102;&#38388;&#20026;&#22522;&#30784;&#36827;&#34892;&#39118;&#38505;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#24182;&#20801;&#35768;&#27604;&#36739;&#19981;&#21516;&#32452;&#36724;&#25215;&#30340;&#29983;&#23384;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.07188</link><description>&lt;p&gt;
&#22312;&#23384;&#22312;&#25130;&#23614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#28378;&#29664;&#36724;&#25215;&#30340;&#23551;&#21629;
&lt;/p&gt;
&lt;p&gt;
Predicting Survival Time of Ball Bearings in the Presence of Censoring. (arXiv:2309.07188v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#39044;&#27979;&#28378;&#29664;&#36724;&#25215;&#30340;&#22833;&#25928;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#39057;&#22495;&#25968;&#25454;&#21644;&#35757;&#32451;&#22810;&#20010;&#29983;&#23384;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#27169;&#22411;&#21487;&#20197;&#20197;&#26102;&#38388;&#20026;&#22522;&#30784;&#36827;&#34892;&#39118;&#38505;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#24182;&#20801;&#35768;&#27604;&#36739;&#19981;&#21516;&#32452;&#36724;&#25215;&#30340;&#29983;&#23384;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28378;&#29664;&#36724;&#25215;&#22312;&#21508;&#31181;&#21046;&#36896;&#21644;&#26426;&#26800;&#39046;&#22495;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#30417;&#27979;&#30952;&#25439;&#24182;&#22312;&#25925;&#38556;&#21457;&#29983;&#20043;&#21069;&#21457;&#29616;&#32570;&#38519;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#28041;&#21450;&#21040;&#25130;&#23614;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21363;&#26410;&#35266;&#23519;&#21040;&#25925;&#38556;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#39044;&#27979;&#28378;&#29664;&#36724;&#25215;&#30340;&#22833;&#25928;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#39057;&#22495;&#20013;&#20998;&#26512;&#36724;&#25215;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#24211;&#23572;&#24052;&#36203;-&#33713;&#24067;&#21202;&#25955;&#24230;&#21644;&#20854;&#30772;&#22351;&#39057;&#29575;&#21306;&#38388;&#19982;&#20854;&#30772;&#22351;&#39057;&#29575;&#21306;&#38388;&#20043;&#38388;&#30340;&#26631;&#20934;&#24046;&#26469;&#26631;&#27880;&#36724;&#25215;&#25925;&#38556;&#26102;&#21051;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22810;&#20010;&#29983;&#23384;&#27169;&#22411;&#65292;&#26681;&#25454;&#26631;&#27880;&#25968;&#25454;&#21644;&#20174;&#26102;&#22495;&#25552;&#21462;&#30340;&#21327;&#21464;&#37327;&#65288;&#22914;&#20559;&#24230;&#12289;&#23792;&#24230;&#21644;&#29109;&#65289;&#26469;&#20272;&#35745;&#22833;&#25928;&#26102;&#38388;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20197;&#26102;&#38388;&#20026;&#22522;&#30784;&#36827;&#34892;&#39118;&#38505;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#27604;&#36739;&#19981;&#21516;&#32452;&#30340;&#36724;&#25215;&#30340;&#29983;&#23384;&#20989;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ball bearings find widespread use in various manufacturing and mechanical domains, and methods based on machine learning have been widely adopted in the field to monitor wear and spot defects before they lead to failures. Few studies, however, have addressed the problem of censored data, in which failure is not observed. In this paper, we propose a novel approach to predict the time to failure in ball bearings using survival analysis. First, we analyze bearing data in the frequency domain and annotate when a bearing fails by comparing the Kullback-Leibler divergence and the standard deviation between its break-in frequency bins and its break-out frequency bins. Second, we train several survival models to estimate the time to failure based on the annotated data and covariates extracted from the time domain, such as skewness, kurtosis and entropy. The models give a probabilistic prediction of risk over time and allow us to compare the survival function between groups of bearings. We demo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#26580;&#24615;&#25705;&#25830;&#30005;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#21307;&#30103;&#29289;&#32852;&#32593;&#31995;&#32479;&#65292;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#30417;&#27979;&#21644;&#20114;&#21160;&#65292;&#21253;&#25324;&#20301;&#32622;/&#36712;&#36857;&#36861;&#36394;&#12289;&#24515;&#29575;&#30417;&#27979;&#21644;&#36523;&#20221;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.07185</link><description>&lt;p&gt;
&#22522;&#20110;&#26580;&#24615;&#25705;&#25830;&#30005;&#20256;&#24863;&#22120;&#30340;&#26234;&#33021;&#21307;&#30103;&#29289;&#32852;&#32593;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#21450;&#20854;&#22312;&#34394;&#25311;&#29616;&#23454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Health Monitoring System Based on Flexible Triboelectric Sensors for Intelligence Medical Internet of Things and its Applications in Virtual Reality. (arXiv:2309.07185v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#26580;&#24615;&#25705;&#25830;&#30005;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#21307;&#30103;&#29289;&#32852;&#32593;&#31995;&#32479;&#65292;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#30417;&#27979;&#21644;&#20114;&#21160;&#65292;&#21253;&#25324;&#20301;&#32622;/&#36712;&#36857;&#36861;&#36394;&#12289;&#24515;&#29575;&#30417;&#27979;&#21644;&#36523;&#20221;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#29289;&#32852;&#32593;&#65288;IoMT&#65289;&#26159;&#23558;&#29289;&#32852;&#32593;&#25216;&#26415;&#19982;&#21307;&#30103;&#24212;&#29992;&#30456;&#32467;&#21512;&#30340;&#24179;&#21488;&#65292;&#23454;&#29616;&#20102;&#25968;&#23383;&#21270;&#21644;&#26234;&#33021;&#21270;&#26102;&#20195;&#30340;&#31934;&#20934;&#21307;&#23398;&#12289;&#26234;&#33021;&#21307;&#30103;&#21644;&#36828;&#31243;&#21307;&#30103;&#12290;&#28982;&#32780;&#65292;IoMT&#38754;&#20020;&#30528;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#20379;&#24212;&#12289;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#36866;&#24212;&#24615;&#21644;&#20256;&#24863;&#22120;&#30340;&#26234;&#33021;&#24615;&#31561;&#22810;&#31181;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#26580;&#24615;&#21487;&#31359;&#25140;&#25705;&#25830;&#30005;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#25968;&#25454;&#20998;&#26512;&#30340;&#21327;&#21516;&#38598;&#25104;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#20581;&#32780;&#26234;&#33021;&#30340;IoMT&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#22235;&#20010;&#25705;&#25830;&#30005;&#20256;&#24863;&#22120;&#23884;&#20837;&#21040;&#25163;&#29615;&#20013;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#26512;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#32930;&#20307;&#36816;&#21160;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25972;&#21512;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#26234;&#33021;&#21307;&#30103;&#30417;&#27979;&#31995;&#32479;&#65292;&#21253;&#25324;&#20301;&#32622;/&#36712;&#36857;&#36861;&#36394;&#12289;&#24515;&#29575;&#30417;&#27979;&#21644;&#36523;&#20221;&#35782;&#21035;&#65292;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#30417;&#27979;&#21644;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of Medical Things (IoMT) is a platform that combines Internet of Things (IoT) technology with medical applications, enabling the realization of precision medicine, intelligent healthcare, and telemedicine in the era of digitalization and intelligence. However, the IoMT faces various challenges, including sustainable power supply, human adaptability of sensors and the intelligence of sensors. In this study, we designed a robust and intelligent IoMT system through the synergistic integration of flexible wearable triboelectric sensors and deep learning-assisted data analytics. We embedded four triboelectric sensors into a wristband to detect and analyze limb movements in patients suffering from Parkinson's Disease (PD). By further integrating deep learning-assisted data analytics, we actualized an intelligent healthcare monitoring system for the surveillance and interaction of PD patients, which includes location/trajectory tracking, heart monitoring and identity recognition.
&lt;/p&gt;</description></item><item><title>CloudBrain-NMR&#26159;&#19968;&#20010;&#26234;&#33021;&#20113;&#35745;&#31639;&#24179;&#21488;&#65292;&#29992;&#20110;NMR&#20809;&#35889;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#36890;&#36807;&#22312;&#32447;&#35775;&#38382;&#65292;&#26080;&#38656;&#29992;&#25143;&#31471;&#23433;&#35013;&#20219;&#20309;&#31243;&#24207;&#65292;&#20351;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#21152;&#24555;&#22788;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.07178</link><description>&lt;p&gt;
CloudBrain-NMR: &#19968;&#31181;&#26234;&#33021;&#20113;&#35745;&#31639;&#24179;&#21488;&#65292;&#29992;&#20110;NMR&#20809;&#35889;&#22788;&#29702;&#12289;&#37325;&#24314;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis. (arXiv:2309.07178v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07178
&lt;/p&gt;
&lt;p&gt;
CloudBrain-NMR&#26159;&#19968;&#20010;&#26234;&#33021;&#20113;&#35745;&#31639;&#24179;&#21488;&#65292;&#29992;&#20110;NMR&#20809;&#35889;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#36890;&#36807;&#22312;&#32447;&#35775;&#38382;&#65292;&#26080;&#38656;&#29992;&#25143;&#31471;&#23433;&#35013;&#20219;&#20309;&#31243;&#24207;&#65292;&#20351;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#21152;&#24555;&#22788;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#30913;&#20849;&#25391;(NMR)&#20809;&#35889;&#23398;&#22312;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20013;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#23376;&#32467;&#26500;&#21644;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#23545;&#20174;NMR&#20809;&#35889;&#20202;&#33719;&#21462;&#30340;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#21644;&#21518;&#32493;&#23450;&#37327;&#20998;&#26512;&#28041;&#21450;&#21508;&#31181;&#19987;&#38376;&#24037;&#20855;&#65292;&#36825;&#23601;&#38656;&#35201;&#20840;&#38754;&#25484;&#25569;&#31243;&#24207;&#21644;NMR&#26041;&#38754;&#30340;&#30693;&#35782;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#35774;&#32622;&#22797;&#26434;&#65292;&#26032;&#20852;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#22312;NMR&#20013;&#30340;&#24212;&#29992;&#24182;&#19981;&#23481;&#26131;&#12290;&#22240;&#27492;&#65292;NMR&#22788;&#29702;&#23545;&#20110;&#21270;&#23398;&#23478;&#21644;&#29983;&#29289;&#23398;&#23478;&#26469;&#35828;&#24182;&#19981;&#26159;&#19968;&#39033;&#23481;&#26131;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CloudBrain-NMR&#65292;&#19968;&#31181;&#26234;&#33021;&#22312;&#32447;&#20113;&#35745;&#31639;&#24179;&#21488;&#65292;&#29992;&#20110;NMR&#25968;&#25454;&#30340;&#35835;&#21462;&#65292;&#22788;&#29702;&#65292;&#37325;&#24314;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;Web&#27983;&#35272;&#22120;&#26041;&#20415;&#22320;&#35775;&#38382;&#65292;&#26080;&#38656;&#29992;&#25143;&#31471;&#23433;&#35013;&#20219;&#20309;&#31243;&#24207;&#12290;CloudBrain-NMR&#20351;&#29992;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#21644;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#36827;&#34892;&#24182;&#34892;&#35745;&#31639;&#65292;&#20174;&#32780;&#22823;&#22823;&#32553;&#30701;&#20102;&#22788;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclear Magnetic Resonance (NMR) spectroscopy has served as a powerful analytical tool for studying molecular structure and dynamics in chemistry and biology. However, the processing of raw data acquired from NMR spectrometers and subsequent quantitative analysis involves various specialized tools, which necessitates comprehensive knowledge in programming and NMR. Particularly, the emerging deep learning tools is hard to be widely used in NMR due to the sophisticated setup of computation. Thus, NMR processing is not an easy task for chemist and biologists. In this work, we present CloudBrain-NMR, an intelligent online cloud computing platform designed for NMR data reading, processing, reconstruction, and quantitative analysis. The platform is conveniently accessed through a web browser, eliminating the need for any program installation on the user side. CloudBrain-NMR uses parallel computing with graphics processing units and central processing units, resulting in significantly shorten
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;HurriCast&#65292;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32452;&#21512;ARIMA&#27169;&#22411;&#21644;K-MEANS&#31639;&#27861;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39123;&#39118;&#36235;&#21183;&#65292;&#24182;&#32467;&#21512;Autoencoder&#36827;&#34892;&#25913;&#36827;&#30340;&#39123;&#39118;&#27169;&#25311;&#65292;&#20174;&#32780;&#26377;&#25928;&#27169;&#25311;&#21382;&#21490;&#39123;&#39118;&#34892;&#20026;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#26410;&#26469;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20840;&#38754;&#19988;&#26377;&#36873;&#25321;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20016;&#23500;&#20102;&#23545;&#39123;&#39118;&#27169;&#24335;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.07174</link><description>&lt;p&gt;
HurriCast&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#29992;&#20110;&#39123;&#39118;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting. (arXiv:2309.07174v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;HurriCast&#65292;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32452;&#21512;ARIMA&#27169;&#22411;&#21644;K-MEANS&#31639;&#27861;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39123;&#39118;&#36235;&#21183;&#65292;&#24182;&#32467;&#21512;Autoencoder&#36827;&#34892;&#25913;&#36827;&#30340;&#39123;&#39118;&#27169;&#25311;&#65292;&#20174;&#32780;&#26377;&#25928;&#27169;&#25311;&#21382;&#21490;&#39123;&#39118;&#34892;&#20026;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#26410;&#26469;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20840;&#38754;&#19988;&#26377;&#36873;&#25321;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20016;&#23500;&#20102;&#23545;&#39123;&#39118;&#27169;&#24335;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39123;&#39118;&#30001;&#20110;&#20854;&#28798;&#23475;&#24615;&#24433;&#21709;&#32780;&#22312;&#32654;&#22269;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#24456;&#37325;&#35201;&#65292;&#20445;&#38505;&#19994;&#22312;&#36825;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20351;&#29992;&#22797;&#26434;&#30340;&#32479;&#35745;&#27169;&#22411;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#24573;&#35270;&#20851;&#38190;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#39123;&#39118;&#27169;&#24335;&#65292;&#24182;&#21463;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#23558;ARIMA&#27169;&#22411;&#21644;K-MEANS&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39123;&#39118;&#36235;&#21183;&#65292;&#24182;&#20351;&#29992;Autoencoder&#36827;&#34892;&#25913;&#36827;&#30340;&#39123;&#39118;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#28151;&#21512;&#26041;&#27861;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#21382;&#21490;&#39123;&#39118;&#34892;&#20026;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#28508;&#22312;&#26410;&#26469;&#36335;&#24452;&#21644;&#24378;&#24230;&#30340;&#35814;&#32454;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#38754;&#32780;&#26377;&#36873;&#25321;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#25311;&#20016;&#23500;&#20102;&#23545;&#39123;&#39118;&#27169;&#24335;&#30340;&#24403;&#21069;&#29702;&#35299;&#65292;&#24182;&#20026;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hurricanes present major challenges in the U.S. due to their devastating impacts. Mitigating these risks is important, and the insurance industry is central in this effort, using intricate statistical models for risk assessment. However, these models often neglect key temporal and spatial hurricane patterns and are limited by data scarcity. This study introduces a refined approach combining the ARIMA model and K-MEANS to better capture hurricane trends, and an Autoencoder for enhanced hurricane simulations. Our experiments show that this hybrid methodology effectively simulate historical hurricane behaviors while providing detailed projections of potential future trajectories and intensities. Moreover, by leveraging a comprehensive yet selective dataset, our simulations enrich the current understanding of hurricane patterns and offer actionable insights for risk management strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26377;&#28508;&#21147;&#22312;&#35880;&#24910;&#30340;&#26694;&#26550;&#21644;&#25552;&#31034;&#35774;&#35745;&#19979;&#36229;&#36234;&#29616;&#26377;&#30340;&#26412;&#20307;&#23545;&#40784;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.07172</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Ontology Alignment. (arXiv:2309.07172v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26377;&#28508;&#21147;&#22312;&#35880;&#24910;&#30340;&#26694;&#26550;&#21644;&#25552;&#31034;&#35774;&#35745;&#19979;&#36229;&#36234;&#29616;&#26377;&#30340;&#26412;&#20307;&#23545;&#40784;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65288;&#22914;GPT&#31995;&#21015;&#21644;Flan-T5&#65289;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#29992;&#20110;&#35782;&#21035;&#26412;&#20307;&#20043;&#38388;&#30340;&#27010;&#24565;&#31561;&#20215;&#26144;&#23556;&#12290;&#20026;&#20102;&#27979;&#35797;Flan-T5-XXL&#21644;GPT-3.5-turbo&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;OAEI Bio-ML track&#30340;&#20004;&#20010;&#31561;&#20215;&#21305;&#37197;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#65292;&#24182;&#32771;&#34385;&#21040;&#27010;&#24565;&#26631;&#31614;&#21644;&#32467;&#26500;&#19978;&#19979;&#25991;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#26377;&#21487;&#33021;&#22312;&#35880;&#24910;&#30340;&#26694;&#26550;&#21644;&#25552;&#31034;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#29616;&#26377;&#30340;&#26412;&#20307;&#23545;&#40784;&#31995;&#32479;&#22914;BERTMap&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#30446;&#26631;&#31354;&#38388;&#36827;&#34892;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#20284;&#20219;&#21153;&#35282;&#33394;&#30340;&#29615;&#22659;&#29366;&#24577;&#38598;&#21512;&#22312;&#19968;&#36215;&#30340;&#32039;&#23494;&#20851;&#32852;&#34920;&#31034;&#26469;&#21457;&#29616;&#23376;&#30446;&#26631;&#65292;&#22312;&#23548;&#33322;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21487;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07168</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#30446;&#26631;&#31354;&#38388;&#36827;&#34892;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis. (arXiv:2309.07168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#30446;&#26631;&#31354;&#38388;&#36827;&#34892;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#20284;&#20219;&#21153;&#35282;&#33394;&#30340;&#29615;&#22659;&#29366;&#24577;&#38598;&#21512;&#22312;&#19968;&#36215;&#30340;&#32039;&#23494;&#20851;&#32852;&#34920;&#31034;&#26469;&#21457;&#29616;&#23376;&#30446;&#26631;&#65292;&#22312;&#23548;&#33322;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21487;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#23398;&#20064;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#20351;&#29992;&#31526;&#21495;&#26041;&#27861;&#36827;&#34892;&#30446;&#26631;&#34920;&#31034;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#32467;&#26500;&#21270;&#20026;&#39640;&#25928;&#19988;&#26131;&#20110;&#36801;&#31227;&#30340;&#23398;&#20064;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;&#20110;&#31526;&#21495;&#25512;&#29702;&#30340;&#29616;&#26377;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#21463;&#38480;&#20110;&#25163;&#21160;&#30446;&#26631;&#34920;&#31034;&#12290;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#30340;&#25361;&#25112;&#22312;&#20110;&#23427;&#24517;&#39035;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#22914;&#29615;&#22659;&#21160;&#21147;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32039;&#23494;&#20851;&#32852;&#65288;&#21363;&#23558;&#20855;&#26377;&#31867;&#20284;&#20219;&#21153;&#35282;&#33394;&#30340;&#29615;&#22659;&#29366;&#24577;&#38598;&#21512;&#22312;&#19968;&#36215;&#65289;&#30340;&#26032;&#20852;&#34920;&#31034;&#21457;&#29616;&#23376;&#30446;&#26631;&#30340;&#21457;&#23637;&#26426;&#21046;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#36880;&#28176;&#23398;&#20064;&#27492;&#34920;&#31034;&#20197;&#21450;&#31574;&#30053;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#23548;&#33322;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#23398;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#23454;&#29616;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this work, we propose a developmental mechanism for subgoal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We create a HRL algorithm that gradually learns this representation along with the policies and evaluate it on navigation tasks to show the learned representation is interpretable and results in data efficiency.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#20026;&#36164;&#28304;&#21463;&#38480;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#28151;&#21512;ASR&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;HMM&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#23637;&#31034;&#20102;&#23454;&#26102;&#21644;&#31934;&#30830;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#19982;&#20302;&#21151;&#32791;&#30828;&#20214;&#20860;&#23481;&#30340;&#29305;&#28857;&#65292;&#20026;&#26080;&#32541;&#20154;&#26426;&#20132;&#20114;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07164</link><description>&lt;p&gt;
&#36164;&#28304;&#21463;&#38480;&#26426;&#22120;&#20154;&#30340;&#28151;&#21512;ASR: HMM&#21644;&#28145;&#24230;&#23398;&#20064;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Hybrid ASR for Resource-Constrained Robots: HMM - Deep Learning Fusion. (arXiv:2309.07164v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07164
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#20026;&#36164;&#28304;&#21463;&#38480;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#28151;&#21512;ASR&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;HMM&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#23637;&#31034;&#20102;&#23454;&#26102;&#21644;&#31934;&#30830;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#19982;&#20302;&#21151;&#32791;&#30828;&#20214;&#20860;&#23481;&#30340;&#29305;&#28857;&#65292;&#20026;&#26080;&#32541;&#20154;&#26426;&#20132;&#20114;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#20026;&#36164;&#28304;&#21463;&#38480;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#26032;&#22411;&#28151;&#21512;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#23558;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#22871;&#25509;&#23383;&#32534;&#31243;&#26377;&#25928;&#22320;&#20998;&#37197;&#22788;&#29702;&#20219;&#21153;&#12290;&#22312;&#35813;&#26550;&#26500;&#20013;&#65292;&#22522;&#20110;HMM&#30340;&#22788;&#29702;&#22312;&#26426;&#22120;&#20154;&#20869;&#37096;&#36827;&#34892;&#65292;&#32780;&#19968;&#20010;&#29420;&#31435;&#30340;&#35745;&#31639;&#26426;&#22788;&#29702;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;HMM&#21644;&#28145;&#24230;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#23454;&#26102;&#21644;&#31934;&#30830;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#31995;&#32479;&#36866;&#24212;&#24615;&#24378;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#22768;&#23398;&#26465;&#20214;&#65292;&#19982;&#20302;&#21151;&#32791;&#30828;&#20214;&#20860;&#23481;&#65292;&#20351;&#20854;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#31181;&#28151;&#21512;&#30340;ASR&#33539;&#24335;&#20026;&#26080;&#32541;&#30340;&#20154;&#26426;&#20132;&#20114;&#25171;&#24320;&#20102;&#26377;&#21069;&#26223;&#30340;&#21487;&#33021;&#24615;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel hybrid Automatic Speech Recognition (ASR) system designed specifically for resource-constrained robots. The proposed approach combines Hidden Markov Models (HMMs) with deep learning models and leverages socket programming to distribute processing tasks effectively. In this architecture, the HMM-based processing takes place within the robot, while a separate PC handles the deep learning model. This synergy between HMMs and deep learning enhances speech recognition accuracy significantly. We conducted experiments across various robotic platforms, demonstrating real-time and precise speech recognition capabilities. Notably, the system exhibits adaptability to changing acoustic conditions and compatibility with low-power hardware, making it highly effective in environments with limited computational resources. This hybrid ASR paradigm opens up promising possibilities for seamless human-robot interaction. In conclusion, our research introduces a pioneering dimens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LSTM&#30340;&#31934;&#30830;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20256;&#24863;&#22120;&#20197;&#21450;&#20462;&#21098;&#25216;&#26415;&#36827;&#34892;&#24615;&#33021;&#20248;&#21270;&#65292;&#31995;&#32479;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;96&#65285;&#30340;&#29305;&#24322;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07154</link><description>&lt;p&gt;
&#20351;&#29992;LSTM&#25581;&#31034;&#31934;&#30830;&#30340;&#36300;&#20498;&#26816;&#27979;&#65306;&#20197;&#21484;&#22238;&#29575;&#20026;&#39537;&#21160;&#30340;&#31934;&#24230;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Recall-driven Precision Refinement: Unveiling Accurate Fall Detection using LSTM. (arXiv:2309.07154v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LSTM&#30340;&#31934;&#30830;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20256;&#24863;&#22120;&#20197;&#21450;&#20462;&#21098;&#25216;&#26415;&#36827;&#34892;&#24615;&#33021;&#20248;&#21270;&#65292;&#31995;&#32479;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;96&#65285;&#30340;&#29305;&#24322;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#32769;&#24180;&#20154;&#36300;&#20498;&#20107;&#20214;&#30340;&#32039;&#36843;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#20934;&#30830;&#30340;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#32467;&#21512;&#20102;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20256;&#24863;&#22120;&#65292;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#12290;&#36890;&#36807;&#26641;&#33683;&#27966;&#30828;&#20214;&#30340;&#38598;&#25104;&#23454;&#29616;&#20102;&#23454;&#26102;&#25191;&#34892;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;LSTM&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#21442;&#25968;&#20197;&#20248;&#21270;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20248;&#20808;&#32771;&#34385;&#21484;&#22238;&#29575;&#32780;&#19981;&#26159;&#31934;&#24230;&#65292;&#26088;&#22312;&#20934;&#30830;&#35782;&#21035;&#36300;&#20498;&#20107;&#20214;&#24182;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#35823;&#25253;&#65292;&#20197;&#20415;&#21450;&#26102;&#24178;&#39044;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#32454;&#33268;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#24378;&#35843;&#20102;&#39640;&#21484;&#22238;&#29575;&#21516;&#26102;&#20445;&#25345;96&#65285;&#30340;&#29305;&#24322;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26368;&#32456;&#24471;&#21040;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#21450;&#26102;&#21457;&#36865;&#36890;&#30693;&#65292;&#30830;&#20445;&#32769;&#24180;&#20154;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative approach to address the pressing concern of fall incidents among the elderly by developing an accurate fall detection system. Our proposed system combines state-of-the-art technologies, including accelerometer and gyroscope sensors, with deep learning models, specifically Long Short-Term Memory (LSTM) networks. Real-time execution capabilities are achieved through the integration of Raspberry Pi hardware. We introduce pruning techniques that strategically fine-tune the LSTM model's architecture and parameters to optimize the system's performance. We prioritize recall over precision, aiming to accurately identify falls and minimize false negatives for timely intervention. Extensive experimentation and meticulous evaluation demonstrate remarkable performance metrics, emphasizing a high recall rate while maintaining a specificity of 96\%. Our research culminates in a state-of-the-art fall detection system that promptly sends notifications, ensuring vulner
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#32534;&#30721;&#22120;&#12289;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#23547;&#25214;&#24433;&#21709;&#32773;&#30340;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26368;&#20339;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07153</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#23547;&#25214;&#24433;&#21709;&#32773;&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finding Influencers in Complex Networks: An Effective Deep Reinforcement Learning Approach. (arXiv:2309.07153v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#32534;&#30721;&#22120;&#12289;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#23547;&#25214;&#24433;&#21709;&#32773;&#30340;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26368;&#20339;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#20013;&#65292;&#26368;&#22823;&#21270;&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#21147;&#26159;&#19968;&#20010;&#23454;&#38469;&#37325;&#35201;&#20294;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#36817;&#20284;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#24040;&#22823;&#30340;&#20154;&#24037;&#35774;&#35745;&#24037;&#20316;&#65292;&#35201;&#20040;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#26080;&#27861;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24179;&#34913;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#23581;&#35797;&#21482;&#27880;&#37325;&#36895;&#24230;&#65292;&#32780;&#32570;&#20047;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#20013;&#65292;&#19982;&#20197;&#21069;&#30340;&#23581;&#35797;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#22312;&#20256;&#32479;&#30340;&#26368;&#20339;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#31639;&#27861;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#21629;&#21517;&#20026;DREIM&#12290;&#36890;&#36807;&#22312;&#23567;&#35268;&#27169;&#21512;&#25104;&#22270;&#19978;&#36827;&#34892;&#22823;&#37327;&#35757;&#32451;&#65292;DREIM&#22312;&#38750;&#24120;&#22823;&#30340;&#21512;&#25104;&#32593;&#32476;&#21644;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#19978;&#22312;&#35299;&#20915;&#36136;&#37327;&#26041;&#38754;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing influences in complex networks is a practically important but computationally challenging task for social network analysis, due to its NPhard nature. Most current approximation or heuristic methods either require tremendous human design efforts or achieve unsatisfying balances between effectiveness and efficiency. Recent machine learning attempts only focus on speed but lack performance enhancement. In this paper, different from previous attempts, we propose an effective deep reinforcement learning model that achieves superior performances over traditional best influence maximization algorithms. Specifically, we design an end-to-end learning framework that combines graph neural network as the encoder and reinforcement learning as the decoder, named DREIM. Trough extensive training on small synthetic graphs, DREIM outperforms the state-of-the-art baseline methods on very large synthetic and real-world networks on solution quality, and we also empirically show its linear sca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;EEG&#25968;&#25454;&#36716;&#25442;&#20026;&#39057;&#35889;&#22270;&#24182;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#22270;&#20687;&#20998;&#31867;&#25945;&#24072;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#37325;&#24314;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.07149</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20174;&#33041;&#30005;&#22270;&#20013;&#35299;&#30721;&#35270;&#35273;&#33041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models. (arXiv:2309.07149v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;EEG&#25968;&#25454;&#36716;&#25442;&#20026;&#39057;&#35889;&#22270;&#24182;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#22270;&#20687;&#20998;&#31867;&#25945;&#24072;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#37325;&#24314;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#35299;&#30721;&#35270;&#35273;&#34920;&#31034;&#24050;&#25104;&#20026;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#33041;&#26426;&#25509;&#21475;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#22270;&#20687;Net&#25968;&#25454;&#38598;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#26469;&#20998;&#31867;&#21644;&#37325;&#26500;&#22270;&#20687;&#65288;&#21363;&#8220;&#33041;&#35299;&#30721;&#8221;&#65289;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26469;&#33258;6&#21517;&#21442;&#19982;&#32773;&#30340;EEG&#35760;&#24405;&#65292;&#27599;&#21517;&#21442;&#19982;&#32773;&#35266;&#30475;&#20102;&#35206;&#30422;40&#20010;&#29420;&#29305;&#35821;&#20041;&#31867;&#21035;&#30340;50&#20010;&#22270;&#20687;&#12290;&#36825;&#20123;EEG&#35835;&#25968;&#34987;&#36716;&#25442;&#20026;&#39057;&#35889;&#22270;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#38598;&#25104;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#22270;&#20687;&#20998;&#31867;&#25945;&#24072;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;80%&#30340;&#21069;&#20116;&#20301;&#20934;&#30830;&#29575;&#65292;&#26174;&#33879;&#20248;&#20110;&#26631;&#20934;CNN&#21644;&#21508;&#31181;&#22522;&#20110;RNN&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#20687;&#37325;&#26500;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding visual representations from human brain activity has emerged as a thriving research domain, particularly in the context of brain-computer interfaces. Our study presents an innovative method that employs to classify and reconstruct images from the ImageNet dataset using electroencephalography (EEG) data from subjects that had viewed the images themselves (i.e. "brain decoding"). We analyzed EEG recordings from 6 participants, each exposed to 50 images spanning 40 unique semantic categories. These EEG readings were converted into spectrograms, which were then used to train a convolutional neural network (CNN), integrated with a knowledge distillation procedure based on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image classification teacher network. This strategy allowed our model to attain a top-5 accuracy of 80%, significantly outperforming a standard CNN and various RNN-based benchmarks. Additionally, we incorporated an image reconstruction mechanism ba
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ECG-Text&#39044;&#35757;&#32451;&#65288;ETP&#65289;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;ECG&#20449;&#21495;&#19982;&#25991;&#26412;&#25253;&#21578;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#36328;&#27169;&#24577;ECG&#29305;&#24449;&#23398;&#20064;&#12290;ETP&#22312;&#32447;&#24615;&#35780;&#20272;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#36328;&#27169;&#24577;ECG&#29305;&#24449;&#23398;&#20064;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07145</link><description>&lt;p&gt;
ETP: &#36890;&#36807;ECG-Text&#39044;&#35757;&#32451;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;ECG&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
ETP: Learning Transferable ECG Representations via ECG-Text Pre-training. (arXiv:2309.07145v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ECG-Text&#39044;&#35757;&#32451;&#65288;ETP&#65289;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;ECG&#20449;&#21495;&#19982;&#25991;&#26412;&#25253;&#21578;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#36328;&#27169;&#24577;ECG&#29305;&#24449;&#23398;&#20064;&#12290;ETP&#22312;&#32447;&#24615;&#35780;&#20272;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#36328;&#27169;&#24577;ECG&#29305;&#24449;&#23398;&#20064;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24515;&#34880;&#31649;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;ECG&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#27880;&#37322;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#24494;&#35843;&#38454;&#27573;&#38590;&#20197;&#22788;&#29702;&#19981;&#23384;&#22312;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ECG-Text&#39044;&#35757;&#32451;&#65288;ETP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#23558;ECG&#20449;&#21495;&#19982;&#25991;&#26412;&#25253;&#21578;&#32852;&#31995;&#36215;&#26469;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#39318;&#27425;&#22312;ECG&#39046;&#22495;&#20013;&#21033;&#29992;&#20102;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;ETP&#20351;&#29992;ECG&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#23558;ECG&#20449;&#21495;&#19982;&#20854;&#30456;&#24212;&#30340;&#25991;&#26412;&#25253;&#21578;&#23545;&#40784;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#32447;&#24615;&#35780;&#20272;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;PTB-XL&#21644;CPSC2018&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#40065;&#26834;&#19988;&#21487;&#36801;&#31227;&#30340;&#36328;&#27169;&#24577;ECG&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of cardiovascular healthcare, the Electrocardiogram (ECG) serves as a critical, non-invasive diagnostic tool. Although recent strides in self-supervised learning (SSL) have been promising for ECG representation learning, these techniques often require annotated samples and struggle with classes not present in the fine-tuning stages. To address these limitations, we introduce ECG-Text Pre-training (ETP), an innovative framework designed to learn cross-modal representations that link ECG signals with textual reports. For the first time, this framework leverages the zero-shot classification task in the ECG domain. ETP employs an ECG encoder along with a pre-trained language model to align ECG signals with their corresponding textual reports. The proposed framework excels in both linear evaluation and zero-shot classification tasks, as demonstrated on the PTB-XL and CPSC2018 datasets, showcasing its ability for robust and generalizable cross-modal ECG feature learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#21644;&#35780;&#20272;&#20050;&#20051;&#29699;&#36816;&#21160;&#21592;&#21160;&#20316;&#25216;&#33021;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25913;&#36827;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#24182;&#21033;&#29992;&#29305;&#24449;&#24037;&#31243;&#12289;&#38477;&#32500;&#21644;&#19981;&#21516;&#35780;&#20272;&#25351;&#26631;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#21160;&#20316;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#23618;&#27425;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.07141</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20050;&#20051;&#29699;&#36816;&#21160;&#21592;&#21160;&#20316;&#25216;&#33021;&#35782;&#21035;&#19982;&#35780;&#20272;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of Recognition and Evaluation System for Table Tennis Players' Motor Skills Based on Artificial Intelligence. (arXiv:2309.07141v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#21644;&#35780;&#20272;&#20050;&#20051;&#29699;&#36816;&#21160;&#21592;&#21160;&#20316;&#25216;&#33021;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25913;&#36827;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#24182;&#21033;&#29992;&#29305;&#24449;&#24037;&#31243;&#12289;&#38477;&#32500;&#21644;&#19981;&#21516;&#35780;&#20272;&#25351;&#26631;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#21160;&#20316;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#23618;&#27425;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#31185;&#23398;&#25216;&#26415;&#30340;&#39134;&#36895;&#21457;&#23637;&#65292;&#23545;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#30740;&#31350;&#19981;&#26029;&#26356;&#26032;&#65292;&#20294;&#30446;&#21069;&#26469;&#30475;&#65292;&#23545;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#35782;&#21035;&#21644;&#20998;&#26512;&#29305;&#23450;&#36816;&#21160;&#30340;&#33021;&#21147;&#36824;&#19981;&#22815;&#20840;&#38754;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25913;&#36827;&#20102;&#20050;&#20051;&#29699;&#36816;&#21160;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#20102;&#20050;&#20051;&#29699;&#36816;&#21160;&#21592;&#21160;&#20316;&#25216;&#33021;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#35774;&#22791;&#26469;&#25910;&#38598;&#20050;&#20051;&#29699;&#36816;&#21160;&#21592;&#30340;&#21160;&#20316;&#20449;&#24687;&#65292;&#24182;&#23545;&#23454;&#38469;&#36816;&#21160;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#12290;&#20854;&#27425;&#65292;&#21046;&#20316;&#20102;&#19968;&#20010;&#28369;&#21160;&#31383;&#21475;&#26469;&#23558;&#25910;&#38598;&#21040;&#30340;&#21160;&#20316;&#25968;&#25454;&#20998;&#21106;&#25104;&#20845;&#20010;&#20050;&#20051;&#29699;&#22522;&#20934;&#21160;&#20316;&#30340;&#29305;&#24449;&#25968;&#25454;&#24211;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#29305;&#24449;&#24037;&#31243;&#26500;&#24314;&#20102;&#21160;&#20316;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#38477;&#32500;&#21518;&#30340;&#19981;&#21516;&#27169;&#22411;&#26469;&#35782;&#21035;&#19981;&#21516;&#30340;&#21160;&#20316;&#25216;&#33021;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#19981;&#21516;&#35780;&#20272;&#25351;&#26631;&#30340;&#25439;&#22833;&#20989;&#25968;&#24314;&#31435;&#20102;&#21160;&#20316;&#25216;&#33021;&#30340;&#23618;&#27425;&#21270;&#35780;&#20272;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of electronic science and technology, the research on wearable devices is constantly updated, but for now, it is not comprehensive for wearable devices to recognize and analyze the movement of specific sports. Based on this, this paper improves wearable devices of table tennis sport, and realizes the pattern recognition and evaluation of table tennis players' motor skills through artificial intelligence. Firstly, a device is designed to collect the movement information of table tennis players and the actual movement data is processed. Secondly, a sliding window is made to divide the collected motion data into a characteristic database of six table tennis benchmark movements. Thirdly, motion features were constructed based on feature engineering, and motor skills were identified for different models after dimensionality reduction. Finally, the hierarchical evaluation system of motor skills is established with the loss functions of different evaluation indexes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07136</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;Transformer&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#24212;&#29992;&#20013;&#26368;&#37325;&#35201;&#30340;&#35786;&#26029;&#24037;&#20855;&#20043;&#19968;&#12290;&#38543;&#30528;&#20808;&#36827;&#31639;&#27861;&#30340;&#20986;&#29616;&#65292;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;ECG&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#20854;&#22312;ECG&#25968;&#25454;&#19978;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#23454;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#29992;&#30340;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;MTECG&#65292;&#23427;&#23558;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;220,251&#20010;ECG&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35786;&#26029;&#27880;&#37322;&#65292;&#20197;&#25506;&#32034;MTECG&#30340;&#29305;&#24615;&#12290;&#22312;&#25552;&#20986;&#30340;&#35757;&#32451;&#31574;&#30053;&#19979;&#65292;&#19968;&#20010;&#21482;&#26377;5.7M&#21442;&#25968;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#65288;5%-75%&#65289;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#12290;&#28040;&#34701;&#30740;&#31350;&#31361;&#20986;&#20102;&#27874;&#21160;&#37325;&#26500;&#30446;&#26631;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;MTECG&#32791;&#26102;&#36739;&#23569;&#19988;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;&#21508;&#31181;&#24515;&#30005;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) is one of the most important diagnostic tools in clinical applications. With the advent of advanced algorithms, various deep learning models have been adopted for ECG tasks. However, the potential of Transformers for ECG data is not yet realized, despite their widespread success in computer vision and natural language processing. In this work, we present a useful masked Transformer method for ECG classification referred to as MTECG, which expands the application of masked autoencoders to ECG time series. We construct a dataset comprising 220,251 ECG recordings with a broad range of diagnoses annoated by medical experts to explore the properties of MTECG. Under the proposed training strategies, a lightweight model with 5.7M parameters performs stably well on a broad range of masking ratios (5%-75%). The ablation studies highlight the importance of fluctuated reconstruction targets, training schedule length, layer-wise LR decay and DropPath rate. The experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#21152;&#26893;&#29289;&#30740;&#31350;&#25968;&#25454;FAIRness&#30340;&#26412;&#20307;&#35770;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#20041;&#26631;&#35760;&#21644;&#30456;&#20851;&#20803;&#25968;&#25454;&#22686;&#21152;&#25968;&#25454;&#30340;&#21487;&#37325;&#29992;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07129</link><description>&lt;p&gt;
&#22686;&#21152;&#26893;&#29289;&#30740;&#31350;&#25968;&#25454;&#30340;FAIRness&#30340;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
Ontologies for increasing the FAIRness of plant research data. (arXiv:2309.07129v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#21152;&#26893;&#29289;&#30740;&#31350;&#25968;&#25454;FAIRness&#30340;&#26412;&#20307;&#35770;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#20041;&#26631;&#35760;&#21644;&#30456;&#20851;&#20803;&#25968;&#25454;&#22686;&#21152;&#25968;&#25454;&#30340;&#21487;&#37325;&#29992;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#30740;&#31350;&#25968;&#25454;&#30340;FAIRness&#65288;&#21487;&#25214;&#21040;&#24615;&#12289;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#37325;&#29992;&#24615;&#65289;&#30340;&#37325;&#35201;&#24615;&#19981;&#21487;&#21542;&#35748;&#65292;&#29305;&#21035;&#26159;&#38754;&#23545;&#24403;&#21069;&#30001;&#32452;&#23398;&#25216;&#26415;&#20135;&#29983;&#30340;&#22823;&#35268;&#27169;&#22797;&#26434;&#25968;&#25454;&#38598;&#12290;&#26412;&#20307;&#35770;&#21487;&#20197;&#20316;&#20026;&#35821;&#20041;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#36890;&#36807;&#28155;&#21152;&#30456;&#20851;&#20803;&#25968;&#25454;&#26469;&#22686;&#21152;&#23545;&#25968;&#25454;&#29983;&#25104;&#26041;&#24335;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#39640;&#20854;&#20114;&#25805;&#20316;&#24615;&#12290;&#26412;&#20307;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#27010;&#24565;&#20197;&#21450;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#26412;&#20307;&#35770;&#26415;&#35821;&#23545;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#65292;&#25968;&#25454;&#26082;&#21487;&#20197;&#20154;&#31867;&#35299;&#37322;&#65292;&#20063;&#21487;&#20197;&#26426;&#22120;&#35299;&#37322;&#65292;&#20174;&#32780;&#22686;&#21152;&#37325;&#29992;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#30740;&#31350;&#39046;&#22495;&#25110;&#25216;&#26415;&#20013;&#35782;&#21035;&#30456;&#20851;&#26412;&#20307;&#35770;&#30340;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#22810;&#26679;&#21270;&#30340;&#22522;&#30784;&#26893;&#29289;&#30740;&#31350;&#39046;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of improving the FAIRness (findability, accessibility, interoperability, reusability) of research data is undeniable, especially in the face of large, complex datasets currently being produced by omics technologies. Facilitating the integration of a dataset with other types of data increases the likelihood of reuse, and the potential of answering novel research questions. Ontologies are a useful tool for semantically tagging datasets as adding relevant metadata increases the understanding of how data was produced and increases its interoperability. Ontologies provide concepts for a particular domain as well as the relationships between concepts. By tagging data with ontology terms, data becomes both human and machine interpretable, allowing for increased reuse and interoperability. However, the task of identifying ontologies relevant to a particular research domain or technology is challenging, especially within the diverse realm of fundamental plant research. In this re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23616;&#37096;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#22522;&#20110;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#21644;&#20108;&#27425;Lyapunov&#20989;&#25968;&#65292;&#24182;&#32467;&#21512;&#20102;&#21407;&#28857;&#38468;&#36817;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#32447;&#24615;&#32467;&#26500;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#20026;&#20934;&#30830;&#21644;&#26377;&#25928;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#23616;&#37096;&#25351;&#25968;&#31283;&#23450;&#24615;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#31946;Lyapunov&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06841</link><description>&lt;p&gt;
&#20851;&#20110;T-S&#27169;&#31946;&#31995;&#32479;&#22312;&#21407;&#28857;&#38468;&#36817;&#30340;&#23616;&#37096;&#20108;&#27425;&#31283;&#23450;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Local Quadratic Stability of T-S Fuzzy Systems in the Vicinity of the Origin. (arXiv:2309.06841v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23616;&#37096;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#22522;&#20110;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#21644;&#20108;&#27425;Lyapunov&#20989;&#25968;&#65292;&#24182;&#32467;&#21512;&#20102;&#21407;&#28857;&#38468;&#36817;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#32447;&#24615;&#32467;&#26500;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#20026;&#20934;&#30830;&#21644;&#26377;&#25928;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#23616;&#37096;&#25351;&#25968;&#31283;&#23450;&#24615;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#31946;Lyapunov&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24341;&#20837;&#26032;&#30340;&#23616;&#37096;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;Takagi-Sugeno&#65288;T-S&#65289;&#27169;&#31946;&#31995;&#32479;&#12290;&#36825;&#20123;&#31283;&#23450;&#24615;&#26465;&#20214;&#22522;&#20110;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#65288;LMIs&#65289;&#21644;&#20108;&#27425;Lyapunov&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#32467;&#21512;&#20102;&#21407;&#28857;&#22788;&#30340;&#38582;&#23646;&#20989;&#25968;&#20449;&#24687;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;&#21407;&#28857;&#38468;&#36817;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#32447;&#24615;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#19982;&#25991;&#29486;&#20013;&#20351;&#29992;&#27169;&#31946;Lyapunov&#20989;&#25968;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25552;&#20986;&#30340;&#26465;&#20214;&#35777;&#26126;&#20102;&#26356;&#23569;&#30340;&#20445;&#23432;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;T-S&#27169;&#31946;&#31995;&#32479;&#23616;&#37096;&#25351;&#25968;&#31283;&#23450;&#24615;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#27169;&#31946;Lyapunov&#26041;&#27861;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#28436;&#31034;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#31034;&#20363;&#65292;&#38416;&#26126;&#20102;&#26680;&#24515;&#27010;&#24565;&#65292;&#24182;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main goal of this paper is to introduce new local stability conditions for continuous-time Takagi-Sugeno (T-S) fuzzy systems. These stability conditions are based on linear matrix inequalities (LMIs) in combination with quadratic Lyapunov functions. Moreover, they integrate information on the membership functions at the origin and effectively leverage the linear structure of the underlying nonlinear system in the vicinity of the origin. As a result, the proposed conditions are proved to be less conservative compared to existing methods using fuzzy Lyapunov functions in the literature. Moreover, we establish that the proposed methods offer necessary and sufficient conditions for the local exponential stability of T-S fuzzy systems. The paper also includes discussions on the inherent limitations associated with fuzzy Lyapunov approaches. To demonstrate the theoretical results, we provide comprehensive examples that elucidate the core concepts and validate the efficacy of the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06800</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#35768;&#22810;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#39044;&#27979;&#20301;&#32622;&#26377;&#23436;&#25972;&#25110;&#33267;&#23569;&#37096;&#20998;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#26080;&#21382;&#21490;&#35760;&#24405;&#30340;&#20301;&#32622;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#21644;&#23433;&#35013;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21487;&#33021;&#21463;&#38480;&#65292;&#36825;&#20351;&#24471;&#22823;&#22810;&#25968;&#24403;&#21069;&#27169;&#22411;&#19981;&#36866;&#29992;&#12290;&#34429;&#28982;&#23569;&#25968;&#25991;&#29486;&#23581;&#35797;&#22312;&#32570;&#22833;&#20301;&#32622;&#19978;&#25554;&#34917;&#20132;&#36890;&#29366;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19982;&#20256;&#24863;&#22120;&#20301;&#32622;&#21516;&#26102;&#35266;&#27979;&#30340;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#32570;&#20047;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#24471;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#20219;&#21153;&#25110;&#28041;&#21450;&#20915;&#31574;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21463;&#21040;&#20808;&#21069;&#30340;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#21327;&#21516;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06604</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#19978;&#30340;&#28151;&#21512;&#31639;&#27861;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;: &#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#20195;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach. (arXiv:2309.06604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#21327;&#21516;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26159;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#20851;&#38190;&#30340;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#25968;&#37327;&#30340;&#22823;&#24133;&#22686;&#21152;&#12289;&#22810;&#26679;&#24615;&#21644;&#20998;&#24067;&#24615;&#65292;&#36825;&#20123;&#27493;&#39588;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#24403;&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#30340;&#35774;&#35745;&#26102;&#65292;&#20250;&#24102;&#26469;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#22810;&#20010;&#29420;&#29305;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21644;&#21327;&#21516;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#20854;&#26597;&#35810;&#32467;&#26500;&#26469;&#25903;&#25345;&#19978;&#36848;&#21151;&#33021;&#65292;&#32780;&#19981;&#38480;&#20110;&#29305;&#23450;&#30340;&#23398;&#20064;&#12289;&#36873;&#25321;&#21644;&#35843;&#25972;&#26426;&#21046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12289;&#24418;&#24335;&#39564;&#35777;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical st
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;</title><link>http://arxiv.org/abs/2309.05918</link><description>&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#65306;&#36208;&#21521;&#31526;&#21495;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#26412;&#20307;&#35770;&#22522;&#20110;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05918
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#22260;&#32469;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#23545;&#25104;&#21151;&#30340;&#29378;&#28909;&#26159;&#26377;&#20123;&#35823;&#23548;&#30340;&#65292;&#21407;&#22240;&#22914;&#19979;&#65306;&#65288;i&#65289;LLMs&#19981;&#33021;&#20381;&#36182;&#20110;&#20107;&#23454;&#20449;&#24687;&#65292;&#22240;&#20026;&#23545;&#20110;LLMs&#26469;&#35828;&#65292;&#25668;&#20837;&#30340;&#25152;&#26377;&#25991;&#26412;&#65288;&#20107;&#23454;&#25110;&#38750;&#20107;&#23454;&#65289;&#37117;&#26159;&#24179;&#31561;&#30340;&#65307;&#65288;ii&#65289;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#35821;&#35328;&#30340;&#20219;&#20309;&#8220;&#30693;&#35782;&#8221;&#37117;&#23558;&#27704;&#36828;&#22475;&#34255;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#26412;&#36523;&#26159;&#26377;&#24847;&#20041;&#30340;&#65307;&#20197;&#21450;&#65288;iii&#65289;LLMs&#22312;&#20960;&#31181;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#24120;&#24120;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#65288;&#22914;&#21517;&#35789;&#22797;&#21512;&#35789;&#12289;&#20849;&#35859;&#35789;&#12289;&#37327;&#35789;&#33539;&#22260;&#27169;&#31946;&#21644;&#24847;&#21521;&#24615;&#19978;&#19979;&#25991;&#65289;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30456;&#23545;&#25104;&#21151;&#19981;&#26159;&#31526;&#21495;&#19982;&#20122;&#31526;&#21495;&#20043;&#36777;&#30340;&#21453;&#26144;&#65292;&#32780;&#26159;&#22312;&#35268;&#27169;&#19978;&#24212;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#30340;&#25104;&#21151;&#31574;&#30053;&#30340;&#21453;&#26144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;&#24212;&#29992;&#20110;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;
&lt;/p&gt;
&lt;p&gt;
In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts. Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbol-ic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#24403;&#21069;&#32842;&#22825;&#26426;&#22120;&#20154;&#27979;&#35797;&#30340;&#23454;&#36341;&#21644;&#24320;&#25918;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#21644;&#20943;&#36731;&#19982;&#29992;&#25143;&#20449;&#20219;&#30456;&#20851;&#30340;&#26381;&#21153;&#25110;&#20135;&#21697;&#24615;&#33021;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#20197;&#21450;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24847;&#22806;&#21518;&#26524;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05680</link><description>&lt;p&gt;
&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#20419;&#36827;&#29992;&#25143;&#30340;&#20449;&#20219; - &#23454;&#36341;&#21644;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Evaluating Chatbots to Promote Users' Trust -- Practices and Open Problems. (arXiv:2309.05680v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#24403;&#21069;&#32842;&#22825;&#26426;&#22120;&#20154;&#27979;&#35797;&#30340;&#23454;&#36341;&#21644;&#24320;&#25918;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#21644;&#20943;&#36731;&#19982;&#29992;&#25143;&#20449;&#20219;&#30456;&#20851;&#30340;&#26381;&#21153;&#25110;&#20135;&#21697;&#24615;&#33021;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#20197;&#21450;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24847;&#22806;&#21518;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36719;&#20214;&#65292;&#36890;&#36807;&#19982;&#23427;&#20204;&#33258;&#28982;&#22320;&#20114;&#21160;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#23613;&#31649;&#32842;&#22825;&#26426;&#22120;&#20154;&#20174;&#20154;&#24037;&#26234;&#33021;&#35806;&#29983;&#20043;&#21021;&#23601;&#24050;&#32463;&#34987;&#30740;&#31350;&#65292;&#20294;&#33258;&#20174;&#26131;&#20110;&#20351;&#29992;&#19988;&#36890;&#29992;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22914;ChatGPT&#25512;&#20986;&#20197;&#26469;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#29305;&#21035;&#21560;&#24341;&#20102;&#20844;&#20247;&#21644;&#20225;&#19994;&#30340;&#20851;&#27880;&#12290;&#20225;&#19994;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#19968;&#31181;&#28508;&#22312;&#25216;&#26415;&#26469;&#21560;&#24341;&#29992;&#25143;&#65292;&#36825;&#20123;&#29992;&#25143;&#21487;&#33021;&#26159;&#26368;&#32456;&#28040;&#36153;&#32773;&#12289;&#20379;&#24212;&#21830;&#65292;&#29978;&#33267;&#26159;&#33258;&#24049;&#30340;&#21592;&#24037;&#65292;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#36866;&#24403;&#30340;&#27979;&#35797;&#20197;&#35299;&#20915;&#21644;&#20943;&#36731;&#19982;&#26381;&#21153;&#25110;&#20135;&#21697;&#24615;&#33021;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#20197;&#21450;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24847;&#22806;&#21518;&#26524;&#30456;&#20851;&#30340;&#20449;&#20219;&#38382;&#39064;&#26159;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;&#32842;&#22825;&#26426;&#22120;&#20154;&#27979;&#35797;&#30340;&#23454;&#36341;&#65292;&#30830;&#23450;&#20102;&#36861;&#27714;&#29992;&#25143;&#20449;&#20219;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21069;&#36827;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots, the common moniker for collaborative assistants, are Artificial Intelligence (AI) software that enables people to naturally interact with them to get tasks done. Although chatbots have been studied since the dawn of AI, they have particularly caught the imagination of the public and businesses since the launch of easy-to-use and general-purpose Large Language Model-based chatbots like ChatGPT. As businesses look towards chatbots as a potential technology to engage users, who may be end customers, suppliers, or even their own employees, proper testing of chatbots is important to address and mitigate issues of trust related to service or product performance, user satisfaction and long-term unintended consequences for society. This paper reviews current practices for chatbot testing, identifies gaps as open problems in pursuit of user trust, and outlines a path forward.
&lt;/p&gt;</description></item><item><title>CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.04802</link><description>&lt;p&gt;
CPMR: &#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#19982;&#20266;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04802
&lt;/p&gt;
&lt;p&gt;
CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#30340;&#21160;&#26426;&#21487;&#20197;&#20998;&#20026;&#38745;&#24577;&#20559;&#22909;&#21644;&#21160;&#24577;&#20852;&#36259;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#65292;&#26368;&#36817;&#30340;&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;&#21033;&#29992;&#20449;&#24687;&#20256;&#25773;&#21644;&#28436;&#21270;&#20174;&#25209;&#37327;&#21040;&#36798;&#30340;&#20114;&#21160;&#20013;&#36827;&#34892;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#22312;&#19978;&#19979;&#25991;&#22330;&#26223;&#20013;&#20154;&#20204;&#24456;&#23481;&#26131;&#21463;&#21040;&#20854;&#20182;&#29992;&#25143;&#30340;&#26368;&#36817;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21382;&#21490;&#20114;&#21160;&#20013;&#24212;&#29992;&#28436;&#21270;&#20250;&#31232;&#37322;&#26368;&#36817;&#20114;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#26080;&#27861;&#20934;&#30830;&#22320;&#24314;&#27169;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20266;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#65288;CPMR&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#29992;&#25143;&#21644;&#39033;&#30446;&#21019;&#24314;&#19977;&#20010;&#34920;&#31034;&#65288;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#65289;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#21644;&#19978;&#19979;&#25991;&#24773;&#22659;&#20013;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#21516;&#26102;&#25552;&#39640;&#26102;&#38388;&#29366;&#24577;&#28436;&#21270;&#21644;&#22686;&#37327;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommenda
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29289;&#20307;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#29289;&#29702;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#22312;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02561</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physically Grounded Vision-Language Models for Robotic Manipulation. (arXiv:2309.02561v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29289;&#20307;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#29289;&#29702;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#22312;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#30740;&#31350;&#36827;&#23637;&#23548;&#33268;&#22312;&#35270;&#35273;&#38382;&#31572;&#21644;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#29616;&#22312;&#21487;&#20197;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VLMs&#22312;&#23545;&#24120;&#35265;&#29289;&#20307;&#30340;&#29289;&#29702;&#27010;&#24565;&#65288;&#20363;&#22914;&#26448;&#26009;&#12289;&#33030;&#24369;&#24615;&#65289;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#28041;&#21450;&#19982;&#36825;&#20123;&#29289;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#29289;&#29702;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhysObjects&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;36.9K&#20010;&#20247;&#21253;&#21644;417K&#20010;&#33258;&#21160;&#21270;&#30340;&#24120;&#35265;&#23478;&#23621;&#29289;&#21697;&#30340;&#29289;&#29702;&#27010;&#24565;&#27880;&#37322;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;PhysObjects&#19978;&#23545;VLM&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#23545;&#29289;&#29702;&#29289;&#20307;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#30340;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23558;&#36825;&#20010;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;VLM&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhysObjects, an object-centric dataset of 36.9K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically-grounded VLM in an interactive framework with a large languag
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#39046;&#22495;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#20026;&#32467;&#26524;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;&#35843;&#26597;&#25552;&#20379;&#20102;&#19968;&#31181;&#19977;&#32423;&#20998;&#31867;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#20855;&#26377;&#35299;&#37322;&#27880;&#37322;&#30340;CMR&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;I-CMR&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.01955</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35299;&#37322;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Interpretable Cross-modal Reasoning. (arXiv:2309.01955v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01955
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#39046;&#22495;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#20026;&#32467;&#26524;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;&#35843;&#26597;&#25552;&#20379;&#20102;&#19968;&#31181;&#19977;&#32423;&#20998;&#31867;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#20855;&#26377;&#35299;&#37322;&#27880;&#37322;&#30340;CMR&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;I-CMR&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#36328;&#27169;&#24577;&#25512;&#29702;&#65288;CMR&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#28085;&#30422;&#20102;&#20174;&#22810;&#23186;&#20307;&#20998;&#26512;&#21040;&#21307;&#30103;&#35786;&#26029;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#37096;&#32626;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#23545;&#36825;&#20123;&#31995;&#32479;&#20915;&#31574;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#29702;&#35299;&#24615;&#30340;&#38656;&#27714;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#65288;I-CMR&#65289;&#39046;&#22495;&#65292;&#20854;&#30446;&#26631;&#19981;&#20165;&#26159;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#35201;&#20026;&#32467;&#26524;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#19968;&#31181;&#19977;&#32423;&#20998;&#31867;&#26041;&#27861;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#20855;&#26377;&#35299;&#37322;&#27880;&#37322;&#30340;CMR&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#26412;&#35843;&#26597;&#24635;&#32467;&#20102;I-CMR&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#24635;&#20043;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#25512;&#21160;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, cross-modal reasoning (CMR), the process of understanding and reasoning across different modalities, has emerged as a pivotal area with applications spanning from multimedia analysis to healthcare diagnostics. As the deployment of AI systems becomes more ubiquitous, the demand for transparency and comprehensibility in these systems' decision-making processes has intensified. This survey delves into the realm of interpretable cross-modal reasoning (I-CMR), where the objective is not only to achieve high predictive performance but also to provide human-understandable explanations for the results. This survey presents a comprehensive overview of the typical methods with a three-level taxonomy for I-CMR. Furthermore, this survey reviews the existing CMR datasets with annotations for explanations. Finally, this survey summarizes the challenges for I-CMR and discusses potential future directions. In conclusion, this survey aims to catalyze the progress of this emerging resea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;DKM&#23454;&#29616;&#65292;&#21363;eDKM&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#35757;&#32451;&#26102;&#26435;&#37325;&#32858;&#31867;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#20943;&#23567;DKM&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#35299;&#20915;&#20102;LLM&#21387;&#32553;&#20013;&#30340;&#35757;&#32451;&#24320;&#38144;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.00964</link><description>&lt;p&gt;
eDKM:&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#35757;&#32451;&#26102;&#26435;&#37325;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models. (arXiv:2309.00964v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;DKM&#23454;&#29616;&#65292;&#21363;eDKM&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#35757;&#32451;&#26102;&#26435;&#37325;&#32858;&#31867;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#20943;&#23567;DKM&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#35299;&#20915;&#20102;LLM&#21387;&#32553;&#20013;&#30340;&#35757;&#32451;&#24320;&#38144;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#22797;&#26434;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#24615;&#33021;&#65292;&#22240;&#27492;&#23558;&#36825;&#20123;LLMs&#24341;&#20837;&#31227;&#21160;&#35774;&#22791;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#21709;&#24212;&#21644;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#35268;&#27169;&#65288;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#38656;&#35201;&#39640;&#25928;&#30340;&#21387;&#32553;&#25165;&#33021;&#36866;&#24212;&#23384;&#20648;&#26377;&#38480;&#30340;&#35774;&#22791;&#12290;&#22312;&#20247;&#22810;&#21387;&#32553;&#25216;&#26415;&#20013;&#65292;&#26435;&#37325;&#32858;&#31867;&#26159;LLM&#21387;&#32553;&#30340;&#39046;&#20808;&#20505;&#36873;&#26041;&#27861;&#20043;&#19968;&#65292;&#24182;&#24471;&#21040;&#20102;&#29616;&#20195;&#26234;&#33021;&#25163;&#26426;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#20854;&#35757;&#32451;&#24320;&#38144;&#23545;LLM&#30340;&#24494;&#35843;&#26469;&#35828;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#21487;&#24494;&#20998;K&#22343;&#20540;&#32858;&#31867;&#65288;DKM&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#21387;&#32553;&#27604;&#21644;&#20934;&#30830;&#24615;&#22238;&#24402;&#20043;&#38388;&#30340;&#26368;&#20808;&#36827;&#25240;&#34935;&#26041;&#26696;&#65292;&#20294;&#20854;&#36739;&#22823;&#30340;&#20869;&#23384;&#22797;&#26434;&#24615;&#20351;&#20854;&#20960;&#20046;&#19981;&#21487;&#33021;&#24212;&#29992;&#20110;&#35757;&#32451;&#26102;&#30340;LLM&#21387;&#32553;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;DKM&#23454;&#29616;&#65292;&#21363;eDKM&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#25216;&#26415;&#20943;&#23567;&#20102;DKM&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since Large Language Models or LLMs have demonstrated high-quality performance on many complex language tasks, there is a great interest in bringing these LLMs to mobile devices for faster responses and better privacy protection. However, the size of LLMs (i.e., billions of parameters) requires highly effective compression to fit into storage-limited devices. Among many compression techniques, weight-clustering, a form of non-linear quantization, is one of the leading candidates for LLM compression, and supported by modern smartphones. Yet, its training overhead is prohibitively significant for LLM fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown the state-of-the-art trade-off between compression ratio and accuracy regression, but its large memory complexity makes it nearly impossible to apply to train-time LLM compression. In this paper, we propose a memory-efficient DKM implementation, eDKM powered by novel techniques to reduce the memory footprint of DKM 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#36827;&#34892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#34920;&#29616;&#12290;&#36890;&#36807;&#36830;&#25509;&#21307;&#23398;&#26415;&#35821;&#21644;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#65292;&#36825;&#31181;&#23884;&#20837;&#26041;&#27861;&#25581;&#31034;&#20102;&#20020;&#24202;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20379;&#20102;&#23545;&#20020;&#24202;&#21307;&#29983;&#26356;&#26131;&#29702;&#35299;&#12289;&#20020;&#24202;&#26356;&#20934;&#30830;&#30340;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.00917</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#29992;&#20110;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#34920;&#29616;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports. (arXiv:2309.00917v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#36827;&#34892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#34920;&#29616;&#12290;&#36890;&#36807;&#36830;&#25509;&#21307;&#23398;&#26415;&#35821;&#21644;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#65292;&#36825;&#31181;&#23884;&#20837;&#26041;&#27861;&#25581;&#31034;&#20102;&#20020;&#24202;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20379;&#20102;&#23545;&#20020;&#24202;&#21307;&#29983;&#26356;&#26131;&#29702;&#35299;&#12289;&#20020;&#24202;&#26356;&#20934;&#30830;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20020;&#24202;&#25991;&#26412;&#30340;&#26041;&#24335;&#21457;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#24341;&#20837;&#23548;&#33268;&#20102;&#38024;&#23545;&#65288;&#29983;&#29289;&#65289;&#21307;&#23398;&#39046;&#22495;&#30340;&#35843;&#25972;&#65292;&#22914;PubMedBERT&#21644;ClinicalBERT&#12290;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#22823;&#37327;&#23384;&#26723;&#30340;&#21307;&#23398;&#25991;&#26723;&#25968;&#25454;&#24211;&#12290;&#34429;&#28982;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#35299;&#37322;&#33021;&#21147;&#30340;&#32570;&#20047;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23427;&#32771;&#34385;&#20102;&#25253;&#21578;&#30340;&#32467;&#26500;&#21644;&#32452;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#35821;&#35328;SNOMED&#20020;&#24202;&#26415;&#35821;&#30693;&#35782;&#24211;&#36830;&#25509;&#25253;&#21578;&#20013;&#30340;&#21307;&#23398;&#26415;&#35821;&#12290;&#29983;&#25104;&#30340;&#22270;&#23884;&#20837;&#25581;&#31034;&#20986;&#20020;&#24202;&#26415;&#35821;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#20020;&#24202;&#21307;&#29983;&#26356;&#26131;&#29702;&#35299;&#12289;&#20020;&#24202;&#26356;&#20934;&#30830;&#30340;&#34920;&#24449;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#23884;&#20837;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The way we analyse clinical texts has undergone major changes over the last years. The introduction of language models such as BERT led to adaptations for the (bio)medical domain like PubMedBERT and ClinicalBERT. These models rely on large databases of archived medical documents. While performing well in terms of accuracy, both the lack of interpretability and limitations to transfer across languages limit their use in clinical setting. We introduce a novel light-weight graph-based embedding method specifically catering radiology reports. It takes into account the structure and composition of the report, while also connecting medical terms in the report through the multi-lingual SNOMED Clinical Terms knowledge base. The resulting graph embedding uncovers the underlying relationships among clinical terms, achieving a representation that is better understandable for clinicians and clinically more accurate, without reliance on large pre-training datasets. We show the use of this embedding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.11764</link><description>&lt;p&gt;
Halo&#65306;&#35780;&#20272;&#21644;&#38477;&#20302;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#12290;&#34429;&#28982;&#23545;&#20110;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#26041;&#20415;&#65292;&#20294;&#26159;&#19982;&#20854;&#26356;&#22823;&#35268;&#27169;&#30340;&#23545;&#24212;&#27169;&#22411;&#30456;&#27604;&#65292;&#24320;&#28304;&#30340;&#21442;&#25968;&#36739;&#23569;&#30340;LLMs&#32463;&#24120;&#20986;&#29616;&#20005;&#37325;&#24187;&#35273;&#38382;&#39064;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#27979;&#37327;&#21644;&#20943;&#23569;BLOOM 7B&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#26159;&#20844;&#24320;&#25552;&#20379;&#32473;&#30740;&#31350;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#24369;&#24320;&#28304;LLMs&#30340;&#20195;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HaloCheck&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26080;&#38656;&#30693;&#35782;&#30340;&#40657;&#30418;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#20302;&#21442;&#25968;LLMs&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;LLMs&#30340;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#28857;&#23545;&#27880;&#24847;&#21147;&#24863;&#30693;&#26426;&#21046;&#30340;&#21452;&#21521;&#23545;&#24212;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#28857;&#21644;&#22330;&#26223;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#28857;&#23545;&#21305;&#37197;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#35266;&#23519;&#36136;&#37327;&#21644;&#36974;&#25377;&#30340;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08518</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#39044;&#27979;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#28857;&#23545;&#27880;&#24847;&#21147;&#30340;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction. (arXiv:2308.08518v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#28857;&#23545;&#27880;&#24847;&#21147;&#24863;&#30693;&#26426;&#21046;&#30340;&#21452;&#21521;&#23545;&#24212;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#28857;&#21644;&#22330;&#26223;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#28857;&#23545;&#21305;&#37197;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#35266;&#23519;&#36136;&#37327;&#21644;&#36974;&#25377;&#30340;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20960;&#20309;&#27880;&#20876;&#20272;&#35745;&#26041;&#27861;&#20165;&#38544;&#24335;&#22320;&#21033;&#29992;CAD&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#23427;&#20204;&#23545;&#35266;&#23519;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#36974;&#25377;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#28857;&#23545;&#27880;&#24847;&#21147;&#24863;&#30693;&#26426;&#21046;&#30340;&#21452;&#21521;&#23545;&#24212;&#39044;&#27979;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#19981;&#20165;&#35201;&#27714;&#27169;&#22411;&#28857;&#39044;&#27979;&#23545;&#24212;&#20851;&#31995;&#65292;&#36824;&#26126;&#30830;&#22320;&#23545;&#35266;&#23519;&#21644;&#27169;&#22411;&#20808;&#39564;&#20043;&#38388;&#30340;&#20960;&#20309;&#30456;&#20284;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#27599;&#20010;&#27169;&#22411;&#28857;&#21644;&#22330;&#26223;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20026;&#23398;&#20064;&#28857;&#23545;&#21305;&#37197;&#25552;&#20379;&#20102;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#29305;&#24449;&#20998;&#24067;&#20998;&#27495;&#24102;&#26469;&#30340;&#30456;&#20851;&#24615;&#22122;&#22768;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20266;&#23402;&#29983;&#32593;&#32476;&#26469;&#25913;&#21892;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#32447;MOD&#12289;YCB-Video&#21644;Occ-LineMOD&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional geometric registration based estimation methods only exploit the CAD model implicitly, which leads to their dependence on observation quality and deficiency to occlusion.To address the problem,the paper proposes a bidirectional correspondence prediction network with a point-wise attention-aware mechanism. This network not only requires the model points to predict the correspondence but also explicitly models the geometric similarities between observations and the model prior.} Our key insight is that the correlations between each model point and scene point provide essential information for learning point-pair matches. To further tackle the correlation noises brought by feature distribution divergence, we design a simple but effective pseudo-siamese network to improve feature homogeneity.Experimental results on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show that the proposed method achieves better performance than other state-of-the-art methods under the sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.07200</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Categorical Priors for Physics-Based Character Control. (arXiv:2308.07200v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23398;&#20064;&#21487;&#37325;&#29992;&#36816;&#21160;&#20808;&#39564;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#33258;&#28982;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36816;&#21160;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#36861;&#36394;&#21644;&#27169;&#20223;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#36816;&#21160;&#21098;&#36753;&#30340;&#36924;&#30495;&#21160;&#20316;&#65292;&#20351;&#29992;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#22914;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#20013;&#25152;&#37319;&#29992;&#30340;&#37027;&#26679;&#12290;&#35813;&#32467;&#26500;&#23558;&#26469;&#33258;&#36816;&#21160;&#21098;&#36753;&#30340;&#26368;&#30456;&#20851;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#20010;&#32039;&#20945;&#32780;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21363;&#19968;&#20010;&#31163;&#25955;&#30340;&#21521;&#37327;&#37327;&#21270;&#30721;&#31354;&#38388;&#12290;&#36890;&#36807;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#20998;&#31867;&#20808;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#31354;&#38388;&#20013;&#30340;&#30721;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20351;&#29992;VQ-VAE&#12290;&#34429;&#28982;&#36825;&#20010;&#20808;&#39564;&#20998;&#24067;&#21487;&#20197;&#36890;&#36807;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#22312;&#23637;&#29616;&#20849;&#24773;&#22238;&#24212;&#21644;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;91.7%&#30340;&#24773;&#20917;&#19979;&#65292;ChatGPT&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24773;&#24863;&#24182;&#20135;&#29983;&#36866;&#24403;&#30340;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2308.03527</link><description>&lt;p&gt;
&#25506;&#32034;ChatGPT&#30340;&#20849;&#24773;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT's Empathic Abilities. (arXiv:2308.03527v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#22312;&#23637;&#29616;&#20849;&#24773;&#22238;&#24212;&#21644;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;91.7%&#30340;&#24773;&#20917;&#19979;&#65292;ChatGPT&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24773;&#24863;&#24182;&#20135;&#29983;&#36866;&#24403;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#36890;&#24120;&#34987;&#29702;&#35299;&#20026;&#20998;&#20139;&#21644;&#29702;&#35299;&#20182;&#20154;&#30340;&#24515;&#24577;&#25110;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#22686;&#21152;&#24212;&#29992;&#65292;&#20363;&#22914;&#20799;&#31461;&#23547;&#27714;&#20316;&#19994;&#24110;&#21161;&#12289;&#20010;&#20154;&#23547;&#27714;&#21307;&#30103;&#24314;&#35758;&#20197;&#21450;&#20154;&#20204;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#26085;&#24120;&#20276;&#20387;&#65292;&#20849;&#24773;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#22312;&#23637;&#29616;&#20849;&#24773;&#22238;&#24212;&#21644;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#65306;(1)&#29702;&#35299;&#21644;&#34920;&#36798;&#24773;&#24863;&#12289;(2)&#24182;&#34892;&#24773;&#24863;&#22238;&#24212;&#20197;&#21450;(3)&#20849;&#24773;&#20010;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#20165;&#22312;&#21508;&#20010;&#20849;&#24773;&#26041;&#38754;&#35780;&#20272;&#20102;ChatGPT&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#34892;&#20026;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#20998;&#26512;&#32842;&#22825;&#26426;&#22120;&#20154;&#20849;&#24773;&#33021;&#21147;&#30340;&#21487;&#33021;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;91.7%&#30340;&#24773;&#20917;&#19979;&#65292;ChatGPT&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24773;&#24863;&#24182;&#20135;&#29983;&#36866;&#24403;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathy is often understood as the ability to share and understand another individual's state of mind or emotion. With the increasing use of chatbots in various domains, e.g., children seeking help with homework, individuals looking for medical advice, and people using the chatbot as a daily source of everyday companionship, the importance of empathy in human-computer interaction has become more apparent. Therefore, our study investigates the extent to which ChatGPT based on GPT-3.5 can exhibit empathetic responses and emotional expressions. We analyzed the following three aspects: (1) understanding and expressing emotions, (2) parallel emotional response, and (3) empathic personality. Thus, we not only evaluate ChatGPT on various empathy aspects and compare it with human behavior but also show a possible way to analyze the empathy of chatbots in general. Our results show, that in 91.7% of the cases, ChatGPT was able to correctly identify emotions and produces appropriate answers. In c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.01921</link><description>&lt;p&gt;
&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#29983;&#29289;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#33647;&#29289;&#20998;&#23376;&#24555;&#36895;&#31579;&#36873;&#26159;&#33647;&#29289;&#21457;&#29616;&#31649;&#32447;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#39640;&#36890;&#37327;&#21644;&#39640;&#20934;&#30830;&#24615;&#20998;&#23376;&#23545;&#25509;&#20195;&#29702;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422;30&#19975;&#31181;&#33647;&#29289;&#20505;&#36873;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22270;&#31070;&#32463;&#25351;&#32441;&#23545;&#25509;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23545;&#22823;&#22810;&#25968;&#23545;&#25509;&#38774;&#28857;&#30340;&#22343;&#26041;&#35823;&#24046;&#20302;&#20110;0.21 kcal/mol&#65292;&#30456;&#27604;&#20256;&#32479;&#22278;&#24418;&#25351;&#32441;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#20026;&#20102;&#20351;&#31070;&#32463;&#25351;&#32441;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#38774;&#28857;&#19978;&#35757;&#32451;&#30340;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#36827;&#34892;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#36816;&#34892;&#12290;&#36825;&#26159;&#23545;Jetson&#24179;&#21488;&#22312;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#30340;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2307.16834</link><description>&lt;p&gt;
&#20351;&#29992;&#31471;&#21040;&#31471;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#26469;&#22522;&#20934;&#27979;&#35797;Jetson&#36793;&#32536;&#35774;&#22791;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#36827;&#34892;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#36816;&#34892;&#12290;&#36825;&#26159;&#23545;Jetson&#24179;&#21488;&#22312;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#24179;&#21488;&#65292;&#29305;&#21035;&#26159;&#30828;&#20214;&#21152;&#36895;&#65292;&#26174;&#30528;&#24433;&#21709;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#21019;&#26032;&#23558;&#20154;&#31867;&#21171;&#21160;&#36716;&#21270;&#20026;&#33258;&#21160;&#21270;&#30340;&#26234;&#33021;&#31995;&#32479;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#29289;&#32852;&#32593;&#21644;&#35768;&#22810;&#20854;&#20182;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;NVIDIA&#30340;Jetson&#24179;&#21488;&#26159;&#22312;&#25191;&#34892;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#33021;&#22815;&#25552;&#20379;&#33021;&#25928;&#21644;&#21534;&#21520;&#29575;&#26368;&#20339;&#24615;&#33021;&#30340;&#20808;&#39537;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#37117;&#26159;&#22522;&#20110;2D&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27599;&#20010;&#27604;&#36739;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#35270;&#39057;&#30340;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#23558;&#35813;&#31995;&#32479;&#37096;&#32626;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#65288;Nano&#12289;AGX Xavier&#12289;Orin Nano&#65289;&#12290;&#27604;&#36739;&#20998;&#26512;&#21253;&#25324;&#23558;Torch-TensorRT&#38598;&#25104;&#20026;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative enhancement in embedded system platforms, specifically hardware accelerations, significantly influence the application of deep learning in real-world scenarios. These innovations translate human labor efforts into automated intelligent systems employed in various areas such as autonomous driving, robotics, Internet-of-Things (IoT), and numerous other impactful applications. NVIDIA's Jetson platform is one of the pioneers in offering optimal performance regarding energy efficiency and throughput in the execution of deep learning algorithms. Previously, most benchmarking analysis was based on 2D images with a single deep learning model for each comparison result. In this paper, we implement an end-to-end video-based crime-scene anomaly detection system inputting from surveillance videos and the system is deployed and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin Nano). The comparison analysis includes the integration of Torch-TensorRT as a softwar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;ASP&#26041;&#27861;&#35299;&#20915;&#20102;&#23454;&#38469;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#23558;&#20854;&#20855;&#20307;&#35201;&#27714;&#24314;&#27169;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#26426;&#22120;&#21152;&#24037;&#12289;&#35774;&#32622;&#12289;&#25209;&#22788;&#29702;&#21644;&#32500;&#25252;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.14799</link><description>&lt;p&gt;
&#28151;&#21512;ASP&#26041;&#27861;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#22810;&#30446;&#26631;&#35843;&#24230;&#65288;&#25193;&#23637;&#29256;&#26412;&#65289;
&lt;/p&gt;
&lt;p&gt;
Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version). (arXiv:2307.14799v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;ASP&#26041;&#27861;&#35299;&#20915;&#20102;&#23454;&#38469;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#23558;&#20854;&#20855;&#20307;&#35201;&#27714;&#24314;&#27169;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#26426;&#22120;&#21152;&#24037;&#12289;&#35774;&#32622;&#12289;&#25209;&#22788;&#29702;&#21644;&#32500;&#25252;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21322;&#23548;&#20307;&#21046;&#36896;&#28041;&#21450;&#22797;&#26434;&#30340;&#29983;&#20135;&#36807;&#31243;&#65292;&#21253;&#25324;&#25968;&#30334;&#20010;&#25805;&#20316;&#65292;&#20174;&#25209;&#27425;&#21457;&#24067;&#21040;&#23436;&#25104;&#21487;&#33021;&#38656;&#35201;&#25968;&#26376;&#26102;&#38388;&#12290;&#36825;&#20123;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#39640;&#31185;&#25216;&#35774;&#22791;&#22810;&#26679;&#21270;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#38454;&#27573;&#19978;&#23545;&#21333;&#20010;&#26230;&#22278;&#12289;&#25209;&#27425;&#25110;&#25209;&#27425;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#38656;&#35201;&#20135;&#21697;&#29305;&#23450;&#30340;&#35774;&#32622;&#21644;&#19987;&#38376;&#30340;&#32500;&#25252;&#31243;&#24207;&#12290;&#36825;&#31181;&#24773;&#20917;&#19982;&#20256;&#32479;&#30340;&#36710;&#38388;&#35843;&#24230;&#22330;&#26223;&#19981;&#21516;&#65292;&#21518;&#32773;&#20855;&#26377;&#36739;&#19981;&#22797;&#26434;&#30340;&#29983;&#20135;&#36807;&#31243;&#21644;&#35774;&#22791;&#65292;&#20027;&#35201;&#20851;&#27880;&#35299;&#20915;&#39640;&#24230;&#32452;&#21512;&#20294;&#25277;&#35937;&#30340;&#35843;&#24230;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24046;&#24322;&#36923;&#36753;&#30340;&#28151;&#21512;ASP&#27169;&#22411;&#26469;&#23545;&#23454;&#38469;&#30340;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#35843;&#24230;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#21253;&#25324;&#28789;&#27963;&#30340;&#26426;&#22120;&#21152;&#24037;&#12289;&#35774;&#32622;&#12289;&#25209;&#22788;&#29702;&#21644;&#32500;&#25252;&#25805;&#20316;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#36138;&#23146;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#29420;&#31435;&#36827;&#34892;&#35843;&#24230;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
Modern semiconductor manufacturing involves intricate production processes consisting of hundreds of operations, which can take several months from lot release to completion. The high-tech machines used in these processes are diverse, operate on individual wafers, lots, or batches in multiple stages, and necessitate product-specific setups and specialized maintenance procedures. This situation is different from traditional job-shop scheduling scenarios, which have less complex production processes and machines, and mainly focus on solving highly combinatorial but abstract scheduling problems. In this work, we address the scheduling of realistic semiconductor manufacturing processes by modeling their specific requirements using hybrid Answer Set Programming with difference logic, incorporating flexible machine processing, setup, batching and maintenance operations. Unlike existing methods that schedule semiconductor manufacturing processes locally with greedy heuristics or by independen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#21365;&#21644;&#24322;&#21365;&#23545;&#27604;&#23398;&#20064;&#65288;IFTCL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#30340;&#27491;&#26679;&#26412;&#23545;&#29983;&#25104;&#26041;&#24335;&#65292;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#20013;&#35299;&#20915;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#35821;&#20041;&#25197;&#26354;&#21644;&#35821;&#20041;&#38388;&#38548;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10932</link><description>&lt;p&gt;
&#21516;&#21365;&#21644;&#24322;&#21365;&#21452;&#32990;&#32974;&#65306;&#21477;&#23376;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations. (arXiv:2307.10932v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#21365;&#21644;&#24322;&#21365;&#23545;&#27604;&#23398;&#20064;&#65288;IFTCL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#30340;&#27491;&#26679;&#26412;&#23545;&#29983;&#25104;&#26041;&#24335;&#65292;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#20013;&#35299;&#20915;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#35821;&#20041;&#25197;&#26354;&#21644;&#35821;&#20041;&#38388;&#38548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27491;&#26679;&#26412;&#19982;&#38170;&#23450;&#26679;&#26412;&#32858;&#31867;&#26469;&#21019;&#24314;&#25152;&#38656;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#20381;&#38752;&#23545;&#27604;&#30446;&#26631;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#26080;&#27861;&#21306;&#20998;&#27491;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#24494;&#23567;&#35821;&#20041;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24120;&#35265;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#32463;&#24120;&#24341;&#20837;&#35821;&#20041;&#25197;&#26354;&#65292;&#23548;&#33268;&#27491;&#26679;&#26412;&#23545;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#38388;&#38548;&#12290;&#34429;&#28982;InfoNCE&#25439;&#22833;&#20989;&#25968;&#24573;&#30053;&#20102;&#35821;&#20041;&#38388;&#38548;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#20248;&#20808;&#32771;&#34385;&#27491;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26368;&#22823;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19981;&#25935;&#24863;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#21365;&#21644;&#24322;&#21365;&#23545;&#27604;&#23398;&#20064;&#65288;&#31216;&#20026;IFTCL&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#30340;&#27491;&#26679;&#26412;&#23545;&#29983;&#25104;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enhancement of unsupervised learning of sentence representations has been significantly achieved by the utility of contrastive learning. This approach clusters the augmented positive instance with the anchor instance to create a desired embedding space. However, relying solely on the contrastive objective can result in sub-optimal outcomes due to its inability to differentiate subtle semantic variations between positive pairs. Specifically, common data augmentation techniques frequently introduce semantic distortion, leading to a semantic margin between the positive pair. While the InfoNCE loss function overlooks the semantic margin and prioritizes similarity maximization between positive pairs during training, leading to the insensitive semantic comprehension ability of the trained model. In this paper, we introduce a novel Identical and Fraternal Twins of Contrastive Learning (named IFTCL) framework, capable of simultaneously adapting to various positive pairs generated by differ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;</title><link>http://arxiv.org/abs/2307.00184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;
&lt;/p&gt;
&lt;p&gt;
Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#39537;&#21160;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#33719;&#24471;&#30340;&#20154;&#26684;&#29305;&#36136;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#20154;&#26684;&#26159;&#20915;&#23450;&#20132;&#27969;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#39564;&#35777;&#30340;&#24515;&#29702;&#27979;&#37327;&#27979;&#35797;&#65292;&#24182;&#23545;&#20174;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#36827;&#34892;&#37327;&#21270;&#12289;&#20998;&#26512;&#21644;&#22609;&#36896;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#26576;&#20123;LLMs&#30340;&#36755;&#20986;&#20013;&#27169;&#25311;&#30340;&#20154;&#26684;&#65288;&#22312;&#29305;&#23450;&#30340;&#25552;&#31034;&#37197;&#32622;&#19979;&#65289;&#26159;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#65307;2&#65289;LLM&#27169;&#25311;&#30340;&#20154;&#26684;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#23545;&#20110;&#26356;&#22823;&#30340;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#26356;&#24378;&#65307;3&#65289;LLM&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#30340;&#32500;&#24230;&#36827;&#34892;&#22609;&#36896;&#65292;&#20197;&#27169;&#20223;&#29305;&#23450;&#30340;&#20154;&#26684;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As LLMs increasingly power conversational agents, the synthesized personality embedded in these models by virtue of their training on large amounts of human-generated data draws attention. Since personality is an important factor determining the effectiveness of communication, we present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used LLMs. We find that: 1) personality simulated in the outputs of some LLMs (under specific prompting configurations) is reliable and valid; 2) evidence of reliability and validity of LLM-simulated personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific personality profiles. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21253;&#21547;&#27491;&#36127;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#27169;&#22411;&#20135;&#29983;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2306.14565</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#25351;&#20196;&#35843;&#25972;&#26469;&#20943;&#36731;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. (arXiv:2306.14565v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21253;&#21547;&#27491;&#36127;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#27169;&#22411;&#20135;&#29983;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#20219;&#21153;&#21462;&#24471;&#20102;&#21487;&#21916;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#24456;&#23481;&#26131;&#22312;&#25551;&#36848;&#22270;&#20687;&#21644;&#20154;&#31867;&#25351;&#20196;&#26102;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#24187;&#35273;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;LRV-Instruction&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#30001;GPT4&#29983;&#25104;&#30340;12&#19975;&#20010;&#35270;&#35273;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;16&#20010;&#24320;&#25918;&#24335;&#25351;&#20196;&#21644;&#31572;&#26696;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20219;&#21153;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#27491;&#25351;&#20196;&#26679;&#26412;&#19981;&#21516;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LRV-Instruction&#20197;&#21253;&#21547;&#26356;&#22810;&#38024;&#23545;&#26356;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#30340;&#27491;&#36127;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#36127;&#25351;&#20196;&#22312;&#20004;&#20010;&#35821;&#20041;&#23618;&#27425;&#19978;&#35774;&#35745;&#65306;&#65288;i&#65289;&#19981;&#23384;&#22312;&#20803;&#32032;&#25805;&#20316;&#21644;&#65288;ii&#65289;&#23384;&#22312;&#20803;&#32032;&#25805;&#20316;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#34913;&#37327;LMM&#25152;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;GPT4&#36741;&#21161;&#30340;&#35270;&#35273;&#25351;&#20196;&#35780;&#20272;&#65288;GAVIE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel approach to eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05659</link><description>&lt;p&gt;
COVER&#65306;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#20687;&#23569;&#37327;&#26679;&#26412;&#22330;&#26223;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;PLMs&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22312;&#22522;&#20110;&#27169;&#26495;&#30340;&#25552;&#31034;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#22312;&#30340;&#28431;&#27934;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#24341;&#36215;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#25552;&#20986;&#22522;&#20110;&#25552;&#31034;&#30340;&#23545;&#25239;&#25915;&#20987;&#25163;&#27573;&#65292;&#25581;&#31034;&#20102;PLMs&#30340;&#19968;&#20123;&#28431;&#27934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23383;&#31526;&#32423;&#21035;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#30772;&#22351;&#25163;&#21160;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#19978;&#36848;&#21551;&#21457;&#24335;&#30772;&#22351;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;BERT&#31995;&#21015;&#27169;&#22411;&#30340;&#19977;&#20010;&#21464;&#31181;&#21644;&#20843;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20219;&#21153;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#36164;&#28304;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;&#20854;&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#21644;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM LLaMA-7B &#27169;&#22411;&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.05064</link><description>&lt;p&gt;
&#23398;&#20064;&#22320;&#29699;&#31185;&#23398;&#30693;&#35782;&#29702;&#35299;&#21644;&#21033;&#29992;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization. (arXiv:2306.05064v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#36164;&#28304;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;&#20854;&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#21644;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM LLaMA-7B &#27169;&#22411;&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24120;&#35268;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26412;&#25991;&#23558;LLM&#24341;&#20837;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#65292;&#26088;&#22312;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;LLM&#65292;&#21629;&#21517;&#20026;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#36164;&#28304;&#65292;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;LLM&#22312;&#22320;&#29699;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20026;LLM&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#65292;&#26088;&#22312;&#23558;LLM&#30456;&#24212;&#19982;&#22320;&#29699;&#31185;&#23398;&#30456;&#20851;&#30340;&#29992;&#25143;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22320;&#36136;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#65292;&#20197;&#22312;&#22320;&#29699;&#31185;&#23398;&#29615;&#22659;&#20013;&#35780;&#20272;LLM&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#36229;&#36807;100&#19975;&#31687;&#22320;&#29699;&#31185;&#23398;&#25991;&#29486;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;&#20102;LLaMA-7B&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;GeoSignal&#30340;&#30417;&#30563;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36801;&#31227;LLM&#30340;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs)have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience, with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBenchmark, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pretrained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on over 1 million pieces of geoscience literature and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can e
&lt;/p&gt;</description></item><item><title>EmbodiedGPT&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#39044;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#36171;&#20104;&#20855;&#26377;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#25191;&#34892;&#33021;&#21147;&#30340;&#23454;&#20307;&#20195;&#29702;&#20154;&#12290;</title><link>http://arxiv.org/abs/2305.15021</link><description>&lt;p&gt;
EmbodiedGPT: &#36890;&#36807;&#24605;&#32500;&#38142;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. (arXiv:2305.15021v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15021
&lt;/p&gt;
&lt;p&gt;
EmbodiedGPT&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#39044;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#36171;&#20104;&#20855;&#26377;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#25191;&#34892;&#33021;&#21147;&#30340;&#23454;&#20307;&#20195;&#29702;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36171;&#20104;&#20855;&#26377;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#25191;&#34892;&#33021;&#21147;&#30340;&#23454;&#20307;&#20195;&#29702;&#20154;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;EmbodiedGPT&#65292;&#29992;&#20110;&#23454;&#20307;&#26234;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20197;&#19979;&#21162;&#21147;&#65306;&#65288;i&#65289;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23884;&#20837;&#24335;&#35268;&#21010;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;EgoCOT&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;Ego4D&#25968;&#25454;&#38598;&#20013;&#31934;&#36873;&#30340;&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#39640;&#36136;&#37327;&#35821;&#35328;&#25351;&#20196;&#32452;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#27169;&#24335;&#29983;&#25104;&#19968;&#31995;&#21015;&#23376;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#23884;&#20837;&#24335;&#35268;&#21010;&#12290;&#65288;ii&#65289;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#31181;&#39640;&#36136;&#37327;&#35745;&#21010;&#29983;&#25104;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#32512;&#35843;&#20248;&#23558;7B&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35843;&#25972;&#21040;EgoCOT&#25968;&#25454;&#38598;&#19978;&#12290;&#65288;iii&#65289;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;LLM&#29983;&#25104;&#30340;&#35745;&#21010;&#20013;&#25552;&#21462;&#19982;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#30340;&#33539;&#20363;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts" mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated pla
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11322</link><description>&lt;p&gt;
SpikeCP: &#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#24310;&#36831;&#33258;&#36866;&#24212;&#21487;&#38752;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#36890;&#36807;&#20869;&#37096;&#20107;&#20214;&#39537;&#21160;&#30340;&#31070;&#32463;&#21160;&#24577;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#33021;&#37327;&#28040;&#32791;&#21462;&#20915;&#20110;&#36755;&#20837;&#28436;&#31034;&#26399;&#38388;&#31070;&#32463;&#20803;&#20043;&#38388;&#20132;&#25442;&#30340;&#33033;&#20914;&#25968;&#37327;&#12290;&#22312;&#20856;&#22411;&#30340;SNN&#20998;&#31867;&#22120;&#23454;&#29616;&#20013;&#65292;&#20915;&#31574;&#26159;&#22312;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#34987;&#22788;&#29702;&#21518;&#20135;&#29983;&#30340;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#27700;&#24179;&#22312;&#36755;&#20837;&#20043;&#38388;&#26159;&#30456;&#23545;&#22343;&#21248;&#30340;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;SNN&#21487;&#26681;&#25454;&#27599;&#20010;&#31034;&#20363;&#30340;&#38590;&#24230;&#26469;&#23450;&#21046;&#25512;&#26029;&#24310;&#36831; - &#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#30340;&#33021;&#32791; - &#36890;&#36807;&#22312;SNN&#27169;&#22411;&#36275;&#22815;&#8220;&#33258;&#20449;&#8221;&#26102;&#20135;&#29983;&#26089;&#26399;&#20915;&#31574;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
&lt;/p&gt;</description></item><item><title>PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.10403</link><description>&lt;p&gt;
PaLM 2 &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10403
&lt;/p&gt;
&lt;p&gt;
PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; PaLM 2&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#27604;&#20854;&#21069;&#36523; PaLM &#22312;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26356;&#21152;&#20986;&#33394;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;PaLM 2 &#26159;&#19968;&#31181;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#20197;&#21450;&#25512;&#29702;&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; PaLM 2 &#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#65292;&#21516;&#26102;&#23637;&#29616;&#20102;&#27604; PaLM &#26356;&#24555;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#30340;&#25928;&#29575;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#21516;&#26102;&#20063;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24555;&#22320;&#21709;&#24212;&#65292;&#20197;&#33719;&#24471;&#26356;&#33258;&#28982;&#30340;&#20132;&#20114;&#33410;&#22863;&#12290;PaLM 2 &#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312; BIG-Bench &#21644;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110; PaLM &#26377;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;PaLM 2 &#22312;&#19968;&#22871;&#36127;&#36131;&#20154;&#30340; AI &#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#38468;&#21152;&#36816;&#34892;&#24320;&#38144;&#25110;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23545;&#27602;&#24615;&#36827;&#34892;&#25512;&#29702;&#26102;&#38388;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Over
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#24207;&#21015;&#20998;&#35299;&#65292;&#36890;&#36807;&#20998;&#26512;&#31243;&#24207;&#20043;&#38388;&#30340;Green&#20851;&#31995;&#65292;&#20026;&#36923;&#36753;&#32534;&#31243;&#20195;&#25968;&#29702;&#35770;&#36827;&#19968;&#27493;&#21457;&#23637;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.13522</link><description>&lt;p&gt;
&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#24207;&#21015;&#20998;&#35299;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sequential decomposition of propositional logic programs. (arXiv:2304.13522v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13522
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#24207;&#21015;&#20998;&#35299;&#65292;&#36890;&#36807;&#20998;&#26512;&#31243;&#24207;&#20043;&#38388;&#30340;Green&#20851;&#31995;&#65292;&#20026;&#36923;&#36753;&#32534;&#31243;&#20195;&#25968;&#29702;&#35770;&#36827;&#19968;&#27493;&#21457;&#23637;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#20102;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#24207;&#21015;&#32452;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31243;&#24207;&#20043;&#38388;&#30340; Green &#20851;&#31995; $\mathcal{L,R,J}$&#65292;&#20174;&#32780;&#30740;&#31350;&#31243;&#24207;&#30340;&#24207;&#21015;&#20998;&#35299;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#65292;&#26412;&#25991;&#26159;&#36923;&#36753;&#32534;&#31243;&#20195;&#25968;&#29702;&#35770;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sequential composition of propositional logic programs has been recently introduced. This paper studies the sequential {\em decomposition} of programs by studying Green's relations $\mathcal{L,R,J}$ -- well-known in semigroup theory -- between programs. In a broader sense, this paper is a further step towards an algebraic theory of logic programming.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38754;&#20020;&#30528;&#30693;&#35782;&#21644;&#21160;&#26426;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#27010;&#24565;&#21270;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#24184;&#31119;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;AI&#34892;&#21160;&#65292;&#20197;&#21450;&#28608;&#21169;&#25514;&#26045;&#12289;&#36130;&#21153;&#21644;&#23459;&#20256;&#39118;&#38505;&#30340;&#19981;&#19968;&#33268;&#20197;&#21450;&#25968;&#25454;&#33719;&#21462;&#30340;&#32570;&#20047;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#31185;&#23398;&#29702;&#35299;AI&#31995;&#32479;&#23545;&#24184;&#31119;&#24433;&#21709;&#26041;&#38754;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25351;&#23548;&#35774;&#35745;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.12241</link><description>&lt;p&gt;
&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#65306;&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Positive AI: Key Challenges for Designing Wellbeing-aligned Artificial Intelligence. (arXiv:2304.12241v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12241
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38754;&#20020;&#30528;&#30693;&#35782;&#21644;&#21160;&#26426;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#27010;&#24565;&#21270;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#24184;&#31119;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;AI&#34892;&#21160;&#65292;&#20197;&#21450;&#28608;&#21169;&#25514;&#26045;&#12289;&#36130;&#21153;&#21644;&#23459;&#20256;&#39118;&#38505;&#30340;&#19981;&#19968;&#33268;&#20197;&#21450;&#25968;&#25454;&#33719;&#21462;&#30340;&#32570;&#20047;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#31185;&#23398;&#29702;&#35299;AI&#31995;&#32479;&#23545;&#24184;&#31119;&#24433;&#21709;&#26041;&#38754;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25351;&#23548;&#35774;&#35745;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#36805;&#36895;&#25913;&#21464;&#31038;&#20250;&#65292;&#36843;&#20999;&#38656;&#35201;&#30830;&#20445;&#20854;&#31215;&#26497;&#24433;&#21709;&#12290;&#26412;&#25991;&#37319;&#29992;&#31215;&#26497;&#35774;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#20854;&#35270;&#20026;&#35774;&#35745;&#20027;&#21160;&#25903;&#25345;&#20154;&#31867;&#24184;&#31119;&#30340;AI&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;AI&#31995;&#32479;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#37319;&#29992;&#25511;&#21046;&#35770;&#30340;&#35270;&#35282;&#65292;&#35782;&#21035;&#20102;&#20004;&#20010;&#31867;&#21035;&#20013;&#30340;&#21313;&#20108;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#30693;&#35782;&#32570;&#20047;&#21644;&#21160;&#26426;&#32570;&#20047;&#12290;&#30693;&#35782;&#38556;&#30861;&#21253;&#25324;&#27010;&#24565;&#21270;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#24184;&#31119;&#65292;&#24182;&#35774;&#35745;&#36866;&#24403;&#30340;AI&#34892;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#21160;&#26426;&#38556;&#30861;&#21253;&#25324;&#19981;&#19968;&#33268;&#30340;&#28608;&#21169;&#25514;&#26045;&#12289;&#36130;&#21153;&#21644;&#23459;&#20256;&#39118;&#38505;&#65292;&#20197;&#21450;&#32570;&#20047;&#25968;&#25454;&#33719;&#21462;&#38459;&#27490;&#20102;&#65288;&#31532;&#19977;&#26041;&#65289;&#23545;&#24184;&#31119;&#36827;&#34892;&#30740;&#31350;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#35758;&#31243;&#65292;&#21253;&#25324;&#25512;&#36827;&#23545;AI&#31995;&#32479;&#23545;&#24184;&#31119;&#24433;&#21709;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#24182;&#25351;&#23548;&#22914;&#20309;&#36827;&#34892;AI&#31995;&#32479;&#30340;&#35774;&#35745;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is rapidly transforming society, creating an urgent need to ensure its positive impact. In this article, we take a positive design approach towards this issue, viewing it as a matter of designing AI systems that actively support human wellbeing. However, designing wellbeing-aligned AI systems is difficult. This article adopts a cybernetic perspective to identify twelve key challenges across two categories: lack of knowledge and lack of motivation. Knowledge barriers include challenges in conceptualizing, measuring, and optimizing for wellbeing, then designing appropriate AI actions. Motivation barriers include misaligned incentives, financial and publicity risks, and a lack of data access preventing (third-party) research on wellbeing. To address these challenges we have captured our key takeaways in a research agenda related to 1) advancing the scientific understanding of the impact of AI systems on wellbeing, and 2) guiding design actions on how AI system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;(MAE-CT)&#65292;&#21033;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#22312;&#26631;&#35760;&#25968;&#25454;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19979;&#28216;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#22270;&#20687;&#30340;&#20016;&#23500;&#29305;&#24449;&#32858;&#31867;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.10520</link><description>&lt;p&gt;
&#23545;&#27604;&#35843;&#33410;: &#24110;&#21161;&#36951;&#24536;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#19968;&#28857;&#23567;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;(MAE-CT)&#65292;&#21033;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#22312;&#26631;&#35760;&#25968;&#25454;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19979;&#28216;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#22270;&#20687;&#30340;&#20016;&#23500;&#29305;&#24449;&#32858;&#31867;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#22914;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36755;&#20837;&#30340;&#20016;&#23500;&#34920;&#31034;&#12290;&#20294;&#26159;&#65292;&#20026;&#20102;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#20016;&#23500;&#30340;&#29305;&#24449;&#19981;&#20165;&#25429;&#33719;&#20102;&#23545;&#35937;&#32780;&#19988;&#36824;&#21253;&#25324;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#32972;&#26223;&#65292;&#22240;&#27492;&#23427;&#20204;&#38656;&#35201;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23454;&#20363;&#36776;&#21035;&#26041;&#27861;&#20391;&#37325;&#20110;&#23545;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23558;MIM&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#19982;ID&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#32570;&#23569;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#19979;&#28216;&#20998;&#31867;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;&#65288;MAE-CT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39034;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#24212;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;MAE&#12290;MAE-CT&#35843;&#25972;&#20102;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#24418;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#26631;&#31614;&#12290;&#24212;&#29992;&#20110;&#22823;&#22411;&#21644;&#24040;&#22411;Vision Transformer&#65288;ViT&#65289;&#27169;&#22411;&#26102;&#65292;MAE-CT&#22312;&#32447;&#24615;&#25506;&#27979;&#65292;k-&#22343;&#20540;&#32858;&#31867;&#21644;&#21322;&#30417;&#30563;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#21305;&#37197;&#25110;&#36229;&#36234;&#20102;&#22312;ImageNet&#19978;&#35757;&#32451;&#30340;&#20808;&#21069;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features capture not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that applies Nearest Neighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CT matches or excels previous self-supervised methods trained on ImageNet in linear probing, k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09960</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#28508;&#22312;&#31354;&#38388;&#29702;&#35770;&#23545;&#24212;&#26032;&#20852;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24182;&#19981;&#26159;&#38543;&#26426;&#29983;&#25104;&#65292;&#32780;&#26159;&#20026;&#20102;&#20256;&#36882;&#20449;&#24687;&#12290;&#35821;&#35328;&#19982;&#20854;&#24213;&#23618;&#21547;&#20041;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#20851;&#32852;&#65292;&#22312;&#20854;&#30456;&#20851;&#24615;&#26041;&#38754;&#26377;&#30528;&#20005;&#37325;&#20559;&#24046;&#30340;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31232;&#30095;&#24615;&#65292;&#36825;&#20123;&#39640;&#23792;&#20540;&#24688;&#22909;&#19982;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#21305;&#37197;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#22823;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;LLMs&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#25506;&#32034;&#32852;&#21512;&#20998;&#24067;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#26377;&#25928;&#25512;&#29702;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#20998;&#31867;&#20026;&#26126;&#30830;&#19982;{\epsilon}-&#27169;&#31946;&#65292;&#24182;&#25552;&#20986;&#23450;&#37327;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#65289;&#37117;&#21487;&#20197;&#24402;&#22240;&#20110;&#23545;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#32452;&#21512;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#22312;&#33258;&#26059;&#29627;&#29827;&#22522;&#24577;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#22914;&#27169;&#25311;&#36864;&#28779;&#25110;&#24182;&#34892;&#36864;&#28779;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#20379;&#30456;&#24403;&#36136;&#37327;&#30340;&#32467;&#26524;&#20043;&#21069;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2302.10848</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#33258;&#26059;&#29627;&#29827;&#22522;&#24577;&#38382;&#39064;&#19978;&#30340;&#27979;&#35797;&#65306;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture. (arXiv:2302.10848v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10848
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#32452;&#21512;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#22312;&#33258;&#26059;&#29627;&#29827;&#22522;&#24577;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#22914;&#27169;&#25311;&#36864;&#28779;&#25110;&#24182;&#34892;&#36864;&#28779;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#20379;&#30456;&#24403;&#36136;&#37327;&#30340;&#32467;&#26524;&#20043;&#21069;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Changjun Fan&#31561;&#20154;&#30340;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#32452;&#21512;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20182;&#20204;&#38024;&#23545;&#20960;&#20010;&#33258;&#26059;&#29627;&#29827;&#22522;&#24577;&#38382;&#39064;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#20854;&#20013;&#38750;&#24179;&#38754;&#32593;&#32476;&#19978;&#30340;&#23454;&#20363;&#36890;&#24120;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#19982;&#20960;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22914;&#27169;&#25311;&#36864;&#28779;&#65288;SA&#65289;&#25110;&#24182;&#34892;&#36864;&#28779;&#65288;PT&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#30456;&#23545;&#20110;SA&#25110;PT&#25913;&#36827;&#20102;&#32467;&#26524;&#65292;&#25110;&#32773;&#33267;&#23569;&#22312;&#33719;&#24471;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#36136;&#37327;&#30340;&#32467;&#26524;&#20043;&#21069;&#65292;&#20943;&#23569;&#20102;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#20026;&#20102;&#35777;&#26126;&#20182;&#20204;&#30340;&#26041;&#27861;&#8220;&#20248;&#36234;&#8221;&#65292;&#20316;&#32773;&#37319;&#21462;&#20102;&#20004;&#31181;&#22522;&#26412;&#31574;&#30053;&#65306;&#65288;1&#65289;&#35843;&#29992;&#21830;&#19994;GUROBI&#27714;&#35299;&#22120;&#33719;&#21462;&#19968;&#20123;&#31934;&#30830;&#22522;&#24577;&#26679;&#26412;&#20316;&#20026;&#27979;&#35797;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23558;&#20182;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#31574;&#30053;&#36827;&#34892;&#20102;&#30452;&#25509;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Changjun Fan et al. [Nature Communications https://doi.org/10.1038/s41467-023-36363-w (2023)], the authors present a deep reinforced learning approach to augment combinatorial optimization heuristics. In particular, they present results for several spin glass ground state problems, for which instances on non-planar networks are generally NP-hard, in comparison with several Monte Carlo based methods, such as simulated annealing (SA) or parallel tempering (PT). Indeed, those results demonstrate that the reinforced learning improves the results over those obtained with SA or PT, or at least allows for reduced runtimes for the heuristics before results of comparable quality have been obtained relative to those other methods. To facilitate the conclusion that their method is ''superior'', the authors pursue two basic strategies: (1) A commercial GUROBI solver is called on to procure a sample of exact ground states as a testbed to compare with, and (2) a head-to-head comparison between th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07260</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20808;&#39564;&#32593;&#32476;&#30340;&#39640;&#32500;&#36755;&#20986;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#28041;&#21450;&#21040;&#26410;&#30693;&#30340;&#39640;&#32500;&#24230;&#26144;&#23556;&#19968;&#32452;&#21487;&#25511;&#21464;&#37327;&#21040;&#26114;&#36149;&#23454;&#39564;&#32467;&#26524;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#20219;&#21153;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#26102;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24403;&#22788;&#29702;&#39640;&#32500;&#36755;&#20986;&#26102;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#32500;&#24230;&#20027;&#35201;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#20030;&#38598;&#25104;&#30340;BO&#21644;&#24207;&#36143;&#20915;&#31574;&#21046;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#20307;&#31995;&#32467;&#26500;&#36873;&#25321;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#36817;&#20284;&#35774;&#35745;&#21464;&#37327;&#21644;&#24863;&#20852;&#36259;&#37327;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#21363;&#20351;&#22312;&#21518;&#32773;&#21462;&#20540;&#20110;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#25110;&#29978;&#33267;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22788;&#29702;&#39640;&#32500;&#24230;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>TargetCall&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#65292;&#28040;&#38500;&#20102;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#65292;&#25552;&#39640;&#20102;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.04953</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#28040;&#38500;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#30340;TargetCall
&lt;/p&gt;
&lt;p&gt;
TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering. (arXiv:2212.04953v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04953
&lt;/p&gt;
&lt;p&gt;
TargetCall&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#65292;&#28040;&#38500;&#20102;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#65292;&#25552;&#39640;&#20102;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Basecalling&#26159;&#32435;&#31859;&#23380;&#27979;&#24207;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#23427;&#23558;&#32435;&#31859;&#23380;&#27979;&#24207;&#20202;&#30340;&#21407;&#22987;&#20449;&#21495;&#36716;&#25442;&#20026;&#26680;&#37240;&#24207;&#21015;&#65292;&#21363;reads&#12290;&#26368;&#20808;&#36827;&#30340;basecallers&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#39640;&#24230;&#30340;basecalling&#20934;&#30830;&#24615;&#12290;&#36825;&#20351;&#24471;basecalling&#22312;&#35745;&#31639;&#19978;&#25928;&#29575;&#20302;&#19979;&#19988;&#20869;&#23384;&#28040;&#32791;&#22823;&#65292;&#25104;&#20026;&#25972;&#20010;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#26469;&#35828;&#65292;&#22823;&#22810;&#25968;reads&#19982;&#24863;&#20852;&#36259;&#30340;&#21442;&#32771;&#22522;&#22240;&#32452;&#19981;&#21305;&#37197;&#65288;&#21363;&#30446;&#26631;&#21442;&#32771;&#22522;&#22240;&#32452;&#65289;&#65292;&#22240;&#27492;&#20250;&#22312;&#21518;&#32493;&#30340;&#22522;&#22240;&#32452;&#27969;&#31243;&#27493;&#39588;&#20013;&#34987;&#20002;&#24323;&#65292;&#28010;&#36153;&#20102;basecalling&#30340;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TargetCall&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#28040;&#38500;basecalling&#20013;&#28010;&#36153;&#35745;&#31639;&#30340;&#39044;&#22522;&#35843;&#36807;&#28388;&#22120;&#12290;TargetCall&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;basecalling&#20043;&#21069;&#20002;&#24323;&#19981;&#20250;&#19982;&#30446;&#26631;&#21442;&#32771;&#22522;&#22240;&#32452;&#21305;&#37197;&#30340;reads&#65288;&#21363;&#38750;&#30446;&#26631;reads&#65289;&#12290;TargetCall&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;1&#65289;LightCall&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#31070;&#32463;&#32593;&#32476;basecaller&#65292;&#20135;&#29983;&#22122;&#22768;reads&#65307;
&lt;/p&gt;
&lt;p&gt;
Basecalling is an essential step in nanopore sequencing analysis where the raw signals of nanopore sequencers are converted into nucleotide sequences, i.e., reads. State-of-the-art basecallers employ complex deep learning models to achieve high basecalling accuracy. This makes basecalling computationally-inefficient and memory-hungry; bottlenecking the entire genome analysis pipeline. However, for many applications, the majority of reads do no match the reference genome of interest (i.e., target reference) and thus are discarded in later steps in the genomics pipeline, wasting the basecalling computation. To overcome this issue, we propose TargetCall, the first pre-basecalling filter to eliminate the wasted computation in basecalling. TargetCall's key idea is to discard reads that will not match the target reference (i.e., off-target reads) prior to basecalling. TargetCall consists of two main components: (1) LightCall, a lightweight neural network basecaller that produces noisy reads;
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#32780;&#19981;&#20381;&#36182;&#20110;&#22870;&#21169;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#20445;&#25252;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2211.10851</link><description>&lt;p&gt;
&#22870;&#21169;&#24182;&#38750;&#24517;&#35201;&#65306;&#22914;&#20309;&#20026;&#32456;&#36523;&#23398;&#20064;&#21019;&#24314;&#19968;&#20010;&#32452;&#21512;&#24615;&#33258;&#25105;&#20445;&#25252;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10851
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#32780;&#19981;&#20381;&#36182;&#20110;&#22870;&#21169;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#20445;&#25252;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#35748;&#20026;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36991;&#20813;&#24809;&#32602;&#26159;&#35299;&#37322;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#29983;&#20013;&#65292;&#29983;&#29289;&#38656;&#35201;&#23398;&#20064;&#20851;&#20110;&#19990;&#30028;&#32467;&#26500;&#30340;&#35768;&#22810;&#19981;&#21516;&#26041;&#38754;&#65306;&#19990;&#30028;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#31227;&#21160;&#21147;&#23398;&#12290;&#38543;&#30528;&#26234;&#33021;&#20307;&#34701;&#20837;&#26032;&#30693;&#35782;&#65292;&#29366;&#24577;&#32452;&#21512;&#30340;&#25968;&#37327;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#19988;&#23545;&#20110;&#32473;&#23450;&#30340;&#29366;&#24577;&#32452;&#21512;&#65292;&#27809;&#26377;&#26126;&#26174;&#23450;&#20041;&#30340;&#39044;&#35774;&#22870;&#21169;&#25110;&#25104;&#26412;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#21152;&#26435;&#38656;&#35201;&#22312;&#26234;&#33021;&#20307;&#22312;&#19990;&#30028;&#20013;&#30340;&#32463;&#39564;&#20043;&#21069;&#23545;&#22909;&#30340;&#21644;&#22351;&#30340;&#32452;&#21512;&#36827;&#34892;&#32534;&#30721;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#20013;&#24320;&#21457;&#26356;&#33258;&#28982;&#30340;&#34892;&#20026;&#21644;&#21160;&#26426;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#65288;&#21363;&#36171;&#20104;&#33021;&#21147;&#65289;&#26159;&#21487;&#33021;&#30340;&#65292;&#35813;&#26631;&#20934;&#34913;&#37327;&#26234;&#33021;&#20307;&#22312;&#36716;&#31227;&#25805;&#20316;&#32773;&#19979;&#23454;&#29616;&#35768;&#22810;&#21487;&#33021;&#26410;&#26469;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36171;&#20104;&#33021;&#21147;&#25193;&#23637;&#21040;&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning views the maximization of rewards and avoidance of punishments as central to explaining goal-directed behavior. However, over a life, organisms will need to learn about many different aspects of the world's structure: the states of the world and state-vector transition dynamics. The number of combinations of states grows exponentially as an agent incorporates new knowledge, and there is no obvious weighted combination of pre-existing rewards or costs defined for a given combination of states, as such a weighting would need to encode information about good and bad combinations prior to an agent's experience in the world. Therefore, we must develop more naturalistic accounts of behavior and motivation in large state-spaces. We show that it is possible to use only the intrinsic motivation metric of empowerment, which measures the agent's capacity to realize many possible futures under a transition operator. We propose to scale empowerment to hierarchical state-space
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DisenPOI&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#22270;&#30340;POI&#25512;&#33616;&#35299;&#24320;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39034;&#24207;&#21644;&#22320;&#29702;&#20851;&#31995;&#24182;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35299;&#24320;&#36825;&#20004;&#31181;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16591</link><description>&lt;p&gt;
DisenPOI: &#35299;&#24320;&#39034;&#24207;&#21644;&#22320;&#29702;&#24433;&#21709;&#30340;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
DisenPOI: Disentangling Sequential and Geographical Influence for Point-of-Interest Recommendation. (arXiv:2210.16591v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DisenPOI&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#22270;&#30340;POI&#25512;&#33616;&#35299;&#24320;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39034;&#24207;&#21644;&#22320;&#29702;&#20851;&#31995;&#24182;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35299;&#24320;&#36825;&#20004;&#31181;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
POI&#65288;&#20852;&#36259;&#28857;&#65289;&#25512;&#33616;&#22312;&#21508;&#31181;&#20301;&#32622;&#24863;&#30693;&#26381;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24050;&#32463;&#35266;&#23519;&#21040;POI&#25512;&#33616;&#21463;&#21040;&#39034;&#24207;&#21644;&#22320;&#29702;&#24433;&#21709;&#30340;&#39537;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#27809;&#26377;&#26126;&#30830;&#25351;&#23450;&#20027;&#23548;&#24433;&#21709;&#30340;&#27880;&#37322;&#26631;&#31614;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20250;&#28151;&#28102;&#36825;&#20004;&#31181;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#25512;&#33616;&#24615;&#33021;&#21644;&#24046;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;DisenPOI&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#22270;&#30340;POI&#25512;&#33616;&#35299;&#24320;&#26694;&#26550;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;DisenPOI&#22312;&#20004;&#20010;&#29420;&#31435;&#30340;&#22270;&#19978;&#21516;&#26102;&#21033;&#29992;&#39034;&#24207;&#21644;&#22320;&#29702;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#35299;&#24320;&#36825;&#20004;&#31181;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#39034;&#24207;&#21644;&#22320;&#29702;&#24433;&#21709;&#30340;&#35299;&#24320;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#31614;&#21040;&#24207;&#21015;&#26500;&#24314;&#20102;&#19968;&#20010;&#22320;&#29702;&#22270;&#21644;&#19968;&#20010;&#39034;&#24207;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point-of-Interest (POI) recommendation plays a vital role in various location-aware services. It has been observed that POI recommendation is driven by both sequential and geographical influences. However, since there is no annotated label of the dominant influence during recommendation, existing methods tend to entangle these two influences, which may lead to sub-optimal recommendation performance and poor interpretability. In this paper, we address the above challenge by proposing DisenPOI, a novel Disentangled dual-graph framework for POI recommendation, which jointly utilizes sequential and geographical relationships on two separate graphs and disentangles the two influences with self-supervision. The key novelty of our model compared with existing approaches is to extract disentangled representations of both sequential and geographical influences with contrastive learning. To be specific, we construct a geographical graph and a sequential graph based on the check-in sequence of a 
&lt;/p&gt;</description></item><item><title>ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.05845</link><description>&lt;p&gt;
ConSpec: &#31361;&#20986;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05845
&lt;/p&gt;
&lt;p&gt;
ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#25104;&#21151;&#24448;&#24448;&#21462;&#20915;&#20110;&#22810;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#22312;&#26102;&#38388;&#19978;&#30456;&#36317;&#36739;&#36828;&#65292;&#19982;&#26368;&#32456;&#22870;&#21169;&#20063;&#30456;&#36317;&#29978;&#36828;&#12290;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20381;&#36182;Bellman&#26041;&#31243;&#65292;&#24456;&#38590;&#35782;&#21035;&#36825;&#20123;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20851;&#38190;&#27493;&#39588;&#12290;&#36825;&#20010;&#31639;&#27861;&#34987;&#31216;&#20026;&#23545;&#27604;&#20869;&#30465;&#65288;ConSpec&#65289;&#65292;&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;ConSpec&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#19982;&#36825;&#20123;&#21407;&#22411;&#20043;&#19968;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#12290;ConSpec&#20013;&#30340;&#21407;&#22411;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;&#23427;&#20204;&#20351;&#24471;&#33021;&#22815;&#36805;&#36895;&#35782;&#21035;&#25152;&#26377;&#20851;&#38190;&#27493;&#39588;&#65307;&#65288;2&#65289;&#23427;&#20204;&#20197;&#23481;&#26131;&#35299;&#37322;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20351;&#24471;&#22312;&#24863;&#35273;&#29305;&#24449;&#25913;&#21464;&#26102;&#21487;&#20197;&#36827;&#34892;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#12290;&#19982;&#20854;&#20182;&#24403;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04688</link><description>&lt;p&gt;
BAFFLE: &#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#35797;&#38169;&#32463;&#39564;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;RL&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;RL&#33539;&#20363;&#65292;&#22240;&#20026;&#23427;&#33410;&#30465;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447;RL&#20013;&#65292;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22823;&#35268;&#27169;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20182;&#20154;&#21487;&#20197;&#22312;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#33539;&#20363;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#23569;&#20851;&#27880;&#30740;&#31350;&#31163;&#32447;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#20851;&#27880;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#20123;&#25200;&#21160;&#34987;&#28155;&#21152;&#21040;&#25968;&#25454;&#65288;&#35266;&#27979;&#20540;&#65289;&#20013;&#65292;&#20351;&#24471;&#22312;&#32473;&#23450;&#27491;&#24120;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#37319;&#21462;&#39640;&#22870;&#21169;&#30340;&#21160;&#20316;&#65292;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAFFLE&#65288;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
&lt;/p&gt;</description></item><item><title>LambdaKG&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#12289;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2210.00305</link><description>&lt;p&gt;
LambdaKG:&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;
&lt;/p&gt;
&lt;p&gt;
LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings. (arXiv:2210.00305v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00305
&lt;/p&gt;
&lt;p&gt;
LambdaKG&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#12289;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36890;&#24120;&#20855;&#26377;&#24322;&#26500;&#30340;&#22270;&#32467;&#26500;&#21644;&#25991;&#26412;&#20016;&#23500;&#30340;&#23454;&#20307;/&#20851;&#31995;&#20449;&#24687;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;KG&#23884;&#20837;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#25551;&#36848;&#36827;&#34892;&#32534;&#30721;&#26469;&#34920;&#31034;&#23454;&#20307;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#19987;&#38376;&#20026;PLM&#19982;KG&#35774;&#35745;&#30340;&#24320;&#28304;&#24211;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LambdaKG&#65292;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;BART&#65292;T5&#65292;GPT-3&#65289;&#24182;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#38382;&#31572;&#65292;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#65289;&#30340;KGE&#24211;&#12290;LambdaKG&#22312;https://github.com/zjunlp/PromptKG/tree/main/lambdaKG&#19978;&#20844;&#24320;&#24320;&#28304;&#65292;&#24182;&#25552;&#20379;&#20102;&#28436;&#31034;&#35270;&#39057;&#21644;&#38271;&#26399;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph structure and text-rich entity/relation information. Text-based KG embeddings can represent entities by encoding descriptions with pre-trained language models, but no open-sourced library is specifically designed for KGs with PLMs at present. In this paper, we present LambdaKG, a library for KGE that equips with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and supports various tasks (e.g., knowledge graph completion, question answering, recommendation, and knowledge probing). LambdaKG is publicly open-sourced at https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at this http URL and long-term maintenance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2208.00085</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#34588;&#34562;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26159;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24050;&#32463;&#35777;&#26126;&#33021;&#22815;&#35299;&#20915;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#30417;&#27979;&#34588;&#34562;&#32676;&#20307;&#24182;&#26816;&#26597;&#20854;&#20581;&#24247;&#29366;&#20917;&#65292;&#20174;&#32780;&#22312;&#24773;&#20917;&#21464;&#24471;&#20005;&#37325;&#20043;&#21069;&#65292;&#35782;&#21035;&#20986;&#28508;&#22312;&#21361;&#38505;&#29366;&#24577;&#65292;&#25110;&#32773;&#26356;&#22909;&#22320;&#35745;&#21010;&#23450;&#26399;&#34588;&#34562;&#32676;&#20307;&#26816;&#26597;&#65292;&#20174;&#32780;&#33410;&#30465;&#37325;&#35201;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#29992;&#20110;&#34588;&#34562;&#30417;&#27979;&#30340;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#24182;&#20197;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#20026;&#20363;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#38754;&#21521;&#20861;&#21307;&#23398;&#21644;&#34588;&#34562;&#23398;&#19987;&#19994;&#20154;&#21592;&#21644;&#19987;&#23478;&#65292;&#26088;&#22312;&#21521;&#20182;&#20204;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#27599;&#20010;&#24212;&#29992;&#31867;&#21035;&#37117;&#20197;&#31616;&#35201;&#30340;&#29702;&#35770;&#20171;&#32461;&#21644;&#19982;&#20854;&#22522;&#26412;&#26041;&#27861;&#30456;&#20851;&#30340;&#21160;&#26426;&#24320;&#31687;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#35770;&#25991;&#33021;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;...
&lt;/p&gt;
&lt;p&gt;
Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#32852;&#37030;&#30456;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#26102;&#34701;&#21512;&#31354;&#38388;&#20449;&#24687;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#21644;&#21160;&#24577;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2206.03420</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#32852;&#37030;&#30456;&#20851;&#26694;&#26550;&#29992;&#20110;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning. (arXiv:2206.03420v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#32852;&#37030;&#30456;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#26102;&#34701;&#21512;&#31354;&#38388;&#20449;&#24687;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#21644;&#21160;&#24577;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30456;&#20851;&#24212;&#29992;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26102;&#31354;&#25968;&#25454;&#21253;&#21547;&#20016;&#23500;&#30340;&#20449;&#24687;&#24182;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20363;&#22914;&#65292;&#21307;&#30103;&#26426;&#26500;&#32463;&#24120;&#20351;&#29992;&#38468;&#30528;&#22312;&#24739;&#32773;&#19981;&#21516;&#37096;&#20301;&#30340;&#30005;&#26497;&#65292;&#20998;&#26512;&#24102;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#30005;&#29983;&#29702;&#25968;&#25454;&#36827;&#34892;&#20581;&#24247;&#35780;&#20272;&#21644;&#30142;&#30149;&#35786;&#26029;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#25552;&#21462;&#38544;&#34255;&#30340;&#26102;&#31354;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#34701;&#21512;&#31354;&#38388;&#20449;&#24687;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#21644;&#21160;&#24577;&#30340;&#26102;&#38388;&#21464;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#23545;&#20110;&#19968;&#20010;&#21033;&#29992;&#36825;&#20123;&#26102;&#31354;&#29305;&#24449;&#26469;&#23436;&#25104;&#22797;&#26434;&#39044;&#27979;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#32852;&#37030;&#30456;&#20851;&#26694;&#26550;&#65292;&#21517;&#20026;.....
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal data contains rich information and has been widely studied in recent years due to the rapid development of relevant applications in many fields. For instance, medical institutions often use electrodes attached to different parts of a patient to analyse the electorencephal data rich with spatial and temporal features for health assessment and disease diagnosis. Existing research has mainly used deep learning techniques such as convolutional neural network (CNN) or recurrent neural network (RNN) to extract hidden spatial-temporal features. Yet, it is challenging to incorporate both inter-dependencies spatial information and dynamic temporal changes simultaneously. In reality, for a model that leverages these spatial-temporal features to fulfil complex prediction tasks, it often requires a colossal amount of training data in order to obtain satisfactory model performance. Considering the above-mentioned challenges, we propose an adaptive federated relevance framework, nam
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;ConDex&#65292;&#29992;&#20110;&#33258;&#20027;&#35782;&#21035;&#20855;&#26377;&#26410;&#30693;&#29289;&#29702;&#23646;&#24615;&#30340;&#38750;&#22343;&#21248;&#23545;&#35937;&#65292;&#24182;&#23454;&#29616;&#31934;&#30830;&#30340;&#25235;&#21462;&#28857;&#20272;&#35745;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ConDex&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#24182;&#19988;&#29983;&#25104;&#20102;&#20004;&#20010;&#26032;&#30340;&#23545;&#35937;&#25968;&#25454;&#38598;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2205.11110</link><description>&lt;p&gt;
&#29289;&#29702;-&#19981;&#21487;&#30693;&#23545;&#35937;&#30340;&#20803;&#23398;&#20064;&#20877;&#25235;&#21462;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Regrasping Strategies for Physical-Agnostic Objects. (arXiv:2205.11110v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;ConDex&#65292;&#29992;&#20110;&#33258;&#20027;&#35782;&#21035;&#20855;&#26377;&#26410;&#30693;&#29289;&#29702;&#23646;&#24615;&#30340;&#38750;&#22343;&#21248;&#23545;&#35937;&#65292;&#24182;&#23454;&#29616;&#31934;&#30830;&#30340;&#25235;&#21462;&#28857;&#20272;&#35745;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ConDex&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#24182;&#19988;&#29983;&#25104;&#20102;&#20004;&#20010;&#26032;&#30340;&#23545;&#35937;&#25968;&#25454;&#38598;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#25235;&#21462;&#38750;&#22343;&#21248;&#23545;&#35937;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21407;&#22240;&#26159;&#23384;&#22312;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#22914;&#36136;&#37327;&#20998;&#24067;&#21644;&#25705;&#25830;&#31995;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ConDex&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65288;CNP&#65289;&#19982;DexNet-2.0&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#33258;&#20027;&#35782;&#21035;&#23545;&#35937;&#30340;&#28508;&#22312;&#29289;&#29702;&#23646;&#24615;&#12290;ConDex&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#35797;&#39564;&#20013;&#33719;&#21462;&#29289;&#29702;&#23884;&#20837;&#65292;&#23454;&#29616;&#31934;&#30830;&#30340;&#25235;&#21462;&#28857;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;ConDex&#33021;&#22815;&#20197;&#22312;&#32447;&#26041;&#24335;&#36845;&#20195;&#22320;&#26356;&#26032;&#39044;&#27979;&#30340;&#25235;&#21462;&#36136;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#29983;&#25104;&#20004;&#20010;&#38754;&#21521;&#38750;&#22343;&#21248;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#35937;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#36136;&#37327;&#20998;&#24067;&#21644;&#25705;&#25830;&#31995;&#25968;&#12290;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;ConDex&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;DexNet-2.0&#21644;&#29616;&#26377;&#30340;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#25235;&#21462;&#27969;&#27700;&#32447;&#12290;&#27492;&#22806;&#65292;ConDex&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Grasping inhomogeneous objects in real-world applications remains a challenging task due to the unknown physical properties such as mass distribution and coefficient of friction. In this study, we propose a meta-learning algorithm called ConDex, which incorporates Conditional Neural Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical properties of objects using depth images. ConDex efficiently acquires physical embeddings from limited trials, enabling precise grasping point estimation. Furthermore, ConDex is capable of updating the predicted grasping quality iteratively from new trials in an online fashion. To the best of our knowledge, we are the first who generate two object datasets focusing on inhomogeneous physical properties with varying mass distributions and friction coefficients. Extensive evaluations in simulation demonstrate ConDex's superior performance over DexNet-2.0 and existing meta-learning-based grasping pipelines. Furthermore, ConDex shows
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#27169;&#22411;&#32454;&#35843;&#21363;&#21487;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2202.10629</link><description>&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#65306;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10629
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#27169;&#22411;&#32454;&#35843;&#21363;&#21487;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35821;&#38899;&#31561;&#25968;&#25454;&#20016;&#23500;&#30340;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#25552;&#20379;&#39640;&#24615;&#33021;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#26041;&#38754;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#29978;&#33267;&#21487;&#20197;&#23398;&#20064;&#36890;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#20197;&#20415;&#26377;&#25928;&#22320;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#32454;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#20173;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#25968;&#25454;&#26377;&#38480;&#65307;&#65288;ii&#65289;&#27169;&#22411;&#24320;&#21457;&#25104;&#26412;&#21463;&#38480;&#65307;&#65288;iii&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#20415;&#26377;&#25928;&#36827;&#34892;&#32454;&#35843;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#27010;&#24565;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#36890;&#36807;&#20174;&#28304;&#39046;&#22495;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#19968;&#20010;&#31934;&#24515;&#24320;&#21457;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#32780;&#26080;&#38656;&#36827;&#34892;&#27169;&#22411;&#32454;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#65292;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#21487;&#20197;&#24046;&#24322;&#24040;&#22823;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-rich domains such as vision, language, and speech, deep learning prevails to deliver high-performance task-specific models and can even learn general task-agnostic representations for efficient finetuning to downstream tasks. However, deep learning in resource-limited domains still faces multiple challenges including (i) limited data, (ii) constrained model development cost, and (iii) lack of adequate pre-trained models for effective finetuning. This paper provides an overview of model reprogramming to bridge this gap. Model reprogramming enables resource-efficient cross-domain machine learning by repurposing and reusing a well-developed pre-trained model from a source domain to solve tasks in a target domain without model finetuning, where the source and target domains can be vastly different. In many applications, model reprogramming outperforms transfer learning and training from scratch. This paper elucidates the methodology of model reprogramming, summarizes existing use c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PolicyCleanse&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28608;&#27963;&#30340;&#29305;&#27931;&#20234;&#26234;&#33021;&#20307;&#32047;&#31215;&#22870;&#21169;&#30340;&#19979;&#38477;&#29305;&#24615;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#23581;&#35797;&#32531;&#35299;&#20854;&#29305;&#27931;&#20234;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2202.03609</link><description>&lt;p&gt;
PolicyCleanse&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#26816;&#27979;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning. (arXiv:2202.03609v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PolicyCleanse&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28608;&#27963;&#30340;&#29305;&#27931;&#20234;&#26234;&#33021;&#20307;&#32047;&#31215;&#22870;&#21169;&#30340;&#19979;&#38477;&#29305;&#24615;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#23581;&#35797;&#32531;&#35299;&#20854;&#29305;&#27931;&#20234;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#20173;&#20540;&#24471;&#26356;&#22810;&#20851;&#27880;&#21644;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21521;&#21463;&#23475;&#32773;&#26234;&#33021;&#20307;&#65288;&#21363;&#29305;&#27931;&#20234;&#26234;&#33021;&#20307;&#65289;&#27880;&#20837;&#21518;&#38376;&#35302;&#21457;&#21160;&#20316;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#20026;&#30830;&#20445;RL&#26234;&#33021;&#20307;&#23545;&#24694;&#24847;&#21518;&#38376;&#30340;&#23433;&#20840;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#21518;&#38376;&#26816;&#27979;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#26816;&#27979;&#29305;&#27931;&#20234;&#26234;&#33021;&#20307;&#20197;&#21450;&#30456;&#24212;&#30340;&#28508;&#22312;&#35302;&#21457;&#21160;&#20316;&#65292;&#24182;&#36827;&#19968;&#27493;&#23581;&#35797;&#32531;&#35299;&#20854;&#29305;&#27931;&#20234;&#34892;&#20026;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28608;&#27963;&#30340;&#29305;&#27931;&#20234;&#26234;&#33021;&#20307;&#32047;&#31215;&#22870;&#21169;&#22312;&#20960;&#20010;&#26102;&#38388;&#27493;&#20043;&#21518;&#26126;&#26174;&#19979;&#38477;&#30340;PolicyCleanse&#12290;&#38500;&#20102;PolicyCleanse&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26426;&#22120;&#26410;&#23436;&#25104;&#30340;&#37096;&#20998;...
&lt;/p&gt;
&lt;p&gt;
While real-world applications of reinforcement learning are becoming popular, the security and robustness of RL systems are worthy of more attention and exploration. In particular, recent works have revealed that, in a multi-agent RL environment, backdoor trigger actions can be injected into a victim agent (a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it sees the backdoor trigger action. To ensure the security of RL agents against malicious backdoors, in this work, we propose the problem of Backdoor Detection in a multi-agent competitive reinforcement learning system, with the objective of detecting Trojan agents as well as the corresponding potential trigger actions, and further trying to mitigate their Trojan behavior. In order to solve this problem, we propose PolicyCleanse that is based on the property that the activated Trojan agents accumulated rewards degrade noticeably after several timesteps. Along with PolicyCleanse, we also design a machine unl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;$S_n$-&#31561;&#21464;&#21367;&#31215;&#37327;&#23376;Ansatze&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#22312;&#20855;&#26377;SU($d$)&#23545;&#31216;&#24615;&#30340;&#24191;&#27867;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#29983;&#25104;&#20219;&#24847;&#24186;&#27491;&#30697;&#38453;&#65292;&#21516;&#26102;&#39564;&#35777;&#20102;4-local SU($d$)&#23545;&#31216;&#24186;&#27491;&#30697;&#38453;&#30340;&#21487;&#23454;&#29616;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.07611</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#31561;&#21464;&#21367;&#31215;&#37327;&#23376;Ansatze&#21152;&#36895;&#23398;&#20064;&#37327;&#23376;&#24577;
&lt;/p&gt;
&lt;p&gt;
Speeding up Learning Quantum States through Group Equivariant Convolutional Quantum Ans\"atze. (arXiv:2112.07611v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;$S_n$-&#31561;&#21464;&#21367;&#31215;&#37327;&#23376;Ansatze&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#22312;&#20855;&#26377;SU($d$)&#23545;&#31216;&#24615;&#30340;&#24191;&#27867;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#29983;&#25104;&#20219;&#24847;&#24186;&#27491;&#30697;&#38453;&#65292;&#21516;&#26102;&#39564;&#35777;&#20102;4-local SU($d$)&#23545;&#31216;&#24186;&#27491;&#30697;&#38453;&#30340;&#21487;&#23454;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;Schur-Weyl&#23545;&#20598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;$S_n$-&#31561;&#21464;&#21367;&#31215;&#37327;&#23376;&#30005;&#36335;&#21644;SU$(d)$&#23545;&#31216;&#24615;&#65292;&#23558;Jordan&#30340;&#32622;&#25442;&#37327;&#23376;&#35745;&#31639;(PQC)&#24418;&#24335;&#20027;&#20041;&#25193;&#23637;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;Okounkov-Vershik&#26041;&#27861;&#26469;&#35777;&#26126;Harrow&#22312;&#22855;&#24322;&#36793;&#34920;&#31034;&#22522;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#21033;&#29992;Young-Jucys-Murphy(YJM)&#20803;&#32032;&#24314;&#31435;&#20102;$S_n$-&#31561;&#21464;&#21367;&#31215;&#37327;&#23376;&#20132;&#26367;Ansatze($S_n$-CQA)&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;$S_n$-CQA&#33021;&#22815;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;$S_n$ irrep&#25159;&#21306;&#20013;&#29983;&#25104;&#20219;&#24847;&#24186;&#27491;&#30697;&#38453;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#20855;&#26377;SU($d$)&#23545;&#31216;&#24615;&#30340;&#24191;&#27867;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#35777;&#26126;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;(QAOA)&#30340;&#26222;&#36866;&#24615;&#30340;&#26041;&#24335;&#65292;&#24182;&#39564;&#35777;&#20102;4-local SU($d$)&#23545;&#31216;&#24186;&#27491;&#30697;&#38453;&#26159;&#21487;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a theoretical framework for $S_n$-equivariant convolutional quantum circuits with SU$(d)$-symmetry, building on and significantly generalizing Jordan's Permutational Quantum Computing (PQC) formalism based on Schur-Weyl duality connecting both SU$(d)$ and $S_n$ actions on qudits. In particular, we utilize the Okounkov-Vershik approach to prove Harrow's statement (Ph.D. Thesis 2005 p.160) on the equivalence between $\operatorname{SU}(d)$ and $S_n$ irrep bases and to establish the $S_n$-equivariant Convolutional Quantum Alternating Ans\"atze ($S_n$-CQA) using Young-Jucys-Murphy (YJM) elements. We prove that $S_n$-CQA is able to generate any unitary in any given $S_n$ irrep sector, which may serve as a universal model for a wide array of quantum machine learning problems with the presence of SU($d$) symmetry. Our method provides another way to prove the universality of Quantum Approximate Optimization Algorithm (QAOA) and verifies that 4-local SU($d$) symmetric unitaries are su
&lt;/p&gt;</description></item></channel></rss>