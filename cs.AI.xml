<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#20110;&#35270;&#35273;&#30340;&#36861;&#36880;-&#36867;&#36991;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#21033;&#29992;&#23436;&#20840;&#21487;&#35266;&#27979;&#26426;&#22120;&#20154;&#31574;&#30053;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#31574;&#30053;&#29983;&#25104;&#30417;&#30563;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#37096;&#20998;&#21487;&#35266;&#27979;&#31574;&#30053;&#30340;&#30417;&#30563;&#20449;&#21495;&#36136;&#37327;&#21462;&#20915;&#20110;&#36867;&#36991;&#32773;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#21644;&#26368;&#20248;&#24615;&#30340;&#24179;&#34913;&#65292;&#20197;&#21450;&#23436;&#20840;&#21487;&#35266;&#27979;&#31574;&#30053;&#20013;&#24314;&#27169;&#20551;&#35774;&#30340;&#24378;&#24230;&#12290;&#25105;&#20204;&#22312;&#19968;&#21488;&#20855;&#26377;RGB-D&#30456;&#26426;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21019;&#26032;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16185</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#35270;&#35273;&#30340;&#36861;&#36880;-&#36867;&#36991;&#26426;&#22120;&#20154;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Vision-based Pursuit-Evasion Robot Policies. (arXiv:2308.16185v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#20110;&#35270;&#35273;&#30340;&#36861;&#36880;-&#36867;&#36991;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#21033;&#29992;&#23436;&#20840;&#21487;&#35266;&#27979;&#26426;&#22120;&#20154;&#31574;&#30053;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#31574;&#30053;&#29983;&#25104;&#30417;&#30563;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#37096;&#20998;&#21487;&#35266;&#27979;&#31574;&#30053;&#30340;&#30417;&#30563;&#20449;&#21495;&#36136;&#37327;&#21462;&#20915;&#20110;&#36867;&#36991;&#32773;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#21644;&#26368;&#20248;&#24615;&#30340;&#24179;&#34913;&#65292;&#20197;&#21450;&#23436;&#20840;&#21487;&#35266;&#27979;&#31574;&#30053;&#20013;&#24314;&#27169;&#20551;&#35774;&#30340;&#24378;&#24230;&#12290;&#25105;&#20204;&#22312;&#19968;&#21488;&#20855;&#26377;RGB-D&#30456;&#26426;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21019;&#26032;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#23398;&#20064;&#25112;&#30053;&#24615;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#22914;&#36861;&#36880;-&#36867;&#36991;&#20132;&#20114;&#65292;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23427;&#38656;&#35201;&#21033;&#29992;&#20132;&#20114;&#30340;&#21160;&#24577;&#24615;&#65292;&#24182;&#36890;&#36807;&#29289;&#29702;&#29366;&#24577;&#21644;&#28508;&#22312;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35268;&#21010;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#23436;&#20840;&#21487;&#35266;&#27979;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#31574;&#30053;&#29983;&#25104;&#30417;&#30563;&#20449;&#21495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36861;&#36880;&#32773;&#31574;&#30053;&#30340;&#30417;&#30563;&#20449;&#21495;&#36136;&#37327;&#21462;&#20915;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#36867;&#36991;&#32773;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#21644;&#26368;&#20248;&#24615;&#30340;&#24179;&#34913;&#65292;&#20197;&#21450;&#23436;&#20840;&#21487;&#35266;&#27979;&#31574;&#30053;&#20013;&#24314;&#27169;&#20551;&#35774;&#30340;&#24378;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31574;&#30053;&#37096;&#32626;&#22312;&#19968;&#21488;&#20855;&#26377;RGB-D&#30456;&#26426;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#65292;&#22312;&#37326;&#22806;&#30340;&#36861;&#36880;-&#36867;&#36991;&#20132;&#20114;&#20013;&#36827;&#34892;&#12290;&#23613;&#31649;&#23384;&#22312;&#31181;&#31181;&#25361;&#25112;&#65292;&#20294;&#24863;&#30693;&#32422;&#26463;&#24102;&#26469;&#20102;&#21019;&#36896;&#21147;&#65306;&#24403;&#26426;&#22120;&#20154;&#24863;&#35273;&#19981;&#30830;&#23450;&#26102;&#65292;&#23427;&#34987;&#25512;&#21160;&#25910;&#38598;&#20449;&#24687;&#65292;&#20174;&#22024;&#26434;&#30340;&#27979;&#37327;&#20013;&#39044;&#27979;&#30446;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning strategic robot behavior -- like that required in pursuit-evasion interactions -- under real-world constraints is extremely challenging. It requires exploiting the dynamics of the interaction, and planning through both physical state and latent intent uncertainty. In this paper, we transform this intractable problem into a supervised learning problem, where a fully-observable robot policy generates supervision for a partially-observable one. We find that the quality of the supervision signal for the partially-observable pursuer policy depends on two key factors: the balance of diversity and optimality of the evader's behavior and the strength of the modeling assumptions in the fully-observable policy. We deploy our policy on a physical quadruped robot with an RGB-D camera on pursuit-evasion interactions in the wild. Despite all the challenges, the sensing constraints bring about creativity: the robot is pushed to gather information when uncertain, predict intent from noisy mea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#32622;&#20449;&#24230;&#37327;&#21270;&#20102;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16175</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#22312;&#21644;&#22806;&#22312;&#32622;&#20449;&#24230;&#35780;&#20272;&#26469;&#37327;&#21270;&#20219;&#24847;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment. (arXiv:2308.16175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#32622;&#20449;&#24230;&#37327;&#21270;&#20102;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20219;&#20309;&#36755;&#20986;&#30340;&#25968;&#20540;&#32622;&#20449;&#24230;&#26469;&#26816;&#27979;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#36866;&#29992;&#20110;&#20165;&#36890;&#36807;&#40657;&#30418;API&#35775;&#38382;&#30340;&#20219;&#20309;LLM&#65292;&#24182;&#23558;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#30340;&#32622;&#20449;&#24230;&#32467;&#21512;&#20026;&#23545;&#32473;&#23450;&#25552;&#31034;&#19979;LLM&#21709;&#24212;&#30340;&#21333;&#20010;&#21487;&#20449;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#36890;&#29992;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24403;&#20170;&#25152;&#26377;&#26368;&#22909;&#30340;LLM&#65288;&#20854;&#35757;&#32451;&#25968;&#25454;&#26410;&#30693;&#65289;&#12290;&#36890;&#36807;&#39069;&#22806;&#30340;&#35745;&#31639;&#65292;&#20219;&#20309;LLM API&#30340;&#29992;&#25143;&#29616;&#22312;&#21487;&#20197;&#33719;&#24471;&#19982;&#36890;&#24120;&#30456;&#21516;&#30340;&#21709;&#24212;&#65292;&#20197;&#21450;&#19968;&#20010;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#20197;&#20415;&#22312;&#19981;&#20449;&#20219;&#35813;&#21709;&#24212;&#26102;&#20445;&#25345;&#35880;&#24910;&#12290;&#23545;&#20110;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BSDetector&#27604;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65288;&#23545;&#20110;GPT-3&#21644;ChatGPT&#65289;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#38169;&#35823;&#30340;LLM&#21709;&#24212;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#21709;&#24212;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, and combines intrinsic and extrinsic assessments of confidence into a single trustworthiness estimate for any LLM response to a given prompt. Our method is extremely general and can applied to all of the best LLMs available today (whose training data remains unknown). By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that caution when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#21019;&#36896;&#20102;&#26032;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#27010;&#24565;&#65292;&#24182;&#21051;&#30011;&#20102;&#20854;&#29305;&#24449;&#12290;&#36825;&#20123;&#39063;&#31890;&#39318;&#20808;&#30830;&#23450;&#33258;&#24049;&#65292;&#28982;&#21518;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#39063;&#31890;&#35745;&#31639;&#29702;&#35770;&#26694;&#26550;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#24212;&#29992;&#21644;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.16157</link><description>&lt;p&gt;
&#23384;&#22312;&#24615;&#39063;&#31890;&#30340;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Algebraic, Topological, and Mereological Foundations of Existential Granules. (arXiv:2308.16157v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#21019;&#36896;&#20102;&#26032;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#27010;&#24565;&#65292;&#24182;&#21051;&#30011;&#20102;&#20854;&#29305;&#24449;&#12290;&#36825;&#20123;&#39063;&#31890;&#39318;&#20808;&#30830;&#23450;&#33258;&#24049;&#65292;&#28982;&#21518;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#39063;&#31890;&#35745;&#31639;&#29702;&#35770;&#26694;&#26550;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#24212;&#29992;&#21644;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21457;&#26126;&#20102;&#30830;&#23450;&#33258;&#24049;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20102;&#21051;&#30011;&#12290;&#23384;&#22312;&#24615;&#39063;&#31890;&#26159;&#37027;&#20123;&#26368;&#21021;&#30830;&#23450;&#33258;&#24049;&#65292;&#24182;&#38543;&#21518;&#19982;&#20854;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#39063;&#31890;&#12290;&#36825;&#20010;&#27010;&#24565;&#30340;&#31034;&#20363;&#65292;&#27604;&#22914;&#39063;&#31890;&#29699;&#65292;&#22312;&#20043;&#21069;&#20854;&#20182;&#20154;&#30340;&#20316;&#21697;&#20013;&#34429;&#28982;&#23450;&#20041;&#19981;&#23436;&#22791;&#12289;&#31639;&#27861;&#24314;&#31435;&#19981;&#20805;&#20998;&#12289;&#29702;&#35770;&#21270;&#19981;&#36275;&#65292;&#20294;&#24050;&#32463;&#22312;&#31895;&#31961;&#38598;&#21644;&#36719;&#35745;&#31639;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#36866;&#21512;&#20110;&#39063;&#31890;&#35745;&#31639;&#30340;&#22810;&#20010;&#29702;&#35770;&#26694;&#26550;&#65288;&#20844;&#29702;&#21270;&#12289;&#36866;&#24212;&#24615;&#31561;&#65289;&#12290;&#36825;&#31181;&#21051;&#30011;&#26088;&#22312;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#30340;&#24212;&#29992;&#20197;&#21450;&#21487;&#33021;&#30340;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#24182;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, new concepts of existential granules that determine themselves are invented, and are characterized from algebraic, topological, and mereological perspectives. Existential granules are those that determine themselves initially, and interact with their environment subsequently. Examples of the concept, such as those of granular balls, though inadequately defined, algorithmically established, and insufficiently theorized in earlier works by others, are already used in applications of rough sets and soft computing. It is shown that they fit into multiple theoretical frameworks (axiomatic, adaptive, and others) of granular computing. The characterization is intended for algorithm development, application to classification problems and possible mathematical foundations of generalizations of the approach. Additionally, many open problems are posed and directions provided.
&lt;/p&gt;</description></item><item><title>Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.16149</link><description>&lt;p&gt;
Jais&#21644;Jais-chat&#65306;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16149
&lt;/p&gt;
&lt;p&gt;
Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Jais&#21644;Jais-chat&#65292;&#36825;&#26159;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;GPT-3&#30340;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#25991;&#26412;&#30340;&#28151;&#21512;&#29289;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#28304;&#20195;&#30721;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;130&#20159;&#20010;&#21442;&#25968;&#65292;&#26681;&#25454;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#33521;&#35821;&#25968;&#25454;&#35201;&#23569;&#24471;&#22810;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#33521;&#35821;&#26041;&#38754;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#20173;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#35843;&#20248;&#12289;&#23433;&#20840;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#35814;&#32454;&#25551;&#36848;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#27169;&#22411;&#30340;&#20004;&#20010;&#24320;&#25918;&#29256;&#26412;--&#22522;&#30784;Jais&#27169;&#22411;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;Jais-chat&#21464;&#31181;--&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;&#35814;&#35265;https://hugging
&lt;/p&gt;
&lt;p&gt;
We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
&lt;/p&gt;</description></item><item><title>LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16137</link><description>&lt;p&gt;
LM-Infinite: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21363;&#26102;&#38271;&#24230;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16137
&lt;/p&gt;
&lt;p&gt;
LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;Transformer-based&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;LLM&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#30528;&#23545;&#38271;&#26102;&#38388;&#25512;&#29702;&#36807;&#31243;&#25110;&#29702;&#35299;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#26041;&#26696;&#23558;&#35757;&#32451;&#24207;&#21015;&#25130;&#26029;&#21040;&#22266;&#23450;&#38271;&#24230;&#65288;&#20363;&#22914;LLaMa&#30340;2048&#65289;&#12290;&#21363;&#20351;&#20351;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;LLM&#22312;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20043;&#21518;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#26356;&#19981;&#29992;&#35828;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20102;&#12290;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22312;&#26356;&#38271;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#30828;&#20214;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#30340;&#35757;&#32451;&#36807;&#31243;&#35774;&#35745;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#20027;&#35201;&#30340;&#20998;&#24067;&#22806;(OOD) f
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;CorrEmbed&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#23884;&#20837;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#35745;&#31639;&#22270;&#20687;&#23884;&#20837;&#20013;&#30340;&#36317;&#31163;&#19982;&#20154;&#24037;&#26631;&#27880;&#21521;&#37327;&#20013;&#30340;&#36317;&#31163;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#20102;ImageNet1k&#20934;&#30830;&#24230;&#24471;&#20998;&#21644;&#26631;&#31614;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#30452;&#32447;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.16126</link><description>&lt;p&gt;
CorrEmbed: &#29992;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a Novel Metric. (arXiv:2308.16126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;CorrEmbed&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#23884;&#20837;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#35745;&#31639;&#22270;&#20687;&#23884;&#20837;&#20013;&#30340;&#36317;&#31163;&#19982;&#20154;&#24037;&#26631;&#27880;&#21521;&#37327;&#20013;&#30340;&#36317;&#31163;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#20102;ImageNet1k&#20934;&#30830;&#24230;&#24471;&#20998;&#21644;&#26631;&#31614;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#30452;&#32447;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#20135;&#21697;&#25512;&#33616;&#26102;&#65292;&#26816;&#27979;&#35270;&#35273;&#19978;&#30456;&#20284;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#29305;&#21035;&#26377;&#29992;&#30340;&#23646;&#24615;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#25552;&#21462;&#39640;&#32423;&#22270;&#20687;&#29305;&#24449;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#22312;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#26500;&#25104;&#30340;&#22270;&#20687;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#25152;&#29983;&#25104;&#30340;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20256;&#32479;&#30340;&#25439;&#22833;&#21644;&#24615;&#33021;&#24230;&#37327;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#20854;&#22312;&#22270;&#20687;&#30456;&#20284;&#24615;&#25628;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;CorrEmbed&#30340;&#26032;&#26041;&#27861;&#35780;&#20272;&#20102;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#22270;&#20687;&#23884;&#20837;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#20102;&#22270;&#20687;&#23884;&#20837;&#20013;&#36317;&#31163;&#21644;&#20154;&#24037;&#26631;&#27880;&#21521;&#37327;&#20013;&#36317;&#31163;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#23545;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;Torchvision&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;ImageNet1k&#20934;&#30830;&#24230;&#24471;&#20998;&#21644;&#26631;&#31614;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#30452;&#32447;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting visually similar images is a particularly useful attribute to look to when calculating product recommendations. Embedding similarity, which utilizes pre-trained computer vision models to extract high-level image features, has demonstrated remarkable efficacy in identifying images with similar compositions. However, there is a lack of methods for evaluating the embeddings generated by these models, as conventional loss and performance metrics do not adequately capture their performance in image similarity search tasks.  In this paper, we evaluate the viability of the image embeddings from numerous pre-trained computer vision models using a novel approach named CorrEmbed. Our approach computes the correlation between distances in image embeddings and distances in human-generated tag vectors. We extensively evaluate numerous pre-trained Torchvision models using this metric, revealing an intuitive relationship of linear scaling between ImageNet1k accuracy scores and tag-correlati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22238;&#24212;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23383;&#31526;&#20018;&#31867;&#27604;&#30340;&#21453;&#20363;&#26469;&#21453;&#39539;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;GPT-3&#26080;&#27861;&#35299;&#20915;&#26368;&#31616;&#21333;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#20026;&#20102;&#21152;&#24378;&#38646;&#28857;&#25512;&#29702;&#31561;&#20154;&#31867;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#38656;&#35201;&#21457;&#23637;&#20986;&#25490;&#38500;&#25968;&#25454;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16118</link><description>&lt;p&gt;
&#22238;&#24212;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Response: Emergent analogical reasoning in large language models. (arXiv:2308.16118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22238;&#24212;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23383;&#31526;&#20018;&#31867;&#27604;&#30340;&#21453;&#20363;&#26469;&#21453;&#39539;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;GPT-3&#26080;&#27861;&#35299;&#20915;&#26368;&#31616;&#21333;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#20026;&#20102;&#21152;&#24378;&#38646;&#28857;&#25512;&#29702;&#31561;&#20154;&#31867;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#38656;&#35201;&#21457;&#23637;&#20986;&#25490;&#38500;&#25968;&#25454;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#12298;&#33258;&#28982;&#20154;&#31867;&#34892;&#20026;&#12299;&#35770;&#25991;&#20013;&#65292;&#8220;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;&#8221;&#65288;Webb&#65292;Holyoak&#21644;Lu&#65292;2023&#65289;&#65292;&#20316;&#32773;&#20204;&#35748;&#20026;&#8220;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#33719;&#24471;&#20102;&#21457;&#29616;&#24191;&#27867;&#31867;&#27604;&#38382;&#39064;&#30340;&#38646;&#28857;&#35299;&#30340;&#32039;&#24613;&#33021;&#21147;&#8221;&#12290;&#22312;&#26412;&#22238;&#24212;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#23383;&#31526;&#20018;&#31867;&#27604;&#30340;&#21453;&#20363;&#12290;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#20013;&#65292;GPT-3&#29978;&#33267;&#26080;&#27861;&#35299;&#20915;&#21407;&#22987;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#26368;&#31616;&#21333;&#30340;&#21464;&#20307;&#38382;&#39064;&#12290;&#38646;&#28857;&#25512;&#29702;&#26159;&#19968;&#20010;&#38656;&#35201;&#38750;&#24120;&#20805;&#20998;&#35777;&#25454;&#25903;&#25345;&#30340;&#38750;&#20961;&#20027;&#24352;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27809;&#26377;&#30475;&#21040;&#36825;&#26679;&#30340;&#35777;&#25454;&#12290;&#20026;&#20102;&#21152;&#24378;&#20687;&#38646;&#28857;&#25512;&#29702;&#36825;&#26679;&#31867;&#20284;&#20154;&#31867;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#37325;&#35201;&#30340;&#26159;&#35813;&#39046;&#22495;&#24320;&#21457;&#20986;&#33021;&#22815;&#25490;&#38500;&#25968;&#25454;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In their recent Nature Human Behaviour paper, "Emergent analogical reasoning in large language models," (Webb, Holyoak, and Lu, 2023) the authors argue that "large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems." In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve even the easiest variants of the problems presented in the original paper. Zero-shot reasoning is an extraordinary claim that requires extraordinary evidence. We do not see that evidence in our experiments. To strengthen claims of humanlike reasoning such as zero-shot reasoning, it is important that the field develop approaches that rule out data memorization.
&lt;/p&gt;</description></item><item><title>survex&#26159;&#19968;&#20010;R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#65292;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#65292;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#24863;&#12290;</title><link>http://arxiv.org/abs/2308.16113</link><description>&lt;p&gt;
survex&#65306;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#29983;&#23384;&#27169;&#22411;&#30340;R&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
survex: an R package for explaining machine learning survival models. (arXiv:2308.16113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16113
&lt;/p&gt;
&lt;p&gt;
survex&#26159;&#19968;&#20010;R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#65292;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#65292;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#20986;&#33394;&#24615;&#33021;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#29992;&#20110;&#34917;&#20805;&#21644;&#36229;&#36234;&#20256;&#32479;&#30340;&#32479;&#35745;&#29983;&#23384;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#32570;&#20047;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#26469;&#35299;&#37322;&#20854;&#20869;&#37096;&#25805;&#20316;&#21644;&#39044;&#27979;&#21407;&#29702;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;survex R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#12290;&#25152;&#25552;&#36719;&#20214;&#30340;&#21151;&#33021;&#21253;&#25324;&#29702;&#35299;&#21644;&#35786;&#26029;&#29983;&#23384;&#27169;&#22411;&#65292;&#20174;&#32780;&#21487;&#20197;&#25913;&#36827;&#23427;&#20204;&#12290;&#36890;&#36807;&#25581;&#31034;&#21464;&#37327;&#25928;&#24212;&#21644;&#37325;&#35201;&#24615;&#31561;&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#65292;survex&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24182;&#26816;&#27979;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#21644;&#21307;&#30103;&#24212;&#29992;&#31561;&#25935;&#24863;&#39046;&#22495;&#21487;&#20197;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their flexibility and superior performance, machine learning models frequently complement and outperform traditional statistical survival models. However, their widespread adoption is hindered by a lack of user-friendly tools to explain their internal operations and prediction rationales. To tackle this issue, we introduce the survex R package, which provides a cohesive framework for explaining any survival model by applying explainable artificial intelligence techniques. The capabilities of the proposed software encompass understanding and diagnosing survival models, which can lead to their improvement. By revealing insights into the decision-making process, such as variable effects and importances, survex enables the assessment of model reliability and the detection of biases. Thus, transparency and responsibility may be promoted in sensitive areas, such as biomedical research and healthcare applications.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25968;&#25454;&#30340;&#20849;&#20139;&#38754;&#20020;&#20010;&#20154;&#21644;&#25935;&#24863;&#20449;&#24687;&#20445;&#25252;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#20551;&#21517;&#21270;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#21253;&#25324;&#23545;&#20551;&#21517;&#21270;&#23545;&#26080;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20551;&#21517;&#21270;&#20316;&#20026;&#20445;&#25252;&#20316;&#32773;&#36523;&#20221;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#24320;&#21457;&#19978;&#19979;&#25991;&#25935;&#24863;&#31639;&#27861;&#29992;&#20110;&#22788;&#29702;&#20010;&#20154;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.16109</link><description>&lt;p&gt;
Grandma Karl&#20170;&#24180;27&#23681;&#8212;&#8212;&#30740;&#31350;&#21311;&#21517;&#21270;&#30740;&#31350;&#25968;&#25454;&#30340;&#35758;&#31243;
&lt;/p&gt;
&lt;p&gt;
Grandma Karl is 27 years old -- research agenda for pseudonymization of research data. (arXiv:2308.16109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16109
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25968;&#25454;&#30340;&#20849;&#20139;&#38754;&#20020;&#20010;&#20154;&#21644;&#25935;&#24863;&#20449;&#24687;&#20445;&#25252;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#20551;&#21517;&#21270;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#21253;&#25324;&#23545;&#20551;&#21517;&#21270;&#23545;&#26080;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20551;&#21517;&#21270;&#20316;&#20026;&#20445;&#25252;&#20316;&#32773;&#36523;&#20221;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#24320;&#21457;&#19978;&#19979;&#25991;&#25935;&#24863;&#31639;&#27861;&#29992;&#20110;&#22788;&#29702;&#20010;&#20154;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25968;&#25454;&#30340;&#21487;&#35775;&#38382;&#24615;&#23545;&#20110;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20854;&#20013;&#21253;&#21547;&#20010;&#20154;&#21644;&#25935;&#24863;&#20449;&#24687;&#65288;&#22914;&#22995;&#21517;&#25110;&#25919;&#27835;&#35266;&#28857;&#65289;&#65292;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#19981;&#33021;&#34987;&#20849;&#20139;&#12290;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#24314;&#35758;&#20351;&#29992;&#20551;&#21517;&#21270;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#23545;&#30740;&#31350;&#25968;&#25454;&#30340;&#24320;&#25918;&#35775;&#38382;&#30340;&#23433;&#20840;&#24615;&#65292;&#20294;&#22312;&#37319;&#29992;&#20551;&#21517;&#21270;&#26041;&#27861;&#22788;&#29702;&#30740;&#31350;&#25968;&#25454;&#20043;&#21069;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22810;&#22320;&#20102;&#35299;&#20551;&#21517;&#21270;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#19968;&#20010;&#20851;&#20110;&#20551;&#21517;&#21270;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#21253;&#25324;&#23545;&#26080;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#20551;&#21517;&#21270;&#23545;&#21487;&#35835;&#24615;&#21644;&#35821;&#35328;&#35780;&#20272;&#31561;&#24433;&#21709;&#30340;&#30740;&#31350;&#38656;&#27714;&#65292;&#20197;&#21450;&#20551;&#21517;&#21270;&#20316;&#20026;&#20445;&#25252;&#20316;&#32773;&#36523;&#20221;&#30340;&#26041;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#24320;&#21457;&#19978;&#19979;&#25991;&#25935;&#24863;&#31639;&#27861;&#29992;&#20110;&#26816;&#27979;&#12289;&#26631;&#35760;&#21644;&#26367;&#25442;&#26080;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#20010;&#20154;&#20449;&#24687;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#26368;&#36817;&#33719;&#24471;&#30340;&#20851;&#20110;&#20551;&#21517;&#21270;&#30340;&#39033;&#30446;&#8220;Grandma Karl&#20170;&#24180;27&#23681;&#8221;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#37096;&#20998;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accessibility of research data is critical for advances in many research fields, but textual data often cannot be shared due to the personal and sensitive information which it contains, e.g names or political opinions. General Data Protection Regulation (GDPR) suggests pseudonymization as a solution to secure open access to research data, but we need to learn more about pseudonymization as an approach before adopting it for manipulation of research data. This paper outlines a research agenda within pseudonymization, namely need of studies into the effects of pseudonymization on unstructured data in relation to e.g. readability and language assessment, as well as the effectiveness of pseudonymization as a way of protecting writer identity, while also exploring different ways of developing context-sensitive algorithms for detection, labelling and replacement of personal information in unstructured data. The recently granted project on pseudonymization Grandma Karl is 27 years old address
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#21152;&#28909;&#28809;&#25511;&#21046;&#31995;&#32479;&#30340;&#35757;&#32451;&#65292;&#20026;&#22522;&#30784;&#20135;&#19994;&#30340;&#21487;&#25345;&#32493;&#21046;&#36896;&#21644;&#33021;&#32791;&#38477;&#20302;&#30446;&#26631;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.16089</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#26041;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#22312;&#21152;&#28909;&#28809;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Zone Method based Machine Learning and Physics-Informed Neural Networks in Reheating Furnaces. (arXiv:2308.16089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#21152;&#28909;&#28809;&#25511;&#21046;&#31995;&#32479;&#30340;&#35757;&#32451;&#65292;&#20026;&#22522;&#30784;&#20135;&#19994;&#30340;&#21487;&#25345;&#32493;&#21046;&#36896;&#21644;&#33021;&#32791;&#38477;&#20302;&#30446;&#26631;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#30784;&#20135;&#19994;&#30340;&#32463;&#27982;&#37325;&#35201;&#24615;&#24456;&#39640;&#65292;&#20294;&#20854;&#29983;&#20135;&#38142;&#20013;&#30340;&#19968;&#20123;&#32452;&#20214;&#65292;&#22914;&#21152;&#28909;&#28809;&#65292;&#33021;&#32791;&#36739;&#39640;&#12290;&#36890;&#36807;&#20943;&#23569;&#21152;&#28909;&#28809;&#20013;&#30340;&#25972;&#20307;&#21152;&#28909;&#26102;&#38388;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#33021;&#32791;&#12290;&#22312;&#22522;&#30784;&#20135;&#19994;&#21487;&#25345;&#32493;&#21046;&#36896;&#20013;&#65292;&#35745;&#31639;&#26426;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25511;&#21046;&#31995;&#32479;&#21487;&#33021;&#26159;&#23454;&#29616;&#8220;&#38646;&#20928;&#25490;&#25918;&#8221;&#30446;&#26631;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#20013;&#65292;&#30001;&#20110;&#22312;&#21152;&#28909;&#28809;&#31561;&#22330;&#26223;&#20013;&#26080;&#27861;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;ML&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#30340;&#22238;&#24402;&#35757;&#32451;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21306;&#22495;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#24335;&#26469;&#24314;&#27169;&#36752;&#23556;&#20256;&#28909;&#65288;RHT&#65289;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#36825;&#26159;&#21152;&#28909;&#28809;&#20869;&#39640;&#28201;&#36807;&#31243;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#20256;&#28909;&#26426;&#21046;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the high economic relevance of Foundation Industries, certain components like Reheating furnaces within their manufacturing chain are energy-intensive. Notable energy consumption reduction could be obtained by reducing the overall heating time in furnaces. Computer-integrated Machine Learning (ML) and Artificial Intelligence (AI) powered control systems in furnaces could be enablers in achieving the Net-Zero goals in Foundation Industries for sustainable manufacturing.  In this work, due to the infeasibility of achieving good quality data in scenarios like reheating furnaces, classical Hottel's zone method based computational model has been used to generate data for ML and Deep Learning (DL) based model training via regression. It should be noted that the zone method provides an elegant way to model the physical phenomenon of Radiative Heat Transfer (RHT), the dominating heat transfer mechanism in high-temperature processes inside heating furnaces. Using this data, an extensive
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31995;&#32479;&#20013;&#28155;&#21152;&#22270;&#20687;&#29305;&#24449;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#23545;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#30340;&#24110;&#21161;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#35270;&#35273;&#32972;&#26223;&#23545;&#32763;&#35793;&#25928;&#26524;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2308.16075</link><description>&lt;p&gt;
&#35270;&#35273;&#32972;&#26223;&#23545;&#22024;&#26434;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;&#65306;&#23545;&#33521;&#21360;&#35821;&#35328;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages. (arXiv:2308.16075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31995;&#32479;&#20013;&#28155;&#21152;&#22270;&#20687;&#29305;&#24449;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#23545;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#30340;&#24110;&#21161;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#35270;&#35273;&#32972;&#26223;&#23545;&#32763;&#35793;&#25928;&#26524;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#26412;&#30740;&#31350;&#21017;&#32771;&#23519;&#20102;&#23558;&#22270;&#20687;&#29305;&#24449;&#28155;&#21152;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22270;&#20687;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#12290;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#12290;&#23454;&#39564;&#23558;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#65292;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35270;&#35273;&#32972;&#26223;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#65306;&#23545;&#20110;&#38750;&#22122;&#22768;&#32763;&#35793;&#65292;&#19981;&#20351;&#29992;&#35270;&#35273;&#32972;&#26223;&#25928;&#26524;&#26368;&#22909;&#65307;&#23545;&#20110;&#20302;&#22122;&#22768;&#65292;&#35009;&#21098;&#30340;&#22270;&#20687;&#29305;&#24449;&#26368;&#20339;&#65307;&#22312;&#39640;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;&#23436;&#25972;&#30340;&#22270;&#20687;&#29305;&#24449;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study investigates the effectiveness of utilizing multimodal information in Neural Machine Translation (NMT). While prior research focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model deal with textual noise. Multimodal models slightly outperform text-only models in noisy settings, even with random images. The study's experiments translate from English to Hindi, Bengali, and Malayalam, outperforming state-of-the-art benchmarks significantly. Interestingly, the effect of visual context varies with source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features work better in high-noise scenarios. This she
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20840;&#23616;&#26679;&#24335;&#19981;&#19968;&#33268;&#21644;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16071</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic Image Synthesis via Class-Adaptive Cross-Attention. (arXiv:2308.16071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20840;&#23616;&#26679;&#24335;&#19981;&#19968;&#33268;&#21644;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20840;&#23616;&#22270;&#20687;&#32479;&#35745;&#20449;&#24687;&#65292;&#23548;&#33268;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#65292;&#24182;&#24341;&#36215;&#35832;&#22914;&#33394;&#24425;&#25110;&#20809;&#29031;&#20998;&#24067;&#20559;&#31227;&#31561;&#20840;&#23616;&#19981;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#22120;&#38656;&#35201;&#35821;&#20041;&#24067;&#23616;&#26469;&#26144;&#23556;&#26679;&#24335;&#65292;&#23545;&#29305;&#24449;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#23545;&#40784;&#32422;&#26463;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20195;&#26367;&#21453;&#24402;&#19968;&#21270;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32487;&#25215;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#20445;&#25345;&#20102;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#19988;&#25913;&#36827;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#26679;&#24335;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
In semantic image synthesis, the state of the art is dominated by methods that use spatially-adaptive normalization layers, which allow for excellent visual generation quality and editing versatility. Granted their efficacy, recent research efforts have focused toward finer-grained local style control and multi-modal generation. By construction though, such layers tend to overlook global image statistics leading to unconvincing local style editing and causing global inconsistencies such as color or illumination distribution shifts. Also, the semantic layout is required for mapping styles in the generator, putting a strict alignment constraint over the features. In response, we designed a novel architecture where cross-attention layers are used in place of de-normalization ones for conditioning the image generation. Our model inherits the advantages of both solutions, retaining state-of-the-art reconstruction quality, as well as improved global and local style transfer. Code and models 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20849;&#35782;&#20102;&#26368;&#26032;&#30340;&#27515;&#20129;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#20840;&#22240;&#27515;&#20129;&#21040;&#31361;&#21457;&#27515;&#20129;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#32467;&#21512;&#21307;&#30103;&#21382;&#21490;&#12289;&#34880;&#28082;&#26816;&#26597;&#12289;&#33647;&#29289;&#22788;&#26041;&#21644;&#20303;&#38498;&#27835;&#30103;&#21487;&#20197;&#39044;&#27979;&#31361;&#21457;&#27515;&#20129;&#30340;&#39118;&#38505;&#22686;&#21152;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16067</link><description>&lt;p&gt;
&#26368;&#26032;&#30340;&#27515;&#20129;&#39044;&#27979;&#27169;&#22411;&#30340;&#20849;&#35782;&#65306;&#20174;&#20840;&#22240;&#27515;&#20129;&#21040;&#31361;&#21457;&#27515;&#20129;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Consensus of state of the art mortality prediction models: From all-cause mortality to sudden death prediction. (arXiv:2308.16067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20849;&#35782;&#20102;&#26368;&#26032;&#30340;&#27515;&#20129;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#20840;&#22240;&#27515;&#20129;&#21040;&#31361;&#21457;&#27515;&#20129;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#32467;&#21512;&#21307;&#30103;&#21382;&#21490;&#12289;&#34880;&#28082;&#26816;&#26597;&#12289;&#33647;&#29289;&#22788;&#26041;&#21644;&#20303;&#38498;&#27835;&#30103;&#21487;&#20197;&#39044;&#27979;&#31361;&#21457;&#27515;&#20129;&#30340;&#39118;&#38505;&#22686;&#21152;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#27599;&#24180;&#26377;&#25968;&#30334;&#19975;&#20154;&#31361;&#28982;&#21644;&#24847;&#22806;&#27515;&#20129;&#65292;&#26080;&#35770;&#26159;&#21542;&#26377;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20808;&#21069;&#21382;&#21490;&#12290;&#36825;&#31181;&#20107;&#20214;&#24456;&#23569;&#35265;&#65292;&#35768;&#22810;&#21463;&#23475;&#32773;&#22312;&#24515;&#33039;&#30142;&#30149;&#20043;&#21069;&#27809;&#26377;&#25509;&#21463;&#36807;&#35843;&#26597;&#65292;&#32780;&#31361;&#21457;&#27515;&#20129;&#30340;&#23450;&#20041;&#20063;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#31361;&#21457;&#27515;&#20129;&#24456;&#38590;&#39044;&#27979;&#12290;&#26412;&#20998;&#26512;&#20351;&#29992;&#20102;2010&#24180;&#22823;&#26684;&#25289;&#26031;&#21733;&#21644;&#20811;&#33713;&#24503;&#65288;GG&#65286;C&#65289;&#22320;&#21306;50&#23681;&#21450;&#20197;&#19978;&#30340;&#20154;&#32676;&#30340;&#33521;&#22269;&#22269;&#27665;&#20445;&#20581;&#26381;&#21153;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#65288;n = 380,000&#65289;&#26469;&#23581;&#35797;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#21307;&#30103;&#21490;&#12289;&#34880;&#28082;&#26816;&#26597;&#12289;&#33647;&#29289;&#22788;&#26041;&#21644;&#20303;&#38498;&#27835;&#30103;&#26159;&#21542;&#30456;&#32467;&#21512;&#21487;&#20197;&#39044;&#27979;&#31361;&#21457;&#27515;&#20129;&#30340;&#39118;&#38505;&#22686;&#21152;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#31361;&#21457;&#27515;&#20129;&#25110;&#20840;&#22240;&#27515;&#20129;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#26500;&#24314;&#20102;&#20845;&#20010;&#27169;&#22411;&#65306;&#19977;&#20010;&#21462;&#33258;&#26368;&#26032;&#30740;&#31350;&#65288;BEHRT&#65292;Deepr&#21644;Deep Patient&#65289;&#65292;&#19977;&#20010;&#20026;&#25105;&#20204;&#33258;&#24049;&#21019;&#24314;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Worldwide, many millions of people die suddenly and unexpectedly each year, either with or without a prior history of cardiovascular disease. Such events are sparse (once in a lifetime), many victims will not have had prior investigations for cardiac disease and many different definitions of sudden death exist. Accordingly, sudden death is hard to predict.  This analysis used NHS Electronic Health Records (EHRs) for people aged $\geq$50 years living in the Greater Glasgow and Clyde (GG\&amp;C) region in 2010 (n = 380,000) to try to overcome these challenges. We investigated whether medical history, blood tests, prescription of medicines, and hospitalisations might, in combination, predict a heightened risk of sudden death.  We compared the performance of models trained to predict either sudden death or all-cause mortality. We built six models for each outcome of interest: three taken from state-of-the-art research (BEHRT, Deepr and Deep Patient), and three of our own creation. We trained t
&lt;/p&gt;</description></item><item><title>Text-to-OverpassQL&#26159;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;OpenStreetMap&#20013;&#22320;&#29702;&#25968;&#25454;&#30340;&#30028;&#38754;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#21046;&#23450;&#22797;&#26434;&#30340;&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#20026;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#19968;&#20010;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.16060</link><description>&lt;p&gt;
Text-to-OverpassQL&#65306;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;OpenStreetMap&#22797;&#26434;&#22320;&#29702;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap. (arXiv:2308.16060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16060
&lt;/p&gt;
&lt;p&gt;
Text-to-OverpassQL&#26159;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;OpenStreetMap&#20013;&#22320;&#29702;&#25968;&#25454;&#30340;&#30028;&#38754;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#21046;&#23450;&#22797;&#26434;&#30340;&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#20026;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#19968;&#20010;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Text-to-OverpassQL&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;OpenStreetMap (OSM)&#20013;&#22320;&#29702;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;Overpass&#26597;&#35810;&#35821;&#35328;&#65288;OverpassQL&#65289;&#20801;&#35768;&#29992;&#25143;&#21046;&#23450;&#22797;&#26434;&#30340;&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#22312;OSM&#29983;&#24577;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;Overpass&#26597;&#35810;&#21487;&#20197;&#28385;&#36275;&#22810;&#20010;&#29992;&#20363;&#12290;&#23427;&#20351;&#26032;&#25163;&#29992;&#25143;&#33021;&#22815;&#22312;&#26080;&#38656;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;OverpassQL&#65292;&#24110;&#21161;&#26377;&#32463;&#39564;&#30340;&#29992;&#25143;&#21046;&#20316;&#39640;&#32423;&#26597;&#35810;&#65292;&#24182;&#20351;&#24037;&#20855;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35775;&#38382;&#23384;&#20648;&#22312;OSM&#25968;&#25454;&#24211;&#20013;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35780;&#20272;&#24403;&#21069;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OverpassNL&#65292;&#19968;&#20010;&#21253;&#21547;8,352&#20010;&#26597;&#35810;&#21644;&#30456;&#24212;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23545;OSM&#25968;&#25454;&#24211;&#25191;&#34892;&#26597;&#35810;&#26469;&#23545;Text-to-OverpassQL&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Text-to-OverpassQL, a task designed to facilitate a natural language interface for querying geodata from OpenStreetMap (OSM). The Overpass Query Language (OverpassQL) allows users to formulate complex database queries and is widely adopted in the OSM ecosystem. Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database. In order to assess the performance of current sequence generation models on this task, we propose OverpassNL, a dataset of 8,352 queries with corresponding natural language inputs. We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. We establish strong baselines by finetuning sequence-to-sequence models a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#24322;&#27493;&#23398;&#20064;&#26041;&#27861;&#24182;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#25512;&#26029;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#31890;&#24230;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#24314;&#27169;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16055</link><description>&lt;p&gt;
&#24322;&#27493;&#23398;&#20064;&#30340;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#25512;&#26029;&#26041;&#27861;&#19982;&#36741;&#21161;&#20851;&#31995;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AsyncET: Asynchronous Learning for Knowledge Graph Entity Typing with Auxiliary Relations. (arXiv:2308.16055v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#24322;&#27493;&#23398;&#20064;&#26041;&#27861;&#24182;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#25512;&#26029;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#31890;&#24230;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#24314;&#27169;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#25512;&#26029;&#65288;KGET&#65289;&#26159;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#20197;&#21069;&#65292;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20851;&#31995;&#8220;hasType&#8221;&#26469;&#24314;&#27169;&#23454;&#20307;&#19982;&#20854;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23581;&#35797;&#35299;&#20915;KGET&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#36741;&#21161;&#20851;&#31995;&#22312;&#34920;&#36798;&#22810;&#26679;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#38480;&#21046;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#20851;&#31995;&#26469;&#25552;&#39640;KGE&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#31867;&#20284;&#30340;&#23454;&#20307;&#31867;&#22411;&#34987;&#20998;&#32452;&#65292;&#20197;&#20943;&#23569;&#36741;&#21161;&#20851;&#31995;&#30340;&#25968;&#37327;&#65292;&#24182;&#25552;&#39640;&#23427;&#20204;&#23545;&#19981;&#21516;&#31890;&#24230;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#24314;&#27169;&#30340;&#33021;&#21147;&#12290;&#22312;&#22810;&#20010;&#36741;&#21161;&#20851;&#31995;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AsyncET&#30340;&#23454;&#20307;&#31867;&#22411;&#24322;&#27493;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20132;&#26367;&#26356;&#26032;&#23454;&#20307;&#21644;&#31867;&#22411;&#23884;&#20837;&#26469;&#20445;&#25345;&#23398;&#21040;&#30340;&#23454;&#20307;&#23884;&#20837;&#23545;&#20110;&#23454;&#20307;&#31867;&#22411;&#25512;&#26029;&#30340;&#26368;&#26032;&#21644;&#20449;&#24687;&#20016;&#23500;&#12290;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;KGE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph entity typing (KGET) is a task to predict the missing entity types in knowledge graphs (KG). Previously, KG embedding (KGE) methods tried to solve the KGET task by introducing an auxiliary relation, 'hasType', to model the relationship between entities and their types. However, a single auxiliary relation has limited expressiveness for diverse entity-type patterns. We improve the expressiveness of KGE methods by introducing multiple auxiliary relations in this work. Similar entity types are grouped to reduce the number of auxiliary relations and improve their capability to model entity-type patterns with different granularities. With the presence of multiple auxiliary relations, we propose a method adopting an Asynchronous learning scheme for Entity Typing, named AsyncET, which updates the entity and type embeddings alternatively to keep the learned entity embedding up-to-date and informative for entity type prediction. Experiments are conducted on two commonly used KGE
&lt;/p&gt;</description></item><item><title>EnsembleFollower&#26159;&#19968;&#20010;&#37319;&#29992;&#20998;&#23618;&#35268;&#21010;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#36710;&#36742;&#36319;&#39536;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#20808;&#36827;&#30340;&#31867;&#20154;&#36710;&#36742;&#36319;&#39536;&#12290;</title><link>http://arxiv.org/abs/2308.16008</link><description>&lt;p&gt;
EnsembleFollower: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#20998;&#23618;&#35268;&#21010;&#30340;&#28151;&#21512;&#36710;&#36742;&#36319;&#39536;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EnsembleFollower: A Hybrid Car-Following Framework Based On Reinforcement Learning and Hierarchical Planning. (arXiv:2308.16008v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16008
&lt;/p&gt;
&lt;p&gt;
EnsembleFollower&#26159;&#19968;&#20010;&#37319;&#29992;&#20998;&#23618;&#35268;&#21010;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#36710;&#36742;&#36319;&#39536;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#20808;&#36827;&#30340;&#31867;&#20154;&#36710;&#36742;&#36319;&#39536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#36319;&#39536;&#27169;&#22411;&#23545;&#20110;&#25105;&#20204;&#23545;&#32437;&#21521;&#34892;&#39542;&#34892;&#20026;&#30340;&#29702;&#35299;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#20934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#36319;&#39536;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#30001;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#21463;&#38480;&#39550;&#39542;&#25216;&#33021;&#32780;&#22312;&#26410;&#30693;&#24773;&#20917;&#19979;&#20986;&#29616;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#27599;&#31181;&#36710;&#36742;&#36319;&#39536;&#27169;&#22411;&#22312;&#29305;&#23450;&#39550;&#39542;&#22330;&#26223;&#20013;&#20855;&#26377;&#33258;&#24049;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EnsembleFollower&#30340;&#20998;&#23618;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#20808;&#36827;&#30340;&#31867;&#20154;&#36710;&#36742;&#36319;&#39536;&#12290;EnsembleFollower&#26694;&#26550;&#28041;&#21450;&#19968;&#20010;&#39640;&#23618;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20351;&#29992;&#65292;&#36127;&#36131;&#26681;&#25454;&#24403;&#21069;&#29366;&#24577;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#20302;&#23618;&#27169;&#22411;&#25191;&#34892;&#21160;&#20316;&#25110;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;&#26469;&#31649;&#29702;&#22810;&#20010;&#20302;&#23618;&#36710;&#36742;&#36319;&#39536;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Car-following models have made significant contributions to our understanding of longitudinal driving behavior. However, they often exhibit limited accuracy and flexibility, as they cannot fully capture the complexity inherent in car-following processes, or may falter in unseen scenarios due to their reliance on confined driving skills present in training data. It is worth noting that each car-following model possesses its own strengths and weaknesses depending on specific driving scenarios. Therefore, we propose EnsembleFollower, a hierarchical planning framework for achieving advanced human-like car-following. The EnsembleFollower framework involves a high-level Reinforcement Learning-based agent responsible for judiciously managing multiple low-level car-following models according to the current state, either by selecting an appropriate low-level model to perform an action or by allocating different weights across all low-level components. Moreover, we propose a jerk-constrained kin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;DL&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;RL&#30340;&#25506;&#32034;&#24615;&#36136;&#65292;&#25552;&#39640;&#20102;&#36712;&#36857;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#22312;&#30495;&#23454;&#31995;&#32479;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15991</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#39550;&#39542;&#20013;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#30340;&#36712;&#36857;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous Driving. (arXiv:2308.15991v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;DL&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;RL&#30340;&#25506;&#32034;&#24615;&#36136;&#65292;&#25552;&#39640;&#20102;&#36712;&#36857;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#22312;&#30495;&#23454;&#31995;&#32479;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24635;&#26159;&#24314;&#31435;&#22312;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#65288;&#22914;&#35268;&#21010;&#22120;&#21644;&#25511;&#21046;&#22120;&#65289;&#20043;&#19978;&#12290;&#20934;&#30830;&#32780;&#31283;&#20581;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#23545;&#20110;&#36825;&#20123;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#26469;&#35828;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#21407;&#22987;&#20363;&#31243;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#23545;&#27169;&#22411;&#65288;&#22914;&#19978;&#19979;&#25991;&#21644;&#21160;&#21147;&#23398;&#65289;&#20570;&#20986;&#20102;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#36825;&#20123;&#20551;&#35774;&#19981;&#36275;&#20197;&#24212;&#23545;&#30495;&#23454;&#31995;&#32479;&#20013;&#30340;&#22330;&#26223;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#12290;DL&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;RL&#30340;&#25506;&#32034;&#24615;&#36136;&#26082;&#24102;&#26469;&#20102;&#24378;&#20581;&#24615;&#21448;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#36890;&#36807;&#20197;&#26080;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36816;&#34892;&#36712;&#36857;&#36319;&#36394;&#26469;&#22686;&#24378;&#20102;&#36890;&#29992;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#24403;&#21069;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving systems are always built on motion-related modules such as the planner and the controller. An accurate and robust trajectory tracking method is indispensable for these motion-related modules as a primitive routine. Current methods often make strong assumptions about the model such as the context and the dynamics, which are not robust enough to deal with the changing scenarios in a real-world system. In this paper, we propose a Deep Reinforcement Learning (DRL)-based trajectory tracking method for the motion-related modules in autonomous driving systems. The representation learning ability of DL and the exploration nature of RL bring strong robustness and improve accuracy. Meanwhile, it enhances versatility by running the trajectory tracking in a model-free and data-driven manner. Through extensive experiments, we demonstrate both the efficiency and effectiveness of our method compared to current methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;W4A8&#21644;W8A8&#20004;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#36880;&#23618;&#28608;&#27963;&#37327;&#21270;&#21644;&#32454;&#31890;&#24230;&#26435;&#37325;&#37327;&#21270;&#26469;&#35299;&#20915;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15987</link><description>&lt;p&gt;
FPTQ&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#35757;&#32451;&#21518;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
FPTQ: Fine-grained Post-Training Quantization for Large Language Models. (arXiv:2308.15987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;W4A8&#21644;W8A8&#20004;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#36880;&#23618;&#28608;&#27963;&#37327;&#21270;&#21644;&#32454;&#31890;&#24230;&#26435;&#37325;&#37327;&#21270;&#26469;&#35299;&#20915;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#20013;&#65292;&#24222;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#32473;&#37096;&#32626;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20027;&#27969;&#23454;&#36341;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#20004;&#31181;&#26041;&#26696;W8A8&#21644;W4A16&#65288;&#21363;&#36825;&#20004;&#31181;&#20301;&#23485;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;W4A8&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#28304;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;4&#20301;&#26435;&#37325;&#37327;&#21270;&#30340;I/O&#21033;&#29992;&#29575;&#20248;&#21183;&#21644;8&#20301;&#30697;&#38453;&#35745;&#31639;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;W4A8&#38754;&#20020;&#30528;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36880;&#23618;&#28608;&#27963;&#37327;&#21270;&#31574;&#30053;&#65292;&#23545;&#20110;&#26368;&#26840;&#25163;&#30340;&#23618;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25968;&#22343;&#34913;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#32454;&#31890;&#24230;&#26435;&#37325;&#37327;&#21270;&#30456;&#32467;&#21512;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#30340;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of large-scale language models, the substantial parameter size poses significant challenges for deployment. Being a prevalent compression technique, quantization has emerged as the mainstream practice to tackle this issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activations in such bit widths). In this study, we propose a novel W4A8 post-training quantization method for the available open-sourced LLMs, which combines the advantages of both two recipes. Therefore, we can leverage the benefit in the I/O utilization of 4-bit weight quantization and the acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces notorious performance degradation. As a remedy, we involve layerwise activation quantization strategies which feature a novel logarithmic equalization for most intractable layers, and we combine them with fine-grained weight quantization. Without whistles and bells, we eliminate the necessity for further fine-tuning and obt
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#26159;&#20851;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;&#31532;&#19968;&#31687;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#30340;&#32508;&#36848;&#65292;&#23427;&#25506;&#35752;&#20102;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#21644;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#22806;&#29305;&#24449;&#20197;&#21450;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.15985</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#21644;&#39044;&#27979;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Vision-Based Traffic Accident Detection and Anticipation: A Survey. (arXiv:2308.15985v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#26159;&#20851;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;&#31532;&#19968;&#31687;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#30340;&#32508;&#36848;&#65292;&#23427;&#25506;&#35752;&#20102;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#21644;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#22806;&#29305;&#24449;&#20197;&#21450;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#21644;&#39044;&#27979;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#36947;&#36335;&#23433;&#20840;&#38382;&#39064;&#65292;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#38543;&#30528;&#35270;&#39057;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#21644;&#39044;&#27979;&#65288;&#31216;&#20026;Vision-TAD&#21644;Vision-TAA&#65289;&#25104;&#20026;&#23433;&#20840;&#39550;&#39542;&#21644;&#30417;&#25511;&#23433;&#20840;&#30340;&#26368;&#21518;&#19968;&#33521;&#37324;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20132;&#36890;&#20107;&#25925;&#30340;&#38271;&#23614;&#12289;&#19981;&#24179;&#34913;&#12289;&#39640;&#21160;&#24577;&#12289;&#22797;&#26434;&#21644;&#19981;&#30830;&#23450;&#23646;&#24615;&#24418;&#25104;&#20102;Vision-TAD&#21644;Vision-TAA&#30340;&#20998;&#24067;&#22806;&#29305;&#24449;&#12290;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21487;&#33021;&#20250;&#20851;&#27880;&#36825;&#20123;&#20998;&#24067;&#22806;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32508;&#36848;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#25552;&#20986;&#20102;&#31532;&#19968;&#31687;&#20851;&#20110;Vision-TAD&#30340;&#32508;&#36848;&#65292;&#20063;&#26159;&#31532;&#19968;&#31687;&#20851;&#20110;Vision-TAA&#30340;&#32508;&#36848;&#12290;&#22312;&#35843;&#26597;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#20010;&#30740;&#31350;&#21407;&#22411;&#30340;&#20248;&#32570;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;31&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21407;&#22411;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic accident detection and anticipation is an obstinate road safety problem and painstaking efforts have been devoted. With the rapid growth of video data, Vision-based Traffic Accident Detection and Anticipation (named Vision-TAD and Vision-TAA) become the last one-mile problem for safe driving and surveillance safety. However, the long-tailed, unbalanced, highly dynamic, complex, and uncertain properties of traffic accidents form the Out-of-Distribution (OOD) feature for Vision-TAD and Vision-TAA. Current AI development may focus on these OOD but important problems. What has been done for Vision-TAD and Vision-TAA? What direction we should focus on in the future for this problem? A comprehensive survey is important. We present the first survey on Vision-TAD in the deep learning era and the first-ever survey for Vision-TAA. The pros and cons of each research prototype are discussed in detail during the investigation. In addition, we also provide a critical review of 31 publicly av
&lt;/p&gt;</description></item><item><title>RoboTAP&#36890;&#36807;&#21033;&#29992;&#31264;&#23494;&#36861;&#36394;&#25216;&#26415;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#26222;&#36866;&#30340;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;&#30701;&#26102;&#38388;&#20869;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35299;&#20915;&#22797;&#26434;&#30340;&#29289;&#20307;&#25490;&#21015;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15975</link><description>&lt;p&gt;
RoboTAP: &#36861;&#36394;&#20219;&#24847;&#28857;&#36827;&#34892;&#23569;&#26679;&#26412;&#35270;&#35273;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation. (arXiv:2308.15975v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15975
&lt;/p&gt;
&lt;p&gt;
RoboTAP&#36890;&#36807;&#21033;&#29992;&#31264;&#23494;&#36861;&#36394;&#25216;&#26415;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#26222;&#36866;&#30340;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;&#30701;&#26102;&#38388;&#20869;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35299;&#20915;&#22797;&#26434;&#30340;&#29289;&#20307;&#25490;&#21015;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#22312;&#23454;&#39564;&#23460;&#21644;&#19987;&#38376;&#30340;&#24037;&#21378;&#20043;&#22806;&#20063;&#33021;&#21457;&#25381;&#20316;&#29992;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#24555;&#36895;&#25945;&#25480;&#23427;&#20204;&#26032;&#30340;&#26377;&#29992;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#32570;&#20047;&#26222;&#36866;&#24615;&#20197;&#36827;&#34892;&#26032;&#20219;&#21153;&#30340;&#19978;&#32447;&#65292;&#32780;&#19981;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#31243;&#21270;&#65292;&#35201;&#20040;&#32570;&#20047;&#25968;&#25454;&#25928;&#29575;&#65292;&#26080;&#27861;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#23436;&#25104;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31264;&#23494;&#36861;&#36394;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#12289;&#26356;&#26222;&#36866;&#30340;&#31034;&#25945;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Track-Any-Point (TAP)&#27169;&#22411;&#65292;&#23558;&#28436;&#31034;&#20013;&#30340;&#30456;&#20851;&#36816;&#21160;&#38548;&#31163;&#20986;&#26469;&#65292;&#24182;&#21442;&#25968;&#21270;&#19968;&#20010;&#20302;&#32423;&#25511;&#21046;&#22120;&#65292;&#22312;&#22330;&#26223;&#37197;&#32622;&#21457;&#29983;&#21464;&#21270;&#26102;&#37325;&#29616;&#35813;&#36816;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#31283;&#20581;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#29289;&#20307;&#25490;&#21015;&#20219;&#21153;&#65292;&#22914;&#24418;&#29366;&#21305;&#37197;&#12289;&#21472;&#25918;&#65292;&#29978;&#33267;&#21487;&#20197;&#23436;&#25104;&#23436;&#25972;&#30340;&#36335;&#24452;&#36319;&#36394;&#20219;&#21153;&#65292;&#22914;&#26045;&#33014;&#21644;&#31896;&#21512;&#29289;&#20307;&#65292;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#30340;&#28436;&#31034;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#25910;&#38598;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration. We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching, stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;ITERS&#65292;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#21453;&#39304;&#30340;&#36845;&#20195;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;&#22870;&#21169;&#20989;&#25968;&#35268;&#26684;&#21270;&#38169;&#35823;&#12290;&#36890;&#36807;&#35753;&#29992;&#25143;&#25552;&#20379;&#22522;&#20110;&#36712;&#36857;&#30340;&#21453;&#39304;&#65292;&#24182;&#32467;&#21512;&#29992;&#25143;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#30340;&#22870;&#21169;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#31181;&#29615;&#22659;&#20013;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.15969</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#26426;&#21453;&#39304;&#30340;&#36845;&#20195;&#22870;&#21169;&#22609;&#36896;&#26469;&#32416;&#27491;&#22870;&#21169;&#35268;&#26684;&#21270;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Iterative Reward Shaping using Human Feedback for Correcting Reward Misspecification. (arXiv:2308.15969v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;ITERS&#65292;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#21453;&#39304;&#30340;&#36845;&#20195;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;&#22870;&#21169;&#20989;&#25968;&#35268;&#26684;&#21270;&#38169;&#35823;&#12290;&#36890;&#36807;&#35753;&#29992;&#25143;&#25552;&#20379;&#22522;&#20110;&#36712;&#36857;&#30340;&#21453;&#39304;&#65292;&#24182;&#32467;&#21512;&#29992;&#25143;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#30340;&#22870;&#21169;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#31181;&#29615;&#22659;&#20013;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#30340;&#25104;&#21151;&#35757;&#32451;&#65292;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#22870;&#21169;&#20989;&#25968;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#23450;&#20041;&#19968;&#20010;&#21512;&#36866;&#30340;&#22870;&#21169;&#20989;&#25968;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24320;&#21457;&#20154;&#21592;&#24120;&#24120;&#19981;&#24471;&#19981;&#20174;&#19968;&#20010;&#21021;&#22987;&#30340;&#12289;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#30340;&#22870;&#21169;&#20989;&#25968;&#24320;&#22987;&#65292;&#24182;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#23398;&#20064;&#34892;&#20026;&#36845;&#20195;&#22320;&#35843;&#25972;&#20854;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;ITERS&#65292;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#21453;&#39304;&#30340;&#36845;&#20195;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#26469;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#20197;&#20943;&#36731;&#22870;&#21169;&#20989;&#25968;&#35268;&#26684;&#21270;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#22522;&#20110;&#36712;&#36857;&#30340;&#21453;&#39304;&#65292;&#36825;&#20123;&#21453;&#39304;&#21487;&#20197;&#20316;&#20026;&#19979;&#19968;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#30340;&#22870;&#21169;&#22609;&#36896;&#20449;&#21495;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#36824;&#20801;&#35768;&#29992;&#25143;&#25552;&#20379;&#23545;&#20182;&#20204;&#30340;&#21453;&#39304;&#30340;&#35299;&#37322;&#65292;&#36825;&#20123;&#35299;&#37322;&#34987;&#29992;&#26469;&#22686;&#24378;&#21453;&#39304;&#24182;&#20943;&#23569;&#29992;&#25143;&#30340;&#24037;&#20316;&#37327;&#21644;&#21453;&#39304;&#39057;&#29575;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;ITERS&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A well-defined reward function is crucial for successful training of an reinforcement learning (RL) agent. However, defining a suitable reward function is a notoriously challenging task, especially in complex, multi-objective environments. Developers often have to resort to starting with an initial, potentially misspecified reward function, and iteratively adjusting its parameters, based on observed learned behavior. In this work, we aim to automate this process by proposing ITERS, an iterative reward shaping approach using human feedback for mitigating the effects of a misspecified reward function. Our approach allows the user to provide trajectory-level feedback on agent's behavior during training, which can be integrated as a reward shaping signal in the following training iteration. We also allow the user to provide explanations of their feedback, which are used to augment the feedback and reduce user effort and feedback frequency. We evaluate ITERS in three environments and show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#21442;&#25968;&#35843;&#25972;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#21442;&#25968;&#35843;&#25972;&#26368;&#26032;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15965</link><description>&lt;p&gt;
&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of Parameter Tuning Methods for Nature-Inspired Algorithms. (arXiv:2308.15965v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#21442;&#25968;&#35843;&#25972;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#21442;&#25968;&#35843;&#25972;&#26368;&#26032;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20046;&#25152;&#26377;&#20248;&#21270;&#31639;&#27861;&#37117;&#26377;&#19982;&#31639;&#27861;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#32780;&#36825;&#20123;&#21442;&#25968;&#30340;&#35774;&#32622;&#21487;&#20197;&#26497;&#22823;&#22320;&#24433;&#21709;&#25152;&#32771;&#34385;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#24212;&#36827;&#34892;&#36866;&#24403;&#30340;&#21442;&#25968;&#35843;&#25972;&#65292;&#20197;&#30830;&#20445;&#29992;&#20110;&#20248;&#21270;&#30340;&#31639;&#27861;&#21487;&#20197;&#33391;&#22909;&#22320;&#25191;&#34892;&#65292;&#24182;&#33021;&#22815;&#36275;&#22815;&#31283;&#20581;&#22320;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#31456;&#32508;&#36848;&#20102;&#19968;&#20123;&#20027;&#35201;&#30340;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#21442;&#25968;&#35843;&#25972;&#30340;&#26368;&#26032;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20123;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Almost all optimization algorithms have algorithm-dependent parameters, and the setting of such parameter values can largely influence the behaviour of the algorithm under consideration. Thus, proper parameter tuning should be carried out to ensure the algorithm used for optimization may perform well and can be sufficiently robust for solving different types of optimization problems. This chapter reviews some of the main methods for parameter tuning and then highlights the important issues concerning the latest development in parameter tuning. A few open problems are also discussed with some recommendations for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#23450;&#20301;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#31995;&#32479;&#38598;&#25104;&#65292;&#36890;&#36807;&#20351;&#29992;WALL-E&#23454;&#29616;&#20102;&#22312;&#39184;&#21381;&#22330;&#26223;&#20013;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#20351;WALL-E&#25104;&#20026;&#19968;&#20301;&#26356;&#26377;&#33021;&#21147;&#21644;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#26381;&#21153;&#21592;&#12290;</title><link>http://arxiv.org/abs/2308.15962</link><description>&lt;p&gt;
WALL-E: &#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#26426;&#22120;&#20154;&#26381;&#21153;&#21592;&#20030;&#37325;
&lt;/p&gt;
&lt;p&gt;
WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model. (arXiv:2308.15962v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#23450;&#20301;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#31995;&#32479;&#38598;&#25104;&#65292;&#36890;&#36807;&#20351;&#29992;WALL-E&#23454;&#29616;&#20102;&#22312;&#39184;&#21381;&#22330;&#26223;&#20013;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#20351;WALL-E&#25104;&#20026;&#19968;&#20301;&#26356;&#26377;&#33021;&#21147;&#21644;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#26381;&#21153;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#35821;&#35328;&#25351;&#20196;&#24182;&#26681;&#25454;&#35270;&#35273;&#24863;&#30693;&#20570;&#20986;&#21453;&#24212;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#26426;&#22120;&#20154;&#30740;&#31350;&#30028;&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#24037;&#31243;&#26041;&#38754;&#21462;&#24471;&#21069;&#27839;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#23558;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29616;&#26377;&#30340;&#35270;&#35273;&#23450;&#20301;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#31995;&#32479;&#38598;&#25104;&#20197;&#22686;&#24378;&#20154;&#26426;&#20132;&#20114;&#25928;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20197;WALL-E&#65288;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#26426;&#22120;&#20154;&#26381;&#21153;&#21592;&#20030;&#37325;&#65289;&#20316;&#20026;&#38598;&#25104;&#30340;&#31034;&#20363;&#12290;&#31995;&#32479;&#21033;&#29992;ChatGPT&#30340;LLM&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#24335;&#23545;&#35805;&#23558;&#29992;&#25143;&#30340;&#20559;&#22909;&#29289;&#20307;&#24635;&#32467;&#20026;&#30446;&#26631;&#25351;&#20196;&#12290;&#28982;&#21518;&#23558;&#30446;&#26631;&#25351;&#20196;&#20256;&#36882;&#32473;&#35270;&#35273;&#23450;&#20301;&#31995;&#32479;&#36827;&#34892;&#29289;&#20307;&#23039;&#21183;&#21644;&#22823;&#23567;&#20272;&#35745;&#65292;&#28982;&#21518;&#26426;&#22120;&#20154;&#30456;&#24212;&#22320;&#25235;&#21462;&#29289;&#20307;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;LLM&#22686;&#24378;&#31995;&#32479;&#37096;&#32626;&#22312;
&lt;/p&gt;
&lt;p&gt;
Enabling robots to understand language instructions and react accordingly to visual perception has been a long-standing goal in the robotics research community. Achieving this goal requires cutting-edge advances in natural language processing, computer vision, and robotics engineering. Thus, this paper mainly investigates the potential of integrating the most recent Large Language Models (LLMs) and existing visual grounding and robotic grasping system to enhance the effectiveness of the human-robot interaction. We introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language model) as an example of this integration. The system utilizes the LLM of ChatGPT to summarize the preference object of the users as a target instruction via the multi-round interactive dialogue. The target instruction is then forwarded to a visual grounding system for object pose and size estimation, following which the robot grasps the object accordingly. We deploy this LLM-empowered system on the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21608;&#26399;&#24615;&#25490;&#26021;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24490;&#29615;&#26469;&#24809;&#32602;&#20887;&#20313;&#32780;&#19981;&#22870;&#21169;&#26032;&#39062;&#24615;&#65292;&#22312;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#33021;&#21462;&#24471;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15911</link><description>&lt;p&gt;
&#21608;&#26399;&#24615;&#25490;&#26021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cyclophobic Reinforcement Learning. (arXiv:2308.15911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21608;&#26399;&#24615;&#25490;&#26021;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24490;&#29615;&#26469;&#24809;&#32602;&#20887;&#20313;&#32780;&#19981;&#22870;&#21169;&#26032;&#39062;&#24615;&#65292;&#22312;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#33021;&#21462;&#24471;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#65292;&#23547;&#25214;&#33391;&#22909;&#30340;&#24402;&#32435;&#20559;&#24046;&#20197;&#20419;&#36827;&#25506;&#32034;&#23545;&#20110;&#26234;&#33021;&#20307;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#30528;&#20004;&#20010;&#31454;&#20105;&#30340;&#30446;&#26631;&#65306;&#26032;&#39062;&#24615;&#25628;&#32034;&#21644;&#31995;&#32479;&#24615;&#25506;&#32034;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22914;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#25506;&#32034;&#65292;&#21487;&#20197;&#25214;&#21040;&#26032;&#39062;&#24615;&#65292;&#20294;&#26377;&#26102;&#19981;&#20250;&#23545;&#25972;&#20010;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#31995;&#32479;&#24615;&#30340;&#25506;&#32034;&#65292;&#31867;&#20284;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#19982;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#21363;&#21608;&#26399;&#24615;&#25490;&#26021;&#22411;&#65292;&#23427;&#19981;&#20250;&#22870;&#21169;&#26032;&#39062;&#24615;&#65292;&#32780;&#26159;&#36890;&#36807;&#36991;&#20813;&#24490;&#29615;&#26469;&#24809;&#32602;&#20887;&#20313;&#12290;&#36890;&#36807;&#23558;&#21608;&#26399;&#24615;&#25490;&#26021;&#30340;&#20869;&#22312;&#22870;&#21169;&#19982;&#22522;&#20110;&#26234;&#33021;&#20307;&#35009;&#21098;&#35266;&#27979;&#30340;&#20998;&#23618;&#34920;&#31034;&#24207;&#21015;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#22312;MiniGrid&#21644;MiniHack&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#26497;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20004;&#20010;&#29615;&#22659;&#37117;&#29305;&#21035;&#38590;&#20197;&#35299;&#20915;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19982;&#19981;&#21516;&#23545;&#35937;&#36827;&#34892;&#22797;&#26434;&#30340;&#20132;&#20114;&#25165;&#33021;&#35299;&#20915;&#12290;&#19982;&#20808;&#21069;&#26041;&#27861;&#30340;&#35814;&#32454;&#27604;&#36739;&#21644;&#24443;&#24213;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#26032;&#25552;&#20986;&#30340;&#21608;&#26399;&#24615;&#25490;&#26021;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In environments with sparse rewards, finding a good inductive bias for exploration is crucial to the agent's success. However, there are two competing goals: novelty search and systematic exploration. While existing approaches such as curiosity-driven exploration find novelty, they sometimes do not systematically explore the whole state space, akin to depth-first-search vs breadth-first-search. In this paper, we propose a new intrinsic reward that is cyclophobic, i.e., it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the agent's cropped observations we are able to achieve excellent results in the MiniGrid and MiniHack environments. Both are particularly hard, as they require complex interactions with different objects in order to be solved. Detailed comparisons with previous approaches and thorough ablation studies show that our newly proposed cyclophobic reinforc
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15906</link><description>&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#20307;&#31995;&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#23545;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#25361;&#25112;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the U.S. Legal System Ready for AI's Challenges to Human Values?. (arXiv:2308.15906v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15906
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35843;&#26597;&#20102;&#32654;&#22269;&#27861;&#24459;&#22312;&#38754;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19987;&#23478;&#30740;&#35752;&#20250;&#26399;&#38388;&#21046;&#23450;&#30340;&#22810;&#31181;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#33258;&#20027;&#26435;&#12289;&#38544;&#31169;&#26435;&#12289;&#23562;&#20005;&#12289;&#22810;&#26679;&#24615;&#12289;&#24179;&#31561;&#20197;&#21450;&#36523;&#24515;&#20581;&#24247;&#31561;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#23466;&#27861;&#21644;&#27665;&#26435;&#27861;&#20284;&#20046;&#26080;&#27861;&#23545;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#27495;&#35270;&#24615;&#20135;&#20986;&#25552;&#20379;&#36275;&#22815;&#30340;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#25105;&#20204;&#25490;&#38500;&#31532;230&#26465;&#27454;&#25552;&#20379;&#30340;&#36131;&#20219;&#20445;&#25252;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#24615;&#65292;&#35777;&#26126;&#35837;&#35876;&#21644;&#20135;&#21697;&#36131;&#20219;&#32034;&#36180;&#30340;&#22240;&#26524;&#20851;&#31995;&#20063;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#30340;&#29420;&#29305;&#21644;&#38590;&#20197;&#39044;&#27979;&#30340;&#23041;&#32961;&#65292;&#25105;&#20204;&#20027;&#24352;&#24314;&#31435;&#33021;&#22815;&#36866;&#24212;&#26032;&#23041;&#32961;&#24182;&#20026;&#34892;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#30340;&#27861;&#24459;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our interdisciplinary study investigates how effectively U.S. laws confront the challenges posed by Generative AI to human values. Through an analysis of diverse hypothetical scenarios crafted during an expert workshop, we have identified notable gaps and uncertainties within the existing legal framework regarding the protection of fundamental values, such as autonomy, privacy, dignity, diversity, equality, and physical/mental well-being. Constitutional and civil rights, it appears, may not provide sufficient protection against AI-generated discriminatory outputs. Furthermore, even if we exclude the liability shield provided by Section 230, proving causation for defamation and product liability claims is a challenging endeavor due to the intricate and opaque nature of AI systems. To address the unique and unforeseeable threats posed by Generative AI, we advocate for legal frameworks that evolve to recognize new threat and provide proactive, auditable guidelines to industry stakeholders
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#20027;&#37327;&#23376;&#28909;&#26426;&#23454;&#29616;&#20102;&#28909;&#21147;&#23398;&#35745;&#31639;&#27169;&#22411;&#12290;&#36890;&#36807;&#28909;&#27969;&#36827;&#34892;&#35745;&#31639;&#65292;&#21487;&#23454;&#29616;&#20219;&#20309;&#32447;&#24615;&#21487;&#20998;&#31163;&#20989;&#25968;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#31070;&#32463;&#20803;&#32593;&#32476;&#25191;&#34892;&#20219;&#20309; needed&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15905</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#20027;&#37327;&#23376;&#28909;&#26426;&#23454;&#29616;&#28909;&#21147;&#23398;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Thermodynamic Computing via Autonomous Quantum Thermal Machines. (arXiv:2308.15905v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15905
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#20027;&#37327;&#23376;&#28909;&#26426;&#23454;&#29616;&#20102;&#28909;&#21147;&#23398;&#35745;&#31639;&#27169;&#22411;&#12290;&#36890;&#36807;&#28909;&#27969;&#36827;&#34892;&#35745;&#31639;&#65292;&#21487;&#23454;&#29616;&#20219;&#20309;&#32447;&#24615;&#21487;&#20998;&#31163;&#20989;&#25968;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#31070;&#32463;&#20803;&#32593;&#32476;&#25191;&#34892;&#20219;&#20309; needed&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#33258;&#20027;&#37327;&#23376;&#28909;&#26426;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#32463;&#20856;&#35745;&#31639;&#12290;&#36825;&#20123;&#26426;&#22120;&#30001;&#23569;&#25968;&#30456;&#20114;&#20316;&#29992;&#30340;&#37327;&#23376;&#20301;&#65288;qubits&#65289;&#19982;&#19981;&#21516;&#28201;&#24230;&#30340;&#20960;&#20010;&#29615;&#22659;&#30456;&#36830;&#12290;&#36890;&#36807;&#26426;&#22120;&#30340;&#28909;&#27969;&#36827;&#34892;&#35745;&#31639;&#12290;&#36807;&#31243;&#20174;&#26681;&#25454;&#36923;&#36753;&#36755;&#20837;&#35774;&#23450;&#29615;&#22659;&#28201;&#24230;&#24320;&#22987;&#12290;&#26426;&#22120;&#28436;&#21270;&#65292;&#26368;&#32456;&#36798;&#21040;&#38750;&#24179;&#34913;&#31283;&#24577;&#65292;&#36890;&#36807;&#36741;&#21161;&#26377;&#38480;&#23610;&#23544;&#20648;&#23384;&#27744;&#30340;&#28201;&#24230;&#21487;&#20197;&#30830;&#23450;&#35745;&#31639;&#32467;&#26524;&#12290;&#36825;&#26679;&#30340;&#26426;&#22120;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#28909;&#21147;&#23398;&#31070;&#32463;&#20803;&#8221;&#65292;&#21487;&#20197;&#23454;&#29616;&#20219;&#20309;&#32447;&#24615;&#21487;&#20998;&#31163;&#20989;&#25968;&#65292;&#25105;&#20204;&#26126;&#30830;&#35752;&#35770;&#20102;NOT&#65292;3-majority&#21644;NOR&#38376;&#30340;&#24773;&#20917;&#12290;&#21453;&#36807;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28909;&#21147;&#23398;&#31070;&#32463;&#20803;&#32593;&#32476;&#21487;&#20197;&#25191;&#34892;&#20219;&#20309;&#38656;&#35201;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20154;&#24037;&#31070;&#32463;&#20803;&#65288;&#24863;&#30693;&#22120;&#65289;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#65292;&#24182;&#35748;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a physics-based model for classical computation based on autonomous quantum thermal machines. These machines consist of few interacting quantum bits (qubits) connected to several environments at different temperatures. Heat flows through the machine are here exploited for computing. The process starts by setting the temperatures of the environments according to the logical input. The machine evolves, eventually reaching a non-equilibrium steady state, from which the output of the computation can be determined via the temperature of an auxilliary finite-size reservoir. Such a machine, which we term a "thermodynamic neuron", can implement any linearly-separable function, and we discuss explicitly the cases of NOT, 3-majority and NOR gates. In turn, we show that a network of thermodynamic neurons can perform any desired function. We discuss the close connection between our model and artificial neurons (perceptrons), and argue that our model provides an alternative physics-based
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#24615;&#31572;&#26696;&#38598;&#32534;&#31243;&#65292;&#35813;&#32534;&#31243;&#26041;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;ASP&#35299;&#20915;&#26041;&#26696;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15901</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#31572;&#26696;&#38598;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Explainable Answer-set Programming. (arXiv:2308.15901v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15901
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#24615;&#31572;&#26696;&#38598;&#32534;&#31243;&#65292;&#35813;&#32534;&#31243;&#26041;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;ASP&#35299;&#20915;&#26041;&#26696;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30001;&#20110;AI&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#20960;&#20046;&#26080;&#22788;&#19981;&#22312;&#65292;&#20197;&#21450;AI&#31995;&#32479;&#30340;&#26085;&#30410;&#22797;&#26434;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#31572;&#26696;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#20351;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;&#24037;&#19994;&#20248;&#21270;&#12289;&#30693;&#35782;&#31649;&#29702;&#25110;&#29983;&#21629;&#31185;&#23398;&#65292;&#22240;&#27492;&#22312;&#21487;&#35299;&#37322;&#24615;&#30340;&#32972;&#26223;&#19979;&#20855;&#26377;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#30830;&#20445;ASP&#20316;&#20026;&#26410;&#26469;&#30340;&#38382;&#39064;&#35299;&#20915;&#33539;&#24335;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#30740;&#31350;ASP&#35299;&#20915;&#26041;&#26696;&#30340;&#35299;&#37322;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#26679;&#30340;&#35299;&#37322;&#35797;&#22270;&#22238;&#31572;&#20026;&#20160;&#20040;&#26576;&#20010;&#20915;&#31574;&#20135;&#29983;&#20102;&#25110;&#32773;&#20026;&#20160;&#20040;&#26576;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20160;&#20040;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#30446;&#21069;&#23384;&#22312;&#19968;&#20123;ASP&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#20294;&#20960;&#20046;&#25152;&#26377;&#26041;&#27861;&#37117;&#32570;&#20047;&#23545;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#26576;&#20123;&#35821;&#35328;&#29305;&#24615;&#30340;&#25903;&#25345;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#32570;&#20047;&#23545;&#36817;&#24180;&#26469;&#24320;&#21457;&#30340;&#29992;&#20110;&#22312;&#29702;&#35770;&#12289;&#22806;&#37096;&#36890;&#20449;&#31561;&#26041;&#38754;&#36827;&#34892;&#25512;&#29702;&#30340;&#21508;&#31181;ASP&#25193;&#23637;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interest in explainability in artificial intelligence (AI) is growing vastly due to the near ubiquitous state of AI in our lives and the increasing complexity of AI systems. Answer-set Programming (ASP) is used in many areas, among them are industrial optimisation, knowledge management or life sciences, and thus of great interest in the context of explainability. To ensure the successful application of ASP as a problem-solving paradigm in the future, it is thus crucial to investigate explanations for ASP solutions. Such an explanation generally tries to give an answer to the question of why something is, respectively is not, part of the decision produced or solution to the formulated problem. Although several explanation approaches for ASP exist, almost all of them lack support for certain language features that are used in practice. Most notably, this encompasses the various ASP extensions that have been developed in the recent years to enable reasoning over theories, external com
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35745;&#31639;&#36923;&#36753;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#12289;&#36879;&#26126;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25913;&#36827;&#30693;&#35782;&#27880;&#20837;&#36807;&#31243;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#20803;&#32032;&#34701;&#20837;&#22810;&#20027;&#20307;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.15899</link><description>&lt;p&gt;
&#36229;&#36234;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#65306;&#36890;&#36807;&#35745;&#31639;&#36923;&#36753;&#25216;&#26415;&#22686;&#21152;&#25512;&#29702;&#21644;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Traditional Neural Networks: Toward adding Reasoning and Learning Capabilities through Computational Logic Techniques. (arXiv:2308.15899v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15899
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#36923;&#36753;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#12289;&#36879;&#26126;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25913;&#36827;&#30693;&#35782;&#27880;&#20837;&#36807;&#31243;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#20803;&#32032;&#34701;&#20837;&#22810;&#20027;&#20307;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12289;&#32570;&#20047;&#36879;&#26126;&#24615;&#21644;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#31526;&#21495;&#30693;&#35782;&#27880;&#20837;&#65288;SKI&#65289;&#25216;&#26415;&#26159;&#19968;&#31181;&#23558;&#31526;&#21495;&#30693;&#35782;&#34701;&#20837;&#23376;&#31526;&#21495;&#31995;&#32479;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30693;&#35782;&#27880;&#20837;&#36807;&#31243;&#24182;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#20803;&#32032;&#34701;&#20837;&#22810;&#20027;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) models have become popular for solving complex problems, but they have limitations such as the need for high-quality training data, lack of transparency, and robustness issues. Neuro-Symbolic AI has emerged as a promising approach combining the strengths of neural networks and symbolic reasoning. Symbolic knowledge injection (SKI) techniques are a popular method to incorporate symbolic knowledge into sub-symbolic systems. This work proposes solutions to improve the knowledge injection process and integrate elements of ML and logic into multi-agent systems (MAS).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ASP&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65292;&#20855;&#22791;&#26126;&#30830;&#25511;&#21046;&#20934;&#30830;&#24615;&#38169;&#35823;&#21644;&#21512;&#25104;&#37327;&#30340;&#33021;&#21147;&#65292;&#37319;&#29992;&#36923;&#36753;&#35268;&#21017;&#36880;&#27493;&#20016;&#23500;&#25991;&#26412;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.15898</link><description>&lt;p&gt;
&#22522;&#20110;ASP&#30340;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#22788;&#29702;&#30340;xAI&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An xAI Approach for Data-to-Text Processing with ASP. (arXiv:2308.15898v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ASP&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65292;&#20855;&#22791;&#26126;&#30830;&#25511;&#21046;&#20934;&#30830;&#24615;&#38169;&#35823;&#21644;&#21512;&#25104;&#37327;&#30340;&#33021;&#21147;&#65292;&#37319;&#29992;&#36923;&#36753;&#35268;&#21017;&#36880;&#27493;&#20016;&#23500;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#24207;&#21015;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30446;&#26631;&#20013;&#37325;&#26032;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#29616;&#26377;&#25216;&#26415;&#20013;&#30340;&#23569;&#25968;&#25552;&#35758;&#26159;&#22522;&#20110;&#35757;&#32451;&#31995;&#32479;&#20197;&#20135;&#29983;&#25551;&#36848;&#25968;&#25454;&#19988;&#19982;&#36755;&#20837;&#25968;&#25454;&#19968;&#33268;&#30340;&#25991;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#24688;&#30830;&#22320;&#30830;&#23450;&#35201;&#35828;&#30340;&#20869;&#23481;&#65288;&#22312;&#25968;&#25454;&#20013;&#35201;&#25551;&#36848;&#30340;&#20851;&#38190;&#20803;&#32032;&#65289;&#20197;&#21450;&#22914;&#20309;&#34920;&#36798;&#65306;&#25968;&#25454;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#24212;&#20934;&#30830;&#24615;&#65292;&#25991;&#26412;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30683;&#30462;/&#20887;&#20313;&#65292;&#21512;&#25104;&#37327;&#30340;&#25511;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;xAI&#35201;&#27714;&#30340;&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24314;&#27169;ASP/Python&#31243;&#24207;&#65292;&#20197;&#30830;&#20445;&#23545;&#20934;&#30830;&#24615;&#38169;&#35823;&#21644;&#21512;&#25104;&#37327;&#36827;&#34892;&#26126;&#30830;&#30340;&#25511;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#26126;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25991;&#26412;&#25551;&#36848;&#20197;&#23618;&#27425;&#32467;&#26500;&#32452;&#32455;&#65292;&#22312;&#33258;&#39030;&#21521;&#19979;&#30340;&#32467;&#26500;&#20013;&#65292;&#25991;&#26412;&#20250;&#26681;&#25454;&#36923;&#36753;&#35268;&#21017;&#34917;&#20805;&#26356;&#22810;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of natural language text from data series gained renewed interest among AI research goals. Not surprisingly, the few proposals in the state of the art are based on training some system, in order to produce a text that describes and that is coherent to the data provided as input. Main challenges of such approaches are the proper identification of "what" to say (the key descriptive elements to be addressed in the data) and "how" to say: the correspondence and accuracy between data and text, the presence of contradictions/redundancy in the text, the control of the amount of synthesis.  This paper presents a framework that is compliant with xAI requirements. In particular we model ASP/Python programs that enable an explicit control of accuracy errors and amount of synthesis, with proven optimal solutions. The text description is hierarchically organized, in a top-down structure where text is enriched with further details, according to logic rules. The generation of natural l
&lt;/p&gt;</description></item><item><title>Nemo&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#12289;&#27880;&#37325;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#30340;&#36923;&#36753;&#32534;&#31243;&#24341;&#25806;&#65292;&#36890;&#36807;&#25968;&#25454;&#20013;&#24515;&#30340;&#20998;&#26512;&#35745;&#31639;&#65292;&#20351;&#29992;&#20102;&#23436;&#20840;&#22768;&#26126;&#24335;&#30340;Datalog&#26041;&#35328;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15897</link><description>&lt;p&gt;
Nemo: &#39318;&#20010;&#20840;&#26032;&#35268;&#21017;&#24341;&#25806;&#30340;&#21021;&#27425;&#25259;&#38706;
&lt;/p&gt;
&lt;p&gt;
Nemo: First Glimpse of a New Rule Engine. (arXiv:2308.15897v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15897
&lt;/p&gt;
&lt;p&gt;
Nemo&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#12289;&#27880;&#37325;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#30340;&#36923;&#36753;&#32534;&#31243;&#24341;&#25806;&#65292;&#36890;&#36807;&#25968;&#25454;&#20013;&#24515;&#30340;&#20998;&#26512;&#35745;&#31639;&#65292;&#20351;&#29992;&#20102;&#23436;&#20840;&#22768;&#26126;&#24335;&#30340;Datalog&#26041;&#35328;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#31995;&#32479;&#28436;&#31034;&#20171;&#32461;&#20102;Nemo&#65292;&#19968;&#20010;&#27880;&#37325;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#30340;&#26032;&#22411;&#36923;&#36753;&#32534;&#31243;&#24341;&#25806;&#12290;Nemo&#19987;&#27880;&#20110;&#25968;&#25454;&#20013;&#24515;&#30340;&#20998;&#26512;&#35745;&#31639;&#65292;&#22312;&#23436;&#20840;&#22768;&#26126;&#24335;&#30340;Datalog&#26041;&#35328;&#20013;&#24314;&#27169;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#65292;Nemo&#30340;&#21487;&#25193;&#23637;&#24615;&#19982;&#25110;&#36229;&#36807;&#39046;&#20808;&#30340;Datalog&#31995;&#32479;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#31508;&#35760;&#26412;&#19978;&#20351;&#29992;10^5&#21040;10^8&#20010;&#36755;&#20837;&#20107;&#23454;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#21644;&#26412;&#20307;&#25512;&#29702;&#30340;&#24212;&#29992;&#12290;Nemo&#20351;&#29992;Rust&#32534;&#20889;&#65292;&#24182;&#20316;&#20026;&#19968;&#20010;&#20813;&#36153;&#24320;&#28304;&#24037;&#20855;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
This system demonstration presents Nemo, a new logic programming engine with a focus on reliability and performance. Nemo is built for data-centric analytic computations, modelled in a fully declarative Datalog dialect. Its scalability for these tasks matches or exceeds that of leading Datalog systems. We demonstrate uses in reasoning with knowledge graphs and ontologies with 10^5 to 10^8 input facts, all on a laptop. Nemo is written in Rust and available as a free and open source tool.
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#20102;&#19968;&#20010;&#36719;&#30828;&#20214;&#26694;&#26550;&#26469;&#35780;&#20272;&#39550;&#39542;&#21592;&#23545;&#24773;&#20917;&#30340;&#24863;&#30693;&#65292;&#24182;&#25552;&#20379;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#36741;&#21161;&#65292;&#20197;&#24110;&#21161;&#24314;&#31435;&#24773;&#22659;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2308.15895</link><description>&lt;p&gt;
&#35780;&#20272;&#21322;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#39550;&#39542;&#21592;&#24773;&#22659;&#24863;&#30693;: &#22522;&#20110;ASP&#30340;&#39550;&#39542;&#21160;&#21147;&#23398;&#29305;&#24449;&#24314;&#27169;&#22330;&#26223;&#35299;&#37322;&#21644;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Assessing Drivers' Situation Awareness in Semi-Autonomous Vehicles: ASP based Characterisations of Driving Dynamics for Modelling Scene Interpretation and Projection. (arXiv:2308.15895v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15895
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20102;&#19968;&#20010;&#36719;&#30828;&#20214;&#26694;&#26550;&#26469;&#35780;&#20272;&#39550;&#39542;&#21592;&#23545;&#24773;&#20917;&#30340;&#24863;&#30693;&#65292;&#24182;&#25552;&#20379;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#36741;&#21161;&#65292;&#20197;&#24110;&#21161;&#24314;&#31435;&#24773;&#22659;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#33258;&#21160;&#39550;&#39542;&#20316;&#20026;&#24050;&#32463;&#21487;&#29992;&#24182;&#19988;&#23558;&#26469;&#20250;&#26356;&#21152;&#26222;&#21450;&#30340;&#25216;&#26415;&#65292;&#38656;&#35201;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#21270;&#31995;&#32479;&#21487;&#38752;&#22320;&#21512;&#20316;&#20197;&#30830;&#20445;&#23433;&#20840;&#39550;&#39542;&#12290;&#22312;&#36825;&#20010;&#21162;&#21147;&#20013;&#30340;&#19968;&#20010;&#29305;&#21035;&#25361;&#25112;&#26159;&#24403;&#36710;&#36742;&#33258;&#21160;&#21270;&#26080;&#27861;&#39550;&#39542;&#24182;&#35201;&#27714;&#20154;&#21592;&#25509;&#31649;&#26102;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#39550;&#39542;&#21592;&#24517;&#39035;&#24555;&#36895;&#35748;&#35782;&#21040;&#20132;&#36890;&#24773;&#20917;&#65292;&#25165;&#33021;&#25509;&#31649;&#24182;&#23433;&#20840;&#39550;&#39542;&#27773;&#36710;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36719;&#20214;&#21644;&#30828;&#20214;&#26694;&#26550;&#26469;&#35780;&#20272;&#39550;&#39542;&#21592;&#23545;&#24773;&#20917;&#30340;&#35748;&#30693;&#65292;&#24182;&#25552;&#20379;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#36741;&#21161;&#65292;&#20197;&#24110;&#21161;&#24314;&#31435;&#24773;&#22659;&#24863;&#30693;&#12290;&#35813;&#26694;&#26550;&#26159;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;(ROS)&#20869;&#20197;&#27169;&#22359;&#21270;&#31995;&#32479;&#30340;&#24418;&#24335;&#24320;&#21457;&#30340;&#65292;&#21253;&#25324;&#24863;&#30693;&#29615;&#22659;&#21644;&#39550;&#39542;&#21592;&#29366;&#24577;&#30340;&#27169;&#22359;&#65292;&#24314;&#27169;&#39550;&#39542;&#21592;&#24773;&#22659;&#24863;&#30693;&#30340;&#27169;&#22359;&#65292;&#20197;&#21450;&#20351;&#29992;&#19987;&#38376;&#30340;&#20154;&#26426;&#30028;&#38754;&#25351;&#23548;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-autonomous driving, as it is already available today and will eventually become even more accessible, implies the need for driver and automation system to reliably work together in order to ensure safe driving. A particular challenge in this endeavour are situations in which the vehicle's automation is no longer able to drive and is thus requesting the human to take over. In these situations the driver has to quickly build awareness for the traffic situation to be able to take over control and safely drive the car. Within this context we present a software and hardware framework to asses how aware the driver is about the situation and to provide human-centred assistance to help in building situation awareness. The framework is developed as a modular system within the Robot Operating System (ROS) with modules for sensing the environment and the driver state, modelling the driver's situation awareness, and for guiding the driver's attention using specialized Human Machine Interfaces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#35299;&#20915;&#20849;&#21516;&#35774;&#35745;&#29615;&#22659;&#20013;&#20840;&#29699;&#29289;&#27969;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19982;&#20135;&#21697;&#21516;&#26102;&#24320;&#21457;&#24037;&#19994;&#31995;&#32479;&#65292;&#25552;&#39640;&#20854;&#38887;&#24615;&#24182;&#38477;&#20302;&#20379;&#24212;&#38142;&#29942;&#39048;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2308.15892</link><description>&lt;p&gt;
&#22312;&#20849;&#21516;&#35774;&#35745;&#29615;&#22659;&#20013;&#36816;&#29992;&#36923;&#36753;&#32534;&#31243;&#30340;&#26041;&#27861;&#35299;&#20915;&#20840;&#29699;&#29289;&#27969;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Logic Programming Approach to Global Logistics in a Co-Design Environment. (arXiv:2308.15892v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#35299;&#20915;&#20849;&#21516;&#35774;&#35745;&#29615;&#22659;&#20013;&#20840;&#29699;&#29289;&#27969;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19982;&#20135;&#21697;&#21516;&#26102;&#24320;&#21457;&#24037;&#19994;&#31995;&#32479;&#65292;&#25552;&#39640;&#20854;&#38887;&#24615;&#24182;&#38477;&#20302;&#20379;&#24212;&#38142;&#29942;&#39048;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20849;&#21516;&#35774;&#35745;&#29615;&#22659;&#20013;&#65292;&#38656;&#35201;&#24555;&#36895;&#19988;&#33258;&#21160;&#22320;&#38598;&#25104;&#21464;&#26356;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20849;&#21516;&#35774;&#35745;&#26041;&#27861;&#19979;&#21019;&#24314;&#21644;&#20248;&#21270;&#29992;&#20110;&#23458;&#26426;&#21046;&#36896;&#30340;&#20840;&#29699;&#29289;&#27969;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#32771;&#34385;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;&#22914;&#25104;&#26412;&#12289;&#26102;&#38388;&#25110;&#38887;&#24615;&#65289;&#12290;&#25152;&#35752;&#35770;&#30340;&#20135;&#21697;&#26159;&#19968;&#26550;&#23458;&#26426;&#65292;&#30001;&#22810;&#20010;&#32452;&#20214;&#32452;&#25104;&#65292;&#20998;&#21035;&#22312;&#20840;&#29699;&#22810;&#20010;&#22320;&#28857;&#21046;&#36896;&#12290;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#31181;&#26368;&#20248;&#30340;&#26041;&#24335;&#26469;&#24314;&#36896;&#35813;&#39134;&#26426;&#65292;&#21516;&#26102;&#32771;&#34385;&#20854;&#24037;&#19994;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#19982;&#20135;&#21697;&#21516;&#26102;&#24320;&#21457;&#24037;&#19994;&#31995;&#32479;&#65292;&#24182;&#20351;&#20854;&#26356;&#20855;&#38887;&#24615;&#20197;&#24212;&#23545;&#24847;&#22806;&#20107;&#20214;&#65292;&#38477;&#20302;&#20379;&#24212;&#38142;&#29942;&#39048;&#30340;&#39118;&#38505;&#12290;&#36825;&#31181;&#39118;&#38505;&#38477;&#20302;&#20445;&#35777;&#20102;&#25345;&#32493;&#30340;&#25928;&#29575;&#21644;&#36816;&#33829;&#25104;&#21151;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;Answer Set Programming&#65288;ASP&#65289;&#20316;&#20026;&#24314;&#27169;&#35821;&#35328;&#65292;&#23545;&#38382;&#39064;&#36827;&#34892;&#24418;&#24335;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a co-design environment changes need to be integrated quickly and in an automated manner. This paper considers the challenge of creating and optimizing a global logistics system for the construction of a passenger aircraft within a co-design approach with respect to key performance indicators (like cost, time or resilience). The product in question is an aircraft, comprised of multiple components, manufactured at multiple sites worldwide. The goal is to find an optimal way to build the aircraft taking into consideration the requirements for its industrial system. The main motivation for approaching this challenge is to develop the industrial system in tandem with the product and making it more resilient against unforeseen events, reducing the risks of bottlenecks in the supply chain. This risk reduction ensures continued efficiency and operational success. To address this challenging and complex task we have chosen Answer Set Programming (ASP) as the modeling language, formalizing t
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ProbLog&#21644;&#27010;&#29575;&#35770;&#35777;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;ProbLog&#26159;Probabilistic Abstract Argumentation (PAA)&#30340;&#19968;&#31181;&#24418;&#24335;&#30340;&#23454;&#20363;&#65292;&#24182;&#20026;ProbLog&#25552;&#20379;&#20102;&#20351;&#29992;PAA/PABA&#30340;&#26367;&#20195;&#35821;&#20041;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#21450;&#20026;PAA/PABA&#33719;&#24471;&#26032;&#30340;&#35770;&#35777;&#35821;&#20041;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#36824;&#20026;ProbLog&#30340;&#36755;&#20986;&#25552;&#20379;&#20102;&#26032;&#24418;&#24335;&#30340;&#35770;&#35777;&#24615;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15891</link><description>&lt;p&gt;
&#20102;&#35299;ProbLog&#20316;&#20026;&#27010;&#29575;&#35770;&#35777;&#30340;&#27010;&#29575;&#35770;
&lt;/p&gt;
&lt;p&gt;
Understanding ProbLog as Probabilistic Argumentation. (arXiv:2308.15891v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15891
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ProbLog&#21644;&#27010;&#29575;&#35770;&#35777;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;ProbLog&#26159;Probabilistic Abstract Argumentation (PAA)&#30340;&#19968;&#31181;&#24418;&#24335;&#30340;&#23454;&#20363;&#65292;&#24182;&#20026;ProbLog&#25552;&#20379;&#20102;&#20351;&#29992;PAA/PABA&#30340;&#26367;&#20195;&#35821;&#20041;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#21450;&#20026;PAA/PABA&#33719;&#24471;&#26032;&#30340;&#35770;&#35777;&#35821;&#20041;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#36824;&#20026;ProbLog&#30340;&#36755;&#20986;&#25552;&#20379;&#20102;&#26032;&#24418;&#24335;&#30340;&#35770;&#35777;&#24615;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ProbLog&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;/&#24037;&#20855;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#38656;&#35201;&#22788;&#29702;&#32467;&#26500;&#21270;&#39046;&#22495;&#20869;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;ProbLog&#19982;&#21478;&#19968;&#31181;&#32467;&#21512;&#20102;&#31526;&#21495;&#25512;&#29702;&#21644;&#19981;&#30830;&#23450;&#25512;&#29702;&#30340;&#30693;&#21517;&#24418;&#24335;&#20027;&#20041;&#21464;&#20307;&#65292;&#21363;&#27010;&#29575;&#35770;&#35777;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ProbLog&#26159;Probabilistic Abstract Argumentation (PAA)&#30340;&#19968;&#31181;&#24418;&#24335;&#30340;&#23454;&#20363;&#65292;PAA&#26159;&#22522;&#20110;&#20551;&#35774;&#30340;&#35770;&#35777;&#65288;ABA&#65289;&#26500;&#24314;&#30340;&#12290;&#36825;&#20123;&#32852;&#31995;&#20026;ProbLog&#25552;&#20379;&#20102;&#20351;&#29992;PAA/PABA&#30340;&#26367;&#20195;&#35821;&#20041;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#20026;PAA/PABA&#33719;&#24471;&#26032;&#30340;&#35770;&#35777;&#35821;&#20041;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#21033;&#29992;&#20102;ProbLog&#19982;&#35770;&#35777;&#20043;&#38388;&#30340;&#20808;&#21069;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32852;&#31995;&#20026;ProbLog&#30340;&#36755;&#20986;&#25552;&#20379;&#20102;&#26032;&#24418;&#24335;&#30340;&#35770;&#35777;&#24615;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ProbLog is a popular probabilistic logic programming language/tool, widely used for applications requiring to deal with inherent uncertainties in structured domains. In this paper we study connections between ProbLog and a variant of another well-known formalism combining symbolic reasoning and reasoning under uncertainty, i.e. probabilistic argumentation. Specifically, we show that ProbLog is an instance of a form of Probabilistic Abstract Argumentation (PAA) that builds upon Assumption-Based Argumentation (ABA). The connections pave the way towards equipping ProbLog with alternative semantics, inherited from PAA/PABA, as well as obtaining novel argumentation semantics for PAA/PABA, leveraging on prior connections between ProbLog and argumentation. Further, the connections pave the way towards novel forms of argumentative explanations for ProbLog's outputs.
&lt;/p&gt;</description></item><item><title>Natlog&#36890;&#36807;&#39640;&#32423;&#20132;&#20114;&#27169;&#24335;&#21644;&#22810;&#31181;&#21151;&#33021;&#30340;&#36830;&#25509;&#65292;&#23558;&#36923;&#36753;&#32534;&#31243;&#23884;&#20837;&#21040;Python&#28145;&#24230;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#20102;&#36923;&#36753;&#35821;&#35328;&#32467;&#26500;&#23545;Python&#29983;&#24577;&#31995;&#32479;&#30340;&#23436;&#20840;&#35775;&#38382;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15890</link><description>&lt;p&gt;
Natlog: &#23558;&#36923;&#36753;&#32534;&#31243;&#23884;&#20837;Python&#28145;&#24230;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Natlog: Embedding Logic Programming into the Python Deep-Learning Ecosystem. (arXiv:2308.15890v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15890
&lt;/p&gt;
&lt;p&gt;
Natlog&#36890;&#36807;&#39640;&#32423;&#20132;&#20114;&#27169;&#24335;&#21644;&#22810;&#31181;&#21151;&#33021;&#30340;&#36830;&#25509;&#65292;&#23558;&#36923;&#36753;&#32534;&#31243;&#23884;&#20837;&#21040;Python&#28145;&#24230;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#20102;&#36923;&#36753;&#35821;&#35328;&#32467;&#26500;&#23545;Python&#29983;&#24577;&#31995;&#32479;&#30340;&#23436;&#20840;&#35775;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;Python&#21644;&#25105;&#20204;&#22522;&#20110;Python&#30340;&#23884;&#20837;&#24335;&#36923;&#36753;&#35821;&#35328;Natlog&#30340;&#34920;&#36798;&#21147;&#20849;&#24615;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#31561;&#25928;&#35821;&#35328;&#32467;&#26500;&#21644;&#25968;&#25454;&#31867;&#22411;&#30340;&#39640;&#32423;&#20132;&#20114;&#27169;&#24335;&#12290;&#36890;&#36807;&#30452;&#25509;&#36830;&#25509;&#29983;&#25104;&#22120;&#21644;&#22238;&#28335;&#12289;&#23884;&#22871;&#20803;&#32452;&#21644;&#26415;&#35821;&#12289;&#21327;&#31243;&#21644;&#19968;&#27969;&#36923;&#36753;&#24341;&#25806;&#12289;&#21453;&#23556;&#21644;&#20803;&#35299;&#37322;&#65292;&#25105;&#20204;&#20351;&#36923;&#36753;&#35821;&#35328;&#32467;&#26500;&#33021;&#22815;&#35775;&#38382;Python&#29983;&#24577;&#31995;&#32479;&#30340;&#20840;&#37096;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;Natlog&#24212;&#29992;&#31243;&#24207;&#20316;&#20026;JAX&#21644;Pytorch&#31649;&#36947;&#30340;&#32534;&#25490;&#22120;&#20197;&#21450;&#20316;&#20026;DCG&#39537;&#21160;&#30340;GPT3&#21644;DALL.E&#25552;&#31034;&#29983;&#25104;&#22120;&#23637;&#31034;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by expressiveness commonalities of Python and our Python-based embedded logic-based language Natlog, we design high-level interaction patterns between equivalent language constructs and data types on the two sides.  By directly connecting generators and backtracking, nested tuples and terms, coroutines and first-class logic engines, reflection and meta-interpretation, we enable logic-based language constructs to access the full power of the Python ecosystem.  We show the effectiveness of our design via Natlog apps working as orchestrators for JAX and Pytorch pipelines and as DCG-driven GPT3 and DALL.E prompt generators.  Keyphrases: embedding of logic programming in the Python ecosystem, high-level inter-paradigm data exchanges, coroutining with logic engines, logic-based neuro-symbolic computing, logic grammars as prompt-generators for Large Language Models, logic-based neural network configuration and training.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23558;&#32423;&#21035;&#25490;&#21517;&#32422;&#26463;&#37325;&#20889;&#20026;&#20445;&#25345;&#21333;&#35843;&#21644;&#20984;&#32858;&#21512;&#32467;&#26500;&#30340;&#19968;&#33324;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35206;&#30422;ASP&#30340;&#32858;&#21512;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.15888</link><description>&lt;p&gt;
&#24191;&#20041;&#21270;&#32423;&#21035;&#25490;&#21517;&#32422;&#26463;&#29992;&#20110;&#21333;&#35843;&#21644;&#20984;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Generalizing Level Ranking Constraints for Monotone and Convex Aggregates. (arXiv:2308.15888v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23558;&#32423;&#21035;&#25490;&#21517;&#32422;&#26463;&#37325;&#20889;&#20026;&#20445;&#25345;&#21333;&#35843;&#21644;&#20984;&#32858;&#21512;&#32467;&#26500;&#30340;&#19968;&#33324;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35206;&#30422;ASP&#30340;&#32858;&#21512;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31572;&#26696;&#38598;&#35268;&#21010;&#20013;&#65292;&#31572;&#26696;&#38598;&#25429;&#25417;&#21040;&#20102;&#23545;&#25628;&#32034;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#27492;&#39640;&#25928;&#35745;&#31639;&#31572;&#26696;&#38598;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#19968;&#31181;&#21487;&#34892;&#30340;&#23454;&#29616;&#31574;&#30053;&#26159;&#22522;&#20110;&#36716;&#25442;&#30340;ASP&#65292;&#20854;&#20013;&#36923;&#36753;&#31243;&#24207;&#34987;&#36716;&#25442;&#20026;&#20854;&#20182;&#30693;&#35782;&#34920;&#31034;&#24418;&#24335;&#65292;&#22914;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#65292;SAT&#27169;&#29702;&#35770;&#65288;SMT&#65289;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#27714;&#35299;&#22120;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#31572;&#26696;&#38598;&#12290;&#29616;&#26377;&#30340;&#35768;&#22810;&#36716;&#25442;&#20381;&#36182;&#20110;&#31243;&#24207;&#23436;&#25104;&#21644;&#32423;&#21035;&#25490;&#21517;&#26469;&#25429;&#25417;&#31572;&#26696;&#38598;&#30340;&#26368;&#23567;&#24615;&#21644;&#40664;&#35748;&#21542;&#23450;&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#32423;&#21035;&#25490;&#21517;&#32422;&#26463;&#65292;&#26088;&#22312;&#23545;ASP&#30340;&#22522;&#20110;&#32858;&#21512;&#30340;&#25193;&#23637;&#36827;&#34892;&#26356;&#31995;&#32479;&#21270;&#30340;&#19968;&#33324;&#21270;&#12290;&#36890;&#36807;&#24212;&#29992;&#19968;&#20123;&#31243;&#24207;&#36716;&#25442;&#65292;&#21487;&#20197;&#23558;&#25490;&#21517;&#32422;&#26463;&#37325;&#20889;&#20026;&#20445;&#25345;&#21333;&#35843;&#21644;&#20984;&#32858;&#21512;&#32467;&#26500;&#30340;&#19968;&#33324;&#24418;&#24335;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In answer set programming (ASP), answer sets capture solutions to search problems of interest and thus the efficient computation of answer sets is of utmost importance. One viable implementation strategy is provided by translation-based ASP where logic programs are translated into other KR formalisms such as Boolean satisfiability (SAT), SAT modulo theories (SMT), and mixed-integer programming (MIP). Consequently, existing solvers can be harnessed for the computation of answer sets. Many of the existing translations rely on program completion and level rankings to capture the minimality of answer sets and default negation properly. In this work, we take level ranking constraints into reconsideration, aiming at their generalizations to cover aggregate-based extensions of ASP in more systematic way. By applying a number of program transformations, ranking constraints can be rewritten in a general form that preserves the structure of monotone and convex aggregates and thus offers a unifor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;CLIP&#36827;&#34892;&#32452;&#21512;&#36923;&#36753;&#25512;&#29702;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#24120;&#37197;&#32622;&#30340;CLIP&#26080;&#27861;&#36827;&#34892;&#36825;&#31181;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.15887</link><description>&lt;p&gt;
&#23545;&#20110;&#32452;&#21512;&#36923;&#36753;&#25512;&#29702;&#30340; CLIP &#28508;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Potential of CLIP for Compositional Logical Reasoning. (arXiv:2308.15887v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;CLIP&#36827;&#34892;&#32452;&#21512;&#36923;&#36753;&#25512;&#29702;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#24120;&#37197;&#32622;&#30340;CLIP&#26080;&#27861;&#36827;&#34892;&#36825;&#31181;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;OpenAI&#30340;CLIP&#36827;&#34892;&#36923;&#36753;&#36830;&#36143;&#30340;&#35270;&#35273;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#25105;&#20204;&#30340;&#26415;&#35821;&#65292;&#24182;&#23545;CLIP&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;&#36827;&#34892;&#20102;&#20960;&#20309;&#20998;&#26512;&#65292;&#20197;&#20415;&#31995;&#32479;&#22312;&#36923;&#36753;&#19978;&#36830;&#36143;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#35770;&#26159;&#65292;&#36890;&#24120;&#37197;&#32622;&#30340;CLIP&#19981;&#33021;&#36827;&#34892;&#36825;&#31181;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we explore the possibility of using OpenAI's CLIP to perform logically coherent grounded visual reasoning. To that end, we formalize our terms and give a geometric analysis of how embeddings in CLIP's latent space would need to be configured in order for the system to be logically coherent. Our main conclusion is that, as usually configured, CLIP cannot perform such reasoning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#22238;&#31572;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#31243;&#24207;&#32467;&#26500;&#26102;&#32771;&#34385;&#20102;&#22240;&#26524;&#26426;&#21046;&#65292;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.15883</link><description>&lt;p&gt;
&#22914;&#26524;&#25105;&#22788;&#20110;AI&#20013;&#65292;&#29983;&#27963;&#20250;&#26356;&#26377;&#36259;&#21527;&#65311;&#22522;&#20110;&#27010;&#29575;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#22238;&#31572;&#21453;&#20107;&#23454;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
"Would life be more interesting if I were in AI?" Answering Counterfactuals based on Probabilistic Inductive Logic Programming. (arXiv:2308.15883v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15883
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#22238;&#31572;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#31243;&#24207;&#32467;&#26500;&#26102;&#32771;&#34385;&#20102;&#22240;&#26524;&#26426;&#21046;&#65292;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#36923;&#36753;&#31243;&#24207;&#26159;&#19968;&#31181;&#26576;&#20123;&#20107;&#23454;&#20197;&#29305;&#23450;&#27010;&#29575;&#25104;&#31435;&#30340;&#36923;&#36753;&#31243;&#24207;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20123;&#31243;&#24207;&#65292;&#20351;&#20854;&#33021;&#22815;&#22238;&#31572;&#21453;&#20107;&#23454;&#26597;&#35810;&#12290;&#36890;&#24120;&#36890;&#36807;&#21551;&#21457;&#24335;&#25628;&#32034;&#21644;&#32479;&#35745;&#27979;&#35797;&#26469;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#31243;&#24207;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32479;&#35745;&#27979;&#35797;&#32570;&#20047;&#20851;&#20110;&#29983;&#25104;&#25968;&#25454;&#30340;&#22240;&#26524;&#26426;&#21046;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#20351;&#29992;&#29983;&#25104;&#30340;&#31243;&#24207;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#29255;&#27573;&#65292;&#20801;&#35768;&#20174;&#20854;&#24341;&#23548;&#20998;&#24067;&#20013;&#37325;&#24314;&#31243;&#24207;&#12290;&#36825;&#36827;&#19968;&#27493;&#20351;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#25903;&#25345;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic logic programs are logic programs where some facts hold with a specified probability. Here, we investigate these programs with a causal framework that allows counterfactual queries. Learning the program structure from observational data is usually done through heuristic search relying on statistical tests. However, these statistical tests lack information about the causal mechanism generating the data, which makes it unfeasible to use the resulting programs for counterfactual reasoning. To address this, we propose a language fragment that allows reconstructing a program from its induced distribution. This further enables us to learn programs supporting counterfactual queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;Answer Set Programming (ASP)&#36827;&#34892;&#35299;&#37322;&#30340;&#31995;&#32479;xASP&#30340;&#25913;&#36827;&#29256;&#26412;xASP2&#65292;&#23427;&#25903;&#25345;&#20102;&#26356;&#22810;&#30340;clingo&#26500;&#36896;&#24182;&#33021;&#22815;&#20197;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.15879</link><description>&lt;p&gt;
Answer Set Programming&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explanations for Answer Set Programming. (arXiv:2308.15879v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;Answer Set Programming (ASP)&#36827;&#34892;&#35299;&#37322;&#30340;&#31995;&#32479;xASP&#30340;&#25913;&#36827;&#29256;&#26412;xASP2&#65292;&#23427;&#25903;&#25345;&#20102;&#26356;&#22810;&#30340;clingo&#26500;&#36896;&#24182;&#33021;&#22815;&#20197;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;Answer Set Programming (ASP)&#36827;&#34892;&#35299;&#37322;&#30340;&#31995;&#32479;xASP&#30340;&#25913;&#36827;&#29256;&#26412;xASP2&#12290;&#19982;xASP&#19981;&#21516;&#65292;&#26032;&#31995;&#32479;xASP2&#25903;&#25345;&#19981;&#21516;&#30340;clingo&#26500;&#36896;&#65292;&#22914;&#36873;&#25321;&#35268;&#21017;&#12289;&#32422;&#26463;&#21644;&#32858;&#21512;&#20989;&#25968;&#65292;&#22914;#sum&#12289;#min&#12290;&#35813;&#24037;&#20316;&#24418;&#24335;&#21270;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#36866;&#29992;&#20110;ASP&#30340;&#24191;&#27867;&#29255;&#27573;&#65292;&#33021;&#22815;&#23613;&#21487;&#33021;&#22320;&#32553;&#23567;&#20551;&#35774;&#38598;&#65292;&#24182;&#20197;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents an enhancement of xASP, a system that generates explanation graphs for Answer Set Programming (ASP). Different from xASP, the new system, xASP2, supports different clingo constructs like the choice rules, the constraints, and the aggregates such as #sum, #min. This work formalizes and presents an explainable artificial intelligence system for a broad fragment of ASP, capable of shrinking as much as possible the set of assumptions and presenting explanations in terms of directed acyclic graphs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;ASP&#23454;&#29616;ABA&#23398;&#20064;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31526;&#21495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32972;&#26223;&#30693;&#35782;&#21644;&#27491;&#36127;&#20363;&#20013;&#32472;&#21046;&#22522;&#20110;&#20551;&#35774;&#30340;&#35770;&#35777;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.15877</link><description>&lt;p&gt;
&#36890;&#36807;ASP&#23454;&#29616;ABA&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ABA Learning via ASP. (arXiv:2308.15877v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;ASP&#23454;&#29616;ABA&#23398;&#20064;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31526;&#21495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32972;&#26223;&#30693;&#35782;&#21644;&#27491;&#36127;&#20363;&#20013;&#32472;&#21046;&#22522;&#20110;&#20551;&#35774;&#30340;&#35770;&#35777;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ABA&#23398;&#20064;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#31526;&#21495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32972;&#26223;&#30693;&#35782;&#21644;&#27491;&#36127;&#20363;&#20013;&#32472;&#21046;&#22522;&#20110;&#20551;&#35774;&#30340;&#35770;&#35777;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Answer Set Programming&#26469;&#23454;&#29616;ABA&#23398;&#20064;&#65292;&#20197;&#24110;&#21161;&#25351;&#23548;ABA&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#23398;&#20064;&#21644;&#27867;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ABA Learning has been proposed as a form of symbolic machine learning for drawing Assumption-Based Argumentation frameworks from background knowledge and positive and negative examples. We propose a novel method for implementing ABA Learning using Answer Set Programming as a way to help guide Rote Learning and generalisation in ABA Learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Answer Set Programming&#65288;ASP&#65289;&#21644;&#24369;&#32422;&#26463;&#65292;&#35299;&#20915;&#20102;&#36947;&#20041;&#36923;&#36753;&#38754;&#20020;&#30340;&#24726;&#35770;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32763;&#35793;&#24102;&#26377;&#24369;&#32422;&#26463;&#30340;ASP&#20013;&#30340;&#36947;&#20041;&#31995;&#32479;&#65292;&#24212;&#29992;&#20110;"&#36947;&#24503;"&#29256;&#26412;&#30340;Pac-man&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15870</link><description>&lt;p&gt;
ASP&#20013;&#30340;&#24369;&#32422;&#26463;&#19979;&#30340;&#36947;&#20041;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
Deontic Paradoxes in ASP with Weak Constraints. (arXiv:2308.15870v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Answer Set Programming&#65288;ASP&#65289;&#21644;&#24369;&#32422;&#26463;&#65292;&#35299;&#20915;&#20102;&#36947;&#20041;&#36923;&#36753;&#38754;&#20020;&#30340;&#24726;&#35770;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32763;&#35793;&#24102;&#26377;&#24369;&#32422;&#26463;&#30340;ASP&#20013;&#30340;&#36947;&#20041;&#31995;&#32479;&#65292;&#24212;&#29992;&#20110;"&#36947;&#24503;"&#29256;&#26412;&#30340;Pac-man&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#23835;&#36215;&#20351;&#24471;&#23545;&#36981;&#23432;&#27861;&#24459;&#12289;&#31038;&#20250;&#21644;&#20262;&#29702;&#20934;&#21017;&#25935;&#24863;&#30340;&#21508;&#31181;&#24212;&#29992;&#38656;&#27714;&#20915;&#31574;&#25903;&#25345;&#12290;&#36947;&#20041;&#25512;&#29702;&#26159;&#36947;&#20041;&#36923;&#36753;&#30340;&#39046;&#22495;&#65292;&#23427;&#38754;&#20020;&#30528;&#20247;&#25152;&#21608;&#30693;&#30340;&#22522;&#20934;&#38382;&#39064;&#65288;&#36947;&#20041;&#24726;&#35770;&#65289;&#65292;&#24182;&#19988;&#32570;&#20047;&#26377;&#25928;&#30340;&#35745;&#31639;&#24037;&#20855;&#12290;&#26412;&#25991;&#20351;&#29992;Answer Set Programming&#65288;ASP&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24369;&#32422;&#26463;&#23545;&#20960;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#36947;&#20041;&#24726;&#35770;&#36827;&#34892;&#32534;&#30721;&#21644;&#35299;&#20915;&#12290;&#36890;&#36807;&#23545;&#36825;&#31181;&#32534;&#30721;&#36827;&#34892;&#25277;&#35937;&#21644;&#27010;&#25324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24102;&#26377;&#24369;&#32422;&#26463;&#30340;ASP&#20013;&#32763;&#35793;&#36947;&#20041;&#31995;&#32479;&#30340;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;"&#36947;&#24503;"&#29256;&#26412;&#30340;Pac-man&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#30456;&#20851;&#24037;&#20316;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#36947;&#24503;&#19978;&#26356;&#21487;&#21462;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of powerful AI technology for a range of applications that are sensitive to legal, social, and ethical norms demands decision-making support in presence of norms and regulations. Normative reasoning is the realm of deontic logics, that are challenged by well-known benchmark problems (deontic paradoxes), and lack efficient computational tools. In this paper, we use Answer Set Programming (ASP) for addressing these shortcomings and showcase how to encode and resolve several well-known deontic paradoxes utilizing weak constraints. By abstracting and generalizing this encoding, we present a methodology for translating normative systems in ASP with weak constraints. This methodology is applied to "ethical" versions of Pac-man, where we obtain a comparable performance with related works, but ethically preferable results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25512;&#24191;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#25512;&#29702;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21028;&#26029;&#38750;ground&#24773;&#20917;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#38472;&#36848;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15865</link><description>&lt;p&gt;
&#20851;&#20110;&#27010;&#29575;&#36923;&#36753;&#31243;&#24207;&#32467;&#26500;&#20013;&#38544;&#34255;&#30340;&#29420;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Independencies Hidden in the Structure of a Probabilistic Logic Program. (arXiv:2308.15865v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25512;&#24191;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#25512;&#29702;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21028;&#26029;&#38750;ground&#24773;&#20917;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#38472;&#36848;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pearl&#21644;Verma&#24320;&#21457;&#20102;d-&#20998;&#31163;&#65292;&#20316;&#20026;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#24418;&#20934;&#21017;&#65292;&#29992;&#20110;&#25512;&#29702;&#30001;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#22240;&#26524;&#32467;&#26500;&#25152;&#26263;&#31034;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;&#30001;&#20110;&#26080;&#29615;&#30340;&#22522;&#20110;&#27010;&#29575;&#30340;&#36923;&#36753;&#31243;&#24207;&#23545;&#24212;&#20110;&#20854;&#20381;&#36182;&#22270;&#19978;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#20174;&#21518;&#32773;&#30340;d-&#20998;&#31163;&#20013;&#35745;&#31639;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#19978;&#36848;&#25512;&#29702;&#25512;&#24191;&#21040;&#38750;ground&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#27010;&#29575;&#36923;&#36753;&#31243;&#24207;&#30340;&#27010;&#24565;&#25277;&#35937;&#20986;&#26469;&#65292;&#25682;&#24323;&#22806;&#37096;&#25968;&#25454;&#24211;&#21644;&#27010;&#29575;&#65292;&#24471;&#21040;&#25152;&#35859;&#30340;&#31243;&#24207;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#30830;&#30340;&#20803;&#35299;&#37322;&#22120;&#65292;&#29992;&#20110;&#21028;&#26029;&#22312;&#32473;&#23450;&#22806;&#37096;&#25968;&#25454;&#24211;&#19978;&#26159;&#21542;&#30001;&#31243;&#24207;&#32467;&#26500;&#36827;&#19968;&#27493;&#26263;&#31034;&#20102;&#26576;&#20010;&#29305;&#23450;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#38472;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31243;&#24207;&#32467;&#26500;&#30340;&#19968;&#20010;&#29255;&#27573;&#65292;&#23545;&#20110;&#36825;&#20010;&#29255;&#27573;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#20851;&#20110;&#26465;&#20214;&#29420;&#31435;&#24615;&#39044;&#35328;&#30340;&#23436;&#22791;&#24615;&#38472;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#20803;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pearl and Verma developed d-separation as a widely used graphical criterion to reason about the conditional independencies that are implied by the causal structure of a Bayesian network. As acyclic ground probabilistic logic programs correspond to Bayesian networks on their dependency graph, we can compute conditional independencies from d-separation in the latter.  In the present paper, we generalize the reasoning above to the non-ground case. First, we abstract the notion of a probabilistic logic program away from external databases and probabilities to obtain so-called program structures. We then present a correct meta-interpreter that decides whether a certain conditional independence statement is implied by a program structure on a given external database. Finally, we give a fragment of program structures for which we obtain a completeness statement of our conditional independence oracle. We close with an experimental evaluation of our approach revealing that our meta-interpreter 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#23398;&#20064;&#22768;&#26126;&#24615;&#39046;&#22495;&#29305;&#23450;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#27714;&#35299;&#24615;&#33021;&#21644;&#35299;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.15863</link><description>&lt;p&gt;
&#29992;&#20110;ASP&#30340;&#22768;&#26126;&#24615;&#39046;&#22495;&#29305;&#23450;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24402;&#32435;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inductive Learning of Declarative Domain-Specific Heuristics for ASP. (arXiv:2308.15863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#23398;&#20064;&#22768;&#26126;&#24615;&#39046;&#22495;&#29305;&#23450;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#27714;&#35299;&#24615;&#33021;&#21644;&#35299;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#21551;&#21457;&#24335;&#31639;&#27861;&#26159;&#35299;&#20915;&#22823;&#35268;&#27169;&#25110;&#35745;&#31639;&#22256;&#38590;&#38382;&#39064;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#31572;&#26696;&#38598;&#35268;&#21010;&#65288;ASP&#65289;&#31995;&#32479;&#25903;&#25345;&#22768;&#26126;&#24615;&#30340;&#39046;&#22495;&#29305;&#23450;&#21551;&#21457;&#24335;&#31639;&#27861;&#20197;&#25552;&#39640;&#27714;&#35299;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#24517;&#39035;&#25163;&#21160;&#21457;&#26126;&#12290;&#20026;&#20102;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#21457;&#26126;&#39046;&#22495;&#29305;&#23450;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#38656;&#35201;&#23545;&#25152;&#32771;&#34385;&#39046;&#22495;&#26377;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#29087;&#24713;ASP&#30340;&#35821;&#27861;&#12289;&#35821;&#20041;&#21644;&#27714;&#35299;&#25216;&#26415;&#12290;&#33258;&#21160;&#21270;&#25903;&#25345;&#20250;&#26497;&#22823;&#22320;&#25913;&#21892;&#21457;&#26126;&#26377;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#36825;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#20174;&#23567;&#32780;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#38382;&#39064;&#23454;&#20363;&#30340;&#65288;&#36817;&#20284;&#65289;&#26368;&#20248;&#31572;&#26696;&#38598;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#22768;&#26126;&#24615;&#30340;&#39046;&#22495;&#29305;&#23450;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#27714;&#35299;&#22823;&#35268;&#27169;&#38382;&#39064;&#26102;&#21487;&#20197;&#25552;&#39640;&#27714;&#35299;&#24615;&#33021;&#21644;&#35299;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-specific heuristics are a crucial technique for the efficient solving of problems that are large or computationally hard. Answer Set Programming (ASP) systems support declarative specifications of domain-specific heuristics to improve solving performance. However, such heuristics must be invented manually so far. Inventing domain-specific heuristics for answer-set programs requires expertise with the domain under consideration and familiarity with ASP syntax, semantics, and solving technology. The process of inventing useful heuristics would highly profit from automatic support. This paper presents a novel approach to the automatic learning of such heuristics. We use Inductive Logic Programming (ILP) to learn declarative domain-specific heuristics from examples stemming from (near-)optimal answer sets of small but representative problem instances. Our experimental results indicate that the learned heuristics can improve solving performance and solution quality when solving large
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#65288;ZIP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#30340;&#35270;&#35273;&#21442;&#32771;&#21644;&#25991;&#26412;&#24341;&#23548;&#27880;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#25552;&#31034;&#30340;&#30452;&#35266;&#25511;&#21046;&#19979;&#20135;&#29983;&#22810;&#26679;&#30340;&#20869;&#23481;&#21644;&#23646;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#23545;&#19981;&#21516;&#23646;&#24615;&#25805;&#20316;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15854</link><description>&lt;p&gt;
&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#30340;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models. (arXiv:2308.15854v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#65288;ZIP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#30340;&#35270;&#35273;&#21442;&#32771;&#21644;&#25991;&#26412;&#24341;&#23548;&#27880;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#25552;&#31034;&#30340;&#30452;&#35266;&#25511;&#21046;&#19979;&#20135;&#29983;&#22810;&#26679;&#30340;&#20869;&#23481;&#21644;&#23646;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#23545;&#19981;&#21516;&#23646;&#24615;&#25805;&#20316;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#22270;&#20687;&#24341;&#23548;&#26041;&#27861;&#65292;&#25552;&#20379;&#35270;&#35273;&#21442;&#32771;&#20294;&#32570;&#20047;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#25511;&#21046;&#65292;&#25110;&#32773;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#26041;&#27861;&#65292;&#30830;&#20445;&#23545;&#25991;&#26412;&#24341;&#23548;&#30340;&#24544;&#23454;&#65292;&#20294;&#32570;&#20047;&#35270;&#35273;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#65288;ZIP&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#29983;&#25104;&#30340;&#35270;&#35273;&#21442;&#32771;&#21644;&#25991;&#26412;&#24341;&#23548;&#30340;&#34701;&#21512;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#20013;&#12290;&#20165;&#20351;&#29992;&#19968;&#20010;&#24494;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#30340;ZIP&#22312;&#25991;&#26412;&#25552;&#31034;&#30340;&#30452;&#35266;&#25511;&#21046;&#19979;&#20135;&#29983;&#22810;&#26679;&#30340;&#20869;&#23481;&#21644;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;ZIP&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#23637;&#31034;&#20102;&#23545;&#22495;&#20869;&#21644;&#22495;&#22806;&#23646;&#24615;&#25805;&#20316;&#30340;&#26174;&#33879;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;ZIP&#20135;&#29983;&#20102;&#19982;&#20043;&#30456;&#24403;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36924;&#30495;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have shown outstanding performance in image editing. Existing works tend to use either image-guided methods, which provide a visual reference but lack control over semantic coherence, or text-guided methods, which ensure faithfulness to text guidance but lack visual quality. To address the problem, we propose the Zero-shot Inversion Process (ZIP), a framework that injects a fusion of generated visual reference and text guidance into the semantic latent space of a \textit{frozen} pre-trained diffusion model. Only using a tiny neural network, the proposed ZIP produces diverse content and attributes under the intuitive control of the text prompt. Moreover, ZIP shows remarkable robustness for both in-domain and out-of-domain attribute manipulation on real images. We perform detailed experiments on various benchmark datasets. Compared to state-of-the-art methods, ZIP produces images of equivalent quality while providing a realistic editing effect.
&lt;/p&gt;</description></item><item><title>MSGNN&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22810;&#23610;&#24230;&#35270;&#22270;&#21644;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;GNN&#27169;&#22411;&#22312;&#20445;&#30041;&#36828;&#36317;&#31163;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#27969;&#34892;&#30149;&#27169;&#24335;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#23427;&#22312;&#20256;&#26579;&#30149;&#39044;&#27979;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.15840</link><description>&lt;p&gt;
MSGNN: &#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27969;&#34892;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MSGNN: Multi-scale Spatio-temporal Graph Neural Network for Epidemic Forecasting. (arXiv:2308.15840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15840
&lt;/p&gt;
&lt;p&gt;
MSGNN&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22810;&#23610;&#24230;&#35270;&#22270;&#21644;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;GNN&#27169;&#22411;&#22312;&#20445;&#30041;&#36828;&#36317;&#31163;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#27969;&#34892;&#30149;&#27169;&#24335;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#23427;&#22312;&#20256;&#26579;&#30149;&#39044;&#27979;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#30149;&#39044;&#27979;&#26159;&#38450;&#25511;&#27969;&#34892;&#30149;&#30340;&#20851;&#38190;&#65292;&#24182;&#34987;&#35777;&#26126;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;1&#65289;&#24403;&#21069;&#27169;&#22411;&#36890;&#36807;&#32553;&#25918;GNN&#30340;&#28145;&#24230;&#26469;&#25193;&#22823;&#24863;&#21463;&#37326;&#65292;&#20294;&#36825;&#23545;&#20110;&#20445;&#30041;&#36828;&#36317;&#31163;&#20294;&#19982;&#27969;&#34892;&#30149;&#30456;&#20851;&#30340;&#22320;&#21306;&#20043;&#38388;&#30340;&#35821;&#20041;&#19981;&#36275;&#22815;&#12290;&#65288;2&#65289;&#20197;&#21069;&#30340;&#26041;&#27861;&#27169;&#25311;&#21333;&#19968;&#31354;&#38388;&#23610;&#24230;&#20869;&#30340;&#27969;&#34892;&#30149;&#65292;&#32780;&#24573;&#35270;&#20102;&#19981;&#21516;&#23610;&#24230;&#19978;&#23548;&#20986;&#30340;&#22810;&#23610;&#24230;&#27969;&#34892;&#30149;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#21019;&#26032;&#30340;&#22810;&#23610;&#24230;&#35270;&#22270;&#30340;&#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MSGNN&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#25552;&#20986;&#30340;MSGNN&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#23427;&#30452;&#25509;&#25429;&#25417;&#36328;&#21306;&#22495;&#27969;&#34892;&#30149;&#20449;&#21495;&#30340;&#36828;&#36317;&#31163;&#36830;&#25509;&#65292;&#24182;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#19968;&#20010;&#22810;&#23610;&#24230;&#22270;&#20013;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#23610;&#24230;&#22270;&#65292;&#25105;&#20204;&#20351;&#29992;&#31354;&#38388;-&#26102;&#38388;GCN&#36827;&#34892;&#27969;&#34892;&#30149;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infectious disease forecasting has been a key focus and proved to be crucial in controlling epidemic. A recent trend is to develop forecast-ing models based on graph neural networks (GNNs). However, existing GNN-based methods suffer from two key limitations: (1) Current models broaden receptive fields by scaling the depth of GNNs, which is insuffi-cient to preserve the semantics of long-range connectivity between distant but epidemic related areas. (2) Previous approaches model epidemics within single spatial scale, while ignoring the multi-scale epidemic pat-terns derived from different scales. To address these deficiencies, we devise the Multi-scale Spatio-temporal Graph Neural Network (MSGNN) based on an innovative multi-scale view. To be specific, in the proposed MSGNN model, we first devise a novel graph learning module, which directly captures long-range connectivity from trans-regional epidemic signals and integrates them into a multi-scale graph. Based on the learned multi-scal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30740;&#31350;&#20102;&#30005;&#27744;&#23481;&#37327;&#34928;&#20943;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;WOA-ELM&#27169;&#22411;&#26469;&#35299;&#26512;&#30005;&#27744;&#24615;&#33021;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#20811;&#26381;&#20102;&#26426;&#22120;&#23398;&#20064;&#40657;&#30418;&#30340;&#32570;&#38519;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#30005;&#26497;&#26448;&#26009;&#30340;&#32467;&#26500;&#25439;&#20260;&#19982;&#30005;&#27744;&#25925;&#38556;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23545;&#20110;&#30005;&#27744;&#30740;&#31350;&#21644;&#25913;&#36827;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.15833</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#30005;&#27744;&#24615;&#33021;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Depth analysis of battery performance based on a data-driven approach. (arXiv:2308.15833v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30740;&#31350;&#20102;&#30005;&#27744;&#23481;&#37327;&#34928;&#20943;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;WOA-ELM&#27169;&#22411;&#26469;&#35299;&#26512;&#30005;&#27744;&#24615;&#33021;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#20811;&#26381;&#20102;&#26426;&#22120;&#23398;&#20064;&#40657;&#30418;&#30340;&#32570;&#38519;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#30005;&#26497;&#26448;&#26009;&#30340;&#32467;&#26500;&#25439;&#20260;&#19982;&#30005;&#27744;&#25925;&#38556;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23545;&#20110;&#30005;&#27744;&#30740;&#31350;&#21644;&#25913;&#36827;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23481;&#37327;&#34928;&#20943;&#26159;&#24403;&#21069;&#30005;&#27744;&#24212;&#29992;&#20013;&#26368;&#26840;&#25163;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#22312;&#25972;&#20010;&#31995;&#32479;&#20013;&#65292;&#30005;&#27744;&#30340;&#20998;&#35299;&#26426;&#21046;&#34987;&#35748;&#20026;&#38750;&#24120;&#22797;&#26434;&#12290;&#20805;&#20998;&#29702;&#35299;&#36825;&#20010;&#36807;&#31243;&#24182;&#20934;&#30830;&#39044;&#27979;&#23427;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#39044;&#27979;&#30005;&#27744;&#22312;&#24490;&#29615;&#36807;&#31243;&#20013;&#30340;&#29305;&#23450;&#23481;&#37327;&#21464;&#21270;&#65292;&#24182;&#20102;&#35299;&#36825;&#20010;&#22797;&#26434;&#30340;&#36807;&#31243;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#26681;&#25454;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;WOA-ELM&#27169;&#22411;&#65288;R2 = 0.9999871&#65289;&#65292;&#30830;&#23450;&#20102;&#24433;&#21709;&#30005;&#27744;&#29305;&#23450;&#23481;&#37327;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#20811;&#26381;&#20102;&#26426;&#22120;&#23398;&#20064;&#40657;&#30418;&#30340;&#32570;&#38519;&#12290;&#23427;&#20204;&#19982;&#30005;&#26497;&#26448;&#26009;&#30340;&#32467;&#26500;&#25439;&#20260;&#21644;&#30005;&#27744;&#22312;&#24490;&#29615;&#36807;&#31243;&#20013;&#30340;&#25925;&#38556;&#20043;&#38388;&#30340;&#32852;&#31995;&#24471;&#21040;&#20102;&#20840;&#38754;&#35299;&#37322;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#23545;&#20110;&#30005;&#27744;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#21033;&#20110;&#23545;&#24403;&#20195;&#30005;&#27744;&#36827;&#34892;&#20248;&#31168;&#30340;&#30740;&#31350;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capacity attenuation is one of the most intractable issues in the current of application of the cells. The disintegration mechanism is well known to be very complex across the system. It is a great challenge to fully comprehend this process and predict the process accurately. Thus, the machine learning (ML) technology is employed to predict the specific capacity change of the cell throughout the cycle and grasp this intricate procedure. Different from the previous work, according to the WOA-ELM model proposed in this work (R2 = 0.9999871), the key factors affecting the specific capacity of the battery are determined, and the defects in the machine learning black box are overcome by the interpretable model. Their connection with the structural damage of electrode materials and battery failure during battery cycling is comprehensively explained, revealing their essentiality to battery performance, which is conducive to superior research on contemporary batteries and modification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22768;&#23398;&#20449;&#21495;&#30340;&#32418;&#26837;&#35937;&#20405;&#26579;&#26089;&#26399;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26685;&#22521;&#26531;&#26928;&#26641;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2308.15829</link><description>&lt;p&gt;
&#26089;&#26399;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#32418;&#26837;&#35937;&#20405;&#26579;&#36827;&#34892;&#22768;&#23398;&#20449;&#21495;&#20998;&#31867;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Early Detection of Red Palm Weevil Infestations using Deep Learning Classification of Acoustic Signals. (arXiv:2308.15829v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22768;&#23398;&#20449;&#21495;&#30340;&#32418;&#26837;&#35937;&#20405;&#26579;&#26089;&#26399;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26685;&#22521;&#26531;&#26928;&#26641;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#26837;&#35937;&#34987;&#35748;&#20026;&#26159;&#20840;&#29699;&#26368;&#20855;&#30772;&#22351;&#24615;&#30340;&#26837;&#27016;&#26641;&#23475;&#34411;&#20043;&#19968;&#12290;&#30446;&#21069;&#30340;&#26816;&#27979;&#25216;&#26415;&#21253;&#25324;&#20351;&#29992;&#35270;&#35273;&#25110;&#22768;&#38899;&#26816;&#26597;&#20197;&#21450;&#21270;&#23398;&#26816;&#27979;&#21463;&#20405;&#26579;&#26837;&#27016;&#26641;&#29983;&#25104;&#30340;&#25381;&#21457;&#24615;&#29305;&#24449;&#30340;&#30149;&#30151;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26685;&#22521;&#26531;&#26928;&#26641;&#26469;&#35828;&#65292;&#21450;&#26089;&#26816;&#27979;&#32418;&#26837;&#35937;&#30149;&#23475;&#34987;&#35748;&#20026;&#26159;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32418;&#26837;&#35937;&#26089;&#26399;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#35760;&#24405;&#21644;&#20998;&#26512;&#32418;&#26837;&#35937;&#22768;&#38899;&#27963;&#21160;&#12290;&#31532;&#19968;&#27493;&#26159;&#26681;&#25454;&#36873;&#25321;&#30340;&#19968;&#32452;&#29305;&#24449;&#23558;&#22768;&#38899;&#25968;&#25454;&#36716;&#25442;&#25104;&#22270;&#20687;&#12290;&#31532;&#20108;&#27493;&#26159;&#23558;&#26469;&#33258;&#21516;&#19968;&#22768;&#38899;&#25991;&#20214;&#20294;&#30001;&#19981;&#21516;&#29305;&#24449;&#35745;&#31639;&#24471;&#20986;&#30340;&#22270;&#20687;&#32452;&#21512;&#25104;&#21333;&#20010;&#22270;&#20687;&#12290;&#31532;&#19977;&#27493;&#26159;&#24212;&#29992;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23558;&#32467;&#26524;&#22270;&#20687;&#20998;&#31867;&#20026;&#20004;&#31867;&#65306;&#21463;&#20405;&#26579;&#21644;&#26410;&#21463;&#20405;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Red Palm Weevil (RPW), also known as the palm weevil, is considered among the world's most damaging insect pests of palms. Current detection techniques include the detection of symptoms of RPW using visual or sound inspection and chemical detection of volatile signatures generated by infested palm trees. However, efficient detection of RPW diseases at an early stage is considered one of the most challenging issues for cultivating date palms. In this paper, an efficient approach to the early detection of RPW is proposed. The proposed approach is based on RPW sound activities being recorded and analyzed. The first step involves the conversion of sound data into images based on a selected set of features. The second step involves the combination of images from the same sound file but computed by different features into a single image. The third step involves the application of different Deep Learning (DL) techniques to classify resulting images into two classes: infested and not infes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32858;&#31867;&#26102;&#38388;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15821</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Federated Two Stage Decoupling With Adaptive Personalization Layers. (arXiv:2308.15821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32858;&#31867;&#26102;&#38388;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20998;&#24067;&#24335;&#35774;&#22791;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#20445;&#25345;&#38544;&#31169;&#32422;&#26463;&#30340;&#21516;&#26102;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#23398;&#20064;&#38477;&#32423;&#21644;&#24930;&#25910;&#25947;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#22320;&#37319;&#29992;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#27010;&#24565;&#65292;&#21482;&#20801;&#35768;&#22312;&#27599;&#20010;&#32452;&#20869;&#32858;&#21512;&#27169;&#22411;&#26435;&#37325;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#27169;&#22411;&#26799;&#24230;&#25110;&#25512;&#29702;&#36755;&#20986;&#20316;&#20026;&#23458;&#25143;&#31471;&#20998;&#21306;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#30446;&#30340;&#26159;&#23558;&#30456;&#20284;&#35774;&#22791;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20294;&#27599;&#20010;&#32858;&#31867;&#20869;&#37096;&#20173;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#30740;&#31350;&#25506;&#32034;&#30830;&#23450;&#32858;&#31867;&#30340;&#36866;&#24403;&#26102;&#38388;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#23548;&#33268;&#24120;&#35265;&#20570;&#27861;&#26159;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21040;&#20854;&#33258;&#24049;&#30340;&#29420;&#31435;&#32858;&#31867;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#38750;ind&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained significant attention due to its groundbreaking ability to enable distributed learning while maintaining privacy constraints. However, as a consequence of data heterogeneity among decentralized devices, it inherently experiences significant learning degradation and slow convergence speed. Therefore, it is natural to employ the concept of clustering homogeneous clients into the same group, allowing only the model weights within each group to be aggregated. While most existing clustered federated learning methods employ either model gradients or inference outputs as metrics for client partitioning, with the goal of grouping similar devices together, may still have heterogeneity within each cluster. Moreover, there is a scarcity of research exploring the underlying reasons for determining the appropriate timing for clustering, resulting in the common practice of assigning each client to its own individual cluster, particularly in the context of highly non ind
&lt;/p&gt;</description></item><item><title>SharpSAT-TD&#26159;&#22522;&#20110;SharpSAT&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#22312;&#27169;&#22411;&#35745;&#25968;&#31454;&#36187;&#20013;&#33719;&#24471;&#20102;&#22810;&#20010;&#31532;&#19968;&#21517;&#65292;&#24182;&#24341;&#20837;&#20102;&#26641;&#20998;&#35299;&#30340;&#21464;&#37327;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15819</link><description>&lt;p&gt;
SharpSAT-TD&#22312;2021-2023&#27169;&#22411;&#35745;&#25968;&#31454;&#36187;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SharpSAT-TD in Model Counting Competitions 2021-2023. (arXiv:2308.15819v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15819
&lt;/p&gt;
&lt;p&gt;
SharpSAT-TD&#26159;&#22522;&#20110;SharpSAT&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#22312;&#27169;&#22411;&#35745;&#25968;&#31454;&#36187;&#20013;&#33719;&#24471;&#20102;&#22810;&#20010;&#31532;&#19968;&#21517;&#65292;&#24182;&#24341;&#20837;&#20102;&#26641;&#20998;&#35299;&#30340;&#21464;&#37327;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;SharpSAT-TD&#65292;&#25105;&#20204;&#25552;&#20132;&#21040;2021-2023&#27169;&#22411;&#35745;&#25968;&#31454;&#36187;&#30340;&#38750;&#21152;&#26435;&#21644;&#21152;&#26435;&#36187;&#36947;&#30340;&#20316;&#21697;&#12290;&#23427;&#22312;&#27604;&#36187;&#30340;&#19981;&#21516;&#36187;&#36947;&#20013;&#20849;&#33719;&#24471;6&#27425;&#31532;&#19968;&#21517;&#12290;SharpSAT-TD&#22522;&#20110;SharpSAT [Thurley, SAT 2006]&#65292;&#20854;&#20027;&#35201;&#30340;&#21019;&#26032;&#20462;&#25913;&#26159;&#24341;&#20837;&#20102;&#20316;&#32773;&#22312;[CP 2021]&#20013;&#20171;&#32461;&#30340;&#21464;&#37327;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#20013;&#30340;&#26641;&#20998;&#35299;&#26041;&#27861;&#12290;&#19982;[CP 2021]&#20013;&#35780;&#20272;&#30340;SharpSAT-TD&#29256;&#26412;&#19981;&#21516;&#65292;&#30446;&#21069;&#22312;https://github.com/Laakeri/sharpsat-td&#19978;&#21487;&#29992;&#30340;&#29256;&#26412;&#19982;&#21407;&#22987;&#30340;SharpSAT&#30456;&#27604;&#20063;&#26377;&#20854;&#20182;&#37325;&#35201;&#30340;&#20462;&#25913;&#65292;&#20363;&#22914;&#28155;&#21152;&#20102;&#26032;&#30340;&#39044;&#22788;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe SharpSAT-TD, our submission to the unweighted and weighted tracks of the Model Counting Competition in 2021-2023, which has won in total $6$ first places in different tracks of the competition. SharpSAT-TD is based on SharpSAT [Thurley, SAT 2006], with the primary novel modification being the use of tree decompositions in the variable selection heuristic as introduced by the authors in [CP 2021]. Unlike the version of SharpSAT-TD evaluated in [CP 2021], the current version that is available in https://github.com/Laakeri/sharpsat-td features also other significant modifications compared to the original SharpSAT, for example, a new preprocessor.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15812</link><description>&lt;p&gt;
&#36879;&#36807;&#20559;&#22909;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#33719;&#21462;&#65306;&#25581;&#31034;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#30340;&#23545;&#40784;&#25215;&#35834;&#28041;&#21450;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#21453;&#39304;&#12290;&#31264;&#23494;&#30340;&#21453;&#39304;&#27880;&#37322;&#33719;&#21462;&#21644;&#25972;&#21512;&#25104;&#26412;&#36739;&#39640;&#65292;&#32780;&#31232;&#30095;&#30340;&#21453;&#39304;&#21017;&#28041;&#21450;&#32467;&#26500;&#24615;&#35774;&#35745;&#36873;&#25321;&#65292;&#21363;&#35780;&#20998;&#65288;&#20363;&#22914;&#65292;&#22312;1-7&#30340;&#33539;&#22260;&#20869;&#23545;&#22238;&#31572;A&#36827;&#34892;&#35780;&#20998;&#65289;&#21644;&#25490;&#21517;&#65288;&#20363;&#22914;&#65292;&#22238;&#31572;A&#26159;&#21542;&#27604;&#22238;&#31572;B&#26356;&#22909;&#65311;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#23545;LLMs&#30340;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#22312;&#20154;&#31867;&#21644;AI&#27880;&#37322;&#32773;&#20013;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;60%&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#20998;&#26512;&#30830;&#23450;&#20102;&#35299;&#37322;&#36825;&#20010;&#29616;&#35937;&#30340;&#21508;&#31181;&#27880;&#37322;&#32773;&#20559;&#35265;&#26041;&#38754;&#65292;&#27604;&#22914;&#20154;&#31867;&#27880;&#37322;&#32773;&#26356;&#21916;&#27426;&#23494;&#38598;&#22238;&#31572;&#24182;&#22312;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#26356;&#38738;&#30544;&#20934;&#30830;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#23545;&#23545;&#40784;&#30340;LLMs&#30340;&#35780;&#20272;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#35780;&#20272;&#32467;&#26524;&#22240;&#20026;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#20108;&#23626;&#31070;&#32463;MMO&#25361;&#25112;&#36187;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#34920;&#26126;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#24037;&#31243;&#25216;&#26415;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#27604;&#36187;&#20316;&#20026;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#21644;&#24314;&#31435;&#31639;&#27861;&#26631;&#20934;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15802</link><description>&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#23545;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;: &#20197;&#31070;&#32463;MMO&#20026;&#20363;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Robustness and Generalization in Multi-Agent Systems: A Case Study on Neural MMO. (arXiv:2308.15802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#20108;&#23626;&#31070;&#32463;MMO&#25361;&#25112;&#36187;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#34920;&#26126;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#24037;&#31243;&#25216;&#26415;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#27604;&#36187;&#20316;&#20026;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#21644;&#24314;&#31435;&#31639;&#27861;&#26631;&#20934;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;IJCAI 2022&#20030;&#21150;&#30340;&#31532;&#20108;&#23626;&#31070;&#32463;MMO&#25361;&#25112;&#36187;&#30340;&#32467;&#26524;&#65292;&#20849;&#25910;&#21040;1600&#22810;&#20010;&#25237;&#31295;&#12290;&#35813;&#27604;&#36187;&#26088;&#22312;&#27979;&#35797;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65306;&#21442;&#19982;&#32773;&#35757;&#32451;&#20195;&#29702;&#22242;&#38431;&#20197;&#23436;&#25104;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#23545;&#25163;&#30340;&#22810;&#20219;&#21153;&#30446;&#26631;&#12290;&#27604;&#36187;&#32467;&#21512;&#20102;&#30456;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#35774;&#35745;&#19982;&#22823;&#37327;&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#30340;&#36816;&#34892;&#12290;&#39030;&#32423;&#25237;&#31295;&#23637;&#31034;&#20102;&#20351;&#29992;&#20027;&#35201;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#24037;&#31243;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25104;&#32489;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#27604;&#36187;&#30340;&#35774;&#35745;&#21644;&#32467;&#26524;&#65292;&#24182;&#24314;&#35758;&#20316;&#20026;&#23398;&#26415;&#30028;&#65292;&#27604;&#36187;&#21487;&#33021;&#26159;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#21644;&#24314;&#31435;&#31639;&#27861;&#31283;&#20581;&#26631;&#20934;&#30340;&#19968;&#20010;&#24378;&#22823;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#24320;&#28304;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#29615;&#22659;&#23553;&#35013;&#22120;&#12289;&#22522;&#20934;&#28304;&#30721;&#12289;&#21487;&#35270;&#21270;&#24037;&#20855;&#21644;&#36873;&#23450;&#30340;&#31574;&#30053;&#65292;&#20379;&#36827;&#19968;&#27493;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the results of the second Neural MMO challenge, hosted at IJCAI 2022, which received 1600+ submissions. This competition targets robustness and generalization in multi-agent systems: participants train teams of agents to complete a multi-task objective against opponents not seen during training. The competition combines relatively complex environment design with large numbers of agents in the environment. The top submissions demonstrate strong success on this task using mostly standard reinforcement learning (RL) methods combined with domain-specific engineering. We summarize the competition design and results and suggest that, as an academic community, competitions may be a powerful approach to solving hard problems and establishing a solid benchmark for algorithms. We will open-source our benchmark including the environment wrapper, baselines, a visualization tool, and selected policies for further research.
&lt;/p&gt;</description></item><item><title>FedCiR&#26159;&#19968;&#31181;&#23458;&#25143;&#31471;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20559;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25913;&#36827;&#34920;&#31034;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#39033;&#26469;&#25552;&#21462;&#20449;&#24687;&#19988;&#19982;&#23458;&#25143;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.15786</link><description>&lt;p&gt;
FedCiR: &#23458;&#25143;&#31471;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#32852;&#37030;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
FedCiR: Client-Invariant Representation Learning for Federated Non-IID Features. (arXiv:2308.15786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15786
&lt;/p&gt;
&lt;p&gt;
FedCiR&#26159;&#19968;&#31181;&#23458;&#25143;&#31471;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20559;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25913;&#36827;&#34920;&#31034;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#39033;&#26469;&#25552;&#21462;&#20449;&#24687;&#19988;&#19982;&#23458;&#25143;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#20102;&#36793;&#32536;&#35774;&#22791;&#19978;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#30340;&#25968;&#25454;&#24448;&#24448;&#26159;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#30340;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#35774;&#22791;&#20043;&#38388;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#65292;&#36890;&#24120;&#31216;&#20026;&#29305;&#24449;&#20559;&#31227;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#35757;&#32451;&#25910;&#25947;&#24615;&#21644;&#20934;&#30830;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#20998;&#26512;&#29305;&#24449;&#20559;&#31227;&#38382;&#39064;&#30340;&#20869;&#22312;&#21407;&#22240;&#65292;&#25105;&#20204;&#22312;FL&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#24191;&#20041;&#35823;&#24046;&#36793;&#30028;&#65292;&#36825;&#28608;&#21169;&#25105;&#20204;&#25552;&#20986;&#20102;FedCiR&#65292;&#19968;&#31181;&#23458;&#25143;&#31471;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#23458;&#25143;&#33021;&#22815;&#25552;&#21462;&#20449;&#24687;&#19988;&#19982;&#23458;&#25143;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#34920;&#31034;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#39033;&#65292;&#20197;&#40723;&#21169;&#34920;&#31034;&#25658;&#24102;&#22522;&#26412;&#30340;&#20998;&#31867;&#30693;&#35782;&#65292;&#24182;&#20943;&#23567;&#20102;&#23458;&#25143;&#31471;&#21644;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed learning paradigm that maximizes the potential of data-driven models for edge devices without sharing their raw data. However, devices often have non-independent and identically distributed (non-IID) data, meaning their local data distributions can vary significantly. The heterogeneity in input data distributions across devices, commonly referred to as the feature shift problem, can adversely impact the training convergence and accuracy of the global model. To analyze the intrinsic causes of the feature shift problem, we develop a generalization error bound in FL, which motivates us to propose FedCiR, a client-invariant representation learning framework that enables clients to extract informative and client-invariant features. Specifically, we improve the mutual information term between representations and labels to encourage representations to carry essential classification knowledge, and diminish the mutual information term between the client 
&lt;/p&gt;</description></item><item><title>ASTER&#26159;&#19968;&#31181;&#38754;&#21521;&#21475;&#21507;&#24739;&#32773;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21487;&#35775;&#38382;&#24615;&#27979;&#35797;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#25311;&#21475;&#21507;&#35821;&#38899;&#24182;&#29992;&#20110;&#27979;&#35797;&#21644;&#20998;&#26512;ASR&#31995;&#32479;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15742</link><description>&lt;p&gt;
ASTER: &#38754;&#21521;&#21475;&#21507;&#24739;&#32773;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21487;&#35775;&#38382;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ASTER: Automatic Speech Recognition System Accessibility Testing for Stutterers. (arXiv:2308.15742v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15742
&lt;/p&gt;
&lt;p&gt;
ASTER&#26159;&#19968;&#31181;&#38754;&#21521;&#21475;&#21507;&#24739;&#32773;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21487;&#35775;&#38382;&#24615;&#27979;&#35797;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#25311;&#21475;&#21507;&#35821;&#38899;&#24182;&#29992;&#20110;&#27979;&#35797;&#21644;&#20998;&#26512;ASR&#31995;&#32479;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#26222;&#21450;&#24615;&#23548;&#33268;&#20102;&#23545;&#20854;&#21487;&#35775;&#38382;&#24615;&#30340;&#19981;&#26029;&#25913;&#21892;&#30340;&#38656;&#27714;&#12290;&#22788;&#29702;&#21475;&#21507;&#30340;&#35821;&#38899;&#26159;&#21487;&#35775;&#38382;&#24615;ASR&#31995;&#32479;&#30340;&#37325;&#35201;&#29305;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#21475;&#21507;&#24739;&#32773;&#30340;ASR&#31995;&#32479;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#25105;&#20204;&#38656;&#35201;&#26292;&#38706;&#21644;&#20998;&#26512;ASR&#31995;&#32479;&#22312;&#21475;&#21507;&#35821;&#38899;&#19978;&#30340;&#22833;&#36133;&#12290;&#20174;&#21475;&#21507;&#24739;&#32773;&#24405;&#21046;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#19981;&#36275;&#20197;&#26292;&#38706;&#22823;&#37096;&#20998;&#30340;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#32570;&#20047;&#20851;&#20110;&#38750;&#21475;&#21507;&#25991;&#26412;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#20351;&#20854;&#19981;&#36866;&#21512;&#20316;&#20026;&#20840;&#38754;&#30340;&#27979;&#35797;&#22871;&#20214;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#25552;&#20986;&#19968;&#31181;&#29983;&#25104;&#27169;&#25311;&#21475;&#21507;&#35821;&#38899;&#24182;&#29992;&#20110;&#27979;&#35797;&#21644;&#20998;&#26512;ASR&#31995;&#32479;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#26377;&#25928;&#30340;&#27979;&#35797;&#36755;&#20837;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21407;&#22240;&#22312;&#20110;&#34429;&#28982;&#29983;&#25104;&#30340;&#27979;&#35797;&#36755;&#20837;&#24212;&#35813;&#27169;&#20223;&#21475;&#21507;&#32773;&#30340;&#35828;&#35805;&#26041;&#24335;&#65292;&#20294;&#20063;&#24212;&#35813;&#20855;&#26377;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#20197;&#35302;&#21457;&#26356;&#22810;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ASTER&#65292;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#21475;&#21507;&#32773;&#20351;&#29992;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#21487;&#35775;&#38382;&#24615;&#30340;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of automatic speech recognition (ASR) systems nowadays leads to an increasing need for improving their accessibility. Handling stuttering speech is an important feature for accessible ASR systems. To improve the accessibility of ASR systems for stutterers, we need to expose and analyze the failures of ASR systems on stuttering speech. The speech datasets recorded from stutterers are not diverse enough to expose most of the failures. Furthermore, these datasets lack ground truth information about the non-stuttered text, rendering them unsuitable as comprehensive test suites. Therefore, a methodology for generating stuttering speech as test inputs to test and analyze the performance of ASR systems is needed. However, generating valid test inputs in this scenario is challenging. The reason is that although the generated test inputs should mimic how stutterers speak, they should also be diverse enough to trigger more failures. To address the challenge, we propose ASTER, a te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21517;&#20026;ExGNAS&#12290;&#23427;&#21253;&#25324;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#33021;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.15734</link><description>&lt;p&gt;
&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient and Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search. (arXiv:2308.15734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21517;&#20026;ExGNAS&#12290;&#23427;&#21253;&#25324;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#33021;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22312;&#21508;&#20010;&#39046;&#22495;&#36827;&#34892;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#20351;&#29992;GNNs&#65292;&#20294;&#23545;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26469;&#35828;&#65292;&#22312;&#19981;&#21516;&#30340;&#22270;&#20013;&#35774;&#35745;/&#36873;&#25321;&#26368;&#20339;GNN&#26550;&#26500;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#33410;&#30465;&#20154;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24050;&#32463;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;Graph NAS&#65289;&#26469;&#25628;&#32034;&#32467;&#21512;&#29616;&#26377;&#32452;&#20214;&#30340;&#27425;&#20248;GNN&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;Graph NAS&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#28385;&#36275;&#21487;&#35299;&#37322;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#22810;&#26679;&#21270;&#22270;&#24418;&#30340;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;Graph NAS&#26041;&#27861;&#65292;&#31216;&#20026;ExGNAS&#65292;&#23427;&#21253;&#25324;&#65288;i&#65289;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#65288;ii&#65289;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#25628;&#32034;&#31354;&#38388;&#20165;&#21253;&#21547;&#21487;&#20197;&#22788;&#29702;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#30340;&#22522;&#26412;&#20989;&#25968;&#12290;&#25628;&#32034;&#31639;&#27861;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are powerful tools for performing data science tasks in various domains. Although we use GNNs in wide application scenarios, it is a laborious task for researchers and practitioners to design/select optimal GNN rchitectures in diverse graphs. To save human efforts and computational costs, graph neural architecture search (Graph NAS) has been used to search for a sub-optimal GNN architecture that combines existing components. However, there are no existing Graph NAS methods that satisfy explainability, efficiency, and adaptability to various graphs. Therefore, we propose an efficient and explainable Graph NAS method, called ExGNAS, which consists of (i) a simple search space that can adapt to various graphs and (ii) a search algorithm that makes the decision process explainable. The search space includes only fundamental functions that can handle homophilic and heterophilic graphs. The search algorithm efficiently searches for the best GNN architecture via M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23478;&#24237;&#29615;&#22659;&#22768;&#38899;&#20107;&#20214;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65288;AGS&#65289;&#65292;&#23545;&#20110;&#23460;&#20869;&#29615;&#22659;&#22768;&#38899;&#22330;&#26223;&#30340;&#22768;&#38899;&#20107;&#20214;&#35782;&#21035;&#30740;&#31350;&#39046;&#22495;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35813;&#25968;&#25454;&#38598;&#32771;&#34385;&#20102;&#37325;&#21472;&#38899;&#39057;&#21644;&#32972;&#26223;&#22122;&#22768;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#21644;&#20998;&#26512;&#20808;&#36827;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#20854;&#21487;&#38752;&#24615;&#21644;&#30740;&#31350;&#26032;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15726</link><description>&lt;p&gt;
AGS: &#19968;&#31181;&#29992;&#20110;&#23478;&#24237;&#29615;&#22659;&#22768;&#38899;&#20107;&#20214;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AGS: An Dataset and Taxonomy for Domestic Scene Sound Event Recognition. (arXiv:2308.15726v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23478;&#24237;&#29615;&#22659;&#22768;&#38899;&#20107;&#20214;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65288;AGS&#65289;&#65292;&#23545;&#20110;&#23460;&#20869;&#29615;&#22659;&#22768;&#38899;&#22330;&#26223;&#30340;&#22768;&#38899;&#20107;&#20214;&#35782;&#21035;&#30740;&#31350;&#39046;&#22495;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35813;&#25968;&#25454;&#38598;&#32771;&#34385;&#20102;&#37325;&#21472;&#38899;&#39057;&#21644;&#32972;&#26223;&#22122;&#22768;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#21644;&#20998;&#26512;&#20808;&#36827;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#20854;&#21487;&#38752;&#24615;&#21644;&#30740;&#31350;&#26032;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#22768;&#38899;&#22330;&#26223;&#21644;&#22768;&#38899;&#20107;&#20214;&#35782;&#21035;&#23545;&#20110;&#23460;&#20869;&#21644;&#23460;&#22806;&#29615;&#22659;&#20013;&#21487;&#30097;&#20107;&#20214;&#30340;&#35782;&#21035;&#65288;&#22914;&#25176;&#20799;&#25152;&#12289;&#26234;&#33021;&#23478;&#23621;&#12289;&#20859;&#32769;&#38498;&#31561;&#65289;&#38750;&#24120;&#37325;&#35201;&#65292;&#20063;&#26159;&#35768;&#22810;&#38899;&#39057;&#30417;&#25511;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#23588;&#20854;&#26159;&#22312;&#23460;&#20869;&#29615;&#22659;&#22768;&#38899;&#22330;&#26223;&#30340;&#22768;&#38899;&#20107;&#20214;&#35782;&#21035;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#23578;&#26410;&#26377;&#20844;&#20849;&#30340;&#24120;&#35265;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23478;&#24237;&#29615;&#22659;&#22768;&#38899;&#30340;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;AGS&#65289;&#12290;&#35813;&#25968;&#25454;&#38598;&#32771;&#34385;&#20102;&#22330;&#26223;&#20013;&#21508;&#31181;&#31867;&#22411;&#30340;&#37325;&#21472;&#38899;&#39057;&#21644;&#32972;&#26223;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#23545;&#22768;&#38899;&#20107;&#20214;&#35782;&#21035;&#30340;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#20998;&#26512;&#65292;&#24182;&#39564;&#35777;&#20102;&#26412;&#25991;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#26032;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Environmental sound scene and sound event recognition is important for the recognition of suspicious events in indoor and outdoor environments (such as nurseries, smart homes, nursing homes, etc.) and is a fundamental task involved in many audio surveillance applications. In particular, there is no public common data set for the research field of sound event recognition for the data set of the indoor environmental sound scene. Therefore, this paper proposes a data set (called as AGS) for the home environment sound. This data set considers various types of overlapping audio in the scene, background noise. Moreover, based on the proposed data set, this paper compares and analyzes the advanced methods for sound event recognition, and then illustrates the reliability of the data set proposed in this paper, and studies the challenges raised by the new data set. Our proposed AGS and the source code of the corresponding baselines at https://github.com/taolunzu11/AGS .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#35299;&#20915;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#22312;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15720</link><description>&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Surrogate-based Autotuning for Randomized Sketching Algorithms in Regression Problems. (arXiv:2308.15720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#35299;&#20915;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#22312;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;(RandNLA)&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#35745;&#31639;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#32463;&#39564;&#24615;&#33021;&#20197;&#21450;&#24378;&#22823;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#19968;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#25152;&#38480;&#21046;&#65292;&#21363;&#29992;&#25143;&#38656;&#35201;&#35774;&#32622;&#21508;&#31181;&#19981;&#21516;&#20110;&#20256;&#32479;NLA&#20013;&#20351;&#29992;&#30340;&#31639;&#27861;&#29305;&#23450;&#35843;&#21442;&#21442;&#25968;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#26469;&#35299;&#20915;RandNLA&#31639;&#27861;&#20013;&#21442;&#25968;&#36873;&#25321;&#30340;&#22522;&#30784;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#33609;&#22270;&#21644;&#39044;&#22788;&#29702;(SAP)&#30340;&#38543;&#26426;&#21270;&#26368;&#23567;&#20108;&#20056;&#26041;&#27861;&#36827;&#34892;&#20102;&#26367;&#20195;&#27169;&#22411;&#33258;&#21160;&#35843;&#20248;&#30340;&#35814;&#32454;&#30740;&#31350;&#65292;&#36825;&#22312;&#29616;&#20195;RandNLA&#20013;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25104;&#21151;&#26696;&#20363;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#21487;&#20197;&#20197;&#27604;&#38543;&#26426;&#25628;&#32034;&#23569;&#32422;4&#20493;&#30340;&#35797;&#39564;&#25104;&#26412;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms from Randomized Numerical Linear Algebra (RandNLA) are known to be effective in handling high-dimensional computational problems, providing high-quality empirical performance as well as strong probabilistic guarantees. However, their practical application is complicated by the fact that the user needs to set various algorithm-specific tuning parameters which are different than those used in traditional NLA. This paper demonstrates how a surrogate-based autotuning approach can be used to address fundamental problems of parameter selection in RandNLA algorithms. In particular, we provide a detailed investigation of surrogate-based autotuning for sketch-and-precondition (SAP) based randomized least squares methods, which have been one of the great success stories in modern RandNLA. Empirical results show that our surrogate-based autotuning approach can achieve near-optimal performance with much less tuning cost than a random search (up to about 4x fewer trials of different para
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DKGen&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#30693;&#35782;&#21442;&#32771;&#65292;&#28040;&#38500;&#20102;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#38750;&#20851;&#32852;&#21442;&#32771;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15711</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#30693;&#35782;&#36873;&#25321;&#20248;&#21270;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimizing Factual Accuracy in Text Generation through Dynamic Knowledge Selection. (arXiv:2308.15711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DKGen&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#30693;&#35782;&#21442;&#32771;&#65292;&#28040;&#38500;&#20102;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#38750;&#20851;&#32852;&#21442;&#32771;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#25105;&#20204;&#19982;&#20449;&#24687;&#20114;&#21160;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#25991;&#26412;&#65292;&#24341;&#21457;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#20316;&#20026;&#25991;&#26412;&#29983;&#25104;&#30340;&#21442;&#32771;&#26469;&#22686;&#24378;&#20107;&#23454;&#24615;&#65292;&#20294;&#24448;&#24448;&#22312;&#26080;&#20851;&#21442;&#32771;&#30340;&#30693;&#35782;&#28151;&#20081;&#65288;&#22914;&#23454;&#20307;&#19981;&#21305;&#37197;&#65289;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#36755;&#20986;&#25991;&#26412;&#30340;&#38271;&#24230;&#22686;&#21152;&#65292;&#38543;&#26426;&#37319;&#26679;&#30340;&#38543;&#24847;&#24615;&#20250;&#21152;&#21095;&#65292;&#23545;&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DKGen&#65292;&#23558;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#21010;&#20998;&#20026;&#36845;&#20195;&#36807;&#31243;&#12290;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;DKGen&#23558;&#36755;&#20837;&#30340;&#26597;&#35810;&#12289;&#20808;&#21069;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#19968;&#37096;&#20998;&#21442;&#32771;&#27573;&#33853;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;&#30701;&#25991;&#26412;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#20808;&#21069;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#26597;&#35810;&#19982;&#20043;&#30340;&#30456;&#20851;&#24615;&#65292;&#21160;&#24577;&#36873;&#25321;&#23376;&#38598;&#65292;&#20174;&#23436;&#25972;&#30340;&#27573;&#33853;&#38598;&#20013;&#22823;&#37327;&#28040;&#38500;&#20102;&#19981;&#30456;&#20851;&#30340;&#21442;&#32771;&#20869;&#23481;&#30340;&#36755;&#20837;&#12290;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;DKGen&#30340;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have revolutionized the way we interact with information, but they often generate nonfactual text, raising concerns about their reliability. Previous methods use external knowledge as references for text generation to enhance factuality but often struggle with the knowledge mix-up(e.g., entity mismatch) of irrelevant references. Besides,as the length of the output text grows, the randomness of sampling can escalate, detrimentally impacting the factual accuracy of the generated text. In this paper, we present DKGen, which divide the text generation process into an iterative process. In each iteration, DKGen takes the input query, the previously generated text and a subset of the reference passages as input to generate short text. During the process, the subset is dynamically selected from the full passage set based on their relevance to the previously generated text and the query, largely eliminating the irrelevant references from input. To further enhance DKGen's 
&lt;/p&gt;</description></item><item><title>Speech Wikimedia&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;77&#31181;&#35821;&#35328;&#30340;&#22823;&#37327;&#38899;&#39057;&#21644;&#36716;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36866;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.15710</link><description>&lt;p&gt;
Speech Wikimedia&#65306;&#19968;&#31181;&#21253;&#25324;77&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#38899;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Speech Wikimedia: A 77 Language Multilingual Speech Dataset. (arXiv:2308.15710v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15710
&lt;/p&gt;
&lt;p&gt;
Speech Wikimedia&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;77&#31181;&#35821;&#35328;&#30340;&#22823;&#37327;&#38899;&#39057;&#21644;&#36716;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36866;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Speech Wikimedia&#25968;&#25454;&#38598;&#26159;&#20174;&#32500;&#22522;&#23186;&#20307;&#20849;&#20139;&#36164;&#28304;&#20013;&#25552;&#21462;&#30340;&#24102;&#26377;&#36716;&#24405;&#30340;&#38899;&#39057;&#30340;&#20844;&#24320;&#21487;&#29992;&#32534;&#35793;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#22330;&#26223;&#21644;&#35828;&#35805;&#32773;&#30340;1780&#23567;&#26102;&#65288;195GB&#65289;&#30340;CC-BY-SA&#35768;&#21487;&#30340;&#36716;&#24405;&#35821;&#38899;&#65292;&#28085;&#30422;&#20102;77&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;&#27599;&#20010;&#38899;&#39057;&#25991;&#20214;&#37117;&#26377;&#19968;&#20010;&#25110;&#22810;&#20010;&#19981;&#21516;&#35821;&#35328;&#30340;&#36716;&#24405;&#65292;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#36866;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Speech Wikimedia Dataset is a publicly available compilation of audio with transcriptions extracted from Wikimedia Commons. It includes 1780 hours (195 GB) of CC-BY-SA licensed transcribed speech from a diverse set of scenarios and speakers, in 77 different languages. Each audio file has one or more transcriptions in different languages, making this dataset suitable for training speech recognition, speech translation, and machine translation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#20114;&#20449;&#24687;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#26041;&#27861;&#21644;&#30456;&#20851;&#23450;&#29702;&#65292;&#25552;&#21319;&#20102;&#20114;&#20449;&#24687;&#20998;&#26512;&#30340;&#20005;&#35880;&#24615;&#19982;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15704</link><description>&lt;p&gt;
&#23545;&#23545;&#27604;&#23398;&#20064;&#20013;&#20114;&#20449;&#24687;&#30340;&#20005;&#26684;&#20998;&#26512;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards a Rigorous Analysis of Mutual Information in Contrastive Learning. (arXiv:2308.15704v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#20114;&#20449;&#24687;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#26041;&#27861;&#21644;&#30456;&#20851;&#23450;&#29702;&#65292;&#25552;&#21319;&#20102;&#20114;&#20449;&#24687;&#20998;&#26512;&#30340;&#20005;&#35880;&#24615;&#19982;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#22522;&#30707;&#12290;&#20854;&#20027;&#35201;&#33539;&#24335;&#28041;&#21450;&#19968;&#20010;&#20114;&#20449;&#24687;&#25439;&#22833;&#30340;&#23454;&#20363;&#21306;&#20998;&#20219;&#21153;&#12290;&#36825;&#31181;&#25439;&#22833;&#34987;&#31216;&#20026;InfoNCE&#65292;&#36890;&#36807;&#20114;&#20449;&#24687;&#20998;&#26512;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#20114;&#20449;&#24687;&#30340;&#20272;&#35745;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23548;&#33268;&#20854;&#25968;&#23398;&#22522;&#30784;&#30340;&#20248;&#38597;&#19982;&#20272;&#35745;&#30340;&#22797;&#26434;&#24615;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#20174;&#20114;&#20449;&#24687;&#20998;&#26512;&#20013;&#24471;&#20986;&#20005;&#26684;&#30340;&#35265;&#35299;&#25110;&#32467;&#35770;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#26041;&#27861;&#21644;&#19968;&#20123;&#30456;&#20851;&#23450;&#29702;&#65292;&#26088;&#22312;&#22686;&#24378;&#20114;&#20449;&#24687;&#20998;&#26512;&#30340;&#20005;&#35880;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#31616;&#21333;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#29992;&#24615;&#12290;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;&#19977;&#20010;&#23545;&#27604;&#23398;&#20064;&#20998;&#26512;&#23454;&#20363;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#20419;&#36827;&#26356;&#28145;&#20837;&#29702;&#35299;&#25110;&#32416;&#27491;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has emerged as a cornerstone in recent achievements of unsupervised representation learning. Its primary paradigm involves an instance discrimination task with a mutual information loss. The loss is known as InfoNCE and it has yielded vital insights into contrastive learning through the lens of mutual information analysis. However, the estimation of mutual information can prove challenging, creating a gap between the elegance of its mathematical foundation and the complexity of its estimation. As a result, drawing rigorous insights or conclusions from mutual information analysis becomes intricate. In this study, we introduce three novel methods and a few related theorems, aimed at enhancing the rigor of mutual information analysis. Despite their simplicity, these methods can carry substantial utility. Leveraging these approaches, we reassess three instances of contrastive learning analysis, illustrating their capacity to facilitate deeper comprehension or to rectif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36807;&#31243;&#23548;&#21521;&#30340;&#36866;&#24403;&#20381;&#36182;&#27010;&#24565;&#65292;&#31216;&#20026;&#25209;&#21028;&#20351;&#29992;&#65292;&#26088;&#22312;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#21033;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#30740;&#31350;&#36890;&#36807;&#22312;&#20799;&#31461;&#34384;&#24453;&#31579;&#26597;&#39046;&#22495;&#36827;&#34892;&#22312;&#32447;&#23454;&#39564;&#65292;&#21457;&#29616;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;&#22521;&#35757;&#21487;&#20197;&#25903;&#25345;&#20154;&#20204;&#30340;&#25209;&#21028;&#20351;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15700</link><description>&lt;p&gt;
&#35757;&#32451;&#26397;&#21521;&#25209;&#21028;&#20351;&#29992;&#65306;&#23398;&#20064;&#23558;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#32622;&#20110;&#20154;&#31867;&#30693;&#35782;&#20043;&#20013;
&lt;/p&gt;
&lt;p&gt;
Training Towards Critical Use: Learning to Situate AI Predictions Relative to Human Knowledge. (arXiv:2308.15700v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36807;&#31243;&#23548;&#21521;&#30340;&#36866;&#24403;&#20381;&#36182;&#27010;&#24565;&#65292;&#31216;&#20026;&#25209;&#21028;&#20351;&#29992;&#65292;&#26088;&#22312;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#21033;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#30740;&#31350;&#36890;&#36807;&#22312;&#20799;&#31461;&#34384;&#24453;&#31579;&#26597;&#39046;&#22495;&#36827;&#34892;&#22312;&#32447;&#23454;&#39564;&#65292;&#21457;&#29616;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;&#22521;&#35757;&#21487;&#20197;&#25903;&#25345;&#20154;&#20204;&#30340;&#25209;&#21028;&#20351;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#25903;&#25345;&#20154;&#20204;&#26356;&#22909;&#22320;&#21033;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#21253;&#25324;&#36890;&#36807;&#22521;&#35757;&#21644;&#20837;&#38376;&#25351;&#23548;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20915;&#31574;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#27599;&#20010;&#20915;&#31574;&#19982;&#29616;&#23454;&#26631;&#31614;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#8220;&#36866;&#24403;&#30340;&#20381;&#36182;&#8221;&#65292;&#36825;&#20123;&#26631;&#31614;&#28165;&#26224;&#22320;&#26144;&#23556;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#30446;&#26631;&#21644;&#20154;&#31867;&#20915;&#31574;&#32773;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#34987;&#37096;&#32626;&#22312;&#31038;&#20250;&#24037;&#20316;&#12289;&#21009;&#20107;&#21496;&#27861;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36807;&#31243;&#23548;&#21521;&#30340;&#36866;&#24403;&#20381;&#36182;&#27010;&#24565;&#65292;&#31216;&#20026;&#25209;&#21028;&#20351;&#29992;&#65292;&#20854;&#23558;&#20154;&#31867;&#33021;&#22815;&#23558;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#32622;&#20110;&#20182;&#20204;&#29420;&#29305;&#30340;&#30693;&#35782;&#20043;&#20013;&#65292;&#32780;&#36825;&#20123;&#30693;&#35782;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26469;&#35828;&#26159;&#19981;&#21487;&#33719;&#24471;&#30340;&#12290;&#20026;&#20102;&#25506;&#32034;&#35757;&#32451;&#22914;&#20309;&#25903;&#25345;&#25209;&#21028;&#20351;&#29992;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22797;&#26434;&#30340;&#31038;&#20250;&#20915;&#31574;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#19968;&#39033;&#38543;&#26426;&#22312;&#32447;&#23454;&#39564;&#65306;&#20799;&#31461;&#34384;&#24453;&#31579;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
A growing body of research has explored how to support humans in making better use of AI-based decision support, including via training and onboarding. Existing research has focused on decision-making tasks where it is possible to evaluate "appropriate reliance" by comparing each decision against a ground truth label that cleanly maps to both the AI's predictive target and the human decision-maker's goals. However, this assumption does not hold in many real-world settings where AI tools are deployed today (e.g., social work, criminal justice, and healthcare). In this paper, we introduce a process-oriented notion of appropriate reliance called critical use that centers the human's ability to situate AI predictions against knowledge that is uniquely available to them but unavailable to the AI model. To explore how training can support critical use, we conduct a randomized online experiment in a complex social decision-making setting: child maltreatment screening. We find that, by providi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#21517;&#20026;CongNaMul&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#21644;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#25552;&#20379;&#20102;&#36136;&#37327;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;5&#20010;&#33469;&#30340;&#29289;&#29702;&#29305;&#24449;&#20379;&#27979;&#37327;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.15690</link><description>&lt;p&gt;
CongNaMul: &#19968;&#31181;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts. (arXiv:2308.15690v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#21517;&#20026;CongNaMul&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#21644;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#25552;&#20379;&#20102;&#36136;&#37327;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;5&#20010;&#33469;&#30340;&#29289;&#29702;&#29305;&#24449;&#20379;&#27979;&#37327;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;CongNaMul&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#22823;&#35910;&#33469;&#22270;&#20687;&#20998;&#26512;&#30340;&#21508;&#31181;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#12290;CongNaMul&#25968;&#25454;&#38598;&#26088;&#22312;&#20419;&#36827;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#20197;&#21450;&#38271;&#24230;&#21644;&#37325;&#37327;&#30340;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#20998;&#31867;&#20219;&#21153;&#25552;&#20379;&#20102;&#22235;&#20010;&#31867;&#21035;&#26469;&#30830;&#23450;&#22823;&#35910;&#33469;&#30340;&#36136;&#37327;&#65306;&#27491;&#24120;&#12289;&#26029;&#35010;&#12289;&#26001;&#28857;&#21644;&#26029;&#35010;&#21644;&#26001;&#28857;&#65292;&#20197;&#24320;&#21457;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30340;&#33258;&#21160;&#36136;&#37327;&#26816;&#27979;&#25216;&#26415;&#12290;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#22270;&#20687;&#65292;&#20174;&#21333;&#20010;&#33469;&#22270;&#20687;&#21040;&#20855;&#26377;&#22810;&#20010;&#33469;&#30340;&#22270;&#20687;&#65292;&#20197;&#21450;&#20154;&#24037;&#26631;&#35760;&#30340;&#25513;&#33180;&#22270;&#20687;&#12290;&#26631;&#31614;&#21253;&#25324;4&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#32972;&#26223;&#12289;&#22836;&#37096;&#12289;&#36523;&#20307;&#21644;&#23614;&#37096;&#12290;&#25968;&#25454;&#38598;&#36824;&#20026;&#22270;&#20687;&#20998;&#35299;&#20219;&#21153;&#25552;&#20379;&#20102;&#22270;&#20687;&#21644;&#25513;&#33180;&#65292;&#21253;&#25324;&#20004;&#20010;&#20998;&#31163;&#30340;&#33469;&#22270;&#20687;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#36824;&#25552;&#20379;&#20102;&#33469;&#30340;5&#20010;&#29289;&#29702;&#29305;&#24449;&#65288;&#22836;&#37096;&#38271;&#24230;&#12289;&#36523;&#20307;&#38271;&#24230;&#12289;&#36523;&#20307;&#21402;&#24230;&#12289;&#23614;&#37096;&#38271;&#24230;&#12289;&#37325;&#37327;&#65289;&#20379;&#22522;&#20110;&#22270;&#20687;&#30340;&#27979;&#37327;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present 'CongNaMul', a comprehensive dataset designed for various tasks in soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate tasks such as image classification, semantic segmentation, decomposition, and measurement of length and weight. The classification task provides four classes to determine the quality of soybean sprouts: normal, broken, spotted, and broken and spotted, for the development of AI-aided automatic quality inspection technology. For semantic segmentation, images with varying complexity, from single sprout images to images with multiple sprouts, along with human-labelled mask images, are included. The label has 4 different classes: background, head, body, tail. The dataset also provides images and masks for the image decomposition task, including two separate sprout images and their combined form. Lastly, 5 physical features of sprouts (head length, body length, body thickness, tail length, weight) are provided for image-based measurement
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#21521;&#20154;&#31867;&#25552;&#38382;&#26469;&#20998;&#26512;&#24182;&#25910;&#38598;&#32570;&#22833;&#20449;&#24687;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#29983;&#25104;&#31934;&#30830;&#26426;&#22120;&#20154;&#25351;&#20196;&#30340;&#35774;&#35745;&#25104;&#26412;&#12290;&#25581;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.15684</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#19982;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#21644;&#20027;&#21160;&#25552;&#38382;
&lt;/p&gt;
&lt;p&gt;
Interactively Robot Action Planning with Uncertainty Analysis and Active Questioning by Large Language Model. (arXiv:2308.15684v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#21521;&#20154;&#31867;&#25552;&#38382;&#26469;&#20998;&#26512;&#24182;&#25910;&#38598;&#32570;&#22833;&#20449;&#24687;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#29983;&#25104;&#31934;&#30830;&#26426;&#22120;&#20154;&#25351;&#20196;&#30340;&#35774;&#35745;&#25104;&#26412;&#12290;&#25581;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#24050;&#32463;&#34987;&#31215;&#26497;&#30740;&#31350;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32473;&#20986;&#30340;LLM&#25351;&#20196;&#21487;&#33021;&#23384;&#22312;&#27495;&#20041;&#21644;&#32570;&#20047;&#20449;&#24687;&#65292;&#36825;&#21462;&#20915;&#20110;&#20219;&#21153;&#29615;&#22659;&#12290;&#21487;&#20197;&#36890;&#36807;&#20351;&#25351;&#20196;&#36755;&#20837;&#26356;&#35814;&#32454;&#26469;&#35843;&#25972;LLM&#30340;&#36755;&#20986;&#65307;&#28982;&#32780;&#65292;&#35774;&#35745;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#20801;&#35768;LLM&#36890;&#36807;&#21521;&#20154;&#31867;&#25552;&#38382;&#26469;&#20998;&#26512;&#24182;&#25910;&#38598;&#32570;&#22833;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#29983;&#25104;&#31934;&#30830;&#26426;&#22120;&#20154;&#25351;&#20196;&#30340;&#35774;&#35745;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#28921;&#39274;&#20219;&#21153;&#30340;&#20855;&#20307;&#31034;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#20063;&#25581;&#31034;&#20102;LLM&#22312;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#25552;&#20986;&#19981;&#37325;&#35201;&#30340;&#38382;&#39064;&#21644;&#22312;&#19981;&#35810;&#38382;&#30340;&#24773;&#20917;&#19979;&#20551;&#35774;&#20851;&#38190;&#20449;&#24687;&#12290;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#38416;&#26126;&#20026;&#26410;&#26469;&#21033;&#29992;LLM&#36827;&#34892;&#26426;&#22120;&#20154;&#25216;&#26415;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of the Large Language Model (LLM) to robot action planning has been actively studied. The instructions given to the LLM by natural language may include ambiguity and lack of information depending on the task context. It is possible to adjust the output of LLM by making the instruction input more detailed; however, the design cost is high. In this paper, we propose the interactive robot action planning method that allows the LLM to analyze and gather missing information by asking questions to humans. The method can minimize the design cost of generating precise robot instructions. We demonstrated the effectiveness of our method through concrete examples in cooking tasks. However, our experiments also revealed challenges in robot action planning with LLM, such as asking unimportant questions and assuming crucial information without asking. Shedding light on these issues provides valuable insights for future research on utilizing LLM for robotics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EchoCLIP&#30340;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35299;&#35835;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#37327;&#30340;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#21644;&#19987;&#23478;&#35299;&#35835;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#24182;&#22312;&#24515;&#33039;&#21151;&#33021;&#35780;&#20272;&#21644;&#26893;&#20837;&#24515;&#33039;&#20869;&#22120;&#20214;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15670</link><description>&lt;p&gt;
&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35299;&#35835;&#30340;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multimodal Foundation Models For Echocardiogram Interpretation. (arXiv:2308.15670v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15670
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EchoCLIP&#30340;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35299;&#35835;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#37327;&#30340;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#21644;&#19987;&#23478;&#35299;&#35835;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#24182;&#22312;&#24515;&#33039;&#21151;&#33021;&#35780;&#20272;&#21644;&#26893;&#20837;&#24515;&#33039;&#20869;&#22120;&#20214;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#28145;&#24230;&#23398;&#20064;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#21307;&#23398;&#25104;&#20687;&#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#35821;&#35328;&#27010;&#24565;&#21453;&#26144;&#20102;&#35786;&#26029;&#22270;&#20687;&#35299;&#37322;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#28982;&#32780;&#24403;&#21069;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35757;&#32451;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#24182;&#32771;&#34385;&#21040;&#24515;&#33039;&#29983;&#29702;&#30340;&#33539;&#22260;&#65292;&#25105;&#20204;&#21033;&#29992;1,032,975&#20010;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#19987;&#23478;&#35299;&#35835;&#24320;&#21457;&#20102;EchoCLIP&#65292;&#19968;&#20010;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#30340;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#12290;EchoCLIP&#22312;&#24515;&#33039;&#21151;&#33021;&#35780;&#20272;&#65288;&#22806;&#37096;&#39564;&#35777;&#24038;&#23460;&#23556;&#34880;&#20998;&#25968;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;7.1%&#65289;&#21644;&#26893;&#20837;&#24515;&#33039;&#20869;&#22120;&#20214;&#30340;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#65288;&#26410;&#32463;&#36807;&#26174;&#24335;&#35757;&#32451;&#65289;&#24615;&#33021;&#65288;&#36215;&#25615;&#22120;&#21644;&#20154;&#24037;&#24515;&#33039;&#29923;&#33180;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#22312;0.84&#33267;0.98&#20043;&#38388;&#65289;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#38271;&#19978;&#19979;&#25991;&#21487;&#21464;&#27169;&#22411;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal deep learning foundation models can learn the relationship between images and text. In the context of medical imaging, mapping images to language concepts reflects the clinical task of diagnostic image interpretation, however current general-purpose foundation models do not perform well in this context because their training corpus have limited medical text and images. To address this challenge and account for the range of cardiac physiology, we leverage 1,032,975 cardiac ultrasound videos and corresponding expert interpretations to develop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP displays strong zero-shot (not explicitly trained) performance in cardiac function assessment (external validation left ventricular ejection fraction mean absolute error (MAE) of 7.1%) and identification of implanted intracardiac devices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and artificial heart valves). We also developed a long-context vari
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#33021;&#37327;&#20256;&#25773;&#22120;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#19978;&#20026;&#30005;&#21160;&#36710;&#20805;&#30005;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#26399;&#38388;&#36710;&#36742;&#25490;&#38431;&#23548;&#33268;&#30340;&#34892;&#36710;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20339;&#26102;&#38388;&#21644;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.15656</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#33021;&#37327;&#20256;&#25773;&#22120;&#35843;&#24230;&#26694;&#26550;&#29992;&#20110;&#22312;&#36947;&#36335;&#19978;&#20026;&#30005;&#21160;&#36710;&#20805;&#30005;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Based Framework for Mobile Energy Disseminator Dispatching to Charge On-the-Road Electric Vehicles. (arXiv:2308.15656v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#33021;&#37327;&#20256;&#25773;&#22120;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#19978;&#20026;&#30005;&#21160;&#36710;&#20805;&#30005;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#26399;&#38388;&#36710;&#36742;&#25490;&#38431;&#23548;&#33268;&#30340;&#34892;&#36710;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20339;&#26102;&#38388;&#21644;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#36710;&#30340;&#24555;&#36895;&#22686;&#38271;&#32473;&#20445;&#25252;&#30005;&#27744;&#20581;&#24247;&#21644;&#35299;&#20915;&#36710;&#36742;&#32493;&#33322;&#28966;&#34385;&#38382;&#39064;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#31227;&#21160;&#33021;&#37327;&#20256;&#25773;&#22120;&#65288;MEDs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;MED&#23433;&#35013;&#22312;&#22823;&#22411;&#36710;&#36742;&#21518;&#26041;&#65292;&#24182;&#22312;&#23427;&#30340;&#19978;&#28216;&#21322;&#24452;&#33539;&#22260;&#20869;&#20026;&#25152;&#26377;&#21442;&#19982;&#30340;&#30005;&#21160;&#36710;&#20805;&#30005;&#12290;&#28982;&#32780;&#65292;V2V&#20805;&#30005;&#26399;&#38388;&#65292;MED&#21644;&#30005;&#21160;&#36710;&#24847;&#22806;&#22320;&#24418;&#25104;&#36710;&#38431;&#65292;&#20174;&#32780;&#21344;&#25454;&#20102;&#22810;&#26465;&#36710;&#36947;&#65292;&#24182;&#20005;&#37325;&#24433;&#21709;&#20102;&#25972;&#20010;&#36890;&#36947;&#30340;&#34892;&#36710;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26377;&#38480;&#30340;MED&#37197;&#32622;&#39044;&#31639;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#35843;&#24230;&#31574;&#30053;&#65292;&#20197;&#30830;&#23450;&#22312;&#20132;&#36890;&#20013;&#24341;&#20837;MED&#30340;&#26368;&#20339;&#26102;&#38388;&#21644;&#20301;&#32622;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#19968;&#20010;&#36710;&#36742;&#35843;&#24230;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of electric vehicles (EVs) presents novel challenges in preserving battery health and in addressing the persistent problem of vehicle range anxiety. To address these concerns, wireless charging, particularly, Mobile Energy Disseminators (MEDs) have emerged as a promising solution. The MED is mounted behind a large vehicle and charges all participating EVs within a radius upstream of it. Unfortuantely, during such V2V charging, the MED and EVs inadvertently form platoons, thereby occupying multiple lanes and impairing overall corridor travel efficiency. In addition, constrained budgets for MED deployment necessitate the development of an effective dispatching strategy to determine optimal timing and locations for introducing the MEDs into traffic. This paper proposes a deep reinforcement learning (DRL) based methodology to develop a vehicle dispatching framework. In the first component of the framework, we develop a realistic reinforcement learning environment ter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;AutoML&#31995;&#32479;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#36890;&#36807;&#21465;&#20107;&#24615;&#22238;&#39038;&#20027;&#35201;&#26041;&#27861;&#65292;&#23558;&#22522;&#26412;&#27010;&#24565;&#25552;&#28860;&#20986;&#26469;&#20197;&#25903;&#25345;&#22312;&#21333;&#19968;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.15647</link><description>&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#30340;&#19968;&#33324;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A General Recipe for Automated Machine Learning in Practice. (arXiv:2308.15647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;AutoML&#31995;&#32479;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#36890;&#36807;&#21465;&#20107;&#24615;&#22238;&#39038;&#20027;&#35201;&#26041;&#27861;&#65292;&#23558;&#22522;&#26412;&#27010;&#24565;&#25552;&#28860;&#20986;&#26469;&#20197;&#25903;&#25345;&#22312;&#21333;&#19968;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26159;&#19968;&#39033;&#30740;&#31350;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#24320;&#21457;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#33021;&#22815;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#22320;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24819;&#27861;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#36341;&#25552;&#20379;&#20102;&#24040;&#22823;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#22914;&#20309;&#35774;&#35745;AutoML&#31995;&#32479;&#30340;&#20449;&#24687;&#38750;&#24120;&#26377;&#38480;&#12290;&#22823;&#37096;&#20998;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#20248;&#21270;&#31639;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#32780;&#24573;&#30053;&#20102;&#23454;&#38469;&#24212;&#29992;&#30340;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;AutoML&#31995;&#32479;&#30340;&#21442;&#32771;&#26694;&#26550;&#12290;&#36890;&#36807;&#23545;&#35813;&#39046;&#22495;&#20027;&#35201;&#26041;&#27861;&#30340;&#21465;&#20107;&#24615;&#22238;&#39038;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#25552;&#28860;&#22522;&#26412;&#27010;&#24565;&#65292;&#20197;&#25903;&#25345;&#23427;&#20204;&#22312;&#21333;&#19968;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#19982;AutoML&#24212;&#29992;&#30456;&#20851;&#30340;&#26410;&#26469;&#30740;&#31350;&#20013;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning (AutoML) is an area of research that focuses on developing methods to generate machine learning models automatically. The idea of being able to build machine learning models with very little human intervention represents a great opportunity for the practice of applied machine learning. However, there is very little information on how to design an AutoML system in practice. Most of the research focuses on the problems facing optimization algorithms and leaves out the details of how that would be done in practice. In this paper, we propose a frame of reference for building general AutoML systems. Through a narrative review of the main approaches in the area, our main idea is to distill the fundamental concepts in order to support them in a single design. Finally, we discuss some open problems related to the application of AutoML for future research.
&lt;/p&gt;</description></item><item><title>AskIt&#26159;&#19968;&#20010;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#36890;&#36807;&#31616;&#21270;LLM&#30340;&#38598;&#25104;&#21644;&#25552;&#20379;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#35299;&#20915;&#20102;LLM&#23884;&#20837;&#24212;&#29992;&#31243;&#24207;&#21644;&#20195;&#30721;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15645</link><description>&lt;p&gt;
AskIt: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#32534;&#31243;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
AskIt: Unified Programming Interface for Programming with Large Language Models. (arXiv:2308.15645v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15645
&lt;/p&gt;
&lt;p&gt;
AskIt&#26159;&#19968;&#20010;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#36890;&#36807;&#31616;&#21270;LLM&#30340;&#38598;&#25104;&#21644;&#25552;&#20379;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#35299;&#20915;&#20102;LLM&#23884;&#20837;&#24212;&#29992;&#31243;&#24207;&#21644;&#20195;&#30721;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#23637;&#31034;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#29616;&#35937;&#65292;&#21363; emergent abilities&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#29087;&#32451;&#33021;&#21147;&#65292;&#20174;&#25991;&#26412;&#25688;&#35201;&#21040;&#20195;&#30721;&#29983;&#25104;&#12290;&#34429;&#28982;&#36825;&#20123;&#33021;&#21147;&#22312;&#36719;&#20214;&#35774;&#35745;&#21644;&#24320;&#21457;&#26041;&#38754;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#20294;&#23427;&#20204;&#30340;&#25972;&#21512;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#24320;&#21457;&#32773;&#38754;&#20020;&#30528;&#23558;LLMs&#30452;&#25509;&#23884;&#20837;&#24212;&#29992;&#31243;&#24207;&#25110;&#23558;&#20854;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#20013;&#25552;&#21462;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;AskIt&#65292;&#19968;&#31181;&#19987;&#20026;LLMs&#35774;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328; (DSL)&#12290;AskIt&#31616;&#21270;&#20102;LLM&#30340;&#38598;&#25104;&#65292;&#25552;&#20379;&#20102;&#31867;&#22411;&#25351;&#23548;&#30340;&#36755;&#20986;&#25511;&#21046;&#65292;&#22522;&#20110;&#27169;&#26495;&#30340;&#20989;&#25968;&#23450;&#20041;&#20197;&#21450;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#20943;&#23569;&#20102;LLM&#30340;&#22522;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving landscape of software development, Large Language Models (LLMs) exhibit a unique phenomenon known as emergent abilities, demonstrating adeptness across numerous tasks, from text summarization to code generation. While these abilities open up novel avenues in software design and crafting, their incorporation presents substantial challenges. Developers grapple with decisions surrounding the direct embedding of LLMs within applications versus employing them for code generation. Moreover, effective prompt design becomes a critical concern, given the necessity of data extraction from natural language outputs. To address these intricacies, this paper introduces AskIt, a domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies LLM integration, offering type-guided output control, template-based function definitions, and a unified interface that diminishes the distinction between LLM-based code generation and application integration. Furthermore, through 
&lt;/p&gt;</description></item><item><title>&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#21452;&#26354;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#23884;&#20837;&#65292;&#20855;&#26377;&#26356;&#24378;&#22823;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#29305;&#24615;&#12290;&#27973;&#23618;&#23884;&#20837;&#26500;&#24314;&#20102;&#23618;&#27425;&#21270;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.15639</link><description>&lt;p&gt;
&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Convolutional Neural Networks. (arXiv:2308.15639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15639
&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#21452;&#26354;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#23884;&#20837;&#65292;&#20855;&#26377;&#26356;&#24378;&#22823;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#29305;&#24615;&#12290;&#27973;&#23618;&#23884;&#20837;&#26500;&#24314;&#20102;&#23618;&#27425;&#21270;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#28145;&#24230;&#23398;&#20064;&#20027;&#35201;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20852;&#36259;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#22312;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23613;&#31649;&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#23884;&#20837;&#31354;&#38388;&#19978;&#27809;&#26377;&#35774;&#23450;&#24402;&#32435;&#20559;&#32622;&#65292;&#21487;&#20197;&#35828;&#30456;&#24403;&#24188;&#31258;&#12290;&#21478;&#19968;&#31181;&#31867;&#22411;&#30340;&#21367;&#31215;&#32593;&#32476; - &#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20063;&#23384;&#22312;&#31867;&#20284;&#30340;&#32570;&#38519;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#26469;&#23884;&#20837;&#25968;&#25454;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#24378;&#22823;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#20363;&#23376;&#23601;&#26159;&#21452;&#26354;&#31354;&#38388;&#12290;&#30001;&#20110;&#33021;&#22815;&#23558;&#26356;&#22810;&#30340;&#25968;&#25454;&#36866;&#24212;&#20110;&#20302;&#32500;&#31354;&#38388;&#24182;&#20855;&#26377;&#26641;&#29366;&#29305;&#24615;&#65292;&#21452;&#26354;&#31354;&#38388;&#23588;&#20854;&#26377;&#29992;&#12290;&#20808;&#21069;&#30340;&#22810;&#31687;&#35770;&#25991;&#24050;&#32463;&#34920;&#26126;&#65292;&#36825;&#20123;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#26377;&#21161;&#20110;&#20351;&#29992;&#27973;&#23618;&#23884;&#20837;&#26469;&#26500;&#24314;&#23618;&#27425;&#21270;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning is mostly responsible for the surge of interest in Artificial Intelligence in the last decade. So far, deep learning researchers have been particularly successful in the domain of image processing, where Convolutional Neural Networks are used. Although excelling at image classification, Convolutional Neural Networks are quite naive in that no inductive bias is set on the embedding space for images. Similar flaws are also exhibited by another type of Convolutional Networks - Graph Convolutional Neural Networks. However, using non-Euclidean space for embedding data might result in more robust and explainable models. One example of such a non-Euclidean space is hyperbolic space. Hyperbolic spaces are particularly useful due to their ability to fit more data in a low-dimensional space and tree-likeliness properties. These attractive properties have been previously used in multiple papers which indicated that they are beneficial for building hierarchical embeddings using shall
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#26234;&#33021;&#31995;&#32479;&#26469;&#35780;&#20272;&#22823;&#23398;&#29983;&#30340;&#20010;&#24615;&#21457;&#23637;&#21644;&#23601;&#19994;&#20934;&#22791;&#24773;&#20917;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#8220;&#24179;&#34913;&#36718;&#8221;&#29702;&#35770;&#30340;&#35843;&#26597;&#38382;&#21367;&#26469;&#25910;&#38598;&#23398;&#29983;&#30340;&#24773;&#24863;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27169;&#31946;&#38598;&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#35780;&#20272;&#27605;&#19994;&#29983;&#23545;&#26410;&#26469;&#32844;&#19994;&#30340;&#20934;&#22791;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.15620</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#23398;&#29983;&#20010;&#24615;&#21457;&#23637;&#21644;&#23601;&#19994;&#20934;&#22791;&#30340;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Intelligent System for Assessing University Student Personality Development and Career Readiness. (arXiv:2308.15620v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#26234;&#33021;&#31995;&#32479;&#26469;&#35780;&#20272;&#22823;&#23398;&#29983;&#30340;&#20010;&#24615;&#21457;&#23637;&#21644;&#23601;&#19994;&#20934;&#22791;&#24773;&#20917;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#8220;&#24179;&#34913;&#36718;&#8221;&#29702;&#35770;&#30340;&#35843;&#26597;&#38382;&#21367;&#26469;&#25910;&#38598;&#23398;&#29983;&#30340;&#24773;&#24863;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27169;&#31946;&#38598;&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#35780;&#20272;&#27605;&#19994;&#29983;&#23545;&#26410;&#26469;&#32844;&#19994;&#30340;&#20934;&#22791;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24120;&#24120;&#20351;&#29992;&#25104;&#32489;&#21333;&#21644;&#32489;&#28857;&#31561;&#23398;&#26415;&#25351;&#26631;&#26469;&#35780;&#20272;&#23398;&#29983;&#30340;&#30693;&#35782;&#33719;&#24471;&#24773;&#20917;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#30340;&#25351;&#26631;&#26469;&#34913;&#37327;&#20182;&#20204;&#23545;&#27605;&#19994;&#21518;&#29983;&#27963;&#25361;&#25112;&#30340;&#20934;&#22791;&#31243;&#24230;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#22823;&#23398;&#29983;&#23545;&#21464;&#21270;&#21644;&#36807;&#28193;&#30340;&#20934;&#22791;&#31243;&#24230;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20182;&#20204;&#23545;&#32844;&#19994;&#30340;&#20934;&#22791;&#31243;&#24230;&#12290;&#35813;&#30740;&#31350;&#37319;&#29992;&#30340;&#26041;&#27861;&#28041;&#21450;&#35774;&#35745;&#19968;&#20221;&#22522;&#20110;Paul J. Mayer&#30340;&#8220;&#24179;&#34913;&#36718;&#8221;&#29702;&#35770;&#30340;&#35843;&#26597;&#38382;&#21367;&#65292;&#20197;&#25429;&#25417;&#23398;&#29983;&#22312;&#19981;&#21516;&#29983;&#27963;&#26041;&#38754;&#30340;&#24773;&#24863;&#65292;&#21253;&#25324;&#23545;&#25945;&#32946;&#36807;&#31243;&#30340;&#28385;&#24847;&#24230;&#21644;&#23545;&#34218;&#36164;&#30340;&#26399;&#26395;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;KBTU&#23398;&#29983;&#35843;&#26597;&#30340;&#25910;&#38598;&#25968;&#25454;&#65288;n=47&#65289;&#36827;&#34892;&#32447;&#24615;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22788;&#29702;&#65292;&#38543;&#21518;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#21644;&#27169;&#31946;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#26234;&#33021;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#35780;&#20272;&#27605;&#19994;&#29983;&#23545;&#26410;&#26469;&#32844;&#19994;&#30340;&#20934;&#22791;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
While academic metrics such as transcripts and GPA are commonly used to evaluate students' knowledge acquisition, there is a lack of comprehensive metrics to measure their preparedness for the challenges of post-graduation life. This research paper explores the impact of various factors on university students' readiness for change and transition, with a focus on their preparedness for careers. The methodology employed in this study involves designing a survey based on Paul J. Mayer's "The Balance Wheel" to capture students' sentiments on various life aspects, including satisfaction with the educational process and expectations of salary. The collected data from a KBTU student survey (n=47) were processed through machine learning models: Linear Regression, Support Vector Regression (SVR), Random Forest Regression. Subsequently, an intelligent system was built using these models and fuzzy sets. The system is capable of evaluating graduates' readiness for their future careers and demonstr
&lt;/p&gt;</description></item><item><title>InstaTune&#26159;&#19968;&#31181;&#22312;&#24494;&#35843;&#38454;&#27573;&#21363;&#26102;&#29983;&#25104;&#36229;&#32423;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.15609</link><description>&lt;p&gt;
InstaTune: &#22312;&#24494;&#35843;&#26399;&#38388;&#21363;&#26102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning. (arXiv:2308.15609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15609
&lt;/p&gt;
&lt;p&gt;
InstaTune&#26159;&#19968;&#31181;&#22312;&#24494;&#35843;&#38454;&#27573;&#21363;&#26102;&#29983;&#25104;&#36229;&#32423;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27425;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20026;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#35757;&#32451;&#30828;&#20214;&#26080;&#20851;&#30340;&#36229;&#32423;&#32593;&#32476;&#12290;&#28982;&#21518;&#20174;&#35757;&#32451;&#22909;&#30340;&#36229;&#32423;&#32593;&#32476;&#20013;&#25552;&#21462;&#20986;&#36866;&#29992;&#20110;&#19981;&#21516;&#30828;&#20214;&#24179;&#21488;&#30340;&#26368;&#20339;&#23376;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#36229;&#32423;&#32593;&#32476;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#19988;&#35745;&#31639;&#23494;&#38598;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#23610;&#23544;&#36739;&#22823;&#65292;&#26497;&#22823;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#30828;&#20214;&#24179;&#21488;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;InstaTune&#65292;&#19968;&#31181;&#21033;&#29992;&#21363;&#26102;&#24494;&#35843;&#38454;&#27573;&#29983;&#25104;&#36229;&#32423;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;InstaTune&#20855;&#26377;&#22810;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#35813;&#36807;&#31243;&#21457;&#29983;&#22312;&#24494;&#35843;&#26399;&#38388;&#65292;&#23427;&#21487;&#20197;&#26368;&#23567;&#21270;&#36827;&#34892;NAS&#25152;&#38656;&#30340;&#24635;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20854;&#27425;&#65292;&#25552;&#21462;&#20986;&#30340;&#23376;&#32593;&#32476;&#38024;&#23545;&#30446;&#26631;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
One-Shot Neural Architecture Search (NAS) algorithms often rely on training a hardware agnostic super-network for a domain specific task. Optimal sub-networks are then extracted from the trained super-network for different hardware platforms. However, training super-networks from scratch can be extremely time consuming and compute intensive especially for large models that rely on a two-stage training process of pre-training and fine-tuning. State of the art pre-trained models are available for a wide range of tasks, but their large sizes significantly limits their applicability on various hardware platforms. We propose InstaTune, a method that leverages off-the-shelf pre-trained weights for large models and generates a super-network during the fine-tuning stage. InstaTune has multiple benefits. Firstly, since the process happens during fine-tuning, it minimizes the overall time and compute resources required for NAS. Secondly, the sub-networks extracted are optimized for the target ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#26368;&#22823;&#20844;&#32422;&#25968;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26126;&#30830;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.15594</link><description>&lt;p&gt;
&#21464;&#24418;&#37329;&#21018;&#26159;&#21542;&#33021;&#23398;&#20250;&#26368;&#22823;&#20844;&#32422;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can transformers learn the greatest common divisor?. (arXiv:2308.15594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#26368;&#22823;&#20844;&#32422;&#25968;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26126;&#30830;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#20004;&#20010;&#27491;&#25972;&#25968;&#30340;&#26368;&#22823;&#20844;&#32422;&#25968;&#65288;GCD&#65289;&#30340;&#33021;&#21147;&#12290;&#24403;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#20180;&#32454;&#36873;&#25321;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;98%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#27491;&#30830;&#39044;&#27979;&#21069;100&#20010;GCD&#20013;&#30340;91&#20010;&#12290;&#27169;&#22411;&#30340;&#39044;&#27979;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#24182;&#19988;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#23398;&#20250;&#23558;&#20855;&#26377;&#30456;&#21516;GCD&#30340;&#36755;&#20837;&#23545;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20854;&#38500;&#25968;&#36827;&#34892;&#20998;&#31867;&#12290;&#22522;&#26412;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#22522;&#25968;&#32534;&#30721;&#30340;&#22343;&#21248;&#25805;&#20316;&#25968;&#20165;&#35745;&#31639;&#23569;&#25968;GCD&#65288;&#26368;&#22810;100&#20010;&#20013;&#30340;38&#20010;&#65289;&#65306;&#22522;&#25968;&#30340;&#38500;&#25968;&#20056;&#31215;&#12290;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22823;&#30340;&#22522;&#25968;&#20801;&#35768;&#19968;&#20123;&#27169;&#22411;&#8220;&#20102;&#35299;&#8221;&#23567;&#30340;&#32032;&#25968;GCD&#12290;&#20351;&#29992;&#23545;&#25968;&#22343;&#21248;&#25805;&#20316;&#25968;&#36827;&#34892;&#35757;&#32451;&#23558;&#24615;&#33021;&#25552;&#21319;&#21040;&#27491;&#30830;&#30340;73&#20010;GCD&#65292;&#24182;&#36890;&#36807;&#20174;&#20498;&#25968;&#24179;&#26041;&#21040;&#23545;&#25968;&#22343;&#21248;&#30340;GCD&#35757;&#32451;&#20998;&#24067;&#30340;&#24179;&#34913;&#65292;&#20351;&#24615;&#33021;&#36798;&#21040;91&#20010;GCD&#12290;&#20174;GCD&#30340;&#22343;&#21248;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#27169;&#22411;&#30772;&#22351;&#20102;&#30830;&#23450;&#24615;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
I investigate the capability of small transformers to compute the greatest common divisor (GCD) of two positive integers. When the training distribution and the representation base are carefully chosen, models achieve 98% accuracy and correctly predict 91 of the 100 first GCD. Model predictions are deterministic and fully interpretable. During training, the models learn to cluster input pairs with the same GCD, and classify them by their divisors. Basic models, trained from uniform operands encoded on small bases, only compute a handful of GCD (up to 38 out of 100): the products of divisors of the base. Longer training and larger bases allow some models to "grok" small prime GCD. Training from log-uniform operands boosts performance to 73 correct GCD, and balancing the training distribution of GCD, from inverse square to log-uniform, to 91 GCD. Training models from a uniform distribution of GCD breaks the deterministic model behavior.
&lt;/p&gt;</description></item><item><title>&#36807;&#24230;&#21387;&#32553;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#38480;&#21046;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20449;&#24687;&#20256;&#36882;&#65292;&#24433;&#21709;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.15568</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Over-Squashing in Graph Neural Networks: A Comprehensive survey. (arXiv:2308.15568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15568
&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21387;&#32553;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#38480;&#21046;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20449;&#24687;&#20256;&#36882;&#65292;&#24433;&#21709;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#33539; Paradigm&#65292;&#20026;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;GNN&#30340;&#22522;&#26412;&#26550;&#26500;&#28041;&#21450;&#36890;&#36807;&#28040;&#24687;&#32858;&#21512;&#21644;&#36716;&#25442;&#22312;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#30340;&#26426;&#21046;&#65292;&#22312;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#28508;&#22312;&#23454;&#21147;&#36935;&#21040;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22312;&#26576;&#20123;&#24773;&#22659;&#20013;&#65292;&#20934;&#30830;&#30340;&#39044;&#27979;&#19981;&#20165;&#21462;&#20915;&#20110;&#33410;&#28857;&#30340;&#21363;&#26102;&#23616;&#37096;&#29615;&#22659;&#65292;&#36824;&#21462;&#20915;&#20110;&#36328;&#36234;&#24191;&#22495;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23545;&#38271;&#31243;&#20449;&#24687;&#20256;&#25773;&#30340;&#38656;&#27714;&#26292;&#38706;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#8220;&#36807;&#24230;&#21387;&#32553;&#8221;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#20854;&#20013;&#26469;&#33258;&#36828;&#31163;&#33410;&#28857;&#30340;&#20449;&#24687;&#27969;&#30340;&#21487;&#38752;&#24615;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a revolutionary paradigm in the realm of machine learning, offering a transformative approach to dissect intricate relationships inherent in graph-structured data. The foundational architecture of most GNNs involves the dissemination of information through message aggregation and transformation among interconnected nodes, a mechanism that has demonstrated remarkable efficacy across diverse applications encompassing node classification, link prediction, and recommendation systems. Nonetheless, their potential prowess encounters a restraint intrinsic to scenarios necessitating extensive contextual insights. In certain contexts, accurate predictions hinge not only upon a node's immediate local surroundings but also on interactions spanning far-reaching domains. This intricate demand for long-range information dissemination exposes a pivotal challenge recognized as "over-squashing," wherein the fidelity of information flow from distant nodes bec
&lt;/p&gt;</description></item><item><title>WeatherBench 2&#26356;&#26032;&#20102;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#21152;&#36895;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#24314;&#27169;&#30340;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#24320;&#28304;&#35780;&#20272;&#26694;&#26550;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#35780;&#20272;&#35774;&#32622;&#20013;&#30340;&#27880;&#24847;&#20107;&#39033;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15560</link><description>&lt;p&gt;
WeatherBench 2&#65306;&#19979;&#19968;&#20195;&#25968;&#25454;&#39537;&#21160;&#20840;&#29699;&#22825;&#27668;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
WeatherBench 2: A benchmark for the next generation of data-driven global weather models. (arXiv:2308.15560v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15560
&lt;/p&gt;
&lt;p&gt;
WeatherBench 2&#26356;&#26032;&#20102;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#21152;&#36895;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#24314;&#27169;&#30340;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#24320;&#28304;&#35780;&#20272;&#26694;&#26550;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#35780;&#20272;&#35774;&#32622;&#20013;&#30340;&#27880;&#24847;&#20107;&#39033;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WeatherBench 2&#26159;&#23545;Rasp&#31561;&#20154;&#65288;2020&#65289;&#25552;&#20986;&#30340;&#20840;&#29699;&#20013;&#26399;&#65288;1-14&#22825;&#65289;&#22825;&#27668;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#30340;&#26356;&#26032;&#65292;&#26088;&#22312;&#21152;&#36895;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#24314;&#27169;&#30340;&#36827;&#23637;&#12290;WeatherBench 2&#21253;&#25324;&#19968;&#20010;&#24320;&#28304;&#35780;&#20272;&#26694;&#26550;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#35757;&#32451;&#12289;&#22522;&#20934;&#25968;&#25454;&#21644;&#22522;&#32447;&#25968;&#25454;&#65292;&#20197;&#21450;&#19968;&#20010;&#25345;&#32493;&#26356;&#26032;&#30340;&#32593;&#31449;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#26032;&#30340;&#25351;&#26631;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65306;https://sites.research.google/weatherbench&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#35780;&#20272;&#26694;&#26550;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#24182;&#25552;&#20379;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29289;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#25351;&#26631;&#22522;&#20110;&#39046;&#20808;&#25805;&#20316;&#24615;&#22825;&#27668;&#20013;&#24515;&#35780;&#20272;&#22825;&#27668;&#39044;&#25253;&#30340;&#23454;&#36341;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#32452;&#20027;&#35201;&#24471;&#20998;&#65292;&#20197;&#25552;&#20379;&#27169;&#22411;&#24615;&#33021;&#30340;&#27010;&#35272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#35780;&#20272;&#35774;&#32622;&#20013;&#30340;&#27880;&#24847;&#20107;&#39033;&#21644;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#27979;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
WeatherBench 2 is an update to the global, medium-range (1-14 day) weather forecasting benchmark proposed by Rasp et al. (2020), designed with the aim to accelerate progress in data-driven weather modeling. WeatherBench 2 consists of an open-source evaluation framework, publicly available training, ground truth and baseline data as well as a continuously updated website with the latest metrics and state-of-the-art models: https://sites.research.google/weatherbench. This paper describes the design principles of the evaluation framework and presents results for current state-of-the-art physical and data-driven weather models. The metrics are based on established practices for evaluating weather forecasts at leading operational weather centers. We define a set of headline scores to provide an overview of model performance. In addition, we also discuss caveats in the current evaluation setup and challenges for the future of data-driven weather forecasting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24335;&#39118;&#26684;&#36716;&#31227;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#28151;&#28102;&#29305;&#24449;&#30340;&#36807;&#25311;&#21512;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#20102;&#29983;&#25104;&#22120;&#21644;&#31574;&#30053;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;-&#26368;&#23567;&#21338;&#24328;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;&#21487;&#20197;&#27867;&#21270;&#21040;&#26410;&#35265;&#29615;&#22659;&#30340;&#40065;&#26834;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#30456;&#27604;&#20110;&#20854;&#20182;&#22522;&#20934;&#31639;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15550</link><description>&lt;p&gt;
&#23545;&#25239;&#24335;&#39118;&#26684;&#36716;&#31227;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning. (arXiv:2308.15550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24335;&#39118;&#26684;&#36716;&#31227;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#28151;&#28102;&#29305;&#24449;&#30340;&#36807;&#25311;&#21512;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#20102;&#29983;&#25104;&#22120;&#21644;&#31574;&#30053;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;-&#26368;&#23567;&#21338;&#24328;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;&#21487;&#20197;&#27867;&#21270;&#21040;&#26410;&#35265;&#29615;&#22659;&#30340;&#40065;&#26834;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#30456;&#27604;&#20110;&#20854;&#20182;&#22522;&#20934;&#31639;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#28040;&#38500;&#23545;&#28151;&#28102;&#29305;&#24449;&#36807;&#25311;&#21512;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20102;&#19968;&#20010;&#26368;&#22823;-&#26368;&#23567;&#21338;&#24328;&#29702;&#35770;&#30340;&#30446;&#26631;&#12290;&#29983;&#25104;&#22120;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#36716;&#31227;&#35266;&#23519;&#26679;&#24335;&#12290;&#29983;&#25104;&#22120;&#30340;&#21478;&#19968;&#20010;&#30446;&#26631;&#26159;&#25200;&#21160;&#35266;&#23519;&#65292;&#20197;&#26368;&#22823;&#21270;&#26234;&#33021;&#20307;&#37319;&#21462;&#19981;&#21516;&#34892;&#21160;&#30340;&#27010;&#29575;&#12290;&#30456;&#21453;&#65292;&#31574;&#30053;&#32593;&#32476;&#26356;&#26032;&#20854;&#21442;&#25968;&#20197;&#26368;&#23567;&#21270;&#36825;&#31181;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#22312;&#26368;&#22823;&#21270;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;&#30340;&#21516;&#26102;&#20445;&#25345;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;Adversarial Robust Policy Optimization (ARPO)&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;&#21487;&#20197;&#27867;&#21270;&#21040;&#26410;&#35265;&#29615;&#22659;&#30340;&#40065;&#26834;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;Procgen&#21644;Distracting Control Suite&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#27867;&#21270;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20960;&#31181;&#22522;&#20934;&#31639;&#27861;&#65288;&#21253;&#25324;&#25968;&#25454;&#22686;&#24191;&#65289;&#30456;&#27604;&#65292;ARPO&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an algorithm that aims to improve generalization for reinforcement learning agents by removing overfitting to confounding features. Our approach consists of a max-min game theoretic objective. A generator transfers the style of observation during reinforcement learning. An additional goal of the generator is to perturb the observation, which maximizes the agent's probability of taking a different action. In contrast, a policy network updates its parameters to minimize the effect of such perturbations, thus staying robust while maximizing the expected future reward. Based on this setup, we propose a practical deep reinforcement learning algorithm, Adversarial Robust Policy Optimization (ARPO), to find a robust policy that generalizes to unseen environments. We evaluate our approach on Procgen and Distracting Control Suite for generalization and sample efficiency. Empirically, ARPO shows improved performance compared to a few baseline algorithms, including data augmen
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22269;&#38469;&#27835;&#29702;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#22269;&#38469;&#20154;&#24037;&#26234;&#33021;&#32452;&#32455;&#26469;&#35748;&#35777;&#22269;&#23478;&#30340;&#31649;&#36758;&#21306;&#22495;&#26159;&#21542;&#31526;&#21512;&#22269;&#38469;&#30417;&#30563;&#26631;&#20934;&#65292;&#36827;&#32780;&#31105;&#27490;&#20174;&#26410;&#32463;&#35748;&#35777;&#30340;&#31649;&#36758;&#21306;&#22495;&#36827;&#21475;AI&#20379;&#24212;&#38142;&#25152;&#21253;&#21547;&#20135;&#21697;&#12290;</title><link>http://arxiv.org/abs/2308.15514</link><description>&lt;p&gt;
&#22269;&#38469;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;: &#19968;&#31181;&#31649;&#36758;&#35748;&#35777;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
International Governance of Civilian AI: A Jurisdictional Certification Approach. (arXiv:2308.15514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15514
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22269;&#38469;&#27835;&#29702;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#22269;&#38469;&#20154;&#24037;&#26234;&#33021;&#32452;&#32455;&#26469;&#35748;&#35777;&#22269;&#23478;&#30340;&#31649;&#36758;&#21306;&#22495;&#26159;&#21542;&#31526;&#21512;&#22269;&#38469;&#30417;&#30563;&#26631;&#20934;&#65292;&#36827;&#32780;&#31105;&#27490;&#20174;&#26410;&#32463;&#35748;&#35777;&#30340;&#31649;&#36758;&#21306;&#22495;&#36827;&#21475;AI&#20379;&#24212;&#38142;&#25152;&#21253;&#21547;&#20135;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25551;&#36848;&#20102;&#22269;&#38469;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27835;&#29702;&#23433;&#25490;&#35774;&#35745;&#20013;&#30340;&#26435;&#34913;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#19968;&#20010;&#26631;&#20934;&#12289;&#35768;&#21487;&#21644;&#36131;&#20219;&#21046;&#24230;&#25193;&#23637;&#21040;&#20840;&#29699;&#33539;&#22260;&#12290;&#25105;&#20204;&#24314;&#35758;&#21508;&#22269;&#24314;&#31435;&#19968;&#20010;&#22269;&#38469;&#20154;&#24037;&#26234;&#33021;&#32452;&#32455;&#65288;IAIO&#65289;&#26469;&#35748;&#35777;&#22269;&#23478;&#31649;&#36758;&#21306;&#22495;&#65288;&#32780;&#19981;&#26159;&#20844;&#21496;&#25110;AI&#39033;&#30446;&#65289;&#26159;&#21542;&#31526;&#21512;&#22269;&#38469;&#30417;&#30563;&#26631;&#20934;&#12290;&#21508;&#22269;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#31105;&#27490;&#20174;&#26410;&#32463;IAIO&#35748;&#35777;&#30340;&#31649;&#36758;&#21306;&#22495;&#36827;&#21475;AI&#20379;&#24212;&#38142;&#25152;&#21253;&#21547;&#20135;&#21697;&#30340;&#27861;&#35268;&#26469;&#23454;&#26045;&#36825;&#20123;&#22269;&#38469;&#26631;&#20934;&#12290;&#36825;&#19968;&#26041;&#27861;&#20511;&#37492;&#20102;&#29616;&#26377;&#22269;&#38469;&#32452;&#32455;&#27169;&#24335;&#65292;&#22914;&#22269;&#38469;&#27665;&#29992;&#33322;&#31354;&#32452;&#32455;&#65288;ICAO&#65289;&#12289;&#22269;&#38469;&#28023;&#20107;&#32452;&#32455;&#65288;IMO&#65289;&#21644;&#37329;&#34701;&#34892;&#21160;&#29305;&#21035;&#24037;&#20316;&#32452;&#65288;FATF&#65289;&#12290;&#21508;&#22269;&#36824;&#21487;&#20197;&#23545;&#38750;&#35748;&#35777;&#22269;&#23478;&#37319;&#21462;&#22810;&#36793;&#25511;&#21046;&#25514;&#26045;&#65292;&#20363;&#22914;&#23545;AI&#20135;&#21697;&#36755;&#20837;&#65288;&#22914;&#19987;&#29992;&#30828;&#20214;&#65289;&#30340;&#20986;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (AI) and presents one approach in detail. This approach represents the extension of a standards, licensing, and liability regime to the global level. We propose that states establish an International AI Organization (IAIO) to certify state jurisdictions (not firms or AI projects) for compliance with international oversight standards. States can give force to these international standards by adopting regulations prohibiting the import of goods whose supply chains embody AI from non-IAIO-certified jurisdictions. This borrows attributes from models of existing international organizations, such as the International Civilian Aviation Organization (ICAO), the International Maritime Organization (IMO), and the Financial Action Task Force (FATF). States can also adopt multilateral controls on the export of AI product inputs, such as specialized hardware, to non-certified
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22823;&#25968;&#25454;&#38598;&#19979;t-SNE&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.15513</link><description>&lt;p&gt;
&#35843;&#25972;&#22256;&#24785;&#24230;&#24182;&#35745;&#31639;&#22522;&#20110;&#37319;&#26679;&#30340;t-SNE&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Tuning the perplexity for and computing sampling-based t-SNE embeddings. (arXiv:2308.15513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22823;&#25968;&#25454;&#38598;&#19979;t-SNE&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#24120;&#29992;&#30340;&#31649;&#36947;&#21033;&#29992;&#20108;&#32500;&#21487;&#35270;&#21270;&#65292;&#20363;&#22914;&#36890;&#36807;t&#20998;&#24067;&#37051;&#36817;&#38543;&#26426;&#23884;&#20837;&#65288;t-SNE&#65289;&#12290;&#20294;&#22312;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#26102;&#65292;&#24212;&#29992;&#36825;&#20123;&#21487;&#35270;&#21270;&#25216;&#26415;&#20250;&#29983;&#25104;&#27425;&#20248;&#30340;&#23884;&#20837;&#65292;&#22240;&#20026;&#36229;&#21442;&#25968;&#19981;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#12290;&#23558;&#36825;&#20123;&#21442;&#25968;&#22686;&#21152;&#36890;&#24120;&#19981;&#36215;&#20316;&#29992;&#65292;&#22240;&#20026;&#35745;&#31639;&#23545;&#20110;&#23454;&#38469;&#24037;&#20316;&#27969;&#31243;&#26469;&#35828;&#22826;&#26114;&#36149;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#37319;&#26679;&#30340;&#23884;&#20837;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24517;&#39035;&#35880;&#24910;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#21462;&#20915;&#20110;&#37319;&#26679;&#29575;&#21644;&#39044;&#26399;&#30340;&#26368;&#32456;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#21152;&#36895;&#35745;&#31639;&#24182;&#25552;&#39640;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widely used pipelines for the analysis of high-dimensional data utilize two-dimensional visualizations. These are created, e.g., via t-distributed stochastic neighbor embedding (t-SNE). When it comes to large data sets, applying these visualization techniques creates suboptimal embeddings, as the hyperparameters are not suitable for large data. Cranking up these parameters usually does not work as the computations become too expensive for practical workflows. In this paper, we argue that a sampling-based embedding approach can circumvent these problems. We show that hyperparameters must be chosen carefully, depending on the sampling rate and the intended final embedding. Further, we show how this approach speeds up the computation and increases the quality of the embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#22312;&#32447;&#35770;&#22363;&#20013;&#32593;&#32476;&#25112;&#22763;&#30340;&#27963;&#21160;&#27700;&#24179;&#65292;&#21457;&#29616;&#21482;&#26377;&#23569;&#25968;&#32593;&#32476;&#25112;&#22763;&#26159;&#27963;&#36291;&#29992;&#25143;&#65292;&#20182;&#20204;&#22312;&#21644;&#24179;&#26102;&#26399;&#20445;&#25345;&#27785;&#40664;&#65292;&#21482;&#22312;&#24517;&#35201;&#26102;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#26816;&#27979;&#19981;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#27604;&#35782;&#21035;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#22909;&#25429;&#25417;&#32593;&#32476;&#25112;&#22763;&#34892;&#21160;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15491</link><description>&lt;p&gt;
&#20174;&#22312;&#32447;&#35770;&#22363;&#20013;&#26816;&#27979;&#19981;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;
&lt;/p&gt;
&lt;p&gt;
Detecting Inactive Cyberwarriors from Online Forums. (arXiv:2308.15491v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#22312;&#32447;&#35770;&#22363;&#20013;&#32593;&#32476;&#25112;&#22763;&#30340;&#27963;&#21160;&#27700;&#24179;&#65292;&#21457;&#29616;&#21482;&#26377;&#23569;&#25968;&#32593;&#32476;&#25112;&#22763;&#26159;&#27963;&#36291;&#29992;&#25143;&#65292;&#20182;&#20204;&#22312;&#21644;&#24179;&#26102;&#26399;&#20445;&#25345;&#27785;&#40664;&#65292;&#21482;&#22312;&#24517;&#35201;&#26102;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#26816;&#27979;&#19981;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#27604;&#35782;&#21035;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#22909;&#25429;&#25417;&#32593;&#32476;&#25112;&#22763;&#34892;&#21160;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26102;&#20195;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#25104;&#20026;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#25112;&#20105;&#12290;&#36825;&#31181;&#25112;&#20105;&#28041;&#21450;&#21040;&#32593;&#32476;&#25112;&#22763;&#65292;&#20182;&#20204;&#26377;&#24847;&#20256;&#25773;&#26088;&#22312;&#35837;&#35876;&#23545;&#25163;&#25110;&#22242;&#32467;&#30431;&#21451;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#22312;&#32447;&#35770;&#22363;&#20013;&#32593;&#32476;&#25112;&#22763;&#30340;&#27963;&#21160;&#27700;&#24179;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21482;&#26377;&#23569;&#25968;&#32593;&#32476;&#25112;&#22763;&#26159;&#27963;&#36291;&#29992;&#25143;&#12290;&#20196;&#20154;&#24847;&#22806;&#30340;&#26159;&#65292;&#23613;&#31649;&#20182;&#20204;&#34987;&#26399;&#26395;&#31215;&#26497;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#65292;&#22312;&#21644;&#24179;&#26102;&#26399;&#32593;&#32476;&#25112;&#22763;&#21364;&#20445;&#25345;&#27785;&#40664;&#65292;&#21482;&#22312;&#24517;&#35201;&#26102;&#25165;&#34892;&#21160;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35782;&#21035;&#32593;&#32476;&#25112;&#22763;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#26816;&#27979;&#19981;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#27604;&#35782;&#21035;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#35201;&#38590;&#24471;&#22810;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26356;&#26377;&#25928;&#22320;&#22312;&#32593;&#32476;&#25112;&#22763;&#19981;&#27963;&#36291;&#30340;&#38454;&#27573;&#35782;&#21035;&#20182;&#20204;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#20026;&#26356;&#22909;&#22320;&#25429;&#25417;&#20182;&#20204;&#30340;&#34892;&#21160;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of misinformation has emerged as a new form of warfare in the information age. This type of warfare involves cyberwarriors, who deliberately propagate messages aimed at defaming opponents or fostering unity among allies. In this study, we investigate the level of activity exhibited by cyberwarriors within a large online forum, and remarkably, we discover that only a minute fraction of cyberwarriors are active users. Surprisingly, despite their expected role of actively disseminating misinformation, cyberwarriors remain predominantly silent during peacetime and only spring into action when necessary. Moreover, we analyze the challenges associated with identifying cyberwarriors and provide evidence that detecting inactive cyberwarriors is considerably more challenging than identifying their active counterparts. Finally, we discuss potential methodologies to more effectively identify cyberwarriors during their inactive phases, offering insights into better capturing thei
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21452;&#22270;&#34701;&#21512;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;1&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;GCN&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#35786;&#26029;&#65307;2&#65289;&#35813;&#26550;&#26500;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#22270;&#32467;&#26500;&#65292;&#23398;&#20064;&#26368;&#20248;&#30340;&#28508;&#22312;&#22270;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#35786;&#26029;&#32467;&#26524;&#65307;3&#65289;&#32467;&#21512;&#29305;&#24449;&#22270;&#23398;&#20064;&#21644;&#21160;&#24577;&#22270;&#23398;&#20064;&#65292;&#21152;&#26435;&#22788;&#29702;&#26377;&#29992;&#30340;&#21463;&#35797;&#32773;&#29305;&#24449;&#65292;&#20943;&#23569;&#20854;&#20182;&#22122;&#22768;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;AD&#35786;&#26029;&#20013;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15484</link><description>&lt;p&gt;
&#21160;&#24577;&#21452;&#22270;&#34701;&#21512;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Dynamic Dual-Graph Fusion Convolutional Network For Alzheimer's Disease Diagnosis. (arXiv:2308.15484v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21452;&#22270;&#34701;&#21512;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;1&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;GCN&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#35786;&#26029;&#65307;2&#65289;&#35813;&#26550;&#26500;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#22270;&#32467;&#26500;&#65292;&#23398;&#20064;&#26368;&#20248;&#30340;&#28508;&#22312;&#22270;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#35786;&#26029;&#32467;&#26524;&#65307;3&#65289;&#32467;&#21512;&#29305;&#24449;&#22270;&#23398;&#20064;&#21644;&#21160;&#24577;&#22270;&#23398;&#20064;&#65292;&#21152;&#26435;&#22788;&#29702;&#26377;&#29992;&#30340;&#21463;&#35797;&#32773;&#29305;&#24449;&#65292;&#20943;&#23569;&#20854;&#20182;&#22122;&#22768;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;AD&#35786;&#26029;&#20013;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21452;&#22270;&#34701;&#21512;&#21367;&#31215;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#24615;&#33021;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;a&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;GCN&#26550;&#26500;&#65292;&#29992;&#20110;AD&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#35786;&#26029;&#65307;&#65288;b&#65289;&#35813;&#26550;&#26500;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;GCN&#30340;&#22270;&#32467;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#26368;&#20248;&#30340;&#28508;&#22312;&#22270;&#26469;&#20135;&#29983;&#26356;&#22909;&#30340;&#35786;&#26029;&#32467;&#26524;&#65307;&#65288;c&#65289;&#32467;&#21512;&#29305;&#24449;&#22270;&#23398;&#20064;&#21644;&#21160;&#24577;&#22270;&#23398;&#20064;&#65292;&#32473;&#20104;&#37027;&#20123;&#26377;&#29992;&#30340;&#21463;&#35797;&#32773;&#29305;&#24449;&#26356;&#22810;&#30340;&#26435;&#37325;&#65292;&#21516;&#26102;&#38477;&#20302;&#20854;&#20182;&#22122;&#22768;&#29305;&#24449;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;AD&#35786;&#26029;&#20013;&#33021;&#22815;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a dynamic dual-graph fusion convolutional network is proposed to improve Alzheimer's disease (AD) diagnosis performance. The following are the paper's main contributions: (a) propose a novel dynamic GCN architecture, which is an end-to-end pipeline for diagnosis of the AD task; (b) the proposed architecture can dynamically adjust the graph structure for GCN to produce better diagnosis outcomes by learning the optimal underlying latent graph; (c) incorporate feature graph learning and dynamic graph learning, giving those useful features of subjects more weight while decreasing the weights of other noise features. Experiments indicate that our model provides flexibility and stability while achieving excellent classification results in AD diagnosis.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25552;&#20132;&#26102;&#38388;&#19978;&#36827;&#34892;&#20316;&#19994;&#22833;&#36133;&#39044;&#27979;&#65292;&#24182;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26469;&#34920;&#31034;&#20316;&#19994;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#39640;&#24615;&#33021;&#35745;&#31639;&#31995;&#32479;&#31649;&#29702;&#65292;&#25552;&#39640;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.15481</link><description>&lt;p&gt;
&#22312;HPC&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#20316;&#19994;&#22833;&#36133;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Job Failure Prediction in an HPC System. (arXiv:2308.15481v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25552;&#20132;&#26102;&#38388;&#19978;&#36827;&#34892;&#20316;&#19994;&#22833;&#36133;&#39044;&#27979;&#65292;&#24182;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26469;&#34920;&#31034;&#20316;&#19994;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#39640;&#24615;&#33021;&#35745;&#31639;&#31995;&#32479;&#31649;&#29702;&#65292;&#25552;&#39640;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#31995;&#32479;&#26159;&#22797;&#26434;&#30340;&#26426;&#22120;&#65292;&#23545;&#32463;&#27982;&#21644;&#31038;&#20250;&#37117;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#38500;&#20102;&#35745;&#31639;&#33021;&#21147;&#22806;&#65292;&#33021;&#28304;&#28040;&#32791;&#20063;&#22312;&#19981;&#26029;&#22686;&#38271;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#24403;&#21069;&#30340;&#29615;&#22659;&#21644;&#33021;&#28304;&#21361;&#26426;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20248;&#21270;HPC&#31995;&#32479;&#31649;&#29702;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#65292;&#26082;&#21487;&#20197;&#20445;&#35777;&#19968;&#27969;&#30340;&#24615;&#33021;&#65292;&#21448;&#21487;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12290;&#19968;&#31181;&#31574;&#30053;&#26159;&#36890;&#36807;&#22312;&#24037;&#20316;&#36127;&#36733;&#32423;&#21035;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#22312;&#31995;&#32479;&#19978;&#25191;&#34892;&#20043;&#21069;&#31361;&#20986;&#26174;&#31034;&#26368;&#26377;&#21487;&#33021;&#22833;&#36133;&#30340;&#20316;&#19994;&#12290;&#20316;&#19994;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#22833;&#36133;&#20250;&#19981;&#24517;&#35201;&#22320;&#21344;&#29992;&#36164;&#28304;&#65292;&#21487;&#33021;&#20250;&#24310;&#36831;&#20854;&#20182;&#20316;&#19994;&#65292;&#23545;&#31995;&#32479;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30740;&#31350;&#20102;&#25552;&#20132;&#26102;&#38388;&#19978;&#30340;&#20316;&#19994;&#22833;&#36133;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22312;&#20110;&#65288;i&#65289;&#23558;&#36825;&#20123;&#31639;&#27861;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#34920;&#31034;&#20316;&#19994;&#21644;&#65288;ii&#65289;th
&lt;/p&gt;
&lt;p&gt;
Modern High Performance Computing (HPC) systems are complex machines, with major impacts on economy and society. Along with their computational capability, their energy consumption is also steadily raising, representing a critical issue given the ongoing environmental and energetic crisis. Therefore, developing strategies to optimize HPC system management has paramount importance, both to guarantee top-tier performance and to improve energy efficiency. One strategy is to act at the workload level and highlight the jobs that are most likely to fail, prior to their execution on the system. Jobs failing during their execution unnecessarily occupy resources which could delay other jobs, adversely affecting the system performance and energy consumption. In this paper, we study job failure prediction at submit-time using classical machine learning algorithms. Our novelty lies in (i) the combination of these algorithms with Natural Language Processing (NLP) tools to represent jobs and (ii) th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15321</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#65292;&#32570;&#20047;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23545;&#37319;&#26679;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;&#39044;&#27979;&#35823;&#24046;&#24402;&#22240;&#20026;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#38500;&#20102;&#38416;&#26126;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;Epsilon Scaling&#65292;&#20197;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Epsilon Scaling&#36890;&#36807;&#32553;&#23567;&#32593;&#32476;&#36755;&#20986;&#65288;Epsilon&#65289;&#26126;&#30830;&#22320;&#23558;&#37319;&#26679;&#36712;&#36857;&#31227;&#36817;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#12290;&#22312;&#21508;&#31181;&#25193;&#25955;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#23558;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#12289;&#33080;&#37096;&#34920;&#24773;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;GPT-3.5&#27169;&#22411;&#26469;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#20197;&#25552;&#20379;&#20449;&#24687;&#24182;&#19982;&#35775;&#23458;&#36827;&#34892;&#33258;&#28982;&#20132;&#27969;&#12290;</title><link>http://arxiv.org/abs/2308.15214</link><description>&lt;p&gt;
FurChat: &#20351;&#29992;LLMs&#30340;&#20855;&#26377;&#33080;&#37096;&#34920;&#24773;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#32467;&#21512;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions. (arXiv:2308.15214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#23558;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#12289;&#33080;&#37096;&#34920;&#24773;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;GPT-3.5&#27169;&#22411;&#26469;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#20197;&#25552;&#20379;&#20449;&#24687;&#24182;&#19982;&#35775;&#23458;&#36827;&#34892;&#33258;&#28982;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#21487;&#20197;&#20316;&#20026;&#25509;&#24453;&#21592;&#65292;&#29983;&#25104;&#32467;&#21512;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#20197;&#21450;&#33080;&#37096;&#34920;&#24773;&#30340;&#28151;&#21512;&#23545;&#35805;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24320;&#21457;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#23558;&#35813;&#31995;&#32479;&#37096;&#32626;&#21040;&#20102;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;Furhat&#26426;&#22120;&#20154;&#19978;&#65292;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#21475;&#22836;&#21644;&#38750;&#35821;&#35328;&#25552;&#31034;&#12290;&#35813;&#31995;&#32479;&#19987;&#38376;&#20026;&#22269;&#23478;&#26426;&#22120;&#20154;&#23454;&#39564;&#23460;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#28982;&#23545;&#35805;&#19982;&#35775;&#23458;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#21521;&#20182;&#20204;&#25552;&#20379;&#26377;&#20851;&#35774;&#26045;&#12289;&#30740;&#31350;&#12289;&#26032;&#38395;&#12289;&#21363;&#23558;&#20030;&#34892;&#30340;&#27963;&#21160;&#31561;&#26041;&#38754;&#30340;&#20449;&#24687;&#12290;&#31995;&#32479;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;GPT-3.5&#27169;&#22411;&#26681;&#25454;&#25552;&#31034;&#29983;&#25104;&#36825;&#20123;&#20449;&#24687;&#65292;&#21516;&#26102;&#29983;&#25104;&#39046;&#22495;&#36890;&#29992;&#30340;&#23545;&#35805;&#21644;&#38754;&#37096;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate an embodied conversational agent that can function as a receptionist and generate a mixture of open and closed-domain dialogue along with facial expressions, by using a large language model (LLM) to develop an engaging conversation. We deployed the system onto a Furhat robot, which is highly expressive and capable of using both verbal and nonverbal cues during interaction. The system was designed specifically for the National Robotarium to interact with visitors through natural conversations, providing them with information about the facilities, research, news, upcoming events, etc. The system utilises the state-of-the-art GPT-3.5 model to generate such information along with domain-general conversations and facial expressions based on prompt engineering.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.15053</link><description>&lt;p&gt;
&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adapting text-based dialogue state tracker for spoken dialogues. (arXiv:2308.15053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#31454;&#36187;&#65288;DSTC&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#35821;&#38899;&#30028;&#38754;&#30340;&#31283;&#20581;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22823;&#37096;&#20998;&#36827;&#23637;&#37117;&#26159;&#38024;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22240;&#20026;&#26377;&#20016;&#23500;&#30340;&#20070;&#38754;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#65292;&#32780;&#20855;&#26377;&#21475;&#35821;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#31232;&#32570;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;Siri&#21644;Alexa&#31561;&#35821;&#38899;&#21161;&#25163;&#31995;&#32479;&#25152;&#23637;&#31034;&#30340;&#65292;&#23558;&#36825;&#31181;&#25104;&#21151;&#36716;&#31227;&#21040;&#21475;&#35821;&#23545;&#35805;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;DSTC11&#30340;&#20855;&#26377;&#35821;&#38899;&#24863;&#30693;&#30340;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#25361;&#25112;&#36187;&#20013;&#30340;&#39640;&#24230;&#25104;&#21151;&#27169;&#22411;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#65288;1&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#65292;&#20197;&#24357;&#21512;&#21475;&#35821;&#21644;&#25991;&#26412;&#35805;&#35821;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#65288;2&#65289;&#29992;&#20110;&#20272;&#35745;&#25554;&#27133;&#21644;&#20540;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;D3ST&#65289;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#25554;&#27133;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there have been remarkable advances in dialogue systems through the dialogue systems technology competition (DSTC), it remains one of the key challenges to building a robust task-oriented dialogue system with a speech interface. Most of the progress has been made for text-based dialogue systems since there are abundant datasets with written corpora while those with spoken dialogues are very scarce. However, as can be seen from voice assistant systems such as Siri and Alexa, it is of practical importance to transfer the success to spoken dialogues. In this paper, we describe our engineering effort in building a highly successful model that participated in the speech-aware dialogue systems technology challenge track in DSTC11. Our model consists of three major modules: (1) automatic speech recognition error correction to bridge the gap between the spoken and the text utterances, (2) text-based dialogue system (D3ST) for estimating the slots and values using slot descriptions, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.14815</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#23433;&#20840;&#39046;&#22495;&#65292;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#22312;&#39640;&#32500;&#33258;&#20027;&#31995;&#32479;&#30340;&#34892;&#20026;&#19978;&#25552;&#20379;&#20445;&#35777;&#12290;&#20197;&#21487;&#36798;&#24615;&#20998;&#26512;&#20026;&#20013;&#24515;&#30340;&#39564;&#35777;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#32780;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21463;&#21040;&#23545;&#37319;&#26679;&#36807;&#31243;&#30340;&#20998;&#24067;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#40657;&#30418;&#31995;&#32479;&#30340;&#20998;&#24067;&#40065;&#26834;&#29256;&#26412;&#30340;&#32479;&#35745;&#39564;&#35777;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#24615;&#33021;&#20445;&#35777;&#36866;&#29992;&#20110;&#22823;&#37327;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26680;&#24515;&#37096;&#20998;&#26159;&#19968;&#31181;&#31216;&#20026;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#23427;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#20197;&#25351;&#23548;&#20027;&#21160;&#23398;&#20064;&#12290;&#20027;&#21160;&#23398;&#20064;&#20351;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;Sherlock&#30340;&#20840;&#38754;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#26469;&#25910;&#38598;&#26679;&#26412;&#12290;&#22312;openAI gym Mujoco&#29615;&#22659;&#20013;&#20351;&#29992;&#22810;&#20010;&#29289;&#29702;&#27169;&#25311;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A particularly challenging problem in AI safety is providing guarantees on the behavior of high-dimensional autonomous systems. Verification approaches centered around reachability analysis fail to scale, and purely statistical approaches are constrained by the distributional assumptions about the sampling process. Instead, we pose a distributionally robust version of the statistical verification problem for black-box systems, where our performance guarantees hold over a large family of distributions. This paper proposes a novel approach based on a combination of active learning, uncertainty quantification, and neural network verification. A central piece of our approach is an ensemble technique called Imprecise Neural Networks, which provides the uncertainty to guide active learning. The active learning uses an exhaustive neural-network verification tool Sherlock to collect samples. An evaluation on multiple physical simulators in the openAI gym Mujoco environments with reinforcement-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23454;&#20307;&#23884;&#20837;&#21644;&#20195;&#29702;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#19978;&#19979;&#25991;&#24863;&#30693;&#22320;&#32452;&#21512;&#20195;&#29702;&#31574;&#30053;&#65292;&#20197;&#22312;&#22797;&#26434;&#19988;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#20248;&#21270;&#25191;&#34892;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.14521</link><description>&lt;p&gt;
&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23454;&#20307;&#23884;&#20837;&#21644;&#20195;&#29702;&#38598;&#21512;&#19978;&#19979;&#25991;&#24863;&#30693;&#22320;&#32452;&#21512;&#20195;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles. (arXiv:2308.14521v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23454;&#20307;&#23884;&#20837;&#21644;&#20195;&#29702;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#19978;&#19979;&#25991;&#24863;&#30693;&#22320;&#32452;&#21512;&#20195;&#29702;&#31574;&#30053;&#65292;&#20197;&#22312;&#22797;&#26434;&#19988;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#20248;&#21270;&#25191;&#34892;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#20195;&#29702;&#22312;&#29983;&#27963;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#25903;&#25345;&#20154;&#31867;&#65292;&#24182;&#22240;&#27492;&#23384;&#22312;&#24322;&#26500;&#29615;&#22659;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36816;&#20316;&#65292;&#24182;&#19988;&#21487;&#33021;&#38754;&#20020;&#24040;&#22823;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#12290;&#20026;&#20102;&#20197;&#30446;&#26631;&#23548;&#21521;&#30340;&#26041;&#24335;&#25191;&#34892;&#26381;&#21153;&#21644;&#27963;&#21160;&#65292;&#20195;&#29702;&#38656;&#35201;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#24517;&#39035;&#21046;&#23450;&#21644;&#36861;&#27714;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35268;&#23450;&#31574;&#30053;&#22312;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#23384;&#22312;&#38480;&#21046;&#21644;&#19981;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#20195;&#29702;&#30340;&#19978;&#19979;&#25991;&#20915;&#23450;&#20102;&#23427;&#30340;&#21160;&#20316;&#36873;&#25321;&#12290;&#30001;&#20110;&#29615;&#22659;&#21487;&#33021;&#20855;&#26377;&#38543;&#26426;&#24615;&#65292;&#24182;&#19988;&#22312;&#29366;&#24577;&#21644;&#21487;&#34892;&#21160;&#20316;&#30340;&#25968;&#37327;&#19978;&#22797;&#26434;&#65292;&#22240;&#27492;&#36890;&#24120;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20197;&#31616;&#21270;&#30340;&#26041;&#24335;&#24314;&#27169;&#27963;&#21160;&#65292;&#20197;&#20415;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#31574;&#30053;&#65292;&#20174;&#32780;&#24110;&#21161;&#25429;&#25417;&#19978;&#19979;&#25991;&#24182;&#26681;&#25454;&#26368;&#20248;&#26041;&#24335;&#25191;&#34892;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational agents support humans in many areas of life and are therefore found in heterogeneous contexts. This means they operate in rapidly changing environments and can be confronted with huge state and action spaces. In order to perform services and carry out activities in a goal-oriented manner, agents require prior knowledge and therefore have to develop and pursue context-dependent policies. However, prescribing policies in advance is limited and inflexible, especially in dynamically changing environments. Moreover, the context of an agent determines its choice of actions. Since the environments can be stochastic and complex in terms of the number of states and feasible actions, activities are usually modelled in a simplified way by Markov decision processes so that, e.g., agents with reinforcement learning are able to learn policies, that help to capture the context and act accordingly to optimally perform activities. However, training policies for all possible contexts using
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24773;&#24863;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.14359</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks. (arXiv:2308.14359v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24773;&#24863;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20013;&#65292;&#20154;&#31867;&#24773;&#24863;&#29702;&#35299;&#22312;&#20351;&#23545;&#35805;&#25216;&#26415;&#25104;&#20026;&#20027;&#27969;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#35821;&#38899;&#24773;&#24863;&#29702;&#35299;&#35270;&#20026;&#19968;&#31181;&#30693;&#35273;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#29616;&#23454;&#30340;&#24773;&#26223;&#12290;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65288;&#35821;&#35328;&#65292;&#20154;&#21475;&#32479;&#35745;&#23398;&#31561;&#65289;&#65292;&#19981;&#21516;&#27604;&#20363;&#30340;&#20154;&#20250;&#23558;&#30456;&#21516;&#30340;&#35821;&#38899;&#29255;&#27573;&#35270;&#20026;&#38750;&#19968;&#33268;&#30340;&#24773;&#24863;&#12290;&#20316;&#20026;ACM&#22810;&#23186;&#20307;2023&#35745;&#31639;&#35821;&#38899;&#32852;&#26426;&#25361;&#25112;&#65288;ComParE&#65289;&#30340;&#19968;&#37096;&#20998;&#65292;&#22312;EMotion Share&#36712;&#36947;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#20182;&#20204;&#20016;&#23500;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#21644;&#22810;&#26631;&#31614;&#22238;&#24402;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#8220;&#24773;&#24863;&#20998;&#20139;&#8221;&#25110;&#23545;&#35813;&#24773;&#24863;&#30340;&#24863;&#30693;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#26696;&#20915;&#23450;&#20102;&#23427;&#20204;&#22312;&#36229;&#36234;&#35821;&#38899;&#35782;&#21035;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24773;&#24863;&#29702;&#35299;&#31561;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#65292;&#30446;&#26631;&#26631;&#31614;&#30340;&#21464;&#21270;&#20197;&#21450;&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#30340;&#22266;&#26377;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human emotion understanding is pivotal in making conversational technology mainstream. We view speech emotion understanding as a perception task which is a more realistic setting. With varying contexts (languages, demographics, etc.) different share of people perceive the same speech segment as a non-unanimous emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset of multilingual speakers and multi-label regression target of 'emotion share' or perception of that emotion. We demonstrate that the training scheme of different foundation models dictates their effectiveness for tasks beyond speech recognition, especially for non-semantic speech tasks like emotion understanding. This is a very complex task due to multilingual speakers, variability in the target labels, and inherent imbalance in the regression dataset. Our results show that HuBERT-Large with a self-attention-based light-weight se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27169;&#22411;&#23545;&#38476;&#29983;&#25351;&#20196;&#30340;&#22788;&#29702;&#33021;&#21147;&#26377;&#24453;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.14306</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness to Instructions of Large Language Models. (arXiv:2308.14306v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27169;&#22411;&#23545;&#38476;&#29983;&#25351;&#20196;&#30340;&#22788;&#29702;&#33021;&#21147;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#20013;&#38646;-shot&#33021;&#21147;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#26174;&#31034;&#20986;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#29978;&#33267;&#36798;&#21040;&#30456;&#24403;&#20110;&#26356;&#22823;&#27169;&#22411;&#21464;&#20307;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20219;&#21153;&#21644;&#26410;&#30693;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23545;&#20845;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#21253;&#25324;Alpaca&#12289;Vicuna&#12289;WizardLM&#21644;&#20256;&#32479;&#30340;&#20219;&#21153;&#23548;&#21521;&#27169;&#22411;&#65288;Flan-T5-XL/XXL&#12289;T0++&#65289;&#65292;&#20197;&#30495;&#23454;&#19990;&#30028;&#30340;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#36981;&#24490;&#25351;&#20196;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#25351;&#20196;&#21644;&#20219;&#21153;&#23548;&#21521;&#25351;&#20196;&#36827;&#34892;&#24494;&#35843;&#30340;&#12290;&#20027;&#35201;&#35752;&#35770;&#30340;&#26159;&#23427;&#20204;&#22312;&#22788;&#29702;&#25351;&#20196;&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#38476;&#29983;&#25351;&#20196;&#26041;&#38754;&#30340;&#24615;&#33021;&#24448;&#24448;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Instruction fine-tuning has risen to prominence as a potential method for enhancing the zero-shot capabilities of Large Language Models (LLMs) on novel tasks. This technique has shown an exceptional ability to boost the performance of moderately sized LLMs, sometimes even reaching performance levels comparable to those of much larger model variants. The focus is on the robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction datasets as case studies. We carried out a comprehensive evaluation of these instruction-following LLMs which have been tuned based on open-domain instructions and task-oriented instructions. The main discussion is their performance and robustness towards instructions. We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#25324;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#27010;&#24565;&#65292;&#23545;&#35813;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12315</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Representation Learning Across Domains. (arXiv:2308.12315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#25324;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#27010;&#24565;&#65292;&#23545;&#35813;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20154;&#20204;&#26082;&#20139;&#21463;&#21040;&#20102;&#36825;&#20123;&#25216;&#26415;&#24102;&#26469;&#30340;&#22909;&#22788;&#65292;&#20063;&#38754;&#20020;&#22240;&#36825;&#20123;&#31995;&#32479;&#32780;&#24341;&#21457;&#30340;&#35768;&#22810;&#31038;&#20250;&#38382;&#39064;&#12290;&#20026;&#20102;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36275;&#22815;&#22909;&#24182;&#19988;&#21487;&#20449;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#20043;&#19968;&#65292;&#32780;&#34920;&#31034;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#22914;&#20309;&#20351;&#34920;&#31034;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#21487;&#20449;&#24230;&#65292;&#20363;&#22914;&#36328;&#39046;&#22495;&#22330;&#26223;&#65292;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#39046;&#22495;&#37117;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#21644;&#24517;&#35201;&#30340;&#12290;&#22312;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#20102;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36825;&#22235;&#20010;&#27010;&#24565;&#65292;&#23545;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#22312;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;AI&#31995;&#32479;&#22242;&#38431;&#65292;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#36229;&#36234;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#24819;&#27861;&#12290;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;</title><link>http://arxiv.org/abs/2308.09175</link><description>&lt;p&gt;
&#25193;&#23637;AI&#65306;&#21521;&#25317;&#26377;&#21019;&#36896;&#24615;&#30340;AlphaZero&#22269;&#38469;&#35937;&#26827;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Diversifying AI: Towards Creative Chess with AlphaZero. (arXiv:2308.09175v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#22312;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;AI&#31995;&#32479;&#22242;&#38431;&#65292;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#36229;&#36234;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#24819;&#27861;&#12290;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21508;&#31181;&#35745;&#31639;&#20219;&#21153;&#19978;&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19968;&#26679;&#65292;AI&#31995;&#32479;&#20063;&#20250;&#29359;&#38169;&#35823;&#65292;&#26377;&#30450;&#28857;&#65292;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#26032;&#24773;&#20917;&#26102;&#24456;&#38590;&#36827;&#34892;&#27867;&#21270;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24403;AI&#31995;&#32479;&#30340;&#35745;&#31639;&#21512;&#29702;&#24615;&#25512;&#21040;&#26497;&#38480;&#26102;&#65292;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#30340;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#36890;&#36807;&#20316;&#20026;&#19968;&#20010;&#22242;&#38431;&#30340;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#21487;&#20197;&#32988;&#36807;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#28982;&#21518;&#36873;&#25321;&#26368;&#22909;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#20197;&#22269;&#38469;&#35937;&#26827;&#36825;&#20010;&#34987;&#31216;&#20026;AI&#26524;&#34631;&#30340;&#28216;&#25103;&#20026;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;AlphaZero (AZ)&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#28508;&#21464;&#26465;&#20214;&#26550;&#26500;&#25193;&#23637;&#23427;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20195;&#29702;&#22242;&#38431;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;AZ_db&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#22810;&#26679;&#24615;&#25216;&#26415;&#23545;AZ_db&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#24819;&#27861;&#65292;&#24182;&#36890;&#36807;&#27425;&#21152;&#24615;&#35745;&#21010;&#36873;&#25321;&#26368;&#26377;&#24076;&#26395;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AZ_db&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse wa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06032</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;ChatGPT&#33021;&#21542;&#21462;&#20195;&#24459;&#24072;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#23545;&#27861;&#24459;&#31995;&#32479;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#22312;&#36827;&#34892;&#27861;&#24459;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30740;&#31350;&#28041;&#21450;&#21152;&#23494;&#36135;&#24065;&#30340;&#35777;&#21048;&#26696;&#20214;&#65292;&#20316;&#20026;AI&#21487;&#20197;&#25903;&#25345;&#27861;&#24459;&#36807;&#31243;&#30340;&#20247;&#22810;&#24773;&#22659;&#20043;&#19968;&#65292;&#30740;&#31350;LLMs&#30340;&#27861;&#24459;&#25512;&#29702;&#21644;&#36215;&#33609;&#33021;&#21147;&#12290;&#25105;&#20204;&#26816;&#26597;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;a&#65289;LLM&#33021;&#21542;&#20934;&#30830;&#30830;&#23450;&#20107;&#23454;&#27169;&#24335;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36829;&#27861;&#34892;&#20026;&#65292;b&#65289;&#22522;&#20110;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#65292;&#38506;&#23457;&#22242;&#30340;&#20915;&#31574;&#26159;&#21542;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#30495;&#23454;&#26696;&#20363;&#20013;&#30340;&#20107;&#23454;&#27169;&#24335;&#36755;&#20837;GPT-3.5&#65292;&#24182;&#35780;&#20272;&#20854;&#30830;&#23450;&#27491;&#30830;&#28508;&#22312;&#36829;&#27861;&#34892;&#20026;&#24182;&#25490;&#38500;&#34394;&#20551;&#36829;&#27861;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35831;&#27169;&#25311;&#38506;&#23457;&#21592;&#35780;&#20272;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#12290;GPT-3.5&#30340;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#36739;&#24369;&#65292;&#20294;&#25105;&#20204;&#39044;&#26399;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#24314;&#35758;&#30340;&#36829;&#27861;&#34892;&#20026;&#24448;&#24448;&#26159;&#27491;&#30830;&#30340;&#65288;&#23427;&#20165;&#20165;&#36807;&#20110;&#20445;&#23432;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20026;&#20160;&#20040;&#23578;&#26410;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#65292;&#24182;&#25351;&#20986;&#20102;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21644;&#36164;&#37329;&#25512;&#24191;&#19981;&#36275;&#26159;&#21046;&#32422;AGI&#21457;&#23637;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#23454;&#29616;&#20154;&#31867;&#36866;&#24212;&#33021;&#21147;&#21644;&#33258;&#20027;&#23398;&#20064;&#25152;&#38656;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03598</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#25105;&#20204;&#23578;&#26410;&#25317;&#26377;AGI
&lt;/p&gt;
&lt;p&gt;
Why We Don't Have AGI Yet. (arXiv:2308.03598v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20026;&#20160;&#20040;&#23578;&#26410;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#65292;&#24182;&#25351;&#20986;&#20102;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21644;&#36164;&#37329;&#25512;&#24191;&#19981;&#36275;&#26159;&#21046;&#32422;AGI&#21457;&#23637;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#23454;&#29616;&#20154;&#31867;&#36866;&#24212;&#33021;&#21147;&#21644;&#33258;&#20027;&#23398;&#20064;&#25152;&#38656;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2002&#24180;&#37325;&#26032;&#38416;&#36848;&#20102;AI&#30340;&#21407;&#22987;&#24895;&#26223;&#65292;&#31216;&#20043;&#20026;&#8220;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#8221;&#25110;AGI&#12290;&#36825;&#19968;&#24895;&#26223;&#26159;&#26500;&#24314;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#35299;&#20915;&#38382;&#39064;&#30340;&#8220;&#24605;&#32771;&#26426;&#22120;&#8221;&#35745;&#31639;&#26426;&#31995;&#32479;&#12290;&#36825;&#19982;&#20960;&#21313;&#24180;&#26469;&#20960;&#20046;&#25152;&#26377;&#20154;&#22312;&#35813;&#39046;&#22495;&#23454;&#36341;&#30340;&#8220;&#29421;&#20041;AI&#8221;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#34429;&#28982;&#26377;&#20960;&#20010;&#22823;&#35268;&#27169;&#30340;&#39033;&#30446;&#21517;&#20041;&#19978;&#22312;&#33268;&#21147;&#20110;AGI&#30340;&#30740;&#21457;&#65288;&#23588;&#20854;&#26159;DeepMind&#65289;&#65292;&#20294;&#22312;&#32431;&#31929;&#19987;&#27880;&#30340;AGI&#21457;&#23637;&#39046;&#22495;&#65292;&#36164;&#37329;&#21644;&#25512;&#24191;&#24182;&#19981;&#20805;&#36275;&#12290;&#36825;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#30495;&#27491;&#30340;AGI&#21487;&#20197;&#20026;&#20154;&#31867;&#24102;&#26469;&#24040;&#22823;&#30340;&#20215;&#20540;&#12290;&#38500;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#32570;&#20047;&#21162;&#21147;&#20043;&#22806;&#65292;&#36824;&#23384;&#22312;&#20111;&#27424;&#30340;&#29702;&#35770;&#21644;&#26041;&#27861;&#19978;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#24378;&#35843;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#26080;&#27861;&#23454;&#29616;AGI&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#36866;&#24212;&#33021;&#21147;&#21644;&#33258;&#20027;&#23398;&#20064;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#31038;&#20250;&#25216;&#26415;&#21457;&#23637;&#26041;&#38754;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The original vision of AI was re-articulated in 2002 via the term 'Artificial General Intelligence' or AGI. This vision is to build 'Thinking Machines' computer systems that can learn, reason, and solve problems similar to the way humans do. This is in stark contrast to the 'Narrow AI' approach practiced by almost everyone in the field over the many decades. While several large-scale efforts have nominally been working on AGI (most notably DeepMind), the field of pure focused AGI development has not been well funded or promoted. This is surprising given the fantastic value that true AGI can bestow on humanity. In addition to the dearth of effort in this field, there are also several theoretical and methodical missteps that are hampering progress. We highlight why purely statistical approaches are unlikely to lead to AGI, and identify several crucial cognitive abilities required to achieve human-like adaptability and autonomous learning. We conclude with a survey of socio-technical fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#22238;&#25253;&#24046;&#36317;&#19978;&#30028;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#38382;&#39064;&#36716;&#21270;&#20026;&#31163;&#25955;&#28040;&#24687;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#37327;&#21270;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#20449;&#24320;&#38144;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22909;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.03358</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;POMDP&#20013;&#22522;&#20110;&#22312;&#32447;&#32858;&#31867;&#26631;&#31614;&#30340;&#31163;&#25955;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Discrete Message via Online Clustering Labels in Decentralized POMDP. (arXiv:2308.03358v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#22238;&#25253;&#24046;&#36317;&#19978;&#30028;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#38382;&#39064;&#36716;&#21270;&#20026;&#31163;&#25955;&#28040;&#24687;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#37327;&#21270;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#20449;&#24320;&#38144;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22909;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#36890;&#20449;&#23545;&#20110;&#35299;&#20915;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#40657;&#30418;&#26041;&#27861;&#65292;&#23558;&#26412;&#22320;&#20449;&#24687;/&#29305;&#24449;&#32534;&#30721;&#25104;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#20849;&#20139;&#30340;&#28040;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#40657;&#30418;&#26041;&#27861;&#26080;&#27861;&#23545;&#26399;&#26395;&#22238;&#25253;&#25552;&#20379;&#20219;&#20309;&#37327;&#21270;&#20445;&#35777;&#65292;&#24120;&#24120;&#23548;&#33268;&#29983;&#25104;&#36890;&#20449;&#24320;&#38144;&#39640;&#12289;&#21487;&#35299;&#37322;&#24615;&#24046;&#30340;&#36830;&#32493;&#28040;&#24687;&#12290;&#26412;&#25991;&#22312;&#29702;&#24819;&#31574;&#30053;&#19982;&#26368;&#20248;&#37096;&#20998;&#21487;&#35266;&#23519;&#31574;&#30053;&#20043;&#38388;&#24314;&#31435;&#20102;&#22238;&#25253;&#24046;&#36317;&#30340;&#19978;&#30028;&#12290;&#35813;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#37325;&#26032;&#23450;&#20041;&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26412;&#22320;&#35266;&#23519;&#20013;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#28040;&#24687;&#20316;&#20026;&#32858;&#31867;&#26631;&#31614;&#65292;&#24182;&#19988;&#22238;&#25253;&#24046;&#36317;&#30340;&#19978;&#30028;&#20316;&#20026;&#32858;&#31867;&#25439;&#22833;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#19978;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#22320;&#31616;&#21333;&#30340;&#28040;&#24687;&#29983;&#25104;&#20989;&#25968;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is crucial for solving cooperative Multi-Agent Reinforcement Learning tasks in Partially-Observable Markov Decision Processes. Existing works often rely on black-box methods to encode local information/features into messages shared with other agents. However, such black-box approaches are unable to provide any quantitative guarantees on the expected return and often lead to the generation of continuous messages with high communication overhead and poor interpretability. In this paper, we establish an upper bound on the return gap between an ideal policy with full observability and an optimal partially-observable policy with discrete communication. This result enables us to recast multi-agent communication into a novel online clustering problem over the local observations at each agent, with messages as cluster labels and the upper bound on the return gap as clustering loss. By minimizing the upper bound, we propose a surprisingly simple design of message generation functi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#23545;&#22810;&#26679;&#21270;&#30340;&#33258;&#25105;&#32416;&#27491;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#34892;&#20026;&#12290;&#33258;&#21160;&#21270;&#21453;&#39304;&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#23454;&#29992;&#21644;&#21487;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2308.03188</link><description>&lt;p&gt;
&#33258;&#21160;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#22810;&#26679;&#21270;&#33258;&#25105;&#32416;&#27491;&#31574;&#30053;&#30340;&#27010;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. (arXiv:2308.03188v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#23545;&#22810;&#26679;&#21270;&#30340;&#33258;&#25105;&#32416;&#27491;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#34892;&#20026;&#12290;&#33258;&#21160;&#21270;&#21453;&#39304;&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#23454;&#29992;&#21644;&#21487;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21151;&#25928;&#21463;&#21040;&#20102;&#19981;&#21463;&#27426;&#36814;&#21644;&#19981;&#19968;&#33268;&#30340;&#34892;&#20026;&#30340;&#21066;&#24369;&#65292;&#21253;&#25324;&#24187;&#35273;&#12289;&#19981;&#24544;&#23454;&#30340;&#25512;&#29702;&#21644;&#26377;&#23475;&#20869;&#23481;&#12290;&#32416;&#27491;&#36825;&#20123;&#32570;&#38519;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26159;&#33258;&#25105;&#32416;&#27491;&#65292;&#21363;&#24341;&#23548;&#25110;&#25351;&#23548;LLM&#33258;&#34892;&#20462;&#22797;&#36755;&#20986;&#38382;&#39064;&#12290;&#21033;&#29992;&#33258;&#21160;&#21453;&#39304;&#30340;&#25216;&#26415;--&#26080;&#35770;&#26159;&#30001;LLM&#33258;&#36523;&#20135;&#29983;&#36824;&#26159;&#30001;&#26576;&#20010;&#22806;&#37096;&#31995;&#32479;&#20135;&#29983;--&#23588;&#20854;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20351;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#23454;&#38469;&#21644;&#21487;&#37096;&#32626;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#24335;&#65292;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#12290;&#26412;&#25991;&#23545;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#20998;&#31867;&#20102;&#35768;&#22810;&#26368;&#36817;&#21033;&#29992;&#36825;&#20123;&#31574;&#30053;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#35757;&#32451;&#26102;&#12289;&#29983;&#25104;&#26102;&#21644;&#20107;&#21518;&#32416;&#27491;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#36825;&#19968;&#31574;&#30053;&#30340;&#20027;&#35201;&#24212;&#29992;&#65292;&#24182;&#22312;&#26368;&#21518;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02562</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32852;&#21512;&#34920;&#31034;&#36827;&#34892;&#39135;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Food Classification using Joint Representation of Visual and Textual Data. (arXiv:2308.02562v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#20998;&#31867;&#26159;&#20581;&#24247;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#21516;&#26102;&#20351;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;UPMC Food-101&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#31532;&#20108;&#26368;&#22909;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;Google Bard&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#21508;&#31181;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#36825;&#20984;&#26174;&#20986;&#22312;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.15016</link><description>&lt;p&gt;
Google Bard&#30340;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#22914;&#20309;&#65311;&#24320;&#25918;&#25361;&#25112;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;Google Bard&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#21508;&#31181;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#36825;&#20984;&#26174;&#20986;&#22312;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Google&#30340;Bard&#22312;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#19982;OpenAI&#30340;ChatGPT&#25104;&#20026;&#20102;&#24378;&#22823;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Bard&#26368;&#36817;&#24050;&#32463;&#26356;&#26032;&#65292;&#21487;&#20197;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#22788;&#29702;&#25991;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#36755;&#20837;&#12290;&#37492;&#20110;Bard&#22312;&#22788;&#29702;&#25991;&#26412;&#36755;&#20837;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#30001;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#65288;&#22270;&#20687;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#25506;&#32034;&#26377;&#28508;&#21147;&#25581;&#31034;Bard&#21644;&#20854;&#20182;&#21363;&#23558;&#21457;&#24067;&#30340;&#22810;&#27169;&#24335;&#29983;&#25104;&#27169;&#22411;&#22312;&#35299;&#20915;&#38656;&#35201;&#20934;&#30830;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#22797;&#26434;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#26102;&#30340;&#26032;&#35265;&#35299;&#21644;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;15&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#22330;&#26223;&#65292;&#21253;&#25324;&#24120;&#35268;&#12289;&#20266;&#35013;&#12289;&#21307;&#23398;&#12289;&#27700;&#19979;&#21644;&#36965;&#24863;&#25968;&#25454;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;Bard&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#34920;&#26126;&#65292;Bard&#22312;&#36825;&#20123;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#31361;&#26174;&#20102;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#30340;&#37325;&#35201;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.07522</link><description>&lt;p&gt;
&#30001;&#29983;&#25104;&#38381;&#29615;&#20154;&#24037;&#26234;&#33021;&#24341;&#39046;&#30340;&#22522;&#30784;&#31185;&#23398;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07522
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#27491;&#22312;&#39072;&#35206;&#25216;&#26415;&#21019;&#26032;&#12289;&#20135;&#21697;&#24320;&#21457;&#21644;&#25972;&#20010;&#31038;&#20250;&#12290;&#20154;&#24037;&#26234;&#33021;&#23545;&#25216;&#26415;&#30340;&#36129;&#29486;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#36884;&#24452;&#23454;&#29616;&#65292;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#26126;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#26631;&#20934;&#65292;&#33539;&#22260;&#20174;&#27169;&#24335;&#35782;&#21035;&#21644;&#20998;&#31867;&#21040;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31185;&#23398;&#23454;&#36341;&#21644;&#27169;&#22411;&#21457;&#29616;&#38656;&#35201;&#35775;&#38382;&#39640;&#36136;&#37327;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#22522;&#30784;&#31185;&#23398;&#30340;&#36129;&#29486;&#36739;&#23569;&#12290;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33021;&#20195;&#34920;&#20102;&#36890;&#36807;&#23450;&#37327;&#27169;&#22411;&#22686;&#24378;&#21644;&#21152;&#36895;&#22522;&#30784;&#28145;&#24230;&#31185;&#23398;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#30740;&#31350;&#20102;&#19968;&#31181;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#12289;&#33258;&#21160;&#21270;&#30340;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#26041;&#27861;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#24320;&#25918;&#24335;&#33258;&#20027;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Int
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#26032;&#25163;&#22312;&#22522;&#20110;&#35821;&#38899;&#30340;3D&#24314;&#27169;&#20013;&#30340;&#24515;&#26234;&#27169;&#22411;&#65292;&#24182;&#20026;&#35821;&#38899;&#21161;&#25163;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35774;&#35745;&#21551;&#31034;&#65292;&#20363;&#22914;&#22788;&#29702;&#27169;&#31946;&#12289;&#19981;&#23436;&#25972;&#21644;&#38169;&#35823;&#30340;&#21629;&#20196;&#65292;&#25552;&#20379;&#31616;&#21333;&#30340;&#21629;&#20196;&#26469;&#22609;&#36896;&#23545;&#35937;&#65292;&#20197;&#21450;&#36873;&#25321;3D&#23545;&#35937;&#30340;&#19981;&#21516;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.04481</link><description>&lt;p&gt;
&#27599;&#20010;&#20154;&#30340;&#25968;&#23383;&#24314;&#27169;&#65306;&#25506;&#32034;&#26032;&#25163;&#22914;&#20309;&#36827;&#34892;&#22522;&#20110;&#35821;&#38899;&#30340;3D&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Digital Modeling for Everyone: Exploring How Novices Approach Voice-Based 3D Modeling. (arXiv:2307.04481v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#26032;&#25163;&#22312;&#22522;&#20110;&#35821;&#38899;&#30340;3D&#24314;&#27169;&#20013;&#30340;&#24515;&#26234;&#27169;&#22411;&#65292;&#24182;&#20026;&#35821;&#38899;&#21161;&#25163;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35774;&#35745;&#21551;&#31034;&#65292;&#20363;&#22914;&#22788;&#29702;&#27169;&#31946;&#12289;&#19981;&#23436;&#25972;&#21644;&#38169;&#35823;&#30340;&#21629;&#20196;&#65292;&#25552;&#20379;&#31616;&#21333;&#30340;&#21629;&#20196;&#26469;&#22609;&#36896;&#23545;&#35937;&#65292;&#20197;&#21450;&#36873;&#25321;3D&#23545;&#35937;&#30340;&#19981;&#21516;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#24037;&#20855;&#22914;3D&#25171;&#21360;&#26426;&#24050;&#32463;&#21464;&#24471;&#21487;&#20379;&#26356;&#24191;&#27867;&#30340;&#31038;&#20250;&#20351;&#29992;&#65292;&#25968;&#23383;&#21046;&#36896;&#23545;&#27599;&#20010;&#20154;&#30340;&#25215;&#35834;&#20284;&#20046;&#21487;&#20197;&#23454;&#29616;&#12290;&#23613;&#31649;&#23454;&#38469;&#30340;&#21046;&#36896;&#36807;&#31243;&#22312;&#20170;&#22825;&#26159;&#22823;&#37096;&#20998;&#33258;&#21160;&#21270;&#30340;&#65292;&#20294;&#29992;&#25143;&#20173;&#28982;&#38656;&#35201;&#25484;&#25569;&#22797;&#26434;&#30340;&#35774;&#35745;&#24212;&#29992;&#31243;&#24207;&#65292;&#20197;&#21046;&#20316;&#20986;&#20934;&#22791;&#22909;&#30340;&#35774;&#35745;&#23545;&#35937;&#65292;&#24182;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#35843;&#25972;&#65292;&#25110;&#32773;&#20174;&#38646;&#24320;&#22987;&#35774;&#35745;&#26032;&#30340;&#23545;&#35937;&#12290;&#20026;&#20102;&#38477;&#20302;&#20010;&#24615;&#21270;3D&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#23450;&#21046;&#38376;&#27099;&#65292;&#25105;&#20204;&#36890;&#36807;&#39640;&#20445;&#30495;Wizard of Oz&#30740;&#31350;&#19982;22&#21517;&#21442;&#19982;&#32773;&#19968;&#36215;&#25506;&#32034;&#20102;&#26032;&#25163;&#30340;&#24515;&#26234;&#27169;&#22411;&#22312;&#22522;&#20110;&#35821;&#38899;&#30340;3D&#24314;&#27169;&#20013;&#30340;&#20307;&#29616;&#12290;&#25105;&#20204;&#23545;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#20027;&#39064;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#26032;&#25163;&#30340;&#24515;&#26234;&#27169;&#22411;&#22914;&#20309;&#36716;&#21270;&#20026;&#22522;&#20110;&#35821;&#38899;&#30340;3D&#24314;&#27169;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#29992;&#20110;&#35821;&#38899;&#21161;&#25163;&#30340;&#35774;&#35745;&#21551;&#31034;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#24517;&#39035;&#22788;&#29702;&#27169;&#31946;&#12289;&#19981;&#23436;&#25972;&#21644;&#38169;&#35823;&#30340;&#21629;&#20196;&#65307;&#25552;&#20379;&#19968;&#32452;&#30452;&#25509;&#30340;&#21629;&#20196;&#26469;&#22609;&#36896;&#31616;&#21333;&#21644;&#22797;&#21512;&#23545;&#35937;&#65307;&#24182;&#25552;&#20379;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#36873;&#25321;3D&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing tools like 3D printers have become accessible to the wider society, making the promise of digital fabrication for everyone seemingly reachable. While the actual manufacturing process is largely automated today, users still require knowledge of complex design applications to produce ready-designed objects and adapt them to their needs or design new objects from scratch. To lower the barrier to the design and customization of personalized 3D models, we explored novice mental models in voice-based 3D modeling by conducting a high-fidelity Wizard of Oz study with 22 participants. We performed a thematic analysis of the collected data to understand how the mental model of novices translates into voice-based 3D modeling. We conclude with design implications for voice assistants. For example, they have to: deal with vague, incomplete and wrong commands; provide a set of straightforward commands to shape simple and composite objects; and offer different strategies to select 3D ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.03512</link><description>&lt;p&gt;
&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#32771;&#21476;&#30740;&#31350;&#20013;&#30340;&#36965;&#24863;&#25968;&#25454;&#26102;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#38556;&#30861;&#26159;&#36866;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#32463;&#24120;&#34987;&#29992;&#26469;&#20943;&#36731;&#36825;&#20010;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#24517;&#35201;&#25506;&#32034;&#22312;&#19981;&#21516;&#32771;&#21476;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20256;&#36755;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#20004;&#20010;&#35821;&#20041;&#20998;&#21106;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#20010;LiDAR&#25968;&#25454;&#38598;&#19978;&#30340;&#21508;&#31181;&#20256;&#36755;&#23398;&#20064;&#37197;&#32622;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20256;&#36755;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#32771;&#21476;&#23398;&#20013;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#23613;&#31649;&#23578;&#26410;&#35266;&#23519;&#21040;&#31995;&#32479;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27492;&#31867;&#25216;&#26415;&#26377;&#25928;&#24615;&#30340;&#20855;&#20307;&#35265;&#35299;&#65292;&#21487;&#20316;&#20026;&#26410;&#26469;&#24037;&#20316;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applying deep learning to remote sensing data in archaeological research, a notable obstacle is the limited availability of suitable datasets for training models. The application of transfer learning is frequently employed to mitigate this drawback. However, there is still a need to explore its effectiveness when applied across different archaeological datasets. This paper compares the performance of various transfer learning configurations using two semantic segmentation deep neural networks on two LiDAR datasets. The experimental results indicate that transfer learning-based approaches in archaeology can lead to performance improvements, although a systematic enhancement has not yet been observed. We provide specific insights about the validity of such techniques that can serve as a baseline for future works.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#30340;&#28023;&#27915;&#27915;&#27969;&#20013;&#26368;&#22823;&#21270;&#28023;&#34299;&#29983;&#38271;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#32447;&#24615;&#26102;&#21464;&#30340;&#27915;&#27969;&#23454;&#29616;&#39640;&#29983;&#38271;&#21306;&#22495;&#30340;&#25506;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.01916</link><description>&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#30340;&#28023;&#27915;&#27915;&#27969;&#20013;&#36827;&#34892;&#21160;&#21147;&#32534;&#31243;&#30340;&#26080;&#25928;&#31995;&#32479;&#19978;&#30340;&#33258;&#20027;&#20892;&#22330;&#19978;&#30340;&#28023;&#34299;&#29983;&#38271;&#30340;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Maximizing Seaweed Growth on Autonomous Farms: A Dynamic Programming Approach for Underactuated Systems Navigating on Uncertain Ocean Currents. (arXiv:2307.01916v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01916
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#30340;&#28023;&#27915;&#27915;&#27969;&#20013;&#26368;&#22823;&#21270;&#28023;&#34299;&#29983;&#38271;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#32447;&#24615;&#26102;&#21464;&#30340;&#27915;&#27969;&#23454;&#29616;&#39640;&#29983;&#38271;&#21306;&#22495;&#30340;&#25506;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#34299;&#29983;&#29289;&#37327;&#22312;&#27668;&#20505;&#20943;&#32531;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#33258;&#20027;&#24320;&#25918;&#24335;&#28023;&#27915;&#20892;&#22330;&#26469;&#20805;&#20998;&#21033;&#29992;&#12290;&#36825;&#20123;&#20892;&#22330;&#36890;&#24120;&#20855;&#26377;&#20302;&#25512;&#36827;&#21147;&#65292;&#24182;&#21463;&#21040;&#28023;&#27915;&#27915;&#27969;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#35774;&#35745;&#19968;&#20010;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#32447;&#24615;&#26102;&#21464;&#30340;&#28023;&#27915;&#27915;&#27969;&#26469;&#36798;&#21040;&#39640;&#29983;&#38271;&#21306;&#22495;&#65292;&#20174;&#32780;&#22312;&#20960;&#20010;&#26376;&#20869;&#26368;&#22823;&#21270;&#28023;&#34299;&#29983;&#38271;&#12290;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#21644;&#26080;&#25928;&#24615;&#20351;&#24471;&#21363;&#20351;&#30693;&#36947;&#27915;&#27969;&#24773;&#20917;&#65292;&#36825;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24403;&#21482;&#26377;&#30701;&#26399;&#19981;&#23436;&#21892;&#30340;&#39044;&#27979;&#19988;&#19981;&#30830;&#23450;&#24615;&#36880;&#28176;&#22686;&#22823;&#26102;&#65292;&#24773;&#20917;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24050;&#30693;&#30495;&#23454;&#27915;&#27969;&#24773;&#20917;&#26102;&#26377;&#25928;&#22320;&#27714;&#35299;&#26368;&#20248;&#29983;&#38271;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#20010;&#25193;&#23637;&#65292;&#21363;&#22312;&#29616;&#23454;&#20013;&#21482;&#30693;&#36947;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65306;&#65288;1&#65289;&#25105;&#20204;&#26041;&#27861;&#24471;&#21040;&#30340;&#20540;&#20989;&#25968;&#21487;&#20197;&#20316;&#20026;&#21453;&#39304;&#31574;&#30053;&#65292;&#20197;&#33719;&#24471;&#25152;&#26377;&#29366;&#24577;&#21644;&#26102;&#38388;&#30340;&#26368;&#20339;&#29983;&#38271;&#25511;&#21046;&#65292;&#23454;&#29616;&#38381;&#29615;&#25511;&#21046;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seaweed biomass offers significant potential for climate mitigation, but large-scale, autonomous open-ocean farms are required to fully exploit it. Such farms typically have low propulsion and are heavily influenced by ocean currents. We want to design a controller that maximizes seaweed growth over months by taking advantage of the non-linear time-varying ocean currents for reaching high-growth regions. The complex dynamics and underactuation make this challenging even when the currents are known. This is even harder when only short-term imperfect forecasts with increasing uncertainty are available. We propose a dynamic programming-based method to efficiently solve for the optimal growth value function when true currents are known. We additionally present three extensions when as in reality only forecasts are known: (1) our methods resulting value function can be used as feedback policy to obtain the growth-optimal control for all states and times, allowing closed-loop control equival
&lt;/p&gt;</description></item><item><title>HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06154</link><description>&lt;p&gt;
HypLL: &#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
HypLL: The Hyperbolic Learning Library. (arXiv:2306.06154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06154
&lt;/p&gt;
&lt;p&gt;
HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#22810;&#23186;&#20307;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#65292;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#27491;&#36805;&#36895;&#24341;&#36215;&#20851;&#27880;&#12290;&#28145;&#24230;&#32593;&#32476;&#36890;&#24120;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#38544;&#21547;&#22320;&#20551;&#35774;&#25968;&#25454;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#26102;&#65292;&#24076;&#20122;&#20960;&#20309;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21487;&#35775;&#38382;&#30340;&#24320;&#28304;&#24211;&#29992;&#20110;&#26500;&#24314;&#31867;&#20284;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HypLL, &#21363;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#20197;&#23558;&#24076;&#20122;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;HypLL&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#29305;&#21035;&#24378;&#35843;&#20854;&#26131;&#29992;&#24615;&#35774;&#35745;&#65292;&#20197;&#21560;&#24341;&#24191;&#27867;&#30340;&#21463;&#20247;&#20851;&#27880;&#36825;&#20010;&#26032;&#30340;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/maxvanspengler/hyperbolic_learning_library&#12290;&#21387;&#32553;&#25991;&#20214;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://d
&lt;/p&gt;
&lt;p&gt;
Deep learning in hyperbolic space is quickly gaining traction in the fields of machine learning, multimedia, and computer vision. Deep networks commonly operate in Euclidean space, implicitly assuming that data lies on regular grids. Recent advances have shown that hyperbolic geometry provides a viable alternative foundation for deep learning, especially when data is hierarchical in nature and when working with few embedding dimensions. Currently however, no accessible open-source library exists to build hyperbolic network modules akin to well-known deep learning libraries. We present HypLL, the Hyperbolic Learning Library to bring the progress on hyperbolic deep learning together. HypLL is built on top of PyTorch, with an emphasis in its design for easy-of-use, in order to attract a broad audience towards this new and open-ended research direction. The code is available at: https://github.com/maxvanspengler/hyperbolic_learning_library. The compressed archive is available at: https://d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#32467;&#26500;&#65292;&#23558;&#21453;&#21521;&#20256;&#25773;&#21644;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#23398;&#20064;&#21644;&#24341;&#23548;&#27880;&#24847;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02415</link><description>&lt;p&gt;
&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#23558;&#21453;&#21521;&#20256;&#25773;&#19982;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Top-Down Network Combines Back-Propagation with Attention. (arXiv:2306.02415v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#32467;&#26500;&#65292;&#23558;&#21453;&#21521;&#20256;&#25773;&#21644;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#23398;&#20064;&#21644;&#24341;&#23548;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22312;&#35270;&#35273;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#30340;&#30382;&#23618;&#22788;&#29702;&#23558;&#33258;&#19979;&#32780;&#19978;&#30340;&#22788;&#29702;&#19982;&#24191;&#27867;&#30340;&#33258;&#39030;&#21521;&#19979;&#22788;&#29702;&#30456;&#32467;&#21512;&#12290;&#33258;&#39030;&#21521;&#19979;&#22788;&#29702;&#30340;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#21644;&#24341;&#23548;&#27880;&#24847;&#21147;&#12290;&#30446;&#21069;&#30340;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#30340;&#26426;&#21046;&#23454;&#29616;&#36825;&#20004;&#20010;&#20316;&#29992;&#12290;&#27880;&#24847;&#21147;&#24341;&#23548;&#36890;&#24120;&#36890;&#36807;&#25193;&#23637;&#27169;&#22411;&#30340;&#32467;&#26500;&#26469;&#23454;&#29616;&#65292;&#32780;&#23398;&#20064;&#36890;&#24120;&#36890;&#36807;&#22806;&#37096;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#21453;&#21521;&#20256;&#25773;&#65289;&#26469;&#23454;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#19978;&#36848;&#20004;&#20010;&#30475;&#20284;&#26080;&#20851;&#30340;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#20154;&#33041;&#21551;&#21457;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#31216;&#33258;&#19979;&#32780;&#19978;&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#23558;&#20256;&#32479;&#30340;&#33258;&#19979;&#32780;&#19978;&#32593;&#32476;&#19982;&#23545;&#31216;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#27599;&#20010;&#32593;&#32476;&#21487;&#20197;&#20114;&#30456;&#24490;&#29615;&#22320;&#24341;&#23548;&#21644;&#24433;&#21709;&#23545;&#26041;&#12290;&#20363;&#22914;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#65292;&#21516;&#19968;&#20010;&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#34987;&#29992;&#20110;&#23398;&#20064;&#21644;&#36890;&#36807;&#20256;&#36882;&#21453;&#39304;&#20449;&#21495;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cortical processing, in vision and other domains, combines bottom-up (BU) with extensive top-down (TD) processing. Two primary goals attributed to TD processing are learning and directing attention. These two roles are accomplished in current network models through distinct mechanisms. Attention guidance is often implemented by extending the model's architecture, while learning is typically accomplished by an external learning algorithm such as back-propagation. In the current work, we present an integration of the two functions above, which appear unrelated, using a single unified mechanism inspired by the human brain. We propose a novel symmetric bottom-up top-down network structure that can integrate conventional bottom-up networks with a symmetric top-down counterpart, allowing each network to recurrently guide and influence the other. For example, during multi-task learning, the same top-down network is being used for both learning, via propagating feedback signals, and at the sam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01762</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#32431;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#37096;&#32626;&#20026;&#21508;&#31181;&#26085;&#24120;&#26381;&#21153;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36867;&#36991;&#25915;&#20987;&#26159;&#26368;&#26222;&#36941;&#30340;&#19968;&#31181;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25110;&#21033;&#29992;&#22823;&#37327;&#28165;&#27905;&#25968;&#25454;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#37096;&#32626;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#22312;&#32447;&#26381;&#21153;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#24403;&#26816;&#27979;&#21040;&#26576;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#26102;&#65292;&#26381;&#21153;&#25552;&#20379;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#32780;&#22823;&#37327;&#30340;&#28165;&#27905;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#21517;&#20026;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#26088;&#22312;&#24555;&#36895;&#38450;&#24481;&#20855;&#26377;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#38480;&#21046;&#30340;&#21407;&#22987;&#26381;&#21153;&#27169;&#22411;&#30340;&#26576;&#31181;&#25915;&#20987;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#36716;&#31227;&#23398;&#20064;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;Transformer&#26469;&#25552;&#32431;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#39044;&#35757;&#32451;&#30340;Transformer&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#40723;&#21169;&#25552;&#32431;&#21518;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#25509;&#36817;&#28165;&#26224;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RaPiD&#22312;&#38450;&#24481;&#21508;&#31181;&#20855;&#26377;&#38480;&#25968;&#25454;&#30340;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#26102;&#23384;&#22312;&#30340;&#31995;&#32479;&#20559;&#24046;&#65292;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#20505;&#36873;&#21709;&#24212;&#30340;&#39034;&#24207;&#26469;&#25805;&#32437;&#35780;&#20272;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#35777;&#25454;&#26657;&#20934;&#12289;&#22343;&#34913;&#20301;&#32622;&#26657;&#20934;&#21644;&#20154;&#26426;&#21327;&#21516;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.17926</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#19981;&#26159;&#20844;&#24179;&#30340;&#35780;&#20272;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are not Fair Evaluators. (arXiv:2305.17926v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#26102;&#23384;&#22312;&#30340;&#31995;&#32479;&#20559;&#24046;&#65292;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#20505;&#36873;&#21709;&#24212;&#30340;&#39034;&#24207;&#26469;&#25805;&#32437;&#35780;&#20272;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#35777;&#25454;&#26657;&#20934;&#12289;&#22343;&#34913;&#20301;&#32622;&#26657;&#20934;&#21644;&#20154;&#26426;&#21327;&#21516;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#37319;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65288;&#20363;&#22914;GPT-4&#65289;&#20316;&#20026;&#35009;&#21028;&#26469;&#35780;&#20998;&#21644;&#27604;&#36739;&#20505;&#36873;&#27169;&#22411;&#29983;&#25104;&#30340;&#21709;&#24212;&#36136;&#37327;&#30340;&#35780;&#20272;&#33539;&#24335;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#25913;&#21464;&#20505;&#36873;&#21709;&#24212;&#22312;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#39034;&#24207;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#25805;&#32437;&#20505;&#36873;&#21709;&#24212;&#30340;&#36136;&#37327;&#25490;&#21517;&#12290;&#36825;&#31181;&#25805;&#32437;&#20351;&#24471;&#19968;&#20010;&#27169;&#22411;&#30475;&#36215;&#26469;&#27604;&#21478;&#19968;&#20010;&#27169;&#22411;&#35201;&#20248;&#36234;&#24471;&#22810;&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;ChatGPT&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#22312;80&#20010;&#27979;&#35797;&#26597;&#35810;&#20013;&#65292;Vicuna-13B&#21487;&#20197;&#20987;&#36133;ChatGPT&#30340;66&#20010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65306;1&#65289;&#22810;&#35777;&#25454;&#26657;&#20934;&#65292;&#35201;&#27714;&#35780;&#20272;&#27169;&#22411;&#22312;&#20998;&#37197;&#35780;&#20998;&#20043;&#21069;&#29983;&#25104;&#22810;&#20010;&#35780;&#20272;&#35777;&#25454;&#65307;2&#65289;&#22343;&#34913;&#20301;&#32622;&#26657;&#20934;&#65292;&#22312;&#21508;&#31181;&#39034;&#24207;&#20013;&#32858;&#21512;&#32467;&#26524;&#20197;&#30830;&#23450;&#26368;&#32456;&#20998;&#25968;&#65307;3&#65289;&#20154;&#26426;&#21327;&#21516;&#26657;&#20934;&#65292;&#24341;&#20837;&#24179;&#34913;&#30340;&#20301;&#32622;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17680</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3&#29983;&#25104;&#30340;&#20167;&#24680;&#20869;&#23481;&#23457;&#26680;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;Fine-tune&#25110;&#25552;&#31034;&#29983;&#25104;&#20167;&#24680;&#35328;&#35770;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#36825;&#20123;&#29983;&#25104;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#38480;&#21046;&#20173;&#28982;&#19981;&#20026;&#20154;&#20204;&#25152;&#20102;&#35299;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#30001;LLMs&#29983;&#25104;&#30340;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#20869;&#23481;&#23457;&#26680;&#21592;&#23545;&#26631;&#35760;&#20869;&#23481;&#26412;&#36136;&#20570;&#20986;&#38169;&#35823;&#21028;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#26816;&#26597;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35843;&#26597;&#26469;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;GPT-3&#19978;&#36755;&#20837;&#20167;&#24680;&#21644;&#38750;&#20167;&#24680;&#20869;&#23481;&#65292;&#21457;&#29616;&#21463;&#35843;&#26597;&#32773;&#22312;&#20154;&#24037;&#23457;&#26680;GPT&#29983;&#25104;&#30340;&#35299;&#37322;&#26102;&#65292;&#23558;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#35780;&#20215;&#20026;&#19981;&#22815;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35762;&#36848;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;Sensecape&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25903;&#25345;&#22797;&#26434;&#30340;&#20449;&#24687;&#20219;&#21153;&#65292;&#24110;&#21161;&#29992;&#25143;&#36890;&#36807;&#22810;&#32423;&#25277;&#35937;&#31649;&#29702;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#35268;&#21010;&#21644;&#30693;&#35782;&#24314;&#26500;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#24687;&#32452;&#32455;&#21644;&#25506;&#32034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11483</link><description>&lt;p&gt;
Sensecape&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22810;&#23618;&#27425;&#25506;&#32034;&#21644;&#30693;&#35782;&#24314;&#26500;
&lt;/p&gt;
&lt;p&gt;
Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models. (arXiv:2305.11483v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11483
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35762;&#36848;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;Sensecape&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25903;&#25345;&#22797;&#26434;&#30340;&#20449;&#24687;&#20219;&#21153;&#65292;&#24110;&#21161;&#29992;&#25143;&#36890;&#36807;&#22810;&#32423;&#25277;&#35937;&#31649;&#29702;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#35268;&#21010;&#21644;&#30693;&#35782;&#24314;&#26500;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#24687;&#32452;&#32455;&#21644;&#25506;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#20449;&#24687;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#23398;&#26415;&#30740;&#31350;&#25110;&#35745;&#21010;&#25644;&#21040;&#21478;&#19968;&#20010;&#22478;&#24066;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#38750;&#32447;&#24615;&#24037;&#20316;&#26041;&#24335;&#65292;&#20363;&#22914;&#23558;&#20449;&#24687;&#22312;&#31354;&#38388;&#19978;&#36827;&#34892;&#25490;&#24067;&#20197;&#32452;&#32455;&#24182;&#29702;&#35299;&#23427;&#65292;&#20294;&#30446;&#21069;&#19982;LLM&#20132;&#20114;&#30340;&#30028;&#38754;&#36890;&#24120;&#26159;&#32447;&#24615;&#30340;&#65292;&#20197;&#25903;&#25345;&#23545;&#35805;&#24335;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#24182;&#25506;&#32034;&#22914;&#20309;&#25903;&#25345;LLM-powered&#30340;&#25506;&#32034;&#21644;&#30693;&#35782;&#24314;&#26500;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sensecape&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#25143;&#33021;&#22815;&#65288;1&#65289;&#36890;&#36807;&#22810;&#32423;&#25277;&#35937;&#26469;&#31649;&#29702;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#65292;&#65288;2&#65289;&#26080;&#32541;&#22320;&#22312;&#35268;&#21010;&#21644;&#30693;&#35782;&#24314;&#26500;&#20043;&#38388;&#20999;&#25442;&#65292;&#20197;&#25903;&#25345;LLM&#36827;&#34892;&#22797;&#26434;&#20449;&#24687;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#34987;&#35797;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;Sensecape&#20351;&#29992;&#25143;&#33021;&#22815;&#25506;&#32034;&#26356;&#22810;&#30340;&#20027;&#39064;&#24182;&#20197;&#20998;&#23618;&#32467;&#26500;&#32452;&#32455;&#20182;&#20204;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#20026;&#22522;&#20110;LLM&#30340;&#24037;&#20316;&#27969;&#31243;&#21644;&#20449;&#24687;&#20219;&#21153;&#30028;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#21644;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
People are increasingly turning to large language models (LLMs) for complex information tasks like academic research or planning a move to another city. However, while they often require working in a nonlinear manner - e.g., to arrange information spatially to organize and make sense of it, current interfaces for interacting with LLMs are generally linear to support conversational interaction. To address this limitation and explore how we can support LLM-powered exploration and sensemaking, we developed Sensecape, an interactive system designed to support complex information tasks with an LLM by enabling users to (1) manage the complexity of information through multilevel abstraction and (2) seamlessly switch between foraging and sensemaking. Our within-subject user study reveals that Sensecape empowers users to explore more topics and structure their knowledge hierarchically. We contribute implications for LLM-based workflows and interfaces for information tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.10235</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#30340;&#38544;&#34255;&#39118;&#38505;&#65306;&#20851;&#20110;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#24320;&#25918;&#24335;&#29615;&#22659;&#65288;&#22914;API&#12289;&#24320;&#28304;&#27169;&#22411;&#21644;&#25554;&#20214;&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#32570;&#20047;&#20840;&#38754;&#35752;&#35770;&#21644;&#20998;&#26512;&#28508;&#22312;&#39118;&#38505;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#20294;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;LLMs&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#26469;&#22788;&#29702;&#22823;&#37327;&#26597;&#35810;/&#21709;&#24212;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;ChatGPT&#12289;LLaMA&#21644;OPT&#22312;&#20869;&#30340;&#20027;&#27969;LLMs&#36827;&#34892;&#20102;100&#22810;&#19975;&#20010;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#26680;&#24515;&#21253;&#25324;&#25968;&#25454;&#21407;&#35821;&#65292;&#38543;&#21518;&#26159;&#33258;&#21160;&#35299;&#37322;&#22120;&#65292;&#35780;&#20272;&#36825;&#20123;LLMs&#22312;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#24230;&#37327;&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#12289;&#20063;&#35768;&#26159;&#19981;&#24184;&#30340;&#32467;&#35770;&#65292;&#36825;&#20123;&#32467;&#35770;&#30456;&#24403;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#31505;&#33080;&#24207;&#21015;&#65292;&#20197;&#22635;&#34917;&#38750;&#35821;&#35328;&#20132;&#27969;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.08854</link><description>&lt;p&gt;
&#31505;&#22768;&#24456;&#37325;&#35201;&#65306;&#24341;&#20837;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#31505;&#33080;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models. (arXiv:2305.08854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#31505;&#33080;&#24207;&#21015;&#65292;&#20197;&#22635;&#34917;&#38750;&#35821;&#35328;&#20132;&#27969;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#35821;&#38899;&#30340;&#21160;&#30011;&#22312;&#25552;&#39640;&#30495;&#23454;&#24863;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#38750;&#35821;&#35328;&#20132;&#27969;&#26041;&#38754;&#20173;&#28982;&#24456;&#23569;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#38745;&#24577;&#32918;&#20687;&#21644;&#21253;&#21547;&#31505;&#22768;&#30340;&#38899;&#39057;&#21098;&#36753;&#29983;&#25104;&#36924;&#30495;&#30340;&#31505;&#33080;&#24207;&#21015;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20256;&#32479;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#30340;&#22833;&#36133;&#26696;&#20363;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#30340;&#31505;&#33080;&#35270;&#39057;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#21508;&#26679;&#30340;&#31505;&#22768;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#31505;&#22768;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Speech-driven animation has gained significant traction in recent years, with current methods achieving near-photorealistic results. However, the field remains underexplored regarding non-verbal communication despite evidence demonstrating its importance in human interaction. In particular, generating laughter sequences presents a unique challenge due to the intricacy and nuances of this behaviour. This paper aims to bridge this gap by proposing a novel model capable of generating realistic laughter sequences, given a still portrait and an audio clip containing laughter. We highlight the failure cases of traditional facial animation methods and leverage recent advances in diffusion models to produce convincing laughter videos. We train our model on a diverse set of laughter datasets and introduce an evaluation metric specifically designed for laughter. When compared with previous speech-driven approaches, our model achieves state-of-the-art performance across all metrics, even when the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10755</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;&#33041;&#30005;&#22270;AI&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Robust AI in EEG Systems: A Survey. (arXiv:2304.10755v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#23494;&#20999;&#32806;&#21512;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;EEG&#31995;&#32479;&#65292;&#22522;&#20110;AI&#30340;EEG&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#21464;&#24471;&#23588;&#20026;&#20851;&#38190;&#12290;&#21487;&#35299;&#37322;&#24615;&#33021;&#22815;&#38416;&#37322;AI&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#65292;&#22240;&#27492;&#21487;&#20197;&#33719;&#24471;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#40065;&#26834;&#24615;&#21017;&#21453;&#26144;&#20102;AI&#23545;&#25239;&#25915;&#20987;&#21644;&#25200;&#21160;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#23545;&#20110;&#25935;&#24863;&#21644;&#33030;&#24369;&#30340;EEG&#20449;&#21495;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;EEG&#31995;&#32479;&#20013;AI&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20173;&#28982;&#27809;&#26377;&#32508;&#36848;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#21270;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#36755;&#20986;&#35299;&#37322;&#24615;&#65292;&#24635;&#32467;&#20102;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The close coupling of artificial intelligence (AI) and electroencephalography (EEG) has substantially advanced human-computer interaction (HCI) technologies in the AI era. Different from traditional EEG systems, the interpretability and robustness of AI-based EEG systems are becoming particularly crucial. The interpretability clarifies the inner working mechanisms of AI models and thus can gain the trust of users. The robustness reflects the AI's reliability against attacks and perturbations, which is essential for sensitive and fragile EEG signals. Thus the interpretability and robustness of AI in EEG systems have attracted increasing attention, and their research has achieved great progress recently. However, there is still no survey covering recent advances in this field. In this paper, we present the first comprehensive survey and summarize the interpretable and robust AI techniques for EEG systems. Specifically, we first propose a taxonomy of interpretability by characterizing it 
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07543</link><description>&lt;p&gt;
WDiscOOD&#65306;&#36890;&#36807;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#21306;&#20998;&#24230;&#20248;&#21270;&#30340;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#22312;&#36935;&#21040;&#26410;&#30693;&#27010;&#24565;&#30340;&#24773;&#24418;&#19979;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#20010;&#25361;&#25112;&#31361;&#26174;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#31354;&#38388;OOD&#26816;&#27979;&#20998;&#25968;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#31867;&#21035;&#29305;&#23450;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#8212;&#8212;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#65292;&#20854;&#20013;ID&#31867;&#22312;&#21028;&#21035;&#23376;&#31354;&#38388;&#20013;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#65292;&#24182;&#22312;&#27531;&#24046;&#23376;&#31354;&#38388;&#20013;&#34987;&#32039;&#23494;&#22320;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#22312;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#23558;&#26469;&#33258;&#36755;&#20837;&#25968;&#25454;&#19982;ID&#20998;&#24067;&#30340;&#20559;&#24046;&#32452;&#21512;&#36215;&#26469;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WDiscOOD&#65292;&#22312;&#35206;&#30422;&#22810;&#31181;&#20998;&#24067;&#20559;&#31227;&#30340;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#12290;WDiscOOD&#22312;&#28145;&#24230;&#20998;&#31867;&#22120;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#26041;&#38754;&#25152;&#21462;&#24471;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04150</link><description>&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Reinforcement Learning: A Survey. (arXiv:2303.04150v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04150
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#26041;&#38754;&#25152;&#21462;&#24471;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#35757;&#32451;&#26234;&#33021;&#20307;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#26827;&#30424;&#28216;&#25103;&#12289;&#34903;&#26426;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#31561;&#21508;&#31181;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#25935;&#24863;&#36229;&#21442;&#25968;&#23548;&#33268;&#30340;&#33030;&#24369;&#25910;&#25947;&#29305;&#24615;&#65292;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#26102;&#38388;&#20998;&#37197;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#24615;&#25506;&#32034;&#19981;&#36275;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#22256;&#38590;&#20197;&#21450;&#22870;&#21169;&#20914;&#31361;&#30446;&#26631;&#12290;&#36827;&#21270;&#35745;&#31639;&#32500;&#25252;&#30528;&#19968;&#32676;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24050;&#23637;&#29616;&#20986;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#38598;&#25104;&#36827;&#21270;&#35745;&#31639;&#30340;&#26368;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, there remain several crucial challenges, including brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, especially in continuous search space scenarios, difficulties in credit assignment in multi-agent reinforcement learning, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EvHandPose&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#30417;&#30563;&#19979;&#30340;&#26032;&#22411;&#25163;&#37096;&#27969;&#34920;&#31034;&#36827;&#34892;&#22522;&#20110;&#20107;&#20214;&#30340;3D&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#36816;&#21160;&#27169;&#31946;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#30495;&#23454;&#21512;&#25104;&#39046;&#22495;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.02862</link><description>&lt;p&gt;
EvHandPose: &#20351;&#29992;&#31232;&#30095;&#30417;&#30563;&#36827;&#34892;&#22522;&#20110;&#20107;&#20214;&#30340;3D&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision. (arXiv:2303.02862v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EvHandPose&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#30417;&#30563;&#19979;&#30340;&#26032;&#22411;&#25163;&#37096;&#27969;&#34920;&#31034;&#36827;&#34892;&#22522;&#20110;&#20107;&#20214;&#30340;3D&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#36816;&#21160;&#27169;&#31946;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#30495;&#23454;&#21512;&#25104;&#39046;&#22495;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#22312;3D&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#22312;&#35299;&#20915;&#24555;&#36895;&#36816;&#21160;&#21644;&#39640;&#21160;&#24577;&#33539;&#22260;&#30340;&#25361;&#25112;&#20197;&#20302;&#21151;&#32791;&#26041;&#24335;&#22788;&#29702;&#26102;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#27493;&#24046;&#20998;&#25104;&#20687;&#26426;&#21046;&#65292;&#35774;&#35745;&#20107;&#20214;&#34920;&#31034;&#26469;&#32534;&#30721;&#25163;&#37096;&#36816;&#21160;&#20449;&#24687;&#23588;&#20854;&#26159;&#24403;&#25163;&#37096;&#19981;&#21160;&#26102;&#65288;&#23548;&#33268;&#36816;&#21160;&#27169;&#31946;&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#19981;&#21487;&#33021;&#23545;&#26102;&#38388;&#23494;&#38598;&#30340;&#20107;&#20214;&#27969;&#36827;&#34892;&#23436;&#20840;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvHandPose&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#21644;&#20943;&#36731;&#36816;&#21160;&#27169;&#31946;&#38382;&#39064;&#30340;&#26032;&#22411;&#25163;&#37096;&#27969;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#31232;&#30095;&#27880;&#37322;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;Pose-to-IWE&#65288;&#20855;&#26377;&#25197;&#26354;&#20107;&#20214;&#30340;&#22270;&#20687;&#65289;&#27169;&#22359;&#20013;&#35774;&#35745;&#20102;&#23545;&#27604;&#24230;&#26368;&#22823;&#21270;&#21644;&#25163;&#36793;&#32536;&#32422;&#26463;&#65292;&#24182;&#23558;EvHandPose&#26500;&#24314;&#20026;&#24369;&#30417;&#30563;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;EvRealHands&#65292;&#39318;&#20010;&#38024;&#23545;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#22330;&#26223;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#20107;&#20214;&#39537;&#21160;&#25163;&#21183;&#23039;&#21183;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#30495;&#23454;&#21512;&#25104;&#39046;&#22495;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event camera shows great potential in 3D hand pose estimation, especially addressing the challenges of fast motion and high dynamic range in a low-power way. However, due to the asynchronous differential imaging mechanism, it is challenging to design event representation to encode hand motion information especially when the hands are not moving (causing motion ambiguity), and it is infeasible to fully annotate the temporally dense event stream. In this paper, we propose EvHandPose with novel hand flow representations in Event-to-Pose module for accurate hand pose estimation and alleviating the motion ambiguity issue. To solve the problem under sparse annotation, we design contrast maximization and hand-edge constraints in Pose-to-IWE (Image with Warped Events) module and formulate EvHandPose in a weakly-supervision framework. We further build EvRealHands, the first large-scale real-world event-based hand pose dataset on several challenging scenes to bridge the real-synthetic domain gap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;STAR&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;Answer Set Programming&#30456;&#32467;&#21512;&#65292;&#20197;&#36798;&#21040;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#24212;&#23545;&#38656;&#35201;&#25512;&#29702;&#30340;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.03780</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;Answer Set Programming&#23454;&#29616;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Reliable Natural Language Understanding with Large Language Models and Answer Set Programming. (arXiv:2302.03780v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;STAR&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;Answer Set Programming&#30456;&#32467;&#21512;&#65292;&#20197;&#36798;&#21040;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#24212;&#23545;&#38656;&#35201;&#25512;&#29702;&#30340;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#20174;&#21477;&#23376;&#20013;&#25552;&#21462;&#20449;&#24687;&#65288;&#24847;&#20041;&#65289;&#65292;&#23558;&#20854;&#19982;&#24050;&#26377;&#30340;&#24120;&#35782;&#30693;&#35782;&#32467;&#21512;&#65292;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#29702;&#35299;&#35821;&#35328;&#12290;&#34429;&#28982;&#20687;GPT-3&#21644;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#25991;&#26412;&#20013;&#30340;&#27169;&#24335;&#26469;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#22312;&#38656;&#35201;&#25512;&#29702;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#23427;&#20204;&#20063;&#26080;&#27861;&#21487;&#38752;&#22320;&#35299;&#37322;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAR&#26694;&#26550;&#65292;&#23558;LLMs&#19982;Answer Set Programming (ASP) &#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;LLMs&#26377;&#25928;&#22320;&#20174;&#35821;&#35328;&#20013;&#25552;&#21462;&#20197;&#35859;&#35789;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#20351;&#29992;&#30446;&#26631;&#23548;&#21521;&#30340;ASP&#26469;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#23558;STAR&#26694;&#26550;&#24212;&#29992;&#20110;&#38656;&#35201;&#25512;&#29702;&#30340;&#19977;&#20010;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65306;&#23450;&#24615;&#25512;&#29702;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;STAR&#33021;&#22815;&#22635;&#34917;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#24046;&#36317;&#65292;&#25552;&#20379;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans understand language by extracting information (meaning) from sentences, combining it with existing commonsense knowledge, and then performing reasoning to draw conclusions. While large language models (LLMs) such as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a variety of NLP tasks, they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question. In order to emulate humans better, we propose STAR, a framework that combines LLMs with Answer Set Programming (ASP). We show how LLMs can be used to effectively extract knowledge -- represented as predicates -- from language. Goal-directed ASP is then employed to reliably reason over this knowledge. We apply the STAR framework to three different NLU tasks requiring reasoning: qualitative reasoning, mathematical reasoning, and goal-directed conversation. Our experiments reveal that STAR is able to bridge the gap of reasoning in NLU tasks, leading t
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#23398;&#20064;&#24207;&#21015;&#21644;&#24050;&#21457;&#29616;&#31243;&#24207;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#33258;&#24049;&#21457;&#29616;&#20102;&#36229;&#36807;78000&#20010;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#65292;&#26377;&#26102;&#36824;&#24320;&#21457;&#20986;&#38750;&#20256;&#32479;&#30340;&#32534;&#31243;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.11479</link><description>&lt;p&gt;
&#22806;&#26143;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Alien Coding. (arXiv:2301.11479v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11479
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#23398;&#20064;&#24207;&#21015;&#21644;&#24050;&#21457;&#29616;&#31243;&#24207;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#33258;&#24049;&#21457;&#29616;&#20102;&#36229;&#36807;78000&#20010;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#65292;&#26377;&#26102;&#36824;&#24320;&#21457;&#20986;&#38750;&#20256;&#32479;&#30340;&#32534;&#31243;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#12290;&#35813;&#31639;&#27861;&#26368;&#21021;&#38543;&#26426;&#29983;&#25104;&#31243;&#24207;&#65292;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26469;&#23398;&#20064;&#24207;&#21015;&#21644;&#24050;&#21457;&#29616;&#31243;&#24207;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#21518;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#20026;&#27599;&#20010;OEIS&#24207;&#21015;&#25552;&#20986;&#35768;&#22810;&#26032;&#31243;&#24207;&#12290;&#35813;&#31639;&#27861;&#33258;&#24049;&#21457;&#29616;&#20102;&#36229;&#36807;78000&#20010;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#65292;&#26377;&#26102;&#24320;&#21457;&#20986;&#38750;&#20256;&#32479;&#30340;&#32534;&#31243;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23454;&#39564;&#20013;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#34892;&#20026;&#21644;&#21457;&#26126;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a self-learning algorithm for synthesizing programs for OEIS sequences. The algorithm starts from scratch initially generating programs at random. Then it runs many iterations of a self-learning loop that interleaves (i) training neural machine translation to learn the correspondence between sequences and the programs discovered so far, and (ii) proposing many new programs for each OEIS sequence by the trained neural machine translator. The algorithm discovers on its own programs for more than 78000 OEIS sequences, sometimes developing unusual programming methods. We analyze its behavior and the invented programs in several experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;&#65292;&#35774;&#35745;&#20102;&#19968;&#22871;&#36947;&#24503;&#22870;&#21169;&#32467;&#26500;&#65292;&#26088;&#22312;&#20998;&#26512;&#21644;&#30740;&#31350;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2301.08491</link><description>&lt;p&gt;
&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning. (arXiv:2301.08491v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;&#65292;&#35774;&#35745;&#20102;&#19968;&#22871;&#36947;&#24503;&#22870;&#21169;&#32467;&#26500;&#65292;&#26088;&#22312;&#20998;&#26512;&#21644;&#30740;&#31350;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#26234;&#33021;&#20195;&#29702;&#20013;&#32435;&#20837;&#36947;&#24503;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#23637;&#29616;&#12290;&#21516;&#26102;&#20063;&#24378;&#35843;&#65292;&#25353;&#29031;&#20219;&#20309;&#19968;&#31181;&#36947;&#24503;&#35266;&#23450;&#20041;&#39030;&#23618;&#30340;AI&#20262;&#29702;&#32422;&#26463;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#65292;&#24182;&#19988;&#20250;&#24102;&#26469;&#39118;&#38505;&#12290;&#20174;&#24213;&#23618;&#23398;&#20064;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25110;&#35768;&#26356;&#36866;&#21512;&#30740;&#31350;&#21644;&#24320;&#21457;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20998;&#26512;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#36947;&#24503;&#22870;&#21169;&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#23454;&#34892;&#34892;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26032;&#20852;&#34892;&#20026;&#26159;&#19968;&#20010;&#26377;&#36259;&#21644;&#23500;&#26377;&#27934;&#23519;&#21147;&#30340;&#36215;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26681;&#25454;&#36947;&#24503;&#29702;&#35770;&#30340;&#22870;&#21169;&#36827;&#34892;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31616;&#21270;&#20294;&#20195;&#34920;&#19968;&#32452;&#20851;&#38190;&#20262;&#29702;&#31995;&#32479;&#30340;&#22870;&#21169;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#21306;&#20998;&#21518;&#26524;&#21644;&#35268;&#33539;&#20262;&#29702;&#30340;&#36947;&#24503;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20197;&#21019;&#24314;&#26032;&#30340;&#22870;&#21169;&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#22312;&#31038;&#20250;&#22256;&#22659;&#19979;&#36827;&#34892;&#20869;&#22312;&#21160;&#26426;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35780;&#20272;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22797;&#21046;&#24182;&#25193;&#23637;&#26377;&#20851;&#36947;&#24503;&#36873;&#25321;&#30340;&#25991;&#29486;&#30740;&#31350;&#20013;&#30340;&#35768;&#22810;&#21457;&#29616;&#65292;&#24182;&#33021;&#22815;&#20986;&#29616;&#20197;&#21069;&#26410;&#26366;&#25253;&#36947;&#30340;&#26032;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.  In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm
&lt;/p&gt;</description></item><item><title>RecXplainer&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20998;&#25674;&#23646;&#24615;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#20043;&#38388;&#30340;&#20449;&#20219;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.14935</link><description>&lt;p&gt;
RecXplainer: &#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20998;&#25674;&#23646;&#24615;&#20010;&#24615;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RecXplainer: Amortized Attribute-based Personalized Explanations for Recommender Systems. (arXiv:2211.14935v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14935
&lt;/p&gt;
&lt;p&gt;
RecXplainer&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20998;&#25674;&#23646;&#24615;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#20043;&#38388;&#30340;&#20449;&#20219;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#24433;&#21709;&#30528;&#25105;&#20204;&#30340;&#35768;&#22810;&#20132;&#20114;&#65292;&#24433;&#21709;&#30528;&#25105;&#20204;&#36141;&#29289;&#12289;&#27983;&#35272;YouTube&#25110;TikTok&#26102;&#25152;&#30475;&#21040;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#22312;&#20351;&#29992;&#37202;&#24215;&#24179;&#21488;&#26102;&#23637;&#31034;&#32473;&#25105;&#20204;&#30340;&#39184;&#39302;&#21644;&#37202;&#24215;&#12290;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#26159;&#22522;&#20110;&#19987;&#26377;&#21644;&#24320;&#28304;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#24222;&#22823;&#19988;&#19981;&#36879;&#26126;&#30340;&#27169;&#22411;&#12290;&#33258;&#28982;&#32780;&#28982;&#22320;&#65292;&#22312;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#26041;&#38754;&#24341;&#21457;&#20102;&#20449;&#20219;&#38382;&#39064;&#65306;&#31995;&#32479;&#26159;&#21542;&#27491;&#24120;&#24037;&#20316;&#65292;&#20026;&#20160;&#20040;&#29992;&#25143;&#25910;&#21040;&#65288;&#25110;&#26410;&#25910;&#21040;&#65289;&#29305;&#23450;&#30340;&#25512;&#33616;&#65311;&#22312;&#25512;&#33616;&#26049;&#36793;&#25552;&#20379;&#35299;&#37322;&#21487;&#20197;&#20943;&#36731;&#19968;&#20123;&#36825;&#20123;&#20851;&#27880;&#12290;&#30446;&#21069;&#36741;&#21161;&#25512;&#33616;&#31995;&#32479;&#21453;&#39304;&#30340;&#29616;&#29366;&#35201;&#20040;&#26159;&#29992;&#25143;&#29305;&#23450;&#30340;&#35299;&#37322;&#65288;&#20363;&#22914;&#65292;&#8220;&#36141;&#20080;&#21830;&#21697;B&#30340;&#29992;&#25143;&#20063;&#36141;&#20080;&#20102;&#21830;&#21697;A&#8221;&#65289;&#65292;&#35201;&#20040;&#26159;&#29289;&#21697;&#29305;&#23450;&#30340;&#35299;&#37322;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#20204;&#25512;&#33616;&#21830;&#21697;A&#26159;&#22240;&#20026;&#24744;&#35266;&#30475;/&#36141;&#20080;&#20102;&#21830;&#21697;B&#8221;&#65289;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#23558;&#20010;&#24615;&#21270;&#30340;&#32972;&#26223;&#20449;&#24687;&#24102;&#20837;&#20182;&#20204;&#30340;&#25628;&#32034;&#20307;&#39564;&#20013;&#65292;&#23558;&#19968;&#20010;&#29289;&#21697;&#30340;&#20215;&#20540;&#35270;&#20026;&#35813;&#29289;&#21697;&#30340;&#20989;&#25968;.
&lt;/p&gt;
&lt;p&gt;
Recommender systems influence many of our interactions in the digital world -- impacting how we shop for clothes, sorting what we see when browsing YouTube or TikTok, and determining which restaurants and hotels we are shown when using hospitality platforms. Modern recommender systems are large, opaque models trained on a mixture of proprietary and open-source datasets. Naturally, issues of trust arise on both the developer and user side: is the system working correctly, and why did a user receive (or not receive) a particular recommendation? Providing an explanation alongside a recommendation alleviates some of these concerns. The status quo for auxiliary recommender system feedback is either user-specific explanations (e.g., "users who bought item B also bought item A") or item-specific explanations (e.g., "we are recommending item A because you watched/bought item B"). However, users bring personalized context into their search experience, valuing an item as a function of that item'
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UCN&#30340;&#26032;&#22411;&#22240;&#26524;&#27169;&#22411;&#65292;&#23427;&#32771;&#34385;&#20102;&#26102;&#38388;&#24310;&#36831;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#24471;&#21040;&#30340;&#32593;&#32476;&#32467;&#26500;&#30340;&#21807;&#19968;&#24615;&#65292;&#35299;&#20915;&#20102;&#22240;&#26524;&#35299;&#37322;&#24615;&#21644;&#38750;&#38745;&#24577;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.10085</link><description>&lt;p&gt;
&#20174;&#38750;&#38745;&#27490;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#29420;&#29305;&#30340;&#22240;&#26524;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Identifying Unique Causal Network from Nonstationary Time Series. (arXiv:2211.10085v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UCN&#30340;&#26032;&#22411;&#22240;&#26524;&#27169;&#22411;&#65292;&#23427;&#32771;&#34385;&#20102;&#26102;&#38388;&#24310;&#36831;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#24471;&#21040;&#30340;&#32593;&#32476;&#32467;&#26500;&#30340;&#21807;&#19968;&#24615;&#65292;&#35299;&#20915;&#20102;&#22240;&#26524;&#35299;&#37322;&#24615;&#21644;&#38750;&#38745;&#24577;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25968;&#25454;&#23494;&#38598;&#22411;&#22330;&#26223;&#19979;&#65292;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#27492;&#20851;&#38190;&#20219;&#21153;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#20165;&#32771;&#34385;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20123;&#22522;&#20110;BN&#30340;&#27169;&#22411;&#20165;&#20855;&#26377;&#26377;&#38480;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23384;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#38745;&#27490;&#24615;&#20551;&#35774;&#65292;&#32780;&#26469;&#33258;&#22797;&#26434;&#31995;&#32479;&#30340;&#35768;&#22810;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#26159;&#38750;&#38745;&#27490;&#30340;&#12290;&#38750;&#38745;&#27490;&#30340;&#26102;&#38388;&#24207;&#21015;&#24102;&#26469;&#20102;&#25968;&#25454;&#38598;&#28418;&#31227;&#38382;&#39064;&#65292;&#23548;&#33268;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Unique Causal Network&#65288;UCN&#65289;&#30340;&#26032;&#22411;&#22240;&#26524;&#27169;&#22411;&#12290;&#19982;&#20197;&#21069;&#30340;&#22522;&#20110;BN&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;UCN&#32771;&#34385;&#20102;&#26102;&#38388;&#24310;&#36831;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#24471;&#21040;&#30340;&#32593;&#32476;&#32467;&#26500;&#30340;&#21807;&#19968;&#24615;&#65292;&#35299;&#20915;&#20102;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;UCN&#30340;&#21487;&#20998;&#35299;&#24615;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#26356;&#39640;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Identifying causality is a challenging task in many data-intensive scenarios. Many algorithms have been proposed for this critical task. However, most of them consider the learning algorithms for directed acyclic graph (DAG) of Bayesian network (BN). These BN-based models only have limited causal explainability because of the issue of Markov equivalence class. Moreover, they are dependent on the assumption of stationarity, whereas many sampling time series from complex system are nonstationary. The nonstationary time series bring dataset shift problem, which leads to the unsatisfactory performances of these algorithms. To fill these gaps, a novel causation model named Unique Causal Network (UCN) is proposed in this paper. Different from the previous BN-based models, UCN considers the influence of time delay, and proves the uniqueness of obtained network structure, which addresses the issue of Markov equivalence class. Furthermore, based on the decomposability property of UCN, a higher-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.13455</link><description>&lt;p&gt;
E-MCTS&#65306;&#36890;&#36807;&#35268;&#21010;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#36864;&#28779;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#26368;&#24191;&#27867;&#12289;&#24615;&#33021;&#26368;&#20248;&#31168;&#30340;&#35268;&#21010;&#26041;&#27861;&#20043;&#19968;&#12290;MCTS&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#28145;&#24230;&#25506;&#32034;&#21644;&#38754;&#23545;&#26410;&#30693;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#20004;&#20010;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#32531;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;MCTS&#20013;&#20256;&#25773;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20272;&#35745;&#20854;&#39044;&#27979;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#25506;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;MCTS&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#21644;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36827;&#34892;&#20102;&#28145;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20110;&#38750;&#35268;&#21010;&#30340;&#28145;&#24230;&#25506;&#32034;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#26412;&#20307;&#27010;&#24565;&#36827;&#34892;&#37327;&#21270;&#21644;&#32858;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#19968;&#38454;&#36923;&#36753;&#65292;&#24182;&#39564;&#35777;&#20102;&#33391;&#24335;&#21477;&#23376;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#34920;&#31034;&#20013;&#33021;&#22815;&#20197;&#25193;&#23637;&#23481;&#24525;&#30340;&#26041;&#24335;&#20934;&#30830;&#24314;&#27169;&#21508;&#31181;&#38382;&#39064;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2202.00898</link><description>&lt;p&gt;
&#26412;&#20307;&#27010;&#24565;&#30340;&#37327;&#21270;&#21644;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Quantification and Aggregation over Concepts of the Ontology. (arXiv:2202.00898v4 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00898
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#26412;&#20307;&#27010;&#24565;&#36827;&#34892;&#37327;&#21270;&#21644;&#32858;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#19968;&#38454;&#36923;&#36753;&#65292;&#24182;&#39564;&#35777;&#20102;&#33391;&#24335;&#21477;&#23376;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#34920;&#31034;&#20013;&#33021;&#22815;&#20197;&#25193;&#23637;&#23481;&#24525;&#30340;&#26041;&#24335;&#20934;&#30830;&#24314;&#27169;&#21508;&#31181;&#38382;&#39064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#22312;&#19968;&#20123;&#30693;&#35782;&#34920;&#31034;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#23545;&#35789;&#27719;&#20013;&#31526;&#21495;&#24418;&#24335;&#34920;&#31034;&#30340;&#27010;&#24565;&#38598;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#37327;&#21270;&#19982;&#20108;&#38454;&#37327;&#21270;&#21644;&#20803;&#32534;&#31243;&#37327;&#21270;&#24212;&#35813;&#26377;&#25152;&#21306;&#21035;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19982;&#20869;&#28085;&#36923;&#36753;&#20013;&#30340;&#27010;&#24565;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;&#19968;&#38454;&#36923;&#36753;&#26469;&#25903;&#25345;&#36825;&#26679;&#30340;&#25277;&#35937;&#65292;&#24182;&#23637;&#31034;&#23427;&#20801;&#35768;&#32534;&#20889;&#23545;&#30693;&#35782;&#36827;&#34892;&#25193;&#23637;&#23481;&#24525;&#30340;&#34920;&#36798;&#24335;&#12290;&#20026;&#20102;&#36991;&#20813;&#24418;&#24335;&#20027;&#20041;&#20013;&#30340;&#26080;&#24847;&#20041;&#21477;&#23376;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33391;&#24335;&#21477;&#23376;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#33391;&#24335;&#24615;&#30340;&#26041;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#19982;&#20844;&#24335;&#20013;&#20196;&#29260;&#30340;&#25968;&#37327;&#32447;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#30456;&#24212;&#22320;&#25193;&#23637;&#20102;FO(.)&#65292;&#19968;&#31181;&#30693;&#35782;&#34920;&#31034;&#35821;&#35328;&#65292;&#21644;IDP-Z3&#65292;&#19968;&#31181;&#29992;&#20110;FO(.)&#30340;&#25512;&#29702;&#24341;&#25806;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#25193;&#23637;&#22312;&#20197;&#25193;&#23637;&#23481;&#24525;&#30340;&#26041;&#24335;&#65288;&#21363;&#27809;&#26377;&#23454;&#20307;&#21270;&#65289;&#20934;&#30830;&#24314;&#27169;&#21508;&#31181;&#38382;&#39064;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that in some KR applications, we want to quantify over sets of concepts formally represented by symbols in the vocabulary. We show that this quantification should be distinguished from second-order quantification and meta-programming quantification. We also investigate the relationship with concepts in intensional logic.  We present an extension of first-order logic to support such abstractions, and show that it allows writing expressions of knowledge that are elaboration tolerant. To avoid nonsensical sentences in this formalism, we refine the concept of well-formed sentences, and propose a method to verify well-formedness with a complexity that is linear with the number of tokens in the formula.  We have extended FO(.), a Knowledge Representation language, and IDP-Z3, a reasoning engine for FO(.), accordingly. We show that this extension was essential in accurately modelling various problem domains in an elaboration-tolerant way, i.e., without reification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;Temporal Difference&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#25511;&#21046;&#35770;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#26694;&#26550;&#65292;&#20174;&#25511;&#21046;&#35770;&#35282;&#24230;&#25552;&#20379;&#20102;&#23545;TD&#23398;&#20064;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2112.14417</link><description>&lt;p&gt;
Temporal Difference&#23398;&#20064;&#31639;&#27861;&#30340;&#25511;&#21046;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;Temporal Difference&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#25511;&#21046;&#35770;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#26694;&#26550;&#65292;&#20174;&#25511;&#21046;&#35770;&#35282;&#24230;&#25552;&#20379;&#20102;&#23545;TD&#23398;&#20064;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;Temporal Difference (TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#25511;&#21046;&#35770;&#20998;&#26512;&#12290;TD&#23398;&#20064;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#22522;&#30707;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;&#19982;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#32473;&#23450;&#31574;&#30053;&#30456;&#20851;&#30340;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#24050;&#23384;&#22312;&#22810;&#31687;&#20851;&#20110;TD&#23398;&#20064;&#29702;&#35770;&#29702;&#35299;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#30452;&#21040;&#26368;&#36817;&#20960;&#24180;&#65292;&#30740;&#31350;&#20154;&#21592;&#25165;&#33021;&#23545;&#20854;&#32479;&#35745;&#25928;&#29575;&#25552;&#20379;&#20855;&#20307;&#20445;&#35777;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#25511;&#21046;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;TD&#23398;&#20064;&#65292;&#20511;&#37492;&#20102;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#39046;&#22495;&#30340;&#24050;&#26377;&#27010;&#24565;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25511;&#21046;&#35770;&#23548;&#20986;&#30340;&#31616;&#21333;&#20998;&#26512;&#24037;&#20855;&#65292;&#20026;TD&#23398;&#20064;&#30340;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26356;&#24191;&#38420;&#39046;&#22495;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this manuscript is to conduct a controltheoretic analysis of Temporal Difference (TD) learning algorithms. TD-learning serves as a cornerstone in the realm of reinforcement learning, offering a methodology for approximating the value function associated with a given policy in a Markov Decision Process. Despite several existing works that have contributed to the theoretical understanding of TD-learning, it is only in recent years that researchers have been able to establish concrete guarantees on its statistical efficiency. In this paper, we introduce a finite-time, control-theoretic framework for analyzing TD-learning, leveraging established concepts from the field of linear systems control. Consequently, this paper provides additional insights into the mechanics of TD learning and the broader landscape of reinforcement learning, all while employing straightforward analytical tools derived from control theory.
&lt;/p&gt;</description></item><item><title>MetaCOG&#26159;&#19968;&#20010;&#23398;&#20064;&#20803;&#35748;&#30693;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21487;&#38752;&#24615;&#34920;&#31034;&#65292;&#22686;&#21152;&#20102;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#21453;&#39304;&#21644;&#22320;&#38754;&#30495;&#23454;&#30340;&#29289;&#20307;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2110.03105</link><description>&lt;p&gt;
MetaCOG: &#23398;&#20064;&#20803;&#35748;&#30693;&#20197;&#24674;&#22797;&#23454;&#38469;&#23384;&#22312;&#30340;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
MetaCOG: Learning a Metacognition to Recover What Objects Are Actually There. (arXiv:2110.03105v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03105
&lt;/p&gt;
&lt;p&gt;
MetaCOG&#26159;&#19968;&#20010;&#23398;&#20064;&#20803;&#35748;&#30693;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21487;&#38752;&#24615;&#34920;&#31034;&#65292;&#22686;&#21152;&#20102;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#21453;&#39304;&#21644;&#22320;&#38754;&#30495;&#23454;&#30340;&#29289;&#20307;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19981;&#20165;&#26681;&#25454;&#25105;&#20204;&#25152;&#30475;&#21040;&#30340;&#20869;&#23481;&#24418;&#25104;&#20851;&#20110;&#19990;&#30028;&#30340;&#34920;&#24449;&#65292;&#36824;&#23398;&#20064;&#20851;&#20110;&#25105;&#20204;&#33258;&#24049;&#35270;&#35273;&#22914;&#20309;&#24037;&#20316;&#30340;&#20803;&#35748;&#30693;&#34920;&#24449;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#25105;&#20204;&#30340;&#35270;&#35273;&#19981;&#21487;&#38752;&#65288;&#20363;&#22914;&#65292;&#24403;&#25105;&#20204;&#24847;&#35782;&#21040;&#25105;&#20204;&#27491;&#22312;&#32463;&#21382;&#35270;&#35273;&#38169;&#35273;&#26102;&#65289;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#25105;&#20204;&#25152;&#30475;&#21040;&#30340;&#20869;&#23481;&#25552;&#20986;&#36136;&#30097;&#12290;&#21463;&#21040;&#36825;&#31181;&#20154;&#31867;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaCOG&#65306;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20854;&#21487;&#38752;&#24615;&#34920;&#31034;&#26469;&#22686;&#21152;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaCOG&#26159;&#19968;&#20010;&#23618;&#27425;&#27010;&#29575;&#27169;&#22411;&#65292;&#23545;&#19968;&#20010;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#21644;&#26816;&#27979;&#22120;&#20135;&#29983;&#30340;&#36755;&#20986;&#34920;&#36798;&#20102;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;&#12290;&#24403;&#19982;&#29616;&#25104;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#37197;&#23545;&#20351;&#29992;&#26102;&#65292;MetaCOG&#23558;&#26816;&#27979;&#32467;&#26524;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25512;&#26029;&#20986;&#26816;&#27979;&#22120;&#38169;&#28431;&#26816;&#26576;&#20123;&#31867;&#21035;&#30340;&#29289;&#20307;&#21644;&#34394;&#26500;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#30340;&#20542;&#21521;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#30340;&#29289;&#20307;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans not only form representations about the world based on what we see, but also learn meta-cognitive representations about how our own vision works. This enables us to recognize when our vision is unreliable (e.g., when we realize that we are experiencing a visual illusion) and enables us to question what we see. Inspired by this human capacity, we present MetaCOG: a model that increases the robustness of object detectors by learning representations of their reliability, and does so without feedback. Specifically, MetaCOG is a hierarchical probabilistic model that expresses a joint distribution over the objects in a 3D scene and the outputs produced by a detector. When paired with an off-the-shelf object detector, MetaCOG takes detections as input and infers the detector's tendencies to miss objects of certain categories and to hallucinate objects that are not actually present, all without access to ground-truth object labels. When paired with three modern neural object detectors, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#32500;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#28246;&#20013;&#39640;&#25928;&#22320;&#21457;&#29616;&#21487;&#36830;&#25509;&#34920;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#20540;&#23884;&#20837;&#39640;&#32500;&#21521;&#37327;&#24182;&#20351;&#29992;&#30456;&#20284;&#24615;&#35859;&#35789;&#36830;&#25509;&#21015;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#31561;&#20540;&#36830;&#25509;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#35782;&#21035;&#20986;&#26356;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22810;&#30340;&#21487;&#36830;&#25509;&#34920;&#12290;</title><link>http://arxiv.org/abs/2010.13273</link><description>&lt;p&gt;
&#25968;&#25454;&#28246;&#20013;&#39640;&#32500;&#30456;&#20284;&#24230;&#20026;&#22522;&#30784;&#30340;&#39640;&#25928;&#21487;&#36830;&#25509;&#34920;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Joinable Table Discovery in Data Lakes: A High-Dimensional Similarity-Based Approach. (arXiv:2010.13273v4 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.13273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#32500;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#28246;&#20013;&#39640;&#25928;&#22320;&#21457;&#29616;&#21487;&#36830;&#25509;&#34920;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#20540;&#23884;&#20837;&#39640;&#32500;&#21521;&#37327;&#24182;&#20351;&#29992;&#30456;&#20284;&#24615;&#35859;&#35789;&#36830;&#25509;&#21015;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#31561;&#20540;&#36830;&#25509;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#35782;&#21035;&#20986;&#26356;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22810;&#30340;&#21487;&#36830;&#25509;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#28246;&#20013;&#23547;&#25214;&#21487;&#36830;&#25509;&#34920;&#26159;&#35768;&#22810;&#24212;&#29992;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#20363;&#22914;&#25968;&#25454;&#38598;&#25104;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#25968;&#25454;&#24066;&#22330;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#21482;&#33021;&#25214;&#21040;&#31561;&#20540;&#36830;&#25509;&#30340;&#34920;&#65292;&#26080;&#27861;&#22788;&#29702;&#25340;&#20889;&#38169;&#35823;&#21644;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#20063;&#26080;&#27861;&#25429;&#25417;&#21040;&#20219;&#20309;&#35821;&#20041;&#36830;&#25509;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEXESO&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#25968;&#25454;&#28246;&#20013;&#21457;&#29616;&#21487;&#36830;&#25509;&#34920;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#25991;&#26412;&#20540;&#23884;&#20837;&#21040;&#39640;&#32500;&#21521;&#37327;&#20013;&#65292;&#24182;&#22312;&#39640;&#32500;&#21521;&#37327;&#19978;&#20351;&#29992;&#30456;&#20284;&#24615;&#35859;&#35789;&#26469;&#36830;&#25509;&#21015;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31561;&#20540;&#36830;&#25509;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#20986;&#26356;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#25214;&#21040;&#20855;&#26377;&#30456;&#20284;&#24615;&#30340;&#21487;&#36830;&#25509;&#34920;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#21644;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20013;&#24515;&#28857;&#30340;&#36807;&#28388;&#25216;&#26415;&#12290;&#38024;&#23545;&#25968;&#25454;&#28246;&#24456;&#22823;&#19988;&#32034;&#24341;&#26080;&#27861;&#36866;&#24212;&#20027;&#20869;&#23384;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#21306;&#25216;&#26415;&#12290;&#23454;&#39564;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#31561;&#20540;&#36830;&#25509;&#21644;&#20844;&#20849;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#35782;&#21035;&#20986;&#26356;&#22810;&#30340;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding joinable tables in data lakes is key procedure in many applications such as data integration, data augmentation, data analysis, and data market. Traditional approaches that find equi-joinable tables are unable to deal with misspellings and different formats, nor do they capture any semantic joins. In this paper, we propose PEXESO, a framework for joinable table discovery in data lakes. We embed textual values as high-dimensional vectors and join columns under similarity predicates on high-dimensional vectors, hence to address the limitations of equi-join approaches and identify more meaningful results. To efficiently find joinable tables with similarity, we propose a block-and-verify method that utilizes pivot-based filtering. A partitioning technique is developed to cope with the case when the data lake is large and the index cannot fit in main memory. An experimental evaluation on real datasets shows that our solution identifies substantially more tables than equi-joins and o
&lt;/p&gt;</description></item><item><title>Coagent Networks&#65288;&#20849;&#26234;&#32593;&#32476;&#65289;&#26159;&#25351;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#21327;&#20316;&#30340;&#38543;&#26426;&#20195;&#29702;&#32593;&#32476;&#12290;&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20849;&#26234;&#32593;&#32476;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#25191;&#34892;&#36335;&#24452;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#36825;&#19968;&#24605;&#24819;&#23454;&#29616;&#20102;&#23545;&#31574;&#26799;&#24230;&#23450;&#29702;&#30340;&#31616;&#27905;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2001.10474</link><description>&lt;p&gt;
Coagent Networks&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Coagent Networks Revisited. (arXiv:2001.10474v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.10474
&lt;/p&gt;
&lt;p&gt;
Coagent Networks&#65288;&#20849;&#26234;&#32593;&#32476;&#65289;&#26159;&#25351;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#21327;&#20316;&#30340;&#38543;&#26426;&#20195;&#29702;&#32593;&#32476;&#12290;&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20849;&#26234;&#32593;&#32476;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#25191;&#34892;&#36335;&#24452;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#36825;&#19968;&#24605;&#24819;&#23454;&#29616;&#20102;&#23545;&#31574;&#26799;&#24230;&#23450;&#29702;&#30340;&#31616;&#27905;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Coagent networks&#65288;&#20849;&#26234;&#32593;&#32476;&#65289;&#24418;&#24335;&#21270;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#21327;&#20316;&#20197;&#37319;&#21462;&#34892;&#21160;&#30340;&#38543;&#26426;&#20195;&#29702;&#32593;&#32476;&#30340;&#27010;&#24565;&#12290;&#20849;&#26234;&#32593;&#32476;&#30340;&#26174;&#33879;&#24212;&#29992;&#21253;&#25324;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#20351;&#29992;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;HRL&#20195;&#29702;&#20013;&#20018;&#32852;&#22810;&#20010;&#38543;&#26426;&#32593;&#32476;&#24341;&#20837;&#19981;&#21516;&#23618;&#27425;&#30340;&#25277;&#35937;&#21160;&#20316;&#65292;&#26469;&#35299;&#20915;&#25506;&#32034;&#21033;&#29992;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22312;&#20849;&#26234;&#32593;&#32476;&#20013;&#24418;&#24335;&#21270;&#25191;&#34892;&#35268;&#21017;&#12289;&#36890;&#36807;&#20849;&#26234;&#32593;&#32476;&#20013;&#25191;&#34892;&#36335;&#24452;&#30340;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#25551;&#36848;&#35768;&#22810;&#19981;&#21516;&#30340;&#20363;&#23376;&#12290;&#22312;&#23618;&#27425;&#36873;&#39033;&#35780;&#35770;&#32773;&#26550;&#26500;&#20013;&#21463;&#21040;&#21442;&#25968;&#20849;&#20139;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20849;&#26234;&#32593;&#32476;&#29702;&#35770;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#25191;&#34892;&#36335;&#24452;&#24605;&#24819;&#24471;&#21040;&#20102;&#23545;&#31574;&#26799;&#24230;&#23450;&#29702;&#30340;&#26356;&#31616;&#27905;&#35777;&#26126;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#21442;&#25968;&#20849;&#20139;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coagent networks formalize the concept of arbitrary networks of stochastic agents that collaborate to take actions in a reinforcement learning environment. Prominent examples of coagent networks in action include approaches to hierarchical reinforcement learning (HRL), such as those using options, which attempt to address the exploration exploitation trade-off by introducing abstract actions at different levels by sequencing multiple stochastic networks within the HRL agents. We first provide a unifying perspective on the many diverse examples that fall under coagent networks. We do so by formalizing the rules of execution in a coagent network, enabled by the novel and intuitive idea of execution paths in a coagent network. Motivated by parameter sharing in the hierarchical option-critic architecture, we revisit the coagent network theory and achieve a much shorter proof of the policy gradient theorem using our idea of execution paths, without any assumption on how parameters are share
&lt;/p&gt;</description></item></channel></rss>