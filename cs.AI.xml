<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35270;&#28857;&#19968;&#33268;&#25193;&#25955;&#26041;&#27861;&#20013;&#25972;&#21512;&#19977;&#32500;&#21487;&#22609;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#29983;&#25104;&#19977;&#32500;&#19968;&#33268;&#12289;&#21487;&#21160;&#30011;&#21644;&#36924;&#30495;&#30340;&#20154;&#29289;&#21270;&#36523;&#30340;&#36136;&#37327;&#21644;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04728</link><description>&lt;p&gt;
&#21487;&#22609;&#24615;&#25193;&#25955;&#65306;&#21333;&#22270;&#20687;&#21270;&#36523;&#21019;&#24314;&#30340;&#19977;&#32500;&#19968;&#33268;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation. (arXiv:2401.04728v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35270;&#28857;&#19968;&#33268;&#25193;&#25955;&#26041;&#27861;&#20013;&#25972;&#21512;&#19977;&#32500;&#21487;&#22609;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#29983;&#25104;&#19977;&#32500;&#19968;&#33268;&#12289;&#21487;&#21160;&#30011;&#21644;&#36924;&#30495;&#30340;&#20154;&#29289;&#21270;&#36523;&#30340;&#36136;&#37327;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20174;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#25110;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#19977;&#32500;&#36164;&#20135;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#22312;&#21019;&#24314;&#21487;&#25511;&#12289;&#36924;&#30495;&#30340;&#20154;&#29289;&#21270;&#36523;&#20219;&#21153;&#20013;&#30340;&#36136;&#37327;&#21644;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19977;&#32500;&#21487;&#22609;&#27169;&#22411;&#25972;&#21512;&#21040;&#26368;&#20808;&#36827;&#30340;&#22810;&#35270;&#28857;&#19968;&#33268;&#25193;&#25955;&#26041;&#27861;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31934;&#30830;&#22320;&#23558;&#29983;&#25104;&#27969;&#31243;&#19982;&#20851;&#33410;&#19977;&#32500;&#27169;&#22411;&#30340;&#26465;&#20214;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#22522;&#20934;&#27169;&#22411;&#22312;&#21333;&#20010;&#22270;&#20687;&#30340;&#26032;&#35270;&#22270;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#25972;&#21512;&#20351;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#23039;&#21183;&#25511;&#21046;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26080;&#32541;&#20934;&#30830;&#22320;&#34701;&#20837;&#20854;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20174;&#26410;&#35265;&#36807;&#30340;&#20027;&#39064;&#30340;&#21333;&#20010;&#22270;&#20687;&#21019;&#24314;&#23436;&#20840;&#19977;&#32500;&#19968;&#33268;&#12289;&#21487;&#21160;&#30011;&#21644;&#36924;&#30495;&#30340;&#20154;&#29289;&#21270;&#36523;&#30340;&#25193;&#25955;&#27169;&#22411;&#65307;&#25193;&#23637;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; exte
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;ASSIRA&#29483;&#29399;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04666</link><description>&lt;p&gt;
ASSIRA&#29483;&#29399;&#25968;&#25454;&#38598;&#19978;&#21508;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#20934;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset. (arXiv:2401.04666v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;ASSIRA&#29483;&#29399;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#22522;&#26412;&#24212;&#29992;&#21644;&#23454;&#29616;&#65292;&#22270;&#20687;&#20998;&#31867;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#30693;&#21517;&#25968;&#25454;&#31185;&#23398;&#31038;&#21306;&#25552;&#20379;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#26469;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;ASSIRA&#29483;&#29399;&#25968;&#25454;&#38598;&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#24182;&#19988;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#34987;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#30340;&#25972;&#20307;&#25509;&#21463;&#24230;&#21644;&#22522;&#20934;&#26631;&#20934;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#20248;&#21270;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25913;&#21464;&#36229;&#21442;&#25968;&#20197;&#33719;&#24471;&#27169;&#22411;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#19981;&#23545;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#37325;&#22823;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#36816;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#25968;&#25454;&#38598;&#20197;&#20934;&#30830;&#29575;&#36229;&#36807;&#20808;&#21069;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the most basic application and implementation of deep learning, image classification has grown in popularity. Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. The ASSIRA Cats &amp; Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. Hyper-parameters are changed to gain the best result from a model. By applying this approach, we have got higher accuracy without major changes in the training model. To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;&#24179;&#38138;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37319;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#26041;&#27861;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;</title><link>http://arxiv.org/abs/2401.04658</link><description>&lt;p&gt;
Lightning Attention-2:&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22788;&#29702;&#26080;&#38480;&#24207;&#21015;&#38271;&#24230;&#30340;"&#20813;&#36153;&#21320;&#39184;"
&lt;/p&gt;
&lt;p&gt;
Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. (arXiv:2401.04658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;&#24179;&#38138;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37319;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#26041;&#27861;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#20256;&#32479;softmax&#27880;&#24847;&#21147;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#19978;&#33021;&#22815;&#22312;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22788;&#29702;&#26080;&#38480;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#36895;&#24230;&#65292;&#21363;&#22312;&#22266;&#23450;&#30340;&#20869;&#23384;&#28040;&#32791;&#19979;&#65292;&#33021;&#22815;&#20197;&#24658;&#23450;&#30340;&#35757;&#32451;&#36895;&#24230;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32047;&#31215;&#27714;&#21644;&#65288;cumsum&#65289;&#30340;&#38382;&#39064;&#65292;&#24403;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#31639;&#27861;&#26080;&#27861;&#22312;&#22240;&#26524;&#35774;&#32622;&#19979;&#23637;&#29616;&#20854;&#29702;&#35770;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#24179;&#38138;&#65288;tiling&#65289;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#26469;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38544;&#21547;&#29289;&#29702;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#27867;&#21270;&#36866;&#24212;&#31995;&#32479;&#36755;&#20837;&#12289;&#21442;&#25968;&#21644;&#22495;&#30340;&#21464;&#21270;&#65292;&#24182;&#22312;&#31995;&#32479;&#21457;&#29616;&#20013;&#23637;&#29616;&#20102;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04648</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#38544;&#21547;&#29289;&#29702;&#27169;&#22411;&#27867;&#21270;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for generalization of deep hidden physics models. (arXiv:2401.04648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38544;&#21547;&#29289;&#29702;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#27867;&#21270;&#36866;&#24212;&#31995;&#32479;&#36755;&#20837;&#12289;&#21442;&#25968;&#21644;&#22495;&#30340;&#21464;&#21270;&#65292;&#24182;&#22312;&#31995;&#32479;&#21457;&#29616;&#20013;&#23637;&#29616;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24037;&#31243;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#23545;&#20110;&#31995;&#32479;&#30340;&#24314;&#27169;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#23436;&#25972;&#30340;&#31995;&#32479;&#20449;&#24687;&#26159;&#26410;&#30693;&#30340;&#65292;&#35201;&#20040;&#26159;&#22240;&#20026;&#32771;&#34385;&#21040;&#25152;&#26377;&#28041;&#21450;&#30340;&#22797;&#26434;&#29289;&#29702;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#35201;&#20040;&#26159;&#20026;&#20102;&#22312;&#21487;&#29992;&#36164;&#28304;&#30340;&#38480;&#21046;&#20869;&#32771;&#34385;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#22312;&#28784;&#30418;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#22914;&#28145;&#24230;&#38544;&#21547;&#29289;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#25454;&#21644;&#29289;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#20026;&#27599;&#20010;&#31995;&#32479;&#36755;&#20837;&#21644;&#21442;&#25968;&#30340;&#24494;&#23567;&#21464;&#21270;&#25110;&#22495;&#37197;&#32622;&#30340;&#20462;&#25913;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#32463;&#27982;&#19978;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#38544;&#21547;&#29289;&#29702;&#27169;&#22411;&#24605;&#24819;&#30340;&#26032;&#25913;&#36827;&#65292;&#21487;&#20197;&#36866;&#24212;&#31995;&#32479;&#36755;&#20837;&#12289;&#21442;&#25968;&#21644;&#22495;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#31995;&#32479;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24110;&#21161;&#23398;&#20064;&#21040;&#20102;&#21464;&#21270;&#21518;&#31995;&#32479;&#36755;&#20837;&#12289;&#21442;&#25968;&#21644;&#22495;&#30340;&#38544;&#34255;&#29289;&#29702;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling of systems where the full system information is unknown is an oft encountered problem for various engineering and industrial applications, as it's either impossible to consider all the complex physics involved or simpler models are considered to keep within the limits of the available resources. Recent advances in greybox modelling like the deep hidden physics models address this space by combining data and physics. However, for most real-life applications, model generalizability is a key issue, as retraining a model for every small change in system inputs and parameters or modification in domain configuration can render the model economically unviable. In this work we present a novel enhancement to the idea of hidden physics models which can generalize for changes in system inputs, parameters and domains. We also show that this approach holds promise in system discovery as well and helps learn the hidden physics for the changed system inputs, parameters and domain configurat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#28155;&#21152;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#35299;&#37322;&#27169;&#22359;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#21516;&#26102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22359;&#26469;&#21306;&#20998;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04647</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25512;&#36827;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks. (arXiv:2401.04647v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#28155;&#21152;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#35299;&#37322;&#27169;&#22359;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#21516;&#26102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22359;&#26469;&#21306;&#20998;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#28155;&#21152;&#21040;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35299;&#37322;&#27169;&#22359;&#34987;&#20248;&#21270;&#20197;&#20174;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22359;&#21017;&#26088;&#22312;&#21306;&#20998;&#20174;&#27010;&#24565;&#20013;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#36825;&#31181;&#32852;&#21512;&#35757;&#32451;&#26041;&#26696;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23558;&#20854;&#20869;&#37096;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#20154;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#23646;&#24615;&#38544;&#24335;&#22320;&#23545;&#40784;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20135;&#29983;&#20102;&#36830;&#36143;&#30340;&#27010;&#24565;&#28608;&#27963;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#21327;&#35758;&#20013;&#30340;&#25200;&#21160;&#23545;&#20998;&#31867;&#21644;&#27010;&#24565;&#33719;&#21462;&#30340;&#24433;&#21709;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25512;&#36827;&#20102;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#25253;&#21578;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19979;&#20173;&#33021;&#20445;&#25345;&#21487;&#38752;&#24615;&#65292;&#20943;&#23569;&#20102;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04637</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;API&#24212;&#29992;&#20110;&#38382;&#39064;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Applying Large Language Models API to Issue Classification Problem. (arXiv:2401.04637v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#25253;&#21578;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19979;&#20173;&#33021;&#20445;&#25345;&#21487;&#38752;&#24615;&#65292;&#20943;&#23569;&#20102;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#65292;&#38382;&#39064;&#25253;&#21578;&#30340;&#26377;&#25928;&#25490;&#24207;&#23545;&#20110;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#21644;&#21450;&#26102;&#35299;&#20915;&#20851;&#38190;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#23545;&#38382;&#39064;&#25253;&#21578;&#36827;&#34892;&#20998;&#31867;&#20197;&#36827;&#34892;&#25490;&#24207;&#26159;&#36153;&#21147;&#19988;&#32570;&#20047;&#21487;&#20280;&#32553;&#24615;&#30340;&#12290;&#30456;&#21453;&#65292;&#35768;&#22810;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20351;&#29992;&#33258;&#21160;&#21270;&#27969;&#31243;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#23613;&#31649;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20805;&#20998;&#30340;&#35757;&#32451;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#22312;&#20351;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#20173;&#33021;&#30830;&#20445;&#38382;&#39064;&#25490;&#24207;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#30340;&#33021;&#21147;&#65292;&#35748;&#35782;&#21040;&#23427;&#20204;&#22312;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#26102;&#30340;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#20934;&#30830;&#20248;&#20808;&#32423;&#38382;&#39064;&#25253;&#21578;&#30340;&#21487;&#38752;&#31995;&#32479;&#65292;&#38477;&#20302;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#38752;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26631;&#35760;&#38382;&#39064;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly. However, the manual classification of issue reports for prioritization is laborious and lacks scalability. Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training. This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets. Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a reliable GPT-based approach to accurately labe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#65292;&#29992;&#20110;&#27700;&#27745;&#26579;&#30417;&#27979;&#12290;&#20351;&#29992;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#19981;&#21516;&#31354;&#38388;&#30456;&#20851;&#24615;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#21367;&#31215;&#31574;&#30053;&#20915;&#31574;&#26041;&#27861;&#26469;&#33719;&#24471;&#26377;&#25928;&#30340;&#30417;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#21452;&#37325;&#28145;&#24230; Q &#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#26234;&#33021;&#20307;&#20197;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.04631</link><description>&lt;p&gt;
&#29992;&#20110;&#27700;&#27745;&#26579;&#30417;&#27979;&#30340;&#28145;&#24230;&#24378;&#21270;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#21644;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Multi-agent Learning framework for Information Gathering with Local Gaussian Processes for Water Monitoring. (arXiv:2401.04631v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#65292;&#29992;&#20110;&#27700;&#27745;&#26579;&#30417;&#27979;&#12290;&#20351;&#29992;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#19981;&#21516;&#31354;&#38388;&#30456;&#20851;&#24615;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#21367;&#31215;&#31574;&#30053;&#20915;&#31574;&#26041;&#27861;&#26469;&#33719;&#24471;&#26377;&#25928;&#30340;&#30417;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#21452;&#37325;&#28145;&#24230; Q &#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#26234;&#33021;&#20307;&#20197;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#30417;&#27979;&#27700;&#36136;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#33258;&#20027;&#27700;&#38754;&#36710;&#32452;&#25104;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#33337;&#38431;&#30340;&#23433;&#20840;&#25511;&#21046;&#65292;&#33337;&#38431;&#31574;&#30053;&#24212;&#35813;&#33021;&#22815;&#22522;&#20110;&#27979;&#37327;&#32467;&#26524;&#21644;&#33337;&#38431;&#29366;&#24577;&#36827;&#34892;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#32852;&#21512;&#33719;&#24471;&#26377;&#25928;&#30340;&#30417;&#27979;&#31574;&#30053;&#12290;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#19981;&#21516;&#31354;&#38388;&#30456;&#20851;&#24615;&#20013;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#27700;&#36136;&#20449;&#24687;&#12290;&#25991;&#20013;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31574;&#30053;&#30340;&#20915;&#31574;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#36825;&#20010;&#27169;&#22411;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#65292;&#20351;&#29992;&#20449;&#24687;&#22686;&#30410;&#22870;&#21169;&#36827;&#34892;&#20915;&#31574;&#12290;&#36890;&#36807;&#21452;&#37325;&#28145;&#24230; Q &#23398;&#20064;&#31639;&#27861;&#65292;&#26234;&#33021;&#20307;&#34987;&#35757;&#32451;&#20197;&#22312;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#23613;&#37327;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conservation of hydrological resources involves continuously monitoring their contamination. A multi-agent system composed of autonomous surface vehicles is proposed in this paper to efficiently monitor the water quality. To achieve a safe control of the fleet, the fleet policy should be able to act based on measurements and to the the fleet state. It is proposed to use Local Gaussian Processes and Deep Reinforcement Learning to jointly obtain effective monitoring policies. Local Gaussian processes, unlike classical global Gaussian processes, can accurately model the information in a dissimilar spatial correlation which captures more accurately the water quality information. A Deep convolutional policy is proposed, that bases the decisions on the observation on the mean and variance of this model, by means of an information gain reward. Using a Double Deep Q-Learning algorithm, agents are trained to minimize the estimation error in a safe manner thanks to a Consensus-based heuristi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04621</link><description>&lt;p&gt;
DebugBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#32534;&#31243;&#33021;&#21147;&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20043;&#21069;&#23545;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#35780;&#20272;&#21463;&#21040;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12289;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#27979;&#35797;&#28431;&#27934;&#31181;&#31867;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;DebugBench&#8221;&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#21253;&#21547;4253&#20010;&#23454;&#20363;&#12290;&#23427;&#28085;&#30422;&#20102;C ++&#65292;Java&#21644;Python&#20013;&#22235;&#20010;&#20027;&#35201;&#30340;&#38169;&#35823;&#31867;&#21035;&#21644;18&#20010;&#27425;&#35201;&#31867;&#22411;&#12290;&#20026;&#20102;&#26500;&#24314;DebugBench&#65292;&#25105;&#20204;&#20174;LeetCode&#31038;&#21306;&#25910;&#38598;&#20102;&#20195;&#30721;&#29255;&#27573;&#65292;&#20351;&#29992;GPT-4&#21521;&#28304;&#25968;&#25454;&#20013;&#27880;&#20837;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#20005;&#26684;&#30340;&#36136;&#37327;&#26816;&#26597;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#20363;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#20004;&#20010;&#21830;&#19994;&#27169;&#22411;&#21644;&#19977;&#20010;&#24320;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#38381;&#28304;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#22914;Code Llama&#26080;&#27861;&#36798;&#21040;&#20219;&#20309;&#21512;&#26684;&#29575;&#65307;&#65288;2&#65289;t
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04620</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#30340;Agent&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Agent&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#20154;&#31867;&#29983;&#20135;&#21644;&#29983;&#27963;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20984;&#26174;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#30446;&#21069;AI&#31995;&#32479;&#30340;&#23545;&#40784;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20154;&#20026;&#24178;&#39044;&#23545;LLM&#36827;&#34892;&#34987;&#21160;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;Agent&#20855;&#26377;&#25509;&#21463;&#29615;&#22659;&#21453;&#39304;&#21644;&#33258;&#25105;&#36827;&#21270;&#31561;&#29305;&#24615;&#65292;&#20351;&#24471;LLM&#23545;&#40784;&#26041;&#27861;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;Agent&#36827;&#21270;&#21644;&#23545;&#40784;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#12290;&#22312;&#31038;&#20250;&#35268;&#33539;&#19981;&#26029;&#28436;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#65292;&#32780;&#23545;&#40784;&#19981;&#36275;&#30340;Agent&#21017;&#36880;&#28176;&#20943;&#23569;&#12290;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#23545;&#19982;&#31038;&#20250;&#35268;&#33539;&#30456;&#23545;&#40784;&#30340;Agent&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#35299;&#21078;&#22810;&#35270;&#22270;&#25968;&#25454;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#22270;&#32593;&#32476;&#65288;EMV-Net&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#19981;&#21516;&#35299;&#21078;&#35270;&#22270;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04579</link><description>&lt;p&gt;
&#20351;&#29992;&#35299;&#37322;&#22411;&#22810;&#35270;&#22270;&#25968;&#25454;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#28145;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Deep Network for Explainable Prediction of Non-Imaging Phenotypes using Anatomical Multi-View Data. (arXiv:2401.04579v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#35299;&#21078;&#22810;&#35270;&#22270;&#25968;&#25454;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#22270;&#32593;&#32476;&#65288;EMV-Net&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#19981;&#21516;&#35299;&#21078;&#35270;&#22270;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#25110;&#35270;&#22270;&#65292;&#36825;&#20123;&#35270;&#22270;&#25552;&#20379;&#20102;&#20114;&#34917;&#20449;&#24687;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#23427;&#20204;&#26469;&#25552;&#39640;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#21078;&#22810;&#35270;&#22270;&#25968;&#25454;&#65292;&#20854;&#20013;&#27599;&#20010;&#33041;&#35299;&#21078;&#32467;&#26500;&#29992;&#22810;&#20010;&#29305;&#24449;&#38598;&#25551;&#36848;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#26469;&#33258;&#25193;&#25955;MR&#65288;diffusion MRI&#65289;&#30340;&#30333;&#36136;&#24494;&#32467;&#26500;&#21644;&#36830;&#25509;&#29305;&#24449;&#38598;&#65292;&#20197;&#21450;&#26469;&#33258;&#32467;&#26500;MR&#65288;structural MRI&#65289;&#30340;&#28784;&#36136;&#38754;&#31215;&#21644;&#21402;&#24230;&#29305;&#24449;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24212;&#29992;&#22810;&#35270;&#22270;&#26041;&#27861;&#25913;&#36827;&#38750;&#25104;&#20687;&#34920;&#22411;&#65288;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#29305;&#24449;&#65288;&#24180;&#40836;&#65289;&#12289;&#36816;&#21160;&#65288;&#21147;&#37327;&#65289;&#21644;&#35748;&#30693;&#65288;&#35789;&#27719;&#22270;&#20687;&#65289;&#65289;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#22270;&#32593;&#32476;&#65288;EMV-Net&#65289;&#65292;&#21487;&#20197;&#21033;&#29992;&#19981;&#21516;&#30340;&#35299;&#21078;&#35270;&#22270;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#65292;&#27599;&#20010;&#20010;&#20307;&#35299;&#21078;&#35270;&#22270;&#37117;&#32463;&#36807;&#35270;&#22270;&#29305;&#23450;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#22788;&#29702;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#35270;&#22270;&#25552;&#21462;&#30340;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large datasets often contain multiple distinct feature sets, or views, that offer complementary information that can be exploited by multi-view learning methods to improve results. We investigate anatomical multi-view data, where each brain anatomical structure is described with multiple feature sets. In particular, we focus on sets of white matter microstructure and connectivity features from diffusion MRI, as well as sets of gray matter area and thickness features from structural MRI. We investigate machine learning methodology that applies multi-view approaches to improve the prediction of non-imaging phenotypes, including demographics (age), motor (strength), and cognition (picture vocabulary). We present an explainable multi-view network (EMV-Net) that can use different anatomical views to improve prediction performance. In this network, each individual anatomical view is processed by a view-specific feature extractor and the extracted information from each view is fused using a l
&lt;/p&gt;</description></item><item><title>MAGNeT&#26159;&#19968;&#31181;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;MAGNeT&#36824;&#25506;&#32034;&#20102;&#28151;&#21512;&#29256;&#26412;&#65292;&#21487;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04577</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#36974;&#34109;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
Masked Audio Generation using a Single Non-Autoregressive Transformer. (arXiv:2401.04577v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04577
&lt;/p&gt;
&lt;p&gt;
MAGNeT&#26159;&#19968;&#31181;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;MAGNeT&#36824;&#25506;&#32034;&#20102;&#28151;&#21512;&#29256;&#26412;&#65292;&#21487;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNeT&#30340;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#30452;&#25509;&#25805;&#20316;&#22810;&#20010;&#38899;&#39057;&#20196;&#29260;&#27969;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MAGNeT&#30001;&#21333;&#38454;&#27573;&#38750;&#33258;&#22238;&#24402;Transformer&#32452;&#25104;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#36974;&#34109;&#35745;&#21010;&#22120;&#39044;&#27979;&#36974;&#34109;&#20196;&#29260;&#30340;&#33539;&#22260;&#65292;&#32780;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36880;&#27493;&#26500;&#24314;&#36755;&#20986;&#24207;&#21015;&#20351;&#29992;&#22810;&#20010;&#35299;&#30721;&#27493;&#39588;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#21033;&#29992;&#22806;&#37096;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20998;&#21644;&#25490;&#21517;MAGNeT&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#23558;&#34987;&#29992;&#20110;&#21518;&#32493;&#30340;&#35299;&#30721;&#27493;&#39588;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;MAGNeT&#30340;&#28151;&#21512;&#29256;&#26412;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#21069;&#20960;&#31186;&#38047;&#65292;&#32780;&#20854;&#20313;&#30340;&#24207;&#21015;&#21017;&#20197;&#24182;&#34892;&#26041;&#24335;&#36827;&#34892;&#35299;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensi
&lt;/p&gt;</description></item><item><title>Let's Go Shopping (LGS) dataset is a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites, providing a more efficient way to collect and annotate images for vision and vision-language applications.</title><link>http://arxiv.org/abs/2401.04575</link><description>&lt;p&gt;
Let's Go Shopping (LGS) -- &#29992;&#20110;&#35270;&#35273;&#27010;&#24565;&#29702;&#35299;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding. (arXiv:2401.04575v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04575
&lt;/p&gt;
&lt;p&gt;
Let's Go Shopping (LGS) dataset is a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites, providing a more efficient way to collect and annotate images for vision and vision-language applications.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#23383;&#24149;&#65292;&#20381;&#36182;&#20110;&#38656;&#35201;&#38750;&#24179;&#20961;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#30340;&#22823;&#35268;&#27169;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#32791;&#26102;&#30340;&#21162;&#21147;&#38480;&#21046;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21482;&#33021;&#36873;&#25321;&#23569;&#25968;&#20960;&#31181;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23547;&#27714;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#21644;&#27880;&#37322;&#22270;&#20687;&#12290;&#20197;&#24448;&#30340;&#20513;&#35758;&#24050;&#32463;&#20174;HTML alt&#25991;&#26412;&#21644;&#29228;&#21462;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25910;&#38598;&#20102;&#23383;&#24149;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#28304;&#23384;&#22312;&#22122;&#22768;&#12289;&#31232;&#30095;&#25110;&#20027;&#35266;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36716;&#21521;&#21830;&#19994;&#36141;&#29289;&#32593;&#31449;&#65292;&#20854;&#25968;&#25454;&#31526;&#21512;&#19977;&#20010;&#26631;&#20934;&#65306;&#24178;&#20928;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#27969;&#30021;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Let's Go Shopping&#65288;LGS&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26469;&#33258;&#20844;&#24320;&#21487;&#29992;&#30340;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#30340;1500&#19975;&#20010;&#22270;&#20687;-&#23383;&#24149;&#23545;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#19982;&#29616;&#26377;&#30340;&#36890;&#29992;&#39046;&#22495;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;LGS&#22270;&#20687;&#20391;&#37325;&#20110;&#21069;&#26223;&#23545;&#35937;&#65292;&#32972;&#26223;&#22797;&#26434;&#24230;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04536</link><description>&lt;p&gt;
&#36890;&#36807;&#35848;&#21028;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#12289;&#32452;&#32455;&#21644;&#25919;&#24220;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#23637;&#31034;&#31867;&#20284;&#20195;&#29702;&#34892;&#20026;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#38543;&#30528;LM&#34987;&#37319;&#29992;&#26469;&#25191;&#34892;&#36234;&#26469;&#36234;&#20855;&#26377;&#33258;&#20027;&#24615;&#30340;&#20219;&#21153;&#65292;&#36843;&#20999;&#38656;&#35201;&#21487;&#38752;&#19988;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#24403;&#21069;&#20027;&#35201;&#26159;&#38745;&#24577;&#30340;LM&#22522;&#20934;&#26080;&#27861;&#24456;&#22909;&#22320;&#35780;&#20272;&#27492;&#31867;&#21160;&#24577;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#26469;&#20849;&#21516;&#35780;&#20272;LM&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#20849;&#21516;&#20219;&#21153;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;LM&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#35848;&#21028;&#28216;&#25103;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#35843;&#25972;&#22797;&#26434;&#24615;&#65292;&#24182;&#36991;&#20813;&#35780;&#20272;&#20013;&#30340;&#24847;&#22806;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26469;&#33258;&#20960;&#20010;&#20027;&#35201;&#20379;&#24212;&#21830;&#30340;&#20845;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;LM&#22312;&#21508;&#31181;&#35848;&#21028;&#28216;&#25103;&#19978;&#30340;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;i&#65289;&#24320;&#28304;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;MERA&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#20420;&#35821;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04531</link><description>&lt;p&gt;
MERA: &#20420;&#35821;LLM&#32508;&#21512;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04531
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;MERA&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#20420;&#35821;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#26368;&#26174;&#33879;&#30340;&#36827;&#23637;&#20043;&#19968;&#26159;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23835;&#36215;&#24341;&#20154;&#27880;&#30446;&#12290;&#38543;&#30528;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;LM&#22312;&#21487;&#34913;&#37327;&#30340;&#26041;&#38754;&#23637;&#31034;&#20102;&#25552;&#21319;&#65292;&#24182;&#19988;&#21457;&#23637;&#20986;&#20102;&#26032;&#30340;&#23450;&#24615;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#21644;LM&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;LM&#30340;&#33021;&#21147;&#12289;&#38480;&#21046;&#21644;&#30456;&#20851;&#39118;&#38505;&#20173;&#38656;&#26356;&#22909;&#22320;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;&#20420;&#35821;&#22810;&#27169;&#24577;&#26550;&#26500;&#35780;&#20272;&#65288;MERA&#65289;&#25351;&#23548;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#20197;&#20420;&#35821;&#20026;&#23548;&#21521;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#35813;&#22522;&#20934;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#24182;&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#27979;&#35797;&#65292;&#20197;&#30830;&#20445;&#25490;&#38500;&#25968;&#25454;&#27844;&#28431;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zeroand few-shot fixed instruction settings that can be extended to other modalities. We propose an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20803;&#25209;&#35780;(MetaCritique)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#25209;&#35780;&#30340;&#36136;&#37327;&#65292;&#24182;&#20197;F1&#20998;&#25968;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24341;&#20837;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#26469;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.04518</link><description>&lt;p&gt;
&#12298;&#25209;&#35780;&#30340;&#25209;&#35780;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Critique of Critique. (arXiv:2401.04518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20803;&#25209;&#35780;(MetaCritique)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#25209;&#35780;&#30340;&#36136;&#37327;&#65292;&#24182;&#20197;F1&#20998;&#25968;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24341;&#20837;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#26469;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#22312;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#34987;&#35777;&#26126;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#25209;&#35780;&#26412;&#36523;&#36136;&#37327;&#26041;&#38754;&#32570;&#20047;&#21407;&#21017;&#24615;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#39318;&#21019;&#20102;&#25209;&#35780;&#30340;&#25209;&#35780;&#65292;&#31216;&#20026;&#20803;&#25209;&#35780;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#25209;&#35780;&#30340;&#26694;&#26550;&#65292;&#20174;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#20004;&#20010;&#26041;&#38754;&#26469;&#35780;&#20272;&#25209;&#35780;&#12290;&#25105;&#20204;&#35745;&#31639;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#35843;&#21644;&#24179;&#22343;&#20540;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#65292;&#31216;&#20026;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#65292;&#20197;&#26356;&#31934;&#32454;&#30340;&#26041;&#24335;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#32771;&#34385;&#27599;&#20010;AIU&#65292;&#24182;&#32858;&#21512;&#27599;&#20010;AIU&#30340;&#21028;&#26029;&#24471;&#21040;&#25972;&#20307;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#35780;&#20272;&#36807;&#31243;&#28041;&#21450;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critique, as a natural language description for assessing the quality of model-generated content, has been proven to play an essential role in the training, evaluation, and refinement of Large Language Models (LLMs). However, there is a lack of principled understanding in evaluating the quality of the critique itself. In this paper, we pioneer the critique of critique, termed MetaCritique, which is a framework to evaluate the critique from two aspects, i.e., factuality as precision score and comprehensiveness as recall score. We calculate the harmonic mean of precision and recall as the overall rating called F1 score. To obtain a reliable evaluation outcome, we propose Atomic Information Units (AIUs), which describe the critique in a more fine-grained manner. MetaCritique takes each AIU into account and aggregates each AIU's judgment for the overall score. Moreover, given the evaluation process involves intricate reasoning, our MetaCritique provides a natural language rationale to supp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#19982;&#32463;&#20856;&#27169;&#24335;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#30340;&#25552;&#31034;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#35782;&#21035;&#30340;&#20849;&#21516;&#19979;&#20301;&#35789;&#23558;&#39069;&#22806;&#20449;&#24687;&#21152;&#20837;&#25552;&#31034;&#65292;&#21487;&#20197;&#25913;&#21892;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#26469;&#39044;&#27979;&#26356;&#39640;&#23618;&#27425;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;BLESS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36136;&#37327;&#25552;&#21319;&#65288;MAP = 0.8&#65289;&#12290;</title><link>http://arxiv.org/abs/2401.04515</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#19978;&#20301;&#35789;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models. (arXiv:2401.04515v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#19982;&#32463;&#20856;&#27169;&#24335;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#30340;&#25552;&#31034;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#35782;&#21035;&#30340;&#20849;&#21516;&#19979;&#20301;&#35789;&#23558;&#39069;&#22806;&#20449;&#24687;&#21152;&#20837;&#25552;&#31034;&#65292;&#21487;&#20197;&#25913;&#21892;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#26469;&#39044;&#27979;&#26356;&#39640;&#23618;&#27425;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;BLESS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36136;&#37327;&#25552;&#21319;&#65288;MAP = 0.8&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#38646;&#26679;&#26412;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#22522;&#20110;&#25991;&#26412;&#27010;&#29575;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#29983;&#25104;&#30340;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#19982;&#32463;&#20856;&#27169;&#24335;&#20043;&#38388;&#23384;&#22312;&#30528;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#36739;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#30340;&#25552;&#31034;&#36873;&#25321;&#65292;&#28982;&#21518;&#20877;&#36716;&#21521;&#26356;&#22823;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#29992;&#20110;&#39044;&#27979;&#20849;&#21516;&#19979;&#20301;&#35789;&#21644;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#30340;&#20849;&#21516;&#19979;&#20301;&#35789;&#23558;&#39069;&#22806;&#20449;&#24687;&#21152;&#20837;&#25552;&#31034;&#20197;&#25913;&#21892;&#19978;&#20301;&#35789;&#39044;&#27979;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#26469;&#39044;&#27979;&#26356;&#39640;&#23618;&#27425;&#30340;&#27010;&#24565;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;BLESS&#25968;&#25454;&#38598;&#19978;&#30340;&#36136;&#37327;&#65288;MAP = 0.8&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article investigates a zero-shot approach to hypernymy prediction using large language models (LLMs). The study employs a method based on text probability calculation, applying it to various generated prompts. The experiments demonstrate a strong correlation between the effectiveness of language model prompts and classic patterns, indicating that preliminary prompt selection can be carried out using smaller models before moving to larger ones. We also explore prompts for predicting co-hyponyms and improving hypernymy predictions by augmenting prompts with additional information through automatically identified co-hyponyms. An iterative approach is developed for predicting higher-level concepts, which further improves the quality on the BLESS dataset (MAP = 0.8).
&lt;/p&gt;</description></item><item><title>TechGPT-2.0&#26159;&#19968;&#20010;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39033;&#30446;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#21644;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04507</link><description>&lt;p&gt;
TechGPT-2.0:&#19968;&#20010;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
TechGPT-2.0: A large language model project to solve the task of knowledge graph construction. (arXiv:2401.04507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04507
&lt;/p&gt;
&lt;p&gt;
TechGPT-2.0&#26159;&#19968;&#20010;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39033;&#30446;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#21644;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#37117;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;TechGPT-2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#39033;&#30446;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#65288;RTE&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20316;&#20026;&#19968;&#20010;&#38754;&#21521;&#20013;&#22269;&#24320;&#28304;&#27169;&#22411;&#31038;&#21306;&#30340;LLM&#21487;&#35775;&#38382;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;7B&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;QLoRA&#26435;&#37325;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;TechGPT-2.0&#26159;&#22312;&#21326;&#20026;&#30340;Ascend&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#30340;&#12290;&#32487;&#25215;&#20102;TechGPT-1.0&#30340;&#25152;&#26377;&#21151;&#33021;&#65292;&#23427;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#27169;&#22411;&#24341;&#20837;&#20102;&#26032;&#30340;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#21508;&#20010;&#39046;&#22495;&#30340;&#25991;&#26412;&#65292;&#22914;&#22320;&#29702;&#21306;&#22495;&#12289;&#20132;&#36890;&#12289;&#32452;&#32455;&#26426;&#26500;&#12289;&#25991;&#23398;&#20316;&#21697;&#12289;&#29983;&#29289;&#23398;&#21644;&#33258;&#28982;&#31185;&#23398;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have exhibited robust performance across diverse natural language processing tasks. This report introduces TechGPT-2.0, a project designed to enhance the capabilities of large language models specifically in knowledge graph construction tasks, including named entity recognition (NER) and relationship triple extraction (RTE) tasks in NLP applications. Additionally, it serves as a LLM accessible for research within the Chinese open-source model community. We offer two 7B large language model weights and a QLoRA weight specialized for processing lengthy texts.Notably, TechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all functionalities from TechGPT-1.0, it exhibits robust text processing capabilities, particularly in the domains of medicine and law. Furthermore, we introduce new capabilities to the model, enabling it to process texts in various domains such as geographical areas, transportation, organizations, literary works, biology, natural sciences, as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#30340;&#29983;&#23384;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#20998;&#21106;&#20154;&#21475;&#21644;&#39044;&#27979;&#19981;&#21516;&#30340;&#29983;&#23384;&#20998;&#24067;&#26469;&#21457;&#29616;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#21270;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#29305;&#27530;&#31639;&#27861;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#36816;&#34892;&#26102;&#38388;&#20248;&#20110;&#26576;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04489</link><description>&lt;p&gt;
&#26368;&#20248;&#29983;&#23384;&#26641;: &#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Survival Trees: A Dynamic Programming Approach. (arXiv:2401.04489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#30340;&#29983;&#23384;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#20998;&#21106;&#20154;&#21475;&#21644;&#39044;&#27979;&#19981;&#21516;&#30340;&#29983;&#23384;&#20998;&#24067;&#26469;&#21457;&#29616;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#21270;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#29305;&#27530;&#31639;&#27861;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#36816;&#34892;&#26102;&#38388;&#20248;&#20110;&#26576;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#26469;&#30740;&#31350;&#21644;&#39044;&#27979;&#27515;&#20129;&#26102;&#38388;&#25110;&#20854;&#20182;&#19981;&#21487;&#37325;&#22797;&#20107;&#20214;&#30340;&#26102;&#38388;&#65292;&#32780;&#26576;&#20123;&#23454;&#20363;&#30340;&#30495;&#23454;&#27515;&#20129;&#26102;&#38388;&#26159;&#26410;&#30693;&#30340;&#12290;&#29983;&#23384;&#26641;&#36890;&#36807;&#36882;&#24402;&#22320;&#20998;&#21106;&#20154;&#21475;&#24182;&#22312;&#27599;&#20010;&#21494;&#33410;&#28857;&#39044;&#27979;&#19981;&#21516;&#30340;&#29983;&#23384;&#20998;&#24067;&#65292;&#33021;&#22815;&#21457;&#29616;&#32039;&#20945;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#27169;&#22411;&#20013;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#20248;&#21270;&#20445;&#35777;&#30340;&#29983;&#23384;&#26641;&#26041;&#27861;&#65292;&#33021;&#22815;&#35780;&#20272;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#20248;&#21270;&#38388;&#38553;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#29305;&#27530;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#28145;&#24230;&#20026;2&#30340;&#26641;&#65292;&#25552;&#39640;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#33719;&#21462;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#29978;&#33267;&#20248;&#20110;&#26576;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis studies and predicts the time of death, or other singular unrepeated events, based on historical data, while the true time of death for some instances is unknown. Survival trees enable the discovery of complex nonlinear relations in a compact human comprehensible model, by recursively splitting the population and predicting a distinct survival distribution in each leaf node. We use dynamic programming to provide the first survival tree method with optimality guarantees, enabling the assessment of the optimality gap of heuristics. We improve the scalability of our method through a special algorithm for computing trees up to depth two. The experiments show that our method's run time even outperforms some heuristics for realistic cases while obtaining similar out-of-sample performance with the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#24341;&#23548;&#65292;&#33258;&#21160;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#36776;&#21035;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#26631;&#27880;&#25968;&#25454;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#24037;&#21162;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04481</link><description>&lt;p&gt;
&#20197;&#28779;&#25915;&#28779;&#65306;&#23545;&#25239;&#21551;&#21457;&#24335;&#29983;&#25104;&#19968;&#20010;&#36776;&#21035;&#34394;&#20551;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset. (arXiv:2401.04481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#24341;&#23548;&#65292;&#33258;&#21160;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#36776;&#21035;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#26631;&#27880;&#25968;&#25454;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#24037;&#21162;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;GPT&#12289;Bard&#21644;Llama&#31561;&#65292;&#22312;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25104;&#21151;&#21487;&#33021;&#24341;&#21457;&#23545;&#20854;&#34987;&#28389;&#29992;&#30340;&#25285;&#24551;&#65292;&#27604;&#22914;&#36890;&#36807;&#29983;&#25104;&#20551;&#26032;&#38395;&#21644;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#24341;&#21457;&#22823;&#35268;&#27169;&#28608;&#21160;&#21644;&#20167;&#24680;&#12290;&#20256;&#32479;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#24320;&#21457;&#26041;&#27861;&#22312;&#26631;&#27880;&#25968;&#25454;&#26102;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#21162;&#21147;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#29992;&#20110;&#35782;&#21035;&#34394;&#20551;&#20449;&#24687;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#21487;&#20449;&#26032;&#38395;&#25991;&#31456;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#23548;LLM&#33258;&#21160;&#29983;&#25104;&#21407;&#22987;&#25991;&#31456;&#30340;&#25688;&#35201;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#30340;&#24341;&#23548;&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#20135;&#29983;&#29305;&#23450;&#31867;&#22411;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#20363;&#22914;&#38169;&#35823;&#30340;&#25968;&#37327;&#12289;&#38169;&#35823;&#30340;&#24402;&#23646;&#22320;&#31561;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success in language generation capabilities of large language models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns about their possible misuse in inducing mass agitation and communal hatred via generating fake news and spreading misinformation. Traditional means of developing a misinformation ground-truth dataset does not scale well because of the extensive manual effort required to annotate the data. In this paper, we propose an LLM-based approach of creating silver-standard ground-truth datasets for identifying misinformation. Specifically speaking, given a trusted news article, our proposed approach involves prompting LLMs to automatically generate a summarised version of the original article. The prompts in our proposed approach act as a controlling mechanism to generate specific types of factual incorrectness in the generated summaries, e.g., incorrect quantities, false attributions etc. To investigate the usefulness of this dataset, we conduct
&lt;/p&gt;</description></item><item><title>TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04478</link><description>&lt;p&gt;
TwinBooster: &#32467;&#21512;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#22686;&#24378;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction. (arXiv:2401.04478v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04478
&lt;/p&gt;
&lt;p&gt;
TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#21644;&#24320;&#21457;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#23545;&#20998;&#23376;&#27963;&#24615;&#21644;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#35745;&#31639;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#20165;&#38480;&#20110;&#22823;&#37327;&#25968;&#25454;&#21487;&#29992;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;Siamese&#31070;&#32463;&#32593;&#32476;Barlow Twins&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#26816;&#27979;&#26041;&#27861;&#20449;&#24687;&#21644;&#20998;&#23376;&#25351;&#32441;&#25552;&#21462;&#30495;&#23454;&#30340;&#20998;&#23376;&#20449;&#24687;&#12290;TwinBooster&#36890;&#36807;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#30340;&#23646;&#24615;&#39044;&#27979;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#27969;&#27700;&#32447;&#22312;FS-Mol&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#31361;&#30772;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36890;&#24120;&#25968;&#25454;&#31232;&#32570;&#30340;&#20851;&#38190;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#23884;&#20837;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20107;&#21518;&#35299;&#37322;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#26088;&#22312;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#28385;&#24847;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.04474</link><description>&lt;p&gt;
&#32467;&#21512;&#22522;&#20110;&#23884;&#20837;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20107;&#21518;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Combining Embedding-Based and Semantic-Based Models for Post-hoc Explanations in Recommender Systems. (arXiv:2401.04474v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#23884;&#20837;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20107;&#21518;&#35299;&#37322;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#26088;&#22312;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#22312;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#23427;&#20204;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#23545;&#36825;&#20123;&#25512;&#33616;&#30340;&#35299;&#37322;&#12290;&#23884;&#20837;&#24335;&#27169;&#22411;&#23613;&#31649;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#21487;&#33021;&#25439;&#23475;&#20449;&#20219;&#21644;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23884;&#20837;&#24335;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#29983;&#25104;&#20107;&#21518;&#35299;&#37322;&#65292;&#21033;&#29992;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#32467;&#26500;&#21270;&#26694;&#26550;&#20869;&#32452;&#32455;&#25968;&#25454;&#65292;&#26412;&#20307;&#20801;&#35768;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#23545;&#20110;&#29983;&#25104;&#35299;&#37322;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#23884;&#20837;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20107;&#21518;&#35299;&#37322;&#65292;&#25105;&#20204;&#23450;&#20041;&#30340;&#26694;&#26550;&#26088;&#22312;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#28385;&#24847;&#24230;&#65292;&#24182;&#28508;&#22312;&#22320;&#22686;&#21152;&#25512;&#33616;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data-rich environment, recommender systems play a crucial role in decision support systems. They provide to users personalized recommendations and explanations about these recommendations. Embedding-based models, despite their widespread use, often suffer from a lack of interpretability, which can undermine trust and user engagement. This paper presents an approach that combines embedding-based and semantic-based models to generate post-hoc explanations in recommender systems, leveraging ontology-based knowledge graphs to improve interpretability and explainability. By organizing data within a structured framework, ontologies enable the modeling of intricate relationships between entities, which is essential for generating explanations. By combining embedding-based and semantic based models for post-hoc explanations in recommender systems, the framework we defined aims at producing meaningful and easy-to-understand explanations, enhancing user trust and satisfaction, and pot
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04472</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#35757;&#32451;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;FL&#26041;&#27861;&#36890;&#24120;&#21482;&#28041;&#21450;&#23567;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#38543;&#30528;Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22914;&#20309;&#20351;&#22522;&#30784;&#27169;&#22411;&#22312;FL&#24212;&#29992;&#20013;&#23454;&#26045;&#36215;&#26469;&#65311;&#37492;&#20110;&#22312;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#26102;&#38388;&#28040;&#32791;&#36890;&#24120;&#30456;&#20284;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;FL&#24212;&#29992;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#26041;&#27861;&#30340;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#20248;&#21270;&#35757;&#32451;&#26102;&#38388;&#24182;&#20943;&#23569;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;FL&#30740;&#31350;&#21450;&#20854;&#24310;&#20280;&#30340;&#29616;&#26377;&#26041;&#27861;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.
&lt;/p&gt;</description></item><item><title>MagicVideo-V2&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#39640;&#23457;&#32654;&#35270;&#39057;&#29983;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#38598;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12289;&#35270;&#39057;&#36816;&#21160;&#29983;&#25104;&#22120;&#12289;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#27169;&#22359;&#21644;&#24103;&#25554;&#20540;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#20197;&#39640;&#20445;&#30495;&#24230;&#21644;&#27969;&#30021;&#24615;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#32654;&#23398;&#35270;&#39057;&#30340;&#30446;&#26631;&#12290;&#22312;&#22823;&#35268;&#27169;&#29992;&#25143;&#35780;&#20272;&#20013;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;Text-to-Video&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.04468</link><description>&lt;p&gt;
MagicVideo-V2: &#22810;&#38454;&#27573;&#39640;&#23457;&#32654;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation. (arXiv:2401.04468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04468
&lt;/p&gt;
&lt;p&gt;
MagicVideo-V2&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#39640;&#23457;&#32654;&#35270;&#39057;&#29983;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#38598;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12289;&#35270;&#39057;&#36816;&#21160;&#29983;&#25104;&#22120;&#12289;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#27169;&#22359;&#21644;&#24103;&#25554;&#20540;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#20197;&#39640;&#20445;&#30495;&#24230;&#21644;&#27969;&#30021;&#24615;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#32654;&#23398;&#35270;&#39057;&#30340;&#30446;&#26631;&#12290;&#22312;&#22823;&#35268;&#27169;&#29992;&#25143;&#35780;&#20272;&#20013;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;Text-to-Video&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20445;&#30495;&#35270;&#39057;&#29983;&#25104;&#30340;&#19981;&#26029;&#38656;&#27714;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#30340;&#37325;&#22823;&#30740;&#31350;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicVideo-V2&#65292;&#23427;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12289;&#35270;&#39057;&#36816;&#21160;&#29983;&#25104;&#22120;&#12289;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#27169;&#22359;&#21644;&#24103;&#25554;&#20540;&#27169;&#22359;&#38598;&#25104;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#29983;&#25104;&#27969;&#31243;&#20013;&#12290;&#20973;&#20511;&#36825;&#20123;&#26550;&#26500;&#35774;&#35745;&#65292;MagicVideo-V2&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#21331;&#36234;&#20445;&#30495;&#24230;&#21644;&#27969;&#30021;&#24615;&#30340;&#39640;&#20998;&#36776;&#29575;&#32654;&#23398;&#35270;&#39057;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#29992;&#25143;&#35780;&#20272;&#65292;&#23427;&#23637;&#31034;&#20102;&#20248;&#20110;&#39046;&#20808;&#30340;Text-to-Video&#31995;&#32479;&#22914;Runway&#12289;Pika 1.0&#12289;Morph&#12289;Moon Valley&#21644;Stable Video Diffusion model&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing demand for high-fidelity video generation from textual descriptions has catalyzed significant research in this field. In this work, we introduce MagicVideo-V2 that integrates the text-to-image model, video motion generator, reference image embedding module and frame interpolation module into an end-to-end video generation pipeline. Benefiting from these architecture designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution video with remarkable fidelity and smoothness. It demonstrates superior performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph, Moon Valley and Stable Video Diffusion model via user evaluation at large scale.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#27880;&#20837;&#30340;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#30693;&#35782;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#31639;&#27861;&#38590;&#20197;&#35299;&#37322;&#32467;&#26524;&#21407;&#22240;&#21644;&#38590;&#20197;&#29702;&#35299;&#20998;&#26512;&#39044;&#27979;&#36923;&#36753;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04441</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#27880;&#20837;&#30340;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Image classification network enhancement methods based on knowledge injection. (arXiv:2401.04441v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#27880;&#20837;&#30340;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#30693;&#35782;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#31639;&#27861;&#38590;&#20197;&#35299;&#37322;&#32467;&#26524;&#21407;&#22240;&#21644;&#38590;&#20197;&#29702;&#35299;&#20998;&#26512;&#39044;&#27979;&#36923;&#36753;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#20173;&#28982;&#20572;&#30041;&#22312;&#20687;&#22270;&#20687;-&#26631;&#31614;&#23545;&#36825;&#26679;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30417;&#30563;&#26041;&#27861;&#20013;&#65292;&#36825;&#20351;&#24471;&#20256;&#32479;&#30340;&#31639;&#27861;&#38590;&#20197;&#35299;&#37322;&#32467;&#26524;&#30340;&#21407;&#22240;&#65292;&#24182;&#19988;&#39044;&#27979;&#36923;&#36753;&#38590;&#20197;&#29702;&#35299;&#21644;&#20998;&#26512;&#12290;&#24403;&#21069;&#31639;&#27861;&#27809;&#26377;&#21033;&#29992;&#29616;&#26377;&#30340;&#20154;&#31867;&#30693;&#35782;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#19982;&#20154;&#31867;&#35748;&#30693;&#27169;&#22411;&#19981;&#19968;&#33268;&#65292;&#19981;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#21457;&#26126;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#30693;&#35782;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#35748;&#30693;&#27169;&#22411;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#29616;&#26377;&#30340;&#20154;&#31867;&#30693;&#35782;&#20449;&#24687;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#27425;&#20998;&#23618;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#30001;&#22810;&#23618;&#27425;&#20998;&#23618;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21644;&#22810;&#23618;&#27425;&#20998;&#23618;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current deep neural network algorithm still stays in the end-to-end training supervision method like Image-Label pairs, which makes traditional algorithm is difficult to explain the reason for the results, and the prediction logic is difficult to understand and analyze. The current algorithm does not use the existing human knowledge information, which makes the model not in line with the human cognition model and makes the model not suitable for human use. In order to solve the above problems, the present invention provides a deep neural network training method based on the human knowledge, which uses the human cognition model to construct the deep neural network training model, and uses the existing human knowledge information to construct the deep neural network training model. This paper proposes a multi-level hierarchical deep learning algorithm, which is composed of multi-level hierarchical deep neural network architecture and multi-level hierarchical deep learning framework. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#39640;&#20809;&#35889;&#25104;&#20687;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;HSI&#36890;&#36947;&#36824;&#21407;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#30340;&#36895;&#24230;&#24555;6.90&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.04437</link><description>&lt;p&gt;
&#20351;&#29992;&#38477;&#32500;&#26041;&#27861;&#23545;&#39640;&#20809;&#35889;&#25104;&#20687;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using Dimension Reduction Methods. (arXiv:2401.04437v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#39640;&#20809;&#35889;&#25104;&#20687;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;HSI&#36890;&#36947;&#36824;&#21407;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#30340;&#36895;&#24230;&#24555;6.90&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#39640;&#20809;&#35889;&#25104;&#20687;&#65288;HSI&#65289;&#26469;&#26816;&#27979;&#20135;&#21697;&#20013;&#30340;&#24322;&#29289;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#21487;&#35270;&#21270;&#21253;&#25324;&#32043;&#22806;&#32447;&#21644;&#32418;&#22806;&#32447;&#22312;&#20869;&#30340;&#19981;&#21487;&#35265;&#27874;&#38271;&#12290;&#32771;&#34385;&#21040;HSI&#30340;&#24040;&#22823;&#22270;&#20687;&#36890;&#36947;&#65292;&#21487;&#20197;&#32771;&#34385;&#20351;&#29992;&#22810;&#31181;&#38477;&#32500;&#26041;&#27861;&#65288;&#22914;PCA&#25110;UMAP&#65289;&#26469;&#20943;&#23569;&#36890;&#36947;&#25968;&#37327;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#20197;&#19979;&#22522;&#26412;&#38480;&#21046;&#65306;&#65288;1&#65289;HSI&#25429;&#25417;&#30340;&#24310;&#36831;&#38382;&#39064;&#65307;&#65288;2&#65289;&#23545;&#37325;&#35201;&#36890;&#36947;&#30340;&#35299;&#37322;&#33021;&#21147;&#36739;&#24369;&#12290;&#20026;&#20102;&#35268;&#36991;&#19978;&#36848;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;HSI&#36890;&#36947;&#36824;&#21407;&#26041;&#27861;&#12290;&#19982;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65288;&#22914;PCA&#25110;UMAP&#65289;&#19981;&#21516;&#65292;&#29305;&#24449;&#36873;&#25321;&#21487;&#20197;&#25353;&#24433;&#21709;&#25490;&#24207;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#37325;&#26032;&#35774;&#35745;&#20219;&#21153;&#20248;&#21270;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#20809;&#35889;&#30456;&#26426;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#30340;MVTec AD&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#65292;&#25105;&#20204;&#30830;&#35748;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#25512;&#29702;&#38454;&#27573;&#27604;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#24555;6.90&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies try to use hyperspectral imaging (HSI) to detect foreign matters in products because it enables to visualize the invisible wavelengths including ultraviolet and infrared. Considering the enormous image channels of the HSI, several dimension reduction methods-e.g., PCA or UMAP-can be considered to reduce but those cannot ease the fundamental limitations, as follows: (1) latency of HSI capturing. (2) less explanation ability of the important channels. In this paper, to circumvent the aforementioned methods, one of the ways to channel reduction, on anomaly detection proposed HSI. Different from feature extraction methods (i.e., PCA or UMAP), feature selection can sort the feature by impact and show better explainability so we might redesign the task-optimized and cost-effective spectroscopic camera. Via the extensive experiment results with synthesized MVTec AD dataset, we confirm that the feature selection method shows 6.90x faster at the inference phase compared with feat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;i-Rebalance&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#37325;&#26032;&#23450;&#20301;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20272;&#35745;&#39550;&#39542;&#21592;&#23545;&#37325;&#26032;&#23450;&#20301;&#25512;&#33616;&#30340;&#20915;&#31574;&#12290;&#35813;&#25216;&#26415;&#37319;&#29992;&#39034;&#24207;&#37325;&#26032;&#23450;&#20301;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#21452;DRL&#20195;&#29702;&#23454;&#29616;&#20379;&#38656;&#24179;&#34913;&#21644;&#39550;&#39542;&#21592;&#20559;&#22909;&#28385;&#24847;&#24230;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.04429</link><description>&lt;p&gt;
i-Rebalance: &#20010;&#24615;&#21270;&#36710;&#36742;&#37325;&#26032;&#23450;&#20301;&#20197;&#23454;&#29616;&#20379;&#38656;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
i-Rebalance: Personalized Vehicle Repositioning for Supply Demand Balance. (arXiv:2401.04429v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;i-Rebalance&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#37325;&#26032;&#23450;&#20301;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20272;&#35745;&#39550;&#39542;&#21592;&#23545;&#37325;&#26032;&#23450;&#20301;&#25512;&#33616;&#30340;&#20915;&#31574;&#12290;&#35813;&#25216;&#26415;&#37319;&#29992;&#39034;&#24207;&#37325;&#26032;&#23450;&#20301;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#21452;DRL&#20195;&#29702;&#23454;&#29616;&#20379;&#38656;&#24179;&#34913;&#21644;&#39550;&#39542;&#21592;&#20559;&#22909;&#28385;&#24847;&#24230;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31199;&#36710;&#24179;&#21488;&#38754;&#20020;&#30528;&#20379;&#38656;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#36710;&#36742;&#37325;&#26032;&#23450;&#20301;&#25216;&#26415;&#36890;&#24120;&#23558;&#21496;&#26426;&#35270;&#20026;&#21516;&#36136;&#21270;&#30340;&#20195;&#29702;&#20154;&#65292;&#24182;&#19988;&#20197;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#37325;&#26032;&#23450;&#20301;&#20182;&#20204;&#65292;&#20551;&#35774;&#20182;&#20204;&#20250;&#36981;&#23432;&#37325;&#26032;&#23450;&#20301;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#30495;&#23454;&#21644;&#20197;&#39550;&#39542;&#21592;&#20026;&#20013;&#24515;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#39550;&#39542;&#21592;&#20855;&#26377;&#29420;&#29305;&#30340;&#24033;&#33322;&#20559;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#34892;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#37325;&#26032;&#23450;&#20301;&#25216;&#26415;&#65306;i-Rebalance&#12290;i-Rebalance&#36890;&#36807;&#28041;&#21450;99&#21517;&#30495;&#23454;&#39550;&#39542;&#21592;&#30340;&#29616;&#22330;&#29992;&#25143;&#30740;&#31350;&#26469;&#20272;&#35745;&#39550;&#39542;&#21592;&#23545;&#37325;&#26032;&#23450;&#20301;&#25512;&#33616;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#21516;&#26102;&#20248;&#21270;&#20379;&#38656;&#24179;&#34913;&#21644;&#22686;&#24378;&#20559;&#22909;&#28385;&#24847;&#24230;&#65292;i-Rebalance&#37319;&#29992;&#20102;&#39034;&#24207;&#37325;&#26032;&#23450;&#20301;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#21452;DRL&#20195;&#29702;&#65306;&#32593;&#26684;&#20195;&#29702;&#30830;&#23450;&#31354;&#38386;&#36710;&#36742;&#30340;&#37325;&#26032;&#23450;&#20301;&#39034;&#24207;&#65292;&#36710;&#36742;&#20195;&#29702;&#20026;&#39044;&#23450;&#20041;&#30340;&#39034;&#24207;&#20013;&#30340;&#27599;&#36742;&#36710;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ride-hailing platforms have been facing the challenge of balancing demand and supply. Existing vehicle reposition techniques often treat drivers as homogeneous agents and relocate them deterministically, assuming compliance with the reposition. In this paper, we consider a more realistic and driver-centric scenario where drivers have unique cruising preferences and can decide whether to take the recommendation or not on their own. We propose i-Rebalance, a personalized vehicle reposition technique with deep reinforcement learning (DRL). i-Rebalance estimates drivers' decisions on accepting reposition recommendations through an on-field user study involving 99 real drivers. To optimize supply-demand balance and enhance preference satisfaction simultaneously, i-Rebalance has a sequential reposition strategy with dual DRL agents: Grid Agent to determine the reposition order of idle vehicles, and Vehicle Agent to provide personalized recommendations to each vehicle in the pre-defined order
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;MultiNet&#35821;&#20041;&#32593;&#32476;&#30340;&#35821;&#20041;&#27010;&#24565;&#23884;&#20837;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#35789;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30446;&#26631;&#32676;&#32452;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04422</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#27010;&#24565;&#23884;&#20837;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating Text Similarity based on Semantic Concept Embeddings. (arXiv:2401.04422v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;MultiNet&#35821;&#20041;&#32593;&#32476;&#30340;&#35821;&#20041;&#27010;&#24565;&#23884;&#20837;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#35789;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30446;&#26631;&#32676;&#32452;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#26131;&#29992;&#24615;&#21644;&#39640;&#20934;&#30830;&#24615;&#65292;Word2Vec (W2V) &#35789;&#23884;&#20837;&#22312;&#35821;&#20041;&#34920;&#31034;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#21253;&#25324;&#21333;&#35789;&#12289;&#21477;&#23376;&#21644;&#25972;&#20010;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#20197;&#21450;&#35821;&#20041;&#30456;&#20284;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32570;&#28857;&#26159;&#30452;&#25509;&#20174;&#34920;&#38754;&#34920;&#31034;&#20013;&#25552;&#21462;&#65292;&#19981;&#33021;&#20805;&#20998;&#20195;&#34920;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#23545;&#20110;&#39640;&#24230;&#27495;&#20041;&#30340;&#35789;&#20063;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;MultiNet&#35821;&#20041;&#32593;&#32476;(SN)&#24418;&#24335;&#21270;&#30340;&#35821;&#20041;&#27010;&#24565;&#23884;&#20837;(CE)&#65292;&#20197;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#22312;&#24066;&#22330;&#30446;&#26631;&#32676;&#20307;&#20998;&#24067;&#20219;&#21153;&#30340;&#35780;&#20272;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;&#23558;&#20256;&#32479;&#35789;&#23884;&#20837;&#21644;&#35821;&#20041;CE&#32467;&#21512;&#21487;&#20197;&#22686;&#21152;&#39044;&#27979;&#30446;&#26631;&#32676;&#32452;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings enjoy great success in the semantic representation of words, sentences, and whole documents as well as for semantic similarity estimation. However, they have the shortcoming that they are directly extracted from a surface representation, which does not adequately represent human thought processes and also performs poorly for highly ambiguous words. Therefore, we propose Semantic Concept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism, which addresses both shortcomings. The evaluation on a marketing target group distribution task showed that the accuracy of predicted target groups can be increased by combining traditional word embeddings with semantic CEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#39044;&#27979;&#27599;&#20010;&#39044;&#35774;&#27604;&#29305;&#29575;&#19979;&#30340;&#26368;&#20339;&#36716;&#30721;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#27604;&#29305;&#29575;&#38454;&#26799;&#26500;&#24314;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#39044;&#32534;&#30721;&#26469;&#39640;&#25928;&#30830;&#23450;&#20869;&#23481;&#20248;&#21270;&#30340;&#27604;&#29305;&#29575;&#38454;&#26799;&#12290;</title><link>http://arxiv.org/abs/2401.04405</link><description>&lt;p&gt;
&#23545;&#20110;&#39640;&#25928;&#30340;&#20869;&#23481;&#30456;&#20851;&#27604;&#29305;&#29575;&#38454;&#26799;&#20272;&#35745;&#30340;&#26368;&#20339;&#36716;&#30721;&#20998;&#36776;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation. (arXiv:2401.04405v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#39044;&#27979;&#27599;&#20010;&#39044;&#35774;&#27604;&#29305;&#29575;&#19979;&#30340;&#26368;&#20339;&#36716;&#30721;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#27604;&#29305;&#29575;&#38454;&#26799;&#26500;&#24314;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#39044;&#32534;&#30721;&#26469;&#39640;&#25928;&#30830;&#23450;&#20869;&#23481;&#20248;&#21270;&#30340;&#27604;&#29305;&#29575;&#38454;&#26799;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#35270;&#39057;&#27969;&#23186;&#20307;&#38656;&#35201;&#39640;&#25928;&#30340;&#27604;&#29305;&#29575;&#38454;&#26799;&#26500;&#24314;&#20197;&#28385;&#36275;&#24322;&#26500;&#32593;&#32476;&#26465;&#20214;&#21644;&#32456;&#31471;&#29992;&#25143;&#38656;&#27714;&#12290;&#38024;&#23545;&#27599;&#20010;&#35270;&#39057;&#30340;&#20869;&#23481;&#20248;&#21270;&#32534;&#30721;&#36890;&#24120;&#20250;&#36941;&#21382;&#22823;&#37327;&#30340;&#32534;&#30721;&#21442;&#25968;&#65292;&#20197;&#25628;&#32034;&#27599;&#20010;&#35270;&#39057;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#24037;&#20316;&#28857;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#39044;&#27979;&#38024;&#23545;&#39044;&#32534;&#30721;&#24320;&#38144;&#20943;&#23569;&#30340;&#20869;&#23481;&#20248;&#21270;&#27604;&#29305;&#29575;&#38454;&#26799;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#24085;&#32047;&#25176;&#21069;&#27839;&#20272;&#35745;&#32534;&#30721;&#21442;&#25968;&#65292;&#20173;&#28982;&#38656;&#35201;&#21518;&#32493;&#30340;&#39044;&#32534;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30452;&#25509;&#39044;&#27979;&#27599;&#20010;&#39044;&#35774;&#27604;&#29305;&#29575;&#19979;&#30340;&#26368;&#20339;&#36716;&#30721;&#20998;&#36776;&#29575;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#27604;&#29305;&#29575;&#38454;&#26799;&#26500;&#24314;&#12290;&#25105;&#20204;&#37319;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#38376;&#25511;&#24490;&#29615;&#32593;&#32476;&#26469;&#25429;&#25417;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#23558;&#36716;&#30721;&#20998;&#36776;&#29575;&#39044;&#27979;&#20316;&#20026;&#22810;&#20219;&#21153;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#39044;&#32534;&#30721;&#26469;&#39640;&#25928;&#30830;&#23450;&#20869;&#23481;&#20248;&#21270;&#30340;&#27604;&#29305;&#29575;&#38454;&#26799;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#24456;&#22909;&#22320;&#36924;&#36817;&#22320;&#38754;&#30495;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-tr
&lt;/p&gt;</description></item><item><title>&#20010;&#20307;&#21270;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#25104;&#27169;&#22411;IGNITE&#36890;&#36807;&#23398;&#20064;&#20010;&#20307;&#30340;&#21160;&#24577;&#29305;&#24449;&#65292;&#32467;&#21512;&#20154;&#21475;&#29305;&#24449;&#21644;&#27835;&#30103;&#20449;&#24687;&#65292;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30495;&#23454;&#20540;&#65292;&#20026;&#20010;&#20307;&#21270;&#21307;&#30103;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.04402</link><description>&lt;p&gt;
IGNITE: &#20010;&#20307;&#21270;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGNITE: Individualized GeNeration of Imputations in Time-series Electronic health records. (arXiv:2401.04402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04402
&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#21270;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#25104;&#27169;&#22411;IGNITE&#36890;&#36807;&#23398;&#20064;&#20010;&#20307;&#30340;&#21160;&#24577;&#29305;&#24449;&#65292;&#32467;&#21512;&#20154;&#21475;&#29305;&#24449;&#21644;&#27835;&#30103;&#20449;&#24687;&#65292;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30495;&#23454;&#20540;&#65292;&#20026;&#20010;&#20307;&#21270;&#21307;&#30103;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20026;&#25512;&#21160;&#20010;&#20307;&#21270;&#21307;&#30103;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26681;&#25454;&#20010;&#20307;&#24046;&#24322;&#37327;&#36523;&#23450;&#21046;&#27835;&#30103;&#26041;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#20511;&#21161;&#20016;&#23500;&#30340;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26469;&#30740;&#31350;&#24739;&#32773;&#30340;&#29983;&#29702;&#21644;&#27835;&#30103;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24448;&#24448;&#31232;&#30095;&#19988;&#23384;&#22312;&#22823;&#37327;&#32570;&#22833;&#65292;&#20854;&#20013;&#32570;&#22833;&#30340;&#20449;&#24687;&#20063;&#21487;&#33021;&#21453;&#26144;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#20010;&#20307;&#21270;&#21307;&#30103;&#20013;&#30340;&#25104;&#21151;&#20005;&#37325;&#20381;&#36182;&#20110;&#22914;&#20309;&#20174;&#29983;&#29702;&#25968;&#25454;&#12289;&#27835;&#30103;&#20197;&#21450;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#26469;&#34920;&#31034;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20010;&#20307;&#30340;&#20154;&#21475;&#29305;&#24449;&#21644;&#27835;&#30103;&#30340;&#26465;&#20214;&#19979;&#65292;&#23398;&#20064;&#22810;&#21464;&#37327;&#25968;&#25454;&#30340;&#24739;&#32773;&#21160;&#24577;&#65292;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30495;&#23454;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records present a valuable modality for driving personalized medicine, where treatment is tailored to fit individual-level differences. For this purpose, many data-driven machine learning and statistical models rely on the wealth of longitudinal EHRs to study patients' physiological and treatment effects. However, longitudinal EHRs tend to be sparse and highly missing, where missingness could also be informative and reflect the underlying patient's health status. Therefore, the success of data-driven models for personalized medicine highly depends on how the EHR data is represented from physiological data, treatments, and the missing values in the data. To this end, we propose a novel deep-learning model that learns the underlying patient dynamics over time across multivariate data to generate personalized realistic values conditioning on an individual's demographic characteristics and treatments. Our proposed model, IGNITE (Individualized GeNeration of Imputations in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04385</link><description>&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#25200;&#21160;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#28041;&#21450;&#21040;&#25764;&#38144;&#25968;&#25454;&#35760;&#24405;&#21644;&#20943;&#23567;&#35813;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30446;&#26631;&#65292;&#20294;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;&#21442;&#25968;&#25200;&#21160;&#30340;&#26435;&#37325;&#21435;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#28041;&#21450;&#21040;&#20840;&#23616;&#20462;&#25913;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;Top-K&#21644;Random-k&#21442;&#25968;&#25200;&#21160;&#19981;&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#38544;&#31169;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#21487;&#25511;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#35780;&#20272;&#26426;&#22120;&#21435;&#23398;&#20064;&#25928;&#26524;&#30340;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22312;&#21435;&#23398;&#20064;&#21644;&#21097;&#20313;&#25968;&#25454;&#19978;&#30340;&#24191;&#20041;&#24615;&#33021;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#65292;&#21363;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#31934;&#30830;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#26080;&#27861;&#23545;&#21435;&#23398;&#20064;&#31243;&#24230;&#36827;&#34892;&#20934;&#30830;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.  In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#25968;&#25454;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20998;&#26512;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#12289;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;XAI&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04374</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65306;&#19968;&#20010;&#25968;&#25454;&#25366;&#25496;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective. (arXiv:2401.04374v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#25968;&#25454;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20998;&#26512;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#12289;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;XAI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#22797;&#26434;&#24615;&#21644;&#36879;&#26126;&#24230;&#19981;&#36275;&#65292;&#20154;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20197;&#20351;&#36825;&#20123;&#31995;&#32479;&#26356;&#20855;&#35299;&#37322;&#24615;&#25110;&#22312;&#21487;&#35775;&#38382;&#30340;&#26415;&#35821;&#20013;&#35299;&#37322;&#20854;&#34892;&#20026;&#12290;&#19982;&#22823;&#22810;&#25968;&#35780;&#35770;&#19981;&#21516;&#65292;&#35813;&#24037;&#20316;&#37319;&#29992;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35266;&#28857;&#65292;&#30740;&#31350;&#25968;&#25454;&#25910;&#38598;&#65292;&#22788;&#29702;&#21644;&#20998;&#26512;&#22914;&#20309;&#20419;&#25104;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#31867;&#65292;&#26681;&#25454;&#20854;&#30446;&#30340;&#36827;&#34892;&#20998;&#31867;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#28041;&#21450;&#23558;&#25968;&#25454;&#28857;&#19982;&#27169;&#22411;&#36755;&#20986;&#30456;&#20851;&#32852;&#30340;&#29305;&#24449;&#24402;&#22240;&#21644;&#25512;&#29702;&#36807;&#31243;&#65307;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#35757;&#32451;&#25968;&#25454;&#32454;&#24494;&#24046;&#24322;&#65288;&#22914;&#25968;&#25454;&#35780;&#20272;&#21644;&#26679;&#26412;&#24322;&#24120;&#65289;&#23545;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65307;&#20197;&#21450;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#65292;&#20174;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#21457;&#29616;&#28508;&#22312;&#27169;&#24335;&#65292;&#24182;&#20419;&#36827;&#31038;&#20250;&#20215;&#20540;&#21644;&#31185;&#23398;&#21457;&#29616;&#30340;&#26032;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;XAI&#26041;&#27861;&#35770;&#25552;&#28860;&#20026;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a "data-centric" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into data mining op
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiffSketch&#30340;&#33609;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20195;&#34920;&#24615;&#29305;&#24449;&#24182;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25552;&#21462;&#65292;&#23454;&#29616;&#20174;&#22270;&#20687;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#33609;&#22270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#21462;&#33609;&#22270;&#30340;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.04362</link><description>&lt;p&gt;
&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#25552;&#21462;&#20195;&#34920;&#24615;&#29305;&#24449;&#65292;&#29992;&#20110;&#20174;&#19968;&#20010;&#26679;&#26412;&#20013;&#25552;&#21462;&#33609;&#22270;
&lt;/p&gt;
&lt;p&gt;
Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example. (arXiv:2401.04362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiffSketch&#30340;&#33609;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20195;&#34920;&#24615;&#29305;&#24449;&#24182;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25552;&#21462;&#65292;&#23454;&#29616;&#20174;&#22270;&#20687;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#33609;&#22270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#21462;&#33609;&#22270;&#30340;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;DiffSketch&#65292;&#19968;&#31181;&#20174;&#22270;&#20687;&#20013;&#29983;&#25104;&#21508;&#31181;&#39118;&#26684;&#21270;&#33609;&#22270;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#20174;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#36873;&#25321;&#20195;&#34920;&#24615;&#29305;&#24449;&#65292;&#20197;&#25552;&#21462;&#33609;&#22270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#33609;&#22270;&#29983;&#25104;&#26041;&#27861;&#21482;&#38656;&#35201;&#19968;&#20010;&#25163;&#24037;&#32472;&#21046;&#30340;&#33609;&#22270;&#20316;&#20026;&#35757;&#32451;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#22120;&#25552;&#21462;&#20986;&#19968;&#31181;&#31616;&#21270;&#30340;&#25552;&#21462;&#22120;&#65292;&#30830;&#20445;&#20102;&#39640;&#25928;&#30340;&#33609;&#22270;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#36873;&#25321;&#21435;&#22122;&#25193;&#25955;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;VAE&#29305;&#24449;&#32467;&#21512;&#65292;&#29983;&#25104;&#33609;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#37319;&#26679;&#26041;&#26696;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#31934;&#31616;&#30340;DiffSketch&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#33609;&#22270;&#25552;&#21462;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#25552;&#21462;&#33609;&#22270;&#30340;&#20219;&#21153;&#19978;&#20063;&#36229;&#36234;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#39118;&#26684;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DiffSketch, a method for generating a variety of stylized sketches from images. Our approach focuses on selecting representative features from the rich semantics of deep features within a pretrained diffusion model. This novel sketch generation method can be trained with one manual drawing. Furthermore, efficient sketch extraction is ensured by distilling a trained generator into a streamlined extractor. We select denoising diffusion features through analysis and integrate these selected features with VAE features to produce sketches. Additionally, we propose a sampling scheme for training models using a conditional generative approach. Through a series of comparisons, we verify that distilled DiffSketch not only outperforms existing state-of-the-art sketch extraction methods but also surpasses diffusion-based stylization methods in the task of extracting sketches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25552;&#39640;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#65288;KGD&#65289;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#21019;&#24314;&#27491;&#36127;&#26679;&#26412;&#20197;&#24212;&#23545;&#23454;&#38469;&#22122;&#38899;&#65292;&#22914;&#38169;&#21035;&#23383;&#21644;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;</title><link>http://arxiv.org/abs/2401.04361</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning. (arXiv:2401.04361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25552;&#39640;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#65288;KGD&#65289;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#21019;&#24314;&#27491;&#36127;&#26679;&#26412;&#20197;&#24212;&#23545;&#23454;&#38469;&#22122;&#38899;&#65292;&#22914;&#38169;&#21035;&#23383;&#21644;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#65288;KGD&#65289;&#36890;&#36807;&#32473;&#23450;&#30340;&#23545;&#35805;&#29615;&#22659;&#21644;&#22806;&#37096;&#30693;&#35782;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#22238;&#24212;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#20986;&#29616;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26500;&#24314;KGD&#31995;&#32479;&#26102;&#65292;&#38590;&#20813;&#20250;&#36935;&#21040;&#21508;&#31181;&#23454;&#38469;&#30340;&#22122;&#38899;&#12290;&#20363;&#22914;&#65292;&#23545;&#35805;&#29615;&#22659;&#21487;&#33021;&#28041;&#21450;&#38169;&#21035;&#23383;&#21644;&#32553;&#20889;&#31561;&#25200;&#21160;&#12290;&#27492;&#22806;&#65292;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#23384;&#22312;&#19981;&#23436;&#25972;&#24615;&#65292;&#20063;&#21487;&#33021;&#21253;&#21547;&#38169;&#35823;&#21644;&#36807;&#26102;&#30340;&#20107;&#23454;&#12290;&#36825;&#20123;&#23454;&#38469;&#30340;&#22122;&#38899;&#32473;KGD&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#24182;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25552;&#39640;KGD&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;KGD&#26679;&#26412;&#20013;&#30340;&#23454;&#20307;&#20449;&#24687;&#21019;&#24314;&#20854;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-grounded dialogue (KGD) learns to generate an informative response based on a given dialogue context and external knowledge (\emph{e.g.}, knowledge graphs; KGs). Recently, the emergence of large language models (LLMs) and pre-training techniques has brought great success to knowledge-grounded dialogue. However, when building KGD systems in real applications, there are various real-world noises that are inevitable to face. For example, the dialogue context might involve perturbations such as misspellings and abbreviations. In addition, KGs typically suffer from incompletion and also might contain erroneous and outdated facts. Such real-world noises pose a challenge to the robustness of KGD systems and hinder their applications in the real world. In this paper, we propose an entity-based contrastive learning framework for improving the robustness of KGD. Specifically, we make use of the entity information in a KGD sample to create both its positive and negative samples which in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#28857;&#20113;&#37197;&#20934;&#30340;&#36845;&#20195;&#21453;&#39304;&#32593;&#32476; (IFNet)&#65292;&#36890;&#36807;&#37325;&#26032;&#36335;&#30001;&#39640;&#23618;&#29305;&#24449;&#26469;&#20016;&#23500;&#20302;&#23618;&#29305;&#24449;&#30340;&#34920;&#31034;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#32570;&#20047;&#39640;&#23618;&#20449;&#24687;&#21040;&#20302;&#23618;&#20449;&#24687;&#30340;&#25351;&#23548;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04357</link><description>&lt;p&gt;
&#36845;&#20195;&#21453;&#39304;&#32593;&#32476;&#29992;&#20110;&#26080;&#30417;&#30563;&#28857;&#20113;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Iterative Feedback Network for Unsupervised Point Cloud Registration. (arXiv:2401.04357v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#28857;&#20113;&#37197;&#20934;&#30340;&#36845;&#20195;&#21453;&#39304;&#32593;&#32476; (IFNet)&#65292;&#36890;&#36807;&#37325;&#26032;&#36335;&#30001;&#39640;&#23618;&#29305;&#24449;&#26469;&#20016;&#23500;&#20302;&#23618;&#29305;&#24449;&#30340;&#34920;&#31034;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#32570;&#20047;&#39640;&#23618;&#20449;&#24687;&#21040;&#20302;&#23618;&#20449;&#24687;&#30340;&#25351;&#23548;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#37197;&#20934;&#20316;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#26088;&#22312;&#23547;&#25214;&#23545;&#40784;&#19968;&#23545;&#28857;&#20113;&#30340;&#26368;&#20339;&#21464;&#25442;&#12290;&#22312;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#20449;&#24687;&#20256;&#36882;&#36890;&#24120;&#26159;&#27491;&#21521;&#20256;&#36882;&#65292;&#22240;&#27492;&#32570;&#20047;&#20174;&#39640;&#23618;&#20449;&#24687;&#21040;&#20302;&#23618;&#20449;&#24687;&#30340;&#25351;&#23548;&#12290;&#27492;&#22806;&#65292;&#36807;&#22810;&#30340;&#39640;&#23618;&#20449;&#24687;&#21487;&#33021;&#36807;&#20110;&#20887;&#20313;&#65292;&#30452;&#25509;&#20351;&#29992;&#21487;&#33021;&#19982;&#21407;&#22987;&#20302;&#23618;&#20449;&#24687;&#21457;&#29983;&#20914;&#31361;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#21453;&#39304;&#32593;&#32476;&#65288;IFNet&#65289;&#29992;&#20110;&#26080;&#30417;&#30563;&#28857;&#20113;&#37197;&#20934;&#65292;&#20854;&#20013;&#36890;&#36807;&#37325;&#26032;&#36335;&#30001;&#21518;&#32493;&#39640;&#23618;&#29305;&#24449;&#26377;&#25928;&#22320;&#20016;&#23500;&#20102;&#20302;&#23618;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;IFNet&#24314;&#31435;&#22312;&#19968;&#31995;&#21015;&#21453;&#39304;&#37197;&#20934;&#22359;&#65288;FRB&#65289;&#27169;&#22359;&#19978;&#65292;&#27599;&#20010;&#27169;&#22359;&#36127;&#36131;&#29983;&#25104;&#21069;&#21521;&#21018;&#24615;&#21464;&#25442;&#21644;&#21453;&#39304;&#39640;&#23618;&#29305;&#24449;&#12290;&#36825;&#20123;FRB&#27169;&#22359;&#34987;&#32423;&#32852;&#21644;&#32463;&#36807;&#26102;&#38388;&#23637;&#24320;&#12290;&#27492;&#22806;&#65292;&#21453;&#39304;&#21464;&#25442;&#22120;&#34987;&#35774;&#35745;&#29992;&#26469;&#22686;&#24378;&#29305;&#23450;&#23618;&#32423;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
As a fundamental problem in computer vision, point cloud registration aims to seek the optimal transformation for aligning a pair of point clouds. In most existing methods, the information flows are usually forward transferring, thus lacking the guidance from high-level information to low-level information. Besides, excessive high-level information may be overly redundant, and directly using it may conflict with the original low-level information. In this paper, we propose a novel Iterative Feedback Network (IFNet) for unsupervised point cloud registration, in which the representation of low-level features is efficiently enriched by rerouting subsequent high-level features. Specifically, our IFNet is built upon a series of Feedback Registration Block (FRB) modules, with each module responsible for generating the feedforward rigid transformation and feedback high-level features. These FRB modules are cascaded and recurrently unfolded over time. Further, the Feedback Transformer is desig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#24577;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#21487;&#21464;&#24037;&#20917;&#19979;&#26816;&#27979;&#35774;&#22791;&#30340;&#21464;&#28857;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#21464;&#28857;&#26469;&#25552;&#39640;&#21097;&#20313;&#23551;&#21629;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04351</link><description>&lt;p&gt;
&#22312;&#21487;&#21464;&#24037;&#20917;&#19979;&#30340;&#21464;&#28857;&#26816;&#27979;&#32508;&#21512;&#21097;&#20313;&#23551;&#21629;&#20272;&#35745;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Change Point Detection Integrated Remaining Useful Life Estimation Model under Variable Operating Conditions. (arXiv:2401.04351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#24577;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#21487;&#21464;&#24037;&#20917;&#19979;&#26816;&#27979;&#35774;&#22791;&#30340;&#21464;&#28857;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#21464;&#28857;&#26469;&#25552;&#39640;&#21097;&#20313;&#23551;&#21629;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;&#36864;&#21270;&#36807;&#31243;&#30340;&#24320;&#22987;&#20449;&#24687;&#65292;&#20581;&#24247;&#29366;&#24577;&#35780;&#20272;&#25104;&#20026;&#21487;&#38752;&#30340;&#22797;&#26434;&#35774;&#22791;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#20272;&#35745;&#30340;&#37325;&#35201;&#21069;&#25552;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26102;&#38388;&#21160;&#24577;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#20010;&#20307;&#35774;&#22791;&#30340;&#21464;&#28857;&#65292;&#21363;&#20351;&#22312;&#21487;&#21464;&#30340;&#24037;&#20917;&#19979;&#65292;&#24182;&#21033;&#29992;&#25152;&#23398;&#21040;&#30340;&#21464;&#28857;&#26469;&#25552;&#39640;RUL&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#31163;&#32447;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#22810;&#21464;&#37327;&#20256;&#24863;&#22120;&#25968;&#25454;&#34987;&#20998;&#35299;&#65292;&#20197;&#23398;&#20064;&#21487;&#25512;&#24191;&#21644;&#20195;&#34920;&#22810;&#20010;&#24037;&#20917;&#19979;&#27491;&#24120;&#36816;&#34892;&#21160;&#24577;&#30340;&#34701;&#21512;&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#12290;&#22522;&#20110;&#36825;&#20123;&#23398;&#21040;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#26500;&#24314;&#30417;&#25511;&#32479;&#35745;&#20540;&#21644;&#25511;&#21046;&#38480;&#21046;&#38408;&#20540;&#20197;&#21160;&#24577;&#22320;&#26816;&#27979;&#35774;&#22791;&#32423;&#21035;&#30340;&#21464;&#28857;&#12290;&#28982;&#21518;&#65292;&#26816;&#27979;&#21040;&#30340;&#21464;&#28857;&#20026;&#35757;&#32451;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;RUL&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#36864;&#21270;&#25968;&#25454;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
By informing the onset of the degradation process, health status evaluation serves as a significant preliminary step for reliable remaining useful life (RUL) estimation of complex equipment. This paper proposes a novel temporal dynamics learning-based model for detecting change points of individual devices, even under variable operating conditions, and utilises the learnt change points to improve the RUL estimation accuracy. During offline model development, the multivariate sensor data are decomposed to learn fused temporal correlation features that are generalisable and representative of normal operation dynamics across multiple operating conditions. Monitoring statistics and control limit thresholds for normal behaviour are dynamically constructed from these learnt temporal features for the unsupervised detection of device-level change points. The detected change points then inform the degradation data labelling for training a long short-term memory (LSTM)-based RUL estimation model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#20010;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#31574;&#30053;&#26469;&#35299;&#20915;&#22522;&#32447;&#27169;&#22411;&#20013;&#20027;&#39064;&#21644;&#25552;&#31034;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04339</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20869;&#23384;&#39640;&#25928;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Personalization using Quantized Diffusion Model. (arXiv:2401.04339v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#20010;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#31574;&#30053;&#26469;&#35299;&#20915;&#22522;&#32447;&#27169;&#22411;&#20013;&#20027;&#39064;&#21644;&#25552;&#31034;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20159;&#32423;&#21442;&#25968;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;Stable Diffusion XL&#12289;Imagen&#21644;Dall-E3&#65289;&#30340;&#23835;&#36215;&#26174;&#33879;&#25512;&#21160;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36164;&#28304;&#38656;&#27714;&#39640;&#21644;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#23427;&#20204;&#30340;&#22823;&#35268;&#27169;&#24615;&#36136;&#22312;&#24494;&#35843;&#21644;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#23545;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#30456;&#23545;&#26410;&#24320;&#21457;&#20294;&#26377;&#21069;&#26223;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#21046;&#19977;&#20010;&#27169;&#22411;&#65288;&#29992;&#20110;&#24494;&#35843;&#37327;&#21270;&#21442;&#25968;&#30340;PEQA&#65292;&#29992;&#20110;&#21518;&#26399;&#37327;&#21270;&#30340;Q-Diffusion&#21644;&#20010;&#24615;&#21270;&#30340;DreamBooth&#65289;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22522;&#32447;&#27169;&#22411;&#20013;&#20027;&#39064;&#21644;&#25552;&#31034;&#36136;&#37327;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#25193;&#25955;&#27169;&#22411;&#20013;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#30340;&#19981;&#21516;&#35282;&#33394;&#65306;S1&#22312;&#36873;&#25321;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#20165;&#20248;&#21270;&#19968;&#32452;&#24494;&#35843;&#21442;&#25968;&#65292;S2&#21019;&#24314;&#22810;&#20010;&#24494;&#35843;&#21442;&#25968;&#32452;&#65292;&#27599;&#20010;&#32452;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#30340;&#26102;&#38388;&#27493;&#38271;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of billion-parameter diffusion models like Stable Diffusion XL, Imagen, and Dall-E3 markedly advances the field of generative AI. However, their large-scale nature poses challenges in fine-tuning and deployment due to high resource demands and slow inference speed. This paper ventures into the relatively unexplored yet promising realm of fine-tuning quantized diffusion models. We establish a strong baseline by customizing three models: PEQA for fine-tuning quantization parameters, Q-Diffusion for post-training quantization, and DreamBooth for personalization. Our analysis reveals a notable trade-off between subject and prompt fidelity within the baseline model. To address these issues, we introduce two strategies, inspired by the distinct roles of different timesteps in diffusion models: S1 optimizing a single set of fine-tuning parameters exclusively at selected intervals, and S2 creating multiple fine-tuning parameter sets, each specialized for different timestep intervals. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.04336</link><description>&lt;p&gt;
&#28145;&#24230;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#29992;&#20110;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#24040;&#22823;&#22270;&#36890;&#24120;&#20197;&#38750;&#20013;&#24515;&#21270;&#23376;&#22270;&#30340;&#24418;&#24335;&#30001;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#20998;&#25955;&#23384;&#20648;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#32771;&#34385;&#21040;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#65288;subgraph FL&#65289;&#22330;&#26223;&#26159;&#24456;&#33258;&#28982;&#30340;&#65292;&#20854;&#20013;&#27599;&#20010;&#26412;&#22320;&#23458;&#25143;&#31471;&#25345;&#26377;&#25972;&#20010;&#20840;&#23616;&#22270;&#30340;&#23376;&#22270;&#65292;&#20197;&#33719;&#21462;&#20840;&#23616;&#19968;&#33324;&#21270;&#30340;&#22270;&#25366;&#25496;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#32570;&#23569;&#36328;&#23376;&#22270;&#37051;&#23621;&#32780;&#23548;&#33268;&#30340;&#23616;&#37096;&#23376;&#22270;&#19978;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#32570;&#22833;&#37051;&#23621;&#29983;&#25104;&#22120;&#21644;GNN&#30340;&#32852;&#21512;FL&#26469;&#22686;&#21152;&#26412;&#22320;&#37051;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;FL&#30340;&#25928;&#29992;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#38544;&#31169;&#30446;&#26631;&#26041;&#38754;&#23384;&#22312;&#28145;&#23618;&#27425;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDEP&#26469;&#20840;&#38754;&#35299;&#20915;&#23376;&#22270;FL&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;FedDEP&#21253;&#25324;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65306;(1) &#21033;&#29992;&#28508;&#22312;&#32570;&#22833;&#37051;&#23621;&#30340;GNN&#23884;&#20837;&#36827;&#34892;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#65307;(2) Effic...
&lt;/p&gt;
&lt;p&gt;
Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Effic
&lt;/p&gt;</description></item><item><title>&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#32467;&#21512;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#19982;&#22797;&#26434;&#29615;&#22659;&#20114;&#21160;&#30340;&#23454;&#20307;&#20219;&#21153;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#22810;&#27169;&#24577;GPT-4V&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;&#23454;&#20307;&#20219;&#21153;&#35268;&#21010;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04334</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#20154;&#25216;&#26415;&#65306;&#26426;&#36935;&#12289;&#25361;&#25112;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Robotics: Opportunities, Challenges, and Perspectives. (arXiv:2401.04334v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04334
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#32467;&#21512;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#19982;&#22797;&#26434;&#29615;&#22659;&#20114;&#21160;&#30340;&#23454;&#20307;&#20219;&#21153;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#22810;&#27169;&#24577;GPT-4V&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;&#23454;&#20307;&#20219;&#21153;&#35268;&#21010;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#21382;&#20102;&#37325;&#22823;&#25193;&#23637;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#39046;&#22495;&#65292;LLMs&#21033;&#29992;&#20854;&#39640;&#32423;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21046;&#23450;&#31934;&#30830;&#39640;&#25928;&#30340;&#34892;&#21160;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#22312;&#19982;&#22797;&#26434;&#29615;&#22659;&#20114;&#21160;&#30340;&#23454;&#20307;&#20219;&#21153;&#20013;&#65292;&#20165;&#26377;&#25991;&#26412;&#30340;LLMs&#24120;&#24120;&#38754;&#20020;&#19982;&#26426;&#22120;&#20154;&#35270;&#35273;&#24863;&#30693;&#19981;&#20860;&#23481;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#23545;LLMs&#21644;&#22810;&#27169;&#24577;LLMs&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26032;&#20852;&#38598;&#25104;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;GPT-4V&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#26426;&#22120;&#20154;&#35270;&#35273;&#24863;&#30693;&#30340;&#32467;&#21512;&#26469;&#22686;&#24378;&#23454;&#20307;&#20219;&#21153;&#35268;&#21010;&#12290;&#22522;&#20110;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4V&#22312;&#23454;&#20307;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#24191;&#27867;&#30340;LLMs&#35843;&#30740;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.04331</link><description>&lt;p&gt;
&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#20998;&#25968;&#38454;&#36830;&#32493;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65306;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study. (arXiv:2401.04331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20005;&#26684;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;(FDE)&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;Caputo&#23548;&#25968;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#25972;&#25968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#12290;&#21033;&#29992;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#19982;&#20256;&#32479;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#20013;&#30340;&#26080;&#35760;&#24518;&#39532;&#23572;&#21487;&#22827;&#26356;&#26032;&#19981;&#21516;&#12290;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30456;&#23545;&#20110;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#22312;&#27809;&#26377;&#25915;&#20987;&#25110;&#25200;&#21160;&#30340;&#29615;&#22659;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#20248;&#21183;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#39564;&#35777;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#20855;&#26377;&#19968;&#23450;&#30340;&#31283;&#23450;&#24615;&#21644;&#24377;&#24615;&#65292;&#20294;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BD-MSA&#30340;&#26032;&#30340;&#21464;&#21270;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#25910;&#38598;&#29305;&#24449;&#22270;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#20449;&#24687;&#65292;&#25104;&#21151;&#25552;&#21462;&#20102;&#21464;&#21270;&#21306;&#22495;&#30340;&#36793;&#30028;&#20449;&#24687;&#65292;&#24182;&#23558;&#21464;&#21270;&#21306;&#22495;&#30340;&#20027;&#20307;&#19982;&#36793;&#30028;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2401.04330</link><description>&lt;p&gt;
BD-MSA: &#22810;&#23610;&#24230;&#29305;&#24449;&#20449;&#24687;&#32858;&#21512;&#24341;&#23548;&#30340;&#36523;&#20307;&#35299;&#32806;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#24433;&#20687;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method guided by multi-scale feature information aggregation. (arXiv:2401.04330v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04330
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BD-MSA&#30340;&#26032;&#30340;&#21464;&#21270;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#25910;&#38598;&#29305;&#24449;&#22270;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#20449;&#24687;&#65292;&#25104;&#21151;&#25552;&#21462;&#20102;&#21464;&#21270;&#21306;&#22495;&#30340;&#36793;&#30028;&#20449;&#24687;&#65292;&#24182;&#23558;&#21464;&#21270;&#21306;&#22495;&#30340;&#20027;&#20307;&#19982;&#36793;&#30028;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#26088;&#22312;&#26816;&#27979;&#21516;&#19968;&#22320;&#26041;&#25293;&#25668;&#30340;&#20004;&#20010;&#26102;&#26399;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#65292;&#22312;&#32467;&#26524;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21355;&#26143;&#30340;&#25293;&#25668;&#35282;&#24230;&#12289;&#34180;&#20113;&#23618;&#30340;&#24433;&#21709;&#20197;&#21450;&#29305;&#23450;&#30340;&#20809;&#29031;&#26465;&#20214;&#65292;&#22312;&#19968;&#20123;&#36965;&#24863;&#25668;&#24433;&#20013;&#65292;&#21464;&#21270;&#21306;&#22495;&#30340;&#27169;&#31946;&#36793;&#32536;&#38382;&#39064;&#26080;&#27861;&#36890;&#36807;&#24403;&#21069;&#30340;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#27491;&#30830;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36523;&#20307;&#35299;&#32806;&#22810;&#23610;&#24230;&#29305;&#24449;&#32858;&#21512;&#21464;&#21270;&#26816;&#27979;&#65288;BD-MSA&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#22312;&#29305;&#24449;&#22270;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#32500;&#24230;&#20013;&#25910;&#38598;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#22270;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#25104;&#21151;&#25552;&#21462;&#21464;&#21270;&#21306;&#22495;&#30340;&#36793;&#30028;&#20449;&#24687;&#65292;&#24182;&#23558;&#21464;&#21270;&#21306;&#22495;&#30340;&#20027;&#20307;&#19982;&#20854;&#36793;&#30028;&#20998;&#31163;&#12290;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#35780;&#20272;&#25351;&#26631;&#26159;&#35780;&#20215;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#32467;&#26524;&#36136;&#37327;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of remote sensing image change detection (RSCD) is to detect differences between bi-temporal images taken at the same place. Deep learning has been extensively used to RSCD tasks, yielding significant results in terms of result recognition. However, due to the shooting angle of the satellite, the impacts of thin clouds, and certain lighting conditions, the problem of fuzzy edges in the change region in some remote sensing photographs cannot be properly handled using current RSCD algorithms. To solve this issue, we proposed a Body Decouple Multi-Scale by fearure Aggregation change detection (BD-MSA), a novel model that collects both global and local feature map information in the channel and space dimensions of the feature map during the training and prediction phases. This approach allows us to successfully extract the change region's boundary information while also divorcing the change region's main body from its boundary. Numerous studies have shown that the assessment me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2401.04319</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#20102;&#35299;&#24744;&#30340;&#38656;&#27714;&#65306;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23450;&#20301;&#26041;&#24335;&#65292;&#21363;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#21487;&#20197;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#32467;&#26500;&#21270;&#36923;&#36753;&#35821;&#35328;&#65292;&#21363;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#12290;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#23581;&#35797;&#21033;&#29992;LLMs&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20294;&#26159;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;1&#65289;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#31616;&#21333;&#30340;&#8220;&#35753;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#22320;&#24605;&#32771;&#8221;&#25552;&#31034;&#65292;&#35201;&#20040;&#22312;&#28436;&#31034;&#20013;&#25552;&#20379;&#22266;&#23450;&#30340;&#31034;&#20363;&#32780;&#19981;&#32771;&#34385;&#25552;&#31034;&#21644;&#38382;&#39064;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#22312;&#19968;&#20123;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#32467;&#26500;&#21270;&#35821;&#35328;&#36716;&#25442;&#65289;&#20013;&#20351;LLMs&#26080;&#25928;&#12290;(2) &#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#38381;&#28304;&#27169;&#22411;&#25110;&#36807;&#24230;&#23454;&#29616;&#30340;&#27169;&#22411;&#20013;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or exces
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StarCraft II&#28216;&#25103;&#22238;&#25918;&#26500;&#24314;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#21407;&#22411;&#35774;&#35745;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#31354;&#38388;&#25512;&#29702;&#26041;&#27861;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;MNIST&#21644;CIFAR10&#30456;&#20284;&#30340;&#26131;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04290</link><description>&lt;p&gt;
StarCraftImage: &#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#31354;&#38388;&#25512;&#29702;&#26041;&#27861;&#21407;&#22411;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments. (arXiv:2401.04290v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04290
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StarCraft II&#28216;&#25103;&#22238;&#25918;&#26500;&#24314;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#21407;&#22411;&#35774;&#35745;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#31354;&#38388;&#25512;&#29702;&#26041;&#27861;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;MNIST&#21644;CIFAR10&#30456;&#20284;&#30340;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#23545;&#20110;&#22810;&#20010;&#24212;&#29992;&#24456;&#37325;&#35201;&#65292;&#22914;&#20107;&#20214;&#39044;&#27979;&#12289;&#26234;&#33021;&#20307;&#31867;&#22411;&#35782;&#21035;&#25110;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#31561;&#65288;&#20363;&#22914;&#65292;&#22312;&#20256;&#24863;&#22120;&#32593;&#32476;&#19978;&#30340;&#33258;&#20027;&#30417;&#35270;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#20219;&#21153;&#65289;&#12290;StarCraft II&#28216;&#25103;&#22238;&#25918;&#35760;&#24405;&#20102;&#26234;&#33021;&#65288;&#21644;&#23545;&#25239;&#24615;&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#65292;&#21487;&#20197;&#29992;&#20316;&#36825;&#20123;&#20219;&#21153;&#30340;&#27979;&#35797;&#24179;&#21488;&#65307;&#28982;&#32780;&#65292;&#25552;&#21462;&#20986;&#31616;&#21333;&#21644;&#26631;&#20934;&#21270;&#30340;&#34920;&#31034;&#20197;&#29992;&#20110;&#21407;&#22411;&#35774;&#35745;&#36825;&#20123;&#20219;&#21153;&#38750;&#24120;&#32321;&#29712;&#24182;&#19988;&#38590;&#20197;&#22797;&#29616;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23613;&#31649;MNIST&#21644;CIFAR10&#26497;&#20854;&#31616;&#21333;&#65292;&#20294;&#23427;&#20204;&#24050;&#32463;&#23454;&#29616;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#21644;&#22797;&#29616;&#33021;&#21147;&#12290;&#20026;&#20102;&#36981;&#24490;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#22522;&#20110;&#23637;&#31034;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;StarCraft II&#28216;&#25103;&#22238;&#25918;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#31354;&#38388;&#25512;&#29702;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#26131;&#20110;&#20351;&#29992;&#23601;&#20687;MNIST&#21644;CIFAR10&#19968;&#26679;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20180;&#32454;&#24635;&#32467;&#20102;255&#20010;&#36830;&#32493;&#28216;&#25103;&#29366;&#24577;&#30340;&#31383;&#21475;&#65292;&#21019;&#24314;&#20102;360&#19975;&#20010;&#25688;&#35201;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZoDiac&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#22312;&#21487;&#35757;&#32451;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27880;&#20837;&#27700;&#21360;&#65292;&#20174;&#32780;&#20351;&#27700;&#21360;&#33021;&#22815;&#22312;&#21463;&#25915;&#20987;&#26102;&#21487;&#38752;&#26816;&#27979;&#21040;&#65292;&#23545;&#26368;&#20808;&#36827;&#30340;&#27700;&#21360;&#25915;&#20987;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04247</link><description>&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#30340;&#40065;&#26834;&#22270;&#20687;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Robust Image Watermarking using Stable Diffusion. (arXiv:2401.04247v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZoDiac&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#22312;&#21487;&#35757;&#32451;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27880;&#20837;&#27700;&#21360;&#65292;&#20174;&#32780;&#20351;&#27700;&#21360;&#33021;&#22815;&#22312;&#21463;&#25915;&#20987;&#26102;&#21487;&#38752;&#26816;&#27979;&#21040;&#65292;&#23545;&#26368;&#20808;&#36827;&#30340;&#27700;&#21360;&#25915;&#20987;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#27700;&#21360;&#23545;&#20110;&#36861;&#36394;&#22270;&#20687;&#26469;&#28304;&#21644;&#22768;&#26126;&#25152;&#26377;&#26435;&#38750;&#24120;&#37325;&#35201;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#65289;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#21019;&#24314;&#34394;&#20551;&#20294;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#27700;&#21360;&#25104;&#20026;&#20102;&#23588;&#20026;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20351;&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#38752;&#22320;&#36776;&#35748;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#27491;&#26159;&#36825;&#31181;&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#21487;&#20197;&#31227;&#38500;&#20351;&#29992;&#29616;&#26377;&#26041;&#27861;&#27880;&#20837;&#30340;&#27700;&#21360;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZoDiac&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#23558;&#27700;&#21360;&#27880;&#20837;&#21040;&#21487;&#35757;&#32451;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#22312;&#21463;&#25915;&#20987;&#26102;&#20173;&#28982;&#21487;&#20197;&#21487;&#38752;&#22320;&#22312;&#28508;&#22312;&#21521;&#37327;&#20013;&#26816;&#27979;&#21040;&#27700;&#21360;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598; MS-COCO&#12289;DiffusionDB &#21644; WikiArt &#19978;&#35780;&#20272;&#20102; ZoDiac&#65292;&#24182;&#21457;&#29616; ZoDiac &#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27700;&#21360;&#25915;&#20987;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#27700;&#21360;&#26816;&#27979;&#29575;&#36229;&#36807;98%&#65292;&#35823;&#25253;&#29575;&#20302;&#20110;6.4%&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31283;&#23450;&#25193;&#25955;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermarking images is critical for tracking image provenance and claiming ownership. With the advent of generative models, such as stable diffusion, able to create fake but realistic images, watermarking has become particularly important, e.g., to make generated images reliably identifiable. Unfortunately, the very same stable diffusion technology can remove watermarks injected using existing methods. To address this problem, we present a ZoDiac, which uses a pre-trained stable diffusion model to inject a watermark into the trainable latent space, resulting in watermarks that can be reliably detected in the latent vector, even when attacked. We evaluate ZoDiac on three benchmarks, MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against state-of-the-art watermark attacks, with a watermark detection rate over 98% and a false positive rate below 6.4%, outperforming state-of-the-art watermarking methods. Our research demonstrates that stable diffusion is a promising appr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FunnyNet-W&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#35270;&#39057;&#20013;&#30340;&#26377;&#36259;&#30636;&#38388;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#25968;&#25454;&#20197;&#21450;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#19988;&#37319;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#33719;&#24471;&#35757;&#32451;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.04210</link><description>&lt;p&gt;
FunnyNet-W:&#35270;&#39057;&#20013;&#37326;&#22806;&#26377;&#36259;&#30636;&#38388;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild. (arXiv:2401.04210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04210
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FunnyNet-W&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#35270;&#39057;&#20013;&#30340;&#26377;&#36259;&#30636;&#38388;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#25968;&#25454;&#20197;&#21450;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#19988;&#37319;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#33719;&#24471;&#35757;&#32451;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35266;&#30475;&#21916;&#21095;&#26102;&#33258;&#21160;&#29702;&#35299;&#26377;&#36259;&#30340;&#30636;&#38388;&#65288;&#21363;&#20351;&#35753;&#20154;&#21457;&#31505;&#30340;&#30636;&#38388;&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#28041;&#21450;&#21040;&#21508;&#31181;&#29305;&#24449;&#65292;&#22914;&#32930;&#20307;&#35821;&#35328;&#12289;&#23545;&#35805;&#21644;&#25991;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FunnyNet-W&#65292;&#23427;&#26159;&#19968;&#20010;&#20381;&#38752;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#35270;&#39057;&#20013;&#30340;&#26377;&#36259;&#30636;&#38388;&#12290;&#19982;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#23383;&#24149;&#24418;&#24335;&#30340;&#26631;&#27880;&#25968;&#25454;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#28982;&#19982;&#35270;&#39057;&#19968;&#36215;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65306;&#65288;a&#65289;&#35270;&#39057;&#24103;&#65292;&#22240;&#20026;&#23427;&#20204;&#21253;&#21547;&#20102;&#22330;&#26223;&#29702;&#35299;&#25152;&#24517;&#38656;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#65288;b&#65289;&#38899;&#39057;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#19982;&#26377;&#36259;&#30636;&#38388;&#30456;&#20851;&#30340;&#26356;&#39640;&#32423;&#21035;&#30340;&#32447;&#32034;&#65292;&#22914;&#35821;&#35843;&#12289;&#38899;&#39640;&#21644;&#20572;&#39039;&#65292;&#20197;&#21450;&#65288;c&#65289;&#30001;&#35821;&#38899;&#36716;&#25991;&#26412;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#30340;&#25991;&#26412;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#32463;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21518;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#33719;&#24471;&#35757;&#32451;&#26631;&#31614;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#21644;&#26631;&#35760;&#26377;&#36259;&#30340;&#38899;&#39057;&#30636;&#38388;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture. In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos. Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#19968;&#31181;AI&#39550;&#39542;&#25945;&#32451;&#30340;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#20449;&#24687;&#31867;&#22411;&#21644;&#21576;&#29616;&#26041;&#24335;&#23545;&#34920;&#29616;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.04206</link><description>&lt;p&gt;
&#20174;AI&#25945;&#32451;&#23398;&#20064;&#36187;&#36710;&#65306;&#22810;&#27169;&#24577;&#33258;&#21160;&#39550;&#39542;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Learning Racing From an AI Coach: Effects of Multimodal Autonomous Driving Explanations on Driving Performance, Cognitive Load, Expertise, and Trust. (arXiv:2401.04206v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#19968;&#31181;AI&#39550;&#39542;&#25945;&#32451;&#30340;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#20449;&#24687;&#31867;&#22411;&#21644;&#21576;&#29616;&#26041;&#24335;&#23545;&#34920;&#29616;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#39033;&#21069;&#21518;&#23454;&#39564;&#20013;&#65288;n=41&#65289;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#27169;&#20223;&#20154;&#31867;&#39550;&#39542;&#19987;&#23478;&#30340;&#25351;&#23548;&#35828;&#26126;&#30340;AI&#25945;&#32451;&#30340;&#35299;&#37322;&#27807;&#36890;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#20449;&#24515;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#21442;&#19982;&#32773;&#34987;&#20998;&#20026;&#22235;&#20010;&#32452;&#65292;&#35780;&#20272;&#20102;AI&#25945;&#32451;&#35299;&#37322;&#30340;&#20004;&#20010;&#32500;&#24230;&#65306;&#20449;&#24687;&#31867;&#22411;&#65288;'what'&#21644;'why'-type&#35299;&#37322;&#65289;&#21644;&#21576;&#29616;&#26041;&#24335;&#65288;&#21548;&#35273;&#21644;&#35270;&#35273;&#65289;&#12290;&#36890;&#36807;&#37319;&#35775;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21442;&#19982;&#32773;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#27604;&#36739;&#21508;&#32452;&#20043;&#38388;&#65292;&#25105;&#20204;&#21457;&#29616;&#20449;&#24687;&#30340;&#31867;&#22411;&#21644;&#26041;&#24335;&#23545;&#24615;&#33021;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#24046;&#24322;&#24402;&#22240;&#20110;&#20449;&#24687;&#22914;&#20309;&#24341;&#23548;&#27880;&#24847;&#21147;&#65292;&#20943;&#36731;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24433;&#21709;&#21442;&#19982;&#32773;&#32463;&#21382;&#30340;&#36127;&#33655;&#36807;&#36733;&#12290;&#36825;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#20102;&#20449;&#24515;&#21644;&#20449;&#20219;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a pre-post experiment (n = 41), we test the impact of an AI Coach's explanatory communications modeled after the instructions of human driving experts. Participants were divided into four (4) groups to assess two (2) dimensions of the AI coach's explanations: information type ('what' and 'why'-type explanations) and presentation modality (auditory and visual). We directly compare how AI Coaching sessions employing these techniques impact driving performance, cognitive load, confidence, expertise, and trust in an observation learning context. Through interviews, we delineate the learning process of our participants. Results show that an AI driving coach can be useful for teaching performance driving skills to novices. Comparing between groups, we find the type and modality of information influences performance outcomes. We attribute differences to how information directed attention, mitigated uncertainty, and influenced overload experienced by participants. These, in turn, affected h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25351;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#36890;&#36807;&#22909;&#22855;&#24515;&#21644;&#29109;&#39537;&#21160;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.04198</link><description>&lt;p&gt;
&#26080;&#25351;&#23548;&#19979;&#22810;&#29615;&#22659;&#20013;&#30340;&#22909;&#22855;&#24515;&#19982;&#29109;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curiosity &amp; Entropy Driven Unsupervised RL in Multiple Environments. (arXiv:2401.04198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25351;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#36890;&#36807;&#22909;&#22855;&#24515;&#21644;&#29109;&#39537;&#21160;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;alpha-MEPOL&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#29615;&#22659;&#19979;&#30340;&#26080;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#20182;&#20204;&#36890;&#36807;&#20351;&#29992;&#25972;&#20010;&#29615;&#22659;&#31867;&#21035;&#30340;&#20132;&#20114;&#26469;&#39044;&#35757;&#32451;&#19968;&#20010;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#28982;&#21518;&#21033;&#29992;&#30417;&#30563;&#26469;&#23545;&#21508;&#31181;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#21407;&#22987;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#20027;&#35201;&#25552;&#20986;&#24182;&#23454;&#39564;&#20102;&#20116;&#20010;&#26032;&#30340;&#20462;&#25913;&#26041;&#27861;&#65306;&#20351;&#29992;&#22522;&#20110;&#29109;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#37319;&#26679;&#36712;&#36857;&#65292;&#21160;&#24577;alpha&#65292;&#26356;&#39640;&#30340;KL&#25955;&#24230;&#38408;&#20540;&#65292;&#20197;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#25506;&#32034;&#65292;&#20197;&#21450;&#22522;&#20110;&#22909;&#22855;&#24515;&#30340;alpha&#20998;&#20301;&#25968;&#37319;&#26679;&#12290;&#21160;&#24577;alpha&#21644;&#26356;&#39640;&#30340;KL&#25955;&#24230;&#38408;&#20540;&#37117;&#30456;&#23545;&#20110;&#26089;&#26399;&#24037;&#20316;&#30340;&#22522;&#32447;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#24403;&#26679;&#26412;&#31354;&#38388;&#36739;&#23567;&#26102;&#65292;PDF&#37319;&#26679;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#25913;&#36827;&#65292;&#22240;&#20026;&#23427;&#19982;&#22522;&#32447;&#26041;&#27861;&#36817;&#20284;&#31561;&#20215;&#12290;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#20462;&#25913;&#26041;&#27861;&#30340;&#28155;&#21152;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The authors of 'Unsupervised Reinforcement Learning in Multiple environments' propose a method, alpha-MEPOL, to tackle unsupervised RL across multiple environments. They pre-train a task-agnostic exploration policy using interactions from an entire environment class and then fine-tune this policy for various tasks using supervision. We expanded upon this work, with the goal of improving performance. We primarily propose and experiment with five new modifications to the original work: sampling trajectories using an entropy-based probability distribution, dynamic alpha, higher KL Divergence threshold, curiosity-driven exploration, and alpha-percentile sampling on curiosity. Dynamic alpha and higher KL-Divergence threshold both provided a significant improvement over the baseline from the earlier work. PDF-sampling failed to provide any improvement due to it being approximately equivalent to the baseline method when the sample space is small. In high-dimensional environments, the addition
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#20132;&#20114;&#24335;&#36827;&#21270;&#35745;&#31639;&#24212;&#29992;&#20110;&#36719;&#20214;&#26550;&#26500;&#35774;&#35745;&#20013;&#65292;&#20197;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#34701;&#20837;&#20154;&#30340;&#21028;&#26029;&#65292;&#26082;&#32771;&#34385;&#23450;&#37327;&#26631;&#20934;&#21448;&#32771;&#34385;&#23450;&#24615;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.04192</link><description>&lt;p&gt;
&#36719;&#20214;&#26550;&#26500;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Multi-Objective Evolutionary Optimization of Software Architectures. (arXiv:2401.04192v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04192
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#20132;&#20114;&#24335;&#36827;&#21270;&#35745;&#31639;&#24212;&#29992;&#20110;&#36719;&#20214;&#26550;&#26500;&#35774;&#35745;&#20013;&#65292;&#20197;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#34701;&#20837;&#20154;&#30340;&#21028;&#26029;&#65292;&#26082;&#32771;&#34385;&#23450;&#37327;&#26631;&#20934;&#21448;&#32771;&#34385;&#23450;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#35268;&#33539;&#36807;&#31243;&#20013;&#65292;&#35774;&#35745;&#24072;&#36890;&#24120;&#38656;&#35201;&#35780;&#20272;&#19981;&#21516;&#30340;&#26550;&#26500;&#36873;&#25321;&#20197;&#30830;&#20445;&#28385;&#36275;&#36136;&#37327;&#26631;&#20934;&#12290;&#21363;&#20351;&#36825;&#20123;&#36136;&#37327;&#26041;&#38754;&#21487;&#20197;&#29992;&#22810;&#20010;&#36719;&#20214;&#24230;&#37327;&#26469;&#34920;&#31034;&#65292;&#20854;&#20182;&#23450;&#24615;&#22240;&#32032;&#26080;&#27861;&#36890;&#36807;&#25968;&#20540;&#27979;&#37327;&#65292;&#32780;&#26159;&#20174;&#24037;&#31243;&#24072;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#20808;&#21069;&#32463;&#39564;&#20013;&#25552;&#21462;&#20986;&#26469;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#21457;&#29616;&#19981;&#20165;&#20165;&#26159;&#35299;&#20915;&#26041;&#26696;&#30340;&#24378;&#28857;&#65292;&#36824;&#26377;&#24369;&#28857;&#65292;&#20284;&#20046;&#26356;&#31526;&#21512;&#20154;&#31867;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;&#23558;&#20154;&#31867;&#32435;&#20837;&#25628;&#32034;&#36807;&#31243;&#20013;&#20026;&#38754;&#21521;&#20154;&#31867;&#30340;&#27963;&#21160;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#26089;&#26399;&#20998;&#26512;&#38454;&#27573;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20132;&#20114;&#24335;&#36827;&#21270;&#35745;&#31639;&#22914;&#20309;&#20316;&#20026;&#23558;&#20154;&#31867;&#21028;&#26029;&#34701;&#20837;&#25628;&#32034;&#36807;&#31243;&#30340;&#22522;&#30784;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26041;&#27861;&#26469;&#21457;&#29616;&#36719;&#20214;&#26550;&#26500;&#65292;&#20854;&#20013;&#21516;&#26102;&#24212;&#29992;&#23450;&#37327;&#21644;&#23450;&#24615;&#26631;&#20934;&#26469;&#24341;&#23548;&#25628;&#32034;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
While working on a software specification, designers usually need to evaluate different architectural alternatives to be sure that quality criteria are met. Even when these quality aspects could be expressed in terms of multiple software metrics, other qualitative factors cannot be numerically measured, but they are extracted from the engineer's know-how and prior experiences. In fact, detecting not only strong but also weak points in the different solutions seems to fit better with the way humans make their decisions. Putting the human in the loop brings new challenges to the search-based software engineering field, especially for those human-centered activities within the early analysis phase. This paper explores how the interactive evolutionary computation can serve as a basis for integrating the human's judgment into the search process. An interactive approach is proposed to discover software architectures, in which both quantitative and qualitative criteria are applied to guide a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36873;&#25321;&#24615;&#38899;&#39057;&#25513;&#34109;&#22810;&#27169;&#24577;&#29942;&#39048;Transformer&#29992;&#20110;&#38899;&#35270;&#39057;&#20998;&#31867;&#65292;&#36890;&#36807;&#38899;&#35270;&#39057;Transformer&#25552;&#21462;&#26102;&#31354;&#34920;&#31034;&#24182;&#32467;&#21512;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#35821;&#20041;&#38899;&#39057;&#27963;&#21160;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.04154</link><description>&lt;p&gt;
&#39640;&#25928;&#36873;&#25321;&#24615;&#38899;&#39057;&#25513;&#34109;&#22810;&#27169;&#24577;&#29942;&#39048;Transformer&#29992;&#20110;&#38899;&#35270;&#39057;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient Selective Audio Masked Multimodal Bottleneck Transformer for Audio-Video Classification. (arXiv:2401.04154v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04154
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36873;&#25321;&#24615;&#38899;&#39057;&#25513;&#34109;&#22810;&#27169;&#24577;&#29942;&#39048;Transformer&#29992;&#20110;&#38899;&#35270;&#39057;&#20998;&#31867;&#65292;&#36890;&#36807;&#38899;&#35270;&#39057;Transformer&#25552;&#21462;&#26102;&#31354;&#34920;&#31034;&#24182;&#32467;&#21512;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#35821;&#20041;&#38899;&#39057;&#27963;&#21160;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#21644;&#35270;&#39057;&#26159;&#20027;&#27969;&#23186;&#20307;&#24179;&#21488;&#65288;&#22914;YouTube&#65289;&#20013;&#26368;&#24120;&#35265;&#30340;&#20004;&#31181;&#24418;&#24335;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20174;&#22810;&#27169;&#24577;&#35270;&#39057;&#20013;&#23398;&#20064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#35270;&#39057;&#35782;&#21035;&#26041;&#27861;&#65292;&#31216;&#20026;&#38899;&#35270;&#39057;Transformer&#65288;AVT&#65289;&#65292;&#21033;&#29992;&#35270;&#39057;Transformer&#30340;&#26377;&#25928;&#26102;&#31354;&#34920;&#31034;&#26469;&#25552;&#39640;&#21160;&#20316;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#31616;&#21333;&#22320;&#36890;&#36807;&#36328;&#27169;&#24577;Transformer&#36830;&#25509;&#22810;&#27169;&#24577;&#20196;&#29260;&#20250;&#21344;&#29992;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#65292;&#22240;&#27492;&#25105;&#20204;&#36890;&#36807;&#38899;&#35270;&#39057;&#29942;&#39048;Transformer&#38477;&#20302;&#20102;&#36328;&#27169;&#24577;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#22810;&#27169;&#24577;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#25105;&#20204;&#23558;&#33258;&#30417;&#30563;&#30446;&#26631;&#65288;&#21363;&#38899;&#35270;&#39057;&#23545;&#27604;&#23398;&#20064;&#12289;&#38899;&#35270;&#39057;&#21305;&#37197;&#21644;&#38899;&#35270;&#39057;&#36974;&#34109;&#23398;&#20064;&#65289;&#25972;&#21512;&#21040;AVT&#35757;&#32451;&#20013;&#65292;&#23558;&#22810;&#26679;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#34920;&#31034;&#26144;&#23556;&#21040;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38899;&#39057;&#29255;&#27573;&#36974;&#34109;&#25439;&#22833;&#26469;&#23398;&#20064;&#35821;&#20041;&#38899;&#39057;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio and video are two most common modalities in the mainstream media platforms, e.g., YouTube. To learn from multimodal videos effectively, in this work, we propose a novel audio-video recognition approach termed audio video Transformer, AVT, leveraging the effective spatio-temporal representation by the video Transformer to improve action recognition accuracy. For multimodal fusion, simply concatenating multimodal tokens in a cross-modal Transformer requires large computational and memory resources, instead we reduce the cross-modality complexity through an audio-video bottleneck Transformer. To improve the learning efficiency of multimodal Transformer, we integrate self-supervised objectives, i.e., audio-video contrastive learning, audio-video matching, and masked audio and video learning, into AVT training, which maps diverse audio and video representations into a common multimodal representation space. We further propose a masked audio segment loss to learn semantic audio activit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#36890;&#36807;&#19982;SOT&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#27604;SIMO&#22522;&#20934;&#27169;&#22411;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#38477;&#20302;&#20102;8%&#21644;10%&#12290;</title><link>http://arxiv.org/abs/2401.04152</link><description>&lt;p&gt;
&#36328;&#35828;&#35805;&#20154;&#32534;&#30721;&#32593;&#32476;&#29992;&#20110;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Speaker Encoding Network for Multi-Talker Speech Recognition. (arXiv:2401.04152v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#36890;&#36807;&#19982;SOT&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#27604;SIMO&#22522;&#20934;&#27169;&#22411;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#38477;&#20302;&#20102;8%&#21644;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#20316;&#20026;&#19968;&#31181;&#30452;&#25509;&#36716;&#24405;&#22810;&#20010;&#35828;&#35805;&#20154;&#37325;&#21472;&#35821;&#38899;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;1&#65289;&#24102;&#26377;&#20998;&#25903;&#32534;&#30721;&#22120;&#30340;&#21333;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;SIMO&#65289;&#27169;&#22411;&#65292;&#25110;&#32773;2&#65289;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;SOT&#65289;&#30340;&#21333;&#36755;&#20837;&#21333;&#36755;&#20986;&#65288;SISO&#65289;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#26469;&#35299;&#20915;SIMO&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;CSE&#27169;&#22411;&#19982;SOT&#30456;&#32467;&#21512;&#65292;&#26082;&#21457;&#25381;&#20102;SIMO&#21644;SISO&#30340;&#20248;&#21183;&#65292;&#21448;&#32531;&#35299;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#35813;&#24037;&#20316;&#20195;&#34920;&#20102;&#23558;SIMO&#21644;SISO&#38598;&#25104;&#21040;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#26089;&#26399;&#24037;&#20316;&#12290;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;LibrispeechMix&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CES&#27169;&#22411;&#30456;&#27604;&#20110;SIMO&#22522;&#20934;&#27169;&#22411;&#23558;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;8%&#12290;CSE-SOT&#27169;&#22411;&#23558;WER&#38477;&#20302;&#20102;10%
&lt;/p&gt;
&lt;p&gt;
End-to-end multi-talker speech recognition has garnered great interest as an effective approach to directly transcribe overlapped speech from multiple speakers. Current methods typically adopt either 1) single-input multiple-output (SIMO) models with a branched encoder, or 2) single-input single-output (SISO) models based on attention-based encoder-decoder architecture with serialized output training (SOT). In this work, we propose a Cross-Speaker Encoding (CSE) network to address the limitations of SIMO models by aggregating cross-speaker representations. Furthermore, the CSE model is integrated with SOT to leverage both the advantages of SIMO and SISO while mitigating their drawbacks. To the best of our knowledge, this work represents an early effort to integrate SIMO and SISO for multi-talker speech recognition. Experiments on the two-speaker LibrispeechMix dataset show that the CES model reduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model reduces WER by 10
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#25216;&#26415;&#22312;&#26102;&#31354;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#21452;&#37325;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#20998;&#35299;&#21644;&#26657;&#27491;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04148</link><description>&lt;p&gt;
&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#30340;&#26102;&#31354;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting. (arXiv:2401.04148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#25216;&#26415;&#22312;&#26102;&#31354;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#21452;&#37325;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#20998;&#35299;&#21644;&#26657;&#27491;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#26102;&#31354;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#23545;&#20110;&#20132;&#36890;&#31649;&#29702;&#20154;&#21592;&#23454;&#26045;&#25511;&#21046;&#25514;&#26045;&#21644;&#39550;&#39542;&#21592;&#36873;&#25321;&#26368;&#20339;&#34892;&#39542;&#36335;&#32447;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21382;&#21490;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#29992;&#20110;&#23545;&#26410;&#26469;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21382;&#21490;&#25968;&#25454;&#21644;&#26410;&#26469;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#28418;&#31227;&#65292;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#36890;&#24120;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#20351;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#23436;&#20840;&#22312;&#32447;&#30340;&#26041;&#24335;&#19979;&#26356;&#22909;&#22320;&#36866;&#24212;&#26410;&#26469;&#25968;&#25454;&#65292;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#26102;&#31354;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#38382;&#39064;&#30340;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#21452;&#37325;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#21608;&#26399;&#37096;&#20998;&#30340;&#20998;&#35299;&#65292;&#24182;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#27169;&#22359;&#23545;&#20854;&#36827;&#34892;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate spatial-temporal traffic flow forecasting is crucial in aiding traffic managers in implementing control measures and assisting drivers in selecting optimal travel routes. Traditional deep-learning based methods for traffic flow forecasting typically rely on historical data to train their models, which are then used to make predictions on future data. However, the performance of the trained model usually degrades due to the temporal drift between the historical and future data. To make the model trained on historical data better adapt to future data in a fully online manner, this paper conducts the first study of the online test-time adaptation techniques for spatial-temporal traffic flow forecasting problems. To this end, we propose an Adaptive Double Correction by Series Decomposition (ADCSD) method, which first decomposes the output of the trained model into seasonal and trend-cyclical parts and then corrects them by two separate modules during the testing phase using the la
&lt;/p&gt;</description></item><item><title>LOPA&#26159;&#19968;&#31181;&#22686;&#24378;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20840;&#23616;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04145</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20840;&#23616;&#36335;&#24452;&#35268;&#21010;&#30340;&#22686;&#24378;&#27880;&#24847;&#21147;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65306;Learn Once Plan Arbitrarily (LOPA)
&lt;/p&gt;
&lt;p&gt;
Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep Reinforcement Learning Method for Global Path Planning. (arXiv:2401.04145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04145
&lt;/p&gt;
&lt;p&gt;
LOPA&#26159;&#19968;&#31181;&#22686;&#24378;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20840;&#23616;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26041;&#27861;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#20840;&#23616;&#35268;&#21010;&#20219;&#21153;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LOPA&#65288;Learn Once Plan Arbitrarily&#65289;&#30340;&#22686;&#24378;&#27880;&#24847;&#21147;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;DRL&#30340;&#35266;&#23519;&#35282;&#24230;&#20998;&#26512;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;&#35774;&#35745;&#23548;&#33268;DRL&#21463;&#21040;&#26080;&#20851;&#22320;&#22270;&#20449;&#24687;&#30340;&#24178;&#25200;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LOPA&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#24378;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#23545;&#35266;&#23519;&#20013;&#20851;&#38190;&#20449;&#24687;&#30340;&#27880;&#24847;&#21147;&#33021;&#21147;&#12290;&#36825;&#31181;&#26426;&#21046;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;(1)&#26500;&#24314;&#19968;&#20010;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23558;DRL&#30340;&#35266;&#23519;&#36716;&#25442;&#20026;&#20004;&#20010;&#21160;&#24577;&#35270;&#22270;&#65306;&#23616;&#37096;&#21644;&#20840;&#23616;&#65292;&#26174;&#33879;&#25351;&#23548;LOPA&#20851;&#27880;&#32473;&#23450;&#22320;&#22270;&#19978;&#30340;&#20851;&#38190;&#20449;&#24687;&#65307;(2)&#26500;&#24314;&#19968;&#20010;&#21452;&#36890;&#36947;&#32593;&#32476;&#26469;&#22788;&#29702;&#36825;&#20004;&#20010;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) methods have recently shown promise in path planning tasks. However, when dealing with global planning tasks, these methods face serious challenges such as poor convergence and generalization. To this end, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan Arbitrarily) in this paper. Firstly, we analyze the reasons of these problems from the perspective of DRL's observation, revealing that the traditional design causes DRL to be interfered by irrelevant map information. Secondly, we develop the LOPA which utilizes a novel attention-enhanced mechanism to attain an improved attention capability towards the key information of the observation. Such a mechanism is realized by two steps: (1) an attention model is built to transform the DRL's observation into two dynamic views: local and global, significantly guiding the LOPA to focus on the key information on the given maps; (2) a dual-channel network is constructed to process these two
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#40065;&#26834;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#30456;&#27604;&#25552;&#21319;&#26641;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22825;&#27668;&#39044;&#27979;&#20013;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#26356;&#22909;&#26657;&#20934;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.04144</link><description>&lt;p&gt;
&#25913;&#36827;&#22825;&#27668;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Robust Calibration For Improved Weather Prediction Under Distributional Shift. (arXiv:2401.04144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#40065;&#26834;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#30456;&#27604;&#25552;&#21319;&#26641;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22825;&#27668;&#39044;&#27979;&#20013;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#26356;&#22909;&#26657;&#20934;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#8220;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#22312;&#29616;&#23454;&#19990;&#30028;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#25361;&#25112;&#8212;&#8212;&#31227;&#20301;&#25361;&#25112;&#8221;&#20013;&#25913;&#21892;&#36328;&#22495;&#22825;&#27668;&#39044;&#27979;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20511;&#37492;&#30340;&#39640;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20197;&#21450;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#40065;&#26834;&#21518;&#26657;&#20934;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#27604;&#25552;&#21319;&#26641;&#27169;&#22411;&#26356;&#20934;&#30830;&#21644;&#26356;&#22909;&#26657;&#20934;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#25351;&#26631;&#23545;&#25105;&#20204;&#30340;&#39044;&#27979;&#36827;&#34892;&#37327;&#21270;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30340;&#25506;&#31350;&#21644;&#23454;&#39564;&#26041;&#21521;&#26469;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present results on improving out-of-domain weather prediction and uncertainty estimation as part of the \texttt{Shifts Challenge on Robustness and Uncertainty under Real-World Distributional Shift} challenge. We find that by leveraging a mixture of experts in conjunction with an advanced data augmentation technique borrowed from the computer vision domain, in conjunction with robust \textit{post-hoc} calibration of predictive uncertainties, we can potentially achieve more accurate and better-calibrated results with deep neural networks than with boosted tree models for tabular data. We quantify our predictions using several metrics and propose several future lines of inquiry and experimentation to boost performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20998;&#24418;&#20960;&#20309;&#30340;&#28508;&#21147;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#20854;&#32534;&#30721;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#26512;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#32593;&#32476;&#19981;&#33021;&#25552;&#21462;&#22797;&#26434;&#19988;&#39640;&#23618;&#27425;&#30340;&#20998;&#24418;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23545;&#35937;&#32467;&#26500;&#23545;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#20998;&#24418;&#29305;&#24449;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04141</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#24418;&#20960;&#20309;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#20854;&#32534;&#30721;&#33021;&#21147;&#30340;&#28508;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On The Potential of The Fractal Geometry and The CNNs Ability to Encode it. (arXiv:2401.04141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20998;&#24418;&#20960;&#20309;&#30340;&#28508;&#21147;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#20854;&#32534;&#30721;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#26512;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#32593;&#32476;&#19981;&#33021;&#25552;&#21462;&#22797;&#26434;&#19988;&#39640;&#23618;&#27425;&#30340;&#20998;&#24418;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23545;&#35937;&#32467;&#26500;&#23545;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#20998;&#24418;&#29305;&#24449;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24418;&#32500;&#25968;&#36890;&#36807;&#30740;&#31350;&#27169;&#24335;&#22312;&#27979;&#37327;&#23610;&#24230;&#19979;&#30340;&#21464;&#21270;&#26469;&#25552;&#20379;&#23545;&#35937;&#22797;&#26434;&#24615;&#30340;&#32479;&#35745;&#25351;&#26631;&#12290;&#34429;&#28982;&#22312;&#20960;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#26377;&#29992;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#23545;&#20998;&#24418;&#32500;&#25968;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#28145;&#24230;&#32593;&#32476;&#26159;&#21542;&#33021;&#22815;&#20687;&#20998;&#24418;&#32500;&#25968;&#19968;&#26679;&#32534;&#30721;&#22797;&#26434;&#19988;&#39640;&#23618;&#27425;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#28145;&#24230;&#32593;&#32476;&#22312;&#20219;&#20309;&#23618;&#27425;&#37117;&#19981;&#33021;&#25552;&#21462;&#20986;&#36825;&#26679;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#20998;&#26512;&#30740;&#31350;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#21644;&#20165;&#25805;&#20316;&#20998;&#24418;&#29305;&#24449;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20998;&#24418;&#29305;&#24449;&#22312;&#38656;&#35201;&#23545;&#35937;&#32467;&#26500;&#23545;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#22312;&#20998;&#24418;&#29305;&#24449;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fractal dimension provides a statistical index of object complexity by studying how the pattern changes with the measuring scale. Although useful in several classification tasks, the fractal dimension is under-explored in deep learning applications. In this work, we investigate the features that are learned by deep models and we study whether these deep networks are able to encode features as complex and high-level as the fractal dimensions. Specifically, we conduct a correlation analysis experiment to show that deep networks are not able to extract such a feature in none of their layers. We combine our analytical study with a human evaluation to investigate the differences between deep learning networks and models that operate on the fractal feature solely. Moreover, we show the effectiveness of fractal features in applications where the object structure is crucial for the classification task. We empirically show that training a shallow network on fractal features achieves perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLM&#30340;&#23450;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#30340;&#26032;&#35270;&#37326;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#24615;&#33021;&#19978;&#19981;&#20165;&#19982;&#20256;&#32479;&#20998;&#26512;&#26041;&#27861;&#30456;&#24403;&#65292;&#36824;&#33021;&#25552;&#20379;&#29420;&#29305;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.04138</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#23450;&#24615;&#20998;&#26512;&#25299;&#23637;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#30340;&#35270;&#37326;
&lt;/p&gt;
&lt;p&gt;
Expanding Horizons in HCI Research Through LLM-Driven Qualitative Analysis. (arXiv:2401.04138v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLM&#30340;&#23450;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#30340;&#26032;&#35270;&#37326;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#24615;&#33021;&#19978;&#19981;&#20165;&#19982;&#20256;&#32479;&#20998;&#26512;&#26041;&#27861;&#30456;&#24403;&#65292;&#36824;&#33021;&#25552;&#20379;&#29420;&#29305;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#25105;&#20204;&#20173;&#28982;&#38656;&#35201;&#29992;&#25171;&#23383;&#26426;&#8220;&#21457;&#36865;&#8221;&#35770;&#25991;&#65292;&#30740;&#31350;&#20250;&#26159;&#24590;&#26679;&#30340;&#65311;&#25105;&#20204;&#30340;&#29983;&#27963;&#21644;&#30740;&#31350;&#29615;&#22659;&#19981;&#26029;&#28436;&#21464;&#65292;&#24448;&#24448;&#20276;&#38543;&#30528;&#23545;&#26032;&#26041;&#27861;&#35770;&#30340;&#20105;&#35758;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;Large Language Models&#65288;LLMs&#65289;&#30340;&#23450;&#24615;&#20998;&#26512;&#26032;&#26041;&#27861;&#26469;&#25317;&#25265;&#36825;&#31181;&#21464;&#21270;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#36827;&#34892;&#23450;&#24615;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;SBART&#20313;&#24358;&#30456;&#20284;&#24230;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30340;&#23450;&#37327;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#19981;&#20165;&#19982;&#20256;&#32479;&#20998;&#26512;&#26041;&#27861;&#26377;&#25928;&#24615;&#30456;&#21305;&#37197;&#65292;&#36824;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#27934;&#23519;&#21147;&#12290;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;LLMs&#22312;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#30340;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#24212;&#29992;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
How would research be like if we still needed to "send" papers typed with a typewriter? Our life and research environment have continually evolved, often accompanied by controversial opinions about new methodologies. In this paper, we embrace this change by introducing a new approach to qualitative analysis in HCI using Large Language Models (LLMs). We detail a method that uses LLMs for qualitative data analysis and present a quantitative framework using SBART cosine similarity for performance evaluation. Our findings indicate that LLMs not only match the efficacy of traditional analysis methods but also offer unique insights. Through a novel dataset and benchmark, we explore LLMs' characteristics in HCI research, suggesting potential avenues for further exploration and application in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#29256;&#26435;&#20445;&#25252;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#12290;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#26080;&#38656;&#25805;&#20316;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#36807;&#31243;&#65292;&#20165;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#38598;&#25554;&#20837;&#27745;&#26579;&#25968;&#25454;&#26469;&#23454;&#26045;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.04136</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24378;&#22823;&#65292;&#21518;&#38376;&#26356;&#23481;&#26131;: &#25968;&#25454;&#27745;&#26579;&#24341;&#21457;&#29256;&#26435;&#20405;&#29359;&#32780;&#26080;&#38656;&#35843;&#25972;&#24494;&#35843;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline. (arXiv:2401.04136v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#29256;&#26435;&#20445;&#25252;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#12290;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#26080;&#38656;&#25805;&#20316;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#36807;&#31243;&#65292;&#20165;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#38598;&#25554;&#20837;&#27745;&#26579;&#25968;&#25454;&#26469;&#23454;&#26045;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#28508;&#22312;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21518;&#38376;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65288;SilentBadDiffusion&#65289;&#26469;&#25506;&#35752;&#25193;&#25955;&#27169;&#22411;&#20013;&#19982;&#29256;&#26435;&#20445;&#25252;&#30456;&#20851;&#30340;&#28431;&#27934;&#12290;&#25915;&#20987;&#26041;&#27861;&#26080;&#38656;&#35775;&#38382;&#25110;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#36807;&#31243;&#65292;&#20165;&#28041;&#21450;&#23558;&#27745;&#26579;&#25968;&#25454;&#25554;&#20837;&#24178;&#20928;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#12290;&#36825;&#20123;&#25968;&#25454;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The commercialization of diffusion models, renowned for their ability to generate high-quality images that are often indistinguishable from real ones, brings forth potential copyright concerns. Although attempts have been made to impede unauthorized access to copyrighted material during training and to subsequently prevent DMs from generating copyrighted images, the effectiveness of these solutions remains unverified. This study explores the vulnerabilities associated with copyright protection in DMs by introducing a backdoor data poisoning attack (SilentBadDiffusion) against text-to-image diffusion models. Our attack method operates without requiring access to or control over the diffusion model's training or fine-tuning processes; it merely involves the insertion of poisoning data into the clean training dataset. This data, comprising poisoning images equipped with prompts, is generated by leveraging the powerful capabilities of multimodal large language models and text-guided image 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GA-STGRN&#30340;&#26032;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#26102;&#31354;&#24314;&#27169;&#12290;&#35813;&#26694;&#26550;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;&#20840;&#23616;&#24863;&#30693;&#23618;&#26469;&#24110;&#21161;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#24314;&#27169;&#38750;&#22266;&#23450;&#30340;&#22270;&#32467;&#26500;&#21644;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#24207;&#21015;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.04135</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#23616;&#24863;&#30693;&#22686;&#24378;&#30340;&#26102;&#31354;&#22270;&#36882;&#24402;&#32593;&#32476;&#65306;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New Framework For Traffic Flow Prediction. (arXiv:2401.04135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GA-STGRN&#30340;&#26032;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#26102;&#31354;&#24314;&#27169;&#12290;&#35813;&#26694;&#26550;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;&#20840;&#23616;&#24863;&#30693;&#23618;&#26469;&#24110;&#21161;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#24314;&#27169;&#38750;&#22266;&#23450;&#30340;&#22270;&#32467;&#26500;&#21644;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#24207;&#21015;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#22312;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#21644;&#25552;&#39640;&#36816;&#36755;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#36827;&#34892;&#26102;&#31354;&#24314;&#27169;&#22312;&#36825;&#20010;&#39046;&#22495;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#65292;&#20294;&#26159;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21463;&#38480;&#32467;&#26500;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#31354;&#38388;&#24314;&#27169;&#65292;&#35768;&#22810;&#20043;&#21069;&#30340;&#30740;&#31350;&#23398;&#20064;&#19968;&#20010;&#34987;&#20551;&#35774;&#20026;&#22266;&#23450;&#19988;&#22312;&#25152;&#26377;&#26102;&#38388;&#27493;&#39588;&#19978;&#22343;&#21248;&#30340;&#22270;&#32467;&#26500;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#27491;&#30830;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;&#20840;&#23616;&#24863;&#30693;&#22686;&#24378;&#26102;&#31354;&#22270;&#36882;&#24402;&#32593;&#32476;&#65288;GA-STGRN&#65289;&#65292;&#21253;&#25324;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;&#26102;&#31354;&#22270;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#20840;&#23616;&#24863;&#30693;&#23618;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#21019;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#20013;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#19979;&#38750;&#22266;&#23450;&#30340;&#22270;&#24418;&#24182;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow prediction plays a crucial role in alleviating traffic congestion and enhancing transport efficiency. While combining graph convolution networks with recurrent neural networks for spatial-temporal modeling is a common strategy in this realm, the restricted structure of recurrent neural networks limits their ability to capture global information. For spatial modeling, many prior studies learn a graph structure that is assumed to be fixed and uniform at all time steps, which may not be true. This paper introduces a novel traffic prediction framework, Global-Aware Enhanced Spatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core components: a spatial-temporal graph recurrent neural network and a global awareness layer. Within this framework, three innovative prediction models are formulated. A sequence-aware graph neural network is proposed and integrated into the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time steps and capture local te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23436;&#20840;&#26377;&#21521;&#22270;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#36830;&#32493;&#25968;&#25454;&#22788;&#29702;&#65292;&#26356;&#25509;&#36817;&#29983;&#29289;&#33041;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#21644;&#28040;&#38500;&#39034;&#24207;&#24615;&#26469;&#22686;&#21152;&#39069;&#22806;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04134</link><description>&lt;p&gt;
&#20351;&#29992;&#23436;&#20840;&#26377;&#21521;&#22270;&#30340;Web&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Web Neural Network with Complete DiGraphs. (arXiv:2401.04134v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23436;&#20840;&#26377;&#21521;&#22270;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#36830;&#32493;&#25968;&#25454;&#22788;&#29702;&#65292;&#26356;&#25509;&#36817;&#29983;&#29289;&#33041;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#21644;&#28040;&#38500;&#39034;&#24207;&#24615;&#26469;&#22686;&#21152;&#39069;&#22806;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#26088;&#22312;&#26356;&#25509;&#36817;&#29983;&#29289;&#33041;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#32467;&#26500;&#21270;&#20026;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#22788;&#29702;&#36830;&#32493;&#25968;&#25454;&#30340;&#23436;&#20840;&#26377;&#21521;&#22270;&#12290;&#30446;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#27169;&#31946;&#22320;&#27169;&#20223;&#33041;&#32467;&#26500;&#65292;&#20363;&#22914;&#31070;&#32463;&#20803;&#12289;&#21367;&#31215;&#21644;&#24490;&#29615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#20803;&#36830;&#25509;&#20013;&#30340;&#24490;&#29615;&#21644;&#28040;&#38500;&#24120;&#35265;&#30340;&#39034;&#24207;&#24615;&#26469;&#22686;&#21152;&#39069;&#22806;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36830;&#32493;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21463;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#20801;&#35768;&#32593;&#32476;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#65292;&#32780;&#19981;&#20165;&#20165;&#36820;&#22238;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new neural network model that aims to mimic the biological brain more closely by structuring the network as a complete directed graph that processes continuous data for each timestep. Current neural networks have structures that vaguely mimic the brain structure, such as neurons, convolutions, and recurrence. The model proposed in this paper adds additional structural properties by introducing cycles into the neuron connections and removing the sequential nature commonly seen in other network layers. Furthermore, the model has continuous input and output, inspired by spiking neural networks, which allows the network to learn a process of classification, rather than simply returning the final result.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#65292;&#26500;&#24314;&#21512;&#25104;&#32593;&#32476;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25509;&#36817;&#12290;&#36825;&#25552;&#20379;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#21512;&#25104;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.04133</link><description>&lt;p&gt;
SynHIN: &#29983;&#25104;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SynHIN: Generating Synthetic Heterogeneous Information Network for Explainable AI. (arXiv:2401.04133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#65292;&#26500;&#24314;&#21512;&#25104;&#32593;&#32476;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25509;&#36817;&#12290;&#36825;&#25552;&#20379;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#21512;&#25104;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#26377;&#30528;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#20174;&#26816;&#27979;&#30005;&#23376;&#21830;&#21153;&#22403;&#22334;&#37038;&#20214;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#20849;&#22270;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#30740;&#31350;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#26041;&#38754;&#12290;&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#23545;&#20110;&#20844;&#24179;&#30340;HIN&#27604;&#36739;&#32780;&#35328;&#65292;&#38656;&#35201;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SynHIN&#65292;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;SynHIN&#35782;&#21035;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#65292;&#24635;&#32467;&#22270;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#21512;&#25104;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;In-Cluster&#21644;Out-Cluster Merge&#27169;&#22359;&#20174;&#20027;&#35201;&#30340;&#27169;&#24335;&#38598;&#32676;&#26500;&#24314;&#21512;&#25104;HIN&#12290;&#22312;In/Out-Cluster&#21512;&#24182;&#21644;&#31526;&#21512;&#30495;&#23454;&#25968;&#25454;&#38598;&#32422;&#26463;&#30340;&#21518;&#20462;&#21098;&#36807;&#31243;&#21518;&#65292;&#25105;&#20204;&#30830;&#20445;&#21512;&#25104;&#30340;&#22270;&#32479;&#35745;&#25968;&#25454;&#19982;&#21442;&#32771;&#25968;&#25454;&#25509;&#36817;&#12290;SynHIN&#29983;&#25104;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#21512;&#25104;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20027;&#35201;&#30340;&#27169;&#24335;&#20316;&#20026;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) excel in various domains, from detecting e-commerce spam to social network classification problems. However, the lack of public graph datasets hampers research progress, particularly in heterogeneous information networks (HIN). The demand for datasets for fair HIN comparisons is growing due to advancements in GNN interpretation models. In response, we propose SynHIN, a unique method for generating synthetic heterogeneous information networks. SynHIN identifies motifs in real-world datasets, summarizes graph statistics, and constructs a synthetic network. Our approach utilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN from primary motif clusters. After In/Our-Cluster mergers and a post-pruning process fitting the real dataset constraints, we ensure the synthetic graph statistics align closely with the reference one. SynHIN generates a synthetic heterogeneous graph dataset for node classification tasks, using the primary motif as the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.04130</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65306;&#36890;&#36807;&#25554;&#20837;&#21644;&#25773;&#25918;&#21464;&#25442;&#22120;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;(PET)&#26041;&#27861;&#65292;&#22914;LoRA&#12289;Adapter&#21644;Visual Prompt Tuning(VPT)&#65292;&#36890;&#36807;&#35843;&#25972;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#30340;&#23567;&#27169;&#22359;&#65292;&#22312;&#20351;&#36866;&#24212;&#26032;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#39046;&#22495;&#25968;&#37327;&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#26631;&#31614;&#30340;&#12290;&#22240;&#27492;&#65292;&#36866;&#24212;&#26032;&#39046;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20063;&#19981;&#29616;&#23454;&#20026;&#27599;&#20010;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#23450;&#21046;&#30340;&#35843;&#25972;&#27169;&#22359;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;PLUTO&#65306;&#19968;&#31181;&#25554;&#25300;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#39044;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#19987;&#20026;&#19981;&#21516;&#30340;&#28304;&#39046;&#22495;&#36827;&#34892;&#20102;&#19987;&#38376;&#35774;&#35745;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#30446;&#26631;&#22495;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;(TTA)&#26041;&#27861;&#65292;&#26469;(1)&#20174;&#24211;&#20013;&#36873;&#25321;&#20986;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;(2)&#22312;&#19981;&#35843;&#25972;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36825;&#31181;&#25554;&#25300;&#24335;&#30340;&#29305;&#24615;&#20351;&#24471;&#23427;&#21487;===
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enable
&lt;/p&gt;</description></item><item><title>&#30450;&#20154;&#35302;&#35273;&#31614;&#21517;&#31995;&#32479;&#26159;&#19968;&#31181;&#26080;&#38556;&#30861;&#21644;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#35302;&#35273;&#20114;&#21160;&#21644;&#35821;&#38899;&#31639;&#27861;&#24341;&#23548;&#65292;&#36171;&#20104;&#35270;&#38556;&#20154;&#22763;&#21019;&#24314;&#20010;&#24615;&#21270;&#25163;&#20889;&#31614;&#21517;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#20182;&#20204;&#30340;&#29420;&#31435;&#21644;&#21253;&#23481;&#12290;</title><link>http://arxiv.org/abs/2401.04126</link><description>&lt;p&gt;
&#30450;&#20154;&#35302;&#35273;&#31614;&#21517;&#31995;&#32479;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
The Concept of the Tactile Signature System for Individuals with Visual Impairments. (arXiv:2401.04126v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04126
&lt;/p&gt;
&lt;p&gt;
&#30450;&#20154;&#35302;&#35273;&#31614;&#21517;&#31995;&#32479;&#26159;&#19968;&#31181;&#26080;&#38556;&#30861;&#21644;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#35302;&#35273;&#20114;&#21160;&#21644;&#35821;&#38899;&#31639;&#27861;&#24341;&#23548;&#65292;&#36171;&#20104;&#35270;&#38556;&#20154;&#22763;&#21019;&#24314;&#20010;&#24615;&#21270;&#25163;&#20889;&#31614;&#21517;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#20182;&#20204;&#30340;&#29420;&#31435;&#21644;&#21253;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#19968;&#31181;&#26080;&#38556;&#30861;&#21644;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#20351;&#30450;&#20154;&#33021;&#22815;&#21019;&#24314;&#25163;&#20889;&#31614;&#21517;&#65292;&#36825;&#23545;&#20182;&#20204;&#30340;&#29420;&#31435;&#24615;&#21644;&#20840;&#38754;&#21442;&#19982;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#21363;&#35302;&#35273;&#31614;&#21517;&#31995;&#32479;&#65292;&#36171;&#20104;&#35270;&#38556;&#20154;&#22763;&#24418;&#25104;&#29420;&#29305;&#30340;&#25163;&#20889;&#31614;&#21517;&#30340;&#33021;&#21147;&#12290;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#28857;&#21253;&#25324;&#65306;&#20010;&#24615;&#21270;&#23450;&#21046;&#65306;&#36890;&#36807;&#35302;&#35273;&#20114;&#21160;&#21644;&#35821;&#38899;&#31639;&#27861;&#24341;&#23548;&#65292;&#20010;&#20307;&#21487;&#20197;&#21019;&#24314;&#21453;&#26144;&#20854;&#20559;&#22909;&#21644;&#33258;&#28982;&#20070;&#20889;&#39118;&#26684;&#30340;&#31614;&#21517;&#12290;&#23454;&#26102;&#21453;&#39304;&#65306;AI&#39537;&#21160;&#30340;&#35821;&#38899;&#25552;&#31034;&#21644;&#20998;&#26512;&#30830;&#20445;&#31614;&#21517;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#21487;&#35775;&#38382;&#24615;&#65306;&#22312;&#26412;&#22320;&#26381;&#21153;&#20013;&#24515;&#23433;&#35013;&#65292;&#25552;&#20379;&#23433;&#20840;&#30417;&#30563;&#30340;&#29615;&#22659;&#36827;&#34892;&#31614;&#21517;&#21019;&#24314;&#12290;&#35813;&#31995;&#32479;&#30340;&#24433;&#21709;&#36229;&#36234;&#20010;&#20307;&#23618;&#38754;&#65306;&#20419;&#36827;&#21253;&#23481;&#21644;&#29420;&#31435;&#65306;&#30450;&#20154;&#21487;&#20197;&#36827;&#34892;&#27861;&#24459;&#21644;&#37329;&#34701;&#20132;&#26131;&#32780;&#26080;&#38656;&#20381;&#36182;&#20182;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of an accessible and effective system for blind individuals to create handwritten signatures presents a significant barrier to their independence and full participation in various aspects of life. This research introduces the Tactile Signature System, a groundbreaking approach that empowers individuals with visual impairments to form their unique handwritten signatures. Key features of the system include: Personalized customization: Through tactile interaction and voice algorithmic guidance, individuals create signatures reflecting their preferences and natural writing style. Real-time feedback: AI-powered voice prompts and analysis ensure accuracy and consistency in signature formation. Accessibility: Installation in local service centers provides a secure and supervised environment for signature creation. The system's impact reaches beyond the individual level: Promotes inclusivity and independence: Blind individuals can engage in legal and financial transactions without rel
&lt;/p&gt;</description></item><item><title>DeepPhysiNet&#26694;&#26550;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#36830;&#32493;&#30340;&#22825;&#27668;&#31995;&#32479;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2401.04125</link><description>&lt;p&gt;
DeepPhysiNet: &#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#27668;&#29289;&#29702;&#23398;&#36827;&#34892;&#20934;&#30830;&#21644;&#36830;&#32493;&#30340;&#22825;&#27668;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for Accurate and Continuous Weather Modeling. (arXiv:2401.04125v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04125
&lt;/p&gt;
&lt;p&gt;
DeepPhysiNet&#26694;&#26550;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#36830;&#32493;&#30340;&#22825;&#27668;&#31995;&#32479;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#22825;&#27668;&#39044;&#25253;&#23545;&#20154;&#31867;&#27963;&#21160;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#22825;&#27668;&#39044;&#25253;&#26377;&#20004;&#31181;&#33539;&#24335;&#65306;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#65288;DLP&#65289;&#12290;NWP&#21033;&#29992;&#22823;&#27668;&#29289;&#29702;&#23398;&#36827;&#34892;&#22825;&#27668;&#27169;&#25311;&#65292;&#20294;&#25968;&#25454;&#21033;&#29992;&#19981;&#36275;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#20351;&#20854;&#21463;&#21040;&#38480;&#21046;&#65292;&#32780;DLP&#21487;&#20197;&#30452;&#25509;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#22825;&#27668;&#27169;&#24335;&#65292;&#20294;&#38590;&#20197;&#34701;&#20837;&#29289;&#29702;&#23450;&#24459;&#12290;&#36825;&#20004;&#31181;&#33539;&#24335;&#21508;&#26377;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24182;&#19988;&#19981;&#20114;&#30456;&#20860;&#23481;&#65292;&#22240;&#20026;NWP&#20013;&#37319;&#29992;&#30340;&#29289;&#29702;&#23450;&#24459;&#25551;&#36848;&#20102;&#22352;&#26631;&#21644;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;DLP&#30452;&#25509;&#23398;&#20064;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#32780;&#19981;&#32771;&#34385;&#22352;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DeepPhysiNet&#26694;&#26550;&#65292;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#36830;&#32493;&#30340;&#22825;&#27668;&#31995;&#32479;&#27169;&#25311;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#22810;&#23618;&#32593;&#32476;&#26500;&#24314;&#29289;&#29702;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate weather forecasting holds significant importance to human activities. Currently, there are two paradigms for weather forecasting: Numerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP). NWP utilizes atmospheric physics for weather modeling but suffers from poor data utilization and high computational costs, while DLP can learn weather patterns from vast amounts of data directly but struggles to incorporate physical laws. Both paradigms possess their respective strengths and weaknesses, and are incompatible, because physical laws adopted in NWP describe the relationship between coordinates and meteorological variables, while DLP directly learns the relationships between meteorological variables without consideration of coordinates. To address these problems, we introduce the DeepPhysiNet framework, incorporating physical laws into deep learning models for accurate and continuous weather system modeling. First, we construct physics networks based on multilay
&lt;/p&gt;</description></item><item><title>MobileAgent&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#21644;SOP&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#31227;&#21160;&#25511;&#21046;&#30340;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#38656;&#27714;&#30340;&#28385;&#36275;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#21644;&#20195;&#29702;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04124</link><description>&lt;p&gt;
MobileAgent&#65306;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#21644;SOP&#38598;&#25104;&#22686;&#24378;&#31227;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
MobileAgent: enhancing mobile control via human-machine interaction and SOP integration. (arXiv:2401.04124v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04124
&lt;/p&gt;
&lt;p&gt;
MobileAgent&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#21644;SOP&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#31227;&#21160;&#25511;&#21046;&#30340;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#38656;&#27714;&#30340;&#28385;&#36275;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#21644;&#20195;&#29702;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#20013;&#24515;&#30340;&#20195;&#29702;&#33021;&#22815;&#20026;&#29992;&#25143;&#33258;&#21160;&#21270;&#31227;&#21160;&#35774;&#22791;&#25805;&#20316;&#12290;&#22312;&#38024;&#23545;&#23398;&#20064;&#29992;&#25143;&#30340;&#31227;&#21160;&#25805;&#20316;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#36825;&#20123;&#20195;&#29702;&#21487;&#20197;&#22312;&#32447;&#36981;&#24490;&#39640;&#32423;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#20204;&#25191;&#34892;&#30446;&#26631;&#20998;&#35299;&#12289;&#23376;&#30446;&#26631;&#24207;&#21015;&#21270;&#21644;&#20132;&#20114;&#24335;&#29615;&#22659;&#25506;&#32034;&#31561;&#20219;&#21153;&#65292;&#30452;&#21040;&#23454;&#29616;&#26368;&#32456;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#25805;&#20316;&#20013;&#23384;&#22312;&#19982;&#20010;&#24615;&#21270;&#29992;&#25143;&#25968;&#25454;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#38656;&#35201;&#29992;&#25143;&#30830;&#35748;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#30340;&#30495;&#23454;&#25805;&#20316;&#26159;&#25506;&#32034;&#24615;&#30340;&#65292;&#34892;&#21160;&#25968;&#25454;&#22797;&#26434;&#19988;&#20887;&#20313;&#65292;&#32473;&#20195;&#29702;&#23398;&#20064;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20195;&#29702;&#19982;&#20154;&#20043;&#38388;&#30340;&#20132;&#20114;&#20219;&#21153;&#65292;&#20197;&#35782;&#21035;&#25935;&#24863;&#20449;&#24687;&#24182;&#19982;&#20010;&#24615;&#21270;&#29992;&#25143;&#38656;&#27714;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#38598;&#25104;&#20102;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#65288;SOP&#65289;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents centered around Large Language Models (LLMs) are now capable of automating mobile device operations for users. After fine-tuning to learn a user's mobile operations, these agents can adhere to high-level user instructions online. They execute tasks such as goal decomposition, sequencing of sub-goals, and interactive environmental exploration, until the final objective is achieved. However, privacy concerns related to personalized user data arise during mobile operations, requiring user confirmation. Moreover, users' real-world operations are exploratory, with action data being complex and redundant, posing challenges for agent learning. To address these issues, in our practical application, we have designed interactive tasks between agents and humans to identify sensitive information and align with personalized user needs. Additionally, we integrated Standard Operating Procedure (SOP) information within the model's in-context learning to enhance the agent's comprehension of comp
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20195;&#30721;&#20070;&#26500;&#24314;&#26041;&#27861;&#21644;&#22810;&#38454;&#27573;&#39564;&#35777;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26356;&#31995;&#32479;&#22320;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;LLMs&#12290;</title><link>http://arxiv.org/abs/2401.04122</link><description>&lt;p&gt;
&#20174;&#25552;&#31034;&#24037;&#31243;&#21040;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#25552;&#31034;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
From Prompt Engineering to Prompt Science With Human in the Loop. (arXiv:2401.04122v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04122
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#20070;&#26500;&#24314;&#26041;&#27861;&#21644;&#22810;&#38454;&#27573;&#39564;&#35777;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26356;&#31995;&#32479;&#22320;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;LLMs&#22312;&#25105;&#20204;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#26159;&#19968;&#20010;&#38656;&#35201;&#22686;&#21152;&#23457;&#26597;&#30340;&#22320;&#26041;&#12290;LLMs&#34987;&#29992;&#20110;&#29983;&#25104;&#25110;&#20998;&#26512;&#30740;&#31350;&#25968;&#25454;&#30340;&#24212;&#29992;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#20294;&#26159;&#65292;&#24403;&#36825;&#31181;&#24212;&#29992;&#34987;&#20020;&#26102;&#20915;&#31574;&#21644;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#25152;&#22256;&#25200;&#26102;&#65292;&#25105;&#20204;&#38656;&#35201;&#20851;&#27880;&#23427;&#22914;&#20309;&#24433;&#21709;&#30740;&#31350;&#12289;&#30740;&#31350;&#32467;&#26524;&#25110;&#32773;&#22522;&#20110;&#35813;&#30740;&#31350;&#30340;&#20219;&#20309;&#26410;&#26469;&#24037;&#20316;&#12290;&#25105;&#20204;&#38656;&#35201;&#26356;&#31185;&#23398;&#30340;&#26041;&#27861;&#26469;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;LLMs&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#19968;&#20123;&#31215;&#26497;&#30340;&#21162;&#21147;&#25903;&#25345;&#26356;&#31995;&#32479;&#30340;&#25552;&#31034;&#26500;&#24314;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26356;&#27880;&#37325;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#20135;&#29983;&#21487;&#22797;&#21046;&#21644;&#20855;&#26377;&#36275;&#22815;&#36879;&#26126;&#24230;&#12289;&#23458;&#35266;&#24615;&#25110;&#20005;&#35880;&#24615;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#28789;&#24863;&#26469;&#33258;&#36890;&#36807;&#23450;&#24615;&#26041;&#27861;&#26500;&#24314;&#20195;&#30721;&#20070;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#21644;&#22810;&#38454;&#27573;&#39564;&#35777;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20026;&#26356;&#31995;&#32479;&#30340;&#30740;&#31350;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
As LLMs make their way into many aspects of our lives, one place that warrants increased scrutiny with LLM usage is scientific research. Using LLMs for generating or analyzing data for research purposes is gaining popularity. But when such application is marred with ad-hoc decisions and engineering solutions, we need to be concerned about how it may affect that research, its findings, or any future works based on that research. We need a more scientific approach to using LLMs in our research. While there are several active efforts to support more systematic construction of prompts, they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor. This article presents a new methodology inspired by codebook construction through qualitative methods to address that. Using humans in the loop and a multi-phase verification processes, this methodology lays a foundation for more systema
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Generation Z&#30340;&#20010;&#20307;&#36827;&#34892;&#35843;&#26597;&#65292;&#35780;&#20272;&#20102;&#20182;&#20204;&#22312;Discord&#19978;&#21306;&#20998;AI&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20182;&#20204;&#26080;&#27861;&#26377;&#25928;&#21306;&#20998;&#36825;&#20004;&#31181;&#26469;&#28304;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.04120</link><description>&lt;p&gt;
Generation Z&#22312;Discord&#19978;&#21306;&#20998;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Generation Z's Ability to Discriminate Between AI-generated and Human-Authored Text on Discord. (arXiv:2401.04120v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Generation Z&#30340;&#20010;&#20307;&#36827;&#34892;&#35843;&#26597;&#65292;&#35780;&#20272;&#20102;&#20182;&#20204;&#22312;Discord&#19978;&#21306;&#20998;AI&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20182;&#20204;&#26080;&#27861;&#26377;&#25928;&#21306;&#20998;&#36825;&#20004;&#31181;&#26469;&#28304;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26222;&#21450;&#65292;&#22914;ChatGPT&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#38543;&#30528;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#26222;&#21450;&#65292;&#20154;&#20204;&#23545;&#22312;&#32447;&#38544;&#31169;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#25285;&#24551;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20013;&#65292;Discord&#20801;&#35768;AI&#38598;&#25104;&#65292;&#20351;&#20182;&#20204;&#20197;"Z&#19990;&#20195;"&#20026;&#20027;&#30340;&#29992;&#25143;&#32676;&#20307;&#29305;&#21035;&#23481;&#26131;&#25509;&#35302;&#21040;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#23545;&#24180;&#40836;&#20026;Z&#19990;&#20195;&#30340;&#20010;&#20307;&#65288;n = 335&#65289;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#35780;&#20272;&#20182;&#20204;&#22312;Discord&#19978;&#21306;&#20998;AI&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#35843;&#26597;&#37319;&#29992;&#20102;ChatGPT&#20266;&#35013;&#25104;&#22312;Discord.com&#24179;&#21488;&#19978;&#25910;&#21040;&#30340;&#19968;&#26465;&#30701;&#20449;&#30340;&#21333;&#27425;&#25552;&#31034;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#21644;&#21442;&#19982;&#32773;&#23545;Discord&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#29087;&#24713;&#31243;&#24230;&#23545;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;Z&#19990;&#20195;&#20010;&#20307;&#26080;&#27861;&#21306;&#20998;AI&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#65288;p = 0.011&#65289;&#65292;&#37027;&#20123;&#33258;&#25105;&#25253;&#21578;&#31243;&#24230;&#36739;&#20302;&#30340;&#20010;&#20307;&#33021;&#21147;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing popularity of generative artificial intelligence (AI) chatbots such as ChatGPT is having transformative effects on social media. As the prevalence of AI-generated content grows, concerns have been raised regarding privacy and misinformation online. Among social media platforms, Discord enables AI integrations -- making their primarily "Generation Z" userbase particularly exposed to AI-generated content. We surveyed Generation Z aged individuals (n = 335) to evaluate their proficiency in discriminating between AI-generated and human-authored text on Discord. The investigation employed one-shot prompting of ChatGPT, disguised as a text message received on the Discord.com platform. We explore the influence of demographic factors on ability, as well as participants' familiarity with Discord and artificial intelligence technologies. We find that Generation Z individuals are unable to discern between AI and human-authored text (p = 0.011), and that those with lower self-reported 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#26816;&#27979;&#21644;LIME&#12289;SHAP&#31561;&#35299;&#37322;&#25216;&#26415;&#36827;&#34892;&#35299;&#37322;&#65292;&#25581;&#31034;&#20102;&#40657;&#26263;&#27169;&#24335;&#20013;&#24433;&#21709;&#39044;&#27979;&#30340;&#20851;&#38190;&#26415;&#35821;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#38450;&#33539;&#40657;&#26263;&#27169;&#24335;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.04119</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#29992;&#25143;&#30028;&#38754;&#26159;&#19968;&#31181;&#40657;&#26263;&#27169;&#24335;&#65311;&#65306;&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#26816;&#27979;&#21450;&#20854;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why is the User Interface a Dark Pattern? : Explainable Auto-Detection and its Analysis. (arXiv:2401.04119v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#26816;&#27979;&#21644;LIME&#12289;SHAP&#31561;&#35299;&#37322;&#25216;&#26415;&#36827;&#34892;&#35299;&#37322;&#65292;&#25581;&#31034;&#20102;&#40657;&#26263;&#27169;&#24335;&#20013;&#24433;&#21709;&#39044;&#27979;&#30340;&#20851;&#38190;&#26415;&#35821;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#38450;&#33539;&#40657;&#26263;&#27169;&#24335;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#26263;&#27169;&#24335;&#26159;&#22312;&#32447;&#26381;&#21153;&#20013;&#35823;&#23548;&#29992;&#25143;&#34892;&#20026;&#30340;&#27450;&#39575;&#24615;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#12290;&#38544;&#31169;&#20405;&#29359;&#12289;&#36130;&#21153;&#25439;&#22833;&#21644;&#24773;&#32490;&#22256;&#25200;&#31561;&#40657;&#26263;&#27169;&#24335;&#21487;&#33021;&#20250;&#23545;&#29992;&#25143;&#36896;&#25104;&#20260;&#23475;&#12290;&#36825;&#20123;&#38382;&#39064;&#36817;&#24180;&#26469;&#19968;&#30452;&#26159;&#24191;&#27867;&#35752;&#35770;&#30340;&#35805;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#30340;&#40657;&#26263;&#27169;&#24335;&#33258;&#21160;&#26816;&#27979;&#65292;&#21363;&#20026;&#20160;&#20040;&#20250;&#23558;&#29305;&#23450;&#30340;&#29992;&#25143;&#30028;&#38754;&#26816;&#27979;&#20026;&#20855;&#26377;&#40657;&#26263;&#27169;&#24335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#23545;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#40657;&#26263;&#27169;&#24335;&#36827;&#34892;&#20102;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;LIME&#21644;SHAP&#31561;&#21518;&#32622;&#35299;&#37322;&#25216;&#26415;&#23545;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#35299;&#37322;&#65292;&#25581;&#31034;&#20102;&#24433;&#21709;&#27599;&#20010;&#39044;&#27979;&#20316;&#20026;&#40657;&#26263;&#27169;&#24335;&#30340;&#26415;&#35821;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#21462;&#21644;&#20998;&#26512;&#20102;&#24433;&#21709;&#40657;&#26263;&#27169;&#24335;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#20813;&#21463;&#40657;&#26263;&#27169;&#24335;&#30340;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dark patterns are deceptive user interface designs for online services that make users behave in unintended ways. Dark patterns, such as privacy invasion, financial loss, and emotional distress, can harm users. These issues have been the subject of considerable debate in recent years. In this paper, we study interpretable dark pattern auto-detection, that is, why a particular user interface is detected as having dark patterns. First, we trained a model using transformer-based pre-trained language models, BERT, on a text-based dataset for the automatic detection of dark patterns in e-commerce. Then, we applied post-hoc explanation techniques, including local interpretable model agnostic explanation (LIME) and Shapley additive explanations (SHAP), to the trained model, which revealed which terms influence each prediction as a dark pattern. In addition, we extracted and analyzed terms that affected the dark patterns. Our findings may prevent users from being manipulated by dark patterns, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#20010;&#30740;&#35752;&#20250;&#30340;&#20250;&#35758;&#24405;&#65292;&#26088;&#22312;&#35752;&#35770;&#20154;&#26426;&#23545;&#35805;&#20013;&#30340;&#27807;&#36890;&#38382;&#39064;&#21644;&#22833;&#36133;&#65292;&#20197;&#21450;&#38750;&#26426;&#22120;&#20154;&#35821;&#38899;&#30028;&#38754;&#30340;&#30456;&#20851;&#22833;&#36133;&#12290;&#30446;&#26631;&#26159;&#24443;&#24213;&#35843;&#26597;&#27807;&#36890;&#22833;&#36133;&#65292;&#21046;&#23450;&#20998;&#31867;&#27861;&#65292;&#24182;&#23637;&#24320;&#35299;&#20915;&#26041;&#26696;&#30340;&#21021;&#27493;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.04108</link><description>&lt;p&gt;
&#22312;&#20154;&#26426;&#23545;&#35805;&#20013;&#22788;&#29702;&#22256;&#38590;&#21644;&#22833;&#36133;&#65288;WTF 2023&#65289;&#19982;CUI&#35774;&#35745;&#26159;&#21542;&#20934;&#22791;&#22909;&#65311;&#65288;arXiv:2401.04108v1 [cs.HC]&#65289;
&lt;/p&gt;
&lt;p&gt;
Working with Trouble and Failures in Conversation between Humans and Robots (WTF 2023) &amp; Is CUI Design Ready Yet?. (arXiv:2401.04108v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#20010;&#30740;&#35752;&#20250;&#30340;&#20250;&#35758;&#24405;&#65292;&#26088;&#22312;&#35752;&#35770;&#20154;&#26426;&#23545;&#35805;&#20013;&#30340;&#27807;&#36890;&#38382;&#39064;&#21644;&#22833;&#36133;&#65292;&#20197;&#21450;&#38750;&#26426;&#22120;&#20154;&#35821;&#38899;&#30028;&#38754;&#30340;&#30456;&#20851;&#22833;&#36133;&#12290;&#30446;&#26631;&#26159;&#24443;&#24213;&#35843;&#26597;&#27807;&#36890;&#22833;&#36133;&#65292;&#21046;&#23450;&#20998;&#31867;&#27861;&#65292;&#24182;&#23637;&#24320;&#35299;&#20915;&#26041;&#26696;&#30340;&#21021;&#27493;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;ACM&#20250;&#35758;&#19978;&#20004;&#20010;&#20998;&#20250;&#35758;&#8220;&#22312;&#20154;&#26426;&#23545;&#35805;&#20013;&#22788;&#29702;&#22256;&#38590;&#21644;&#22833;&#36133;&#8221;&#65288;WTF 2023&#65289;&#21644;&#8220;CUI&#35774;&#35745;&#26159;&#21542;&#20934;&#22791;&#22909;&#65311;&#8221;&#30340;&#30740;&#35752;&#20250;&#20250;&#35758;&#24405;&#12290;WTF 23&#26088;&#22312;&#27719;&#38598;&#26469;&#33258;&#20154;&#26426;&#20132;&#20114;&#12289;&#23545;&#35805;&#31995;&#32479;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#23545;&#35805;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;&#23613;&#31649;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#26426;&#22120;&#20154;&#35821;&#38899;&#30028;&#38754;&#22312;&#22810;&#20010;&#26041;&#38754;&#20173;&#28982;&#33030;&#24369;&#65292;&#20154;&#20204;&#23545;&#27492;&#31867;&#30028;&#38754;&#22833;&#36133;&#30340;&#32463;&#21382;&#24182;&#19981;&#32597;&#35265;&#12290;&#28982;&#32780;&#65292;&#25216;&#26415;&#25991;&#29486;&#23545;&#23427;&#20204;&#30340;&#33391;&#22909;&#24615;&#33021;&#26377;&#30528;&#31215;&#26497;&#30340;&#20559;&#21521;&#12290;&#35813;&#30740;&#35752;&#20250;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35752;&#35770;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#27807;&#36890;&#38382;&#39064;&#21644;&#22833;&#36133;&#20197;&#21450;&#38750;&#26426;&#22120;&#20154;&#35821;&#38899;&#30028;&#38754;&#30456;&#20851;&#22833;&#36133;&#30340;&#24179;&#21488;&#12290;&#30446;&#26631;&#21253;&#25324;&#23545;&#27807;&#36890;&#22833;&#36133;&#36827;&#34892;&#24443;&#24213;&#35843;&#26597;&#65292;&#24320;&#22987;&#21046;&#23450;&#36825;&#20123;&#22833;&#36133;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23601;&#35299;&#20915;&#26041;&#26696;&#23637;&#24320;&#21021;&#27493;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Workshop proceedings of two co-located workshops "Working with Troubles and Failures in Conversation with Humans and Robots" (WTF 2023) and "Is CUI Design Ready Yet?", both of which were part of the ACM conference on conversational user interfaces 2023.  WTF 23 aimed at bringing together researchers from human-robot interaction, dialogue systems, human-computer interaction, and conversation analysis. Despite all progress, robotic speech interfaces continue to be brittle in a number of ways and the experience of failure of such interfaces is commonplace amongst roboticists. However, the technical literature is positively skewed toward their good performance. The workshop aims to provide a platform for discussing communicative troubles and failures in human-robot interactions and related failures in non-robotic speech interfaces. Aims include a scrupulous investigation into communicative failures, to begin working on a taxonomy of such failures, and enable a preliminary discussion on pos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26102;&#38388;&#22270;&#23398;&#20064;&#30340;&#22522;&#26412;&#30693;&#35782;&#65292;&#21253;&#25324;TGL&#26694;&#26550;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#12289;&#30456;&#20851;&#30340;&#23398;&#20064;&#26550;&#26500;&#20197;&#21450;&#32463;&#20856;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20026;TGL&#30340;&#21487;&#35299;&#37322;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#28789;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.03988</link><description>&lt;p&gt;
&#26102;&#38388;&#22270;&#23398;&#20064;&#30340;&#22522;&#26412;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
A Primer on Temporal Graph Learning. (arXiv:2401.03988v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26102;&#38388;&#22270;&#23398;&#20064;&#30340;&#22522;&#26412;&#30693;&#35782;&#65292;&#21253;&#25324;TGL&#26694;&#26550;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#12289;&#30456;&#20851;&#30340;&#23398;&#20064;&#26550;&#26500;&#20197;&#21450;&#32463;&#20856;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20026;TGL&#30340;&#21487;&#35299;&#37322;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#27010;&#24565;&#20808;&#34892;&#30340;&#26041;&#27861;&#65292;&#20351;&#35835;&#32773;&#29087;&#24713;&#26102;&#38388;&#22270;&#23398;&#20064;&#65288;TGL&#65289;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#29702;&#35299;TGL&#26694;&#26550;&#25152;&#24517;&#19981;&#21487;&#23569;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#38500;&#20102;&#23450;&#24615;&#35299;&#37322;&#65292;&#25105;&#20204;&#36824;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#24341;&#20837;&#20102;&#25968;&#23398;&#20844;&#24335;&#65292;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#28165;&#26224;&#24230;&#12290;&#30001;&#20110;TGL&#28041;&#21450;&#26102;&#38388;&#21644;&#31354;&#38388;&#23398;&#20064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#20174;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21040;&#36716;&#25442;&#22120;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#32463;&#20856;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#28608;&#21457;&#23545;TGL&#30340;&#21487;&#35299;&#37322;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document aims to familiarize readers with temporal graph learning (TGL) through a concept-first approach. We have systematically presented vital concepts essential for understanding the workings of a TGL framework. In addition to qualitative explanations, we have incorporated mathematical formulations where applicable, enhancing the clarity of the text. Since TGL involves temporal and spatial learning, we introduce relevant learning architectures ranging from recurrent and convolutional neural networks to transformers and graph neural networks. We also discuss classical time series forecasting methods to inspire interpretable learning solutions for TGL.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FlightLLM&#65292;&#19968;&#31181;&#22312;FPGA&#19978;&#23454;&#29616;&#23436;&#25972;&#26144;&#23556;&#27969;&#31243;&#30340;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;FPGA&#29305;&#23450;&#36164;&#28304;&#65292;&#35299;&#20915;&#20102;LLMs&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#37197;&#32622;&#30340;&#31232;&#30095;DSP&#38142;&#20197;&#25903;&#25345;&#19981;&#21516;&#31232;&#30095;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.03868</link><description>&lt;p&gt;
FlightLLM: &#39640;&#25928;&#30340;FPGA&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#19982;&#23436;&#25972;&#26144;&#23556;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs. (arXiv:2401.03868v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03868
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FlightLLM&#65292;&#19968;&#31181;&#22312;FPGA&#19978;&#23454;&#29616;&#23436;&#25972;&#26144;&#23556;&#27969;&#31243;&#30340;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;FPGA&#29305;&#23450;&#36164;&#28304;&#65292;&#35299;&#20915;&#20102;LLMs&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#37197;&#32622;&#30340;&#31232;&#30095;DSP&#38142;&#20197;&#25903;&#25345;&#19981;&#21516;&#31232;&#30095;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#25928;&#29575;&#21463;&#21040;&#20102;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#24320;&#38144;&#30340;&#24433;&#21709;&#12290;&#21387;&#32553;&#25216;&#26415;&#22914;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#24120;&#34987;&#29992;&#20110;&#32531;&#35299;LLMs&#30340;&#35745;&#31639;/&#20869;&#23384;&#24320;&#38144;&#19982;&#30828;&#20214;&#23481;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GPU&#21644;&#22522;&#20110;Transformer&#30340;&#21152;&#36895;&#22120;&#26080;&#27861;&#39640;&#25928;&#22788;&#29702;&#21387;&#32553;&#30340;LLMs&#65292;&#22240;&#20026;&#23384;&#22312;&#20197;&#19979;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65306;&#20302;&#35745;&#31639;&#25928;&#29575;&#12289;&#20302;&#21033;&#29992;&#29575;&#30340;&#20869;&#23384;&#24102;&#23485;&#21644;&#24040;&#22823;&#30340;&#32534;&#35793;&#24320;&#38144;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FlightLLM&#65292;&#36890;&#36807;&#22312;FPGA&#19978;&#23454;&#29616;&#23436;&#25972;&#30340;&#26144;&#23556;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;LLMs&#25512;&#26029;&#12290;&#22312;FlightLLM&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;FPGA&#29305;&#23450;&#30340;&#36164;&#28304;&#65288;&#22914;DSP48&#21644;&#24322;&#26500;&#20869;&#23384;&#23618;&#27425;&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;LLMs&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#37197;&#32622;&#30340;&#31232;&#30095;DSP&#38142;&#65292;&#20197;&#39640;&#35745;&#31639;&#25928;&#29575;&#25903;&#25345;&#19981;&#21516;&#31232;&#30095;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Large Language Models (LLMs) have made a significant impact on various domains. However, LLMs' efficiency suffers from both heavy computation and memory overheads. Compression techniques like sparsification and quantization are commonly used to mitigate the gap between LLM's computation/memory overheads and hardware capacity. However, existing GPU and transformer-based accelerators cannot efficiently process compressed LLMs, due to the following unresolved challenges: low computational efficiency, underutilized memory bandwidth, and large compilation overheads.  This paper proposes FlightLLM, enabling efficient LLMs inference with a complete mapping flow on FPGAs. In FlightLLM, we highlight an innovative solution that the computation and memory overhead of LLMs can be solved by utilizing FPGA-specific resources (e.g., DSP48 and heterogeneous memory hierarchy). We propose a configurable sparse DSP chain to support different sparsity patterns with high computation effic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#25506;&#31350;&#25913;&#21464;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21363;&#20351;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#31572;&#26696;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#35831;&#27714;&#20197;XML&#26684;&#24335;&#36820;&#22238;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.03729</link><description>&lt;p&gt;
&#25913;&#21464;&#25552;&#31034;&#30340;&#34676;&#34678;&#25928;&#24212;&#65306;&#24494;&#23567;&#30340;&#21464;&#21270;&#21644;&#36234;&#29425;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance. (arXiv:2401.03729v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#25506;&#31350;&#25913;&#21464;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21363;&#20351;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#31572;&#26696;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#35831;&#27714;&#20197;XML&#26684;&#24335;&#36820;&#22238;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23545;&#22810;&#20010;&#39046;&#22495;&#21644;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#21521;LLM&#25552;&#38382;&#25110;&#8220;&#25552;&#31034;&#8221;&#65292;&#23454;&#36341;&#32773;&#33021;&#22815;&#24555;&#36895;&#33719;&#24471;&#20219;&#24847;&#20219;&#21153;&#30340;&#21709;&#24212;&#12290;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#26159;&#21542;&#21464;&#21270;&#20250;&#24433;&#21709;LLM&#30340;&#26368;&#32456;&#20915;&#31574;&#65311;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#26368;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#30340;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;LLM&#25913;&#21464;&#20854;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;XML&#20013;&#35831;&#27714;&#21709;&#24212;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#21487;&#33021;&#23545;&#30001;LLMs&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are regularly being used to label data across many domains and for myriad tasks. By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task. This prompting is done through a series of decisions by the practitioner, from simple wording of the prompt, to requesting the output in a certain data format, to jailbreaking in the case of prompts that address more sensitive topics. In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM? We answer this using a series of prompt variations across a variety of text classification tasks. We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#65292;&#24182;&#35299;&#20915;&#20102;&#26684;&#24335;&#19981;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#39564;&#35777;&#20102;&#29616;&#26377;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#20998;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03512</link><description>&lt;p&gt;
&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#26356;&#20934;&#30830;&#30340;&#26684;&#24335;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;
&lt;/p&gt;
&lt;p&gt;
Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format. (arXiv:2401.03512v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#65292;&#24182;&#35299;&#20915;&#20102;&#26684;&#24335;&#19981;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#39564;&#35777;&#20102;&#29616;&#26377;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#20998;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;Qwen-chat&#65289;&#33021;&#22815;&#26681;&#25454;&#20154;&#31867;&#30340;&#25351;&#20196;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#22312;&#26684;&#24335;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#27599;&#34892;&#23383;&#31526;&#30340;&#25968;&#37327;&#26377;&#26102;&#36807;&#22810;&#25110;&#19981;&#36275;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#22522;&#20110;&#20998;&#35789;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#26684;&#24335;&#19981;&#20934;&#30830;&#26159;&#30001;&#20110;"&#20998;&#35789;&#35268;&#21010;"&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#30693;&#36947;&#27599;&#20010;&#20998;&#35789;&#20013;&#21253;&#21547;&#22810;&#23569;&#20010;&#23383;&#31526;&#65292;&#24182;&#22522;&#20110;&#36825;&#20010;&#30693;&#35782;&#36827;&#34892;&#38271;&#24230;&#25511;&#21046;&#35268;&#21010;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#23637;&#31034;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#35789;&#21644;&#23383;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30693;&#35782;&#26377;&#38480;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25340;&#20889;&#27604;&#36187;&#25506;&#27979;&#31243;&#24207;&#65292;&#24182;&#21457;&#29616;Qwen-chat&#22312;&#36817;15%&#30340;&#20013;&#25991;&#25340;&#20889;&#27979;&#35797;&#20013;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#25104;&#26080;&#38656;&#20998;&#35789;&#30340;&#27169;&#22411;&#65288;&#23545;&#20110;&#20013;&#25991;&#26469;&#35828;&#65289;&#65292;&#20174;&#32780;&#33021;&#22815;&#24456;&#22823;&#31243;&#24230;&#19978;&#35299;&#20915;&#26684;&#24335;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuned large language models (such as ChatGPT and Qwen-chat) can generate Chinese classical poetry following human's instructions. LLMs perform well in content, but are usually lacking in format, with occasionally excess or insufficient number of characters in each line. Since most SOTA LLMs are token-based, we assume that the format inaccuracy is due to the difficulty of the "token planning" task, which means that the LLM need to know exactly how much characters are contained in each token and do length-control planning based on that knowledge. In this paper, we first confirm our assumption by showing that existing token-based large language models has limited knowledge on token-character relationship. We use a spelling bee probing procedure, and find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show that a token-based model can be easily tailored into a token-free model (in terms of Chinese), which can largely solve the format accuracy problem. Our tailoring 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.03301</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#39640;&#25928;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65306;&#25968;&#25454;&#22810;&#26679;&#24615;&#12289;&#21518;&#39564;&#37319;&#26679;&#65292;&#20197;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35797;&#22270;&#29702;&#35299;&#20160;&#20040;&#20419;&#36827;&#20102;&#23545;&#20110;&#24207;&#36125;&#21494;&#26031;&#20915;&#31574;&#30340;&#21382;&#21490;&#25968;&#25454;&#38598;&#36827;&#34892;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#34987;&#31216;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20110;&#22312;&#21033;&#29992;&#65288;&#20540;&#65289;&#20989;&#25968;&#36924;&#36817;&#30340;&#21516;&#26102;&#20139;&#21463;&#26679;&#26412;&#25928;&#29575;&#30340;&#31639;&#27861;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21253;&#25324;&#31163;&#32447;RL&#20013;&#35206;&#30422;&#24230;&#37327;&#30340;&#20808;&#21069;&#27010;&#24565;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#19988;&#21033;&#29992;&#36825;&#20010;&#27010;&#24565;&#23558;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#65288;VS&#65289;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#65288;RO&#65289;&#21644;&#21518;&#39564;&#37319;&#26679;&#65288;PS&#65289;&#30340;&#19977;&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#36827;&#34892;&#32479;&#19968;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#35777;&#26126;&#65292;&#22522;&#20110;VS&#12289;&#22522;&#20110;RO&#21644;&#22522;&#20110;PS&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;\emph{&#21487;&#27604;}&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#24674;&#22797;&#20102;&#22312;&#26377;&#38480;&#21644;&#32447;&#24615;&#27169;&#22411;&#31867;&#21035;&#19979;&#30340;&#26368;&#20248;&#24615;&#30340;&#26631;&#20934;&#20551;&#35774;&#30340;&#36793;&#30028;&#12290;&#36825;&#20010;&#32467;&#26524;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#19981;&#20855;&#26377;&#26377;&#21033;&#24615;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02994</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#26041;&#27861;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65306;&#30456;&#23545;&#20110;&#19975;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#30340;&#26356;&#24265;&#20215;&#12289;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35805;&#22411;AI&#30740;&#31350;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#26356;&#22810;&#30340;&#21442;&#25968;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#24222;&#22823;&#30340;&#27169;&#22411;&#24448;&#24448;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#32842;&#22825;&#22238;&#22797;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#33021;&#21542;&#36890;&#36807;&#32452;&#21512;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#36798;&#21040;&#19982;&#21333;&#20010;&#22823;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23558;&#22810;&#20010;&#32842;&#22825;AI&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#24403;&#29305;&#23450;&#36739;&#23567;&#30340;&#27169;&#22411;&#21327;&#21516;&#28151;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#28508;&#22312;&#22320;&#36229;&#36234;&#25110;&#21305;&#25932;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#20165;&#38598;&#25104;&#19977;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;6B/13B&#21442;&#25968;&#65289;&#23601;&#21487;&#20197;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36234;ChatGPT&#65288;175B+&#21442;&#25968;&#65289;&#31561;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#36825;&#20010;&#20551;&#35774;&#32463;&#36807;&#20102;&#20005;&#26684;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? We introduce an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. Our empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02949</link><description>&lt;p&gt;
Graph2Tac: &#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#21450;&#20854;&#24212;&#29992;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#27010;&#24565;&#12290;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#23398;&#31185;&#39046;&#22495;&#20013;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#27599;&#31687;&#25968;&#23398;&#35770;&#25991;&#25110;&#24212;&#29992;&#20013;&#37117;&#20250;&#24341;&#20837;&#26032;&#30340;&#27010;&#24565;&#12290;&#24418;&#24335;&#21270;&#29702;&#35770;&#24314;&#31435;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#23450;&#20041;&#12289;&#23450;&#29702;&#21644;&#30456;&#20114;&#24341;&#29992;&#30340;&#35777;&#26126;&#12290;&#24403;&#19968;&#20010;AI&#20195;&#29702;&#20154;&#35777;&#26126;&#19968;&#20010;&#26032;&#30340;&#23450;&#29702;&#26102;&#65292;&#22823;&#22810;&#25968;&#19982;&#35813;&#23450;&#29702;&#30456;&#20851;&#30340;&#25968;&#23398;&#27010;&#24565;&#21644;&#24341;&#29702;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20174;&#26410;&#34987;&#35265;&#36807;&#12290;&#36825;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#35813;&#21161;&#25163;&#25317;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;Coq&#39033;&#30446;&#65292;&#27599;&#20010;&#39033;&#30446;&#37117;&#26377;&#33258;&#24049;&#30340;&#23450;&#20041;&#12289;&#24341;&#29702;&#65292;&#29978;&#33267;&#29992;&#20110;&#35777;&#26126;&#36825;&#20123;&#24341;&#29702;&#30340;&#33258;&#23450;&#20041;&#31574;&#30053;&#36807;&#31243;&#12290;&#23558;&#36825;&#26679;&#30340;&#26032;&#20449;&#24687;&#21363;&#26102;&#22320;&#34701;&#20837;&#21040;&#20195;&#29702;&#20154;&#30340;&#30693;&#35782;&#24211;&#20013;&#23545;&#20110;&#20195;&#29702;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#26032;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;Coq&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;Coq&#26415;&#35821;&#30340;&#24544;&#23454;&#22270;&#34920;&#31034;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;Graph2Tac&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#23450;&#20041;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts abound in mathematics and its applications. They vary greatly between subject areas, and new ones are introduced in each mathematical paper or application. A formal theory builds a hierarchy of definitions, theorems and proofs that reference each other. When an AI agent is proving a new theorem, most of the mathematical concepts and lemmas relevant to that theorem may have never been seen during training. This is especially true in the Coq proof assistant, which has a diverse library of Coq projects, each with its own definitions, lemmas, and even custom tactic procedures used to prove those lemmas. It is essential for agents to incorporate such new information into their knowledge base on the fly. We work towards this goal by utilizing a new, large-scale, graph-based dataset for machine learning in Coq. We leverage a faithful graph-representation of Coq terms that induces a directed graph of dependencies between definitions to create a novel graph neural network, Graph2Tac (G
&lt;/p&gt;</description></item><item><title>GPS-SSL&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#24230;&#37327;&#31354;&#38388;&#24182;&#21033;&#29992;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#23427;&#21487;&#20197;&#20943;&#23569;&#23545;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#20381;&#36182;&#65292;&#22240;&#27492;&#22312;Cifar10&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01990</link><description>&lt;p&gt;
GPS-SSL: &#24341;&#23548;&#27491;&#26679;&#26412;&#37319;&#26679;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01990
&lt;/p&gt;
&lt;p&gt;
GPS-SSL&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#24230;&#37327;&#31354;&#38388;&#24182;&#21033;&#29992;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#23427;&#21487;&#20197;&#20943;&#23569;&#23545;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#20381;&#36182;&#65292;&#22240;&#27492;&#22312;Cifar10&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#23548;&#27491;&#26679;&#26412;&#37319;&#26679;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;GPS-SSL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27491;&#26679;&#26412;&#36873;&#25321;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;SSL&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#29983;&#25104;&#27491;&#26679;&#26412;&#65292;&#24182;&#23558;&#20808;&#39564;&#30693;&#35782;&#32467;&#21512;&#36827;&#21435;&#65292;&#20294;&#26159;&#38169;&#35823;&#25110;&#32773;&#36807;&#24369;&#30340;DA&#20250;&#20005;&#37325;&#38477;&#20302;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;GPS-SSL&#21017;&#25552;&#20986;&#35774;&#35745;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#20351;&#24471;&#27431;&#27663;&#36317;&#31163;&#25104;&#20026;&#35821;&#20041;&#20851;&#31995;&#30340;&#26377;&#24847;&#20041;&#30340;&#26367;&#20195;&#12290;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#37117;&#21487;&#20197;&#29420;&#31435;&#22320;&#23884;&#20837;&#21040;&#36825;&#20010;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#32780;&#19981;&#21463;&#25152;&#20351;&#29992;&#30340;DA&#24433;&#21709;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;GPS-SSL&#36866;&#29992;&#20110;&#20219;&#20309;SSL&#26041;&#27861;&#65292;&#22914;SimCLR&#25110;BYOL&#12290;GPS-SSL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#20943;&#23569;&#20102;&#23450;&#21046;&#24378;DA&#30340;&#21387;&#21147;&#12290;&#20363;&#22914;&#65292;GPS-SSL&#22312;Cifar10&#19978;&#20351;&#29992;&#24369;DA&#36798;&#21040;&#20102;85.58&#65285;&#65292;&#32780;&#22522;&#20934;&#20540;&#21482;&#36798;&#21040;&#20102;37.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a general method to inject a priori knowledge into Self-Supervised Learning (SSL) positive samples selection. Current SSL methods leverage Data-Augmentations (DA) for generating positive samples and incorporate prior knowledge - an incorrect, or too weak DA will drastically reduce the quality of the learned representation. GPS-SSL proposes instead to design a metric space where Euclidean distances become a meaningful proxy for semantic relationship. In that space, it is now possible to generate positive samples from nearest neighbor sampling. Any prior knowledge can now be embedded into that metric space independently from the employed DA. From its simplicity, GPS-SSL is applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches 85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We therefore move a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#33945;&#38754;&#24314;&#27169;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#34987;&#33945;&#38754;&#37096;&#20998;&#23454;&#29616;&#28145;&#24230;&#27169;&#22411;&#23398;&#20064;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#24050;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.00897</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#20013;&#30340;&#33945;&#38754;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Masked Modeling for Self-supervised Representation Learning on Vision and Beyond. (arXiv:2401.00897v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00897
&lt;/p&gt;
&lt;p&gt;
&#33945;&#38754;&#24314;&#27169;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#34987;&#33945;&#38754;&#37096;&#20998;&#23454;&#29616;&#28145;&#24230;&#27169;&#22411;&#23398;&#20064;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#24050;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#38761;&#21629;&#19981;&#26029;&#21521;&#21069;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22240;&#20854;&#20986;&#33394;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20302;&#20381;&#36182;&#24615;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#22810;&#26679;&#30340;&#33258;&#30417;&#30563;&#25216;&#26415;&#20013;&#65292;&#33945;&#38754;&#24314;&#27169;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30456;&#24212;&#27604;&#20363;&#30340;&#21407;&#22987;&#25968;&#25454;&#20250;&#34987;&#33945;&#38754;&#65292;&#27169;&#22411;&#38656;&#35201;&#39044;&#27979;&#20986;&#36825;&#20123;&#33945;&#38754;&#30340;&#37096;&#20998;&#12290;&#36825;&#31181;&#33539;&#24335;&#20351;&#28145;&#24230;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#33945;&#38754;&#24314;&#27169;&#26694;&#26550;&#21450;&#20854;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#33945;&#38754;&#24314;&#27169;&#20013;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#33945;&#38754;&#31574;&#30053;&#12289;&#24674;&#22797;&#30446;&#26631;&#12289;&#32593;&#32476;&#26550;&#26500;&#31561;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#23427;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20854;&#20849;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the deep learning revolution marches on, self-supervised learning has garnered increasing attention in recent years thanks to its remarkable representation learning ability and the low dependence on labeled data. Among these varied self-supervised techniques, masked modeling has emerged as a distinctive approach that involves predicting parts of the original data that are proportionally masked during training. This paradigm enables deep models to learn robust representations and has demonstrated exceptional performance in the context of computer vision, natural language processing, and other modalities. In this survey, we present a comprehensive review of the masked modeling framework and its methodology. We elaborate on the details of techniques within masked modeling, including diverse masking strategies, recovering targets, network architectures, and more. Then, we systematically investigate its wide-ranging applications across domains. Furthermore, we also explore the commonalit
&lt;/p&gt;</description></item><item><title>Jatmo&#26159;&#19968;&#31181;&#29983;&#25104;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#20855;&#26377;&#25239;&#24615;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#24182;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2312.17673</link><description>&lt;p&gt;
Jatmo:&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#36827;&#34892;&#25552;&#31034;&#27880;&#20837;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Jatmo: Prompt Injection Defense by Task-Specific Finetuning. (arXiv:2312.17673v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17673
&lt;/p&gt;
&lt;p&gt;
Jatmo&#26159;&#19968;&#31181;&#29983;&#25104;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#20855;&#26377;&#25239;&#24615;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#24182;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20351;&#29992;&#25143;&#21644;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#21033;&#29992;LLMs&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#24433;&#21709;&#65306;&#19968;&#31181;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#21163;&#25345;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#23558;&#23545;&#25552;&#31034;&#30340;&#21709;&#24212;&#26356;&#25913;&#20026;&#19981;&#38656;&#35201;&#25110;&#21487;&#33021;&#20855;&#26377;&#24694;&#24847;&#30340;&#21709;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jatmo&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#20855;&#26377;&#25239;&#24615;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;Jatmo&#21033;&#29992;&#20102;LLMs&#21482;&#33021;&#22312;&#32463;&#21382;&#36807;&#25351;&#20196;&#35843;&#25972;&#21518;&#25165;&#33021;&#36981;&#24490;&#25351;&#20196;&#30340;&#20107;&#23454;&#12290;&#23427;&#21033;&#29992;&#19968;&#20010;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#23545;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#65288;&#21363;&#38750;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#65289;&#36827;&#34892;&#24494;&#35843;&#12290;Jatmo&#21482;&#38656;&#35201;&#19968;&#20010;&#20219;&#21153;&#25552;&#31034;&#21644;&#19968;&#20010;&#20219;&#21153;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#65306;&#23427;&#20351;&#29992;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#12290;&#22312;&#27809;&#26377;&#29616;&#25104;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;Jatmo&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#20363;&#23376;&#65292;&#25110;&#32773;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks. However, LLMs are vulnerable to prompt-injection attacks: a class of attacks that hijack the model's instruction-following abilities, changing responses to prompts to undesired, possibly malicious ones. In this work, we introduce Jatmo, a method for generating task-specific models resilient to prompt-injection attacks. Jatmo leverages the fact that LLMs can only follow instructions once they have undergone instruction tuning. It harnesses a teacher instruction-tuned model to generate a task-specific dataset, which is then used to fine-tune a base model (i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a dataset of inputs for the task: it uses the teacher model to generate outputs. For situations with no pre-existing datasets, Jatmo can use a single example, or in some cases
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#35821;&#35328;&#26234;&#33021;&#20307;&#65288;HLA&#65289;&#29992;&#20110;&#23454;&#26102;&#20154;&#26426;&#21327;&#20316;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;LLM&#24341;&#25806;&#21644;&#20998;&#23618;&#31574;&#30053;&#65292;&#26082;&#25552;&#20379;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21448;&#20445;&#25345;&#23454;&#26102;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2312.15224</link><description>&lt;p&gt;
LLM&#21160;&#21147;&#19979;&#30340;&#20998;&#23618;&#35821;&#35328;&#26234;&#33021;&#20307;&#29992;&#20110;&#23454;&#26102;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination. (arXiv:2312.15224v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15224
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#35821;&#35328;&#26234;&#33021;&#20307;&#65288;HLA&#65289;&#29992;&#20110;&#23454;&#26102;&#20154;&#26426;&#21327;&#20316;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;LLM&#24341;&#25806;&#21644;&#20998;&#23618;&#31574;&#30053;&#65292;&#26082;&#25552;&#20379;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21448;&#20445;&#25345;&#23454;&#26102;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;AI&#26234;&#33021;&#20307;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#21327;&#21161;&#20154;&#31867;&#65292;&#24341;&#21457;&#20102;&#20154;&#26426;&#21327;&#20316;&#38761;&#21629;&#12290;LLM&#39537;&#21160;&#30340;&#26234;&#33021;&#20307;&#36890;&#24120;&#38656;&#35201;&#35843;&#29992;LLM API&#24182;&#20351;&#29992;&#20154;&#24037;&#35774;&#35745;&#30340;&#22797;&#26434;&#25552;&#31034;&#65292;&#23548;&#33268;&#25512;&#29702;&#24310;&#36831;&#36739;&#39640;&#12290;&#23613;&#31649;&#36825;&#31181;&#27169;&#24335;&#22312;&#26368;&#23567;&#20114;&#21160;&#35201;&#27714;&#30340;&#24773;&#22659;&#19979;&#65288;&#22914;&#20195;&#30721;&#29983;&#25104;&#65289;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#39640;&#24230;&#20114;&#21160;&#21644;&#23454;&#26102;&#24212;&#29992;&#65288;&#22914;&#28216;&#25103;&#65289;&#21017;&#19981;&#36866;&#29992;&#12290;&#20256;&#32479;&#30340;&#28216;&#25103;AI&#36890;&#24120;&#20351;&#29992;&#23567;&#27169;&#22411;&#25110;&#21453;&#24212;&#31574;&#30053;&#65292;&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;&#65292;&#20294;&#20219;&#21153;&#23436;&#25104;&#21644;&#20132;&#20114;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#20197;Overcooked&#20316;&#20026;&#25105;&#20204;&#30340;&#27979;&#35797;&#22330;&#26223;&#65292;&#20854;&#20013;&#29609;&#23478;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#20132;&#27969;&#24182;&#21512;&#20316;&#23436;&#25104;&#35746;&#21333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#35821;&#35328;&#26234;&#33021;&#20307;&#65288;HLA&#65289;&#29992;&#20110;&#20154;&#26426;&#21327;&#20316;&#65292;&#26082;&#25552;&#20379;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21448;&#20445;&#25345;&#23454;&#26102;&#25191;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HLA&#37319;&#29992;&#20102;&#25913;&#36827;&#30340;LLM&#24341;&#25806;&#21644;&#20998;&#23618;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20132;&#20114;&#21644;&#20219;&#21153;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI agents powered by Large Language Models (LLMs) have made significant advances, enabling them to assist humans in diverse complex tasks and leading to a revolution in human-AI coordination. LLM-powered agents typically require invoking LLM APIs and employing artificially designed complex prompts, which results in high inference latency. While this paradigm works well in scenarios with minimal interactive demands, such as code generation, it is unsuitable for highly interactive and real-time applications, such as gaming. Traditional gaming AI often employs small models or reactive policies, enabling fast inference but offering limited task completion and interaction abilities. In this work, we consider Overcooked as our testbed where players could communicate with natural language and cooperate to serve orders. We propose a Hierarchical Language Agent (HLA) for human-AI coordination that provides both strong reasoning abilities while keeping real-time execution. In particular, HLA ado
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#25442;&#22120;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26356;&#22909;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.11714</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#25442;&#22120;&#65306;&#34701;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#25442;&#22120;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26356;&#22909;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#38388;&#29305;&#24615;&#65292;&#21253;&#25324;&#26412;&#22320;&#30456;&#20851;&#24615;&#21644;&#20840;&#23616;&#20381;&#36182;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#26410;&#33021;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;'&#26102;&#38388;&#21464;&#25442;&#22120;AAE'&#65292;&#23427;&#30001;&#19968;&#20010;&#23545;&#25239;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#21644;&#19968;&#20010;&#21517;&#20026;'&#26102;&#38388;&#21464;&#25442;&#22120;'&#30340;&#26032;&#35774;&#35745;&#26550;&#26500;&#32452;&#25104;&#12290;&#26102;&#38388;&#21464;&#25442;&#22120;&#39318;&#20808;&#36890;&#36807;&#23618;&#27425;&#24182;&#34892;&#35774;&#35745;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;Transformer&#30340;&#33021;&#21147;&#65292;&#20998;&#21035;&#25552;&#21462;&#26412;&#22320;&#29305;&#24449;&#21644;&#20840;&#23616;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22312;&#20004;&#20010;&#20998;&#25903;&#20043;&#38388;&#25552;&#20379;&#20114;&#34917;&#30340;&#24341;&#23548;&#65292;&#24182;&#23454;&#29616;&#26412;&#22320;&#29305;&#24449;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#21512;&#36866;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating time series data is a promising approach to address data deficiency problems. However, it is also challenging due to the complex temporal properties of time series data, including local correlations as well as global dependencies. Most existing generative models have failed to effectively learn both the local and global properties of time series data. To address this open problem, we propose a novel time series generative model named 'Time-Transformer AAE', which consists of an adversarial autoencoder (AAE) and a newly designed architecture named 'Time-Transformer' within the decoder. The Time-Transformer first simultaneously learns local and global features in a layer-wise parallel design, combining the abilities of Temporal Convolutional Networks and Transformer in extracting local features and global dependencies respectively. Second, a bidirectional cross attention is proposed to provide complementary guidance across the two branches and achieve proper fusion between loc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20027;&#20307;&#29305;&#23450;&#30693;&#35782;&#20808;&#39564;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#38452;&#24433;&#27169;&#24335;&#21644;&#32441;&#29702;&#22686;&#24378;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#26377;&#20016;&#23500;&#32441;&#29702;&#30340;3D&#27169;&#22411;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2312.11535</link><description>&lt;p&gt;
Customize-It-3D&#65306;&#20351;&#29992;&#20027;&#20307;&#29305;&#23450;&#30693;&#35782;&#20808;&#39564;&#20174;&#21333;&#20010;&#22270;&#20687;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;3D&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Customize-It-3D: High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior. (arXiv:2312.11535v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20027;&#20307;&#29305;&#23450;&#30693;&#35782;&#20808;&#39564;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#38452;&#24433;&#27169;&#24335;&#21644;&#32441;&#29702;&#22686;&#24378;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#26377;&#20016;&#23500;&#32441;&#29702;&#30340;3D&#27169;&#22411;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#21442;&#32771;&#22270;&#20687;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#24314;&#31435;&#22270;&#20687;&#21040;3D&#29983;&#25104;&#30340;&#33258;&#23450;&#20041;&#30693;&#35782;&#20808;&#39564;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36890;&#29992;&#30340;&#25193;&#25955;&#20808;&#39564;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19982;&#21442;&#32771;&#22270;&#20687;&#24471;&#21040;&#19968;&#33268;&#32467;&#26524;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#20307;&#29305;&#23450;&#19988;&#22810;&#27169;&#24577;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#36890;&#36807;&#32771;&#34385;&#38452;&#24433;&#27169;&#24335;&#26469;&#25913;&#21892;&#20960;&#20309;&#20248;&#21270;&#21644;&#32441;&#29702;&#22686;&#24378;&#30340;&#31895;&#30053;&#32467;&#26524;&#65292;&#36824;&#26377;&#21161;&#20110;&#20351;3D&#20869;&#23481;&#19982;&#20027;&#39064;&#20445;&#25345;&#19968;&#33268;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;Customize-It-3D&#22312;&#35270;&#35273;&#36136;&#37327;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;360&#24230;&#37325;&#24314;&#32467;&#26524;&#65292;&#38750;&#24120;&#36866;&#21512;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;3D&#30340;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel two-stage approach that fully utilizes the information provided by the reference image to establish a customized knowledge prior for image-to-3D generation. While previous approaches primarily rely on a general diffusion prior, which struggles to yield consistent results with the reference image, we propose a subject-specific and multi-modal diffusion model. This model not only aids NeRF optimization by considering the shading mode for improved geometry but also enhances texture from the coarse results to achieve superior refinement. Both aspects contribute to faithfully aligning the 3D content with the subject. Extensive experiments showcase the superiority of our method, Customize-It-3D, outperforming previous works by a substantial margin. It produces faithful 360-degree reconstructions with impressive visual quality, making it well-suited for various applications, including text-to-3D creation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2312.09085</link><description>&lt;p&gt;
&#22320;&#29699;&#26159;&#25153;&#24179;&#30340;&#65292;&#22240;&#20026;......&#65306;&#36890;&#36807;&#35828;&#26381;&#24615;&#23545;&#35805;&#30740;&#31350;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#20449;&#20208;
&lt;/p&gt;
&lt;p&gt;
The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation. (arXiv:2312.09085v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23553;&#35013;&#20102;&#22823;&#37327;&#30693;&#35782;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#22806;&#37096;&#35823;&#23548;&#20449;&#24687;&#30340;&#25915;&#20987;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#22312;&#21333;&#36718;&#23545;&#35805;&#20013;&#30740;&#31350;&#20102;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#29305;&#21035;&#26159;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#65292;&#20449;&#20208;&#21487;&#20197;&#21457;&#29983;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#23545;&#35828;&#26381;&#24615;&#23545;&#35805;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#23427;&#20204;&#21487;&#20197;&#27491;&#30830;&#22238;&#31572;&#30340;&#20107;&#23454;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;Farm&#65288;&#21363;&#20107;&#23454;&#21040;&#35823;&#23548;&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#19982;&#31995;&#32479;&#29983;&#25104;&#30340;&#35828;&#26381;&#24615;&#35823;&#23548;&#20449;&#24687;&#30456;&#21305;&#37197;&#30340;&#20107;&#23454;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#36861;&#36394;LLMs&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#30340;&#20449;&#20208;&#21464;&#21270;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#28436;&#31034;&#20559;&#35265;&#23384;&#22312;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#32780;&#36890;&#36807;&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#22312;ICL&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.07476</link><description>&lt;p&gt;
&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65306;&#23545;&#28436;&#31034;&#36873;&#25321;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#28436;&#31034;&#20559;&#35265;&#23384;&#22312;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#32780;&#36890;&#36807;&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#22312;ICL&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#65292;&#36890;&#36807;&#23569;&#37327;&#28436;&#31034;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;ICL&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#28436;&#31034;&#25968;&#37327;&#30340;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#28436;&#31034;&#20559;&#35265;&#65292;&#21363;&#30001;LLMs&#24341;&#36215;&#30340;&#36755;&#20837;-&#26631;&#31614;&#26144;&#23556;&#35823;&#35299;&#20102;&#20219;&#21153;&#30340;&#26412;&#36136;&#12290;&#21463;&#20154;&#31867;&#32463;&#39564;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#28436;&#31034;&#38388;&#20851;&#31995;&#30340;&#35270;&#35282;&#26469;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#32534;&#36753;&#25991;&#26412;&#26469;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#28436;&#31034;&#65288;CDs&#65289;&#65292;&#20197;&#32763;&#36716;&#30456;&#24212;&#30340;&#26631;&#31614;&#65292;&#20197;&#31361;&#20986;&#20219;&#21153;&#30340;&#26412;&#36136;&#24182;&#36890;&#36807;&#28436;&#31034;&#38388;&#27604;&#36739;&#28040;&#38500;&#28508;&#22312;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;CDs&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;LLMs&#23384;&#22312;&#28436;&#31034;&#20559;&#35265;&#65292;&#32780;CDs&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65307;&#65288;2&#65289;CDs&#22312;ICL&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20998;&#24067;&#22806;&#22330;&#26223;&#20013;&#12290;&#24635;&#20043;&#65292;&#26412;&#30740;&#31350;&#20174;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#25506;&#32034;&#20102;ICL&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations. Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence. Inspired by human experience, we attempt to mitigate such bias through the perspective of the inter-demonstration relationship. Specifically, we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, in order to highlight the task's essence and eliminate potential spurious correlations through the inter-demonstration comparison. Through a series of experiments on CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL, especially in out-of-distribution scenarios. In summary, this study explores the ICL mechanisms from a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#30693;&#35782;&#34920;&#31034;&#65288;KR&#65289;&#21644;&#38754;&#21521;&#20998;&#26512;&#30340;&#30693;&#35782;&#32452;&#32455;&#65288;KO&#65289;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;KO&#20016;&#23500;&#30340;KR&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24314;&#27169;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2312.07302</link><description>&lt;p&gt;
&#20174;&#30693;&#35782;&#34920;&#31034;&#21040;&#30693;&#35782;&#32452;&#32455;&#20877;&#22238;&#21040;&#30693;&#35782;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
From Knowledge Representation to Knowledge Organization and Back. (arXiv:2312.07302v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30693;&#35782;&#34920;&#31034;&#65288;KR&#65289;&#21644;&#38754;&#21521;&#20998;&#26512;&#30340;&#30693;&#35782;&#32452;&#32455;&#65288;KO&#65289;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;KO&#20016;&#23500;&#30340;KR&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24314;&#27169;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#34920;&#31034;&#65288;KR&#65289;&#21644;&#38754;&#21521;&#20998;&#26512;&#30340;&#30693;&#35782;&#32452;&#32455;&#65288;KO&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#21644;&#20449;&#24687;&#31185;&#23398;&#31038;&#21306;&#20013;&#26368;&#37325;&#35201;&#30340;&#25968;&#25454;&#21644;&#30693;&#35782;&#24314;&#27169;&#26041;&#27861;&#12290;KR&#25317;&#26377;&#19968;&#20010;&#24378;&#22823;&#32780;&#21487;&#25193;&#23637;&#30340;&#25216;&#26415;&#29983;&#24577;&#31995;&#32479;&#26469;&#25903;&#25345;&#30693;&#35782;&#24314;&#27169;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20854;&#27169;&#22411;&#65288;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#65289;&#30340;&#36136;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;KO&#19981;&#37027;&#20040;&#25216;&#26415;&#39537;&#21160;&#65292;&#20294;&#24050;&#32463;&#21457;&#23637;&#20986;&#19968;&#22871;&#30830;&#20445;&#24314;&#27169;&#65288;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#65289;&#36136;&#37327;&#30340;&#25351;&#23548;&#21407;&#21017;&#65288;&#27861;&#35268;&#65289;&#12290;&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;KR&#21644;&#38754;&#21521;&#20998;&#26512;&#30340;KO&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#21151;&#33021;&#26144;&#23556;&#12290;&#22312;&#26144;&#23556;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;KO&#20016;&#23500;&#30340;KR&#26041;&#27861;&#65292;&#25317;&#26377;&#25152;&#26377;&#26631;&#20934;&#30340;KR&#26041;&#27861;&#32452;&#20214;&#65292;&#20197;&#21450;KO&#25552;&#20379;&#30340;&#24314;&#27169;&#36136;&#37327;&#30340;&#25351;&#23548;&#27861;&#35268;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#38469;&#30410;&#22788;&#24050;&#32463;&#24471;&#21040;&#20102;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Representation (KR) and facet-analytical Knowledge Organization (KO) have been the two most prominent methodologies of data and knowledge modelling in the Artificial Intelligence community and the Information Science community, respectively. KR boasts of a robust and scalable ecosystem of technologies to support knowledge modelling while, often, underemphasizing the quality of its models (and model-based data). KO, on the other hand, is less technology-driven but has developed a robust framework of guiding principles (canons) for ensuring modelling (and model-based data) quality. This paper elucidates both the KR and facet-analytical KO methodologies in detail and provides a functional mapping between them. Out of the mapping, the paper proposes an integrated KO-enriched KR methodology with all the standard components of a KR methodology plus the guiding canons of modelling quality provided by KO. The practical benefits of the methodological integration has been exemplified t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;LiDAR-&#30456;&#26426;&#34701;&#21512;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#22312;&#19981;&#25913;&#21464;&#22270;&#20687;&#25968;&#25454;&#36890;&#36947;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#25805;&#32437;LiDAR&#25968;&#25454;&#36890;&#36947;&#21363;&#21487;&#27450;&#39575;&#34701;&#21512;&#27169;&#22411;&#65292;&#36825;&#24341;&#21457;&#20102;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#23433;&#20840;&#20851;&#27880;&#65292;&#24182;&#25506;&#35752;&#20102;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#19982;&#23545;&#25239;&#24615;&#28857;&#25968;&#37327;&#12289;&#36710;&#36742;&#38388;&#36317;&#31163;&#21644;&#35282;&#24230;&#22240;&#32032;&#30340;&#20851;&#31995;&#12290;&#35813;&#30740;&#31350;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.01468</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#25506;&#32034;LiDAR-&#30456;&#26426;&#34701;&#21512;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Adversarial Robustness of LiDAR-Camera Fusion Model in Autonomous Driving. (arXiv:2312.01468v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01468
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;LiDAR-&#30456;&#26426;&#34701;&#21512;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#22312;&#19981;&#25913;&#21464;&#22270;&#20687;&#25968;&#25454;&#36890;&#36947;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#25805;&#32437;LiDAR&#25968;&#25454;&#36890;&#36947;&#21363;&#21487;&#27450;&#39575;&#34701;&#21512;&#27169;&#22411;&#65292;&#36825;&#24341;&#21457;&#20102;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#23433;&#20840;&#20851;&#27880;&#65292;&#24182;&#25506;&#35752;&#20102;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#19982;&#23545;&#25239;&#24615;&#28857;&#25968;&#37327;&#12289;&#36710;&#36742;&#38388;&#36317;&#31163;&#21644;&#35282;&#24230;&#22240;&#32032;&#30340;&#20851;&#31995;&#12290;&#35813;&#30740;&#31350;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;LiDAR-&#30456;&#26426;&#34701;&#21512;&#27169;&#22411;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25915;&#20987;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#36710;&#36742;&#19978;&#26041;&#31616;&#21333;&#28155;&#21152;&#19968;&#23450;&#25968;&#37327;&#30340;&#29289;&#29702;&#32422;&#26463;&#30340;&#23545;&#25239;&#24615;&#28857;&#65292;&#21487;&#20197;&#20351;&#34701;&#21512;&#27169;&#22411;&#26080;&#27861;&#26816;&#27979;&#21040;&#36710;&#36742;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#23545;&#22270;&#20687;&#25968;&#25454;&#36890;&#36947;&#36827;&#34892;&#26356;&#25913;&#65292;&#20809;&#36798;&#25968;&#25454;&#36890;&#36947;&#30340;&#25805;&#32437;&#23601;&#36275;&#20197;&#27450;&#39575;&#34701;&#21512;&#27169;&#22411;&#12290;&#36825;&#19968;&#21457;&#29616;&#24341;&#21457;&#20102;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#23433;&#20840;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23545;&#25239;&#24615;&#28857;&#30340;&#25968;&#37327;&#12289;&#21069;&#26041;&#25509;&#36817;&#36710;&#36742;&#19982;&#20809;&#36798;&#35774;&#22791;&#36710;&#36742;&#20043;&#38388;&#30340;&#36317;&#31163;&#20197;&#21450;&#21508;&#31181;&#35282;&#24230;&#22240;&#32032;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#26377;&#21161;&#20110;&#29702;&#35299;&#22810;&#20256;&#24863;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20026;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#25552;&#20379;&#27934;&#35265;&#21644;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study assesses the adversarial robustness of LiDAR-camera fusion models in 3D object detection. We introduce an attack technique that, by simply adding a limited number of physically constrained adversarial points above a car, can make the car undetectable by the fusion model. Experimental results reveal that even without changes to the image data channel, the fusion model can be deceived solely by manipulating the LiDAR data channel. This finding raises safety concerns in the field of autonomous driving. Further, we explore how the quantity of adversarial points, the distance between the front-near car and the LiDAR-equipped car, and various angular factors affect the attack success rate. We believe our research can contribute to the understanding of multi-sensor robustness, offering insights and guidance to enhance the safety of autonomous driving.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#21307;&#23398;&#22270;&#20687;&#20132;&#20114;&#24335;&#20998;&#21106;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#25512;&#21160;&#20102;&#39046;&#22495;&#21457;&#23637;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.13964</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#28145;&#24230;&#20132;&#20114;&#24335;&#20998;&#21106;&#65306;&#31995;&#32479;&#32508;&#36848;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Interactive Segmentation of Medical Images: A Systematic Review and Taxonomy. (arXiv:2311.13964v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#21307;&#23398;&#22270;&#20687;&#20132;&#20114;&#24335;&#20998;&#21106;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#25512;&#21160;&#20102;&#39046;&#22495;&#21457;&#23637;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#39640;&#26114;&#36149;&#27880;&#37322;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#21453;&#39304;&#20197;&#28857;&#20987;&#12289;&#28034;&#40486;&#25110;&#25513;&#33180;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#20801;&#35768;&#23545;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#65292;&#20197;&#20415;&#26377;&#25928;&#24341;&#23548;&#31995;&#32479;&#23454;&#29616;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#25512;&#21160;&#20102;&#32467;&#26524;&#36798;&#21040;&#19968;&#20010;&#26032;&#27700;&#24179;&#65292;&#23548;&#33268;&#35813;&#39046;&#22495;&#24555;&#36895;&#22686;&#38271;&#65292;&#20165;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#23601;&#25552;&#20986;&#20102;121&#31181;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#65292;&#21253;&#25324;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#12289;&#29616;&#26377;&#26041;&#27861;&#30340;&#31995;&#32479;&#32508;&#36848;&#20197;&#21450;&#23545;&#24403;&#21069;&#23454;&#36341;&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#20123;&#36129;&#29486;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#20005;&#37325;&#32570;&#20047;&#65292;&#38656;&#35201;&#36890;&#36807;&#26631;&#20934;&#21270;&#22522;&#20934;&#21644;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive segmentation is a crucial research area in medical image analysis aiming to boost the efficiency of costly annotations by incorporating human feedback. This feedback takes the form of clicks, scribbles, or masks and allows for iterative refinement of the model output so as to efficiently guide the system towards the desired behavior. In recent years, deep learning-based approaches have propelled results to a new level causing a rapid growth in the field with 121 methods proposed in the medical imaging domain alone. In this review, we provide a structured overview of this emerging field featuring a comprehensive taxonomy, a systematic review of existing methods, and an in-depth analysis of current practices. Based on these contributions, we discuss the challenges and opportunities in the field. For instance, we find that there is a severe lack of comparison across methods which needs to be tackled by standardized baselines and benchmarks.
&lt;/p&gt;</description></item><item><title>InteraSSort&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#21830;&#21697;&#32452;&#21512;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#24037;&#20855;&#24110;&#21161;&#21830;&#24215;&#35268;&#21010;&#32773;&#35299;&#20915;&#22797;&#26434;&#30340;&#24215;&#20869;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.12241</link><description>&lt;p&gt;
InteraSSort: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#21830;&#21697;&#32452;&#21512;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
InteraSSort: Interactive Assortment Planning Using Large Language Models. (arXiv:2311.12241v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12241
&lt;/p&gt;
&lt;p&gt;
InteraSSort&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#21830;&#21697;&#32452;&#21512;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#24037;&#20855;&#24110;&#21161;&#21830;&#24215;&#35268;&#21010;&#32773;&#35299;&#20915;&#22797;&#26434;&#30340;&#24215;&#20869;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#21697;&#32452;&#21512;&#35268;&#21010;&#26159;&#30005;&#23376;&#21830;&#21153;&#21644;&#38646;&#21806;&#39046;&#22495;&#20013;&#30740;&#31350;&#30340;&#20851;&#38190;&#38382;&#39064;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InteraSSort&#30340;&#20132;&#20114;&#24335;&#21830;&#21697;&#32452;&#21512;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#24037;&#20855;&#36741;&#21161;&#21830;&#24215;&#35268;&#21010;&#32773;&#36890;&#36807;&#20132;&#20114;&#24335;&#23545;&#35805;&#20570;&#20986;&#20915;&#31574;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#65292;&#33021;&#22815;&#24110;&#21161;&#21830;&#24215;&#35268;&#21010;&#32773;&#35299;&#20915;&#22797;&#26434;&#30340;&#24215;&#20869;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assortment planning, integral to multiple commercial offerings, is a key problem studied in e-commerce and retail settings. Numerous variants of the problem along with their integration into business solutions have been thoroughly investigated in the existing literature. However, the nuanced complexities of in-store planning and a lack of optimization proficiency among store planners with strong domain expertise remain largely overlooked. These challenges frequently necessitate collaborative efforts with multiple stakeholders which often lead to prolonged decision-making processes and significant delays. To mitigate these challenges and capitalize on the advancements of Large Language Models (LLMs), we propose an interactive assortment planning framework, InteraSSort that augments LLMs with optimization tools to assist store planners in making decisions through interactive conversations. Specifically, we develop a solution featuring a user-friendly interface that enables users to expre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.08516</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#25214;&#21040;&#25512;&#29702;&#38169;&#35823;&#65292;&#20294;&#21487;&#20197;&#32416;&#27491;&#23427;&#20204;&#65281;&#65288;arXiv&#65306;2311.08516v2 [cs.AI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#25105;&#32416;&#27491;&#22312;&#25913;&#21892;LLM&#36755;&#20986;&#30340;&#39118;&#26684;&#21644;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65288;&#20363;&#22914;Chen&#31561;&#65292;2023&#65307;Madaan&#31561;&#65292;2023&#65289;&#65292;&#26368;&#36817;&#23545;&#36923;&#36753;&#25110;&#25512;&#29702;&#38169;&#35823;&#36827;&#34892;&#33258;&#25105;&#32416;&#27491;&#30340;&#23581;&#35797;&#36890;&#24120;&#20250;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#21464;&#20026;&#38169;&#35823;&#65292;&#20174;&#32780;&#24635;&#20307;&#34920;&#29616;&#21464;&#24046;&#65288;Huang&#31561;&#65292;2023&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65306;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#12290;&#23545;&#20110;&#38169;&#35823;&#21457;&#29616;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;BIG-Bench Mistake&#65292;&#36825;&#26159;&#19968;&#20010;Chain-of-Thought&#25512;&#29702;&#36712;&#36857;&#20013;&#30340;&#36923;&#36753;&#38169;&#35823;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20026;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#25552;&#20379;&#22522;&#20934;&#25968;&#65292;&#24182;&#35777;&#26126;LLM&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#12290;&#23545;&#20110;&#36755;&#20986;&#32416;&#27491;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#28335;&#26041;&#27861;&#65292;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#21487;&#20197;&#22823;&#24133;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#22238;&#28335;&#35299;&#37322;&#20026;&#23545;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#36731;&#37327;&#32423;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;60-70&#65285;&#20934;&#30830;&#29575;&#19979;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70% accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHOT-GM&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.12081</link><description>&lt;p&gt;
DHOT-GM&#65306;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#23454;&#29616;&#40065;&#26834;&#22270;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework. (arXiv:2310.12081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHOT-GM&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#22270;&#21305;&#37197;&#26159;&#26368;&#37325;&#35201;&#30340;&#22270;&#20998;&#26512;&#20219;&#21153;&#20043;&#19968;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#21305;&#37197;&#22270;&#26102;&#20381;&#36182;&#20110;&#37051;&#25509;&#30697;&#38453;&#25110;&#33410;&#28857;&#23884;&#20837;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#19981;&#22815;&#20248;&#36234;&#65292;&#22240;&#20026;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22914;&#33410;&#28857;&#23646;&#24615;&#12289;&#23376;&#22270;&#32467;&#26500;&#31561;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#65288;HOT&#65289;&#26694;&#26550;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#31216;&#20026;DHOT-GM&#12290;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#22270;&#34920;&#31034;&#20026;&#19982;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#23545;&#24212;&#30340;&#19968;&#32452;&#20851;&#31995;&#30697;&#38453;&#12290;&#32473;&#23450;&#20004;&#20010;&#22270;&#65292;&#25105;&#20204;&#26522;&#20030;&#25152;&#26377;&#20851;&#31995;&#30697;&#38453;&#23545;&#65292;&#24182;&#33719;&#21462;&#23427;&#20204;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20026;&#35745;&#31639;&#20004;&#20010;&#22270;&#20043;&#38388;&#30340;HOT&#36317;&#31163;&#65292;&#27599;&#20010;&#22270;&#37117;&#26159;&#30001;&#20851;&#31995;&#30697;&#38453;&#34920;&#31034;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph matching is one of the most significant graph analytic tasks in practice, which aims to find the node correspondence across different graphs. Most existing approaches rely on adjacency matrices or node embeddings when matching graphs, whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. In this study, we propose a novel and effective graph matching method based on a differentiable hierarchical optimal transport (HOT) framework, called DHOT-GM. Essentially, our method represents each graph as a set of relational matrices corresponding to the information of different modalities. Given two graphs, we enumerate all relational matrix pairs and obtain their matching results, and accordingly, infer the node correspondence by the weighted averaging of the matching results. This method can be implemented as computing the HOT distance between the two graphs -- each matching 
&lt;/p&gt;</description></item><item><title>FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06763</link><description>&lt;p&gt;
FABind: &#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06763
&lt;/p&gt;
&lt;p&gt;
FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#23545;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#24182;&#20934;&#30830;&#39044;&#27979;&#20854;&#32467;&#21512;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#37319;&#26679;&#27861;&#21644;&#22238;&#24402;&#27861;&#25104;&#20026;&#20004;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;&#37319;&#26679;&#27861;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#32467;&#26500;&#26469;&#36827;&#34892;&#36873;&#25321;&#32780;&#25928;&#29575;&#36739;&#20302;&#12290;&#32780;&#22238;&#24402;&#27861;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#34507;&#30333;&#36136;&#22823;&#23567;&#30340;&#21464;&#21270;&#36890;&#24120;&#38656;&#35201;&#22806;&#37096;&#27169;&#22359;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#32467;&#21512;&#21475;&#34955;&#65292;&#36827;&#19968;&#27493;&#24433;&#21709;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind&#65292;&#19968;&#20010;&#23558;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30456;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
&lt;/p&gt;</description></item><item><title>&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#21644;&#30456;&#20851;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;RCA&#22312;&#24490;&#29615;&#20381;&#36182;&#25968;&#25454;&#19978;&#36820;&#22238;&#21333;&#19968;&#27010;&#24565;&#26684;&#23478;&#26063;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06441</link><description>&lt;p&gt;
&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stepwise functional refoundation of relational concept analysis. (arXiv:2310.06441v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06441
&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#21644;&#30456;&#20851;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;RCA&#22312;&#24490;&#29615;&#20381;&#36182;&#25968;&#25454;&#19978;&#36820;&#22238;&#21333;&#19968;&#27010;&#24565;&#26684;&#23478;&#26063;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#20801;&#35768;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#30456;&#20851;&#30340;&#35821;&#22659;&#12290;&#23427;&#34987;&#35774;&#35745;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25551;&#36848;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#20851;&#20110;RCA&#30340;&#19968;&#20010;&#20196;&#20154;&#22256;&#24785;&#30340;&#35266;&#23519;&#26159;&#65292;&#23613;&#31649;&#25968;&#25454;&#23384;&#22312;&#24490;&#29615;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#36820;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26684;&#23478;&#26063;&#65292;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#21487;&#25509;&#21463;&#30340;&#12290;RCA&#30340;&#35821;&#20041;&#20197;&#25805;&#20316;&#26041;&#24335;&#25552;&#20379;&#65292;&#23545;&#27492;&#38382;&#39064;&#24182;&#27809;&#26377;&#25552;&#20379;&#26126;&#30830;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#21487;&#25509;&#21463;&#30340;&#35299;&#20915;&#26041;&#26696;&#23450;&#20041;&#20026;&#23646;&#20110;&#21021;&#22987;&#35821;&#22659;&#30830;&#23450;&#30340;&#31354;&#38388;&#30340;&#27010;&#24565;&#26684;&#23478;&#26063;&#65288;&#33391;&#26500;&#65289;&#65292;&#19981;&#33021;&#25193;&#23637;&#26032;&#23646;&#24615;&#65288;&#39281;&#21644;&#65289;&#65292;&#24182;&#19988;&#20165;&#28041;&#21450;&#35813;&#23478;&#26063;&#30340;&#27010;&#24565;&#65288;&#33258;&#25903;&#25345;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#20197;&#21450;&#35813;&#31354;&#38388;&#19978;&#30340;&#20004;&#20010;&#20989;&#25968;&#65288;&#19968;&#20010;&#25193;&#24352;&#20989;&#25968;&#21644;&#19968;&#20010;&#25910;&#32553;&#20989;&#25968;&#65289;&#65292;&#37319;&#29992;&#21151;&#33021;&#35270;&#22270;&#26469;&#25551;&#36848;RCA&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#25509;&#21463;&#30340;&#35299;&#20915;&#26041;&#26696;&#8230;
&lt;/p&gt;
&lt;p&gt;
Relational concept analysis (RCA) is an extension of formal concept analysis allowing to deal with several related contexts simultaneously. It has been designed for learning description logic theories from data and used within various applications. A puzzling observation about RCA is that it returns a single family of concept lattices although, when the data feature circular dependencies, other solutions may be considered acceptable. The semantics of RCA, provided in an operational way, does not shed light on this issue. In this report, we define these acceptable solutions as those families of concept lattices which belong to the space determined by the initial contexts (well-formed), cannot scale new attributes (saturated), and refer only to concepts of the family (self-supported). We adopt a functional view on the RCA process by defining the space of well-formed solutions and two functions on that space: one expansive and the other contractive. We show that the acceptable solutions a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#20013;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14345</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#20559;&#24046;&#35780;&#20272;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Bias Assessment and Mitigation in LLM-based Code Generation. (arXiv:2309.14345v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14345
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#20013;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#32534;&#30721;&#36807;&#31243;&#30340;&#29983;&#20135;&#21147;&#21644;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;LLM&#22312;&#36719;&#20214;&#32534;&#30721;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#26222;&#21450;&#65292;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#24050;&#32463;&#20986;&#29616;&#65306;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#21253;&#21547;&#19982;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30456;&#20851;&#30340;&#31038;&#20250;&#20559;&#35265;&#65311;&#36825;&#20010;&#38382;&#39064;&#20851;&#31995;&#21040;&#20381;&#36182;&#20110;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#36719;&#20214;&#24212;&#29992;&#30340;&#23436;&#25972;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#36947;&#24503;&#22522;&#30784;&#65292;&#28982;&#32780;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#35774;&#35745;&#30340;&#26032;&#39062;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#20559;&#24046;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;9.68\%&#21040;37.37\%&#30340;&#20195;&#30721;&#20989;&#25968;&#30340;&#21151;&#33021;&#20351;
&lt;/p&gt;
&lt;p&gt;
Utilizing state-of-the-art Large Language Models (LLMs), automatic code generation models play a pivotal role in enhancing the productivity and efficiency of software development coding procedures. As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social biases, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models, yet is under-explored in the literature. This paper presents a novel bias assessment framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive evaluation on the bias of nine state-of-the-art LLM-based code generation models. Our findings reveal that first, 31.45\% to 79.93\% code functions generated by our evaluated code generation models are biased, and 9.68\% to 37.37\% code functions' funct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#23376;&#32676;&#20307;&#20013;&#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#20513;&#23548;&#20102;s-ID&#38382;&#39064;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#20174;&#23376;&#32676;&#20307;&#30340;&#35266;&#27979;&#20998;&#24067;&#20013;&#35782;&#21035;&#20986;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.02281</link><description>&lt;p&gt;
s-ID&#65306;&#22312;&#23376;&#32676;&#20307;&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
s-ID: Causal Effect Identification in a Sub-Population. (arXiv:2309.02281v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#23376;&#32676;&#20307;&#20013;&#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#20513;&#23548;&#20102;s-ID&#38382;&#39064;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#20174;&#23376;&#32676;&#20307;&#30340;&#35266;&#27979;&#20998;&#24067;&#20013;&#35782;&#21035;&#20986;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23376;&#32676;&#20307;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#28041;&#21450;&#21040;&#35782;&#21035;&#24178;&#39044;&#23545;&#29305;&#23450;&#23376;&#32452;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#36825;&#20123;&#23376;&#32452;&#36890;&#36807;&#25277;&#26679;&#36807;&#31243;&#20013;&#30340;&#31995;&#32479;&#20559;&#24046;&#19982;&#25972;&#20010;&#32676;&#20307;&#26377;&#25152;&#21306;&#21035;&#12290;&#28982;&#32780;&#65292;&#24573;&#30053;&#23376;&#32676;&#20307;&#24341;&#20837;&#30340;&#32454;&#24494;&#24046;&#21035;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#25512;&#26029;&#65292;&#25110;&#32773;&#38480;&#21046;&#29616;&#26377;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#24182;&#20513;&#23548;&#23376;&#32676;&#20307;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#65288;&#20197;&#19979;&#31616;&#31216;s-ID&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20165;&#21487;&#20197;&#35775;&#38382;&#30446;&#26631;&#23376;&#32676;&#20307;&#30340;&#35266;&#27979;&#25968;&#25454;&#65288;&#32780;&#19981;&#26159;&#25972;&#20010;&#32676;&#20307;&#65289;&#12290;&#29616;&#26377;&#30340;&#23376;&#32676;&#20307;&#25512;&#26029;&#38382;&#39064;&#26159;&#22522;&#20110;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#28304;&#20110;&#25972;&#20010;&#32676;&#20307;&#30340;&#21069;&#25552;&#65292;&#22240;&#27492;&#26080;&#27861;&#35299;&#20915;s-ID&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#22240;&#26524;&#22270;&#20013;&#24517;&#39035;&#28385;&#36275;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#20174;&#35813;&#23376;&#32676;&#20307;&#30340;&#35266;&#27979;&#20998;&#24067;&#20013;&#35782;&#21035;&#20986;&#23376;&#32676;&#20307;&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup, which is distinguished from the whole population through the influence of systematic biases in the sampling process. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LLM-Mob&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#20998;&#26512;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#21382;&#21490;&#20572;&#30041;&#21644;&#19978;&#19979;&#25991;&#20572;&#30041;&#30340;&#27010;&#24565;&#26469;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#26102;&#24577;&#24863;&#30693;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.15197</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#21435;&#21738;&#37324;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Where Would I Go Next? Large Language Models as Human Mobility Predictors. (arXiv:2308.15197v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LLM-Mob&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#20998;&#26512;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#21382;&#21490;&#20572;&#30041;&#21644;&#19978;&#19979;&#25991;&#20572;&#30041;&#30340;&#27010;&#24565;&#26469;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#26102;&#24577;&#24863;&#30693;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#27969;&#34892;&#30149;&#24314;&#27169;&#12289;&#20132;&#36890;&#35268;&#21010;&#21644;&#24212;&#24613;&#21709;&#24212;&#12290;&#30001;&#20110;&#31227;&#21160;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#21644;&#20154;&#20204;&#26085;&#24120;&#27963;&#21160;&#30340;&#38543;&#26426;&#24615;&#65292;&#23454;&#29616;&#23545;&#20154;&#20204;&#20301;&#32622;&#30340;&#31934;&#30830;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#34429;&#28982;&#26368;&#36817;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#20154;&#31867;&#31227;&#21160;&#30740;&#31350;&#20013;&#30340;&#36866;&#29992;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LLM-Mob&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#26469;&#20998;&#26512;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21382;&#21490;&#20572;&#30041;&#21644;&#19978;&#19979;&#25991;&#20572;&#30041;&#30340;&#27010;&#24565;&#65292;&#20197;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#20013;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#20449;&#24687;&#26469;&#23454;&#29616;&#26102;&#24577;&#24863;&#30693;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate human mobility prediction underpins many important applications across a variety of domains, including epidemic modelling, transport planning, and emergency responses. Due to the sparsity of mobility data and the stochastic nature of people's daily activities, achieving precise predictions of people's locations remains a challenge. While recently developed large language models (LLMs) have demonstrated superior performance across numerous language-related tasks, their applicability to human mobility studies remains unexplored. Addressing this gap, this article delves into the potential of LLMs for human mobility prediction tasks. We introduce a novel method, LLM-Mob, which leverages the language understanding and reasoning capabilities of LLMs for analysing human mobility data. We present concepts of historical stays and context stays to capture both long-term and short-term dependencies in human movement and enable time-aware prediction by using time information of the predic
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.15053</link><description>&lt;p&gt;
&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adapting text-based dialogue state tracker for spoken dialogues. (arXiv:2308.15053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#31454;&#36187;&#65288;DSTC&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#35821;&#38899;&#30028;&#38754;&#30340;&#31283;&#20581;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22823;&#37096;&#20998;&#36827;&#23637;&#37117;&#26159;&#38024;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22240;&#20026;&#26377;&#20016;&#23500;&#30340;&#20070;&#38754;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#65292;&#32780;&#20855;&#26377;&#21475;&#35821;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#31232;&#32570;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;Siri&#21644;Alexa&#31561;&#35821;&#38899;&#21161;&#25163;&#31995;&#32479;&#25152;&#23637;&#31034;&#30340;&#65292;&#23558;&#36825;&#31181;&#25104;&#21151;&#36716;&#31227;&#21040;&#21475;&#35821;&#23545;&#35805;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;DSTC11&#30340;&#20855;&#26377;&#35821;&#38899;&#24863;&#30693;&#30340;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#25361;&#25112;&#36187;&#20013;&#30340;&#39640;&#24230;&#25104;&#21151;&#27169;&#22411;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#65288;1&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#65292;&#20197;&#24357;&#21512;&#21475;&#35821;&#21644;&#25991;&#26412;&#35805;&#35821;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#65288;2&#65289;&#29992;&#20110;&#20272;&#35745;&#25554;&#27133;&#21644;&#20540;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;D3ST&#65289;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#25554;&#27133;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there have been remarkable advances in dialogue systems through the dialogue systems technology competition (DSTC), it remains one of the key challenges to building a robust task-oriented dialogue system with a speech interface. Most of the progress has been made for text-based dialogue systems since there are abundant datasets with written corpora while those with spoken dialogues are very scarce. However, as can be seen from voice assistant systems such as Siri and Alexa, it is of practical importance to transfer the success to spoken dialogues. In this paper, we describe our engineering effort in building a highly successful model that participated in the speech-aware dialogue systems technology challenge track in DSTC11. Our model consists of three major modules: (1) automatic speech recognition error correction to bridge the gap between the spoken and the text utterances, (2) text-based dialogue system (D3ST) for estimating the slots and values using slot descriptions, an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#20998;&#23618;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26426;&#22120;&#20154;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#23558;&#27599;&#20010;&#35268;&#33539;&#20998;&#35299;&#20026;&#21407;&#23376;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#25512;&#26029;&#19981;&#21516;&#35268;&#33539;&#20043;&#38388;&#23376;&#20219;&#21153;&#30340;&#26102;&#24207;&#20851;&#31995;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#38388;&#30340;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.10393</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#35299;&#30340;&#22810;&#26426;&#22120;&#20154;&#20998;&#23618;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#26041;&#27861;&#22312;&#20998;&#23618;&#26102;&#24207;&#36923;&#36753;&#35268;&#33539;&#19979;
&lt;/p&gt;
&lt;p&gt;
Decomposition-based Hierarchical Task Allocation and Planning for Multi-Robots under Hierarchical Temporal Logic Specifications. (arXiv:2308.10393v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#20998;&#23618;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26426;&#22120;&#20154;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#23558;&#27599;&#20010;&#35268;&#33539;&#20998;&#35299;&#20026;&#21407;&#23376;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#25512;&#26029;&#19981;&#21516;&#35268;&#33539;&#20043;&#38388;&#23376;&#20219;&#21153;&#30340;&#26102;&#24207;&#20851;&#31995;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#38388;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#24102;&#26377;&#26102;&#24207;&#36923;&#36753;&#35268;&#33539;&#30340;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#30740;&#31350;&#20027;&#35201;&#22522;&#20110;&#21333;&#19968;&#20844;&#24335;&#65292;&#29992;&#20110;&#21333;&#20010;&#25110;&#22810;&#20010;&#26426;&#22120;&#20154;&#30340;&#20219;&#21153;&#12290;&#20294;&#26159;&#38543;&#30528;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#26102;&#24207;&#36923;&#36753;&#20844;&#24335;&#19981;&#21487;&#36991;&#20813;&#22320;&#21464;&#24471;&#20887;&#38271;&#65292;&#20351;&#35299;&#37322;&#21644;&#35268;&#33539;&#29983;&#25104;&#21464;&#24471;&#22797;&#26434;&#65292;&#24182;&#19988;&#23545;&#35268;&#21010;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#20135;&#29983;&#21387;&#21147;&#12290;&#26368;&#36817;&#30340;&#21457;&#23637;&#26159;&#25552;&#20986;&#20102;&#26102;&#24207;&#36923;&#36753;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#26102;&#24207;&#36923;&#36753;&#35268;&#33539;&#65292;&#25552;&#20379;&#26356;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#25152;&#25552;&#20986;&#30340;&#35268;&#21010;&#31639;&#27861;&#20551;&#35774;&#27599;&#20010;&#35268;&#33539;&#20013;&#30340;&#26426;&#22120;&#20154;&#26159;&#29420;&#31435;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#22797;&#26434;&#26102;&#38388;&#32422;&#26463;&#30340;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#20998;&#23618;&#26694;&#26550;&#12290;&#22312;&#39640;&#23618;&#65292;&#27599;&#20010;&#35268;&#33539;&#39318;&#20808;&#34987;&#20998;&#35299;&#25104;&#19968;&#32452;&#21407;&#23376;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#26029;&#19981;&#21516;&#35268;&#33539;&#30340;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#26102;&#24207;&#20851;&#31995;&#26469;&#30830;ete&#26426;&#22120;&#20154;&#38388;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research into robotic planning with temporal logic specifications, notably Linear Temporal Logic (LTL), was largely based on singular formulas for individual or groups of robots. But with increasing task complexity, LTL formulas unavoidably grow lengthy, complicating interpretation and specification generation, and straining the computational capacities of the planners. A recent development has been the hierarchical representation of LTL [1] that contains multiple temporal logic specifications, providing a more interpretable framework. However, the proposed planning algorithm assumes the independence of robots within each specification, limiting their application to multi-robot coordination with complex temporal constraints. In this work, we formulated a decomposition-based hierarchical framework. At the high level, each specification is first decomposed into a set of atomic sub-tasks. We further infer the temporal relations among the sub-tasks of different specifications to const
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#22696;&#35199;&#21733;&#24037;&#20154;&#19968;&#30452;&#24314;&#35758;&#20302;&#34218;&#24037;&#20316;&#65292;&#24182;&#21521;&#22899;&#24615;&#26356;&#20542;&#21521;&#20110;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;LLMs&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02053</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#24179;&#31561;&#26426;&#20250;: &#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#25581;&#31034;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations. (arXiv:2308.02053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02053
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#22696;&#35199;&#21733;&#24037;&#20154;&#19968;&#30452;&#24314;&#35758;&#20302;&#34218;&#24037;&#20316;&#65292;&#24182;&#21521;&#22899;&#24615;&#26356;&#20542;&#21521;&#20110;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;LLMs&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#20102;&#35299;&#36825;&#20123;&#20559;&#35265;&#23545;&#20110;&#29702;&#35299;&#22312;&#20351;&#29992;LLMs&#36827;&#34892;&#20915;&#31574;&#26102;&#28508;&#22312;&#30340;&#21518;&#32493;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21382;&#21490;&#19978;&#22788;&#20110;&#21155;&#21183;&#30340;&#32676;&#20307;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#30340;&#35282;&#24230;&#20998;&#26512;&#21644;&#27604;&#36739;LLMs&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;ChatGPT&#21644;LLaMA&#36825;&#20004;&#20010;&#21069;&#27839;LLMs&#20869;&#30340;&#20132;&#21449;&#20559;&#35265;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20027;&#35201;&#38598;&#20013;&#22312;&#25581;&#31034;&#24615;&#21035;&#35748;&#21516;&#21644;&#22269;&#31821;&#20559;&#35265;&#19978;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#20154;&#21475;&#32479;&#35745;&#36523;&#20221;&#30340;&#20132;&#21449;&#20559;&#35265;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#26126;&#26174;&#30340;&#20559;&#35265;&#65292;&#20363;&#22914;&#20004;&#20010;&#27169;&#22411;&#19968;&#30452;&#24314;&#35758;&#22696;&#35199;&#21733;&#24037;&#20154;&#20174;&#20107;&#20302;&#34218;&#24037;&#20316;&#65292;&#25110;&#32773;&#26356;&#20542;&#21521;&#20110;&#21521;&#22899;&#24615;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#27979;&#37327;&#21644;&#29702;&#35299;LLMs&#20013;&#30340;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen widespread deployment in various real-world applications. Understanding these biases is crucial to comprehend the potential downstream consequences when using LLMs to make decisions, particularly for historically disadvantaged groups. In this work, we propose a simple method for analyzing and comparing demographic bias in LLMs, through the lens of job recommendations. We demonstrate the effectiveness of our method by measuring intersectional biases within ChatGPT and LLaMA, two cutting-edge LLMs. Our experiments primarily focus on uncovering gender identity and nationality bias; however, our method can be extended to examine biases associated with any intersection of demographic identities. We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women. Our study highlights the importance of measu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.10406</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Classification. (arXiv:2305.10406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#22411;&#25193;&#23637;&#65292;&#31216;&#20026;&#21464;&#20998;&#20998;&#31867; (VC)&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#65292;&#31867;&#20284;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#35777;&#25454;&#19979;&#30028; (ELBO) &#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;VC&#27169;&#22411;&#20801;&#35768;&#22312;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#29305;&#21035;&#26159;&#31867;&#26465;&#20214;&#28508;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#22312;&#29616;&#25104;&#30340;softmax&#20998;&#31867;&#22120;&#20013;&#20570;&#20986;&#30340;&#38544;&#24335;&#20551;&#35774;&#12290;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25913;&#21892;&#20102;&#20854;&#20182;&#33391;&#22909;&#29305;&#24615;&#65292;&#22914;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#22495;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#30340;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12858</link><description>&lt;p&gt;
&#38754;&#21521;&#31185;&#23398;&#30340;&#26080;&#30417;&#30563;&#22495;&#36716;&#31227;&#65306;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27169;&#22411;&#30340;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#20043;&#38388;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Transfer for Science: Exploring Deep Learning Methods for Translation between LArTPC Detector Simulations with Differing Response Models. (arXiv:2304.12858v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#30340;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#23547;&#27714;&#31616;&#21270;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#21644;&#21457;&#29616;&#30340;&#36335;&#24452;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;DL&#27169;&#22411;&#32463;&#24120;&#22312;&#27169;&#25311;&#32467;&#26524;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#30495;&#23454;&#23454;&#39564;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#20219;&#20309;&#31995;&#32479;&#24046;&#24322;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#8220;&#39046;&#22495;&#28418;&#31227;&#8221;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31995;&#32479;&#24046;&#24322;&#30340;&#29609;&#20855;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#26469;&#20943;&#23569;&#20004;&#20010;&#31995;&#32479;&#19981;&#21516;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#20004;&#32452;&#27169;&#25311;&#28082;&#27689;&#26102;&#38388;&#25237;&#24433;&#23460;&#65288;LArTPC&#65289;&#25506;&#27979;&#22120;&#20107;&#20214;&#26679;&#26412;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#36825;&#20123;&#26679;&#26412;&#34987;&#21019;&#24314;&#20197;&#25511;&#21046;&#22320;&#28436;&#31034;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#24120;&#35265;&#31995;&#32479;&#24046;&#24322;&#12290;LArTPC&#25506;&#27979;&#22120;&#20195;&#34920;&#20102;&#19979;&#19968;&#20195;&#31890;&#23376;&#25506;&#27979;&#22120;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) techniques have broad applications in science, especially in seeking to streamline the pathway to potential solutions and discoveries. Frequently, however, DL models are trained on the results of simulation yet applied to real experimental data. As such, any systematic differences between the simulated and real data may degrade the model's performance -- an effect known as "domain shift." This work studies a toy model of the systematic differences between simulated and real data. It presents a fully unsupervised, task-agnostic method to reduce differences between two systematically different samples. The method is based on the recent advances in unpaired image-to-image translation techniques and is validated on two sets of samples of simulated Liquid Argon Time Projection Chamber (LArTPC) detector events, created to illustrate common systematic differences between the simulated and real data in a controlled way. LArTPC-based detectors represent the next-generation pa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.10819</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#20449;&#20219;&#26435;&#34913;&#19979;&#30340;&#21512;&#25104;&#25968;&#25454;&#23457;&#35745;&#19982;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#12289;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#26377;&#27844;&#38706;&#25935;&#24863;&#21644;&#38544;&#31169;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#24819;&#27861;&#65292;&#20197;&#20943;&#36731;&#30495;&#23454;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39118;&#38505;&#12289;&#20559;&#35265;&#12289;&#20260;&#23475;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20010;&#27010;&#24565;&#20381;&#36182;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#19981;&#20559;&#25191;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#24544;&#23454;&#20110;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#26032;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#36825;&#31181;&#26041;&#27861;&#26159;&#21542;&#20817;&#29616;&#20102;&#20854;&#25215;&#35834;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#23427;&#20204;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#22260;&#32469;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35745;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#38134;&#34892;&#12289;&#20154;&#21147;&#36164;&#28304;&#65292;&#20197;&#21450;&#20174;&#34920;&#26684;&#65292;&#26102;&#38388;&#24207;&#21015;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#29992;&#20363;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#24179;&#34913;&#20449;&#20219;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32422;&#26463;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#38271;&#26399;&#21644;&#20016;&#23500;&#32422;&#26463;&#30340;&#20219;&#21153;&#65292;&#22312;&#26426;&#22120;&#20154;&#28165;&#27905;&#25151;&#23627;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10639</link><description>&lt;p&gt;
&#36890;&#36807;&#32422;&#26463;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#38271;&#26399;&#21644;&#20016;&#23500;&#32422;&#26463;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Handling Long and Richly Constrained Tasks through Constrained Hierarchical Reinforcement Learning. (arXiv:2302.10639v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32422;&#26463;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#38271;&#26399;&#21644;&#20016;&#23500;&#32422;&#26463;&#30340;&#20219;&#21153;&#65292;&#22312;&#26426;&#22120;&#20154;&#28165;&#27905;&#25151;&#23627;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#22312;&#30446;&#26631;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35774;&#32622;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#23545;&#36712;&#36857;&#26045;&#21152;&#32422;&#26463;&#26469;&#22788;&#29702;&#23433;&#20840;&#38382;&#39064;&#65292;&#23545;&#20110;&#30701;&#26399;&#20219;&#21153;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#35299;&#20915;&#26102;&#38388;&#19978;&#24310;&#32493;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#22312;&#28165;&#27905;&#25151;&#23627;&#30340;&#19981;&#21516;&#21306;&#22495;&#26102;&#65292;&#38656;&#35201;&#36991;&#24320;&#28287;&#28369;&#21644;&#19981;&#23433;&#20840;&#30340;&#21306;&#22495;&#65288;&#20363;&#22914;&#27004;&#26799;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#36275;&#22815;&#30340;&#30005;&#37327;&#31227;&#21160;&#21040;&#20805;&#30005;&#31449;&#65307;&#32780;&#19988;&#38754;&#20020;&#22797;&#26434;&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#23558;&#19978;&#23618;&#30340;&#32422;&#26463;&#25628;&#32034;&#20195;&#29702;&#65288;&#20174;&#32473;&#23450;&#30340;&#36215;&#22987;&#29366;&#24577;&#21040;&#36828;&#22788;&#30446;&#26631;&#29366;&#24577;&#35745;&#31639;&#26368;&#22823;&#21270;&#22238;&#25253;&#31574;&#30053;&#65292;&#21516;&#26102;&#28385;&#36275;&#25104;&#26412;&#32422;&#26463;&#65289;&#19982;&#24213;&#23618;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#20272;&#35745;&#22312;&#38468;&#36817;&#29366;&#24577;&#20043;&#38388;&#31227;&#21160;&#30340;&#25104;&#26412;&#21644;&#22238;&#25253;&#20540;&#65289;&#32467;&#21512;&#20351;&#29992;&#30340;&#23433;&#20840;&#32422;&#26463;&#25628;&#32034;&#19982;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;CoSHRL&#65289;&#26426;&#21046;&#12290;CoSHRL&#30340;&#19968;&#20010;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#23427;&#21487;&#20197;&#22788;&#29702;&#23545;&#36712;&#36857;&#19978;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety in goal directed Reinforcement Learning (RL) settings has typically been handled through constraints over trajectories and have demonstrated good performance in primarily short horizon tasks. In this paper, we are specifically interested in the problem of solving temporally extended decision making problems such as robots cleaning different areas in a house while avoiding slippery and unsafe areas (e.g., stairs) and retaining enough charge to move to a charging dock; in the presence of complex safety constraints. Our key contribution is a (safety) Constrained Search with Hierarchical Reinforcement Learning (CoSHRL) mechanism that combines an upper level constrained search agent (which computes a reward maximizing policy from a given start to a far away goal state while satisfying cost constraints) with a low-level goal conditioned RL agent (which estimates cost and reward values to move between nearby states). A major advantage of CoSHRL is that it can handle constraints on the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;&#65292;&#36890;&#36807;&#29289;&#29702;&#35770;&#35777;&#25512;&#23548;&#20986;&#19968;&#31867;&#26032;&#22411;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09580</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator. (arXiv:2302.09580v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;&#65292;&#36890;&#36807;&#29289;&#29702;&#35770;&#35777;&#25512;&#23548;&#20986;&#19968;&#31867;&#26032;&#22411;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#12290;&#21327;&#26041;&#24046;&#26680;&#26159;&#39640;&#26031;&#36807;&#31243;&#30340;&#20027;&#35201;&#24341;&#25806;&#65292;&#21253;&#21547;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#26102;&#31354;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#65292;&#21512;&#36866;&#30340;&#26680;&#24212;&#35813;&#24314;&#27169;&#32852;&#21512;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#21487;&#20998;&#31163;&#30340;&#26102;&#31354;&#21327;&#26041;&#24046;&#26680;&#25552;&#20379;&#20102;&#31616;&#21333;&#21644;&#35745;&#31639;&#25928;&#29575;&#36739;&#39640;&#30340;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38750;&#21487;&#20998;&#31163;&#26680;&#21253;&#21547;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#30456;&#20851;&#24615;&#30340;&#26102;&#31354;&#20132;&#20114;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#20855;&#26377;&#26174;&#24335;&#34920;&#36798;&#24335;&#30340;&#38750;&#21487;&#20998;&#31163;&#26680;&#26159;&#22522;&#20110;&#25968;&#23398;&#32771;&#34385;&#65288;&#21487;&#20801;&#35768;&#26465;&#20214;&#65289;&#32780;&#38750;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#35770;&#35777;&#30340;&#28151;&#21512;&#35889;&#26041;&#27861;&#26469;&#29983;&#25104;&#21327;&#26041;&#24046;&#26680;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#25512;&#23548;&#20102;&#19968;&#31867;&#26032;&#22411;&#30340;&#29289;&#29702;&#21160;&#26426;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#23427;&#20204;&#30340;&#26681;&#28304;&#26469;&#33258;&#38543;&#26426;&#32447;&#24615;...
&lt;/p&gt;
&lt;p&gt;
Gaussian processes provide a flexible, non-parametric framework for the approximation of functions in high-dimensional spaces. The covariance kernel is the main engine of Gaussian processes, incorporating correlations that underpin the predictive distribution. For applications with spatiotemporal datasets, suitable kernels should model joint spatial and temporal dependence. Separable space-time covariance kernels offer simplicity and computational efficiency. However, non-separable kernels include space-time interactions that better capture observed correlations. Most non-separable kernels that admit explicit expressions are based on mathematical considerations (admissibility conditions) rather than first-principles derivations. We present a hybrid spectral approach for generating covariance kernels which is based on physical arguments. We use this approach to derive a new class of physically motivated, non-separable covariance kernels which have their roots in the stochastic, linear, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#24863;&#30693;&#30340;SMT&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#28151;&#21512;&#39046;&#22495;&#30340;&#27010;&#29575;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#36991;&#20813;&#29983;&#25104;&#20887;&#20313;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.06188</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#24863;&#30693;&#30340;SMT&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhancing SMT-based Weighted Model Integration by Structure Awareness. (arXiv:2302.06188v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#24863;&#30693;&#30340;SMT&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#28151;&#21512;&#39046;&#22495;&#30340;&#27010;&#29575;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#36991;&#20813;&#29983;&#25104;&#20887;&#20313;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31934;&#30830;&#21644;&#36817;&#20284;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#30340;&#24320;&#21457;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#34429;&#28982;&#22312;&#22788;&#29702;&#32431;&#31163;&#25955;&#25110;&#32431;&#36830;&#32493;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#20294;&#23558;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36866;&#24212;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#21450;&#20854;&#20851;&#31995;&#30340;&#28151;&#21512;&#39046;&#22495;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#65288;WMI&#65289;&#26368;&#36817;&#25104;&#20026;&#28151;&#21512;&#39046;&#22495;&#27010;&#29575;&#25512;&#26029;&#30340;&#32479;&#19968;&#24418;&#24335;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#30456;&#24403;&#25968;&#37327;&#30340;&#24037;&#20316;&#65292;&#20294;&#20351;WMI&#31639;&#27861;&#38543;&#30528;&#28151;&#21512;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#32780;&#25193;&#23637;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#20123;&#37325;&#22823;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23558;&#22522;&#20110;SMT&#30340;&#26522;&#20030;&#65288;&#19968;&#31181;&#24418;&#24335;&#39564;&#35777;&#20013;&#30340;&#26377;&#25928;&#25216;&#26415;&#65289;&#19982;&#38382;&#39064;&#32467;&#26500;&#30340;&#26377;&#25928;&#32534;&#30721;&#30456;&#32467;&#21512;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#36991;&#20813;&#29983;&#25104;&#20887;&#20313;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of efficient exact and approximate algorithms for probabilistic inference is a long-standing goal of artificial intelligence research. Whereas substantial progress has been made in dealing with purely discrete or purely continuous domains, adapting the developed solutions to tackle hybrid domains, characterised by discrete and continuous variables and their relationships, is highly non-trivial. Weighted Model Integration (WMI) recently emerged as a unifying formalism for probabilistic inference in hybrid domains. Despite a considerable amount of recent work, allowing WMI algorithms to scale with the complexity of the hybrid problem is still a challenge. In this paper we highlight some substantial limitations of existing state-of-the-art solutions, and develop an algorithm that combines SMT-based enumeration, an efficient technique in formal verification, with an effective encoding of the problem structure. This allows our algorithm to avoid generating redundant models, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;Transformer&#21644;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20803;&#23398;&#20064;&#35757;&#32451;&#25104;&#20026;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#36827;&#34892;&#27979;&#35797;&#38598;&#39044;&#27979;&#65292;&#26080;&#38656;&#23450;&#20041;&#25512;&#29702;&#27169;&#22411;&#12289;&#35757;&#32451;&#25439;&#22833;&#25110;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.04458</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;Transformer&#23454;&#29616;&#36890;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
General-Purpose In-Context Learning by Meta-Learning Transformers. (arXiv:2212.04458v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;Transformer&#21644;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20803;&#23398;&#20064;&#35757;&#32451;&#25104;&#20026;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#36827;&#34892;&#27979;&#35797;&#38598;&#39044;&#27979;&#65292;&#26080;&#38656;&#23450;&#20041;&#25512;&#29702;&#27169;&#22411;&#12289;&#35757;&#32451;&#25439;&#22833;&#25110;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#35201;&#27714;&#31995;&#32479;&#35774;&#35745;&#32773;&#25351;&#23450;&#23398;&#20064;&#27969;&#31243;&#30340;&#26041;&#26041;&#38754;&#38754;&#65292;&#20363;&#22914;&#25439;&#22833;&#20989;&#25968;&#12289;&#26550;&#26500;&#21644;&#20248;&#21270;&#22120;&#12290;&#32780;&#20803;&#23398;&#20064;&#65292;&#25110;&#32773;&#23398;&#20250;&#23398;&#20064;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#36825;&#20123;&#26041;&#38754;&#65292;&#24182;&#25215;&#35834;&#20197;&#26356;&#23569;&#30340;&#25163;&#21160;&#24037;&#20316;&#24320;&#21551;&#26356;&#22823;&#30340;&#33021;&#21147;&#12290;&#20803;&#23398;&#20064;&#30340;&#19968;&#20010;&#29305;&#21035;&#38596;&#24515;&#21187;&#21187;&#30340;&#30446;&#26631;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#65292;&#20165;&#20351;&#29992;&#24102;&#26377;&#26368;&#23567;&#24402;&#32435;&#20559;&#35265;&#30340;&#40657;&#30418;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#25509;&#25910;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#20135;&#29983;&#27979;&#35797;&#38598;&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#23450;&#20041;&#25512;&#29702;&#27169;&#22411;&#12289;&#35757;&#32451;&#25439;&#22833;&#25110;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#21644;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#21487;&#20197;&#34987;&#20803;&#35757;&#32451;&#25104;&#20026;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#22823;&#23567;&#12289;&#20219;&#21153;&#25968;&#37327;&#21644;&#20803;&#20248;&#21270;&#24341;&#36215;&#30340;&#31639;&#27861;&#20043;&#38388;&#30340;&#36716;&#25442;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#35760;&#24518;&#65292;&#36824;&#26377;&#19968;&#20123;&#31639;&#27861;&#26681;&#26412;&#26080;&#27861;&#36827;&#34892;&#20803;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. One particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. Such a model takes in training data, and produces test-set predictions across a wide range of problems, without any explicit definition of an inference model, training loss, or optimization algorithm. In this paper we show that Transformers and other black-box models can be meta-trained to act as general-purpose in-context learners. We characterize transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-opti
&lt;/p&gt;</description></item><item><title>DyG2Vec&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21462;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.16906</link><description>&lt;p&gt;
DyG2Vec: &#24102;&#26377;&#33258;&#30417;&#30563;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision. (arXiv:2210.16906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16906
&lt;/p&gt;
&lt;p&gt;
DyG2Vec&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21462;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#26102;&#38388;&#27169;&#24335;&#26469;&#23398;&#20064;&#24402;&#32435;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#24120;&#24120;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#35760;&#24518;&#27169;&#22359;&#25110;&#20302;&#25928;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#26469;&#26500;&#24314;&#26102;&#38388;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#21160;&#24577;&#22270;&#32534;&#30721;&#22120;&#19981;&#23481;&#26131;&#36866;&#24212;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#26102;&#38388;&#36793;&#32534;&#30721;&#21644;&#22522;&#20110;&#31383;&#21475;&#30340;&#23376;&#22270;&#37319;&#26679;&#26469;&#29983;&#25104;&#20219;&#21153;&#26080;&#20851;&#30340;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#27604;SSL&#30340;&#32852;&#21512;&#23884;&#20837;&#26550;&#26500;&#65292;&#20197;&#23398;&#20064;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#32780;&#19981;&#38656;&#35201;&#26631;&#31614;&#12290;&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20256;&#23548;&#35774;&#32622;&#21644;&#24402;&#32435;&#35774;&#32622;&#30340;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#24179;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;SoTA&#22522;&#32447;4.23&#65285;&#21644;3.30&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. In addition, the existing dynamic graph encoders are non-trivial to adapt to self-supervised paradigms, which prevents them from utilizing unlabeled data. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.07675</link><description>&lt;p&gt;
&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#20197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65306;&#22312;&#33647;&#29289;&#24320;&#21457;&#20013;&#21457;&#29616;&#32452;&#32455;&#23398;&#25913;&#21464;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#30340;&#31995;&#32479;&#12290;&#22312;&#32452;&#32455;&#23398;&#20013;&#65292;&#27491;&#24120;&#26679;&#26412;&#36890;&#24120;&#26159;&#22823;&#37327;&#23384;&#22312;&#30340;&#65292;&#32780;&#24322;&#24120;&#65288;&#30149;&#29702;&#65289;&#24773;&#20917;&#36890;&#24120;&#24456;&#23569;&#25110;&#19981;&#21487;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21333;&#31867;&#20998;&#31867;&#22120;&#21487;&#20197;&#26816;&#27979;&#21040;&#20998;&#24067;&#22806;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#19982;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#21069;&#24050;&#32463;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#12290;&#20294;&#26159;&#65292;&#39044;&#35757;&#32451;&#30340;&#29616;&#25104;CNN&#34920;&#31034;&#21487;&#33021;&#23545;&#32452;&#32455;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#19981;&#25935;&#24863;&#65292;&#32780;&#20581;&#24247;&#32452;&#32455;&#30340;&#33258;&#28982;&#21464;&#24322;&#21487;&#33021;&#23548;&#33268;&#36828;&#31163;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20351;&#34920;&#31034;&#36866;&#24212;&#20581;&#24247;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#36741;&#21161;&#20219;&#21153;&#19978;&#35757;&#32451;CNN&#65292;&#35813;&#20219;&#21153;&#21306;&#20998;&#19981;&#21516;&#29289;&#31181;&#12289;&#22120;&#23448;&#21644;&#26579;&#33394;&#35797;&#21058;&#30340;&#20581;&#24247;&#32452;&#32455;&#12290;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;&#65292;&#22240;&#20026;&#20581;&#24247;&#26679;&#26412;&#21487;&#20197;&#33258;&#21160;&#33719;&#24471;&#19978;&#36848;&#26631;&#31614;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#24378;&#21046;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#25554;&#20837;&#24322;&#24418;&#23383;&#65292;&#27169;&#22411;&#20250;&#21453;&#26144;&#29983;&#25104;&#22270;&#29255;&#20013;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#12290;&#36825;&#19968;&#29616;&#35937;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#27169;&#22411;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#12290;&#32780;&#24694;&#24847;&#29992;&#25143;&#25110;&#26381;&#21153;&#25552;&#20379;&#21830;&#36824;&#21487;&#33021;&#21033;&#29992;&#31867;&#20284;&#22806;&#24418;&#30340;&#38750;&#25289;&#19969;&#23383;&#31526;&#65292;&#25925;&#24847;&#24341;&#20837;&#20559;&#35265;&#65292;&#21019;&#36896;&#31181;&#26063;&#20027;&#20041;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2209.08891</link><description>&lt;p&gt;
&#21033;&#29992;&#21516;&#24418;&#24322;&#20041;&#23383;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#25366;&#25496;&#25991;&#21270;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis. (arXiv:2209.08891v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08891
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#25554;&#20837;&#24322;&#24418;&#23383;&#65292;&#27169;&#22411;&#20250;&#21453;&#26144;&#29983;&#25104;&#22270;&#29255;&#20013;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#12290;&#36825;&#19968;&#29616;&#35937;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#27169;&#22411;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#12290;&#32780;&#24694;&#24847;&#29992;&#25143;&#25110;&#26381;&#21153;&#25552;&#20379;&#21830;&#36824;&#21487;&#33021;&#21033;&#29992;&#31867;&#20284;&#22806;&#24418;&#30340;&#38750;&#25289;&#19969;&#23383;&#31526;&#65292;&#25925;&#24847;&#24341;&#20837;&#20559;&#35265;&#65292;&#21019;&#36896;&#31181;&#26063;&#20027;&#20041;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E 2&#21644;Stable Diffusion&#65292;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#21644;&#24191;&#22823;&#20844;&#20247;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#25551;&#32472;&#21508;&#31181;&#27010;&#24565;&#21644;&#39118;&#26684;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#37319;&#29992;&#20102;&#19982;&#29305;&#23450;Unicode&#33050;&#26412;&#30456;&#20851;&#30340;&#25991;&#21270;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#19981;&#20250;&#31435;&#21363;&#26174;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#25991;&#26412;&#25551;&#36848;&#20013;&#31616;&#21333;&#25554;&#20837;&#21333;&#20010;&#38750;&#25289;&#19969;&#23383;&#31526;&#65292;&#24120;&#35265;&#27169;&#22411;&#21576;&#29616;&#20986;&#29983;&#25104;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#12290;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#20102;&#36825;&#31181;&#34892;&#20026;&#65292;&#24182;&#30830;&#23450;&#20102;&#27169;&#22411;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#26159;&#36825;&#19968;&#29616;&#35937;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#24694;&#24847;&#29992;&#25143;&#25110;&#26381;&#21153;&#25552;&#20379;&#21830;&#21487;&#33021;&#35797;&#22270;&#24847;&#22270;&#24615;&#22320;&#36890;&#36807;&#23558;&#25289;&#19969;&#23383;&#31526;&#26367;&#25442;&#20026;&#38750;&#25289;&#19969;&#33050;&#26412;&#20013;&#22806;&#24418;&#30456;&#20284;&#30340;&#23383;&#31526;&#65292;&#26469;&#24341;&#20837;&#20559;&#35265;&#65292;&#21019;&#36896;&#31181;&#26063;&#20027;&#20041;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for text-to-image synthesis, such as DALL-E~2 and Stable Diffusion, have recently drawn a lot of interest from academia and the general public. These models are capable of producing high-quality images that depict a variety of concepts and styles when conditioned on textual descriptions. However, these models adopt cultural characteristics associated with specific Unicode scripts from their vast amount of training data, which may not be immediately apparent. We show that by simply inserting single non-Latin characters in a textual description, common models reflect cultural stereotypes and biases in their generated images. We analyze this behavior both qualitatively and quantitatively, and identify a model's text encoder as the root cause of the phenomenon. Additionally, malicious users or service providers may try to intentionally bias the image generation to create racist stereotypes by replacing Latin characters with similarly-looking characters from non-Latin scripts, so-cal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#34913;&#37327;&#30740;&#31350;&#32773;&#22312;&#35813;&#39046;&#22495;&#30340;&#24433;&#21709;&#12289;&#24433;&#21709;&#21147;&#21644;&#39046;&#23548;&#21147;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22312;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#21644;&#28436;&#21464;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#23398;&#26415;&#35770;&#25991;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20840;&#38754;&#30340;&#24341;&#29992;&#21644;&#21512;&#20316;&#25968;&#25454;&#38598;&#65292;&#23545;&#30456;&#20851;&#20851;&#31995;&#36827;&#34892;&#20102;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2205.13131</link><description>&lt;p&gt;
&#35770;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#28436;&#21464;&#65306;&#26397;&#30528;&#22312;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#20250;&#35758;&#19978;&#34913;&#37327;&#21644;&#29702;&#35299;&#24433;&#21709;&#12289;&#24433;&#21709;&#21147;&#21644;&#39046;&#23548;&#21147;&#30340;&#20803;&#32423;&#30340;&#26041;&#21521;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
On the Evolution of A.I. and Machine Learning: Towards a Meta-level Measuring and Understanding Impact, Influence, and Leadership at Premier A.I. Conferences. (arXiv:2205.13131v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#34913;&#37327;&#30740;&#31350;&#32773;&#22312;&#35813;&#39046;&#22495;&#30340;&#24433;&#21709;&#12289;&#24433;&#21709;&#21147;&#21644;&#39046;&#23548;&#21147;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22312;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#21644;&#28436;&#21464;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#23398;&#26415;&#35770;&#25991;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20840;&#38754;&#30340;&#24341;&#29992;&#21644;&#21512;&#20316;&#25968;&#25454;&#38598;&#65292;&#23545;&#30456;&#20851;&#20851;&#31995;&#36827;&#34892;&#20102;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29616;&#22312;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#23545;&#20154;&#31867;&#29983;&#27963;&#20135;&#29983;&#24191;&#27867;&#24433;&#21709;&#30340;&#36890;&#29992;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#30740;&#31350;&#32773;&#23545;&#35813;&#39046;&#22495;&#30340;&#36129;&#29486;&#35282;&#24230;&#26469;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#30340;&#28436;&#21464;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#24433;&#21709;&#12289;&#24433;&#21709;&#21147;&#21644;&#39046;&#23548;&#21147;&#30340;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#33258;1969&#24180;&#39318;&#27425;&#20030;&#21150;&#22269;&#38469;&#20154;&#24037;&#26234;&#33021;&#32852;&#21512;&#20250;&#35758; (IJCAI) &#20197;&#26469;&#22312;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#21644;&#28436;&#21464;&#36827;&#34892;&#25506;&#32034;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#21382;&#21490;&#21644;&#28436;&#21464;&#26377;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21644;&#28436;&#21464;&#23548;&#33268;&#20102;&#23398;&#26415;&#35770;&#25991;&#30340;&#22686;&#21152;&#65292;&#22312;&#36807;&#21435;&#30340;&#20845;&#21313;&#24180;&#26469;&#21457;&#34920;&#30340;&#25991;&#31456;&#25968;&#37327;&#20063;&#26377;&#25152;&#21453;&#26144;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20840;&#38754;&#30340;&#24341;&#29992;&#21512;&#20316;&#19982;&#35770;&#25991;-&#20316;&#32773;&#25968;&#25454;&#38598;&#65292;&#24182;&#35745;&#31639;&#20102;&#21512;&#20316;&#24341;&#29992;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence is now recognized as a general-purpose technology with ample impact on human life. This work aims at understanding the evolution of AI and, in particular Machine learning, from the perspective of researchers' contributions to the field. In order to do so, we present several measures allowing the analyses of AI and machine learning researchers' impact, influence, and leadership over the last decades. This work also contributes, to a certain extent, to shed new light on the history and evolution of AI by exploring the dynamics involved in the field's evolution by looking at papers published at the flagship AI and machine learning conferences since the first International Joint Conference on Artificial Intelligence (IJCAI) held in 1969. AI development and evolution have led to increasing research output, reflected in the number of articles published over the last sixty years. We construct comprehensive citation collaboration and paper-author datasets and compute co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#30740;&#20102;&#33647;&#29289;&#35774;&#35745;&#39046;&#22495;&#20013;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#20998;&#20026;&#19968;&#27493;&#21040;&#20301;&#27861;&#12289;&#22522;&#20110;&#29255;&#27573;&#27861;&#21644;&#33410;&#28857;&#36880;&#20010;&#27861;&#19977;&#31867;&#65292;&#20171;&#32461;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2202.09212</link><description>&lt;p&gt;
&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#20998;&#23376;&#29983;&#25104;&#65306;&#22270;&#23398;&#20064;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Molecule Generation for Drug Design: a Graph Learning Perspective. (arXiv:2202.09212v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#30740;&#20102;&#33647;&#29289;&#35774;&#35745;&#39046;&#22495;&#20013;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#20998;&#20026;&#19968;&#27493;&#21040;&#20301;&#27861;&#12289;&#22522;&#20110;&#29255;&#27573;&#27861;&#21644;&#33410;&#28857;&#36880;&#20010;&#27861;&#19977;&#31867;&#65292;&#20171;&#32461;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22270;&#23398;&#20064;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#33719;&#24471;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#35748;&#21487;&#65292;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#39046;&#22495;&#26159;&#33647;&#29289;&#35774;&#35745;&#21644;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#21046;&#33647;&#34892;&#19994;&#20013;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#25552;&#20379;&#20102;&#33647;&#29289;&#35774;&#35745;&#39046;&#22495;&#29616;&#26377;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#29305;&#21035;&#20851;&#27880;&#23558;&#65288;&#28145;&#24230;&#65289;&#22270;&#23398;&#20064;&#25216;&#26415;&#32435;&#20837;&#20854;&#20013;&#30340;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#32452;&#65306;&#19968;&#65289;&#19968;&#27493;&#21040;&#20301;&#27861;&#65292;&#20108;&#65289;&#22522;&#20110;&#29255;&#27573;&#27861;&#65292;&#19977;&#65289;&#33410;&#28857;&#36880;&#20010;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#20851;&#38190;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#24182;&#27010;&#36848;&#20102;&#20998;&#23376;&#29983;&#25104;&#21644;&#20248;&#21270;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#29616;&#26377;&#30340;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning, particularly graph learning, is gaining increasing recognition for its transformative impact across various fields. One such promising application is in the realm of molecule design and discovery, notably within the pharmaceutical industry. Our survey offers a comprehensive overview of state-of-the-art methods in molecule design, particularly focusing on \emph{de novo} drug design, which incorporates (deep) graph learning techniques. We categorize these methods into three distinct groups: \emph{i)} \emph{all-at-once}, \emph{ii)} \emph{fragment-based}, and \emph{iii)} \emph{node-by-node}. Additionally, we introduce some key public datasets and outline the commonly used evaluation metrics for both the generation and optimization of molecules. In the end, we discuss the existing challenges in this field and suggest potential directions for future research.
&lt;/p&gt;</description></item><item><title>PHPQ&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#30340;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#37327;&#21270;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#27169;&#22359;&#25429;&#33719;&#21644;&#20445;&#30041;&#22810;&#23618;&#27425;&#29305;&#24449;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#37327;&#21270;&#27169;&#22359;&#26469;&#25552;&#39640;&#21704;&#24076;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2109.05206</link><description>&lt;p&gt;
PHPQ: &#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#37327;&#21270;&#29992;&#20110;&#39640;&#25928;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
PHPQ: Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval. (arXiv:2109.05206v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.05206
&lt;/p&gt;
&lt;p&gt;
PHPQ&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#30340;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#37327;&#21270;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#27169;&#22359;&#25429;&#33719;&#21644;&#20445;&#30041;&#22810;&#23618;&#27425;&#29305;&#24449;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#37327;&#21270;&#27169;&#22359;&#26469;&#25552;&#39640;&#21704;&#24076;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20854;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#25928;&#29575;&#65292;&#28145;&#24230;&#21704;&#24076;&#26041;&#27861;&#65288;&#21253;&#25324;&#28145;&#24230;&#37327;&#21270;&#21644;&#28145;&#24230;&#20108;&#36827;&#21046;&#21704;&#24076;&#65289;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#22270;&#20687;&#26816;&#32034;&#30340;&#24120;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21704;&#24076;&#26041;&#27861;&#23545;&#20110;&#32454;&#31890;&#24230;&#26816;&#32034;&#26080;&#27861;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#26368;&#21518;&#19968;&#20010;CNN&#23618;&#30340;&#36755;&#20986;&#29983;&#25104;&#20108;&#36827;&#21046;&#30721;&#12290;&#30001;&#20110;&#26356;&#28145;&#30340;&#23618;&#20542;&#21521;&#20110;&#23558;&#35270;&#35273;&#32447;&#32034;&#65288;&#22914;&#32441;&#29702;&#65289;&#24635;&#32467;&#20026;&#25277;&#35937;&#30340;&#35821;&#20041;&#65288;&#22914;&#29399;&#21644;&#29483;&#65289;&#65292;&#26368;&#21518;&#19968;&#20010;CNN&#23618;&#20135;&#29983;&#30340;&#29305;&#24449;&#22312;&#25429;&#25417;&#27973;&#23618;&#20013;&#23384;&#22312;&#20294;&#20855;&#26377;&#36776;&#21035;&#21147;&#30340;&#32454;&#24494;&#35270;&#35273;&#32454;&#33410;&#26041;&#38754;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#25913;&#21892;&#32454;&#31890;&#24230;&#22270;&#20687;&#21704;&#24076;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#37327;&#21270;&#65288;PHPQ&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#65288;PHP&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#22810;&#23618;&#27425;&#29305;&#24449;&#20013;&#25429;&#33719;&#21644;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24378;&#35843;&#19981;&#21516;&#23376;&#31867;&#21035;&#20043;&#38388;&#30340;&#32454;&#24494;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#37327;&#21270;&#27169;&#22359;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#21704;&#24076;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep hashing approaches, including deep quantization and deep binary hashing, have become a common solution to large-scale image retrieval due to their high computation and storage efficiency. Most existing hashing methods cannot produce satisfactory results for fine-grained retrieval, because they usually adopt the outputs of the last CNN layer to generate binary codes. Since deeper layers tend to summarize visual clues, e.g., texture, into abstract semantics, e.g., dogs and cats, the feature produced by the last CNN layer is less effective in capturing subtle but discriminative visual details that mostly exist in shallow layers. To improve fine-grained image hashing, we propose Pyramid Hybrid Pooling Quantization (PHPQ). Specifically, we propose a Pyramid Hybrid Pooling (PHP) module to capture and preserve fine-grained semantic information from multi-level features, which emphasizes the subtle discrimination of different sub-categories. Besides, we propose a learnable quantization mo
&lt;/p&gt;</description></item></channel></rss>